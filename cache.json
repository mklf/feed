{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning to Ignore Adversarial Attacks. (arXiv:2205.11551v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11551","description":"<p>Despite the strong performance of current NLP models, they can be brittle\nagainst adversarial attacks. To enable effective learning against adversarial\ninputs, we introduce the use of rationale models that can explicitly learn to\nignore attack tokens. We find that the rationale models can successfully ignore\nover 90\\% of attack tokens. This approach leads to consistent sizable\nimprovements ($\\sim$10\\%) over baseline models in robustness on three datasets\nfor both BERT and RoBERTa, and also reliably outperforms data augmentation with\nadversarial examples alone. In many cases, we find that our method is able to\nclose the gap between model performance on a clean test set and an attacked\ntest set and hence reduce the effect of adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangqiaoyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carton_S/0/1/0/all/0/1\">Samuel Carton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Recurrence Improves Masked Language Models. (arXiv:2205.11588v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11588","description":"<p>In this work, we explore whether modeling recurrence into the Transformer\narchitecture can both be beneficial and efficient, by building an extremely\nsimple recurrent module into the Transformer. We compare our model to baselines\nfollowing the training and evaluation recipe of BERT. Our results confirm that\nrecurrence can indeed improve Transformer models by a consistent margin,\nwithout requiring low-level performance optimizations, and while keeping the\nnumber of parameters constant. For example, our base model achieves an absolute\nimprovement of 2.1 points averaged across 10 tasks and also demonstrates\nincreased stability in fine-tuning over a range of learning rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Ran Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges in Measuring Bias via Open-Ended Language Generation. (arXiv:2205.11601v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11601","description":"<p>Researchers have devised numerous ways to quantify social biases vested in\npretrained language models. As some language models are capable of generating\ncoherent completions given a set of textual prompts, several prompting datasets\nhave been proposed to measure biases between social groups -- posing language\ngeneration as a way of identifying biases. In this opinion paper, we analyze\nhow specific choices of prompt sets, metrics, automatic tools and sampling\nstrategies affect bias results. We find out that the practice of measuring\nbiases through text completion is prone to yielding contradicting results under\ndifferent experiment settings. We additionally provide recommendations for\nreporting biases in open-ended language generation for a more complete outlook\nof biases exhibited by a given language model. Code to reproduce the results is\nreleased under https://github.com/feyzaakyurek/bias-textgen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1\">Afra Feyza Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocyigit_M/0/1/0/all/0/1\">Muhammed Yusuf Kocyigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_S/0/1/0/all/0/1\">Sejin Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeded Hierarchical Clustering for Expert-Crafted Taxonomies. (arXiv:2205.11602v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11602","description":"<p>Practitioners from many disciplines (e.g., political science) use\nexpert-crafted taxonomies to make sense of large, unlabeled corpora. In this\nwork, we study Seeded Hierarchical Clustering (SHC): the task of automatically\nfitting unlabeled data to such taxonomies using only a small set of labeled\nexamples. We propose HierSeed, a novel weakly supervised algorithm for this\ntask that uses only a small set of labeled seed examples. It is both data and\ncomputationally efficient. HierSeed assigns documents to topics by weighing\ndocument density against topic hierarchical structure. It outperforms both\nunsupervised and supervised baselines for the SHC task on three real-world\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Anish Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananthram_A/0/1/0/all/0/1\">Amith Ananthram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving language models fine-tuning with representation consistency targets. (arXiv:2205.11603v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11603","description":"<p>Fine-tuning contextualized representations learned by pre-trained language\nmodels has become a standard practice in the NLP field. However, pre-trained\nrepresentations are prone to degradation (also known as representation\ncollapse) during fine-tuning, which leads to instability, suboptimal\nperformance, and weak generalization. In this paper, we propose a novel\nfine-tuning method that avoids representation collapse during fine-tuning by\ndiscouraging undesirable changes in the representations. We show that our\napproach matches or exceeds the performance of the existing\nregularization-based fine-tuning methods across 13 language understanding tasks\n(GLUE benchmark and six additional datasets). We also demonstrate its\neffectiveness in low-data settings and robustness to label perturbation.\nFurthermore, we extend previous studies of representation collapse and propose\nseveral metrics to quantify it. Using these metrics and previously proposed\nexperiments, we show that our approach obtains significant improvements in\nretaining the expressive power of representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razdaibiedina_A/0/1/0/all/0/1\">Anastasia Razdaibiedina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_V/0/1/0/all/0/1\">Vivek Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1\">Zohar Karnin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khetan_A/0/1/0/all/0/1\">Ashish Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_V/0/1/0/all/0/1\">Vishaal Kapoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Measuring Social Biases in Prompt-Based Multi-Task Learning. (arXiv:2205.11605v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11605","description":"<p>Large language models trained on a mixture of NLP tasks that are converted\ninto a text-to-text format using prompts, can generalize into novel forms of\nlanguage and handle novel tasks. A large body of work within prompt engineering\nattempts to understand the effects of input forms and prompts in achieving\nsuperior performance. We consider an alternative measure and inquire whether\nthe way in which an input is encoded affects social biases promoted in outputs.\nIn this paper, we study T0, a large-scale multi-task text-to-text language\nmodel trained using prompt-based learning. We consider two different forms of\nsemantically equivalent inputs: question-answer format and premise-hypothesis\nformat. We use an existing bias benchmark for the former BBQ and create the\nfirst bias benchmark in natural language inference BBNLI with hand-written\nhypotheses while also converting each benchmark into the other form. The\nresults on two benchmarks suggest that given two different formulations of\nessentially the same input, T0 conspicuously acts more biased in question\nanswering form, which is seen during training, compared to premise-hypothesis\nform which is unlike its training examples. Code and data are released under\nhttps://github.com/feyzaakyurek/bbnli.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_A/0/1/0/all/0/1\">Afra Feyza Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paik_S/0/1/0/all/0/1\">Sejin Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocyigit_M/0/1/0/all/0/1\">Muhammed Yusuf Kocyigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbiyik_S/0/1/0/all/0/1\">Seda Akbiyik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runyun_S/0/1/0/all/0/1\">&#x15e;erife Leman Runyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment. (arXiv:2205.11616v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11616","description":"<p>Word translation without parallel corpora has become feasible, rivaling the\nperformance of supervised methods. Recent findings have shown that the accuracy\nand robustness of unsupervised word translation (UWT) can be improved by making\nuse of visual observations, which are universal representations across\nlanguages. In this work, we investigate the potential of using not only visual\nobservations but also pretrained language-image models for enabling a more\nefficient and robust UWT. Specifically, we develop a novel UWT method dubbed\nWord Alignment using Language-Image Pretraining (WALIP), which leverages visual\nobservations via the shared embedding space of images and texts provided by\nCLIP models (Radford et al., 2021). WALIP has a two-step procedure. First, we\nretrieve word pairs with high confidences of similarity, computed using our\nproposed image-based fingerprints, which define the initial pivot for the word\nalignment. Second, we apply our robust Procrustes algorithm to estimate the\nlinear mapping between two embedding spaces, which iteratively corrects and\nrefines the estimated alignment. Our extensive experiments show that WALIP\nimproves upon the state-of-the-art performance of bilingual word alignment for\na few language pairs across different word embeddings and displays great\nrobustness to the dissimilarity of language pairs or training corpora for two\nword embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1\">Tuan Dinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajput_S/0/1/0/all/0/1\">Shashank Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ossowski_T/0/1/0/all/0/1\">Timothy Ossowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yifei Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer. (arXiv:2205.11631v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11631","description":"<p>In Neural Machine Translation (NMT), each token prediction is conditioned on\nthe source sentence and the target prefix (what has been previously translated\nat a decoding step). However, previous work on interpretability in NMT has\nfocused solely on source sentence tokens attributions. Therefore, we lack a\nfull understanding of the influences of every input token (source sentence and\ntarget prefix) in the model predictions. In this work, we propose an\ninterpretability method that tracks complete input token attributions. Our\nmethod, which can be extended to any encoder-decoder Transformer-based model,\nallows us to better comprehend the inner workings of current NMT models. We\napply the proposed method to both bilingual and multilingual Transformers and\npresent insights into their behaviour.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1\">Javier Ferrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1\">Belen Alastruey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Natural Language Processing Pipeline for Detecting Informal Data References in Academic Literature. (arXiv:2205.11651v1 [cs.DL])","link":"http://arxiv.org/abs/2205.11651","description":"<p>Discovering authoritative links between publications and the datasets that\nthey use can be a labor-intensive process. We introduce a natural language\nprocessing pipeline that retrieves and reviews publications for informal\nreferences to research datasets, which complements the work of data librarians.\nWe first describe the components of the pipeline and then apply it to expand an\nauthoritative bibliography linking thousands of social science studies to the\ndata-related publications in which they are used. The pipeline increases recall\nfor literature to review for inclusion in data-related collections of\npublications and makes it possible to detect informal data references at scale.\nWe contribute (1) a novel Named Entity Recognition (NER) model that reliably\ndetects informal data references and (2) a dataset connecting items from social\nscience literature with datasets they reference. Together, these contributions\nenable future work on data reference, data citation networks, and data reuse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lafia_S/0/1/0/all/0/1\">Sara Lafia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lizhou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1\">Libby Hemphill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?. (arXiv:2205.11656v1 [cs.LG])","link":"http://arxiv.org/abs/2205.11656","description":"<p>The existence of a plethora of language models makes the problem of selecting\nthe best one for a custom task challenging. Most state-of-the-art methods\nleverage transformer-based models (e.g., BERT) or their variants. Training such\nmodels and exploring their hyperparameter space, however, is computationally\nexpensive. Prior work proposes several neural architecture search (NAS) methods\nthat employ performance predictors (e.g., surrogate models) to address this\nissue; however, analysis has been limited to homogeneous models that use fixed\ndimensionality throughout the network. This leads to sub-optimal architectures.\nTo address this limitation, we propose a suite of heterogeneous and flexible\nmodels, namely FlexiBERT, that have varied encoder layers with a diverse set of\npossible operations and different hidden dimensions. For better-posed surrogate\nmodeling in this expanded design space, we propose a new graph-similarity-based\nembedding scheme. We also propose a novel NAS policy, called BOSHNAS, that\nleverages this new scheme, Bayesian modeling, and second-order optimization, to\nquickly train and use a neural surrogate model to converge to the optimal\narchitecture. A comprehensive set of experiments shows that the proposed\npolicy, when applied to the FlexiBERT design space, pushes the performance\nfrontier upwards compared to traditional models. FlexiBERT-Mini, one of our\nproposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9%\nhigher GLUE score. A FlexiBERT model with equivalent performance as the best\nhomogeneous model achieves 2.6x smaller size. FlexiBERT-Large, another proposed\nmodel, achieves state-of-the-art results, outperforming the baseline models by\nat least 5.7% on the GLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuli_S/0/1/0/all/0/1\">Shikhar Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dedhia_B/0/1/0/all/0/1\">Bhishma Dedhia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuli_S/0/1/0/all/0/1\">Shreshth Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1\">Niraj K. Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions. (arXiv:2205.11658v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11658","description":"<p>Generics express generalizations about the world (e.g., \"birds can fly\").\nHowever, they are not universally true -- while sparrows and penguins are both\nbirds, only sparrows can fly and penguins cannot. Commonsense knowledge bases,\nwhich are used extensively in many NLP tasks as a source of world-knowledge,\ncan often encode generic knowledge but, by-design, cannot encode such\nexceptions. Therefore, it is crucial to realize the specific instances when a\ngeneric statement is true or false. In this work, we present a novel framework\nto generate pragmatically relevant true and false instances of a generic. We\nuse pre-trained language models, constraining the generation based on insights\nfrom linguistic theory, and produce ${\\sim}20k$ exemplars for ${\\sim}650$\ngenerics. Our system outperforms few-shot generation from GPT-3 (by 12.5\nprecision points) and our analysis highlights the importance of constrained\ndecoding for this task and the implications of generics exemplars for language\ninference tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allaway_E/0/1/0/all/0/1\">Emily Allaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization. (arXiv:2205.11686v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11686","description":"<p>Integrating vision and language has gained notable attention following the\nsuccess of pretrained language models. Despite that, a fraction of emerging\nmultimodal models is suitable for text generation conditioned on images. This\nminority is typically developed and evaluated for image captioning, a text\ngeneration task conditioned solely on images with the goal to describe what is\nexplicitly visible in an image. In this paper, we take a step back and ask: How\ndo these models work for more complex generative tasks, conditioned on both\ntext and images? Are models based on joint multimodal pretraining, visually\nadapted pretrained language models, or models that combine these two\napproaches, more promising for such tasks? We address these questions in the\ncontext of self-rationalization (jointly generating task labels/answers and\nfree-text explanations) of three tasks: (i) visual question answering in VQA-X,\n(ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment\nin E-SNLI-VE. We show that recent advances in each modality, CLIP image\nrepresentations and scaling of language models, do not consistently improve\nmultimodal self-rationalization of tasks with multimodal inputs. We also\nobserve that no model type works universally the best across tasks/datasets and\nfinetuning data sizes. Our findings call for a backbone modelling approach that\ncan be built on to advance text generation from images and text beyond image\ncaptioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palaskar_S/0/1/0/all/0/1\">Shruti Palaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagia_A/0/1/0/all/0/1\">Akshita Bhagia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Workflow Discovery from Dialogues in the Low Data Regime. (arXiv:2205.11690v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11690","description":"<p>Text-based dialogues are now widely used to solve real-world problems. In\ncases where solution strategies are already known, they can sometimes be\ncodified into workflows and used to guide humans or artificial agents through\nthe task of helping clients. We are interested in the situation where a formal\nworkflow may not yet exist, but we wish to discover the steps of actions that\nhave been taken to resolve problems. We examine a novel transformer-based\napproach for this situation and we present experiments where we summarize\ndialogues in the Action-Based Conversations Dataset (ABCD) with workflows.\nSince the ABCD dialogues were generated using known workflows to guide agents\nwe can evaluate our ability to extract such workflows using ground truth\nsequences of action steps, organized as workflows. We propose and evaluate an\napproach that conditions models on the set of allowable action steps and we\nshow that using this strategy we can improve workflow discovery (WD)\nperformance. Our conditioning approach also improves zero-shot and few-shot WD\nperformance when transferring learned models to entirely new domains (i.e. the\nMultiWOZ setting). Further, a modified variant of our architecture achieves\nstate-of-the-art performance on the related but different problems of Action\nState Tracking (AST) and Cascading Dialogue Success (CDS) on the ABCD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hattami_A/0/1/0/all/0/1\">Amine El Hattami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raimondo_S/0/1/0/all/0/1\">Stefania Raimondo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Chris Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Neural Open Information Extraction: Current Status and Future Directions. (arXiv:2205.11725v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11725","description":"<p>Open Information Extraction (OpenIE) facilitates domain-independent discovery\nof relational facts from large corpora. The technique well suits many\nopen-world natural language understanding scenarios, such as automatic\nknowledge base construction, open-domain question answering, and explicit\nreasoning. Thanks to the rapid development in deep learning technologies,\nnumerous neural OpenIE architectures have been proposed and achieve\nconsiderable performance improvement. In this survey, we provide an extensive\noverview of the-state-of-the-art neural OpenIE models, their key design\ndecisions, strengths and weakness. Then, we discuss limitations of current\nsolutions and the open issues in OpenIE problem itself. Finally we list recent\ntrends that could help expand its scope and applicability, setting up promising\ndirections for future research in OpenIE. To our best knowledge, this paper is\nthe first review on this specific topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shaowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Cheng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Role of Bidirectionality in Language Model Pre-Training. (arXiv:2205.11726v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11726","description":"<p>Prior work on language model pre-training has explored different\narchitectures and learning objectives, but differences in data, hyperparameters\nand evaluation make a principled comparison difficult. In this work, we focus\non bidirectionality as a key factor that differentiates existing approaches,\nand present a comprehensive study of its role in next token prediction, text\ninfilling, zero-shot priming and fine-tuning. We propose a new framework that\ngeneralizes prior approaches, including fully unidirectional models like GPT,\nfully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.\nOur framework distinguishes between two notions of bidirectionality\n(bidirectional context and bidirectional attention) and allows us to control\neach of them separately. We find that the optimal configuration is largely\napplication-dependent (e.g., bidirectional attention is beneficial for\nfine-tuning and infilling, but harmful for next token prediction and zero-shot\npriming). We train models with up to 6.7B parameters, and find differences to\nremain consistent at scale. While prior work on scaling has focused on\nleft-to-right autoregressive models, our results suggest that this approach\ncomes with some trade-offs, and it might be worthwhile to develop very large\nbidirectional models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingfei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1\">Naman Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Ves Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering. (arXiv:2205.11729v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11729","description":"<p>Multi-hop question answering (QA) is a challenging task requiring QA systems\nto perform complex reasoning over multiple documents and provide supporting\nfacts together with the exact answer. Existing works tend to utilize\ngraph-based reasoning and question decomposition to obtain the reasoning chain,\nwhich inevitably introduces additional complexity and cumulative error to the\nsystem. To address the above issue, we propose a simple yet effective novel\nframework, From Easy to Hard (FE2H), to remove distracting information and\nobtain better contextual representations for the multi-hop QA task. Inspired by\nthe iterative document selection process and the progressive learning custom of\nhumans, FE2H divides both the document selector and reader into two stages\nfollowing an easy-to-hard manner. Specifically, we first select the document\nmost relevant to the question and then utilize the question together with this\ndocument to select other pertinent documents. As for the QA phase, our reader\nis first trained on a single-hop QA dataset and then transferred into the\nmulti-hop QA task. We comprehensively evaluate our model on the popular\nmulti-hop QA benchmark HotpotQA. Experimental results demonstrate that our\nmethod ourperforms all other methods in the leaderboard of HotpotQA (distractor\nsetting).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin-Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wei-Jun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu-Bin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PERT: A New Solution to Pinyin to Character Conversion Task. (arXiv:2205.11737v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11737","description":"<p>Pinyin to Character conversion (P2C) task is the key task of Input Method\nEngine (IME) in commercial input software for Asian languages, such as Chinese,\nJapanese, Thai language and so on. It's usually treated as sequence labelling\ntask and resolved by language model, i.e. n-gram or RNN. However, the low\ncapacity of the n-gram or RNN limits its performance. This paper introduces a\nnew solution named PERT which stands for bidirectional Pinyin Encoder\nRepresentations from Transformers. It achieves significant improvement of\nperformance over baselines. Furthermore, we combine PERT with n-gram under a\nMarkov framework, and improve performance further. Lastly, the external lexicon\nis incorporated into PERT so as to resolve the OOD issue of IME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanfeng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haiteng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabyBear: Cheap inference triage for expensive language models. (arXiv:2205.11747v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11747","description":"<p>Transformer language models provide superior accuracy over previous models\nbut they are computationally and environmentally expensive. Borrowing the\nconcept of model cascading from computer vision, we introduce BabyBear, a\nframework for cascading models for natural language processing (NLP) tasks to\nminimize cost. The core strategy is inference triage, exiting early when the\nleast expensive model in the cascade achieves a sufficiently high-confidence\nprediction. We test BabyBear on several open source data sets related to\ndocument classification and entity recognition. We find that for common NLP\ntasks a high proportion of the inference load can be accomplished with cheap,\nfast models that have learned by observing a deep learning model. This allows\nus to reduce the compute cost of large-scale classification jobs by more than\n50% while retaining overall accuracy. For named entity recognition, we save 33%\nof the deep learning compute while maintaining an F1 score higher than 95% on\nthe CoNLL benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalili_L/0/1/0/all/0/1\">Leila Khalili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohannon_J/0/1/0/all/0/1\">John Bohannon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models. (arXiv:2205.11758v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11758","description":"<p>The emergent cross-lingual transfer seen in multilingual pretrained models\nhas sparked significant interest in studying their behavior. However, because\nthese analyses have focused on fully trained multilingual models, little is\nknown about the dynamics of the multilingual pretraining process. We\ninvestigate when these models acquire their in-language and cross-lingual\nabilities by probing checkpoints taken from throughout XLM-R pretraining, using\na suite of linguistic tasks. Our analysis shows that the model achieves high\nin-language performance early on, with lower-level linguistic skills acquired\nbefore more complex ones. In contrast, when the model learns to transfer\ncross-lingually depends on the language pair. Interestingly, we also observe\nthat, across many languages and tasks, the final, converged model checkpoint\nexhibits significant performance degradation and that no one checkpoint\nperforms best on all languages. Taken together with our other findings, these\ninsights highlight the complexity and interconnectedness of multilingual\npretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blevins_T/0/1/0/all/0/1\">Terra Blevins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat. (arXiv:2205.11764v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11764","description":"<p>In a depression-diagnosis-directed clinical session, doctors initiate a\nconversation with ample emotional support that guides the patients to expose\ntheir symptoms based on clinical diagnosis criteria. Such a dialog is a\ncombination of task-oriented and chitchat, different from traditional\nsingle-purpose human-machine dialog systems. However, due to the social stigma\nassociated with mental illness, the dialogue data related to depression\nconsultation and diagnosis are rarely disclosed. Though automatic\ndialogue-based diagnosis foresees great application potential, data sparsity\nhas become one of the major bottlenecks restricting research on such\ntask-oriented chat dialogues. Based on clinical depression diagnostic criteria\nICD-11 and DSM-5, we construct the D$^4$: a Chinese Dialogue Dataset for\nDepression-Diagnosis-Oriented Chat which simulates the dialogue between doctors\nand patients during the diagnosis of depression, including diagnosis results\nand symptom summary given by professional psychiatrists for each\ndialogue.Finally, we finetune on state-of-the-art pre-training models and\nrespectively present our dataset baselines on four tasks including response\ngeneration, topic prediction, dialog summary, and severity classification of\ndepressive episode and suicide risk. Multi-scale evaluation results demonstrate\nthat a more empathy-driven and diagnostic-accurate consultation dialogue system\ntrained on our dataset can be achieved compared to rule-based bots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Binwei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Likai Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Lingfeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition. (arXiv:2205.11799v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11799","description":"<p>Fine-tuning pre-trained language models has recently become a common practice\nin building NLP models for various tasks, especially few-shot tasks. We argue\nthat under the few-shot setting, formulating fine-tuning closer to the\npre-training objectives shall be able to unleash more benefits from the\npre-trained language models. In this work, we take few-shot named entity\nrecognition (NER) for a pilot study, where existing fine-tuning strategies are\nmuch different from pre-training. We propose a novel few-shot fine-tuning\nframework for NER, FFF-NER. Specifically, we introduce three new types of\ntokens, \"is-entity\", \"which-type\" and bracket, so we can formulate the NER\nfine-tuning as (masked) token prediction or generation, depending on the choice\nof pre-trained language models. In our experiments, we apply FFF-NER to\nfine-tune both BERT and BART for few-shot NER on several benchmark datasets and\nobserve significant improvements over existing fine-tuning strategies,\nincluding sequence labeling, prototype meta-learning, and prompt-based\napproaches. We further perform a series of ablation studies, showing few-shot\nNER performance is strongly correlated with the similarity between fine-tuning\nand pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kewen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeDef: Weakly Supervised Backdoor Defense for Text Classification. (arXiv:2205.11803v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11803","description":"<p>Existing backdoor defense methods are only effective for limited trigger\ntypes. To defend different trigger types at once, we start from the\nclass-irrelevant nature of the poisoning process and propose a novel weakly\nsupervised backdoor defense framework WeDef. Recent advances in weak\nsupervision make it possible to train a reasonably accurate text classifier\nusing only a small number of user-provided, class-indicative seed words. Such\nseed words shall be considered independent of the triggers. Therefore, a weakly\nsupervised text classifier trained by only the poisoned documents without their\nlabels will likely have no backdoor. Inspired by this observation, in WeDef, we\ndefine the reliability of samples based on whether the predictions of the weak\nclassifier agree with their labels in the poisoned training set. We further\nimprove the results through a two-phase sanitization: (1) iteratively refine\nthe weak classifier based on the reliable samples and (2) train a binary poison\nclassifier by distinguishing the most unreliable samples from the most reliable\nsamples. Finally, we train the sanitized model on the samples that the poison\nclassifier predicts as benign. Extensive experiments show that WeDefis\neffective against popular trigger-based attacks (e.g., words, sentences, and\nparaphrases), outperforming existing defense methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lesheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. (arXiv:2205.11822v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11822","description":"<p>Despite their impressive capabilities, large pre-trained language models\n(LMs) struggle with consistent reasoning; recently, prompting LMs to generate\nexplanations that self-guide the inference has emerged as a promising direction\nto amend this. However, these approaches are fundamentally bounded by the\ncorrectness of explanations, which themselves are often noisy and inconsistent.\nIn this work, we develop Maieutic Prompting, which infers a correct answer to a\nquestion even from the noisy and inconsistent generations of LM. Maieutic\nPrompting induces a tree of explanations abductively (e.g. X is true, because\n...) and recursively, then frames the inference as a satisfiability problem\nover these explanations and their logical relations. We test Maieutic Prompting\nfor true/false QA on three challenging benchmarks that require complex\ncommonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy\nthan state-of-the-art prompting methods, and as a fully unsupervised approach,\nperforms competitively with supervised models. We also show that Maieutic\nPrompting improves robustness in inference while providing interpretable\nrationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jaehun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lack of Fluency is Hurting Your Translation Model. (arXiv:2205.11826v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11826","description":"<p>Many machine translation models are trained on bilingual corpus, which\nconsist of aligned sentence pairs from two different languages with same\nsemantic. However, there is a qualitative discrepancy between train and test\nset in bilingual corpus. While the most train sentences are created via\nautomatic techniques such as crawling and sentence-alignment methods, the test\nsentences are annotated with the consideration of fluency by human. We suppose\nthis discrepancy in training corpus will yield performance drop of translation\nmodel. In this work, we define \\textit{fluency noise} to determine which parts\nof train sentences cause them to seem unnatural. We show that \\textit{fluency\nnoise} can be detected by simple gradient-based method with pre-trained\nclassifier. By removing \\textit{fluency noise} in train sentences, our final\nmodel outperforms the baseline on WMT-14 DE$\\rightarrow$EN and\nRU$\\rightarrow$EN. We also show the compatibility with back-translation\naugmentation, which has been commonly used to improve the fluency of the\ntranslation model. At last, the qualitative analysis of \\textit{fluency noise}\nprovides the insight of what points we should focus on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaehyo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Lottery Tickets Boost Ensemble from a Single Pretrained Model. (arXiv:2205.11833v1 [cs.LG])","link":"http://arxiv.org/abs/2205.11833","description":"<p>Ensembling is a popular method used to improve performance as a last resort.\nHowever, ensembling multiple models finetuned from a single pretrained model\nhas been not very effective; this could be due to the lack of diversity among\nensemble members. This paper proposes Multi-Ticket Ensemble, which finetunes\ndifferent subnetworks of a single pretrained model and ensembles them. We\nempirically demonstrated that winning-ticket subnetworks produced more diverse\npredictions than dense networks, and their ensemble outperformed the standard\nensemble on some tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Sosuke Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Charon: a FrameNet Annotation Tool for Multimodal Corpora. (arXiv:2205.11836v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11836","description":"<p>This paper presents Charon, a web tool for annotating multimodal corpora with\nFrameNet categories. Annotation can be made for corpora containing both static\nimages and video sequences paired - or not - with text sequences. The pipeline\nfeatures, besides the annotation interface, corpus import and pre-processing\ntools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belcavello_F/0/1/0/all/0/1\">Frederico Belcavello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viridiano_M/0/1/0/all/0/1\">Marcelo Viridiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_E/0/1/0/all/0/1\">Ely Edison Matos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrent_T/0/1/0/all/0/1\">Tiago Timponi Torrent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lutma: a Frame-Making Tool for Collaborative FrameNet Development. (arXiv:2205.11840v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11840","description":"<p>This paper presents Lutma, a collaborative, semi-constrained, tutorial-based\ntool for contributing frames and lexical units to the Global FrameNet\ninitiative. The tool parameterizes the process of frame creation, avoiding\nconsistency violations and promoting the integration of frames contributed by\nthe community with existing frames. Lutma is structured in a wizard-like\nfashion so as to provide users with text and video tutorials relevant for each\nstep in the frame creation process. We argue that this tool will allow for a\nsensible expansion of FrameNet coverage in terms of both languages and cultural\nperspectives encoded by them, positioning frames as a viable alternative for\nrepresenting perspective in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torrent_T/0/1/0/all/0/1\">Tiago Timponi Torrent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_A/0/1/0/all/0/1\">Arthur Lorenzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_E/0/1/0/all/0/1\">Ely Edison da Silva Matos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belcavello_F/0/1/0/all/0/1\">Frederico Belcavello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viridiano_M/0/1/0/all/0/1\">Marcelo Viridiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamonal_M/0/1/0/all/0/1\">Maucha Andrade Gamonal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of STEM Science as Process, Method, Material, and Data Named Entities. (arXiv:2205.11863v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11863","description":"<p>We are faced with an unprecedented production in scholarly publications\nworldwide. Stakeholders in the digital libraries posit that the document-based\npublishing paradigm has reached the limits of adequacy. Instead, structured,\nmachine-interpretable, fine-grained scholarly knowledge publishing as Knowledge\nGraphs (KG) is strongly advocated. In this work, we develop and analyze a\nlarge-scale structured dataset of STEM articles across 10 different\ndisciplines, viz. Agriculture, Astronomy, Biology, Chemistry, Computer Science,\nEarth Science, Engineering, Material Science, Mathematics, and Medicine. Our\nanalysis is defined over a large-scale corpus comprising 60K abstracts\nstructured as four scientific entities process, method, material, and data.\nThus our study presents, for the first-time, an analysis of a large-scale\nmultidisciplinary corpus under the construct of four named entity labels that\nare specifically defined and selected to be domain-independent as opposed to\ndomain-specific. The work is then inadvertently a feasibility test of\ncharacterizing multidisciplinary science with domain-independent concepts.\nFurther, to summarize the distinct facets of scientific knowledge per concept\nper discipline, a set of word cloud visualizations are offered. The\nSTEM-NER-60k corpus, created in this work, comprises over 1M extracted entities\nfrom 60k STEM articles obtained from a major publishing platform and is\npublicly released https://github.com/jd-coderepos/stem-ner-60k.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a Dialogue Corpus Annotated with Expressed and Experienced Emotions. (arXiv:2205.11867v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11867","description":"<p>In communication, a human would recognize the emotion of an interlocutor and\nrespond with an appropriate emotion, such as empathy and comfort. Toward\ndeveloping a dialogue system with such a human-like ability, we propose a\nmethod to build a dialogue corpus annotated with two kinds of emotions. We\ncollect dialogues from Twitter and annotate each utterance with the emotion\nthat a speaker put into the utterance (expressed emotion) and the emotion that\na listener felt after listening to the utterance (experienced emotion). We\nbuilt a dialogue corpus in Japanese using this method, and its statistical\nanalysis revealed the differences between expressed and experienced emotions.\nWe conducted experiments on recognition of the two kinds of emotions. The\nexperimental results indicated the difficulty in recognizing experienced\nemotions and the effectiveness of multi-task learning of the two kinds of\nemotions. We hope that the constructed corpus will facilitate the study on\nemotion recognition in a dialogue and emotion-aware dialogue response\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ide_T/0/1/0/all/0/1\">Tatsuya Ide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_D/0/1/0/all/0/1\">Daisuke Kawahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accuracy on In-Domain Samples Matters When Building Out-of-Domain detectors: A Reply to Marek et al. (2021). (arXiv:2205.11887v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11887","description":"<p>We have noticed that Marek et al. (2021) try to re-implement our paper Zheng\net al. (2020a) in their work \"OodGAN: Generative Adversarial Network for\nOut-of-Domain Data Generation\". Our paper proposes a model to generate pseudo\nOOD samples that are akin to IN-Domain (IND) input utterances. These pseudo OOD\nsamples can be used to improve the OOD detection performance by optimizing an\nentropy regularization term when building the IND classifier. Marek et al.\n(2021) report a large gap between their re-implemented results and ours on the\nCLINC150 dataset (Larson et al., 2019). This paper discusses some key\nobservations that may have led to such a large gap. Most of these observations\noriginate from our experiments because Marek et al. (2021) have not released\ntheir codes1. One of the most important observations is that stronger IND\nclassifiers usually exhibit a more robust ability to detect OOD samples. We\nhope these observations help other researchers, including Marek et al. (2021),\nto develop better OOD detectors in their applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building an Effective Automated Assessment System for C/C++ Introductory Programming Courses in ODL Environment. (arXiv:2205.11915v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11915","description":"<p>Assessments help in evaluating the knowledge gained by a learner at any\nspecific point as well as in continuous improvement of the curriculum design\nand the whole learning process. However, with the increase in students'\nenrollment at University level in either conventional or distance education\nenvironment, traditional ways of assessing students' work are becoming\ninsufficient in terms of both time and effort. In distance education\nenvironment, such assessments become additionally more challenging in terms of\nhefty remuneration for hiring large number of tutors. The availability of\nautomated tools to assist the evaluation of students' work and providing\nstudents with appropriate and timely feedback can really help in overcoming\nthese problems. We believe that building such tools for assessing students'\nwork for all kinds of courses in not yet possible. However, courses that\ninvolve some formal language of expression can be automated, such as,\nprogramming courses in Computer Science (CS) discipline. Instructors provide\nvarious practical exercises to students as assignments to build these skills.\nUsually, instructors manually grade and provide feedbacks on these assignments.\nAlthough in literature, various tools have been reported to automate this\nprocess, but most of these tools have been developed by the host institutions\nthemselves for their own use. We at COMSATS Institute of Information\nTechnology, Lahore are conducting a pioneer effort in Pakistan to automate the\nmarking of assignments of introductory programming courses that involve C or\nC++ languages with the capability of associating appropriate feedbacks for\nstudents. In this paper, we basically identify different components that we\nbelieve are necessary in building an effective automated assessment system in\nthe context of introductory programming courses that involve C/C++ programming.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Adnan Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humayoun_M/0/1/0/all/0/1\">Muhammad Humayoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Zero-Shot Reasoners. (arXiv:2205.11916v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11916","description":"<p>Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding ``Let's think step by step''\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nan off-the-shelf 175B parameter model. The versatility of this single prompt\nacross very diverse reasoning tasks hints at untapped and understudied\nfundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task\nbroad cognitive capabilities may be extracted through simple prompting. We hope\nour work not only serves as the minimal strongest zero-shot baseline for the\nchallenging reasoning benchmarks, but also highlights the importance of\ncarefully exploring and analyzing the enormous zero-shot knowledge hidden\ninside LLMs before crafting finetuning datasets or few-shot exemplars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_T/0/1/0/all/0/1\">Takeshi Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Community Question Answering Entity Linking via Leveraging Auxiliary Data. (arXiv:2205.11917v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11917","description":"<p>Community Question Answering (CQA) platforms contain plenty of CQA texts\n(i.e., questions and answers corresponding to the question) where named\nentities appear ubiquitously. In this paper, we define a new task of CQA entity\nlinking (CQAEL) as linking the textual entity mentions detected from CQA texts\nwith their corresponding entities in a knowledge base. This task can facilitate\nmany downstream applications including expert finding and knowledge base\nenrichment. Traditional entity linking methods mainly focus on linking entities\nin news documents, and are suboptimal over this new task of CQAEL since they\ncannot effectively leverage various informative auxiliary data involved in the\nCQA platform to aid entity linking, such as parallel answers and two types of\nmeta-data (i.e., topic tags and users). To remedy this crucial issue, we\npropose a novel transformer-based framework to effectively harness the\nknowledge delivered by different kinds of auxiliary data to promote the linking\nperformance. We validate the superiority of our framework through extensive\nexperiments over a newly released CQAEL data set against state-of-the-art\nentity linking methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yadong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Human is Human Evaluation? Improving the Gold Standard for NLG with Utility Theory. (arXiv:2205.11930v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11930","description":"<p>Human ratings are treated as the gold standard in NLG evaluation. The\nstandard protocol is to collect ratings of generated text, average across\nannotators, and then rank NLG systems by their average scores. However, little\nconsideration has been given as to whether this approach faithfully captures\nhuman preferences. In this work, we analyze this standard protocol through the\nlens of utility theory in economics. We first identify the implicit assumptions\nit makes about annotators and find that these assumptions are often violated in\npractice, in which case annotator ratings become an unfaithful reflection of\ntheir preferences. The most egregious violations come from using Likert scales,\nwhich provably reverse the direction of the true preference in certain cases.\nWe suggest improvements to the standard protocol to make it more theoretically\nsound, but even in its improved form, it cannot be used to evaluate open-ended\ntasks like story generation. For the latter, we propose a new evaluation\nprotocol called $\\textit{system-level probabilistic assessment}$ (SPA). In our\nexperiments, we find that according to SPA, annotators prefer larger GPT-3\nvariants to smaller ones -- as expected -- with all comparisons being\nstatistically significant. In contrast, the standard protocol only yields\nsignificant results half the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentional Mixtures of Soft Prompt Tuning for Parameter-efficient Multi-task Knowledge Sharing. (arXiv:2205.11961v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11961","description":"<p>This work introduces ATTEMPT (Attentional Mixture of Prompt Tuning), a new\nmodular, multi-task, and parameter-efficient language model (LM) tuning\napproach that combines knowledge transferred across different tasks via a\nmixture of soft prompts while keeping original LM unchanged. ATTEMPT\ninterpolates a set of prompts trained on large-scale source tasks and a newly\ninitialized target task prompt using instance-wise attention computed by a\nlightweight sub-network trained on multiple target tasks. ATTEMPT is\nparameter-efficient (e.g., updates 1,600 times fewer parameters than\nfine-tuning) and enables multi-task learning and flexible extensions;\nimportantly, it is also more interpretable because it demonstrates which source\ntasks affect the final model decision on target tasks. Experimental results\nacross 17 diverse datasets show that ATTEMPT improves prompt tuning by up to a\n22% absolute performance gain and outperforms or matches fully fine-tuned or\nother parameter-efficient tuning approaches that use over ten times more\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmark Data and Evaluation Framework for Intent Discovery Around COVID-19 Vaccine Hesitancy. (arXiv:2205.11966v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11966","description":"<p>The COVID-19 pandemic has made a huge global impact and cost millions of\nlives. As COVID-19 vaccines were rolled out, they were quickly met with\nwidespread hesitancy. To address the concerns of hesitant people, we launched\nVIRA, a public dialogue system aimed at addressing questions and concerns\nsurrounding the COVID-19 vaccines. Here, we release VIRADialogs, a dataset of\nover 8k dialogues conducted by actual users with VIRA, providing a unique\nreal-world conversational dataset. In light of rapid changes in users' intents,\ndue to updates in guidelines or as a response to new information, we highlight\nthe important task of intent discovery in this use-case. We introduce a novel\nautomatic evaluation framework for intent discovery, leveraging the existing\nintent classifier of a given dialogue system. We use this framework to report\nbaseline intent-discovery results over VIRADialogs, that highlight the\ndifficulty of this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gretz_S/0/1/0/all/0/1\">Shai Gretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_A/0/1/0/all/0/1\">Assaf Toledo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_R/0/1/0/all/0/1\">Roni Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weeks_R/0/1/0/all/0/1\">Rose Weeks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Zeev_N/0/1/0/all/0/1\">Naor Bar-Zeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangha_P/0/1/0/all/0/1\">Pooja Sangha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Dependency Treebank for Odia Language. (arXiv:2205.11976v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11976","description":"<p>This paper presents the first publicly available treebank of Odia, a\nmorphologically rich low resource Indian language. The treebank contains\napprox. 1082 tokens (100 sentences) in Odia selected from \"Samantar\", the\nlargest available parallel corpora collection for Indic languages. All the\nselected sentences are manually annotated following the ``Universal Dependency\n(UD)\" guidelines. The morphological analysis of the Odia treebank was performed\nusing machine learning techniques. The Odia annotated treebank will enrich the\nOdia language resource and will help in building language technology tools for\ncross-lingual learning and typological research. We also build a preliminary\nOdia parser using a machine learning approach. The accuracy of the parser is\n86.6% Tokenization, 64.1% UPOS, 63.78% XPOS, 42.04% UAS and 21.34% LAS.\nFinally, the paper briefly discusses the linguistic analysis of the Odia UD\ntreebank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parida_S/0/1/0/all/0/1\">Shantipriya Parida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_K/0/1/0/all/0/1\">Kalyanamalini Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Atul Kr. Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1\">Saraswati Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Satya Ranjan Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_B/0/1/0/all/0/1\">Bijayalaxmi Dash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word-order typology in Multilingual BERT: A case study in subordinate-clause detection. (arXiv:2205.11987v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11987","description":"<p>The capabilities and limitations of BERT and similar models are still unclear\nwhen it comes to learning syntactic abstractions, in particular across\nlanguages. In this paper, we use the task of subordinate-clause detection\nwithin and across languages to probe these properties. We show that this task\nis deceptively simple, with easy gains offset by a long tail of harder cases,\nand that BERT's zero-shot performance is dominated by word-order effects,\nmirroring the SVO/VSO/SOV typology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">Dmitry Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition. (arXiv:2205.11998v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11998","description":"<p>The choice of modeling units affects the performance of the acoustic modeling\nand plays an important role in automatic speech recognition (ASR). In mandarin\nscenarios, the Chinese characters represent meaning but are not directly\nrelated to the pronunciation. Thus only considering the writing of Chinese\ncharacters as modeling units is insufficient to capture speech features. In\nthis paper, we present a novel method involves with multi-level modeling units,\nwhich integrates multi-level information for mandarin speech recognition.\nSpecifically, the encoder block considers syllables as modeling units, and the\ndecoder block deals with character modeling units. During inference, the input\nfeature sequences are converted into syllable sequences by the encoder block\nand then converted into Chinese characters by the decoder block. This process\nis conducted by a unified end-to-end model without introducing additional\nconversion models. By introducing InterCE auxiliary task, our method achieves\ncompetitive results with CER of 4.1%/4.6% and 4.6%/5.2% on the widely used\nAISHELL-1 benchmark without a language model, using the Conformer and the\nTransformer backbones respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Binbin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuke Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections. (arXiv:2205.12005v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12005","description":"<p>Large-scale pretrained foundation models have been an emerging paradigm for\nbuilding artificial intelligence (AI) systems, which can be quickly adapted to\na wide range of downstream tasks. This paper presents mPLUG, a new\nvision-language foundation model for both cross-modal understanding and\ngeneration. Most existing pre-trained models suffer from the problems of low\ncomputational efficiency and information asymmetry brought by the long visual\nsequence in cross-modal alignment. To address these problems, mPLUG introduces\nan effective and efficient vision-language architecture with novel cross-modal\nskip-connections, which creates inter-layer shortcuts that skip a certain\nnumber of layers for time-consuming full self-attention on the vision side.\nmPLUG is pre-trained end-to-end on large-scale image-text pairs with both\ndiscriminative and generative objectives. It achieves state-of-the-art results\non a wide range of vision-language downstream tasks, such as image captioning,\nimage-text retrieval, visual grounding and visual question answering. mPLUG\nalso demonstrates strong zero-shot transferability when directly transferred to\nmultiple video-language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1\">Bin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysing the Greek Parliament Records with Emotion Classification. (arXiv:2205.12012v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12012","description":"<p>In this project, we tackle emotion classification for the Greek language,\npresenting and releasing a new dataset in Greek. We fine-tune and assess\nTransformer-based masked language models that were pre-trained on monolingual\nand multilingual resources, and we present the results per emotion and by\naggregating at the sentiment and subjectivity level. The potential of the\npresented resources is investigated by detecting and studying the emotion of\n`disgust' in the Greek Parliament records. We: (a) locate the months with the\nhighest values from 1989 to present, (b) rank the Greek political parties based\non the presence of this emotion in their speeches, and (c) study the emotional\ncontext shift of words used to stigmatise people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlopoulos_J/0/1/0/all/0/1\">John Pavlopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lislevand_V/0/1/0/all/0/1\">Vanessa Lislevand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetroMAE: Pre-training Retrieval-oriented Transformers via Masked Auto-Encoder. (arXiv:2205.12035v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12035","description":"<p>Pre-trained models have demonstrated superior power on many important tasks.\nHowever, it is still an open problem of designing effective pre-training\nstrategies so as to promote the models' usability on dense retrieval. In this\npaper, we propose a novel pre-training framework for dense retrieval based on\nthe Masked Auto-Encoder, known as RetroMAE. Our proposed framework is\nhighlighted for the following critical designs: 1) a MAE based pre-training\nworkflow, where the input sentence is polluted on both encoder and decoder side\nwith different masks, and original sentence is reconstructed based on both\nsentence embedding and masked sentence; 2) asymmetric model architectures, with\na large-scale expressive transformer for sentence encoding and a extremely\nsimplified transformer for sentence reconstruction; 3) asymmetric masking\nratios, with a moderate masking on the encoder side (15%) and an aggressive\nmasking ratio on the decoder side (50~90%). We pre-train a BERT like encoder on\nEnglish Wikipedia and BookCorpus, where it notably outperforms the existing\npre-trained models on a wide range of dense retrieval benchmarks, like MS\nMARCO, Open-domain Question Answering, and BEIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Phonological Parameters in Sign Languages. (arXiv:2205.12072v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12072","description":"<p>Signers compose sign language phonemes that enable communication by combining\nphonological parameters such as handshape, orientation, location, movement, and\nnon-manual features. Linguistic research often breaks down signs into their\nconstituent parts to study sign languages and often a lot of effort is invested\ninto the annotation of the videos. In this work we show how a single model can\nbe used to recognise the individual phonological parameters within sign\nlanguages with the aim of either to assist linguistic annotations or to\ndescribe the signs for the sign recognition models. We use Danish Sign Language\ndata set `Ordbog over Dansk Tegnsprog' to generate multiple data sets using\npose estimation model, which are then used for training the multi-label Fast\nR-CNN model to support multi-label modelling. Moreover, we show that there is a\nsignificant co-dependence between the orientation and location phonological\nparameters in the generated data and we incorporate this co-dependence in the\nmodel to achieve better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mocialov_B/0/1/0/all/0/1\">Boris Mocialov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_G/0/1/0/all/0/1\">Graham Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1\">Helen Hastie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphQ IR: Unifying Semantic Parsing of Graph Query Language with Intermediate Representation. (arXiv:2205.12078v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12078","description":"<p>Subject to the semantic gap lying between natural and formal language, neural\nsemantic parsing is typically bottlenecked by the paucity and imbalance of\ndata. In this paper, we propose a unified intermediate representation (IR) for\ngraph query languages, namely GraphQ IR. With the IR's natural-language-like\nrepresentation that bridges the semantic gap and its formally defined syntax\nthat maintains the graph structure, neural semantic parser can more effectively\nconvert user queries into our GraphQ IR, which can be later automatically\ncompiled into different downstream graph query languages. Extensive experiments\nshow that our approach can consistently achieve state-of-the-art performance on\nbenchmarks KQA Pro, Overnight and MetaQA. Evaluations under compositional\ngeneralization and few-shot learning settings also validate the promising\ngeneralization ability of GraphQ IR with at most 11% accuracy improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lunyiu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1\">Jidong Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval. (arXiv:2205.12105v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12105","description":"<p>In the past few years, the emergence of vision-language pre-training (VLP)\nhas brought cross-modal retrieval to a new era. However, due to the latency and\ncomputation demand, it is commonly challenging to apply VLP in a real-time\nonline retrieval system. To alleviate the defect, this paper proposes a\n\\textbf{Hi}erarchical \\textbf{V}ision-\\textbf{}Language \\textbf{P}re-Training\n(\\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a\nnovel hierarchical retrieval objective, which uses the representation of\ndifferent dimensions for coarse-to-fine ITR, i.e., using low-dimensional\nrepresentation for large-scale coarse retrieval and high-dimensional\nrepresentation for small-scale fine retrieval. We evaluate our proposed HiVLP\non two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.\nExtensive experiments demonstrate that our HiVLP not only has fast inference\nspeed but also can be easily scaled to large-scale ITR scenarios. The detailed\nresults show that HiVLP is $1,427$$\\sim$$120,649\\times$ faster than the\nfusion-based model UNITER and 2$\\sim$5 faster than the fastest embedding-based\nmodel LightingDot in different candidate scenarios. It also achieves about +4.9\nAR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable\nperformance with the state-of-the-art (SOTA) fusion-based model METER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianlong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Curious Case of Control. (arXiv:2205.12113v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12113","description":"<p>Children acquiring English make systematic errors on subject control\nsentences even after they have reached near-adult competence (C. Chomsky,\n1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).\nGiven the advanced fluency of large generative language models, we ask whether\nmodel outputs are consistent with these heuristics, and to what degree\ndifferent models are consistent with each other. We find that models can be\ncategorized by behavior into three separate groups, with broad differences\nbetween the groups. The outputs of models in the largest group are consistent\nwith positional heuristics that succeed on subject control but fail on object\ncontrol. This result is surprising, given that object control is orders of\nmagnitude more frequent in the text data used to train such models. We examine\nto what degree the models are sensitive to prompting with agent-patient\ninformation, finding that raising the salience of agent and patient relations\nresults in significant changes in the outputs of most models. Based on this\nobservation, we leverage an existing dataset of semantic proto-role annotations\n(White, et al. 2020) to explore the connections between control and labeling\nevent participants with properties typically associated with agents and\npatients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer. (arXiv:2205.12148v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12148","description":"<p>Massively multilingual models are promising for transfer learning across\ntasks and languages. However, existing methods are unable to fully leverage\ntraining data when it is available in different task-language combinations. To\nexploit such heterogeneous supervision we propose Hyper-X, a unified\nhypernetwork that generates weights for parameter-efficient adapter modules\nconditioned on both tasks and language embeddings. By learning to combine task\nand language-specific knowledge our model enables zero-shot transfer for unseen\nlanguages and task-language combinations. Our experiments on a diverse set of\nlanguages demonstrate that Hyper-X achieves the best gain when a mixture of\nmultiple resources is available while performing on par with strong baselines\nin the standard scenario. Finally, Hyper-X consistently produces strong results\nin few-shot scenarios for new languages and tasks showing the effectiveness of\nour approach beyond zero-shot transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ustun_A/0/1/0/all/0/1\">Ahmet &#xdc;st&#xfc;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouma_G/0/1/0/all/0/1\">Gosse Bouma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamic, Interpreted CheckList for Meaning-oriented NLG Metric Evaluation -- through the Lens of Semantic Similarity Rating. (arXiv:2205.12176v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12176","description":"<p>Evaluating the quality of generated text is difficult, since traditional NLG\nevaluation metrics, focusing more on surface form than meaning, often fail to\nassign appropriate scores. This is especially problematic for AMR-to-text\nevaluation, given the abstract nature of AMR. Our work aims to support the\ndevelopment and improvement of NLG evaluation metrics that focus on meaning, by\ndeveloping a dynamic CheckList for NLG metrics that is interpreted by being\norganized around meaning-relevant linguistic phenomena. Each test instance\nconsists of a pair of sentences with their AMR graphs and a human-produced\ntextual semantic similarity or relatedness score. Our CheckList facilitates\ncomparative evaluation of metrics and reveals strengths and weaknesses of novel\nand traditional metrics. We demonstrate the usefulness of CheckList by\ndesigning a new metric GraCo that computes lexical cohesion graphs over AMR\nconcepts. Our analysis suggests that GraCo presents an interesting NLG metric\nworth future investigation and that meaning-oriented NLG metrics can profit\nfrom graph-based metric components using AMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeidler_L/0/1/0/all/0/1\">Laura Zeidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial-input baselines show that NLI models can ignore context, but they don't. (arXiv:2205.12181v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12181","description":"<p>When strong partial-input baselines reveal artifacts in crowdsourced NLI\ndatasets, the performance of full-input models trained on such datasets is\noften dismissed as reliance on spurious correlations. We investigate whether\nstate-of-the-art NLI models are capable of overriding default inferences made\nby a partial-input baseline. We introduce an evaluation set of 600 examples\nconsisting of perturbed premises to examine a RoBERTa model's sensitivity to\nedited contexts. Our results indicate that NLI models are still capable of\nlearning to condition on context--a necessary component of inferential\nreasoning--despite being trained on artifact-ridden datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srikanth_N/0/1/0/all/0/1\">Neha Srikanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning for Expressive Task-Related Sentence Representations. (arXiv:2205.12186v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12186","description":"<p>NLP models learn sentence representations for downstream tasks by tuning a\nmodel which is pre-trained by masked language modeling. However, after tuning,\nthe learned sentence representations may be skewed heavily toward label space\nand thus are not expressive enough to represent whole samples, which should\ncontain task-related information of both sentence inputs and labels. In this\nwork, we learn expressive sentence representations for supervised tasks which\n(1). contain task-related information in the sentence inputs, and (2). enable\ncorrect label predictions. To achieve this goal, we first propose a new\nobjective which explicitly points out the label token space in the input, and\npredicts categories of labels via an added [MASK] token. This objective\nencourages fusing the semantic information of both the label and sentence. Then\nwe develop a neighbor attention module, added on a frozen pre-trained model, to\nbuild connections between label/sentence tokens via their neighbors. The\npropagation can be further guided by the regularization on neighborhood\nrepresentations to encourage expressiveness. Experimental results show that,\ndespite tuning only 5% additional parameters over a frozen pre-trained model,\nour model can achieve classification results comparable to the SOTA while\nmaintaining strong expressiveness as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xueying Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jinghuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization. (arXiv:2205.12191v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12191","description":"<p>Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data\nhave demonstrated strong performance on various tasks such as image captioning\nand visual question answering (VQA). The quality of such models is commonly\nassessed by measuring their performance on unseen data that typically comes\nfrom the same distribution as the training data. However, we observe that these\nmodels exhibit poor out-of-distribution (OOD) generalization on the task of\nVQA. To better understand the underlying causes of poor generalization, we\ncomprehensively investigate performance of two pretrained V&amp;L models under\ndifferent settings (i.e. classification and open-ended text generation) by\nconducting cross-dataset evaluations. We find that these models tend to learn\nto solve the benchmark, rather than learning the high-level skills required by\nthe VQA task. We also argue that in most cases generative models are less\nsusceptible to shifts in data distribution, while frequently performing better\non our tested benchmarks. Moreover, we find that multimodal pretraining\nimproves OOD performance in most settings. Finally, we revisit assumptions\nunderlying the use of automatic VQA evaluation metrics, and empirically show\nthat their stringent nature repeatedly penalizes models for correct responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Aishwarya Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajic_I/0/1/0/all/0/1\">Ivana Kaji&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoodi_E/0/1/0/all/0/1\">Elnaz Davoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gergely_A/0/1/0/all/0/1\">Anita Gergely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel's Weekly Video Podcasts. (arXiv:2205.12194v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12194","description":"<p>We introduce the Merkel Podcast Corpus, an audio-visual-text corpus in German\ncollected from 16 years of (almost) weekly Internet podcasts of former German\nchancellor Angela Merkel. To the best of our knowledge, this is the first\nsingle speaker corpus in the German language consisting of audio, visual and\ntext modalities of comparable size and temporal extent. We describe the methods\nused with which we have collected and edited the data which involves\ndownloading the videos, transcripts and other metadata, forced alignment,\nperforming active speaker recognition and face detection to finally curate the\nsingle speaker dataset consisting of utterances spoken by Angela Merkel. The\nproposed pipeline is general and can be used to curate other datasets of\nsimilar nature, such as talk show contents. Through various statistical\nanalyses and applications of the dataset in talking face generation and TTS, we\nshow the utility of the dataset. We argue that it is a valuable contribution to\nthe research community, in particular, due to its realistic and challenging\nmaterial at the boundary between prepared and spontaneous speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1\">Debjoy Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Shravan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumann_T/0/1/0/all/0/1\">Timo Baumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation. (arXiv:2205.12206v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12206","description":"<p>Formal verse poetry imposes strict constraints on the meter and rhyme scheme\nof poems. Most prior work on generating this type of poetry uses existing poems\nfor supervision, which are difficult to obtain for most languages and poetic\nforms. In this work, we propose an unsupervised approach to generate poems\nfollowing any given meter and rhyme scheme, without requiring any poetic text\nfor training. Our method works by splitting a regular, non-poetic corpus into\nphrases, prepending control codes that describe the length and end rhyme of\neach phrase, and training a transformer language model in the augmented corpus.\nDuring inference, we build control codes for the desired meter and rhyme\nscheme, and condition our language model on them to generate formal verse\npoetry. Experiments in Spanish and Basque show that our approach is able to\ngenerate valid poems, which are often comparable in quality to those written by\nhumans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1\">Aitor Ormazabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirrezabal_M/0/1/0/all/0/1\">Manex Agirrezabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start. (arXiv:2205.12209v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12209","description":"<p>We present EdiT5 - a novel semi-autoregressive text-editing approach designed\nto combine the strengths of non-autoregressive text-editing and autoregressive\ndecoding. EdiT5 is faster at inference times than conventional\nsequence-to-sequence (seq2seq) models, while being capable of modeling flexible\ninput-output transformations.\n</p>\n<p>This is achieved by decomposing the generation process into three sub-tasks:\n(1) tagging to decide on the subset of input tokens to be preserved in the\noutput, (2) re-ordering to define their order in the output text, and (3)\ninsertion to infill the missing tokens that are not present in the input. The\ntagging and re-ordering steps, which are responsible for generating the largest\nportion of the output, are non-autoregressive, while the insertion uses an\nautoregressive decoder.\n</p>\n<p>Depending on the task, EdiT5 requires significantly fewer autoregressive\nsteps demonstrating speedups of up to 25x when compared to classic seq2seq\nmodels. Quality-wise, EdiT5 is initialized with a pre-trained T5 checkpoint\nyielding comparable performance to T5 in high-resource settings and clearly\noutperforms it on low-resource settings when evaluated on three NLG tasks:\nSentence Fusion, Grammatical Error Correction, and Decontextualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallinson_J/0/1/0/all/0/1\">Jonathan Mallinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamek_J/0/1/0/all/0/1\">Jakub Adamek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1\">Eric Malmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1\">Aliaksei Severyn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Principled Paraphrase Generation with Parallel Corpora. (arXiv:2205.12213v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12213","description":"<p>Round-trip Machine Translation (MT) is a popular choice for paraphrase\ngeneration, which leverages readily available parallel corpora for supervision.\nIn this paper, we formalize the implicit similarity function induced by this\napproach, and show that it is susceptible to non-paraphrase pairs sharing a\nsingle ambiguous translation. Based on these insights, we design an alternative\nsimilarity metric that mitigates this issue by requiring the entire translation\ndistribution to match, and implement a relaxation of it through the Information\nBottleneck method. Our approach incorporates an adversarial term into MT\ntraining in order to learn representations that encode as much information\nabout the reference translation as possible, while keeping as little\ninformation about the input as possible. Paraphrases can be generated by\ndecoding back to the source from this representation, without having to\ngenerate pivot translations. In addition to being more principled and efficient\nthan round-trip MT, our approach offers an adjustable parameter to control the\nfidelity-diversity trade-off, and obtains better results in our experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1\">Aitor Ormazabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1\">Mikel Artetxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1\">Gorka Labaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1\">Aitor Soroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages. (arXiv:2205.12215v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12215","description":"<p>We introduce DivEMT, the first publicly available post-editing study of\nNeural Machine Translation (NMT) over a typologically diverse set of target\nlanguages. Using a strictly controlled setup, 18 professional translators were\ninstructed to translate or post-edit the same set of English documents into\nArabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process,\ntheir edits, keystrokes, editing times, pauses, and perceived effort were\nrecorded, enabling an in-depth, cross-lingual evaluation of NMT quality and its\npost-editing process. Using this new dataset, we assess the impact on\ntranslation productivity of two state-of-the-art NMT systems, namely: Google\nTranslate and the open-source multilingual model mBART50. We find that, while\npost-editing is consistently faster than translation from scratch, the\nmagnitude of its contribution varies largely across systems and languages,\nranging from doubled productivity in Dutch and Italian to marginal gains in\nArabic, Turkish and Ukrainian, for some of the evaluated modalities. Moreover,\nthe observed cross-language variability appears to partly reflect source-target\nrelatedness and type of target morphology, while remaining hard to predict even\nbased on state-of-the-art automatic MT quality metrics. We publicly release the\ncomplete dataset, including all collected behavioural data, to foster new\nresearch on the ability of state-of-the-art NMT systems to generate text in\ntypologically diverse languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarti_G/0/1/0/all/0/1\">Gabriele Sarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arenas_A/0/1/0/all/0/1\">Ana Guerberof Arenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation. (arXiv:2205.12216v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12216","description":"<p>We present a new approach to perform zero-shot cross-modal transfer between\nspeech and text for translation tasks. Multilingual speech and text are encoded\nin a joint fixed-size representation space. Then, we compare different\napproaches to decode these multimodal and multilingual fixed-size\nrepresentations, enabling zero-shot translation between languages and\nmodalities. All our models are trained without the need of cross-modal labeled\ntranslation data. Despite a fixed-size representation, we achieve very\ncompetitive results on several text and speech translation tasks. In\nparticular, we significantly improve the state-of-the-art for zero-shot speech\ntranslation on Must-C. Incorporating a speech decoder in our framework, we\nintroduce the first results for zero-shot direct speech-to-speech and\ntext-to-speech translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duquenne_P/0/1/0/all/0/1\">Paul-Ambroise Duquenne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwenk_H/0/1/0/all/0/1\">Holger Schwenk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12219","description":"<p>The ability to converse with humans and follow commands in natural language\nis crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can\nrelieve people's burden of holding a controller all the time, allow\nmultitasking, and make drone control more accessible for people with\ndisabilities or with their hands occupied. To this end, we introduce Aerial\nVision-and-Dialog Navigation (AVDN), to navigate a drone via natural language\nconversation. We build a drone simulator with a continuous photorealistic\nenvironment and collect a new AVDN dataset of over 3k recorded navigation\ntrajectories with asynchronous human-human dialogs between commanders and\nfollowers. The commander provides initial navigation instruction and further\nguidance by request, while the follower navigates the drone in the simulator\nand asks questions when needed. During data collection, followers' attention on\nthe drone's visual observation is also recorded. Based on the AVDN dataset, we\nstudy the tasks of aerial navigation from (full) dialog history and propose an\neffective Human Attention Aided (HAA) baseline model, which learns to predict\nboth navigation waypoints and human attention. Dataset and code will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Winson Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tongzhou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Fact Verification: Comparing and Contrasting Claims on Contentious Topics. (arXiv:2205.12221v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12221","description":"<p>As the importance of identifying misinformation is increasing, many\nresearchers focus on verifying textual claims on the web. One of the most\npopular tasks to achieve this is fact verification, which retrieves an evidence\nsentence from a large knowledge source such as Wikipedia to either verify or\nrefute each factual claim. However, while such problem formulation is helpful\nfor detecting false claims and fake news, it is not applicable to catching\nsubtle differences in factually consistent claims which still might implicitly\nbias the readers, especially in contentious topics such as political, gender,\nor racial issues. In this study, we propose ClaimDiff, a novel dataset to\ncompare the nuance between claim pairs in both a discriminative and a\ngenerative manner, with the underlying assumption that one is not necessarily\nmore true than the other. This differs from existing fact verification datasets\nthat verify the target sentence with respect to an absolute truth. We hope this\ntask assists people in making more informed decisions among various sources of\nmedia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ko_M/0/1/0/all/0/1\">Miyoung Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seong_I/0/1/0/all/0/1\">Ingyu Seong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joonsuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems. (arXiv:2205.12228v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12228","description":"<p>In natural language understanding (NLU) production systems, users' evolving\nneeds necessitate the addition of new features over time, indexed by new\nsymbols added to the meaning representation space. This requires additional\ntraining data and results in ever-growing datasets. We present the first\nsystematic investigation into this incremental symbol learning scenario. Our\nanalyses reveal a troubling quirk in building (broad-coverage) NLU systems: as\nthe training dataset grows, more data is needed to learn new symbols, forming a\nvicious cycle. We show that this trend holds for multiple mainstream models on\ntwo common NLU tasks: intent recognition and semantic parsing. Rejecting class\nimbalance as the sole culprit, we reveal that the trend is closely associated\nwith an effect we call source signal dilution, where strong lexical cues for\nthe new symbol become diluted as the training dataset grows. Selectively\ndropping training examples to prevent dilution often reverses the trend,\nshowing the over-reliance of mainstream neural NLU models on simple lexical\ncues and their lack of contextual understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platanios_E/0/1/0/all/0/1\">Emmanouil Antonios Platanios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauls_A/0/1/0/all/0/1\">Adam Pauls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_S/0/1/0/all/0/1\">Sam Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chunk-based Nearest Neighbor Machine Translation. (arXiv:2205.12230v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12230","description":"<p>Semi-parametric models, which augment generation with retrieval, have led to\nimpressive results in language modeling and machine translation, due to their\nability to leverage information retrieved from a datastore of examples. One of\nthe most prominent approaches, $k$NN-MT, has an outstanding performance on\ndomain adaptation by retrieving tokens from a domain-specific datastore\n\\citep{khandelwal2020nearest}. However, $k$NN-MT requires retrieval for every\nsingle generated token, leading to a very low decoding speed (around 8 times\nslower than a parametric model). In this paper, we introduce a\n\\textit{chunk-based} $k$NN-MT model which retrieves chunks of tokens from the\ndatastore, instead of a single token. We propose several strategies for\nincorporating the retrieved chunks into the generation process, and for\nselecting the steps at which the model needs to search for neighbors in the\ndatastore. Experiments on machine translation in two settings, static domain\nadaptation and ``on-the-fly'' adaptation, show that the chunk-based $k$NN-MT\nmodel leads to a significant speed-up (up to 4 times) with only a small drop in\ntranslation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIRATrustData: A Trust-Annotated Corpus of Human-Chatbot Conversations About COVID-19 Vaccines. (arXiv:2205.12240v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12240","description":"<p>Public trust in medical information is crucial for successful application of\npublic health policies such as vaccine uptake. This is especially true when the\ninformation is offered remotely, by chatbots, which have become increasingly\npopular in recent years. Here, we explore the challenging task of human-bot\nturn-level trust classification. We rely on a recently released data of\nobservationally-collected (rather than crowdsourced) dialogs with VIRA chatbot,\na COVID-19 Vaccine Information Resource Assistant. These dialogs are centered\naround questions and concerns about COVID-19 vaccines, where trust is\nparticularly acute. We annotated $3k$ VIRA system-user conversational turns for\nLow Institutional Trust or Low Agent Trust vs. Neutral or High Trust. We\nrelease the labeled dataset, VIRATrustData, the first of its kind to the best\nof our knowledge. We demonstrate how this task is non-trivial and compare\nseveral models that predict the different levels of trust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_R/0/1/0/all/0/1\">Roni Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gretz_S/0/1/0/all/0/1\">Shai Gretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_A/0/1/0/all/0/1\">Assaf Toledo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weeks_R/0/1/0/all/0/1\">Rose Weeks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Zeev_N/0/1/0/all/0/1\">Naor Bar-Zeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of Hierarchical Conversation Structure. (arXiv:2205.12244v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12244","description":"<p>Human conversations can evolve in many different ways, creating challenges\nfor automatic understanding and summarization. Goal-oriented conversations\noften have meaningful sub-dialogue structure, but it can be highly\ndomain-dependent. This work introduces an unsupervised approach to learning\nhierarchical conversation structure, including turn and sub-dialogue segment\nlabels, corresponding roughly to dialogue acts and sub-tasks, respectively. The\ndecoded structure is shown to be useful in enhancing neural models of language\nfor three conversation-level understanding tasks. Further, the learned\nfinite-state sub-dialogue network is made interpretable through automatic\nsummarization. Our code and trained models are available at\n\\url{https://github.com/boru-roylu/THETA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo-Ru Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yushi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models. (arXiv:2205.12247v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12247","description":"<p>Recent work has shown that Pre-trained Language Models (PLMs) have the\nability to store the relational knowledge from pre-training data in their model\nparameters. However, it is not clear up to what extent do PLMs store\ngeo-diverse commonsense knowledge, the knowledge associated with a culture and\nonly shared locally. For instance, the color of bridal dress is white in\nAmerican weddings whereas it is red in Chinese weddings. Here, we wish to probe\nif PLMs can predict red and white as the color of the bridal dress when queried\nfor American and Chinese weddings, respectively. To this end, we introduce a\nframework for geo-diverse commonsense probing on multilingual PLMs (mPLMs) and\nintroduce a corresponding benchmark Geo-diverse Commonsense Multilingual\nLanguage Model Analysis (GeoMLAMA) dataset. GeoMLAMA contains 3125 prompts in\nEnglish, Chinese, Hindi, Persian, and Swahili, with a wide coverage of concepts\nshared by people from American, Chinese, Indian, Iranian and Kenyan cultures.\nWe benchmark 11 standard mPLMs which include variants of mBERT, XLM, mT5, and\nXGLM on GeoMLAMA. Interestingly, we find that 1) larger mPLM variants do not\nnecessarily store geo-diverse concepts better than its smaller variant; 2)\nmPLMs are not intrinsically biased towards knowledge from the Western countries\n(the United States); 3) the native language of a country may not be the best\nlanguage to probe its knowledge and 4) a language may better probe knowledge\nabout a non-native country than its native country.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1\">Hritik Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RevUp: Revise and Update Information Bottleneck for Event Representation. (arXiv:2205.12248v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12248","description":"<p>In machine learning, latent variables play a key role to capture the\nunderlying structure of data, but they are often unsupervised. When we have\nside knowledge that already has high-level information about the input data, we\ncan use that source to guide latent variables and capture the available\nbackground information in a process called \"parameter injection.\" In that\nregard, we propose a semi-supervised information bottleneck-based model that\nenables the use of side knowledge, even if it is noisy and imperfect, to direct\nthe learning of discrete latent variables. Fundamentally, we introduce an\nauxiliary continuous latent variable as a way to reparameterize the model's\ndiscrete variables with a light-weight hierarchical structure. With this\nreparameterization, the model's discrete latent variables are learned to\nminimize the mutual information between the observed data and optional side\nknowledge that is not already captured by the new, auxiliary variables. We\ntheoretically show that our approach generalizes an existing method of\nparameter injection, and perform an empirical case study of our approach on\nlanguage-based event modeling. We corroborate our theoretical results with\nstrong empirical experiments, showing that the proposed method outperforms\nprevious proposed approaches on multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaee_M/0/1/0/all/0/1\">Mehdi Rezaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing. (arXiv:2205.12253v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12253","description":"<p>Despite their strong performance on many tasks, pre-trained language models\nhave been shown to struggle on out-of-distribution compositional\ngeneralization. Meanwhile, recent work has shown considerable improvements on\nmany NLP tasks from model scaling. Can scaling up model size also improve\ncompositional generalization in semantic parsing? We evaluate encoder-decoder\nmodels up to 11B parameters and decoder-only models up to 540B parameters, and\ncompare model scaling curves for three different methods for transfer learning:\nfine-tuning all parameters, prompt tuning, and in-context learning. We observe\nthat fine-tuning generally has flat or negative scaling curves on\nout-of-distribution compositional generalization in semantic parsing\nevaluations. In-context learning has positive scaling curves, but is generally\noutperformed by much smaller fine-tuned models. Prompt-tuning can outperform\nfine-tuning, suggesting further potential improvements from scaling as it\nexhibits a more positive scaling curve. Additionally, we identify several error\ntrends that vary with model scale. For example, larger models are generally\nbetter at modeling the syntax of the output space, but are also more prone to\ncertain types of overfitting. Overall, our study highlights limitations of\ncurrent techniques for effectively leveraging model scale for compositional\ngeneralization, while our analysis also suggests promising directions for\nfuture work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tianze Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1\">Jonathan Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitler_E/0/1/0/all/0/1\">Emily Pitler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretation Quality Score for Measuring the Quality of interpretability methods. (arXiv:2205.12254v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12254","description":"<p>Machine learning (ML) models have been applied to a wide range of natural\nlanguage processing (NLP) tasks in recent years. In addition to making accurate\ndecisions, the necessity of understanding how models make their decisions has\nbecome apparent in many applications. To that end, many interpretability\nmethods that help explain the decision processes of ML models have been\ndeveloped. Yet, there currently exists no widely-accepted metric to evaluate\nthe quality of explanations generated by these methods. As a result, there\ncurrently is no standard way of measuring to what degree an interpretability\nmethod achieves an intended objective. Moreover, there is no accepted standard\nof performance by which we can compare and rank the current existing\ninterpretability methods. In this paper, we propose a novel metric for\nquantifying the quality of explanations generated by interpretability methods.\nWe compute the metric on three NLP tasks using six interpretability methods and\npresent our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuansheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1\">Saeed Hassanpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TALM: Tool Augmented Language Models. (arXiv:2205.12255v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12255","description":"<p>Transformer based language models (LMs) demonstrate increasing performance\nwith scale across a wide variety of tasks. Scale alone however cannot enable\nmodels to solve tasks that require access to ephemeral, changing, or private\ndata that was unavailable at training time. Many useful tasks may also benefit\nfrom LMs being able to access APIs that read or modify state. In this work, we\npresent Tool Augmented Language Models (TALM), combining a text-only approach\nto augment language models with non-differentiable tools, and an iterative\n\"self-play\" technique to bootstrap performance starting from few tool\ndemonstrations. TALM exhibits strong performance on both a knowledge-heavy QA\ntask and a reasoning oriented math task with simple tools. At a given model\nscale, TALM significantly outperforms non-augmented LMs. We further demonstrate\nthat TALM successfully performs out-of-distribution inferences on both QA and\nmath tasks, where non-augmented LMs fail. Our results suggest that Tool\nAugmented Language Models are a promising direction to enrich LMs'\ncapabilities, with less dependence on scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1\">Aaron Parisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1\">Noah Fiedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"History Compression via Language Models in Reinforcement Learning. (arXiv:2205.12258v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12258","description":"<p>In a partially observable Markov decision process (POMDP), an agent typically\nuses a representation of the past to approximate the underlying MDP. We propose\nto utilize a frozen Pretrained Language Transformer (PLT) for history\nrepresentation and compression to improve sample efficiency. To avoid training\nof the Transformer, we introduce FrozenHopfield, which automatically associates\nobservations with original token embeddings. To form these associations, a\nmodern Hopfield network stores the original token embeddings, which are\nretrieved by queries that are obtained by a random but fixed projection of\nobservations. Our new method, HELM, enables actor-critic network architectures\nthat contain a pretrained language Transformer for history representation as a\nmemory module. Since a representation of the past need not be learned, HELM is\nmuch more sample efficient than competitors. On Minigrid and Procgen\nenvironments HELM achieves new state-of-the-art results. Our code is available\nat https://github.com/ml-jku/helm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paischer_F/0/1/0/all/0/1\">Fabian Paischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_T/0/1/0/all/0/1\">Thomas Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1\">Vihang Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitto_Nemling_A/0/1/0/all/0/1\">Angela Bitto-Nemling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzleitner_M/0/1/0/all/0/1\">Markus Holzleitner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehner_S/0/1/0/all/0/1\">Sebastian Lehner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eghbal_zadeh_H/0/1/0/all/0/1\">Hamid Eghbal-zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1\">Sepp Hochreiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Policy Compliance Detection via Expression Tree Inference. (arXiv:2205.12259v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12259","description":"<p>Policy Compliance Detection (PCD) is a task we encounter when reasoning over\ntexts, e.g. legal frameworks. Previous work to address PCD relies heavily on\nmodeling the task as a special case of Recognizing Textual Entailment.\nEntailment is applicable to the problem of PCD, however viewing the policy as a\nsingle proposition, as opposed to multiple interlinked propositions, yields\npoor performance and lacks explainability. To address this challenge, more\nrecent proposals for PCD have argued for decomposing policies into expression\ntrees consisting of questions connected with logic operators. Question\nanswering is used to obtain answers to these questions with respect to a\nscenario. Finally, the expression tree is evaluated in order to arrive at an\noverall solution. However, this work assumes expression trees are provided by\nexperts, thus limiting its applicability to new policies. In this work, we\nlearn how to infer expression trees automatically from policy texts. We ensure\nthe validity of the inferred trees by introducing constrained decoding using a\nfinite state automaton to ensure the generation of valid trees. We determine\nthrough automatic evaluation that 63% of the expression trees generated by our\nconstrained generation model are logically equivalent to gold trees. Human\nevaluation shows that 88% of trees generated by our model are correct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotonya_N/0/1/0/all/0/1\">Neema Kotonya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-domain Detection for Natural Language Understanding in Dialog Systems. (arXiv:1909.03862v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1909.03862","description":"<p>Natural Language Understanding (NLU) is a vital component of dialogue\nsystems, and its ability to detect Out-of-Domain (OOD) inputs is critical in\npractical applications, since the acceptance of the OOD input that is\nunsupported by the current system may lead to catastrophic failure. However,\nmost existing OOD detection methods rely heavily on manually labeled OOD\nsamples and cannot take full advantage of unlabeled data. This limits the\nfeasibility of these models in practical applications.\n</p>\n<p>In this paper, we propose a novel model to generate high-quality pseudo OOD\nsamples that are akin to IN-Domain (IND) input utterances, and thereby improves\nthe performance of OOD detection. To this end, an autoencoder is trained to map\nan input utterance into a latent code. and the codes of IND and OOD samples are\ntrained to be indistinguishable by utilizing a generative adversarial network.\nTo provide more supervision signals, an auxiliary classifier is introduced to\nregularize the generated OOD samples to have indistinguishable intent labels.\nExperiments show that these pseudo OOD samples generated by our model can be\nused to effectively improve OOD detection in NLU. Besides, we also demonstrate\nthat the effectiveness of these pseudo OOD data can be further improved by\nefficiently utilizing unlabeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Easy and Efficient Transformer : Scalable Inference Solution For large NLP model. (arXiv:2104.12470v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12470","description":"<p>Recently, large-scale transformer-based models have been proven to be\neffective over various tasks across many domains. Nevertheless, applying them\nin industrial production requires tedious and heavy works to reduce inference\ncosts. To fill such a gap, we introduce a scalable inference solution: Easy and\nEfficient Transformer (EET), including a series of transformer inference\noptimization at the algorithm and implementation levels. First, we design\nhighly optimized kernels for long inputs and large hidden sizes. Second, we\npropose a flexible CUDA memory manager to reduce the memory footprint when\ndeploying a large model. Compared with the state-of-the-art transformer\ninference library (Faster Transformer v4.0), EET can achieve an average of\n1.40-4.20x speedup on the transformer decoder layer with an A100 GPU\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jingzhen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Enhanced Explainable Finetuning for Open-Domain Dialogues. (arXiv:2106.03065v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.03065","description":"<p>This paper propose to combine pretrained language models with the modular\ndialogue paradigm for open-domain dialogue modeling. Our method,\nsemantic-enhanced finetuning, instantiates conversation understanding,\nplanning, and response generation as a language model finetuning task. At\ninference, we disentangle semantic and token variations by specifying sampling\nmethods and constraints for each module separately. For training and\nevaluation, we present X-Weibo, a Chinese multi-turn open-domain dialogue\ndataset with automatic annotation for emotions, DAs, and topical words.\nExperiments show that semantic-enhanced finetuning outperforms strong baselines\non non-semantic and semantic metrics, improves the human-evaluated relevance,\ncoherence, and informativeness, and exhibits considerable controllability over\nsemantic variables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear-time calculation of the expected sum of edge lengths in random projective linearizations of trees. (arXiv:2107.03277v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03277","description":"<p>The syntactic structure of a sentence is often represented using syntactic\ndependency trees. The sum of the distances between syntactically related words\nhas been in the limelight for the past decades. Research on dependency\ndistances led to the formulation of the principle of dependency distance\nminimization whereby words in sentences are ordered so as to minimize that sum.\nNumerous random baselines have been defined to carry out related quantitative\nstudies on languages. The simplest random baseline is the expected value of the\nsum in unconstrained random permutations of the words in the sentence, namely\nwhen all the shufflings of the words of a sentence are allowed and equally\nlikely. Here we focus on a popular baseline: random projective permutations of\nthe words of the sentence, that is, permutations where the syntactic dependency\nstructure is projective, a formal constraint that sentences satisfy often in\nlanguages. Thus far, the expectation of the sum of dependency distances in\nrandom projective shufflings of a sentence has been estimated approximately\nwith a Monte Carlo procedure whose cost is of the order of $Rn$, where $n$ is\nthe number of words of the sentence and $R$ is the number of samples; it is\nwell known that the larger $R$, the lower the error of the estimation but the\nlarger the time cost. Here we present formulae to compute that expectation\nwithout error in time of the order of $n$. Furthermore, we show that star trees\nmaximize it, and give an algorithm to retrieve the trees that minimize it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks. (arXiv:2107.07610v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07610","description":"<p>In this paper, we present an approach to improve the robustness of BERT\nlanguage models against word substitution-based adversarial attacks by\nleveraging adversarial perturbations for self-supervised contrastive learning.\nWe create a word-level adversarial attack generating hard positives on-the-fly\nas adversarial examples during contrastive learning. In contrast to previous\nworks, our method improves model robustness without using any labeled data.\nExperimental results show that our method improves robustness of BERT against\nfour different word substitution-based adversarial attacks, and combining our\nmethod with adversarial training gives higher robustness than adversarial\ntraining alone. As our method improves the robustness of BERT purely with\nunlabeled data, it opens up the possibility of using large text datasets to\ntrain robust language models against word substitution-based adversarial\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yihan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ground Visual Objects for Visual Dialog. (arXiv:2109.06013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06013","description":"<p>Visual dialog is challenging since it needs to answer a series of coherent\nquestions based on understanding the visual environment. How to ground related\nvisual objects is one of the key problems. Previous studies utilize the\nquestion and history to attend to the image and achieve satisfactory\nperformance, however these methods are not sufficient to locate related visual\nobjects without any guidance. The inappropriate grounding of visual objects\nprohibits the performance of visual dialog models. In this paper, we propose a\nnovel approach to Learn to Ground visual objects for visual dialog, which\nemploys a novel visual objects grounding mechanism where both prior and\nposterior distributions over visual objects are used to facilitate visual\nobjects grounding. Specifically, a posterior distribution over visual objects\nis inferred from both context (history and questions) and answers, and it\nensures the appropriate grounding of visual objects during the training\nprocess. Meanwhile, a prior distribution, which is inferred from context only,\nis used to approximate the posterior distribution so that appropriate visual\nobjects can be grounded even without answers during the inference process.\nExperimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our\napproach improves the previous strong models in both generative and\ndiscriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual Knowledge Learning of Language Models. (arXiv:2110.03215v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03215","description":"<p>Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs. The benchmark datasets, evaluation script, and\nbaseline code to reproduce our results are available at\nhttps://github.com/joeljang/continual-knowledge-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Janghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeonghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Stanley Jungkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-aware Attention Improves Neural Machine Translation. (arXiv:2110.06920v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06920","description":"<p>The integration of syntactic structures into Transformer machine translation\nhas shown positive results, but to our knowledge, no work has attempted to do\nso with semantic structures. In this work we propose two novel parameter-free\nmethods for injecting semantic information into Transformers, both rely on\nsemantics-aware masking of (some of) the attention heads. One such method\noperates on the encoder, through a Scene-Aware Self-Attention (SASA) head.\nAnother on the decoder, through a Scene-Aware Cross-Attention (SACrA) head. We\nshow a consistent improvement over the vanilla Transformer and syntax-aware\nmodels for four language pairs. We further show an additional gain when using\nboth semantic and syntactic structures in some language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slobodkin_A/0/1/0/all/0/1\">Aviv Slobodkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing. (arXiv:2110.07205v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.07205","description":"<p>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in\npre-trained natural language processing models, we propose a unified-modal\nSpeechT5 framework that explores the encoder-decoder pre-training for\nself-supervised speech/text representation learning. The SpeechT5 framework\nconsists of a shared encoder-decoder network and six modal-specific\n(speech/text) pre/post-nets. After preprocessing the input speech/text through\nthe pre-nets, the shared encoder-decoder network models the\nsequence-to-sequence transformation, and then the post-nets generate the output\nin the speech/text modality based on the output of the decoder. Leveraging\nlarge-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a\nunified-modal representation, hoping to improve the modeling capability for\nboth speech and text. To align the textual and speech information into this\nunified semantic space, we propose a cross-modal vector quantization approach\nthat randomly mixes up speech/text states with latent units as the interface\nbetween encoder and decoder. Extensive evaluations show the superiority of the\nproposed SpeechT5 framework on a wide variety of spoken language processing\ntasks, including automatic speech recognition, speech synthesis, speech\ntranslation, voice conversion, speech enhancement, and speaker identification.\nWe release our code and model at https://github.com/microsoft/SpeechT5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ao_J/0/1/0/all/0/1\">Junyi Ao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why KDAC? A general activation function for knowledge discovery. (arXiv:2111.13858v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13858","description":"<p>Deep learning oriented named entity recognition (DNER) has gradually become\nthe paradigm of knowledge discovery, which greatly promotes domain\nintelligence. However, the current activation function of DNER fails to treat\ngradient vanishing, no negative output or non-differentiable existence, which\nmay impede knowledge exploration caused by the omission and incomplete\nrepresentation of latent semantics. To break through the dilemma, we present a\nnovel activation function termed KDAC. Detailly, KDAC is an aggregation\nfunction with multiple conversion modes. The backbone of the activation region\nis the interaction between exponent and linearity, and the both ends extend\nthrough adaptive linear divergence, which surmounts the obstacle of gradient\nvanishing and no negative output. Crucially, the non-differentiable points are\nalerted and eliminated by an approximate smoothing algorithm. KDAC has a series\nof brilliant properties, including nonlinear, stable near-linear transformation\nand derivative, as well as dynamic style, etc. We perform experiments based on\nBERT-BiLSTM-CNN-CRF model on six benchmark datasets containing different domain\nknowledge, such as Weibo, Clinical, E-commerce, Resume, HAZOP and People's\ndaily. The evaluation results show that KDAC is advanced and effective, and can\nprovide more generalized activation to stimulate the performance of DNER. We\nhope that KDAC can be exploited as a promising activation function to devote\nitself to the construction of knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanglin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge. (arXiv:2112.08544v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08544","description":"<p>Claim detection and verification are crucial for news understanding and have\nemerged as promising technologies for mitigating news misinformation. However,\nmost existing work has focused on claim sentence analysis while overlooking\ncrucial background attributes (e.g., claimer, claim objects). In this work, we\npresent NewsClaims, a new benchmark for knowledge-aware claim detection in the\nnews domain. We redefine the claim detection problem to include extraction of\nadditional background attributes related to each claim and release 889 claims\nannotated over 143 news articles. NewsClaims aims to benchmark claim detection\nsystems in emerging scenarios, comprising unseen topics with little or no\ntraining data. To this end, we provide a comprehensive evaluation of zero-shot\nand prompt-based baselines for NewsClaims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetan_S/0/1/0/all/0/1\">Sai Chetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conger_K/0/1/0/all/0/1\">Kathryn Conger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_A/0/1/0/all/0/1\">Ahmed Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossSum: Beyond English-Centric Cross-Lingual Abstractive Text Summarization for 1500+ Language Pairs. (arXiv:2112.08804v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08804","description":"<p>We present CrossSum, a large-scale cross-lingual abstractive summarization\ndataset comprising 1.7 million article-summary samples in 1500+ language pairs.\nWe create CrossSum by aligning identical articles written in different\nlanguages via cross-lingual retrieval from a multilingual summarization\ndataset. We propose a multi-stage data sampling algorithm to effectively train\na cross-lingual summarization model capable of summarizing an article in any\ntarget language. We also propose LaSE, a new metric for automatically\nevaluating model-generated summaries and showing a strong correlation with\nROUGE. Performance on ROUGE and LaSE indicate that pretrained models fine-tuned\non CrossSum consistently outperform baseline models, even when the source and\ntarget language pairs are linguistically distant. To the best of our knowledge,\nCrossSum is the largest cross-lingual summarization dataset and the first-ever\nthat does not rely solely on English as the pivot language. We are releasing\nthe dataset, alignment and training scripts, and the models to spur future\nresearch on cross-lingual abstractive summarization. The resources can be found\nat https://github.com/csebuetnlp/CrossSum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yong-Bin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Questions Generate Named Entity Recognition Datasets. (arXiv:2112.08808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08808","description":"<p>Recent named entity recognition (NER) models often rely on human-annotated\ndatasets requiring the vast engagement of professional knowledge on the target\ndomain and entities. This work introduces an ask-to-generate approach, which\nautomatically generates NER datasets by asking simple natural language\nquestions to an open-domain question answering system (e.g., \"Which disease?\").\nDespite using fewer training resources, our models solely trained on the\ngenerated datasets largely outperform strong low-resource models by 20.8 F1\nscore on average across six popular NER benchmarks. Our models also show\ncompetitive performance with rich-resource models that additionally leverage\nin-domain dictionaries provided by domain experts. In few-shot NER, we\noutperform the previous best model by 5.2 F1 score on three benchmarks and\nachieve new state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaehyo Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RuMedBench: A Russian Medical Language Understanding Benchmark. (arXiv:2201.06499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06499","description":"<p>The paper describes the open Russian medical language understanding benchmark\ncovering several task types (classification, question answering, natural\nlanguage inference, named entity recognition) on a number of novel text sets.\nGiven the sensitive nature of the data in healthcare, such a benchmark\npartially closes the problem of Russian medical dataset absence. We prepare the\nunified format labeling, data split, and evaluation metrics for new tasks. The\nremaining tasks are from existing datasets with a few modifications. A\nsingle-number metric expresses a model's ability to cope with the benchmark.\nMoreover, we implement several baseline models, from simple ones to neural\nnetworks with transformer architecture, and release the code. Expectedly, the\nmore advanced models yield better performance, but even a simple model is\nenough for a decent result in some tasks. Furthermore, for all tasks, we\nprovide a human evaluation. Interestingly the models outperform humans in the\nlarge-scale classification tasks. However, the advantage of natural\nintelligence remains in the tasks requiring more knowledge and reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blinov_P/0/1/0/all/0/1\">Pavel Blinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reshetnikova_A/0/1/0/all/0/1\">Arina Reshetnikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesterov_A/0/1/0/all/0/1\">Aleksandr Nesterov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubkova_G/0/1/0/all/0/1\">Galina Zubkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12114","description":"<p>Attention mechanisms are dominating the explainability of deep models. They\nproduce probability distributions over the input, which are widely deemed as\nfeature-importance indicators. However, in this paper, we find one critical\nlimitation in attention explanations: weakness in identifying the polarity of\nfeature impact. This would be somehow misleading -- features with higher\nattention weights may not faithfully contribute to model predictions; instead,\nthey can impose suppression effects. With this finding, we reflect on the\nexplainability of current attention-based techniques, such as\nAttentio$\\odot$Gradient and LRP-based attention explanations. We first propose\nan actionable diagnostic methodology (henceforth faithfulness violation test)\nto measure the consistency between explanation weights and the impact polarity.\nThrough the extensive experiments, we then show that most tested explanation\nmethods are unexpectedly hindered by the faithfulness violation issue,\nespecially the raw attention. Empirical analyses on the factors affecting\nviolation issues further provide useful observations for adopting explanation\nmethods in attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning. (arXiv:2202.07206v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07206","description":"<p>Pretrained Language Models (LMs) have demonstrated ability to perform\nnumerical reasoning by extrapolating from a few examples in few-shot settings.\nHowever, the extent to which this extrapolation relies on robust reasoning is\nunclear. In this paper, we investigate how well these models reason with terms\nthat are less frequent in the pretraining data. In particular, we examine the\ncorrelations between the model performance on test instances and the frequency\nof terms from those instances in the pretraining data. We measure the strength\nof this correlation for a number of GPT-based language models (pretrained on\nthe Pile dataset) on various numerical deduction tasks (e.g., arithmetic and\nunit conversion). Our results consistently demonstrate that models are more\naccurate on instances whose terms are more prevalent, in some cases above\n$70\\%$ (absolute) more accurate on the top 10\\% frequent terms in comparison to\nthe bottom 10\\%. Overall, although LMs exhibit strong performance at few-shot\nnumerical reasoning tasks, our results raise the question of how much models\nactually generalize beyond pretraining data, and we encourage researchers to\ntake the pretraining data into account when interpreting evaluation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razeghi_Y/0/1/0/all/0/1\">Yasaman Razeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logan_R/0/1/0/all/0/1\">Robert L. Logan IV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logical Fallacy Detection. (arXiv:2202.13758v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13758","description":"<p>Reasoning is central to human intelligence. However, fallacious arguments are\ncommon, and some exacerbate problems such as spreading misinformation about\nclimate change. In this paper, we propose the task of logical fallacy\ndetection, and provide a new dataset (Logic) of logical fallacies generally\nfound in text, together with an additional challenge set for detecting logical\nfallacies in climate change claims (LogicClimate). Detecting logical fallacies\nis a hard problem as the model must understand the underlying logical structure\nof the argument. We find that existing pretrained large language models perform\npoorly on this task. In contrast, we show that a simple structure-aware\nclassifier outperforms the best language model by 5.46% on Logic and 4.51% on\nLogicClimate. We encourage future work to explore this task as (a) it can serve\nas a new reasoning challenge for language models, and (b) it can have potential\napplications in tackling the spread of misinformation. Our dataset and code are\navailable at https://github.com/causalNLP/logical-fallacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalwani_A/0/1/0/all/0/1\">Abhinav Lalwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidhya_T/0/1/0/all/0/1\">Tejas Vaidhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Resource-Constrained Keyphrase Generation. (arXiv:2203.08118v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08118","description":"<p>State-of-the-art keyphrase generation methods generally depend on large\nannotated datasets, limiting their performance in domains with limited\nannotated data. To overcome this challenge, we design a data-oriented approach\nthat first identifies salient information using unsupervised corpus-level\nstatistics, and then learns a task-specific intermediate representation based\non a pre-trained language model. We introduce salient span recovery and salient\nspan prediction as denoising training objectives that condense the\nintra-article and inter-article knowledge essential for keyphrase generation.\nThrough experiments on multiple keyphrase generation benchmarks, we show the\neffectiveness of the proposed approach for facilitating low-resource and\nzero-shot keyphrase generation. We further observe that the method especially\nbenefits the generation of absent keyphrases, approaching the performance of\nmodels trained with large training sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine. (arXiv:2203.10232v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10232","description":"<p>In this paper, we present DuReader-retrieval, a large-scale Chinese dataset\nfor passage retrieval. DuReader-retrieval contains more than 90K queries and\nover 8M unique passages from Baidu search. To ensure the quality of our\nbenchmark and address the shortcomings in other existing datasets, we (1)\nreduce the false negatives in development and testing sets by pooling the\nresults from multiple retrievers with human annotations, (2) and de-duplicate\nthe semantically similar questions between training with development and\ntesting sets. Additionally, we provide two out-of-domain testing sets for\ncross-domain evaluation, as well as a cross-lingual set that has been manually\ntranslated for cross-lingual retrieval. The experiments demonstrate that\nDuReader-retrieval is challenging and there is still plenty of room for\nimprovement, e.g. salient phrase and syntax mismatch between query and\nparagraph. These experimental results show that the dense retriever does not\ngeneralize well across domains, and cross-lingual retrieval is essentially\nchallenging. DuReader-retrieval will be publicly available at\nhttps://github.com/baidu/DuReader/tree/master/DuReader-Retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yifu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Ranking and Aggregation of Label Descriptions for Zero-Shot Classifiers. (arXiv:2204.09481v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09481","description":"<p>Zero-shot text classifiers based on label descriptions embed an input text\nand a set of labels into the same space: measures such as cosine similarity can\nthen be used to select the most similar label description to the input text as\nthe predicted label. In a true zero-shot setup, designing good label\ndescriptions is challenging because no development set is available. Inspired\nby the literature on Learning with Disagreements, we look at how probabilistic\nmodels of repeated rating analysis can be used for selecting the best label\ndescriptions in an unsupervised fashion. We evaluate our method on a set of\ndiverse datasets and tasks (sentiment, topic and stance). Furthermore, we show\nthat multiple, noisy label descriptions can be aggregated to boost the\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basile_A/0/1/0/all/0/1\">Angelo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AiSocrates: Towards Answering Ethical Quandary Questions. (arXiv:2205.05989v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05989","description":"<p>Considerable advancements have been made in various NLP tasks based on the\nimpressive power of large pre-trained language models (LLMs). These results\nhave inspired efforts to understand the limits of LLMs so as to evaluate how\nfar we are from achieving human level general natural language understanding.\nIn this work, we challenge the capability of LLMs with the new task of Ethical\nQuandary Generative Question Answering. Ethical quandary questions are more\nchallenging to address because multiple conflicting answers may exist to a\nsingle quandary. We propose a system, AiSocrates, that provides an answer with\na deliberative exchange of different perspectives to an ethical quandary, in\nthe approach of Socratic philosophy, instead of providing a closed answer like\nan oracle. AiSocrates searches for different ethical principles applicable to\nthe ethical quandary and generates an answer conditioned on the chosen\nprinciples through prompt-based few-shot learning. We also address safety\nconcerns by providing a human controllability option in choosing ethical\nprinciples. We show that AiSocrates generates promising answers to ethical\nquandary questions with multiple perspectives, 6.92% more often than answers\nwritten by human philosophers by one measure, but the system still needs\nimprovement to match the coherence of human philosophers fully. We argue that\nAiSocrates is a promising step toward developing an NLP system that\nincorporates human values explicitly by prompt instructions. We are releasing\nthe code for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalatbari_L/0/1/0/all/0/1\">Leila Khalatbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kee_H/0/1/0/all/0/1\">Hayden Kee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Budge programming language. (arXiv:2205.07979v3 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2205.07979","description":"<p>We present a simple, esoteric programming language based on G\\\"odel numbering\nand prime factorization, enhanced with explicit, scoped loops, allowing for\neasy program composition. We will show the syntax and semantics and then\nprovide a few example programs and their evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitnikovski_B/0/1/0/all/0/1\">Boro Sitnikovski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator. (arXiv:2205.09058v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09058","description":"<p>Contextual knowledge is essential for reducing speech recognition errors on\nhigh-valued long-tail words. This paper proposes a novel tree-constrained\npointer generator (TCPGen) component that enables end-to-end ASR models to bias\ntowards a list of long-tail words obtained using external contextual\ninformation. With only a small overhead in memory use and computation cost,\nTCPGen can structure thousands of biasing words efficiently into a symbolic\nprefix-tree and creates a neural shortcut between the tree and the final ASR\noutput to facilitate the recognition of the biasing words. To enhance TCPGen,\nwe further propose a novel minimum biasing word error (MBWE) loss that directly\noptimises biasing word errors during training, along with a biasing-word-driven\nlanguage model discounting (BLMD) method during the test. All contextual ASR\nsystems were evaluated on the public Librispeech audiobook corpus and the data\nfrom the dialogue state tracking challenges (DSTC) with the biasing lists\nextracted from the dialogue-system ontology. Consistent word error rate (WER)\nreductions were achieved with TCPGen, which were particularly significant on\nthe biasing words with around 40\\% relative reductions in the recognition error\nrates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved\nmore significant WER reductions on the biasing words. TCPGen also achieved\nzero-shot learning of words not in the audio training set with large WER\nreductions on the out-of-vocabulary words in the biasing list.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeper vs Wider: A Revisit of Transformer Configuration. (arXiv:2205.10505v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.10505","description":"<p>Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianghai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-level Textual Adversarial Attack with Label Preservation. (arXiv:2205.10710v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10710","description":"<p>Generating high-quality textual adversarial examples is critical for\ninvestigating the pitfalls of natural language processing (NLP) models and\nfurther promoting their robustness. Existing attacks are usually realized\nthrough word-level or sentence-level perturbations, which either limit the\nperturbation space or sacrifice fluency and textual quality, both affecting the\nattack effectiveness. In this paper, we propose Phrase-Level Textual\nAdversarial aTtack (PLAT) that generates adversarial samples through\nphrase-level perturbations. PLAT first extracts the vulnerable phrases as\nattack targets by a syntactic parser, and then perturbs them by a pre-trained\nblank-infilling model. Such flexible perturbation design substantially expands\nthe search space for more effective attacks without introducing too many\nmodifications, and meanwhile maintaining the textual fluency and grammaticality\nvia contextualized generation using surrounding texts. Moreover, we develop a\nlabel-preservation filter leveraging the likelihoods of language models\nfine-tuned on each class, rather than textual similarity, to rule out those\nperturbations that potentially alter the original class label for humans.\nExtensive experiments and human evaluation demonstrate that PLAT has a superior\nattack effectiveness as well as a better label consistency than strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yibin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dianqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Proof Generation via Iterative Backward Reasoning. (arXiv:2205.10714v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10714","description":"<p>We present IBR, an Iterative Backward Reasoning model to solve the proof\ngeneration tasks on rule-based Question Answering (QA), where models are\nrequired to reason over a series of textual rules and facts to find out the\nrelated proof path and derive the final answer. We handle the limitations of\nexisted works in two folds: 1) enhance the interpretability of reasoning\nprocedures with detailed tracking, by predicting nodes and edges in the proof\npath iteratively backward from the question; 2) promote the efficiency and\naccuracy via reasoning on the elaborate representations of nodes and history\npaths, without any intermediate texts that may introduce external noise during\nproof generation. There are three main modules in IBR, QA and proof strategy\nprediction to obtain the answer and offer guidance for the following procedure;\nparent node prediction to determine a node in the existing proof that a new\nchild node will link to; child node prediction to find out which new node will\nbe added to the proof. Experiments on both synthetic and paraphrased datasets\ndemonstrate that IBR has better in-domain performance as well as cross-domain\ntransferability than several strong baselines. Our code and models are\navailable at https://github.com/find-knowledge/IBR .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Hanhao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relphormer: Relational Graph Transformer for Knowledge Graph Representation. (arXiv:2205.10852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10852","description":"<p>Transformers have achieved remarkable performance in widespread fields,\nincluding natural language processing, computer vision and graph mining.\nHowever, in the knowledge graph representation, where translational distance\nparadigm dominates this area, vanilla Transformer architectures have not\nyielded promising improvements. Note that vanilla Transformer architectures\nstruggle to capture the intrinsically semantic and structural information of\nknowledge graphs and can hardly scale to long-distance neighbors due to\nquadratic dependency. To this end, we propose a new variant of Transformer for\nknowledge graph representation dubbed Relphormer. Specifically, we introduce\nTriple2Seq which can dynamically sample contextualized sub-graph sequences as\nthe input of the Transformer to alleviate the scalability issue. We then\npropose a novel structure-enhanced self-attention mechanism to encode the\nrelational information and keep the globally semantic information among\nsub-graphs. Moreover, we propose masked knowledge modeling as a new paradigm\nfor knowledge graph representation learning to unify different link prediction\ntasks. Experimental results show that our approach can obtain better\nperformance on benchmark datasets compared with baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Siyuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BanglaNLG: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla. (arXiv:2205.11081v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11081","description":"<p>This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language in the web domain. We aggregate three challenging\nconditional text generation tasks under the BanglaNLG benchmark. Then, using a\nclean corpus of 27.5 GB of Bangla data, we pretrain BanglaT5, a\nsequence-to-sequence Transformer model for Bangla. BanglaT5 achieves\nstate-of-the-art performance in all of these tasks, outperforming mT5 (base) by\nup to 5.4%. We are making the BanglaT5 language model and a leaderboard\npublicly available in the hope of advancing future research and evaluation on\nBangla NLG. The resources can be found at\nhttps://github.com/csebuetnlp/BanglaNLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for Multi-stage Ranking. (arXiv:2205.11245v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2205.11245","description":"<p>This paper describes the PASH participation in TREC 2021 Deep Learning Track.\nIn the recall stage, we adopt a scheme combining sparse and dense retrieval\nmethod. In the multi-stage ranking phase, point-wise and pair-wise ranking\nstrategies are used one after another based on model continual pre-trained on\ngeneral knowledge and document-level data. Compared to TREC 2020 Deep Learning\nTrack, we have additionally introduced the generative model T5 to further\nenhance the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yixuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yongquan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tuozhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xianbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenfeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracing Knowledge in Language Models Back to the Training Data. (arXiv:2205.11482v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11482","description":"<p>Neural language models (LMs) have been shown to memorize a great deal of\nfactual knowledge. But when an LM generates an assertion, it is often difficult\nto determine where it learned this information and whether it is true. In this\npaper, we introduce a new benchmark for fact tracing: tracing language models'\nassertions back to the training examples that provided evidence for those\npredictions. Prior work has suggested that dataset-level influence methods\nmight offer an effective framework for tracing predictions back to training\ndata. However, such methods have not been evaluated for fact tracing, and\nresearchers primarily have studied them through qualitative analysis or as a\ndata cleaning technique for classification/regression tasks. We present the\nfirst experiments that evaluate influence methods for fact tracing, using\nwell-understood information retrieval (IR) metrics. We compare two popular\nfamilies of influence methods -- gradient-based and embedding-based -- and show\nthat neither can fact-trace reliably; indeed, both methods fail to outperform\nan IR baseline (BM25) that does not even access the LM. We explore why this\noccurs (e.g., gradient saturation) and demonstrate that existing influence\nmethods must be improved significantly before they can reliably attribute\nfactual predictions in LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolukbasi_T/0/1/0/all/0/1\">Tolga Bolukbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1\">Binbin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Exploring Concept Contribution Spatially: Hidden Layer Interpretation with Spatial Activation Concept Vector. (arXiv:2205.11511v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11511","description":"<p>To interpret deep learning models, one mainstream is to explore the learned\nconcepts by networks. Testing with Concept Activation Vector (TCAV) presents a\npowerful tool to quantify the contribution of query concepts (represented by\nuser-defined guidance images) to a target class. For example, we can\nquantitatively evaluate whether and to what extent concept striped contributes\nto model prediction zebra with TCAV. Therefore, TCAV whitens the reasoning\nprocess of deep networks. And it has been applied to solve practical problems\nsuch as diagnosis. However, for some images where the target object only\noccupies a small fraction of the region, TCAV evaluation may be interfered with\nby redundant background features because TCAV calculates concept contribution\nto a target class based on a whole hidden layer. To tackle this problem, based\non TCAV, we propose Spatial Activation Concept Vector (SACV) which identifies\nthe relevant spatial locations to the query concept while evaluating their\ncontributions to the model prediction of the target class. Experiment shows\nthat SACV generates a more fine-grained explanation map for a hidden layer and\nquantifies concepts' contributions spatially. Moreover, it avoids interference\nfrom background features. The code is available on\nhttps://github.com/AntonotnaWang/Spatial-Activation-Concept-Vector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Andong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wei-Ning Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cardiomegaly Detection using Deep Convolutional Neural Network with U-Net. (arXiv:2205.11515v1 [eess.IV])","link":"http://arxiv.org/abs/2205.11515","description":"<p>Cardiomegaly is indeed a medical disease in which the heart is enlarged.\nCardiomegaly is better to handle if caught early, so early detection is\ncritical. The chest X-ray, being one of the most often used radiography\nexaminations, has been used to detect and visualize abnormalities of human\norgans for decades. X-ray is also a significant medical diagnosis tool for\ncardiomegaly. Even for domain experts, distinguishing the many types of\ndiseases from the X-ray is a difficult and time-consuming task. Deep learning\nmodels are also most effective when used on huge data sets, yet due to privacy\nconcerns, large datasets are rarely available inside the medical industry. A\nDeep learning-based customized retrained U-Net model for detecting Cardiomegaly\ndisease is presented in this research. In the training phase, chest X-ray\nimages from the \"ChestX-ray8\" open source real dataset are used. To reduce\ncomputing time, this model performs data preprocessing, picture improvement,\nimage compression, and classification before moving on to the training step.\nThe work used a chest x-ray image dataset to simulate and produced a diagnostic\naccuracy of 94%, a sensitivity of 96.2 percent, and a specificity of 92.5\npercent, which beats prior pre-trained model findings for identifying\nCardiomegaly disease.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sarpotdar_S/0/1/0/all/0/1\">Soham S.Sarpotdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging via Differentiable Microscopy. (arXiv:2205.11521v1 [eess.IV])","link":"http://arxiv.org/abs/2205.11521","description":"<p>With applications ranging from metabolomics to histopathology, quantitative\nphase microscopy (QPM) is a powerful label-free imaging modality. Despite\nsignificant advances in fast multiplexed imaging sensors and\ndeep-learning-based inverse solvers, the throughput of QPM is currently limited\nby the speed of electronic hardware. Complementarily, to improve throughput\nfurther, here we propose to acquire images in a compressed form such that more\ninformation can be transferred beyond the existing electronic hardware\nbottleneck. To this end, we present a learnable optical\ncompression-decompression framework that learns content-specific features. The\nproposed differentiable optical-electronic quantitative phase microscopy\n($\\partial \\mu$) first uses learnable optical feature extractors as image\ncompressors. The intensity representation produced by these networks is then\ncaptured by the imaging sensor. Finally, a reconstruction network running on\nelectronic hardware decompresses the QPM images. The proposed system achieves\ncompression of $\\times$ 64 while maintaining the SSIM of $\\sim 0.90$ and PSNR\nof $\\sim 30$ dB. The promising results demonstrated by our experiments open up\na new pathway for achieving end-to-end optimized (i.e., optics and electronic)\ncompact QPM systems that provide unprecedented throughput improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Haputhanthri_U/0/1/0/all/0/1\">Udith Haputhanthri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herath_K/0/1/0/all/0/1\">Kithmini Herath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hettiarachchi_R/0/1/0/all/0/1\">Ramith Hettiarachchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kariyawasam_H/0/1/0/all/0/1\">Hasindu Kariyawasam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmad_A/0/1/0/all/0/1\">Azeem Ahmad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahluwalia_B/0/1/0/all/0/1\">Balpreet S. Ahluwalia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edussooriya_C/0/1/0/all/0/1\">Chamira U. S. Edussooriya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wadduwage_D/0/1/0/all/0/1\">Dushan N. Wadduwage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating the creation of instance segmentation training sets through bounding box annotation. (arXiv:2205.11563v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11563","description":"<p>Collecting image annotations remains a significant burden when deploying CNN\nin a specific applicative context. This is especially the case when the\nannotation consists in binary masks covering object instances. Our work\nproposes to delineate instances in three steps, based on a semi-automatic\napproach: (1) the extreme points of an object (left-most, right-most, top,\nbottom pixels) are manually defined, thereby providing the object bounding-box,\n(2) a universal automatic segmentation tool like Deep Extreme Cut is used to\nturn the bounded object into a segmentation mask that matches the extreme\npoints; and (3) the predicted mask is manually corrected. Various strategies\nare then investigated to balance the human manual annotation resources between\nbounding-box definition and mask correction, including when the correction of\ninstance masks is prioritized based on their overlap with other instance\nbounding-boxes, or the outcome of an instance segmentation model trained on a\npartially annotated dataset. Our experimental study considers a teamsport\nplayer segmentation task, and measures how the accuracy of the Panoptic-Deeplab\ninstance segmentation model depends on the human annotation resources\nallocation strategy. It reveals that the sole definition of extreme points\nresults in a model accuracy that would require up to 10 times more resources if\nthe masks were defined through fully manual delineation of instances. When\ntargeting higher accuracies, prioritizing the mask correction among the\ntraining set instances is also shown to save up to 80\\% of correction\nannotation resources compared to a systematic frame by frame correction of\ninstances, for a same trained instance segmentation model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sayez_N/0/1/0/all/0/1\">Niels Sayez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1\">Christophe De Vleeschouwer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VPAIR -- Aerial Visual Place Recognition and Localization in Large-scale Outdoor Environments. (arXiv:2205.11567v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11567","description":"<p>Visual Place Recognition and Visual Localization are essential components in\nnavigation and mapping for autonomous vehicles especially in GNSS-denied\nnavigation scenarios. Recent work has focused on ground or close to ground\napplications such as self-driving cars or indoor-scenarios and low-altitude\ndrone flights. However, applications such as Urban Air Mobility require\noperations in large-scale outdoor environments at medium to high altitudes. We\npresent a new dataset named VPAIR. The dataset was recorded on board a light\naircraft flying at an altitude of more than 300 meters above ground capturing\nimages with a downwardfacing camera. Each image is paired with a high\nresolution reference render including dense depth information and 6-DoF\nreference poses. The dataset covers a more than one hundred kilometers long\ntrajectory over various types of challenging landscapes, e.g. urban, farmland\nand forests. Experiments on this dataset illustrate the challenges introduced\nby the change in perspective to a bird's eye view such as in-plane rotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schleiss_M/0/1/0/all/0/1\">Michael Schleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouatbi_F/0/1/0/all/0/1\">Fahmi Rouatbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Feature Learning through Feature Distance Loss. (arXiv:2205.11606v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11606","description":"<p>Convolutional neural networks have shown remarkable ability to learn\ndiscriminative semantic features in image recognition tasks. Though, for\nclassification they often concentrate on specific regions in images. This work\nproposes a novel method that combines variant rich base models to concentrate\non different important image regions for classification. A feature distance\nloss is implemented while training an ensemble of base models to force them to\nlearn discriminative feature concepts. The experiments on benchmark\nconvolutional neural networks (VGG16, ResNet, AlexNet), popular datasets\n(Cifar10, Cifar100, miniImageNet, NEU, BSD, TEX), and different training\nsamples (3, 5, 10, 20, 50, 100 per class) show our methods effectiveness and\ngeneralization ability. Our method outperforms ensemble versions of the base\nmodels without feature distance loss, and the Class Activation Maps explicitly\nproves the ability to learn different discriminative feature concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiwen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noack_B/0/1/0/all/0/1\">Benjamin Noack</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Contrario multi-scale anomaly detection method for industrial quality inspection. (arXiv:2205.11611v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11611","description":"<p>Anomalies can be defined as any non-random structure which deviates from\nnormality. Anomaly detection methods reported in the literature are numerous\nand diverse, as what is considered anomalous usually varies depending on\nparticular scenarios and applications. In this work we propose an a contrario\nframework to detect anomalies in images applying statistical analysis to\nfeature maps obtained via convolutions. We evaluate filters learned from the\nimage under analysis via patch PCA, Gabor filters and the feature maps obtained\nfrom a pre-trained deep neural network (Resnet). The proposed method is\nmulti-scale and fully unsupervised and is able to detect anomalies in a wide\nvariety of scenarios. While the end goal of this work is the detection of\nsubtle defects in leather samples for the automotive industry, we show that the\nsame algorithm achieves state-of-the-art results in public anomalies datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tailanian_M/0/1/0/all/0/1\">Mat&#xed;as Tailanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muse_P/0/1/0/all/0/1\">Pablo Mus&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">&#xc1;lvaro Pardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransforMatcher: Match-to-Match Attention for Semantic Correspondence. (arXiv:2205.11634v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11634","description":"<p>Establishing correspondences between images remains a challenging task,\nespecially under large appearance changes due to different viewpoints or\nintra-class variations. In this work, we introduce a strong semantic image\nmatching learner, dubbed TransforMatcher, which builds on the success of\ntransformer networks in vision domains. Unlike existing convolution- or\nattention-based schemes for correspondence, TransforMatcher performs global\nmatch-to-match attention for precise match localization and dynamic refinement.\nTo handle a large number of matches in a dense correlation map, we develop a\nlight-weight attention architecture to consider the global match-to-match\ninteractions. We also propose to utilize a multi-channel correlation map for\nrefinement, treating the multi-level scores as features instead of a single\nscore to fully exploit the richer layer-wise semantics. In experiments,\nTransforMatcher sets a new state of the art on SPair-71k while performing on\npar with existing SOTA methods on the PF-PASCAL dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1\">Juhong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Model Generalization for Monocular 3D Object Detection. (arXiv:2205.11664v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11664","description":"<p>Monocular 3D object detection (Mono3D) has achieved tremendous improvements\nwith emerging large-scale autonomous driving datasets and the rapid development\nof deep learning techniques. However, caused by severe domain gaps (e.g., the\nfield of view (FOV), pixel size, and object size among datasets), Mono3D\ndetectors have difficulty in generalization, leading to drastic performance\ndegradation on unseen domains. To solve these issues, we combine the\nposition-invariant transform and multi-scale training with the pixel-size depth\nstrategy to construct an effective unified camera-generalized paradigm (CGP).\nIt fully considers discrepancies in the FOV and pixel size of images captured\nby different cameras. Moreover, we further investigate the obstacle in\nquantitative metrics when cross-dataset inference through an exhaustive\nsystematic study. We discern that the size bias of prediction leads to a\ncolossal failure. Hence, we propose the 2D-3D geometry-consistent object\nscaling strategy (GCOS) to bridge the gap via an instance-level augment. Our\nmethod called DGMono3D achieves remarkable performance on all evaluated\ndatasets and surpasses the SoTA unsupervised domain adaptation scheme even\nwithout utilizing data on the target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithm Development for Controlling Movement of a Robotic Platform by Digital Image Processing. (arXiv:2205.11666v1 [cs.RO])","link":"http://arxiv.org/abs/2205.11666","description":"<p>The following work shows an algorithm that can process images digitally with\nthe goal of control the movement of a mobile robotic platform in a certain\nenvironment. The platform is identified with a specific color, and displacement\nenvironment of the platform shift has identified obstacles with different\ncolors, for both cases it worked with the RGB color scale. To obtain the\ncontrol's movement of the robotic platform, the algorithm was developed in C\nprogramming language, and used the Open CV libraries for processing images\ncaptured by a video camera on the Dev-platform C + +. The video camera was\npreviously calibrated using ZHANG technique where parameters were obtained\nfocal length and tilt focal pixel. In the algorithm histogram analysis and\nsegmentation of the image were developed, allowing to determine exactly the\nrelative position of the platform with respect to the obstacles and movement\nstrategy to follow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zapata_B/0/1/0/all/0/1\">Benjamin Andres Huerfano Zapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_H/0/1/0/all/0/1\">Humberto Numpaque Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_C/0/1/0/all/0/1\">Cindy Lorena Diaz Murillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning multi-scale functional representations of proteins from single-cell microscopy data. (arXiv:2205.11676v1 [q-bio.QM])","link":"http://arxiv.org/abs/2205.11676","description":"<p>Protein function is inherently linked to its localization within the cell,\nand fluorescent microscopy data is an indispensable resource for learning\nrepresentations of proteins. Despite major developments in molecular\nrepresentation learning, extracting functional information from biological\nimages remains a non-trivial computational task. Current state-of-the-art\napproaches use autoencoder models to learn high-quality features by\nreconstructing images. However, such methods are prone to capturing noise and\nimaging artifacts. In this work, we revisit deep learning models used for\nclassifying major subcellular localizations, and evaluate representations\nextracted from their final layers. We show that simple convolutional networks\ntrained on localization classification can learn protein representations that\nencapsulate diverse functional information, and significantly outperform\nautoencoder-based models. We also propose a robust evaluation strategy to\nassess quality of protein representations across different scales of biological\nfunction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Razdaibiedina_A/0/1/0/all/0/1\">Anastasia Razdaibiedina</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brechalov_A/0/1/0/all/0/1\">Alexander Brechalov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization. (arXiv:2205.11686v1 [cs.CL])","link":"http://arxiv.org/abs/2205.11686","description":"<p>Integrating vision and language has gained notable attention following the\nsuccess of pretrained language models. Despite that, a fraction of emerging\nmultimodal models is suitable for text generation conditioned on images. This\nminority is typically developed and evaluated for image captioning, a text\ngeneration task conditioned solely on images with the goal to describe what is\nexplicitly visible in an image. In this paper, we take a step back and ask: How\ndo these models work for more complex generative tasks, conditioned on both\ntext and images? Are models based on joint multimodal pretraining, visually\nadapted pretrained language models, or models that combine these two\napproaches, more promising for such tasks? We address these questions in the\ncontext of self-rationalization (jointly generating task labels/answers and\nfree-text explanations) of three tasks: (i) visual question answering in VQA-X,\n(ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment\nin E-SNLI-VE. We show that recent advances in each modality, CLIP image\nrepresentations and scaling of language models, do not consistently improve\nmultimodal self-rationalization of tasks with multimodal inputs. We also\nobserve that no model type works universally the best across tasks/datasets and\nfinetuning data sizes. Our findings call for a backbone modelling approach that\ncan be built on to advance text generation from images and text beyond image\ncaptioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palaskar_S/0/1/0/all/0/1\">Shruti Palaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagia_A/0/1/0/all/0/1\">Akshita Bhagia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-Fashion: High-Fidelity Multi-modal Image Generation and Editing. (arXiv:2205.11705v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11705","description":"<p>The fashion industry has diverse applications in multi-modal image generation\nand editing. It aims to create a desired high-fidelity image with the\nmulti-modal conditional signal as guidance. Most existing methods learn\ndifferent condition guidance controls by introducing extra models or ignoring\nthe style prior knowledge, which is difficult to handle multiple signal\ncombinations and faces a low-fidelity problem. In this paper, we adapt both\nstyle prior knowledge and flexibility of multi-modal control into one unified\ntwo-stage framework, M6-Fashion, focusing on the practical AI-aided Fashion\ndesign. It decouples style codes in both spatial and semantic dimensions to\nguarantee high-fidelity image generation in the first stage. M6-Fashion\nutilizes self-correction for the non-autoregressive generation to improve\ninference speed, enhance holistic consistency, and support various signal\ncontrols. Extensive experiments on a large-scale clothing dataset M2C-Fashion\ndemonstrate superior performances on various image generation and editing\ntasks. M6-Fashion model serves as a highly potential AI designer for the\nfashion industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhikang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiling Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shuai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCVRL: Shuffled Contrastive Video Representation Learning. (arXiv:2205.11710v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11710","description":"<p>We propose SCVRL, a novel contrastive-based framework for self-supervised\nlearning for videos. Differently from previous contrast learning based methods\nthat mostly focus on learning visual semantics (e.g., CVRL), SCVRL is capable\nof learning both semantic and motion patterns. For that, we reformulate the\npopular shuffling pretext task within a modern contrastive learning paradigm.\nWe show that our transformer-based network has a natural capacity to learn\nmotion in self-supervised settings and achieves strong performance,\noutperforming CVRL on four benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dorkenwald_M/0/1/0/all/0/1\">Michael Dorkenwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fanyi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brattoli_B/0/1/0/all/0/1\">Biagio Brattoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Geometric Moment. (arXiv:2205.11722v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11722","description":"<p>Deep networks for image classification often rely more on texture information\nthan object shape. While efforts have been made to make deep-models\nshape-aware, it is often difficult to make such models simple, interpretable,\nor rooted in known mathematical definitions of shape. This paper presents a\ndeep-learning model inspired by geometric moments, a classically well\nunderstood approach to measure shape-related properties. The proposed method\nconsists of a trainable network for generating coordinate bases and affine\nparameters for making the features geometrically invariant, yet in a\ntask-specific manner. The proposed model improves the final feature's\ninterpretation. We demonstrate the effectiveness of our method on standard\nimage classification datasets. The proposed model achieves higher\nclassification performance as compared to the baseline and standard ResNet\nmodels while substantially improving interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajhans Singh</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Ankita Shukla</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan Turaga</a> (1) ((1) Arizona State University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images. (arXiv:2205.11733v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11733","description":"<p>This paper deals with the challenging task of synthesizing novel views for\nin-the-wild photographs. Existing methods have shown promising results\nleveraging monocular depth estimation and color inpainting with layered depth\nrepresentations. However, these methods still have limited capability to handle\nscenes with complex 3D geometry. We propose a new method based on the\nmultiplane image (MPI) representation. To accommodate diverse scene layouts in\nthe wild and tackle the difficulty in producing high-dimensional MPI contents,\nwe design a network structure that consists of two novel modules, one for plane\ndepth adjustment and another for depth-aware color prediction. The former\nadjusts the initial plane positions using the RGBD context feature and an\nattention mechanism. Given adjusted depth values, the latter predicts the color\nand density for each plane separately with proper inter-plane interactions\nachieved via a feature masking strategy. To train our method, we construct\nlarge-scale stereo training data using only unconstrained single-view image\ncollections by a simple yet effective warp-back strategy. The experiments on\nboth synthetic and real datasets demonstrate that our trained model works\nremarkably well and achieves state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yuxuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alleviating Robust Overfitting of Adversarial Training With Consistency Regularization. (arXiv:2205.11744v1 [cs.LG])","link":"http://arxiv.org/abs/2205.11744","description":"<p>Adversarial training (AT) has proven to be one of the most effective ways to\ndefend Deep Neural Networks (DNNs) against adversarial attacks. However, the\nphenomenon of robust overfitting, i.e., the robustness will drop sharply at a\ncertain stage, always exists during AT. It is of great importance to decrease\nthis robust generalization gap in order to obtain a robust model. In this\npaper, we present an in-depth study towards the robust overfitting from a new\nangle. We observe that consistency regularization, a popular technique in\nsemi-supervised learning, has a similar goal as AT and can be used to alleviate\nrobust overfitting. We empirically validate this observation, and find a\nmajority of prior solutions have implicit connections to consistency\nregularization. Motivated by this, we introduce a new AT solution, which\nintegrates the consistency regularization and Mean Teacher (MT) strategy into\nAT. Specifically, we introduce a teacher model, coming from the average weights\nof the student models over the training steps. Then we design a consistency\nloss function to make the prediction distribution of the student models over\nadversarial examples consistent with that of the teacher model over clean\nsamples. Experiments show that our proposed method can effectively alleviate\nrobust overfitting and improve the robustness of DNN models against common\nadversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shudong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haichang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UMSNet: An Universal Multi-sensor Network for Human Activity Recognition. (arXiv:2205.11756v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11756","description":"<p>Human activity recognition (HAR) based on multimodal sensors has become a\nrapidly growing branch of biometric recognition and artificial intelligence.\nHowever, how to fully mine multimodal time series data and effectively learn\naccurate behavioral features has always been a hot topic in this field.\nPractical applications also require a well-generalized framework that can\nquickly process a variety of raw sensor data and learn better feature\nrepresentations. This paper proposes a universal multi-sensor network (UMSNet)\nfor human activity recognition. In particular, we propose a new lightweight\nsensor residual block (called LSR block), which improves the performance by\nreducing the number of activation function and normalization layers, and adding\ninverted bottleneck structure and grouping convolution. Then, the Transformer\nis used to extract the relationship of series features to realize the\nclassification and recognition of human activities. Our framework has a clear\nstructure and can be directly applied to various types of multi-modal Time\nSeries Classification (TSC) tasks after simple specialization. Extensive\nexperiments show that the proposed UMSNet outperforms other state-of-the-art\nmethods on two popular multi-sensor human activity recognition datasets (i.e.\nHHAR dataset and MHEALTH dataset).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Haotian Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNet#: A UNet-like Redesigning Skip Connections for Medical Image Segmentation. (arXiv:2205.11759v1 [eess.IV])","link":"http://arxiv.org/abs/2205.11759","description":"<p>As an essential prerequisite for developing a medical intelligent assistant\nsystem, medical image segmentation has received extensive research and\nconcentration from the neural network community. A series of UNet-like networks\nwith encoder-decoder architecture has achieved extraordinary success, in which\nUNet2+ and UNet3+ redesign skip connections, respectively proposing dense skip\nconnection and full-scale skip connection and dramatically improving compared\nwith UNet in medical image segmentation. However, UNet2+ lacks sufficient\ninformation explored from the full scale, which will affect the learning of\norgans' location and boundary. Although UNet3+ can obtain the full-scale\naggregation feature map, owing to the small number of neurons in the structure,\nit does not satisfy the segmentation of tiny objects when the number of samples\nis small. This paper proposes a novel network structure combining dense skip\nconnections and full-scale skip connections, named UNet-sharp (UNet\\#) for its\nshape similar to symbol \\#. The proposed UNet\\# can aggregate feature maps of\ndifferent scales in the decoder sub-network and capture fine-grained details\nand coarse-grained semantics from the full scale, which benefits learning the\nexact location and accurately segmenting the boundary of organs or lesions. We\nperform deep supervision for model pruning to speed up testing and make it\npossible for the model to run on mobile devices; furthermore, designing two\nclassification-guided modules to reduce false positives achieves more accurate\nsegmentation results. Various experiments of semantic segmentation and instance\nsegmentation on different modalities (EM, CT, MRI) and dimensions (2D, 3D)\ndatasets, including the nuclei, brain tumor, liver, and lung, demonstrate that\nthe proposed method outperforms state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1\">Ledan Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Z/0/1/0/all/0/1\">Zhongyi Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ranking-Based Siamese Visual Tracking. (arXiv:2205.11761v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11761","description":"<p>Current Siamese-based trackers mainly formulate the visual tracking into two\nindependent subtasks, including classification and localization. They learn the\nclassification subnetwork by processing each sample separately and neglect the\nrelationship among positive and negative samples. Moreover, such tracking\nparadigm takes only the classification confidence of proposals for the final\nprediction, which may yield the misalignment between classification and\nlocalization. To resolve these issues, this paper proposes a ranking-based\noptimization algorithm to explore the relationship among different proposals.\nTo this end, we introduce two ranking losses, including the classification one\nand the IoU-guided one, as optimization constraints. The classification ranking\nloss can ensure that positive samples rank higher than hard negative ones,\ni.e., distractors, so that the trackers can select the foreground samples\nsuccessfully without being fooled by the distractors. The IoU-guided ranking\nloss aims to align classification confidence scores with the Intersection over\nUnion(IoU) of the corresponding localization prediction for positive samples,\nenabling the well-localized prediction to be represented by high classification\nconfidence. Specifically, the proposed two ranking losses are compatible with\nmost Siamese trackers and incur no additional computation for inference.\nExtensive experiments on seven tracking benchmarks, including OTB100, UAV123,\nTC128, VOT2016, NFS30, GOT-10k and LaSOT, demonstrate the effectiveness of the\nproposed ranking-based optimization algorithm. The code and raw results are\navailable at https://github.com/sansanfree/RBO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Feng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1\">Qiang Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Augmentation for Efficient Visual Representation Learning for Self-supervised Pre-training. (arXiv:2205.11772v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11772","description":"<p>In recent years, self-supervised learning has been studied to deal with the\nlimitation of available labeled-dataset. Among the major components of\nself-supervised learning, the data augmentation pipeline is one key factor in\nenhancing the resulting performance. However, most researchers manually\ndesigned the augmentation pipeline, and the limited collections of\ntransformation may cause the lack of robustness of the learned feature\nrepresentation. In this work, we proposed Multi-Augmentations for\nSelf-Supervised Representation Learning (MA-SSRL), which fully searched for\nvarious augmentation policies to build the entire pipeline to improve the\nrobustness of the learned feature representation. MA-SSRL successfully learns\nthe invariant feature representation and presents an efficient, effective, and\nadaptable data augmentation pipeline for self-supervised pre-training on\ndifferent distribution and domain datasets. MA-SSRL outperforms the previous\nstate-of-the-art methods on transfer and semi-supervised benchmarks while\nrequiring fewer training epochs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Van-Nhiem Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chi-En Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shen-Hsuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kai-Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Timothy Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yung-Hui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFNet-M: Adaptive Fusion Network with Masks for 2D+3D Facial Expression Recognition. (arXiv:2205.11785v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11785","description":"<p>2D+3D facial expression recognition (FER) can effectively cope with\nillumination changes and pose variations by simultaneously merging 2D texture\nand more robust 3D depth information. Most deep learning-based approaches\nemploy the simple fusion strategy that concatenates the multimodal features\ndirectly after fully-connected layers, without considering the different\ndegrees of significance for each modality. Meanwhile, how to focus on both 2D\nand 3D local features in salient regions is still a great challenge. In this\nletter, we propose the adaptive fusion network with masks (AFNet-M) for 2D+3D\nFER. To enhance 2D and 3D local features, we take the masks annotating salient\nregions of the face as prior knowledge and design the mask attention module\n(MA) which can automatically learn two modulation vectors to adjust the feature\nmaps. Moreover, we introduce a novel fusion strategy that can perform adaptive\nfusion at convolutional layers through the designed importance weights\ncomputing module (IWC). Experimental results demonstrate that our AFNet-M\nachieves the state-of-the-art performance on BU-3DFE and Bosphorus datasets and\nrequires fewer parameters in comparison with other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_M/0/1/0/all/0/1\">Mingzhe Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaoqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"G-Rep: Gaussian Representation for Arbitrary-Oriented Object Detection. (arXiv:2205.11796v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11796","description":"<p>Arbitrary-oriented object representations contain the oriented bounding box\n(OBB), quadrilateral bounding box (QBB), and point set (PointSet). Each\nrepresentation encounters problems that correspond to its characteristics, such\nas the boundary discontinuity, square-like problem, representation ambiguity,\nand isolated points, which lead to inaccurate detection. Although many\neffective strategies have been proposed for various representations, there is\nstill no unified solution. Current detection methods based on Gaussian modeling\nhave demonstrated the possibility of breaking this dilemma; however, they\nremain limited to OBB. To go further, in this paper, we propose a unified\nGaussian representation called G-Rep to construct Gaussian distributions for\nOBB, QBB, and PointSet, which achieves a unified solution to various\nrepresentations and problems. Specifically, PointSet or QBB-based objects are\nconverted into Gaussian distributions, and their parameters are optimized using\nthe maximum likelihood estimation algorithm. Then, three optional Gaussian\nmetrics are explored to optimize the regression loss of the detector because of\ntheir excellent parameter optimization mechanisms. Furthermore, we also use\nGaussian metrics for sampling to align label assignment and regression loss.\nExperimental results on several public available datasets, DOTA, HRSC2016,\nUCAS-AOD, and ICDAR2015 show the excellent performance of the proposed method\nfor arbitrary-oriented object detection. The code has been open sourced at\nhttps://github.com/open-mmlab/mmrotate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liping Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Ke Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Expression Transformer: A Computer Vision Approach for Symbolic Regression. (arXiv:2205.11798v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11798","description":"<p>Symbolic Regression (SR) is a type of regression analysis to automatically\nfind the mathematical expression that best fits the data. Currently, SR still\nbasically relies on various searching strategies so that a sample-specific\nmodel is required to be optimized for every expression, which significantly\nlimits the model's generalization and efficiency. Inspired by the fact that\nhuman beings can infer a mathematical expression based on the curve of it, we\npropose Symbolic Expression Transformer (SET), a sample-agnostic model from the\nperspective of computer vision for SR. Specifically, the collected data is\nrepresented as images and an image caption model is employed for translating\nimages to symbolic expressions. A large-scale dataset without overlap between\ntraining and testing sets in the image domain is released. Our results\ndemonstrate the effectiveness of SET and suggest the promising direction of\nimage-based model for solving the challenging SR problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hong-Bin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Package Theft Detection from Smart Home Security Cameras. (arXiv:2205.11804v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11804","description":"<p>Package theft detection has been a challenging task mainly due to lack of\ntraining data and a wide variety of package theft cases in reality. In this\npaper, we propose a new Global and Local Fusion Package Theft Detection\nEmbedding (GLF-PTDE) framework to generate package theft scores for each\nsegment within a video to fulfill the real-world requirements on package theft\ndetection. Moreover, we construct a novel Package Theft Detection dataset to\nfacilitate the research on this task. Our method achieves 80% AUC performance\non the newly proposed dataset, showing the effectiveness of the proposed\nGLF-PTDE framework and its robustness in different real scenes for package\ntheft detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xinyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Baohua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhongwei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Assemble Geometric Shapes. (arXiv:2205.11809v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11809","description":"<p>Assembling parts into an object is a combinatorial problem that arises in a\nvariety of contexts in the real world and involves numerous applications in\nscience and engineering. Previous related work tackles limited cases with\nidentical unit parts or jigsaw-style parts of textured shapes, which greatly\nmitigate combinatorial challenges of the problem. In this work, we introduce\nthe more challenging problem of shape assembly, which involves textureless\nfragments of arbitrary shapes with indistinctive junctions, and then propose a\nlearning-based approach to solving it. We demonstrate the effectiveness on\nshape assembly tasks with various scenarios, including the ones with abnormal\nfragments (e.g., missing and distorted), the different number of fragments, and\ndifferent rotation discretization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhwi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungtaek Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunsoo Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thunder: Thumbnail based Fast Lightweight Image Denoising Network. (arXiv:2205.11823v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11823","description":"<p>To achieve promising results on removing noise from real-world images, most\nof existing denoising networks are formulated with complex network structure,\nmaking them impractical for deployment. Some attempts focused on reducing the\nnumber of filters and feature channels but suffered from large performance\nloss, and a more practical and lightweight denoising network with fast\ninference speed is of high demand.\n</p>\n<p>To this end, a \\textbf{Thu}mb\\textbf{n}ail based \\textbf{D}\\textbf{e}noising\nNetwo\\textbf{r}k dubbed Thunder, is proposed and implemented as a lightweight\nstructure for fast restoration without comprising the denoising capabilities.\nSpecifically, the Thunder model contains two newly-established modules:\n</p>\n<p>(1) a wavelet-based Thumbnail Subspace Encoder (TSE) which can leverage\nsub-bands correlation to provide an approximate thumbnail based on the\nlow-frequent feature; (2) a Subspace Projection based Refine Module (SPR) which\ncan restore the details for thumbnail progressively based on the subspace\nprojection approach.\n</p>\n<p>Extensive experiments have been carried out on two real-world denoising\nbenchmarks, demonstrating that the proposed Thunder outperforms the existing\nlightweight models and achieves competitive performance on PSNR and SSIM when\ncompared with the complex designs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yifeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huimin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Difference Learning for Noisy Rigid Image Alignment. (arXiv:2205.11829v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11829","description":"<p>Rigid image alignment is a fundamental task in computer vision, while the\ntraditional algorithms are either too sensitive to noise or time-consuming.\nRecent unsupervised image alignment methods developed based on spatial\ntransformer networks show an improved performance on clean images but will not\nachieve satisfactory performance on noisy images due to its heavy reliance on\npixel value comparations. To handle such challenging applications, we report a\nnew unsupervised difference learning (UDL) strategy and apply it to rigid image\nalignment. UDL exploits the quantitative properties of regression tasks and\nconverts the original unsupervised problem to pseudo supervised problem. Under\nthe new UDL-based image alignment pipeline, rotation can be accurately\nestimated on both clean and noisy images and translations can then be easily\nsolved. Experimental results on both nature and cryo-EM images demonstrate the\nefficacy of our UDL-based unsupervised rigid image alignment method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Xuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dagan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hong-Bin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TraCon: A novel dataset for real-time traffic cones detection using deep learning. (arXiv:2205.11830v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11830","description":"<p>Substantial progress has been made in the field of object detection in road\nscenes. However, it is mainly focused on vehicles and pedestrians. To this end,\nwe investigate traffic cone detection, an object category crucial for road\neffects and maintenance. In this work, the YOLOv5 algorithm is employed, in\norder to find a solution for the efficient and fast detection of traffic cones.\nThe YOLOv5 can achieve a high detection accuracy with the score of IoU up to\n91.31%. The proposed method is been applied to an RGB roadwork image dataset,\ncollected from various sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katsamenis_I/0/1/0/all/0/1\">Iason Katsamenis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karolou_E/0/1/0/all/0/1\">Eleni Eirini Karolou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davradou_A/0/1/0/all/0/1\">Agapi Davradou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_A/0/1/0/all/0/1\">Anastasios Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogeras_D/0/1/0/all/0/1\">Dimitris Kalogeras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDFKD-MFS: Collaborative Data-free Knowledge Distillation via Multi-level Feature Sharing. (arXiv:2205.11845v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11845","description":"<p>Recently, the compression and deployment of powerful deep neural networks\n(DNNs) on resource-limited edge devices to provide intelligent services have\nbecome attractive tasks. Although knowledge distillation (KD) is a feasible\nsolution for compression, its requirement on the original dataset raises\nprivacy concerns. In addition, it is common to integrate multiple pretrained\nmodels to achieve satisfactory performance. How to compress multiple models\ninto a tiny model is challenging, especially when the original data are\nunavailable. To tackle this challenge, we propose a framework termed\ncollaborative data-free knowledge distillation via multi-level feature sharing\n(CDFKD-MFS), which consists of a multi-header student module, an asymmetric\nadversarial data-free KD module, and an attention-based aggregation module. In\nthis framework, the student model equipped with a multi-level feature-sharing\nstructure learns from multiple teacher models and is trained together with a\ngenerator in an asymmetric adversarial manner. When some real samples are\navailable, the attention module adaptively aggregates predictions of the\nstudent headers, which can further improve performance. We conduct extensive\nexperiments on three popular computer visual datasets. In particular, compared\nwith the most competitive alternative, the accuracy of the proposed framework\nis 1.18\\% higher on the CIFAR-100 dataset, 1.67\\% higher on the Caltech-101\ndataset, and 2.99\\% higher on the mini-ImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhiwei Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jianping An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative 3D Object Detection for Automatic Vehicle Systems via Learnable Communications. (arXiv:2205.11849v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11849","description":"<p>Accurate detection of objects in 3D point clouds is a key problem in\nautonomous driving systems. Collaborative perception can incorporate\ninformation from spatially diverse sensors and provide significant benefits for\nimproving the perception accuracy of autonomous driving systems. In this work,\nwe consider that the autonomous vehicle uses local point cloud data and\ncombines information from neighboring infrastructures through wireless links\nfor cooperative 3D object detection. However, information sharing among vehicle\nand infrastructures in predefined communication schemes may result in\ncommunication congestion and/or bring limited performance improvement. To this\nend, we propose a novel collaborative 3D object detection framework that\nconsists of three components: feature learning networks that map point clouds\ninto feature maps; an efficient communication block that propagates compact and\nfine-grained query feature maps from vehicle to support infrastructures and\noptimizes attention weights between query and key to refine support feature\nmaps; a region proposal network that fuses local feature maps and weighted\nsupport feature maps for 3D object detection. We evaluate the performance of\nthe proposed framework using a synthetic cooperative dataset created in two\ncomplex driving scenarios: a roundabout and a T-junction. Experiment results\nand bandwidth usage analysis demonstrate that our approach can save\ncommunication and computation costs and significantly improve detection\nperformance under different detection difficulties in all scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yi Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration. (arXiv:2205.11876v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11876","description":"<p>Recent learning-based image fusion methods have marked numerous progress in\npre-registered multi-modality data, but suffered serious ghosts dealing with\nmisaligned multi-modality data, due to the spatial deformation and the\ndifficulty narrowing cross-modality discrepancy. To overcome the obstacles, in\nthis paper, we present a robust cross-modality generation-registration paradigm\nfor unsupervised misaligned infrared and visible image fusion (IVIF).\nSpecifically, we propose a Cross-modality Perceptual Style Transfer Network\n(CPSTN) to generate a pseudo infrared image taking a visible image as input.\nBenefiting from the favorable geometry preservation ability of the CPSTN, the\ngenerated pseudo infrared image embraces a sharp structure, which is more\nconducive to transforming cross-modality image alignment into mono-modality\nregistration coupled with the structure-sensitive of the infrared image. In\nthis case, we introduce a Multi-level Refinement Registration Network (MRRN) to\npredict the displacement vector field between distorted and pseudo infrared\nimages and reconstruct registered infrared image under the mono-modality\nsetting. Moreover, to better fuse the registered infrared images and visible\nimages, we present a feature Interaction Fusion Module (IFM) to adaptively\nselect more meaningful features for fusion in the Dual-path Interaction Fusion\nNetwork (DIFN). Extensive experimental results suggest that the proposed method\nperforms superior capability on misaligned cross-modality image fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Vectorization for Portrait Images. (arXiv:2205.11880v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11880","description":"<p>Aiming at developing intuitive and easy-to-use portrait editing tools, we\npropose a novel vectorization method that can automatically convert raster\nimages into a 3-tier hierarchical representation. The base layer consists of a\nset of sparse diffusion curves (DC) which characterize salient geometric\nfeatures and low-frequency colors and provide means for semantic color transfer\nand facial expression editing. The middle level encodes specular highlights and\nshadows to large and editable Poisson regions (PR) and allows the user to\ndirectly adjust illumination via tuning the strength and/or changing shape of\nPR. The top level contains two types of pixel-sized PRs for high-frequency\nresiduals and fine details such as pimples and pigmentation. We also train a\ndeep generative model that can produce high-frequency residuals automatically.\nThanks to the meaningful organization of vector primitives, editing portraits\nbecomes easy and intuitive. In particular, our method supports color transfer,\nfacial expression editing, highlight and shadow editing and automatic\nretouching. Thanks to the linearity of the Laplace operator, we introduce alpha\nblending, linear dodge and linear burn to vector editing and show that they are\neffective in editing highlights and shadows. To quantitatively evaluate the\nresults, we extend the commonly used FLIP metric (which measures differences\nbetween two images) by considering illumination. The new metric, called\nillumination-sensitive FLIP or IS-FLIP, can effectively capture the salient\nchanges in color transfer results, and is more consistent with human perception\nthan FLIP and other quality measures on portrait images. We evaluate our method\non the FFHQR dataset and show that our method is effective for common portrait\nediting tasks, such as retouching, light editing, color transfer and expression\nediting. We will make the code and trained models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Fei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind The Gap: Alleviating Local Imbalance for Unsupervised Cross-Modality Medical Image Segmentation. (arXiv:2205.11888v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11888","description":"<p>Unsupervised cross-modality medical image adaptation aims to alleviate the\nsevere domain gap between different imaging modalities without using the target\ndomain label. A key in this campaign relies upon aligning the distributions of\nsource and target domain. One common attempt is to enforce the global alignment\nbetween two domains, which, however, ignores the fatal local-imbalance domain\ngap problem, i.e., some local features with larger domain gap are harder to\ntransfer. Recently, some methods conduct alignment focusing on local regions to\nimprove the efficiency of model learning. While this operation may cause a\ndeficiency of critical information from contexts. To tackle this limitation, we\npropose a novel strategy to alleviate the domain gap imbalance considering the\ncharacteristics of medical images, namely Global-Local Union Alignment.\nSpecifically, a feature-disentanglement style-transfer module first synthesizes\nthe target-like source-content images to reduce the global domain gap. Then, a\nlocal feature mask is integrated to reduce the 'inter-gap' for local features\nby prioritizing those discriminative features with larger domain gap. This\ncombination of global and local alignment can precisely localize the crucial\nregions in segmentation target while preserving the overall semantic\nconsistency. We conduct a series of experiments with two cross-modality\nadaptation tasks, i,e. cardiac substructure and abdominal multi-organ\nsegmentation. Experimental results indicate that our method exceeds the SOTA\nmethods by 3.92% Dice score in MRI-CT cardiac segmentation and 3.33% in the\nreverse direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zixian Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiufeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuyao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An interpretation of the final fully connected layer. (arXiv:2205.11908v1 [cs.LG])","link":"http://arxiv.org/abs/2205.11908","description":"<p>In recent years neural networks have achieved state-of-the-art accuracy for\nvarious tasks but the the interpretation of the generated outputs still remains\ndifficult. In this work we attempt to provide a method to understand the learnt\nweights in the final fully connected layer in image classification models. We\nmotivate our method by drawing a connection between the policy gradient\nobjective in RL and supervised learning objective. We suggest that the commonly\nused cross entropy based supervised learning objective can be regarded as a\nspecial case of the policy gradient objective. Using this insight we propose a\nmethod to find the most discriminative and confusing parts of an image. Our\nmethod does not make any prior assumption about neural network achitecture and\nhas low computational cost. We apply our method on publicly available\npre-trained models and report the generated results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddhartha/0/1/0/all/0/1\">Siddhartha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust 3D Object Detection in Cold Weather Conditions. (arXiv:2205.11925v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11925","description":"<p>Adverse weather conditions can negatively affect LiDAR-based object\ndetectors. In this work, we focus on the phenomenon of vehicle gas exhaust\ncondensation in cold weather conditions. This everyday effect can influence the\nestimation of object sizes, orientations and introduce ghost object detections,\ncompromising the reliability of the state of the art object detectors. We\npropose to solve this problem by using data augmentation and a novel training\nloss term. To effectively train deep neural networks, a large set of labeled\ndata is needed. In case of adverse weather conditions, this process can be\nextremely laborious and expensive. We address this issue in two steps: First,\nwe present a gas exhaust data generation method based on 3D surface\nreconstruction and sampling which allows us to generate large sets of gas\nexhaust clouds from a small pool of labeled data. Second, we introduce a point\ncloud augmentation process that can be used to add gas exhaust to datasets\nrecorded in good weather conditions. Finally, we formulate a new training loss\nterm that leverages the augmented point cloud to increase object detection\nrobustness by penalizing predictions that include noise. In contrast to other\nworks, our method can be used with both grid-based and point-based detectors.\nMoreover, since our approach does not require any network architecture changes,\ninference times remain unchanged. Experimental results on real data show that\nour proposed method greatly increases robustness to gas exhaust and noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piroli_A/0/1/0/all/0/1\">Aldi Piroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1\">Vinzenz Dallabetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walessa_M/0/1/0/all/0/1\">Marc Walessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meissner_D/0/1/0/all/0/1\">Daniel Meissner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_J/0/1/0/all/0/1\">Johannes Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Trinarization Using a Partial Differential Equations: A Novel Approach to Automatic Sperm Image Analysis. (arXiv:2205.11927v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11927","description":"<p>Partial differential equations have recently garnered substantial attention\nas an image processing framework due to their extensibility, the ability to\nrigorously engineer and analyse the governing dynamics as well as the ease of\nimplementation using numerical methods. This paper explores a novel approach to\nimage trinarization with a concrete real-world application of classifying\nregions of sperm images used in the automatic analysis of sperm morphology. The\nproposed methodology engineers a diffusion equation with non-linear source\nterm, exhibiting three steady-states. The model is implemented as an image\nprocessor using a standard finite difference method to illustrate the efficacy\nof the proposed approach. The performance of the proposed approach is\nbenchmarked against standard image clustering/segmentation methods and shown to\nbe highly effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_B/0/1/0/all/0/1\">B. A. Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraSens: A Gabor Residual Anti-aliasing Sensing Framework for Action Recognition using WiFi. (arXiv:2205.11945v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11945","description":"<p>WiFi-based human action recognition (HAR) has been regarded as a promising\nsolution in applications such as smart living and remote monitoring due to the\npervasive and unobtrusive nature of WiFi signals. However, the efficacy of WiFi\nsignals is prone to be influenced by the change in the ambient environment and\nvaries over different sub-carriers. To remedy this issue, we propose an\nend-to-end Gabor residual anti-aliasing sensing network (GraSens) to directly\nrecognize the actions using the WiFi signals from the wireless devices in\ndiverse scenarios. In particular, a new Gabor residual block is designed to\naddress the impact of the changing surrounding environment with a focus on\nlearning reliable and robust temporal-frequency representations of WiFi\nsignals. In each block, the Gabor layer is integrated with the anti-aliasing\nlayer in a residual manner to gain the shift-invariant features. Furthermore,\nfractal temporal and frequency self-attention are proposed in a joint effort to\nexplicitly concentrate on the efficacy of WiFi signals and thus enhance the\nquality of output features scattered in different subcarriers. Experimental\nresults throughout our wireless-vision action recognition dataset (WVAR) and\nthree public datasets demonstrate that our proposed GraSens scheme outperforms\nstate-of-the-art methods with respect to recognition accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanling Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhiyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1\">Xidong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHARP: Shape-Aware Reconstruction of People in Loose Clothing. (arXiv:2205.11948v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11948","description":"<p>Recent advancements in deep learning have enabled 3D human body\nreconstruction from a monocular image, which has broad applications in multiple\ndomains. In this paper, we propose SHARP (SHape Aware Reconstruction of People\nin loose clothing), a novel end-to-end trainable network that accurately\nrecovers the 3D geometry and appearance of humans in loose clothing from a\nmonocular image. SHARP uses a sparse and efficient fusion strategy to combine\nparametric body prior with a non-parametric 2D representation of clothed\nhumans. The parametric body prior enforces geometrical consistency on the body\nshape and pose, while the non-parametric representation models loose clothing\nand handle self-occlusions as well. We also leverage the sparseness of the\nnon-parametric representation for faster training of our network while using\nlosses on 2D maps. Another key contribution is 3DHumans, our new life-like\ndataset of 3D human body scans with rich geometrical and textural details. We\nevaluate SHARP on 3DHumans and other publicly available datasets and show\nsuperior qualitative and quantitative performance than existing\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jinka_S/0/1/0/all/0/1\">Sai Sagar Jinka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Astitva Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokhariya_C/0/1/0/all/0/1\">Chandradeep Pokhariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Avinash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1\">P.J. Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffuse Map Guiding Unsupervised Generative Adversarial Network for SVBRDF Estimation. (arXiv:2205.11951v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11951","description":"<p>Reconstructing materials in the real world has always been a difficult\nproblem in computer graphics. Accurately reconstructing the material in the\nreal world is critical in the field of realistic rendering. Traditionally,\nmaterials in computer graphics are mapped by an artist, then mapped onto a\ngeometric model by coordinate transformation, and finally rendered with a\nrendering engine to get realistic materials. For opaque objects, the industry\ncommonly uses physical-based bidirectional reflectance distribution function\n(BRDF) rendering models for material modeling. The commonly used physical-based\nrendering models are Cook-Torrance BRDF, Disney BRDF. In this paper, we use the\nCook-Torrance model to reconstruct the materials. The SVBRDF material\nparameters include Normal, Diffuse, Specular and Roughness. This paper presents\na Diffuse map guiding material estimation method based on the Generative\nAdversarial Network(GAN). This method can predict plausible SVBRDF maps with\nglobal features using only a few pictures taken by the mobile phone. The main\ncontributions of this paper are: 1) We preprocess a small number of input\npictures to produce a large number of non-repeating pictures for training to\nreduce over-fitting. 2) We use a novel method to directly obtain the guessed\ndiffuse map with global characteristics, which provides more prior information\nfor the training process. 3) We improve the network architecture of the\ngenerator so that it can generate fine details of normal maps and reduce the\npossibility to generate over-flat normal maps. The method used in this paper\ncan obtain prior knowledge without using dataset training, which greatly\nreduces the difficulty of material reconstruction and saves a lot of time to\ngenerate and calibrate datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiyao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongnan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D helical CT reconstruction with memory efficient invertible Learned Primal-Dual method. (arXiv:2205.11952v1 [eess.IV])","link":"http://arxiv.org/abs/2205.11952","description":"<p>Helical acquisition geometry is the most common geometry used in computed\ntomography (CT) scanners for medical imaging. We adapt the invertible Learned\nPrimal-Dual (iLPD) deep neural network architecture so that it can be applied\nto helical 3D CT reconstruction. We achieve this by splitting the geometry and\nthe data in parts that fit the memory and by splitting images into\ncorresponding sub-volumes. The architecture can be applied to images different\nin size along the rotation axis. We perform the experiments on tomographic data\nsimulated from realistic helical geometries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bajic_B/0/1/0/all/0/1\">Buda Baji&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oktem_O/0/1/0/all/0/1\">Ozan &#xd6;ktem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudzusika_J/0/1/0/all/0/1\">Jevgenija Rudzusika</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Wireless-Vision Dataset for Privacy Preserving Human Activity Recognition. (arXiv:2205.11962v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11962","description":"<p>Human Activity Recognition (HAR) has recently received remarkable attention\nin numerous applications such as assisted living and remote monitoring.\nExisting solutions based on sensors and vision technologies have obtained\nachievements but still suffering from considerable limitations in the\nenvironmental requirement. Wireless signals like WiFi-based sensing have\nemerged as a new paradigm since it is convenient and not restricted in the\nenvironment. In this paper, a new WiFi-based and video-based neural network\n(WiNN) is proposed to improve the robustness of activity recognition where the\nsynchronized video serves as the supplement for the wireless data. Moreover, a\nwireless-vision benchmark (WiVi) is collected for 9 class actions recognition\nin three different visual conditions, including the scenes without occlusion,\nwith partial occlusion, and with full occlusion. Both machine learning methods\n- support vector machine (SVM) as well as deep learning methods are used for\nthe accuracy verification of the data set. Our results show that WiVi data set\nsatisfies the primary demand and all three branches in the proposed pipeline\nkeep more than $80\\%$ of activity recognition accuracy over multiple action\nsegmentation from 1s to 3s. In particular, WiNN is the most robust method in\nterms of all the actions on three action segmentation compared to the others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanling Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhiyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuanwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Models for Reproducible Coronary Calcium Scoring. (arXiv:2205.11967v1 [eess.IV])","link":"http://arxiv.org/abs/2205.11967","description":"<p>Purpose: Coronary artery calcium (CAC) score, i.e. the amount of CAC\nquantified in CT, is a strong and independent predictor of coronary heart\ndisease (CHD) events. However, CAC scoring suffers from limited interscan\nreproducibility, which is mainly due to the clinical definition requiring\napplication of a fixed intensity level threshold for segmentation of\ncalcifications. This limitation is especially pronounced in\nnon-ECG-synchronized CT where lesions are more impacted by cardiac motion and\npartial volume effects. Therefore, we propose a CAC quantification method that\ndoes not require a threshold for segmentation of CAC. Approach: Our method\nutilizes a generative adversarial network where a CT with CAC is decomposed\ninto an image without CAC and an image showing only CAC. The method, using a\nCycleGAN, was trained using 626 low-dose chest CTs and 514 radiotherapy\ntreatment planning CTs. Interscan reproducibility was compared to clinical\ncalcium scoring in radiotherapy treatment planning CTs of 1,662 patients, each\nhaving two scans. Results: A lower relative interscan difference in CAC mass\nwas achieved by the proposed method: 47% compared to 89% manual clinical\ncalcium scoring. The intraclass correlation coefficient of Agatston scores was\n0.96 for the proposed method compared to 0.91 for automatic clinical calcium\nscoring. Conclusions: The increased interscan reproducibility achieved by our\nmethod may lead to increased reliability of CHD risk categorization and\nimproved accuracy of CHD event prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Velzen_S/0/1/0/all/0/1\">Sanne G.M. van Velzen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vos_B/0/1/0/all/0/1\">Bob D. de Vos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noothout_J/0/1/0/all/0/1\">Julia M.H. Noothout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verkooijen_H/0/1/0/all/0/1\">Helena M. Verkooijen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viergever_M/0/1/0/all/0/1\">Max A. Viergever</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isgum_I/0/1/0/all/0/1\">Ivana I&#x161;gum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPOM: Customized Invisible Cloak towards Face Privacy Protection. (arXiv:2205.11981v1 [cs.CV])","link":"http://arxiv.org/abs/2205.11981","description":"<p>While convenient in daily life, face recognition technologies also raise\nprivacy concerns for regular users on the social media since they could be used\nto analyze face images and videos, efficiently and surreptitiously without any\nsecurity restrictions. In this paper, we investigate the face privacy\nprotection from a technology standpoint based on a new type of customized\ncloak, which can be applied to all the images of a regular user, to prevent\nmalicious face recognition systems from uncovering their identity.\nSpecifically, we propose a new method, named one person one mask (OPOM), to\ngenerate person-specific (class-wise) universal masks by optimizing each\ntraining sample in the direction away from the feature subspace of the source\nidentity. To make full use of the limited training images, we investigate\nseveral modeling methods, including affine hulls, class centers, and convex\nhulls, to obtain a better description of the feature subspace of source\nidentities. The effectiveness of the proposed method is evaluated on both\ncommon and celebrity datasets against black-box face recognition models with\ndifferent loss functions and network architectures. In addition, we discuss the\nadvantages and potential problems of the proposed method. In particular, we\nconduct an application study on the privacy protection of a video dataset,\nSherlock, to demonstrate the potential practical usage of the proposed method.\nDatasets and code are available at https://github.com/zhongyy/OPOM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yaoyao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections. (arXiv:2205.12005v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12005","description":"<p>Large-scale pretrained foundation models have been an emerging paradigm for\nbuilding artificial intelligence (AI) systems, which can be quickly adapted to\na wide range of downstream tasks. This paper presents mPLUG, a new\nvision-language foundation model for both cross-modal understanding and\ngeneration. Most existing pre-trained models suffer from the problems of low\ncomputational efficiency and information asymmetry brought by the long visual\nsequence in cross-modal alignment. To address these problems, mPLUG introduces\nan effective and efficient vision-language architecture with novel cross-modal\nskip-connections, which creates inter-layer shortcuts that skip a certain\nnumber of layers for time-consuming full self-attention on the vision side.\nmPLUG is pre-trained end-to-end on large-scale image-text pairs with both\ndiscriminative and generative objectives. It achieves state-of-the-art results\non a wide range of vision-language downstream tasks, such as image captioning,\nimage-text retrieval, visual grounding and visual question answering. mPLUG\nalso demonstrates strong zero-shot transferability when directly transferred to\nmultiple video-language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1\">Bin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hehong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guohai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SFace: Sigmoid-Constrained Hypersphere Loss for Robust Face Recognition. (arXiv:2205.12010v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12010","description":"<p>Deep face recognition has achieved great success due to large-scale training\ndatabases and rapidly developing loss functions. The existing algorithms devote\nto realizing an ideal idea: minimizing the intra-class distance and maximizing\nthe inter-class distance. However, they may neglect that there are also low\nquality training images which should not be optimized in this strict way.\nConsidering the imperfection of training databases, we propose that intra-class\nand inter-class objectives can be optimized in a moderate way to mitigate\noverfitting problem, and further propose a novel loss function, named\nsigmoid-constrained hypersphere loss (SFace). Specifically, SFace imposes\nintra-class and inter-class constraints on a hypersphere manifold, which are\ncontrolled by two sigmoid gradient re-scale functions respectively. The sigmoid\ncurves precisely re-scale the intra-class and inter-class gradients so that\ntraining samples can be optimized to some degree. Therefore, SFace can make a\nbetter balance between decreasing the intra-class distances for clean examples\nand preventing overfitting to the label noise, and contributes more robust deep\nface recognition models. Extensive experiments of models trained on\nCASIA-WebFace, VGGFace2, and MS-Celeb-1M databases, and evaluated on several\nface recognition benchmarks, such as LFW, MegaFace and IJB-C databases, have\ndemonstrated the superiority of SFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yaoyao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiani Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_D/0/1/0/all/0/1\">Dongchao Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Naive Few-Shot Learning: Sequence Consistency Evaluation. (arXiv:2205.12013v1 [cs.AI])","link":"http://arxiv.org/abs/2205.12013","description":"<p>Cognitive psychologists often use the term $\\textit{fluid intelligence}$ to\ndescribe the ability of humans to solve novel tasks without any prior training.\nIn contrast to humans, deep neural networks can perform cognitive tasks only\nafter extensive (pre-)training with a large number of relevant examples.\nMotivated by fluid intelligence research in the cognitive sciences, we built a\nbenchmark task which we call sequence consistency evaluation (SCE) that can be\nused to address this gap. Solving the SCE task requires the ability to extract\nsimple rules from sequences, a basic computation that is required for solving\nvarious intelligence tests in humans. We tested $\\textit{untrained}$ (naive)\ndeep learning models in the SCE task. Specifically, we compared Relation\nNetworks (RN) and Contrastive Predictive Coding (CPC), two models that can\nextract simple rules from sequences, and found that the latter, which imposes a\nstructure on the predictable rule does better. We further found that simple\nnetworks fare better in this task than complex ones. Finally, we show that this\napproach can be used for security camera anomaly detection without any prior\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barak_T/0/1/0/all/0/1\">Tomer Barak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loewenstein_Y/0/1/0/all/0/1\">Yonatan Loewenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance. (arXiv:2205.12022v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12022","description":"<p>With the rapid development of the Metaverse, virtual humans have emerged, and\nhuman image synthesis and editing techniques, such as pose transfer, have\nrecently become popular. Most of the existing techniques rely on GANs, which\ncan generate good human images even with large variants and occlusions. But\nfrom our best knowledge, the existing state-of-the-art method still has the\nfollowing problems: the first is that the rendering effect of the synthetic\nimage is not realistic, such as poor rendering of some regions. And the second\nis that the training of GAN is unstable and slow to converge, such as model\ncollapse. Based on the above two problems, we propose several methods to solve\nthem. To improve the rendering effect, we use the Residual Fast Fourier\nTransform Block to replace the traditional Residual Block. Then, spectral\nnormalization and Wasserstein distance are used to improve the speed and\nstability of GAN training. Experiments demonstrate that the methods we offer\nare effective at solving the problems listed above, and we get state-of-the-art\nscores in LPIPS and PSNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Shijing Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation. (arXiv:2205.12028v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12028","description":"<p>Human Body Dimensions Estimation (HBDE) is a task that an intelligent agent\ncan perform to attempt to determine human body information from images (2D) or\npoint clouds or meshes (3D). More specifically, if we define the HBDE problem\nas inferring human body measurements from images, then HBDE is a difficult,\ninverse, multi-task regression problem that can be tackled with machine\nlearning techniques, particularly convolutional neural networks (CNN). Despite\nthe community's tremendous effort to advance human shape analysis, there is a\nlack of systematic experiments to assess CNNs estimation of human body\ndimensions from images. Our contribution lies in assessing a CNN estimation\nperformance in a series of controlled experiments. To that end, we augment our\nrecently published neural anthropometer dataset by rendering images with\ndifferent camera distance. We evaluate the network inference absolute and\nrelative mean error between the estimated and actual HBDs. We train and\nevaluate the CNN in four scenarios: (1) training with subjects of a specific\ngender, (2) in a specific pose, (3) sparse camera distance and (4) dense camera\ndistance. Not only our experiments demonstrate that the network can perform the\ntask successfully, but also reveal a number of relevant facts that contribute\nto better understand the task of HBDE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tejeda_Y/0/1/0/all/0/1\">Yansel G&#xf3;nzalez Tejeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_H/0/1/0/all/0/1\">Helmut A. Mayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification. (arXiv:2205.12029v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12029","description":"<p>Multimodal learning from document data has achieved great success lately as\nit allows to pre-train semantically meaningful features as a prior into a\nlearnable downstream approach. In this paper, we approach the document\nclassification problem by learning cross-modal representations through language\nand vision cues, considering intra- and inter-modality relationships. Instead\nof merging features from different modalities into a common representation\nspace, the proposed method exploits high-level interactions and learns relevant\nsemantic information from effective attention flows within and across\nmodalities. The proposed learning objective is devised between intra- and\ninter-modality alignment tasks, where the similarity distribution per task is\ncomputed by contracting positive sample pairs while simultaneously contrasting\nnegative ones in the common feature representation space}. Extensive\nexperiments on public document classification datasets demonstrate the\neffectiveness and the generalization capacity of our model on both low-scale\nand large-scale datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakkali_S/0/1/0/all/0/1\">Souhail Bakkali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zuheng Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coustaty_M/0/1/0/all/0/1\">Mickael Coustaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinol_M/0/1/0/all/0/1\">Mar&#xe7;al Rusi&#xf1;ol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terrades_O/0/1/0/all/0/1\">Oriol Ramos Terrades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Image Classification Using Vision Transformer. (arXiv:2205.12041v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12041","description":"<p>In this paper, we propose a privacy-preserving image classification method\nthat is based on the combined use of encrypted images and the vision\ntransformer (ViT). The proposed method allows us not only to apply images\nwithout visual information to ViT models for both training and testing but to\nalso maintain a high classification accuracy. ViT utilizes patch embedding and\nposition embedding for image patches, so this architecture is shown to reduce\nthe influence of block-wise image transformation. In an experiment, the\nproposed method for privacy-preserving image classification is demonstrated to\noutperform state-of-the-art methods in terms of classification accuracy and\nrobustness against various attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zheng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MaungMaung_A/0/1/0/all/0/1\">AprilPyone MaungMaung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinoshita_Y/0/1/0/all/0/1\">Yuma Kinoshita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Efficient CNNS: Tweaking the Nuts and Bolts of Neural Networks for Lighter, Faster and Robust Models. (arXiv:2205.12050v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12050","description":"<p>Deep Learning has revolutionized the fields of computer vision, natural\nlanguage understanding, speech recognition, information retrieval and more.\nMany techniques have evolved over the past decade that made models lighter,\nfaster, and robust with better generalization. However, many deep learning\npractitioners persist with pre-trained models and architectures trained mostly\non standard datasets such as Imagenet, MS-COCO, IMDB-Wiki Dataset, and\nKinetics-700 and are either hesitant or unaware of redesigning the architecture\nfrom scratch that will lead to better performance. This scenario leads to\ninefficient models that are not suitable on various devices such as mobile,\nedge, and fog. In addition, these conventional training methods are of concern\nas they consume a lot of computing power. In this paper, we revisit various\nSOTA techniques that deal with architecture efficiency (Global Average Pooling,\ndepth-wise convolutions &amp; squeeze and excitation, Blurpool), learning rate\n(Cyclical Learning Rate), data augmentation (Mixup, Cutout), label manipulation\n(label smoothing), weight space manipulation (stochastic weight averaging), and\noptimizer (sharpness aware minimization). We demonstrate how an efficient deep\nconvolution network can be built in a phased manner by sequentially reducing\nthe number of training parameters and using the techniques mentioned above. We\nachieved a SOTA accuracy of 99.2% on MNIST data with just 1500 parameters and\nan accuracy of 86.01% with just over 140K parameters on the CIFAR-10 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ethiraj_S/0/1/0/all/0/1\">Sabeesh Ethiraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolla_B/0/1/0/all/0/1\">Bharath Kumar Bolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Attention Network for Skeleton Extraction. (arXiv:2205.12066v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12066","description":"<p>Skeleton extraction is a task focused on providing a simple representation of\nan object by extracting the skeleton from the given binary or RGB image. In\nrecent years many attractive works in skeleton extraction have been made. But\nas far as we know, there is little research on how to utilize the context\ninformation in the binary shape of objects. In this paper, we propose an\nattention-based model called Context Attention Network (CANet), which\nintegrates the context extraction module in a UNet architecture and can\neffectively improve the ability of network to extract the skeleton pixels.\nMeanwhile, we also use some novel techniques including distance transform,\nweight focal loss to achieve good results on the given dataset. Finally,\nwithout model ensemble and with only 80% of the training images, our method\nachieves 0.822 F1 score during the development phase and 0.8507 F1 score during\nthe final phase of the Pixel SkelNetOn Competition, ranking 1st place on the\nleaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ruili Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Phonological Parameters in Sign Languages. (arXiv:2205.12072v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12072","description":"<p>Signers compose sign language phonemes that enable communication by combining\nphonological parameters such as handshape, orientation, location, movement, and\nnon-manual features. Linguistic research often breaks down signs into their\nconstituent parts to study sign languages and often a lot of effort is invested\ninto the annotation of the videos. In this work we show how a single model can\nbe used to recognise the individual phonological parameters within sign\nlanguages with the aim of either to assist linguistic annotations or to\ndescribe the signs for the sign recognition models. We use Danish Sign Language\ndata set `Ordbog over Dansk Tegnsprog' to generate multiple data sets using\npose estimation model, which are then used for training the multi-label Fast\nR-CNN model to support multi-label modelling. Moreover, we show that there is a\nsignificant co-dependence between the orientation and location phonological\nparameters in the generated data and we incorporate this co-dependence in the\nmodel to achieve better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mocialov_B/0/1/0/all/0/1\">Boris Mocialov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_G/0/1/0/all/0/1\">Graham Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hastie_H/0/1/0/all/0/1\">Helen Hastie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution. (arXiv:2205.12089v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12089","description":"<p>Service robots should be able to interact naturally with non-expert human\nusers, not only to help them in various tasks but also to receive guidance in\norder to resolve ambiguities that might be present in the instruction. We\nconsider the task of visual grounding, where the agent segments an object from\na crowded scene given a natural language description. Modern holistic\napproaches to visual grounding usually ignore language structure and struggle\nto cover generic domains, therefore relying heavily on large datasets.\nAdditionally, their transfer performance in RGB-D datasets suffers due to high\nvisual discrepancy between the benchmark and the target domains. Modular\napproaches marry learning with domain modeling and exploit the compositional\nnature of language to decouple visual representation from language parsing, but\neither rely on external parsers or are trained in an end-to-end fashion due to\nthe lack of strong supervision. In this work, we seek to tackle these\nlimitations by introducing a fully decoupled modular framework for\ncompositional visual grounding of entities, attributes, and spatial relations.\nWe exploit rich scene graph annotations generated in a synthetic domain and\ntrain each module independently. Our approach is evaluated both in simulation\nand in two real RGB-D scene datasets. Experimental results show that the\ndecoupled nature of our framework allows for easy integration with domain\nadaptation approaches for Sim-To-Real visual recognition, offering a\ndata-efficient, robust, and interpretable solution to visual grounding in\nrobotic applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tziafas_G/0/1/0/all/0/1\">Georgios Tziafas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1\">Hamidreza Kasaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval. (arXiv:2205.12105v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12105","description":"<p>In the past few years, the emergence of vision-language pre-training (VLP)\nhas brought cross-modal retrieval to a new era. However, due to the latency and\ncomputation demand, it is commonly challenging to apply VLP in a real-time\nonline retrieval system. To alleviate the defect, this paper proposes a\n\\textbf{Hi}erarchical \\textbf{V}ision-\\textbf{}Language \\textbf{P}re-Training\n(\\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a\nnovel hierarchical retrieval objective, which uses the representation of\ndifferent dimensions for coarse-to-fine ITR, i.e., using low-dimensional\nrepresentation for large-scale coarse retrieval and high-dimensional\nrepresentation for small-scale fine retrieval. We evaluate our proposed HiVLP\non two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.\nExtensive experiments demonstrate that our HiVLP not only has fast inference\nspeed but also can be easily scaled to large-scale ITR scenarios. The detailed\nresults show that HiVLP is $1,427$$\\sim$$120,649\\times$ faster than the\nfusion-based model UNITER and 2$\\sim$5 faster than the fastest embedding-based\nmodel LightingDot in different candidate scenarios. It also achieves about +4.9\nAR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable\nperformance with the state-of-the-art (SOTA) fusion-based model METER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianlong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Drive Using Sparse Imitation Reinforcement Learning. (arXiv:2205.12128v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12128","description":"<p>In this paper, we propose Sparse Imitation Reinforcement Learning (SIRL), a\nhybrid end-to-end control policy that combines the sparse expert driving\nknowledge with reinforcement learning (RL) policy for autonomous driving (AD)\ntask in CARLA simulation environment. The sparse expert is designed based on\nhand-crafted rules which is suboptimal but provides a risk-averse strategy by\nenforcing experience for critical scenarios such as pedestrian and vehicle\navoidance, and traffic light detection. As it has been demonstrated, training a\nRL agent from scratch is data-inefficient and time consuming particularly for\nthe urban driving task, due to the complexity of situations stemming from the\nvast size of state space. Our SIRL strategy provides a solution to solve these\nproblems by fusing the output distribution of the sparse expert policy and the\nRL policy to generate a composite driving policy. With the guidance of the\nsparse expert during the early training stage, SIRL strategy accelerates the\ntraining process and keeps the RL exploration from causing a catastrophe\noutcome, and ensures safe exploration. To some extent, the SIRL agent is\nimitating the driving expert's behavior. At the same time, it continuously\ngains knowledge during training therefore it keeps making improvement beyond\nthe sparse expert, and can surpass both the sparse expert and a traditional RL\nagent. We experimentally validate the efficacy of proposed SIRL approach in a\ncomplex urban scenario within the CARLA simulator. Besides, we compare the SIRL\nagent's performance for risk-averse exploration and high learning efficiency\nwith the traditional RL approach. We additionally demonstrate the SIRL agent's\ngeneralization ability to transfer the driving skill to unseen environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yuci Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-Reference Calibration-Free Image Quality Assessment. (arXiv:2205.12129v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12129","description":"<p>One major problem of objective Image Quality Assessment (IQA) methods is the\nlack of linearity of their quality estimates with respect to scores expressed\nby human subjects. For this reason, usually IQA metrics undergo a calibration\nprocess based on subjective quality examples. However, example-based training\nmakes generalization problematic, hampering result comparison across different\napplications and operative conditions. In this paper, new Full Reference (FR)\ntechniques, providing estimates linearly correlated with human scores without\nusing calibration are introduced. To reach this objective, these techniques are\ndeeply rooted on principles and theoretical constraints. Restricting the\ninterest on the IQA of the set of natural images, it is first recognized that\napplication of estimation theory and psycho physical principles to images\ndegraded by Gaussian blur leads to a so-called canonical IQA method, whose\nestimates are not only highly linearly correlated to subjective scores, but are\nalso straightforwardly related to the Viewing Distance (VD). Then, it is shown\nthat mainstream IQA methods can be reconducted to the canonical method applying\na preliminary metric conversion based on a unique specimen image. The\napplication of this scheme is then extended to a significant class of degraded\nimages other than Gaussian blur, including noisy and compressed images. The\nresulting calibration-free FR IQA methods are suited for applications where\ncomparability and interoperability across different imaging systems and on\ndifferent VDs is a major requirement. A comparison of their statistical\nperformance with respect to some conventional calibration prone methods is\nfinally provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Claudio_E/0/1/0/all/0/1\">Elio D. Di Claudio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannitrapani_P/0/1/0/all/0/1\">Paolo Giannitrapani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacovitti_G/0/1/0/all/0/1\">Giovanni Jacovitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Deforestation from Sentinel-1 Data in the Absence of Reliable Reference Data. (arXiv:2205.12131v1 [stat.ME])","link":"http://arxiv.org/abs/2205.12131","description":"<p>Forests are vital for the wellbeing of our planet. Large and small scale\ndeforestation across the globe is threatening the stability of our climate,\nforest biodiversity, and therefore the preservation of fragile ecosystems and\nour natural habitat as a whole. With increasing public interest in climate\nchange issues and forest preservation, a large demand for carbon offsetting,\ncarbon footprint ratings, and environmental impact assessments is emerging.\nMost often, deforestation maps are created from optical data such as Landsat\nand MODIS. These maps are not typically available at less than annual intervals\ndue to persistent cloud cover in many parts of the world, especially the\ntropics where most of the world's forest biomass is concentrated. Synthetic\nAperture Radar (SAR) can fill this gap as it penetrates clouds. We propose and\nevaluate a novel method for deforestation detection in the absence of reliable\nreference data which often constitutes the largest practical hurdle. This\nmethod achieves a change detection sensitivity (producer's accuracy) of 96.5%\nin the study area, although false positives lead to a lower user's accuracy of\nabout 75.7%, with a total balanced accuracy of 90.4%. The change detection\naccuracy is maintained when adding up to 20% noise to the reference labels.\nWhile further work is required to reduce the false positive rate, improve\ndetection delay, and validate this method in additional circumstances, the\nresults show that Sentinel-1 data have the potential to advance the timeliness\nof global deforestation monitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Hansen_J/0/1/0/all/0/1\">Johannes N. Hansen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mitchard_E/0/1/0/all/0/1\">Edward T.A. Mitchard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+King_S/0/1/0/all/0/1\">Stuart King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Latent Space of Image Style Transfer. (arXiv:2205.12135v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12135","description":"<p>Existing neural style transfer researches have studied to match statistical\ninformation between the deep features of content and style images, which were\nextracted by a pre-trained VGG, and achieved significant improvement in\nsynthesizing artistic images. However, in some cases, the feature statistics\nfrom the pre-trained encoder may not be consistent with the visual style we\nperceived. For example, the style distance between images of different styles\nis less than that of the same style. In such an inappropriate latent space, the\nobjective function of the existing methods will be optimized in the wrong\ndirection, resulting in bad stylization results. In addition, the lack of\ncontent details in the features extracted by the pre-trained encoder also leads\nto the content leak problem. In order to solve these issues in the latent space\nused by style transfer, we propose two contrastive training schemes to get a\nrefined encoder that is more suitable for this task. The style contrastive loss\npulls the stylized result closer to the same visual style image and pushes it\naway from the content image. The content contrastive loss enables the encoder\nto retain more available details. We can directly add our training scheme to\nsome existing style transfer methods and significantly improve their results.\nExtensive experimental results demonstrate the effectiveness and superiority of\nour methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunpeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cairong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Performance of Federated Person Re-identification: Benchmarking and Analysis. (arXiv:2205.12144v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12144","description":"<p>The increasingly stringent data privacy regulations limit the development of\nperson re-identification (ReID) because person ReID training requires\ncentralizing an enormous amount of data that contains sensitive personal\ninformation. To address this problem, we introduce federated person\nre-identification (FedReID) -- implementing federated learning, an emerging\ndistributed training method, to person ReID. FedReID preserves data privacy by\naggregating model updates, instead of raw data, from clients to a central\nserver. Furthermore, we optimize the performance of FedReID under statistical\nheterogeneity via benchmark analysis. We first construct a benchmark with an\nenhanced algorithm, two architectures, and nine person ReID datasets with large\nvariances to simulate the real-world statistical heterogeneity. The benchmark\nresults present insights and bottlenecks of FedReID under statistical\nheterogeneity, including challenges in convergence and poor performance on\ndatasets with large volumes. Based on these insights, we propose three\noptimization approaches: (1) We adopt knowledge distillation to facilitate the\nconvergence of FedReID by better transferring knowledge from clients to the\nserver; (2) We introduce client clustering to improve the performance of large\ndatasets by aggregating clients with similar data distributions; (3) We propose\ncosine distance weight to elevate performance by dynamically updating the\nweights for aggregation depending on how well models are trained in clients.\nExtensive experiments demonstrate that these approaches achieve satisfying\nconvergence with much better performance on all datasets. We believe that\nFedReID will shed light on implementing and optimizing federated learning on\nmore computer vision applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_X/0/1/0/all/0/1\">Xin Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning. (arXiv:2205.12183v1 [cs.GR])","link":"http://arxiv.org/abs/2205.12183","description":"<p>3D scene stylization aims at generating stylized images of the scene from\narbitrary novel views following a given set of style examples, while ensuring\nconsistency when rendered from different views. Directly applying methods for\nimage or video stylization to 3D scenes cannot achieve such consistency. Thanks\nto recently proposed neural radiance fields (NeRF), we are able to represent a\n3D scene in a consistent way. Consistent 3D scene stylization can be\neffectively achieved by stylizing the corresponding NeRF. However, there is a\nsignificant domain gap between style examples which are 2D images and NeRF\nwhich is an implicit volumetric representation. To address this problem, we\npropose a novel mutual learning framework for 3D scene stylization that\ncombines a 2D image stylization network and NeRF to fuse the stylization\nability of 2D stylization network with the 3D consistency of NeRF. We first\npre-train a standard NeRF of the 3D scene to be stylized and replace its color\nprediction module with a style network to obtain a stylized NeRF.It is followed\nby distilling the prior knowledge of spatial consistency from NeRF to the 2D\nstylization network through an introduced consistency loss. We also introduce a\nmimic loss to supervise the mutual learning of the NeRF style module and\nfine-tune the 2D stylization decoder. In order to further make our model handle\nambiguities of 2D stylization results, we introduce learnable latent codes that\nobey the probability distributions conditioned on the style. They are attached\nto training samples as conditional inputs to better learn the style module in\nour novel stylized NeRF. Experimental results demonstrate that our method is\nsuperior to existing approaches in both visual quality and long-range\nconsistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi-Hua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yu-Jie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization. (arXiv:2205.12191v1 [cs.CL])","link":"http://arxiv.org/abs/2205.12191","description":"<p>Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data\nhave demonstrated strong performance on various tasks such as image captioning\nand visual question answering (VQA). The quality of such models is commonly\nassessed by measuring their performance on unseen data that typically comes\nfrom the same distribution as the training data. However, we observe that these\nmodels exhibit poor out-of-distribution (OOD) generalization on the task of\nVQA. To better understand the underlying causes of poor generalization, we\ncomprehensively investigate performance of two pretrained V&amp;L models under\ndifferent settings (i.e. classification and open-ended text generation) by\nconducting cross-dataset evaluations. We find that these models tend to learn\nto solve the benchmark, rather than learning the high-level skills required by\nthe VQA task. We also argue that in most cases generative models are less\nsusceptible to shifts in data distribution, while frequently performing better\non our tested benchmarks. Moreover, we find that multimodal pretraining\nimproves OOD performance in most settings. Finally, we revisit assumptions\nunderlying the use of automatic VQA evaluation metrics, and empirically show\nthat their stringent nature repeatedly penalizes models for correct responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Aishwarya Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajic_I/0/1/0/all/0/1\">Ivana Kaji&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoodi_E/0/1/0/all/0/1\">Elnaz Davoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gergely_A/0/1/0/all/0/1\">Anita Gergely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blunsom_P/0/1/0/all/0/1\">Phil Blunsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Absolute Triangulation Algorithms for Space Exploration. (arXiv:2205.12197v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12197","description":"<p>Images are an important source of information for spacecraft navigation and\nfor three-dimensional reconstruction of observed space objects. Both of these\napplications take the form of a triangulation problem when the camera has a\nknown attitude and the measurements extracted from the image are line of sight\n(LOS) directions. This work provides a comprehensive review of the history and\ntheoretical foundations of triangulation. A variety of classical triangulation\nalgorithms are reviewed, including a number of suboptimal linear methods (many\nLOS measurements) and the optimal method of Hartley and Sturm (only two LOS\nmeasurements). Two new optimal non-iterative triangulation algorithms are\nintroduced that provide the same solution as Hartley and Sturm. The optimal\ntwo-measurement case can be solved as a quadratic equation in many common\nsituations. The optimal many-measurement case may be solved without iteration\nas a linear system using the new Linear Optimal Sine Triangulation (LOST)\nmethod. The various triangulation algorithms are assessed with a few numerical\nexamples, including planetary terrain relative navigation, angles-only optical\nnavigation at Uranus, 3-D reconstruction of Notre-Dame de Paris, and\nangles-only relative navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henry_S/0/1/0/all/0/1\">Sebastien Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christian_J/0/1/0/all/0/1\">John A. Christian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12219","description":"<p>The ability to converse with humans and follow commands in natural language\nis crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can\nrelieve people's burden of holding a controller all the time, allow\nmultitasking, and make drone control more accessible for people with\ndisabilities or with their hands occupied. To this end, we introduce Aerial\nVision-and-Dialog Navigation (AVDN), to navigate a drone via natural language\nconversation. We build a drone simulator with a continuous photorealistic\nenvironment and collect a new AVDN dataset of over 3k recorded navigation\ntrajectories with asynchronous human-human dialogs between commanders and\nfollowers. The commander provides initial navigation instruction and further\nguidance by request, while the follower navigates the drone in the simulator\nand asks questions when needed. During data collection, followers' attention on\nthe drone's visual observation is also recorded. Based on the AVDN dataset, we\nstudy the tasks of aerial navigation from (full) dialog history and propose an\neffective Human Attention Aided (HAA) baseline model, which learns to predict\nboth navigation waypoints and human attention. Dataset and code will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Winson Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tongzhou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLOBUS: GLObal Building heights for Urban Studies. (arXiv:2205.12224v1 [physics.ao-ph])","link":"http://arxiv.org/abs/2205.12224","description":"<p>Urban weather and climate studies continue to be important as extreme events\ncause economic loss and impact public health. Weather models seek to represent\nurban areas but are oversimplified due to data availability, especially\nbuilding information. This paper introduces a novel Level of Detail-1 (LoD-1)\nbuilding dataset derived from a Deep Neural Network (DNN) called GLObal\nBuilding heights for Urban Studies (GLOBUS). GLOBUS uses open-source datasets\nas predictors: Advanced Land Observation Satellite (ALOS) Digital Surface Model\n(DSM) normalized using Shuttle Radar Topography Mission (SRTM) Digital\nElevation Model (DEM), Landscan population density, and building footprints.\nThe building information from GLOBUS can be ingested in Numerical Weather\nPrediction (NWP) and urban energy-water balance models to study localized\nphenomena such as the Urban Heat Island (UHI) effect. GLOBUS has been trained\nand validated using the United States Geological Survey (USGS) 3DEP Light\nDetection and Ranging (LiDAR) data. We used data from 5 US cities for training\nand the model was validated over 6 cities. Performance metrics are computed at\na spatial resolution of 300-meter. The Root Mean Squared Error (RMSE) and Mean\nAbsolute Percentage Error (MAPE) were 5.15 meters and 28.8 %, respectively. The\nstandard deviation and histogram of building heights over a 300-meter grid are\nwell represented using GLOBUS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Kamath_H/0/1/0/all/0/1\">Harsh G. Kamath</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_M/0/1/0/all/0/1\">Manmeet Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Magruder_L/0/1/0/all/0/1\">Lori A. Magruder</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_Z/0/1/0/all/0/1\">Zong-Liang Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Niyogi_D/0/1/0/all/0/1\">Dev Niyogi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions. (arXiv:2205.12231v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12231","description":"<p>We present ASSET, a neural architecture for automatically modifying an input\nhigh-resolution image according to a user's edits on its semantic segmentation\nmap. Our architecture is based on a transformer with a novel attention\nmechanism. Our key idea is to sparsify the transformer's attention matrix at\nhigh resolutions, guided by dense attention extracted at lower image\nresolutions. While previous attention mechanisms are computationally too\nexpensive for handling high-resolution images or are overly constrained within\nspecific image regions hampering long-range interactions, our novel attention\nmechanism is both computationally efficient and effective. Our sparsified\nattention mechanism is able to capture long-range interactions and context,\nleading to synthesizing interesting phenomena in scenes, such as reflections of\nlandscapes onto water or flora consistent with the rest of the landscape, that\nwere not possible to generate reliably with previous convnets and transformer\napproaches. We present qualitative and quantitative results, along with user\nstudies, demonstrating the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Difan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1\">Sandesh Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinz_T/0/1/0/all/0/1\">Tobias Hinz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Taesung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gacs-Korner Common Information Variational Autoencoder. (arXiv:2205.12239v1 [cs.LG])","link":"http://arxiv.org/abs/2205.12239","description":"<p>We propose a notion of common information that allows one to quantify and\nseparate the information that is shared between two random variables from the\ninformation that is unique to each. Our notion of common information is a\nvariational relaxation of the G\\'acs-K\\\"orner common information, which we\nrecover as a special case, but is more amenable to optimization and can be\napproximated empirically using samples from the underlying distribution. We\nthen provide a method to partition and quantify the common and unique\ninformation using a simple modification of a traditional variational\nauto-encoder. Empirically, we demonstrate that our formulation allows us to\nlearn semantically meaningful common and unique factors of variation even on\nhigh-dimensional data such as images and videos. Moreover, on datasets where\nground-truth latent factors are known, we show that we can accurately quantify\nthe common information between the random variables. Additionally, we show that\nthe auto-encoder that we learn recovers semantically meaningful disentangled\nfactors of variation, even though we do not explicitly optimize for it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kleinman_M/0/1/0/all/0/1\">Michael Kleinman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Jonathan Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Dynamics for Articulated 3d Human Motion Reconstruction. (arXiv:2205.12256v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12256","description":"<p>We introduce DiffPhy, a differentiable physics-based model for articulated 3d\nhuman motion reconstruction from video. Applications of physics-based reasoning\nin human motion analysis have so far been limited, both by the complexity of\nconstructing adequate physical models of articulated human motion, and by the\nformidable challenges of performing stable and efficient inference with physics\nin the loop. We jointly address such modeling and inference challenges by\nproposing an approach that combines a physically plausible body representation\nwith anatomical joint limits, a differentiable physics simulator, and\noptimization techniques that ensure good performance and robustness to\nsuboptimal local optima. In contrast to several recent methods, our approach\nreadily supports full-body contact including interactions with objects in the\nscene. Most importantly, our model connects end-to-end with images, thus\nsupporting direct gradient-based physics optimization by means of image-based\nloss functions. We validate the model by demonstrating that it can accurately\nreconstruct physically plausible 3d human motion from monocular video, both on\npublic benchmarks with available 3d ground-truth, and on videos from the\ninternet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gartner_E/0/1/0/all/0/1\">Erik G&#xe4;rtner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andriluka_M/0/1/0/all/0/1\">Mykhaylo Andriluka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coumans_E/0/1/0/all/0/1\">Erwin Coumans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OnePose: One-Shot Object Pose Estimation without CAD Models. (arXiv:2205.12257v1 [cs.CV])","link":"http://arxiv.org/abs/2205.12257","description":"<p>We propose a new method named OnePose for object pose estimation. Unlike\nexisting instance-level or category-level methods, OnePose does not rely on CAD\nmodels and can handle objects in arbitrary categories without instance- or\ncategory-specific network training. OnePose draws the idea from visual\nlocalization and only requires a simple RGB video scan of the object to build a\nsparse SfM model of the object. Then, this model is registered to new query\nimages with a generic feature matching network. To mitigate the slow runtime of\nexisting visual localization methods, we propose a new graph attention network\nthat directly matches 2D interest points in the query image with the 3D points\nin the SfM model, resulting in efficient and robust pose estimation. Combined\nwith a feature-based pose tracker, OnePose is able to stably detect and track\n6D poses of everyday household objects in real-time. We also collected a\nlarge-scale dataset that consists of 450 sequences of 150 objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiaming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongcheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the competition between detection and ReID in Multi-Object Tracking. (arXiv:2010.12138v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12138","description":"<p>Due to balanced accuracy and speed, one-shot models which jointly learn\ndetection and identification embeddings, have drawn great attention in\nmulti-object tracking (MOT). However, the inherent differences and relations\nbetween detection and re-identification (ReID) are unconsciously overlooked\nbecause of treating them as two isolated tasks in the one-shot tracking\nparadigm. This leads to inferior performance compared with existing two-stage\nmethods. In this paper, we first dissect the reasoning process for these two\ntasks, which reveals that the competition between them inevitably would destroy\ntask-dependent representations learning. To tackle this problem, we propose a\nnovel reciprocal network (REN) with a self-relation and cross-relation design\nso that to impel each branch to better learn task-dependent representations.\nThe proposed model aims to alleviate the deleterious tasks competition,\nmeanwhile improve the cooperation between detection and ReID. Furthermore, we\nintroduce a scale-aware attention network (SAAN) that prevents semantic level\nmisalignment to improve the association capability of ID embeddings. By\nintegrating the two delicately designed networks into a one-shot online MOT\nsystem, we construct a strong MOT tracker, namely CSTrack. Our tracker achieves\nthe state-of-the-art performance on MOT16, MOT17 and MOT20 datasets, without\nother bells and whistles. Moreover, CSTrack is efficient and runs at 16.4 FPS\non a single modern GPU, and its lightweight version even runs at 34.6 FPS. The\ncomplete code has been released at https://github.com/JudasDie/SOTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shuyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey. (arXiv:2101.00734v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2101.00734","description":"<p>This is a tutorial and survey paper on factor analysis, probabilistic\nPrincipal Component Analysis (PCA), variational inference, and Variational\nAutoencoder (VAE). These methods, which are tightly related, are dimensionality\nreduction and generative models. They assume that every data point is generated\nfrom or caused by a low-dimensional latent factor. By learning the parameters\nof distribution of latent space, the corresponding low-dimensional factors are\nfound for the sake of dimensionality reduction. For their stochastic and\ngenerative behaviour, these models can also be used for generation of new data\npoints in the data space. In this paper, we first start with variational\ninference where we derive the Evidence Lower Bound (ELBO) and Expectation\nMaximization (EM) for learning the parameters. Then, we introduce factor\nanalysis, derive its joint and marginal distributions, and work out its EM\nsteps. Probabilistic PCA is then explained, as a special case of factor\nanalysis, and its closed-form solutions are derived. Finally, VAE is explained\nwhere the encoder, decoder and sampling from the latent space are introduced.\nTraining VAE using both EM and backpropagation are explained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effects of Image Size on Deep Learning. (arXiv:2101.11508v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11508","description":"<p>This paper presents the evaluation of the effects of image size on deep\nlearning performance via semantic segmentation of magnetic resonance heart\nimages with U-net for fully automated quantification of myocardial infarction.\nBoth non-extra pixel and extra pixel interpolation algorithms are used to\nchange the size of images in datasets of interest. Extra class labels, in\ninterpolated ground truth segmentation images, are removed using thresholding,\nmedian filtering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rukundo_O/0/1/0/all/0/1\">Olivier Rukundo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.03097","description":"<p>Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets, applications, and our\nopen-sourced codebase for fair evaluation. Finally, we summarize existing\nliterature and present some potential research topics for the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yidong Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances on Neural Network Pruning at Initialization. (arXiv:2103.06460v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.06460","description":"<p>Neural network pruning typically removes connections or neurons from a\npretrained converged model; while a new pruning paradigm, pruning at\ninitialization (PaI), attempts to prune a randomly initialized network. This\npaper offers the first survey concentrated on this emerging pruning fashion. We\nfirst introduce a generic formulation of neural network pruning, followed by\nthe major classic pruning topics. Then, as the main body of this paper, a\nthorough and structured literature review of PaI methods is presented,\nconsisting of two major tracks (sparse training and sparse selection). Finally,\nwe summarize the surge of PaI compared to PaT and discuss the open problems.\nApart from the dedicated literature review, this paper also offers a code base\nfor easy sanity-checking and benchmarking of different PaI methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Can Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BADet: Boundary-Aware 3D Object Detection from Point Clouds. (arXiv:2104.10330v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10330","description":"<p>Currently, existing state-of-the-art 3D object detectors are in two-stage\nparadigm. These methods typically comprise two steps: 1) Utilize a region\nproposal network to propose a handful of high-quality proposals in a bottom-up\nfashion. 2) Resize and pool the semantic features from the proposed regions to\nsummarize RoI-wise representations for further refinement. Note that these\nRoI-wise representations in step 2) are considered individually as uncorrelated\nentries when fed to following detection headers. Nevertheless, we observe these\nproposals generated by step 1) offset from ground truth somehow, emerging in\nlocal neighborhood densely with an underlying probability. Challenges arise in\nthe case where a proposal largely forsakes its boundary information due to\ncoordinate offset while existing networks lack corresponding information\ncompensation mechanism. In this paper, we propose $BADet$ for 3D object\ndetection from point clouds. Specifically, instead of refining each proposal\nindependently as previous works do, we represent each proposal as a node for\ngraph construction within a given cut-off threshold, associating proposals in\nthe form of local neighborhood graph, with boundary correlations of an object\nbeing explicitly exploited. Besides, we devise a lightweight Region Feature\nAggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise\nfeatures with expanding receptive fields for more informative RoI-wise\nrepresentations. We validate BADet both on widely used KITTI Dataset and highly\nchallenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par\nperformance on KITTI 3D detection leaderboard and ranks $1^{st}$ on $Moderate$\ndifficulty of $Car$ category on KITTI BEV detection leaderboard. The source\ncode is available at https://github.com/rui-qian/BADet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Real-Time Monocular SLAM Using Semantic Segmentation on Selective Frames. (arXiv:2105.00114v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00114","description":"<p>Monocular simultaneous localization and mapping (SLAM) is emerging in\nadvanced driver assistance systems and autonomous driving, because a single\ncamera is cheap and easy to install. Conventional monocular SLAM has two major\nchallenges leading inaccurate localization and mapping. First, it is\nchallenging to estimate scales in localization and mapping. Second,\nconventional monocular SLAM uses inappropriate mapping factors such as dynamic\nobjects and low-parallax areas in mapping. This paper proposes an improved\nreal-time monocular SLAM that resolves the aforementioned challenges by\nefficiently using deep learning-based semantic segmentation. To achieve the\nreal-time execution of the proposed method, we apply semantic segmentation only\nto downsampled keyframes in parallel with mapping processes. In addition, the\nproposed method corrects scales of camera poses and three-dimensional (3D)\npoints, using estimated ground plane from road-labeled 3D points and the real\ncamera height. The proposed method also removes inappropriate corner features\nlabeled as moving objects and low parallax areas. Experiments with eight video\nsequences demonstrate that the proposed monocular SLAM system achieves\nsignificantly improved and comparable trajectory tracking accuracy, compared to\nexisting state-of-the-art monocular and stereo SLAM systems, respectively. The\nproposed system can achieve real-time tracking on a standard CPU potentially\nwith a standard GPU support, whereas existing segmentation-aided monocular SLAM\ndoes not.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_M/0/1/0/all/0/1\">Muhyun Back</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Soo Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_I/0/1/0/all/0/1\">Il Yong Chun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.01100","description":"<p>During lung radiotherapy, the position of infrared reflective objects on the\nchest can be recorded to estimate the tumor location. However, radiotherapy\nsystems have a latency inherent to robot control limitations that impedes the\nradiation delivery precision. Prediction with online learning of recurrent\nneural networks (RNN) allows for adaptation to non-stationary respiratory\nsignals, but classical methods such as RTRL and truncated BPTT are respectively\nslow and biased. This study investigates the capabilities of unbiased online\nrecurrent optimization (UORO) to forecast respiratory motion and enhance safety\nin lung radiotherapy.\n</p>\n<p>We used 9 observation records of the 3D position of 3 external markers on the\nchest and abdomen of healthy individuals breathing during intervals from 73s to\n222s. The sampling frequency was 10Hz, and the amplitudes of the recorded\ntrajectories range from 6mm to 40mm in the superior-inferior direction. We\nforecast the 3D location of each marker simultaneously with a horizon value\nbetween 0.1s and 2.0s, using an RNN trained with UORO. We compare its\nperformance with an RNN trained with RTRL, LMS, and offline linear regression.\nWe provide closed-form expressions for quantities involved in the gradient loss\ncalculation in UORO, thereby making its implementation efficient. Training and\ncross-validation were performed during the first minute of each sequence.\n</p>\n<p>On average over the horizon values considered and the 9 sequences, UORO\nachieves the lowest root-mean-square (RMS) error and maximum error among the\ncompared algorithms. These errors are respectively equal to 1.3mm and 8.8mm,\nand the prediction time per time step was lower than 2.8ms (Dell Intel core\ni9-9900K 3.60 GHz). Linear regression has the lowest RMS error for the horizon\nvalues 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,\nand UORO for horizon values greater than 0.6s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pohl_M/0/1/0/all/0/1\">Michel Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uesaka_M/0/1/0/all/0/1\">Mitsuru Uesaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takahashi_H/0/1/0/all/0/1\">Hiroyuki Takahashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demachi_K/0/1/0/all/0/1\">Kazuyuki Demachi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chhatkuli_R/0/1/0/all/0/1\">Ritu Bhusal Chhatkuli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.02229","description":"<p>In this paper, we investigate the fundamental question: To what extent are\ngradient-based neural architecture search (NAS) techniques applicable to RL?\nUsing the original DARTS as a convenient baseline, we discover that the\ndiscrete architectures found can achieve up to 250% performance compared to\nmanual architecture designs on both discrete and continuous action space\nenvironments across off-policy and on-policy RL algorithms, at only 3x more\ncomputation time. Furthermore, through numerous ablation studies, we\nsystematically verify that not only does DARTS correctly upweight operations\nduring its supernet phrase, but also gradually improves resulting discrete\ncells up to 30x more efficiently than random search, suggesting DARTS is\nsurprisingly an effective tool for improving architectures in RL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yingjie Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingyou Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1\">John D. Co-Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Daiyi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Summer Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1\">Eugene Brevdo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Object Detection for Autonomous Driving: A Survey. (arXiv:2106.10823v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10823","description":"<p>Autonomous driving is regarded as one of the most promising remedies to\nshield human beings from severe crashes. To this end, 3D object detection\nserves as the core basis of perception stack especially for the sake of path\nplanning, motion prediction, and collision avoidance etc. Taking a quick glance\nat the progress we have made, we attribute challenges to visual appearance\nrecovery in the absence of depth information from images, representation\nlearning from partially occluded unstructured point clouds, and semantic\nalignments over heterogeneous features from cross modalities. Despite existing\nefforts, 3D object detection for autonomous driving is still in its infancy.\nRecently, a large body of literature have been investigated to address this 3D\nvision task. Nevertheless, rather a few investigations have looked into\ncollecting and structuring this growing knowledge. We therefore aim to fill\nthis gap in a comprehensive survey, encompassing all the main concerns\nincluding sensors, datasets, performance metrics and the recent\nstate-of-the-art detection methods, together with their pros and cons.\nFurthermore, we provide quantitative comparisons with the state of the art. A\ncase study on fifteen selected representative methods is presented, involved\nwith runtime analysis, error analysis, and robustness analysis. Finally, we\nprovide concluding remarks after an in-depth analysis of the surveyed works and\nidentify promising directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Top-Down Just Noticeable Difference Estimation of Natural Images. (arXiv:2108.05058v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.05058","description":"<p>Just noticeable difference (JND) of natural images refers to the maximum\npixel intensity change magnitude that typical human visual system (HVS) cannot\nperceive. Existing efforts on JND estimation mainly dedicate to modeling the\ndiverse masking effects in either/both spatial or/and frequency domains, and\nthen fusing them into an overall JND estimate. In this work, we turn to a\ndramatically different way to address this problem with a top-down design\nphilosophy. Instead of explicitly formulating and fusing different masking\neffects in a bottom-up way, the proposed JND estimation model dedicates to\nfirst predicting a critical perceptual lossless (CPL) counterpart of the\noriginal image and then calculating the difference map between the original\nimage and the predicted CPL image as the JND map. We conduct subjective\nexperiments to determine the critical points of 500 images and find that the\ndistribution of cumulative normalized KLT coefficient energy values over all\n500 images at these critical points can be well characterized by a Weibull\ndistribution. Given a testing image, its corresponding critical point is\ndetermined by a simple weighted average scheme where the weights are determined\nby a fitted Weibull distribution function. The performance of the proposed JND\nmodel is evaluated explicitly with direct JND prediction and implicitly with\ntwo applications including JND-guided noise injection and JND-guided image\ncompression. Experimental results have demonstrated that our proposed JND model\ncan achieve better performance than several latest JND models. In addition, we\nalso compare the proposed JND model with existing visual difference predicator\n(VDP) metrics in terms of the capability in distortion detection and\ndiscrimination. The results indicate that our JND model also has a good\nperformance in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiuping Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhentao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_F/0/1/0/all/0/1\">Feng Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ground Visual Objects for Visual Dialog. (arXiv:2109.06013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06013","description":"<p>Visual dialog is challenging since it needs to answer a series of coherent\nquestions based on understanding the visual environment. How to ground related\nvisual objects is one of the key problems. Previous studies utilize the\nquestion and history to attend to the image and achieve satisfactory\nperformance, however these methods are not sufficient to locate related visual\nobjects without any guidance. The inappropriate grounding of visual objects\nprohibits the performance of visual dialog models. In this paper, we propose a\nnovel approach to Learn to Ground visual objects for visual dialog, which\nemploys a novel visual objects grounding mechanism where both prior and\nposterior distributions over visual objects are used to facilitate visual\nobjects grounding. Specifically, a posterior distribution over visual objects\nis inferred from both context (history and questions) and answers, and it\nensures the appropriate grounding of visual objects during the training\nprocess. Meanwhile, a prior distribution, which is inferred from context only,\nis used to approximate the posterior distribution so that appropriate visual\nobjects can be grounded even without answers during the inference process.\nExperimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our\napproach improves the previous strong models in both generative and\ndiscriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How much human-like visual experience do current self-supervised learning algorithms need in order to achieve human-level object recognition?. (arXiv:2109.11523v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11523","description":"<p>This paper addresses a fundamental question: how good are our current\nself-supervised visual representation learning algorithms relative to humans?\nMore concretely, how much \"human-like\" natural visual experience would these\nalgorithms need in order to reach human-level performance in a complex,\nrealistic visual object recognition task such as ImageNet? Using a scaling\nexperiment, here we estimate that the answer is several orders of magnitude\nlonger than a human lifetime: typically on the order of a million to a billion\nyears of natural visual experience (depending on the algorithm used). We obtain\neven larger estimates for achieving human-level performance in ImageNet-derived\nrobustness benchmarks. The exact values of these estimates are sensitive to\nsome underlying assumptions, however even in the most optimistic scenarios they\nremain orders of magnitude larger than a human lifetime. We discuss the main\ncaveats surrounding our estimates and the implications of these surprising\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Splits: Improved K-Means Clustering Algorithm to Automatically Detect the Number of Clusters. (arXiv:2110.04660v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04660","description":"<p>This paper introduces k-splits, an improved hierarchical algorithm based on\nk-means to cluster data without prior knowledge of the number of clusters.\nK-splits starts from a small number of clusters and uses the most significant\ndata distribution axis to split these clusters incrementally into better fits\nif needed. Accuracy and speed are two main advantages of the proposed method.\nWe experiment on six synthetic benchmark datasets plus two real-world datasets\nMNIST and Fashion-MNIST, to prove that our algorithm has excellent accuracy in\nfinding the correct number of clusters under different conditions. We also show\nthat k-splits is faster than similar methods and can even be faster than the\nstandard k-means in lower dimensions. Finally, we suggest using k-splits to\nuncover the exact position of centroids and then input them as initial points\nto the k-means algorithm to fine-tune the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1\">Seyed Omid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhor_A/0/1/0/all/0/1\">Ahmad Kalhor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodaghi_H/0/1/0/all/0/1\">Hossein Bodaghi</a> (University of Tehran, College of Engineering, School of Electrical and Computer Engineering, Tehran, Iran)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. (arXiv:2110.10832v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.10832","description":"<p>In Domain Generalization (DG) settings, models trained independently on a\ngiven set of training domains have notoriously chaotic performance on\ndistribution shifted test domains, and stochasticity in optimization (e.g.\nseed) plays a big role. This makes deep learning models unreliable in real\nworld settings. We first show that this chaotic behavior exists even along the\ntraining optimization trajectory of a single model, and propose a simple model\naveraging protocol that both significantly boosts domain generalization and\ndiminishes the impact of stochasticity by improving the rank correlation\nbetween the in-domain validation accuracy and out-domain test accuracy, which\nis crucial for reliable early stopping. Taking advantage of our observation, we\nshow that instead of ensembling unaveraged models (that is typical in\npractice), ensembling moving average models (EoA) from independent runs further\nboosts performance. We theoretically explain the boost in performance of\nensembling and model averaging by adapting the well known Bias-Variance\ntrade-off to the domain generalization setting. On the DomainBed benchmark,\nwhen using a pre-trained ResNet-50, this ensemble of averages achieves an\naverage of $68.0\\%$, beating vanilla ERM (w/o averaging/ensembling) by $\\sim\n4\\%$, and when using a pre-trained RegNetY-16GF, achieves an average of\n$76.6\\%$, beating vanilla ERM by $6\\%$. Our code is available at\n\\url{https://github.com/salesforce/ensemble-of-averages}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1\">Devansh Arpit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Deep Representation with Energy-Based Self-Expressiveness for Subspace Clustering. (arXiv:2110.15037v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.15037","description":"<p>Deep subspace clustering has attracted increasing attention in recent years.\nAlmost all the existing works are required to load the whole training data into\none batch for learning the self-expressive coefficients in the framework of\ndeep learning. Although these methods achieve promising results, such a\nlearning fashion severely prevents from the usage of deeper neural network\narchitectures (e.g., ResNet), leading to the limited representation abilities\nof the models. In this paper, we propose a new deep subspace clustering\nframework, motivated by the energy-based models. In contrast to previous\napproaches taking the weights of a fully connected layer as the self-expressive\ncoefficients, we propose to learn an energy-based network to obtain the\nself-expressive coefficients by mini-batch training. By this means, it is no\nlonger necessary to load all data into one batch for learning, and it thus\nbecomes a reality that we can utilize deeper neural network models for subspace\nclustering. Considering the powerful representation ability of the recently\npopular self-supervised learning, we attempt to leverage self-supervised\nrepresentation learning to learn the dictionary. Finally, we propose a joint\nframework to learn both the self-expressive coefficients and dictionary\nsimultaneously, and train the model in an end-to-end manner. The experiments\nare performed on three publicly available datasets, and extensive experimental\nresults demonstrate our method can significantly outperform the other related\napproaches. For instance, on the three datasets, our method can averagely\nachieve $13.8\\%$, $15.4\\%$, $20.8\\%$ improvements in terms of Accuracy, NMI,\nand ARI over SENet which is proposed very recently and obtains the second best\nresults in the experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoren Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion. (arXiv:2111.14690v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14690","description":"<p>A typical pipeline for multi-object tracking (MOT) is to use a detector for\nobject localization, and following re-identification (re-ID) for object\nassociation. This pipeline is partially motivated by recent progress in both\nobject detection and re-ID, and partially motivated by biases in existing\ntracking datasets, where most objects tend to have distinguishing appearance\nand re-ID models are sufficient for establishing associations. In response to\nsuch bias, we would like to re-emphasize that methods for multi-object tracking\nshould also work when object appearance is not sufficiently discriminative. To\nthis end, we propose a large-scale dataset for multi-human tracking, where\nhumans have similar appearance, diverse motion and extreme articulation. As the\ndataset contains mostly group dancing videos, we name it \"DanceTrack\". We\nexpect DanceTrack to provide a better platform to develop more MOT algorithms\nthat rely less on visual discrimination and depend more on motion analysis. We\nbenchmark several state-of-the-art trackers on our dataset and observe a\nsignificant performance drop on DanceTrack when compared against existing\nbenchmarks. The dataset, project code and competition server are released at:\n\\url{https://github.com/DanceTrack}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jinkun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Strong Scaling Through Burst Parallel Training. (arXiv:2112.10065v3 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2112.10065","description":"<p>As emerging deep neural network (DNN) models continue to grow in size, using\nlarge GPU clusters to train DNNs is becoming an essential requirement to\nachieving acceptable training times. In this paper, we consider the case where\nfuture increases in cluster size will cause the global batch size that can be\nused to train models to reach a fundamental limit: beyond a certain point,\nlarger global batch sizes cause sample efficiency to degrade, increasing\noverall time to accuracy. As a result, to achieve further improvements in\ntraining performance, we must instead consider \"strong scaling\" strategies that\nhold the global batch size constant and allocate smaller batches to each GPU.\nUnfortunately, this makes it significantly more difficult to use cluster\nresources efficiently. We present DeepPool, a system that addresses this\nefficiency challenge through two key ideas. First, burst parallelism allocates\nlarge numbers of GPUs to foreground jobs in bursts to exploit the unevenness in\nparallelism across layers. Second, GPU multiplexing prioritizes throughput for\nforeground training jobs, while packing in background training jobs to reclaim\nunderutilized GPU resources, thereby improving cluster-wide utilization.\nTogether, these two ideas enable DeepPool to deliver a 1.2 - 2.3x improvement\nin total cluster throughput over standard data parallelism with a single task\nwhen the cluster scale is large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seo Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_J/0/1/0/all/0/1\">Joshua Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1\">Mohammad Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belay_A/0/1/0/all/0/1\">Adam Belay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer with Deformable Attention. (arXiv:2201.00520v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00520","description":"<p>Transformers have recently shown superior performances on various vision\ntasks. The large, sometimes even global, receptive field endows Transformer\nmodels with higher representation power over their CNN counterparts.\nNevertheless, simply enlarging receptive field also gives rise to several\nconcerns. On the one hand, using dense attention e.g., in ViT, leads to\nexcessive memory and computational cost, and features can be influenced by\nirrelevant parts which are beyond the region of interests. On the other hand,\nthe sparse attention adopted in PVT or Swin Transformer is data agnostic and\nmay limit the ability to model long range relations. To mitigate these issues,\nwe propose a novel deformable self-attention module, where the positions of key\nand value pairs in self-attention are selected in a data-dependent way. This\nflexible scheme enables the self-attention module to focus on relevant regions\nand capture more informative features. On this basis, we present Deformable\nAttention Transformer, a general backbone model with deformable attention for\nboth image classification and dense prediction tasks. Extensive experiments\nshow that our models achieve consistently improved results on comprehensive\nbenchmarks. Code is available at https://github.com/LeapLabTHU/DAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhuofan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Erran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jacobian Computation for Cumulative B-Splines on SE(3) and Application to Continuous-Time Object Tracking. (arXiv:2201.10602v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10602","description":"<p>In this paper we propose a method that estimates the $SE(3)$ continuous\ntrajectories (orientation and translation) of the dynamic rigid objects present\nin a scene, from multiple RGB-D views. Specifically, we fit the object\ntrajectories to cumulative B-Splines curves, which allow us to interpolate, at\nany intermediate time stamp, not only their poses but also their linear and\nangular velocities and accelerations. Additionally, we derive in this work the\nanalytical $SE(3)$ Jacobians needed by the optimization, being applicable to\nany other approach that uses this type of curves. To the best of our knowledge\nthis is the first work that proposes 6-DoF continuous-time object tracking,\nwhich we endorse with significant computational cost reduction thanks to our\nanalytical derivations. We evaluate our proposal in synthetic data and in a\npublic benchmark, showing competitive results in localization and significant\nimprovements in velocity estimation in comparison to discrete-time approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirado_J/0/1/0/all/0/1\">Javier Tirado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12114","description":"<p>Attention mechanisms are dominating the explainability of deep models. They\nproduce probability distributions over the input, which are widely deemed as\nfeature-importance indicators. However, in this paper, we find one critical\nlimitation in attention explanations: weakness in identifying the polarity of\nfeature impact. This would be somehow misleading -- features with higher\nattention weights may not faithfully contribute to model predictions; instead,\nthey can impose suppression effects. With this finding, we reflect on the\nexplainability of current attention-based techniques, such as\nAttentio$\\odot$Gradient and LRP-based attention explanations. We first propose\nan actionable diagnostic methodology (henceforth faithfulness violation test)\nto measure the consistency between explanation weights and the impact polarity.\nThrough the extensive experiments, we then show that most tested explanation\nmethods are unexpectedly hindered by the faithfulness violation issue,\nespecially the raw attention. Empirical analyses on the factors affecting\nviolation issues further provide useful observations for adopting explanation\nmethods in attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices. (arXiv:2203.13436v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13436","description":"<p>In this paper, we consider the problem of real-time video-based facial\nemotion analytics, namely, facial expression recognition, prediction of valence\nand arousal and detection of action unit points. We propose the novel\nframe-level emotion recognition algorithm by extracting facial features with\nthe single EfficientNet model pre-trained on AffectNet. As a result, our\napproach may be implemented even for video analytics on mobile devices.\nExperimental results for the large scale Aff-Wild2 database from the third\nAffective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our\nsimple model is significantly better when compared to the VggFace baseline. In\nparticular, our method is characterized by 0.15-0.2 higher performance measures\nfor validation sets in uni-task Expression Classification, Valence-Arousal\nEstimation and Expression Classification. Due to simplicity, our approach may\nbe considered as a new baseline for all four sub-challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey V. Savchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stack operation of tensor networks. (arXiv:2203.16338v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.16338","description":"<p>The tensor network, as a facterization of tensors, aims at performing the\noperations that are common for normal tensors, such as addition, contraction\nand stacking. However, due to its non-unique network structure, only the tensor\nnetwork contraction is so far well defined. In this paper, we propose a\nmathematically rigorous definition for the tensor network stack approach, that\ncompress a large amount of tensor networks into a single one without changing\ntheir structures and configurations. We illustrate the main ideas with the\nmatrix product states based machine learning as an example. Our results are\ncompared with the for loop and the efficient coding method on both CPU and GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Erping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_L/0/1/0/all/0/1\">L. K. Ang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Compositional Consistency for Video Question Answering. (arXiv:2204.07190v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07190","description":"<p>Recent video question answering benchmarks indicate that state-of-the-art\nmodels struggle to answer compositional questions. However, it remains unclear\nwhich types of compositional reasoning cause models to mispredict. Furthermore,\nit is difficult to discern whether models arrive at answers using compositional\nreasoning or by leveraging data biases. In this paper, we develop a question\ndecomposition engine that programmatically deconstructs a compositional\nquestion into a directed acyclic graph of sub-questions. The graph is designed\nsuch that each parent question is a composition of its children. We present\nAGQA-Decomp, a benchmark containing $2.3M$ question graphs, with an average of\n$11.49$ sub-questions per graph, and $4.55M$ total new sub-questions. Using\nquestion graphs, we evaluate three state-of-the-art models with a suite of\nnovel compositional consistency metrics. We find that models either cannot\nreason correctly through most compositions or are reliant on incorrect\nreasoning to reach answers, frequently contradicting themselves or achieving\nhigh accuracies when failing at intermediate reasoning steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Mona Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gul_M/0/1/0/all/0/1\">Mustafa Omer Gul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_E/0/1/0/all/0/1\">Eva Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grunde_McLaughlin_M/0/1/0/all/0/1\">Madeleine Grunde-McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1\">Ranjay Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawala_M/0/1/0/all/0/1\">Maneesh Agrawala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imposing Consistency for Optical Flow Estimation. (arXiv:2204.07262v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07262","description":"<p>Imposing consistency through proxy tasks has been shown to enhance\ndata-driven learning and enable self-supervision in various tasks. This paper\nintroduces novel and effective consistency strategies for optical flow\nestimation, a problem where labels from real-world data are very challenging to\nderive. More specifically, we propose occlusion consistency and zero forcing in\nthe forms of self-supervised learning and transformation consistency in the\nform of semi-supervised learning. We apply these consistency techniques in a\nway that the network model learns to describe pixel-level motions better while\nrequiring no additional annotations. We demonstrate that our consistency\nstrategies applied to a strong baseline network model using the original\ndatasets and labels provide further improvements, attaining the\nstate-of-the-art results on the KITTI-2015 scene flow benchmark in the\nnon-stereo category. Our method achieves the best foreground accuracy (4.33% in\nFl-all) over both the stereo and non-stereo categories, even though using only\nmonocular image inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisoo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jamie Menjay Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning. (arXiv:2204.08499v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.08499","description":"<p>Coreset selection, which aims to select a subset of the most informative\ntraining samples, is a long-standing learning problem that can benefit many\ndownstream tasks such as data-efficient learning, continual learning, neural\narchitecture search, active learning, etc. However, many existing coreset\nselection methods are not designed for deep learning, which may have high\ncomplexity and poor generalization performance. In addition, the recently\nproposed methods are evaluated on models, datasets, and settings of different\ncomplexities. To advance the research of coreset selection in deep learning, we\ncontribute a comprehensive code library, namely DeepCore, and provide an\nempirical study on popular coreset selection methods on CIFAR10 and ImageNet\ndatasets. Extensive experiments on CIFAR10 and ImageNet datasets verify that,\nalthough various methods have advantages in certain experiment settings, random\nselection is still a strong baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yanbing Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation. (arXiv:2204.10762v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10762","description":"<p>A high-resolution network exhibits remarkable capability in extracting\nmulti-scale features for human pose estimation, but fails to capture long-range\ninteractions between joints and has high computational complexity. To address\nthese problems, we present a Dynamic lightweight High-Resolution Network\n(Dite-HRNet), which can efficiently extract multi-scale contextual information\nand model long-range spatial dependency for human pose estimation.\nSpecifically, we propose two methods, dynamic split convolution and adaptive\ncontext modeling, and embed them into two novel lightweight blocks, which are\nnamed dynamic multi-scale context block and dynamic global context block. These\ntwo blocks, as the basic component units of our Dite-HRNet, are specially\ndesigned for the high-resolution networks to make full use of the parallel\nmulti-resolution architecture. Experimental results show that the proposed\nnetwork achieves superior performance on both COCO and MPII human pose\nestimation datasets, surpassing the state-of-the-art lightweight networks. Code\nis available at: https://github.com/ZiyiZhang27/Dite-HRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation. (arXiv:2204.12484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12484","description":"<p>Although no specific domain knowledge is considered in the design, plain\nvision transformers have shown excellent performance in visual recognition\ntasks. However, little effort has been made to reveal the potential of such\nsimple structures for pose estimation tasks. In this paper, we show the\nsurprisingly good capabilities of plain vision transformers for pose estimation\nfrom various aspects, namely simplicity in model structure, scalability in\nmodel size, flexibility in training paradigm, and transferability of knowledge\nbetween models, through a simple baseline model called ViTPose. Specifically,\nViTPose employs plain and non-hierarchical vision transformers as backbones to\nextract features for a given person instance and a lightweight decoder for pose\nestimation. It can be scaled up from 100M to 1B parameters by taking the\nadvantages of the scalable model capacity and high parallelism of transformers,\nsetting a new Pareto front between throughput and performance. Besides, ViTPose\nis very flexible regarding the attention type, input resolution, pre-training\nand finetuning strategy, as well as dealing with multiple pose tasks. We also\nempirically demonstrate that the knowledge of large ViTPose models can be\neasily transferred to small ones via a simple knowledge token. Experimental\nresults show that our basic ViTPose model outperforms representative methods on\nthe challenging MS COCO Keypoint Detection benchmark, while the largest model\nsets a new state-of-the-art. The code and models are available at\nhttps://github.com/ViTAE-Transformer/ViTPose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.13170","description":"<p>In Federated Learning (FL), a number of clients or devices collaborate to\ntrain a model without sharing their data. Models are optimized locally at each\nclient and further communicated to a central hub called the server for\naggregation. While FL serves as an attractive decentralized training paradigm,\nheterogeneity amongst clients' data can cause the local optimization to drift\naway with respect to the global objective. In order to estimate and therefore\nremove this drift, variance reduction techniques have been incorporated into FL\noptimization recently. However, these approaches inaccurately estimate the\nclients' drift and ultimately fail to remove it properly. In this work, we\naddress this challenge by introducing an adaptive algorithm that efficiently\nreduces clients' drift. In this work, we propose an adaptive algorithm that\naccurately estimates drift across clients. Further, unlike previous works, our\napproach uses less or the same level of communication bandwidth, compute or\nmemory. Additionally, our proposed methodology induces stability by\nconstraining the norm of estimates for client drift, making it more practical\nfor large scale FL settings. Experimental findings demonstrate that the\nproposed algorithm converges significantly faster and achieves higher accuracy\ncompared to the baselines across various benchmarks for FL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varno_F/0/1/0/all/0/1\">Farshid Varno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saghayi_M/0/1/0/all/0/1\">Marzie Saghayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafiee_L/0/1/0/all/0/1\">Laya Rafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sharut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1\">Stan Matwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaei_M/0/1/0/all/0/1\">Mohammad Havaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network. (arXiv:2205.01550v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01550","description":"<p>In recent years, with the development of computing resources and LiDAR, point\ncloud semantic segmentation has attracted many researchers. For the sparsity of\npoint clouds, although there is already a way to deal with sparse convolution,\nmulti-scale features are not considered. In this letter, we propose a feature\nextraction module based on multi-scale sparse convolution and a feature\nselection module based on channel attention and build a point cloud\nsegmentation network framework based on this. By introducing multi-scale sparse\nconvolution, the network could capture richer feature information based on\nconvolution kernels with different sizes, improving the segmentation result of\npoint cloud segmentation. Experimental results on Stanford large-scale 3-D\nIndoor Spaces(S3DIS) dataset and outdoor dataset(SemanticKITTI), demonstrate\neffectiveness and superiority of the proposed mothod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yunzheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance. (arXiv:2205.05677v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05677","description":"<p>Marker-less monocular 3D human motion capture (MoCap) with scene interactions\nis a challenging research topic relevant for extended reality, robotics and\nvirtual avatar generation. Due to the inherent depth ambiguity of monocular\nsettings, 3D motions captured with existing methods often contain severe\nartefacts such as incorrect body-scene inter-penetrations, jitter and body\nfloating. To tackle these issues, we propose HULC, a new approach for 3D human\nMoCap which is aware of the scene geometry. HULC estimates 3D poses and dense\nbody-environment surface contacts for improved 3D localisations, as well as the\nabsolute scale of the subject. Furthermore, we introduce a 3D pose trajectory\noptimisation based on a novel pose manifold sampling that resolves erroneous\nbody-environment inter-penetrations. Although the proposed method requires less\nstructured inputs compared to existing scene-aware monocular MoCap algorithms,\nit produces more physically-plausible poses: HULC significantly and\nconsistently outperforms the existing approaches in various experiments and on\ndifferent metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BronchusNet: Region and Structure Prior Embedded Representation Learning for Bronchus Segmentation and Classification. (arXiv:2205.06947v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.06947","description":"<p>CT-based bronchial tree analysis plays an important role in the\ncomputer-aided diagnosis for respiratory diseases, as it could provide\nstructured information for clinicians. The basis of airway analysis is\nbronchial tree reconstruction, which consists of bronchus segmentation and\nclassification. However, there remains a challenge for accurate bronchial\nanalysis due to the individual variations and the severe class imbalance. In\nthis paper, we propose a region and structure prior embedded framework named\nBronchusNet to achieve accurate segmentation and classification of bronchial\nregions in CT images. For bronchus segmentation, we propose an adaptive hard\nregion-aware UNet that incorporates multi-level prior guidance of hard\npixel-wise samples in the general Unet segmentation network to achieve better\nhierarchical feature learning. For the classification of bronchial branches, we\npropose a hybrid point-voxel graph learning module to fully exploit bronchial\nstructure priors and to support simultaneous feature interactions across\ndifferent branches. To facilitate the study of bronchial analysis, we\ncontribute~\\textbf{BRSC}: an open-access benchmark of \\textbf{BR}onchus imaging\nanalysis with high-quality pixel-wise \\textbf{S}egmentation masks and the\n\\textbf{C}lass of bronchial segments. Experimental results on BRSC show that\nour proposed method not only achieves the state-of-the-art performance for\nbinary segmentation of bronchial region but also exceeds the best existing\nmethod on bronchial branches classification by 6.9\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_H/0/1/0/all/0/1\">Haifan Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haofeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Hong Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation. (arXiv:2205.09853v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09853","description":"<p>Video prediction is a challenging task. The quality of video frames from\ncurrent state-of-the-art (SOTA) generative models tends to be poor and\ngeneralization beyond the training data is difficult. Furthermore, existing\nprediction frameworks are typically not capable of simultaneously handling\nother video-related tasks such as unconditional generation or interpolation. In\nthis work, we devise a general-purpose framework called Masked Conditional\nVideo Diffusion (MCVD) for all of these video synthesis tasks using a\nprobabilistic conditional score-based denoising diffusion model, conditioned on\npast and/or future frames. We train the model in a manner where we randomly and\nindependently mask all the past frames or all the future frames. This novel but\nstraightforward setup allows us to train a single model that is capable of\nexecuting a broad range of video tasks, specifically: future/past prediction --\nwhen only future/past frames are masked; unconditional generation -- when both\npast and future frames are masked; and interpolation -- when neither past nor\nfuture frames are masked. Our experiments show that this approach can generate\nhigh-quality frames for diverse types of videos. Our MCVD models are built from\nsimple non-recurrent 2D-convolutional architectures, conditioning on blocks of\nframes and generating blocks of frames. We generate videos of arbitrary lengths\nautoregressively in a block-wise manner. Our approach yields SOTA results\nacross standard video prediction and interpolation benchmarks, with computation\ntimes for training models measured in 1-12 days using $\\le$ 4 GPUs.\nhttps://mask-cond-video-diffusion.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jolicoeur_Martineau_A/0/1/0/all/0/1\">Alexia Jolicoeur-Martineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EXPANSE: A Deep Continual / Progressive Learning System for Deep Transfer Learning. (arXiv:2205.10356v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.10356","description":"<p>Deep transfer learning techniques try to tackle the limitations of deep\nlearning, the dependency on extensive training data and the training costs, by\nreusing obtained knowledge. However, the current DTL techniques suffer from\neither catastrophic forgetting dilemma (losing the previously obtained\nknowledge) or overly biased pre-trained models (harder to adapt to target data)\nin finetuning pre-trained models or freezing a part of the pre-trained model,\nrespectively. Progressive learning, a sub-category of DTL, reduces the effect\nof the overly biased model in the case of freezing earlier layers by adding a\nnew layer to the end of a frozen pre-trained model. Even though it has been\nsuccessful in many cases, it cannot yet handle distant source and target data.\nWe propose a new continual/progressive learning approach for deep transfer\nlearning to tackle these limitations. To avoid both catastrophic forgetting and\noverly biased-model problems, we expand the pre-trained model by expanding\npre-trained layers (adding new nodes to each layer) in the model instead of\nonly adding new layers. Hence the method is named EXPANSE. Our experimental\nresults confirm that we can tackle distant source and target data using this\ntechnique. At the same time, the final model is still valid on the source data,\nachieving a promising deep continual learning approach. Moreover, we offer a\nnew way of training deep learning models inspired by the human education\nsystem. We termed this two-step training: learning basics first, then adding\ncomplexities and uncertainties. The evaluation implies that the two-step\ntraining extracts more meaningful features and a finer basin on the error\nsurface since it can achieve better accuracy in comparison to regular training.\nEXPANSE (model expansion and two-step training) is a systematic continual\nlearning approach applicable to different problems and DL models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iman_M/0/1/0/all/0/1\">Mohammadreza Iman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1\">John A. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khaled Rasheed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branch_R/0/1/0/all/0/1\">Robert M. Branch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Omnidirectional Vision: A Survey and New Perspectives. (arXiv:2205.10468v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10468","description":"<p>Omnidirectional image (ODI) data is captured with a 360x180 field-of-view,\nwhich is much wider than the pinhole cameras and contains richer spatial\ninformation than the conventional planar images. Accordingly, omnidirectional\nvision has attracted booming attention due to its more advantageous performance\nin numerous applications, such as autonomous driving and virtual reality. In\nrecent years, the availability of customer-level 360 cameras has made\nomnidirectional vision more popular, and the advance of deep learning (DL) has\nsignificantly sparked its research and applications. This paper presents a\nsystematic and comprehensive review and analysis of the recent progress in DL\nmethods for omnidirectional vision. Our work covers four main contents: (i) An\nintroduction to the principle of omnidirectional imaging, the convolution\nmethods on the ODI, and datasets to highlight the differences and difficulties\ncompared with the 2D planar image data; (ii) A structural and hierarchical\ntaxonomy of the DL methods for omnidirectional vision; (iii) A summarization of\nthe latest novel learning strategies and applications; (iv) An insightful\ndiscussion of the challenges and open problems by highlighting the potential\nresearch directions to trigger more research in the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_H/0/1/0/all/0/1\">Hao Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zidong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinjing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haotian Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yucheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeper vs Wider: A Revisit of Transformer Configuration. (arXiv:2205.10505v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.10505","description":"<p>Transformer-based models have delivered impressive results on many tasks,\nparticularly vision and language tasks. In many model training situations,\nconventional configurations are typically adopted. For example, we often set\nthe base model with hidden dimensions (i.e. model width) to be 768 and the\nnumber of transformer layers (i.e. model depth) to be 12. In this paper, we\nrevisit these conventional configurations. Through theoretical analysis and\nexperimental evaluation, we show that the masked autoencoder is effective in\nalleviating the over-smoothing issue in deep transformer training. Based on\nthis finding, we propose Bamboo, an idea of using deeper and narrower\ntransformer configurations, for masked autoencoder training. On ImageNet, with\nsuch a simple change in configuration, re-designed model achieves 87.1% top-1\naccuracy and outperforms SoTA models like MAE and BEiT. On language tasks,\nre-designed model outperforms BERT with default setting by 1.1 points on\naverage, on GLUE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianghai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10667","description":"<p>Traditionally, extracting patterns from eye movement data relies on\nstatistics of different macro-events such as fixations and saccades. This\nrequires an additional preprocessing step to separate the eye movement\nsubtypes, often with a number of parameters on which the classification results\ndepend. Besides that, definitions of such macro events are formulated in\ndifferent ways by different researchers.\n</p>\n<p>We propose an application of a new class of features to the quantitative\nanalysis of personal eye movement trajectories structure. This new class of\nfeatures based on algebraic topology allows extracting patterns from different\nmodalities of gaze such as time series of coordinates and amplitudes, heatmaps,\nand point clouds in a unified way at all scales from micro to macro. We\nexperimentally demonstrate the competitiveness of the new class of features\nwith the traditional ones and their significant synergy while being used\ntogether for the person authentication task on the recently published eye\nmovement trajectories dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onuchin_A/0/1/0/all/0/1\">Arsenii Onuchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachan_O/0/1/0/all/0/1\">Oleg Kachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners. (arXiv:2205.10747v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10747","description":"<p>The goal of this work is to build flexible video-language models that can\ngeneralize to various video-to-text tasks from few examples, such as\ndomain-specific captioning, question answering, and future event prediction.\nExisting few-shot video-language learners focus exclusively on the encoder,\nresulting in the absence of a video-to-text decoder to handle generative tasks.\nVideo captioners have been pretrained on large-scale video-language datasets,\nbut they rely heavily on finetuning and lack the ability to generate text for\nunseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language\nLearner via Image and Language models, which demonstrates strong performance on\nfew-shot video-to-text tasks without the necessity of pretraining or finetuning\non any video datasets. We use the image-language models to translate the video\ncontent into frame captions, object, attribute, and event phrases, and compose\nthem into a temporal structure template. We then instruct a language model,\nwith a prompt containing a few in-context examples, to generate a target output\nfrom the composed content. The flexibility of prompting allows the model to\ncapture any form of text input, such as automatic speech recognition (ASR)\ntranscripts. Our experiments demonstrate the power of language models in\nunderstanding videos on a wide variety of video-language tasks, including video\ncaptioning, video question answering, video caption retrieval, and video future\nevent prediction. Especially, on video future event prediction, our few-shot\nmodel significantly outperforms state-of-the-art supervised models trained on\nlarge-scale video datasets. Code and resources are publicly available for\nresearch purposes at https://github.com/MikeWangWZHL/VidIL .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhailong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real Time Detection Free Tracking of Multiple Objects Via Equilibrium Optimizer. (arXiv:2205.10756v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10756","description":"<p>Multiple objects tracking (MOT) is a difficult task, as it usually requires\nspecial hardware and higher computation complexity. In this work, we present a\nnew framework of MOT by using of equilibrium optimizer (EO) algorithm and\nreducing the resolution of the bounding boxes of the objects to solve such\nproblems in the detection free framework. First, in the first frame the target\nobjects are initialized and its size is computed, then its resolution is\nreduced if it is higher than a threshold, and then modeled by their kernel\ncolor histogram to establish a feature model. The Bhattacharya distances\nbetween the histogram of object models and other candidates are used as the\nfitness function to be optimized. Multiple agents are generated by EO,\naccording to the number of the target objects to be tracked. EO algorithm is\nused because of its efficiency and lower computation cost compared to other\nalgorithms in global optimization. Experimental results confirm that EO\nmulti-object tracker achieves satisfying tracking results then other trackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Charef_Khodja_D/0/1/0/all/0/1\">Djemai Charef-Khodja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abida_T/0/1/0/all/0/1\">Toumi Abida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCP: Recurrent Closest Point for Scene Flow Estimation on 3D Point Clouds. (arXiv:2205.11028v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11028","description":"<p>3D motion estimation including scene flow and point cloud registration has\ndrawn increasing interest. Inspired by 2D flow estimation, recent methods\nemploy deep neural networks to construct the cost volume for estimating\naccurate 3D flow. However, these methods are limited by the fact that it is\ndifficult to define a search window on point clouds because of the irregular\ndata structure. In this paper, we avoid this irregularity by a simple yet\neffective method.We decompose the problem into two interlaced stages, where the\n3D flows are optimized point-wisely at the first stage and then globally\nregularized in a recurrent network at the second stage. Therefore, the\nrecurrent network only receives the regular point-wise information as the\ninput. In the experiments, we evaluate the proposed method on both the 3D scene\nflow estimation and the point cloud registration task. For 3D scene flow\nestimation, we make comparisons on the widely used FlyingThings3D and\nKITTIdatasets. For point cloud registration, we follow previous works and\nevaluate the data pairs with large pose and partially overlapping from\nModelNet40. The results show that our method outperforms the previous method\nand achieves a new state-of-the-art performance on both 3D scene flow\nestimation and point cloud registration, which demonstrates the superiority of\nthe proposed zero-order method on irregular point cloud data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaodong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zuozhuo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Network approaches for Analysing Videos of Music Performances. (arXiv:2205.11232v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11232","description":"<p>This paper presents a framework to automate the labelling process for\ngestures in musical performance videos with a 3D Convolutional Neural Network\n(CNN). While this idea was proposed in a previous study, this paper introduces\nseveral novelties: (i) Presents a novel method to overcome the class imbalance\nchallenge and make learning possible for co-existent gestures by batch\nbalancing approach and spatial-temporal representations of gestures. (ii)\nPerforms a detailed study on 7 and 18 categories of gestures generated during\nthe performance (guitar play) of musical pieces that have been video-recorded.\n(iii) Investigates the possibility to use audio features. (iv) Extends the\nanalysis to multiple videos. The novel methods significantly improve the\nperformance of gesture identification by 12 %, when compared to the previous\nwork (51 % in this study over 39 % in previous work). We successfully validate\nthe proposed methods on 7 super classes (72 %), an ensemble of the 18\ngestures/classes, and additional videos (75 %).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Simistira Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1\">Richa Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhipa_P/0/1/0/all/0/1\">Prakash Chandra Chhipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Killian Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visi_F/0/1/0/all/0/1\">Federico Visi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostersjo_S/0/1/0/all/0/1\">Stefan &#xd6;stersj&#xf6;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer: Vit and its Derivatives. (arXiv:2205.11239v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11239","description":"<p>Transformer, an attention-based encoder-decoder architecture, has not only\nrevolutionized the field of natural language processing (NLP), but has also\ndone some pioneering work in the field of computer vision (CV). Compared to\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\nexcellent modeling capabilities to achieve very good performance on several\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\nself-attention mechanism in natural language processing, where word embeddings\nare replaced with patch embeddings.\n</p>\n<p>This paper reviews the derivatives of ViT and the cross-applications of ViT\nwith other fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zujun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}