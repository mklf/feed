{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder. (arXiv:2109.04500v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04500","description":"<p>We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for\nsemantic parsing in task-oriented dialog. Our model consists of an encoder\nnetwork that incrementally builds the semantic parse tree by predicting the\nnon-terminal label and its positions in the linearized tree. At the generation\ntime, the model constructs the semantic parse tree by recursively inserting the\npredicted non-terminal labels at the predicted positions until termination.\nRINE achieves state-of-the-art exact match accuracy on low- and high-resource\nversions of the conversational semantic parsing benchmark TOP (Gupta et al.,\n2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and\ntransition-based parsers. We also show that our model design is applicable to\nnested named entity recognition task, where it performs on par with\nstate-of-the-art approach designed for that task. Finally, we demonstrate that\nour approach is 2-3.5 times faster than the sequence-to-sequence model at\ninference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach. (arXiv:2109.04513v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04513","description":"<p>We present models which complete missing text given transliterations of\nancient Mesopotamian documents, originally written on cuneiform clay tablets\n(2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on\ncontextual cues to manually fill in missing parts in the text in a subjective\nand time-consuming process. We identify that this challenge can be formulated\nas a masked language modelling task, used mostly as a pretraining objective for\ncontextualized language models. Following, we develop several architectures\nfocusing on the Akkadian language, the lingua franca of the time. We find that\ndespite data scarcity (1M tokens) we can achieve state of the art performance\non missing tokens prediction (89% hit@5) using a greedy decoding scheme and\npretraining on data from other languages and different time periods. Finally,\nwe conduct human evaluations showing the applicability of our models in\nassisting experts to transcribe texts in extinct languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saret_B/0/1/0/all/0/1\">Benny Saret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1\">Asaf Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horowitz_W/0/1/0/all/0/1\">Wayne Horowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserman_N/0/1/0/all/0/1\">Nathan Wasserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Morality Frames in Political Tweets using Relational Learning. (arXiv:2109.04535v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04535","description":"<p>Extracting moral sentiment from text is a vital component in understanding\npublic opinion, social movements, and policy decisions. The Moral Foundation\nTheory identifies five moral foundations, each associated with a positive and\nnegative polarity. However, moral sentiment is often motivated by its targets,\nwhich can correspond to individuals or collective entities. In this paper, we\nintroduce morality frames, a representation framework for organizing moral\nattitudes directed at different entities, and come up with a novel and\nhigh-quality annotated dataset of tweets written by US politicians. Then, we\npropose a relational learning model to predict moral attitudes towards entities\nand moral foundations jointly. We do qualitative and quantitative evaluations,\nshowing that moral sentiment towards entities differs highly across political\nideologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_M/0/1/0/all/0/1\">Maria Leonor Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generic resources are what you need: Style transfer tasks without task-specific parallel training data. (arXiv:2109.04543v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04543","description":"<p>Style transfer aims to rewrite a source text in a different target style\nwhile preserving its content. We propose a novel approach to this task that\nleverages generic resources, and without using any task-specific parallel\n(source-target) data outperforms existing unsupervised approaches on the two\nmost popular style transfer tasks: formality transfer and polarity swap. In\npractice, we adopt a multi-step procedure which builds on a generic pre-trained\nsequence-to-sequence model (BART). First, we strengthen the model's ability to\nrewrite by further pre-training BART on both an existing collection of generic\nparaphrases, as well as on synthetic pairs created using a general-purpose\nlexical resource. Second, through an iterative back-translation approach, we\ntrain two models, each in a transfer direction, so that they can provide each\nother with synthetically generated pairs, dynamically in the training process.\nLastly, we let our best reresulting model generate static synthetic pairs to be\nused in a supervised training regime. Besides methodology and state-of-the-art\nresults, a core contribution of this work is a reflection on the nature of the\ntwo tasks we address, and how their differences are highlighted by their\nresponse to our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints. (arXiv:2109.04546v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04546","description":"<p>We study the problem of generating arithmetic math word problems (MWPs) given\na math equation that specifies the mathematical computation and a context that\nspecifies the problem scenario. Existing approaches are prone to generating\nMWPs that are either mathematically invalid or have unsatisfactory language\nquality. They also either ignore the context or require manual specification of\na problem template, which compromises the diversity of the generated MWPs. In\nthis paper, we develop a novel MWP generation approach that leverages i)\npre-trained language models and a context keyword selection model to improve\nthe language quality of the generated MWPs and ii) an equation consistency\nconstraint for math equations to improve the mathematical validity of the\ngenerated MWPs. Extensive quantitative and qualitative experiments on three\nreal-world MWP datasets demonstrate the superior performance of our approach\ncompared to various baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew S. Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeDyT: A General Framework for Multi-Step Event Forecasting via Sequence Modeling on Dynamic Entity Embeddings. (arXiv:2109.04550v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04550","description":"<p>Temporal Knowledge Graphs store events in the form of subjects, relations,\nobjects, and timestamps which are often represented by dynamic heterogeneous\ngraphs. Event forecasting is a critical and challenging task in Temporal\nKnowledge Graph reasoning that predicts the subject or object of an event in\nthe future. To obtain temporal embeddings multi-step away in the future,\nexisting methods learn generative models that capture the joint distribution of\nthe observed events. To reduce the high computation costs, these methods rely\non unrealistic assumptions of independence and approximations in training and\ninference. In this work, we propose SeDyT, a discriminative framework that\nperforms sequence modeling on the dynamic entity embeddings to solve the\nmulti-step event forecasting problem. SeDyT consists of two components: a\nTemporal Graph Neural Network that generates dynamic entity embeddings in the\npast and a sequence model that predicts the entity embeddings in the future.\nCompared with the generative models, SeDyT does not rely on any heuristic-based\nprobability model and has low computation complexity in both training and\ninference. SeDyT is compatible with most Temporal Graph Neural Networks and\nsequence models. We also design an efficient training method that trains the\ntwo components in one gradient descent propagation. We evaluate the performance\nof SeDyT on five popular datasets. By combining temporal Graph Neural Network\nmodels and sequence models, SeDyT achieves an average of 2.4% MRR improvement\nwhen not using the validation set and more than 10% MRR improvement when using\nthe validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongkuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orme_Rogers_J/0/1/0/all/0/1\">James Orme-Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_R/0/1/0/all/0/1\">Rajgopal Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1\">Viktor Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPECTRA: Sparse Structured Text Rationalization. (arXiv:2109.04552v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04552","description":"<p>Selective rationalization aims to produce decisions along with rationales\n(e.g., text highlights or word alignments between two sentences). Commonly,\nrationales are modeled as stochastic binary masks, requiring sampling-based\ngradient estimators, which complicates training and requires careful\nhyperparameter tuning. Sparse attention mechanisms are a deterministic\nalternative, but they lack a way to regularize the rationale extraction (e.g.,\nto control the sparsity of a text highlight or the number of alignments). In\nthis paper, we present a unified framework for deterministic extraction of\nstructured explanations via constrained inference on a factor graph, forming a\ndifferentiable layer. Our approach greatly eases training and rationale\nregularization, generally outperforming previous work on what comes to\nperformance and plausibility of the extracted rationales. We further provide a\ncomparative study of stochastic and deterministic methods for rationale\nextraction for classification and natural language inference tasks, jointly\nassessing their predictive power, quality of the explanations, and model\nvariability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1\">Nuno Miguel Guerreiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subword Mapping and Anchoring across Languages. (arXiv:2109.04556v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04556","description":"<p>State-of-the-art multilingual systems rely on shared vocabularies that\nsufficiently cover all considered languages. To this end, a simple and\nfrequently used approach makes use of subword vocabularies constructed jointly\nover several languages. We hypothesize that such vocabularies are suboptimal\ndue to false positives (identical subwords with different meanings across\nlanguages) and false negatives (different subwords with similar meanings). To\naddress these issues, we propose Subword Mapping and Anchoring across Languages\n(SMALA), a method to construct bilingual subword vocabularies. SMALA extracts\nsubword alignments using an unsupervised state-of-the-art mapping technique and\nuses them to create cross-lingual anchors based on subword similarities. We\ndemonstrate the benefits of SMALA for cross-lingual natural language inference\n(XNLI), where it improves zero-shot transfer to an unseen language without\ntask-specific data, but only by sharing subword embeddings. Moreover, in neural\nmachine translation, we show that joint subword vocabularies obtained with\nSMALA lead to higher BLEU scores on sentences that contain many false positives\nand false negatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_Belis_A/0/1/0/all/0/1\">Andrei Popescu-Belis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling. (arXiv:2109.04562v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04562","description":"<p>Human conversations naturally evolve around different topics and fluently\nmove between them. In research on dialog systems, the ability to actively and\nsmoothly transition to new topics is often ignored. In this paper we introduce\nTIAGE, a new topic-shift aware dialog benchmark constructed utilizing human\nannotations on topic shifts. Based on TIAGE, we introduce three tasks to\ninvestigate different scenarios of topic-shift modeling in dialog settings:\ntopic-shift detection, topic-shift triggered response generation and\ntopic-aware dialog generation. Experiments on these tasks show that the\ntopic-shift signals in TIAGE are useful for topic-shift response generation. On\nthe other hand, dialog systems still struggle to decide when to change topic.\nThis indicates further research is needed in topic-shift aware dialog modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Huiyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copestake_A/0/1/0/all/0/1\">Ann Copestake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speechformer: Reducing Information Loss in Direct Speech Translation. (arXiv:2109.04574v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04574","description":"<p>Transformer-based models have gained increasing popularity achieving\nstate-of-the-art performance in many research fields including speech\ntranslation. However, Transformer's quadratic complexity with respect to the\ninput sequence length prevents its adoption as is with audio signals, which are\ntypically represented by long sequences. Current solutions resort to an initial\nsub-optimal compression based on a fixed sampling of raw audio features.\nTherefore, potentially useful linguistic information is not accessible to\nhigher-level layers in the architecture. To solve this issue, we propose\nSpeechformer, an architecture that, thanks to reduced memory usage in the\nattention layers, avoids the initial lossy compression and aggregates\ninformation only at a higher level according to more informed linguistic\ncriteria. Experiments on three language pairs (en-&gt;de/es/nl) show the efficacy\nof our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and\nof up to 4.0 BLEU in a low resource scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04587","description":"<p>The dominant paradigm for semantic parsing in recent years is to formulate\nparsing as a sequence-to-sequence task, generating predictions with\nauto-regressive sequence decoders. In this work, we explore an alternative\nparadigm. We formulate semantic parsing as a dependency parsing task, applying\ngraph-based decoding techniques developed for syntactic parsing. We compare\nvarious decoding techniques given the same pre-trained Transformer encoder on\nthe TOP dataset, including settings where training data is limited or contains\nonly partially-annotated examples. We find that our graph-based approach is\ncompetitive with sequence decoders on the standard setting, and offers\nsignificant improvements in data efficiency and settings where\npartially-annotated data is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nanjiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Luheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation. (arXiv:2109.04588v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04588","description":"<p>The success of bidirectional encoders using masked language models, such as\nBERT, on numerous natural language processing tasks has prompted researchers to\nattempt to incorporate these pre-trained models into neural machine translation\n(NMT) systems. However, proposed methods for incorporating pre-trained models\nare non-trivial and mainly focus on BERT, which lacks a comparison of the\nimpact that other pre-trained models may have on translation performance. In\nthis paper, we demonstrate that simply using the output (contextualized\nembeddings) of a tailored and suitable bilingual pre-trained language model\n(dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art\ntranslation performance. Moreover, we also propose a stochastic layer selection\napproach and a concept of dual-directional translation model to ensure the\nsufficient utilization of contextualized embeddings. In the case of without\nusing back translation, our best models achieve BLEU scores of 30.45 for En-&gt;De\nand 38.61 for De-&gt;En on the IWSLT'14 dataset, and 31.26 for En-&gt;De and 34.94\nfor De-&gt;En on the WMT'14 dataset, which exceeds all published numbers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-Scale Study of Machine Translation in the Turkic Languages. (arXiv:2109.04593v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04593","description":"<p>Recent advances in neural machine translation (NMT) have pushed the quality\nof machine translation systems to the point where they are becoming widely\nadopted to build competitive systems. However, there is still a large number of\nlanguages that are yet to reap the benefits of NMT. In this paper, we provide\nthe first large-scale case study of the practical application of MT in the\nTurkic language family in order to realize the gains of NMT for Turkic\nlanguages under high-resource to extremely low-resource scenarios. In addition\nto presenting an extensive analysis that identifies the bottlenecks towards\nbuilding competitive systems to ameliorate data scarcity, our study has several\nkey contributions, including, i) a large parallel corpus covering 22 Turkic\nlanguages consisting of common public datasets in combination with new datasets\nof approximately 2 million parallel sentences, ii) bilingual baselines for 26\nlanguage pairs, iii) novel high-quality test sets in three different\ntranslation domains and iv) human evaluation scores. All models, scripts, and\ndata will be released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Anoop Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariev_S/0/1/0/all/0/1\">Sherzod Kariev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyers_F/0/1/0/all/0/1\">Francis Tyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abduraufov_O/0/1/0/all/0/1\">Otabek Abduraufov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajili_M/0/1/0/all/0/1\">Mammad Hajili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_S/0/1/0/all/0/1\">Sardana Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaytbaev_A/0/1/0/all/0/1\">Abror Khaytbaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1\">Antonio Laverghetta Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moydinboyev_B/0/1/0/all/0/1\">Behzodbek Moydinboyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onal_E/0/1/0/all/0/1\">Esra Onal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulatova_S/0/1/0/all/0/1\">Shaxnoza Pulatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Ahsan Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappan_S/0/1/0/all/0/1\">Sriram Chellappan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04600","description":"<p>Temporal grounding aims to predict a time interval of a video clip\ncorresponding to a natural language query input. In this work, we present\nEVOQUER, a temporal grounding framework incorporating an existing text-to-video\ngrounding model and a video-assisted query generation network. Given a query\nand an untrimmed video, the temporal grounding model predicts the target\ninterval, and the predicted video clip is fed into a video translation task by\ngenerating a simplified version of the input query. EVOQUER forms closed-loop\nlearning by incorporating loss functions from both temporal grounding and query\ngeneration serving as feedback. Our experiments on two widely used datasets,\nCharades-STA and ActivityNet, show that EVOQUER achieves promising improvements\nby 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could\nfacilitate error analysis by explaining temporal grounding model behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lulu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jason Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations. (arXiv:2109.04602v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04602","description":"<p>Current language models are usually trained using a self-supervised scheme,\nwhere the main focus is learning representations at the word or sentence level.\nHowever, there has been limited progress in generating useful discourse-level\nrepresentations. In this work, we propose to use ideas from predictive coding\ntheory to augment BERT-style language models with a mechanism that allows them\nto learn suitable discourse-level representations. As a result, our proposed\napproach is able to predict future sentences using explicit top-down\nconnections that operate at the intermediate layers of the network. By\nexperimenting with benchmarks designed to evaluate discourse-related knowledge\nusing pre-trained sentence representations, we demonstrate that our approach\nimproves performance in 6 out of 11 tasks by excelling in discourse\nrelationship detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villa_A/0/1/0/all/0/1\">Andr&#xe9;s Villa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1\">Marcelo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04604","description":"<p>The general goal of text simplification (TS) is to reduce text complexity for\nhuman consumption. This paper investigates another potential use of neural TS:\nassisting machines performing natural language processing (NLP) tasks. We\nevaluate the use of neural TS in two ways: simplifying input texts at\nprediction time and augmenting data to provide machines with additional\ninformation during training. We demonstrate that the latter scenario provides\npositive effects on machine performance on two separate datasets. In\nparticular, the latter use of TS improves the performances of LSTM (1.82-1.98%)\nand SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,\nreal-world relation extraction task. Further, the same setting yields\nimprovements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT\ntext classifier on MNLI, a practical natural language inference dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hoang Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization. (arXiv:2109.04607v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04607","description":"<p>We present IndoBERTweet, the first large-scale pretrained model for\nIndonesian Twitter that is trained by extending a monolingually-trained\nIndonesian BERT model with additive domain-specific vocabulary. We focus in\nparticular on efficient model adaptation under vocabulary mismatch, and\nbenchmark different ways of initializing the BERT embedding layer for new word\ntypes. We find that initializing with the average BERT subword embedding makes\npretraining five times faster, and is more effective than proposed methods for\nvocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploratory Study on Long Dialogue Summarization: What Works and What's Next. (arXiv:2109.04609v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04609","description":"<p>Dialogue summarization helps readers capture salient information from long\nconversations in meetings, interviews, and TV series. However, real-world\ndialogues pose a great challenge to current summarization models, as the\ndialogue length typically exceeds the input limits imposed by recent\ntransformer-based pre-trained models, and the interactive nature of dialogues\nmakes relevant information more context-dependent and sparsely distributed than\nnews articles. In this work, we perform a comprehensive study on long dialogue\nsummarization by investigating three strategies to deal with the lengthy input\nproblem and locate relevant information: (1) extended transformer models such\nas Longformer, (2) retrieve-then-summarize pipeline models with several\ndialogue utterance retrieval methods, and (3) hierarchical dialogue encoding\nmodels such as HMNet. Our experimental results on three long dialogue datasets\n(QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline\nmodels yield the best performance. We also demonstrate that the summary quality\ncan be further improved with a stronger retrieval model and pretraining on\nproper external summarization datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])","link":"http://arxiv.org/abs/2109.04611","description":"<p>Transformer-based rankers have shown state-of-the-art performance. However,\ntheir self-attention operation is mostly unable to process long sequences. One\nof the common approaches to train these rankers is to heuristically select some\nsegments of each document, such as the first segment, as training data.\nHowever, these segments may not contain the query-related parts of documents.\nTo address this problem, we propose query-driven segment selection from long\ndocuments to build training data. The segment selector provides relevant\nsamples with more accurate labels and non-relevant samples which are harder to\nbe predicted. The experimental results show that the basic BERT-based ranker\ntrained with the proposed segment selector significantly outperforms that\ntrained by the heuristically selected segments, and performs equally to the\nstate-of-the-art model with localized self-attention that can process longer\ninput sequences. Our findings open up new direction to design efficient\ntransformer-based rankers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04620","description":"<p>Current approaches to incorporating terminology constraints in machine\ntranslation (MT) typically assume that the constraint terms are provided in\ntheir correct morphological forms. This limits their application to real-world\nscenarios where constraint terms are provided as lemmas. In this paper, we\nintroduce a modular framework for incorporating lemma constraints in neural MT\n(NMT) in which linguistic knowledge and diverse types of NMT models can be\nflexibly applied. It is based on a novel cross-lingual inflection module that\ninflects the target lemma constraints based on the source context. We explore\nlinguistically motivated rule-based and data-driven neural-based inflection\nmodules and design English-German health and English-Lithuanian news test\nsuites to evaluate them in domain adaptation and low-resource MT settings.\nResults show that our rule-based inflection module helps NMT models incorporate\nlemma constraints more accurately than a neural module and outperforms the\nexisting end-to-end approach with lower training costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CINS: Comprehensive Instruction for Few-shot Learning in Task-orientedDialog Systems. (arXiv:2109.04645v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04645","description":"<p>As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema(definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5)is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers. (arXiv:2109.04650v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04650","description":"<p>GPT-3 shows remarkable in-context learning ability of large-scale language\nmodels (LMs) trained on hundreds of billion scale data. Here we address some\nremaining issues less reported by the GPT-3 paper, such as a non-English LM,\nthe performances of different sized models, and the effect of recently\nintroduced prompt optimization on in-context learning. To achieve this, we\nintroduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric\ncorpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA\nwith our training configuration shows state-of-the-art in-context zero-shot and\nfew-shot learning performances on various downstream tasks in Korean. Also, we\nshow the performance benefits of prompt-based learning and demonstrate how it\ncan be integrated into the prompt engineering pipeline. Then we discuss the\npossibility of materializing the No Code AI paradigm by providing AI\nprototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,\nan interactive prompt engineering interface. Lastly, we demonstrate the\npotential of our methods with three successful in-house applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Boseop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyoungSeok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gichang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_D/0/1/0/all/0/1\">Dong Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungju Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonhoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1\">Dongpil Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heungsub Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minyoung Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsub Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Suk Hyun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Taeyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Soyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_N/0/1/0/all/0/1\">Na-Hyeon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1\">Soobin Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+In_S/0/1/0/all/0/1\">Sookyo In</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungduk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hiun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yong Goo Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_D/0/1/0/all/0/1\">Donghoon Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dongju Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Min Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewook Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_I/0/1/0/all/0/1\">Inho Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Woomyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_N/0/1/0/all/0/1\">Nako Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining. (arXiv:2109.04652v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04652","description":"<p>Natural language relies on a finite lexicon to express an unbounded set of\nemerging ideas. One result of this tension is the formation of new\ncompositions, such that existing linguistic units can be combined with emerging\nitems into novel expressions. We develop a framework that exploits the\ncognitive mechanisms of chaining and multimodal knowledge to predict emergent\ncompositional expressions through time. We present the syntactic frame\nextension model (SFEM) that draws on the theory of chaining and knowledge from\n\"percept\", \"concept\", and \"language\" to infer how verbs extend their frames to\nform new compositions with existing and novel nouns. We evaluate SFEM\nrigorously on the 1) modalities of knowledge and 2) categorization models of\nchaining, in a syntactically parsed English corpus over the past 150 years. We\nshow that multimodal SFEM predicts newly emerged verb syntax and arguments\nsubstantially better than competing models using purely linguistic or unimodal\nknowledge. We find support for an exemplar view of chaining as opposed to a\nprototype view and reveal how the joint approach of multimodal chaining may be\nfundamental to the creation of literal and figurative language uses including\nmetaphor and metonymy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation. (arXiv:2109.04653v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04653","description":"<p>Pre-trained language-vision models have shown remarkable performance on the\nvisual question answering (VQA) task. However, most pre-trained models are\ntrained by only considering monolingual learning, especially the resource-rich\nlanguage like English. Training such models for multilingual setups demand high\ncomputing resources and multilingual language-vision dataset which hinders\ntheir application in practice. To alleviate these challenges, we propose a\nknowledge distillation approach to extend an English language-vision model\n(teacher) into an equally effective multilingual and code-mixed model\n(student). Unlike the existing knowledge distillation methods, which only use\nthe output from the last layer of the teacher network for distillation, our\nstudent model learns and imitates the teacher from multiple intermediate layers\n(language and vision encoders) with appropriately designed distillation\nobjectives for incremental knowledge extraction. We also create the large-scale\nmultilingual and code-mixed VQA dataset in eleven different language setups\nconsidering the multiple Indian and European languages. Experimental results\nand in-depth analysis show the effectiveness of the proposed VQA model over the\npre-trained language-vision models on eleven diverse language setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Humair Raj Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dialogue State Tracking via Cross-Task Transfer. (arXiv:2109.04655v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04655","description":"<p>Zero-shot transfer learning for dialogue state tracking (DST) enables us to\nhandle a variety of task-oriented dialogue domains without the expense of\ncollecting in-domain data. In this work, we propose to transfer the\n\\textit{cross-task} knowledge from general question answering (QA) corpora for\nthe zero-shot DST task. Specifically, we propose TransferQA, a transferable\ngenerative QA model that seamlessly combines extractive QA and multi-choice QA\nvia a text-to-text transformer framework, and tracks both categorical slots and\nnon-categorical slots in DST. In addition, we introduce two effective ways to\nconstruct unanswerable questions, namely, negative question sampling and\ncontext truncation, which enable our model to handle \"none\" value slots in the\nzero-shot DST setting. The extensive experiments show that our approaches\nsubstantially improve the existing zero-shot and few-shot results on MultiWoz.\nMoreover, compared to the fully trained baseline on the Schema-Guided Dialogue\ndataset, our approach shows better generalization ability in unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crook_P/0/1/0/all/0/1\">Paul Crook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhenpeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunjoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subba_R/0/1/0/all/0/1\">Rajen Subba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Euphemistic Phrase Detection by Masked Language Model. (arXiv:2109.04666v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04666","description":"<p>It is a well-known approach for fringe groups and organizations to use\neuphemisms -- ordinary-sounding and innocent-looking words with a secret\nmeaning -- to conceal what they are discussing. For instance, drug dealers\noften use \"pot\" for marijuana and \"avocado\" for heroin. From a social media\ncontent moderation perspective, though recent advances in NLP have enabled the\nautomatic detection of such single-word euphemisms, no existing work is capable\nof automatically detecting multi-word euphemisms, such as \"blue dream\"\n(marijuana) and \"black tar\" (heroin). Our paper tackles the problem of\neuphemistic phrase detection without human effort for the first time, as far as\nwe are aware. We first perform phrase mining on a raw text corpus (e.g., social\nmedia posts) to extract quality phrases. Then, we utilize word embedding\nsimilarities to select a set of euphemistic phrase candidates. Finally, we rank\nthose candidates by a masked language model -- SpanBERT. Compared to strong\nbaselines, we report 20-50% higher detection accuracies using our algorithm for\ndetecting euphemistic phrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanzheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1\">Suma Bhat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model. (arXiv:2109.04672v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04672","description":"<p>The transformer-based pre-trained language models have been tremendously\nsuccessful in most of the conventional NLP tasks. But they often struggle in\nthose tasks where numerical understanding is required. Some possible reasons\ncan be the tokenizers and pre-training objectives which are not specifically\ndesigned to learn and preserve numeracy. Here we investigate the ability of\ntext-to-text transfer learning model (T5), which has outperformed its\npredecessors in the conventional NLP tasks, to learn numeracy. We consider four\nnumeracy tasks: numeration, magnitude order prediction, finding minimum and\nmaximum in a series, and sorting. We find that, although T5 models perform\nreasonably well in the interpolation setting, they struggle considerably in the\nextrapolation setting across all four tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization. (arXiv:2109.04673v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04673","description":"<p>Identifying relevant knowledge to be used in conversational systems that are\ngrounded in long documents is critical to effective response generation. We\nintroduce a knowledge identification model that leverages the document\nstructure to provide dialogue-contextualized passage encodings and better\nlocate knowledge relevant to the conversation. An auxiliary loss captures the\nhistory of dialogue-document connections. We demonstrate the effectiveness of\nour model on two document-grounded conversational datasets and provide analyses\nshowing generalization to unseen documents and long dialogue contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo-Ru Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning. (arXiv:2109.04689v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04689","description":"<p>Motivated by suggested question generation in conversational news\nrecommendation systems, we propose a model for generating question-answer pairs\n(QA pairs) with self-contained, summary-centric questions and\nlength-constrained, article-summarizing answers. We begin by collecting a new\ndataset of news articles with questions as titles and pairing them with\nsummaries of varying length. This dataset is used to learn a QA pair generation\nmodel producing summaries as answers that balance brevity with sufficiency\njointly with their corresponding questions. We then reinforce the QA pair\ngeneration process with a differentiable reward function to mitigate exposure\nbias, a common problem in natural language generation. Both automatic metrics\nand human evaluation demonstrate these QA pairs successfully capture the\ncentral gists of the articles and achieve high answer accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04699","description":"<p>While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Graph Neural Networks for Keyphrase Generation. (arXiv:2109.04703v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04703","description":"<p>The encoder-decoder framework achieves state-of-the-art results in keyphrase\ngeneration (KG) tasks by predicting both present keyphrases that appear in the\nsource document and absent keyphrases that do not. However, relying solely on\nthe source document can result in generating uncontrollable and inaccurate\nabsent keyphrases. To address these problems, we propose a novel graph-based\nmethod that can capture explicit knowledge from related references. Our model\nfirst retrieves some document-keyphrases pairs similar to the source document\nfrom a pre-defined index as references. Then a heterogeneous graph is\nconstructed to capture relationships of different granularities between the\nsource document and its references. To guide the decoding process, a\nhierarchical attention and copy mechanism is introduced, which directly copies\nappropriate words from both the source document and its references based on\ntheir relevance and significance. The experimental results on multiple KG\nbenchmarks show that the proposed model achieves significant improvements\nagainst other baseline models, especially with regard to the absent keyphrase\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruijian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables. (arXiv:2109.04705v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04705","description":"<p>Zero-shot translation, directly translating between language pairs unseen in\ntraining, is a promising capability of multilingual neural machine translation\n(NMT). However, it usually suffers from capturing spurious correlations between\nthe output language and language invariant semantics due to the maximum\nlikelihood training objective, leading to poor transfer performance on\nzero-shot translation. In this paper, we introduce a denoising autoencoder\nobjective based on pivot language into traditional training objective to\nimprove the translation accuracy on zero-shot directions. The theoretical\nanalysis from the perspective of latent variables shows that our approach\nactually implicitly maximizes the probability distributions for zero-shot\ndirections. On two benchmark machine translation datasets, we demonstrate that\nthe proposed method is able to effectively eliminate the spurious correlations\nand significantly outperforms state-of-the-art methods with a remarkable\nperformance. Our code is available at https://github.com/Victorwz/zs-nmt-dae.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Aware Meta-learning for Low-Resource Text Classification. (arXiv:2109.04707v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04707","description":"<p>Meta-learning has achieved great success in leveraging the historical learned\nknowledge to facilitate the learning process of the new task. However, merely\nlearning the knowledge from the historical tasks, adopted by current\nmeta-learning algorithms, may not generalize well to testing tasks when they\nare not well-supported by training tasks. This paper studies a low-resource\ntext classification problem and bridges the gap between meta-training and\nmeta-testing tasks by leveraging the external knowledge bases. Specifically, we\npropose KGML to introduce additional representation for each sentence learned\nfrom the extracted sentence-specific knowledge graph. The extensive experiments\non three datasets demonstrate the effectiveness of KGML under both supervised\nadaptation and unsupervised adaptation settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1\">Maruan Al-Shedivat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Terminology Integration for COVID-19 and other Emerging Domains. (arXiv:2109.04708v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04708","description":"<p>The majority of language domains require prudent use of terminology to ensure\nclarity and adequacy of information conveyed. While the correct use of\nterminology for some languages and domains can be achieved by adapting\ngeneral-purpose MT systems on large volumes of in-domain parallel data, such\nquantities of domain-specific data are seldom available for less-resourced\nlanguages and niche domains. Furthermore, as exemplified by COVID-19 recently,\nno domain-specific parallel data is readily available for emerging domains.\nHowever, the gravity of this recent calamity created a high demand for reliable\ntranslation of critical information regarding pandemic and infection\nprevention. This work is part of WMT2021 Shared Task: Machine Translation using\nTerminologies, where we describe Tilde MT systems that are capable of dynamic\nterminology integration at the time of translation. Our systems achieve up to\n94% COVID-19 term use accuracy on the test set of the EN-FR language pair\nwithout having access to any form of in-domain information during system\ntraining. We conclude our work with a broader discussion considering the Shared\nTask itself and terminology translation in MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bergmanis_T/0/1/0/all/0/1\">Toms Bergmanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinnis_M/0/1/0/all/0/1\">M&#x101;rcis Pinnis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-train or Annotate? Domain Adaptation with a Constrained Budget. (arXiv:2109.04711v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04711","description":"<p>Recent work has demonstrated that pre-training in-domain language models can\nboost performance when adapting to a new domain. However, the costs associated\nwith pre-training raise an important question: given a fixed budget, what steps\nshould an NLP practitioner take to maximize performance? In this paper, we\nstudy domain adaptation under budget constraints, and approach it as a customer\nchoice problem between data annotation and pre-training. Specifically, we\nmeasure the annotation cost of three procedural text datasets and the\npre-training cost of three in-domain language models. Then we evaluate the\nutility of different combinations of pre-training and data annotation under\nvarying budget constraints to assess which combination strategy works best. We\nfind that, for small budgets, spending all funds on annotation leads to the\nbest performance; once the budget becomes large enough, a combination of data\nannotation and in-domain pre-training works more optimally. We therefore\nsuggest that task-specific data annotation should be part of an economical\nstrategy when adapting an NLP model to a new domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution. (arXiv:2109.04712v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04712","description":"<p>Multi-label text classification is a challenging task because it requires\ncapturing label dependencies. It becomes even more challenging when class\ndistribution is long-tailed. Resampling and re-weighting are common approaches\nused for addressing the class imbalance problem, however, they are not\neffective when there is label dependency besides class imbalance because they\nresult in oversampling of common labels. Here, we introduce the application of\nbalancing loss functions for multi-label text classification. We perform\nexperiments on a general domain dataset with 90 labels (Reuters-21578) and a\ndomain-specific dataset from PubMed with 18211 labels. We find that a\ndistribution-balanced loss function, which inherently addresses both the class\nimbalance and label linkage problems, outperforms commonly used loss functions.\nDistribution balancing methods have been successfully used in the image\nrecognition field. Here, we show their effectiveness in natural language\nprocessing. Source code is available at\nhttps://github.com/blessu/BalancedLossNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giledereli_B/0/1/0/all/0/1\">Buse Giledereli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozkirimli_E/0/1/0/all/0/1\">Elif Ozkirimli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages. (arXiv:2109.04715v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04715","description":"<p>Reproducible benchmarks are crucial in driving progress of machine\ntranslation research. However, existing machine translation benchmarks have\nbeen mostly limited to high-resource or well-represented languages. Despite an\nincreasing interest in low-resource machine translation, there are no\nstandardized reproducible benchmarks for many African languages, many of which\nare used by millions of speakers but have less digitized textual data. To\ntackle these challenges, we propose AfroMT, a standardized, clean, and\nreproducible machine translation benchmark for eight widely spoken African\nlanguages. We also develop a suite of analysis tools for system diagnosis\ntaking into account the unique properties of these languages. Furthermore, we\nexplore the newly considered case of low-resource focused pretraining and\ndevelop two novel data augmentation-based strategies, leveraging word-level\nalignment information and pseudo-monolingual data for pretraining multilingual\nsequence-to-sequence models. We demonstrate significant improvements when\npretraining on 11 languages, with gains of up to 2 BLEU points over strong\nbaselines. We also show gains of up to 12 BLEU points over cross-lingual\ntransfer baselines in data-constrained scenarios. All code and pretrained\nmodels will be released as further steps towards larger reproducible benchmarks\nfor African languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04726","description":"<p>Deep neural models for low-resource named entity recognition (NER) have shown\nimpressive results by leveraging distant super-vision or other meta-level\ninformation (e.g. explanation). However, the costs of acquiring such additional\ninformation are generally prohibitive, especially in domains where existing\nresources (e.g. databases to be used for distant supervision) may not exist. In\nthis paper, we present a novel two-stage framework (AutoTriggER) to improve NER\nperformance by automatically generating and leveraging \"entity triggers\" which\nare essentially human-readable clues in the text that can help guide the model\nto make better decisions. Thus, the framework is able to both create and\nleverage auxiliary supervision by itself. Through experiments on three\nwell-studied NER datasets, we show that our automatically extracted triggers\nare well-matched to human triggers, and AutoTriggER improves performance over a\nRoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a\nlow resource setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvam_R/0/1/0/all/0/1\">Ravi Kiran Selvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mahak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations. (arXiv:2109.04727v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04727","description":"<p>Language agnostic and semantic-language information isolation is an emerging\nresearch direction for multilingual representations models. We explore this\nproblem from a novel angle of geometric algebra and semantic space. A simple\nbut highly effective method \"Language Information Removal (LIR)\" factors out\nlanguage identity information from semantic related components in multilingual\nrepresentations pre-trained on multi-monolingual data. A post-training and\nmodel-agnostic method, LIR only uses simple linear operations, e.g. matrix\nfactorization and orthogonal projection. LIR reveals that for weak-alignment\nmultilingual systems, the principal components of semantic spaces primarily\nencodes language identity information. We first evaluate the LIR on a\ncross-lingual question answer retrieval task (LAReQA), which requires the\nstrong alignment for the multilingual embedding space. Experiment shows that\nLIR is highly effectively on this task, yielding almost 100% relative\nimprovement in MAP for weak-alignment models. We then evaluate the LIR on\nAmazon Reviews and XEVAL dataset, with the observation that removing language\ninformation is able to improve the cross-lingual transfer performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darve_E/0/1/0/all/0/1\">Eric Darve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Reliability of Word Embedding Gender Bias Measures. (arXiv:2109.04732v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04732","description":"<p>Various measures have been proposed to quantify human-like social biases in\nword embeddings. However, bias scores based on these measures can suffer from\nmeasurement error. One indication of measurement quality is reliability,\nconcerning the extent to which a measure produces consistent results. In this\npaper, we assess three types of reliability of word embedding gender bias\nmeasures, namely test-retest reliability, inter-rater consistency and internal\nconsistency. Specifically, we investigate the consistency of bias scores across\ndifferent choices of random seeds, scoring rules and words. Furthermore, we\nanalyse the effects of various factors on these measures' reliability scores.\nOur findings inform better design of word embedding gender bias measures.\nMoreover, we urge researchers to be more critical about the application of such\nmeasures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yupei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Genre as Weak Supervision for Cross-lingual Dependency Parsing. (arXiv:2109.04733v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04733","description":"<p>Recent work has shown that monolingual masked language models learn to\nrepresent data-driven notions of language variation which can be used for\ndomain-targeted training data selection. Dataset genre labels are already\nfrequently available, yet remain largely unexplored in cross-lingual setups. We\nharness this genre metadata as a weak supervision signal for targeted data\nselection in zero-shot dependency parsing. Specifically, we project\ntreebank-level genre information to the finer-grained sentence level, with the\ngoal to amplify information implicitly stored in unsupervised contextualized\nrepresentations. We demonstrate that genre is recoverable from multilingual\ncontextual embeddings and that it provides an effective signal for training\ndata selection in cross-lingual, zero-shot scenarios. For 12 low-resource\nlanguage treebanks, six of which are test-only, our genre-specific methods\nsignificantly outperform competitive baselines as well as recent\nembedding-based methods for data selection. Moreover, genre-based data\nselection provides new state-of-the-art results for three of these target\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy. (arXiv:2109.04740v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04740","description":"<p>It is widely accepted that fine-tuning pre-trained language models usually\nbrings about performance improvements in downstream tasks. However, there are\nlimited studies on the reasons behind this effectiveness, particularly from the\nviewpoint of structural changes in the embedding space. Trying to fill this\ngap, in this paper, we analyze the extent to which the isotropy of the\nembedding space changes after fine-tuning. We demonstrate that, even though\nisotropy is a desirable geometrical property, fine-tuning does not necessarily\nresult in isotropy enhancements. Moreover, local structures in pre-trained\ncontextual word representations (CWRs), such as those encoding token types or\nfrequency, undergo a massive change during fine-tuning. Our experiments show\ndramatic growth in the number of elongated directions in the embedding space,\nwhich, in contrast to pre-trained CWRs, carry the essential linguistic\nknowledge in the fine-tuned embedding space, making existing isotropy\nenhancement methods ineffective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-State Capsule Networks for Text Classification. (arXiv:2109.04762v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04762","description":"<p>Text classification systems based on contextual embeddings are not viable\noptions for many of the low resource languages. On the other hand, recently\nintroduced capsule networks have shown performance in par with these text\nclassification models. Thus, they could be considered as a viable alternative\nfor text classification for languages that do not have pre-trained contextual\nembedding models. However, current capsule networks depend upon spatial\npatterns without considering the sequential features of the text. They are also\nsub-optimal in capturing the context-level information in longer sequences.\nThis paper presents a novel Dual-State Capsule (DS-Caps) network-based\ntechnique for text classification, which is optimized to mitigate these issues.\nTwo varieties of states, namely sentence-level and word-level, are integrated\nwith capsule layers to capture deeper context-level information for language\nmodeling. The dynamic routing process among capsules was also optimized using\nthe context-level information obtained through sentence-level states. The\nDS-Caps networks outperform the existing capsule network architectures for\nmultiple datasets, particularly for tasks with longer sequences of text. We\nalso demonstrate the superiority of DS-Caps in text classification for a low\nresource language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demotte_P/0/1/0/all/0/1\">Piyumal Demotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Strong Baseline for Query Efficient Attacks in a Black Box Setting. (arXiv:2109.04775v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04775","description":"<p>Existing black box search methods have achieved high success rate in\ngenerating adversarial attacks against NLP models. However, such search methods\nare inefficient as they do not consider the amount of queries required to\ngenerate adversarial attacks. Also, prior attacks do not maintain a consistent\nsearch space while comparing different search methods. In this paper, we\npropose a query efficient attack strategy to generate plausible adversarial\nexamples on text classification and entailment tasks. Our attack jointly\nleverages attention mechanism and locality sensitive hashing (LSH) to reduce\nthe query count. We demonstrate the efficacy of our approach by comparing our\nattack with four baselines across three different search spaces. Further, we\nbenchmark our results across the same search space used in prior attacks. In\ncomparison to attacks proposed, on an average, we are able to reduce the query\ncount by 75% across all datasets and target models. We also demonstrate that\nour attack achieves a higher success rate when compared to prior attacks in a\nlimited query setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwary_R/0/1/0/all/0/1\">Rishabh Maheshwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwary_S/0/1/0/all/0/1\">Saket Maheshwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pudi_V/0/1/0/all/0/1\">Vikram Pudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04778","description":"<p>Multilingual Neural Machine Translation (NMT) enables one model to serve all\ntranslation directions, including ones that are unseen during training, i.e.\nzero-shot translation. Despite being theoretically attractive, current models\noften produce low quality translations -- commonly failing to even produce\noutputs in the right target language. In this work, we observe that off-target\ntranslation is dominant even in strong multilingual systems, trained on massive\nmultilingual corpora. To address this issue, we propose a joint approach to\nregularize NMT models at both representation-level and gradient-level. At the\nrepresentation level, we leverage an auxiliary target language prediction task\nto regularize decoder outputs to retain information about the target language.\nAt the gradient level, we leverage a small amount of direct data (in thousands\nof sentence pairs) to regularize model gradients. Our results demonstrate that\nour approach is highly effective in both reducing off-target translation\noccurrences and improving zero-shot translation performance by +5.59 and +10.38\nBLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our\nmethod also works well when the small amount of direct data is not available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eriguchi_A/0/1/0/all/0/1\">Akiko Eriguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1\">Prasad Tadepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04780","description":"<p>Transformer-based pre-trained models, such as BERT, have achieved remarkable\nresults on machine reading comprehension. However, due to the constraint of\nencoding length (e.g., 512 WordPiece tokens), a long document is usually split\ninto multiple chunks that are independently read. It results in the reading\nfield being limited to individual chunks without information collaboration for\nlong document machine reading comprehension. To address this problem, we\npropose RoR, a read-over-read method, which expands the reading field from\nchunk to document. Specifically, RoR includes a chunk reader and a document\nreader. The former first predicts a set of regional answers for each chunk,\nwhich are then compacted into a highly-condensed version of the original\ndocument, guaranteeing to be encoded once. The latter further predicts the\nglobal answers from this condensed document. Eventually, a voting strategy is\nutilized to aggregate and rerank the regional and global answers for final\nprediction. Extensive experiments on two benchmarks QuAC and TriviaQA\ndemonstrate the effectiveness of RoR for long document reading. Notably, RoR\nranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of\nsubmission (May 17th, 2021).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition. (arXiv:2109.04783v1 [cs.SD])","link":"http://arxiv.org/abs/2109.04783","description":"<p>When a sufficiently large far-field training data is presented, jointly\noptimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech\nRecognition (ASR) backend shows promising results. Recent literature has shown\ntraditional beamformer designs, such as MVDR (Minimum Variance Distortionless\nResponse) or fixed beamformers can be successfully integrated as the frontend\ninto an E2E ASR system with learnable parameters. In this work, we propose the\nself-attention channel combinator (SACC) ASR frontend, which leverages the\nself-attention mechanism to combine multichannel audio signals in the magnitude\nspectral domain. Experiments conducted on a multichannel playback test data\nshows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed\nbeamformer-based frontend, both jointly optimized with a ContextNet-based ASR\nbackend. We also demonstrate the connection between the SACC and the\ntraditional beamformers, and analyze the intermediate outputs of the SACC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quillen_C/0/1/0/all/0/1\">Carl Quillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dushyant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goderre_A/0/1/0/all/0/1\">Andrew Goderre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lainez_J/0/1/0/all/0/1\">Jos&#xe9; La&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanovic_L/0/1/0/all/0/1\">Ljubomir Milanovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exophoric Pronoun Resolution in Dialogues with Topic Regularization. (arXiv:2109.04787v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04787","description":"<p>Resolving pronouns to their referents has long been studied as a fundamental\nnatural language understanding problem. Previous works on pronoun coreference\nresolution (PCR) mostly focus on resolving pronouns to mentions in text while\nignoring the exophoric scenario. Exophoric pronouns are common in daily\ncommunications, where speakers may directly use pronouns to refer to some\nobjects present in the environment without introducing the objects first.\nAlthough such objects are not mentioned in the dialogue text, they can often be\ndisambiguated by the general topics of the dialogue. Motivated by this, we\npropose to jointly leverage the local context and global topics of dialogues to\nsolve the out-of-text PCR problem. Extensive experiments demonstrate the\neffectiveness of adding topic regularization for resolving exophoric pronouns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xintong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT. (arXiv:2109.04810v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04810","description":"<p>Infusing factual knowledge into pre-trained models is fundamental for many\nknowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions\n(MoP), an infusion approach that can handle a very large knowledge graph (KG)\nby partitioning it into smaller sub-graphs and infusing their specific\nknowledge into various BERT models using lightweight adapters. To leverage the\noverall factual knowledge for a target task, these sub-graph adapters are\nfurther fine-tuned along with the underlying BERT through a mixture layer. We\nevaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on\nsix downstream tasks (inc. NLI, QA, Classification), and the results show that\nour MoP consistently enhances the underlying BERTs in task performance, and\nachieves new SOTA performances on five evaluated datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_T/0/1/0/all/0/1\">Thomas Hikaru Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework. (arXiv:2109.04817v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04817","description":"<p>Style is an integral part of natural language. However, evaluation methods\nfor style measures are rare, often task-specific and usually do not control for\ncontent. We propose the modular, fine-grained and content-controlled\nsimilarity-based STyle EvaLuation framework (STEL) to test the performance of\nany model that can compare two sentences on style. We illustrate STEL with two\ngeneral dimensions of style (formal/informal and simple/complex) as well as two\nspecific characteristics of style (contrac'tion and numb3r substitution). We\nfind that BERT-based methods outperform simple versions of commonly used style\nmeasures like 3-grams, punctuation frequency and LIWC-based approaches. We\ninvite the addition of further tasks and task instances to STEL and hope to\nfacilitate the improvement of style-sensitive measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegmann_A/0/1/0/all/0/1\">Anna Wegmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04825","description":"<p>The impressive capabilities of recent generative models to create texts that\nare challenging to distinguish from the human-written ones can be misused for\ngenerating fake news, product reviews, and even abusive content. Despite the\nprominent performance of existing methods for artificial text detection, they\nstill lack interpretability and robustness towards unseen models. To this end,\nwe propose three novel types of interpretable topological features for this\ntask based on Topological Data Analysis (TDA) which is currently understudied\nin the field of NLP. We empirically show that the features derived from the\nBERT model outperform count- and neural-based baselines up to 10\\% on three\ncommon datasets, and tend to be the most robust towards unseen GPT-style\ngeneration models as opposed to existing methods. The probing analysis of the\nfeatures reveals their sensitivity to the surface and syntactic properties. The\nresults demonstrate that TDA is a promising line with respect to NLP tasks,\nspecifically the ones that incorporate surface and structural information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1\">Alexander Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking It All: Generating Contextualized Questions for any Semantic Role. (arXiv:2109.04832v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04832","description":"<p>Asking questions about a situation is an inherent step towards understanding\nit. To this end, we introduce the task of role question generation, which,\ngiven a predicate mention and a passage, requires producing a set of questions\nasking about all possible semantic roles of the predicate. We develop a\ntwo-stage model for this task, which first produces a context-independent\nquestion prototype for each role and then revises it to be contextually\nappropriate for the passage. Unlike most existing approaches to question\ngeneration, our approach does not require conditioning on existing answers in\nthe text. Instead, we condition on the type of information to inquire about,\nregardless of whether the answer appears explicitly in the text, could be\ninferred from it, or should be sought elsewhere. Our evaluation demonstrates\nthat we generate diverse and well-formed questions for a large, broad-coverage\nontology of predicates and roles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Julian Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model. (arXiv:2109.04834v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04834","description":"<p>Multi-turn response selection models have recently shown comparable\nperformance to humans in several benchmark datasets. However, in the real\nenvironment, these models often have weaknesses, such as making incorrect\npredictions based heavily on superficial patterns without a comprehensive\nunderstanding of the context. For example, these models often give a high score\nto the wrong response candidate containing several keywords related to the\ncontext but using the inconsistent tense. In this study, we analyze the\nweaknesses of the open-domain Korean Multi-turn response selection models and\npublish an adversarial dataset to evaluate these weaknesses. We also suggest a\nstrategy to build a robust model in this adversarial environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kijong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seojin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-hun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features. (arXiv:2109.04835v1 [cs.SI])","link":"http://arxiv.org/abs/2109.04835","description":"<p>In recent years, with the expansion of the Internet and attractive social\nmedia infrastructures, people prefer to follow the news through these media.\nDespite the many advantages of these media in the news field, the lack of any\ncontrol and verification mechanism has led to the spread of fake news, as one\nof the most important threats to democracy, economy, journalism and freedom of\nexpression. Designing and using automatic methods to detect fake news on social\nmedia has become a significant challenge. In this paper, we examine the\npublishers' role in detecting fake news on social media. We also suggest a high\naccurate multi-modal framework, namely FR-Detect, using user-related and\ncontent-related features with early detection capability. For this purpose, two\nnew user-related features, namely Activity Credibility and Influence, have been\nintroduced for publishers. Furthermore, a sentence-level convolutional neural\nnetwork is provided to combine these features with latent textual content\nfeatures properly. Experimental results have shown that the publishers'\nfeatures can improve the performance of content-based models by up to 13% and\n29% in accuracy and F1-score, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarrahi_A/0/1/0/all/0/1\">Ali Jarrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safari_L/0/1/0/all/0/1\">Leila Safari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block Pruning For Faster Transformers. (arXiv:2109.04838v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04838","description":"<p>Pre-training has improved model accuracy for both classification and\ngeneration tasks at the cost of introducing much larger and slower models.\nPruning methods have proven to be an effective way of reducing model size,\nwhereas distillation methods are proven for speeding up inference. We introduce\na block pruning approach targeting both small and fast models. Our approach\nextends structured methods by considering blocks of any size and integrates\nthis structure into the movement pruning paradigm for fine-tuning. We find that\nthis approach learns to prune out full components of the underlying model, such\nas attention heads. Experiments consider classification and generation tasks,\nyielding among other results a pruned model that is a 2.4x faster, 74% smaller\nBERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models\nin speed and pruned models in size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_F/0/1/0/all/0/1\">Fran&#xe7;ois Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlaix_E/0/1/0/all/0/1\">Ella Charlaix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04847","description":"<p>Labeling data can be an expensive task as it is usually performed manually by\ndomain experts. This is cumbersome for deep learning, as it is dependent on\nlarge labeled datasets. Active learning (AL) is a paradigm that aims to reduce\nlabeling effort by only using the data which the used model deems most\ninformative. Little research has been done on AL in a text classification\nsetting and next to none has involved the more recent, state-of-the-art NLP\nmodels. Here, we present an empirical study that compares different\nuncertainty-based algorithms with BERT$_{base}$ as the used classifier. We\nevaluate the algorithms on two NLP classification datasets: Stanford Sentiment\nTreebank and KvK-Frontpages. Additionally, we explore heuristics that aim to\nsolve presupposed problems of uncertainty-based AL; namely, that it is\nunscalable and that it is prone to selecting outliers. Furthermore, we explore\nthe influence of the query-pool size on the performance of AL. Whereas it was\nfound that the proposed heuristics for AL did not improve performance of AL;\nour results show that using uncertainty-based AL with BERT$_{base}$ outperforms\nrandom sampling of data. This difference in performance can decrease as the\nquery-pool size gets larger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_P/0/1/0/all/0/1\">Pieter Floris Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenniger_G/0/1/0/all/0/1\">Gideon Maillette de Buy Wenniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco Wiering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification. (arXiv:2109.04853v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04853","description":"<p>Large-Scale Multi-Label Text Classification (LMTC) includes tasks with\nhierarchical label spaces, such as automatic assignment of ICD-9 codes to\ndischarge summaries. Performance of models in prior art is evaluated with\nstandard precision, recall, and F1 measures without regard for the rich\nhierarchical structure. In this work we argue for hierarchical evaluation of\nthe predictions of neural LMTC models. With the example of the ICD-9 ontology\nwe describe a structural issue in the representation of the structured label\nspace in prior art, and propose an alternative representation based on the\ndepth of the ontology. We propose a set of metrics for hierarchical evaluation\nusing the depth-based representation. We compare the evaluation scores from the\nproposed metrics with previously used metrics on prior art LMTC models for\nICD-9 coding in MIMIC-III. We also propose further avenues of research\ninvolving the proposed ontological representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Falis_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Falis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Studying word order through iterative shuffling. (arXiv:2109.04867v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04867","description":"<p>As neural language models approach human performance on NLP benchmark tasks,\ntheir advances are widely seen as evidence of an increasingly complex\nunderstanding of syntax. This view rests upon a hypothesis that has not yet\nbeen empirically tested: that word order encodes meaning essential to\nperforming these tasks. We refute this hypothesis in many cases: in the GLUE\nsuite and in various genres of English text, the words in a sentence or phrase\ncan rarely be permuted to form a phrase carrying substantially different\ninformation. Our surprising result relies on inference by iterative shuffling\n(IBIS), a novel, efficient procedure that finds the ordering of a bag of words\nhaving the highest likelihood under a fixed language model. IBIS can use any\nblack-box model without additional training and is superior to existing word\nordering algorithms. Coalescing our findings, we discuss how shuffling\ninference procedures such as IBIS can benefit language modeling and constrained\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1\">Nikolay Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanka_S/0/1/0/all/0/1\">Sameera Lanka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Pranav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiAzterTest: a Multilingual Analyzer on Multiple Levels of Language for Readability Assessment. (arXiv:2109.04870v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04870","description":"<p>Readability assessment is the task of determining how difficult or easy a\ntext is or which level/grade it has. Traditionally, language dependent\nreadability formula have been used, but these formulae take few text\ncharacteristics into account. However, Natural Language Processing (NLP) tools\nthat assess the complexity of texts are able to measure more different features\nand can be adapted to different languages. In this paper, we present the\nMultiAzterTest tool: (i) an open source NLP tool which analyzes texts on over\n125 measures of cohesion,language, and readability for English, Spanish and\nBasque, but whose architecture is designed to easily adapt other languages;\n(ii) readability assessment classifiers that improve the performance of\nCoh-Metrix in English, Coh-Metrix-Esp in Spanish and ErreXail in Basque; iii) a\nweb tool. MultiAzterTest obtains 90.09 % in accuracy when classifying into\nthree reading levels (elementary, intermediate, and advanced) in English and\n95.50 % in Basque and 90 % in Spanish when classifying into two reading levels\n(simple and complex) using a SMO classifier. Using cross-lingual features,\nMultiAzterTest also obtains competitive results above all in a complex vs\nsimple distinction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bengoetxea_K/0/1/0/all/0/1\">Kepa Bengoetxea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Dios_I/0/1/0/all/0/1\">Itziar Gonzalez-Dios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Approaches to Word Representation. (arXiv:2109.04876v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04876","description":"<p>The problem of representing the atomic elements of language in modern neural\nlearning systems is one of the central challenges of the field of natural\nlanguage processing. I present a survey of the distributional, compositional,\nand relational approaches to addressing this task, and discuss various means of\nintegrating them into systems, with special emphasis on the word level and the\nout-of-vocabulary phenomenon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Test Time Adapter Ensembling for Low-resource Language Varieties. (arXiv:2109.04877v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04877","description":"<p>Adapters are light-weight modules that allow parameter-efficient fine-tuning\nof pretrained models. Specialized language and task adapters have recently been\nproposed to facilitate cross-lingual transfer of multilingual pretrained models\n(Pfeiffer et al., 2020b). However, this approach requires training a separate\nlanguage adapter for every language one wishes to support, which can be\nimpractical for languages with limited data. An intuitive solution is to use a\nrelated language adapter for the new language variety, but we observe that this\nsolution can lead to sub-optimal performance. In this paper, we aim to improve\nthe robustness of language adapters to uncovered languages without training new\nadapters. We find that ensembling multiple existing language adapters makes the\nfine-tuned model significantly more robust to other language varieties not\nincluded in these adapters. Building upon this observation, we propose Entropy\nMinimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble\nweights of the pretrained language adapters for each test sentence by\nminimizing the entropy of its predictions. Experiments on three diverse groups\nof language varieties show that our method leads to significant improvements on\nboth named entity recognition and part-of-speech tagging across all languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-level Entity-based Extraction as Template Generation. (arXiv:2109.04901v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04901","description":"<p>Document-level entity-based extraction (EE), aiming at extracting\nentity-centric information such as entity roles and entity relations, is key to\nautomatic knowledge acquisition from text corpora for various domains. Most\ndocument-level EE systems build extractive models, which struggle to model\nlong-term dependencies among entities at the document level. To address this\nissue, we propose a generative framework for two document-level EE tasks:\nrole-filler entity extraction (REE) and relation extraction (RE). We first\nformulate them as a template generation problem, allowing models to efficiently\ncapture cross-entity dependencies, exploit label semantics, and avoid the\nexponential computation complexity of identifying N-ary relations. A novel\ncross-attention guided copy mechanism, TopK Copy, is incorporated into a\npre-trained sequence-to-sequence model to enhance the capabilities of\nidentifying key information in the input document. Experiments done on the\nMUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%),\nbinary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kung-Hsiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sam Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReasonBERT: Pre-trained to Reason with Distant Supervision. (arXiv:2109.04912v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04912","description":"<p>We present ReasonBert, a pre-training method that augments language models\nwith the ability to reason over long-range relations and multiple, possibly\nhybrid contexts. Unlike existing pre-training methods that only harvest\nlearning signals from local contexts of naturally occurring texts, we propose a\ngeneralized notion of distant supervision to automatically connect multiple\npieces of text and tables to create pre-training examples that require\nlong-range reasoning. Different types of reasoning are simulated, including\nintersecting multiple pieces of evidence, bridging from one piece of evidence\nto another, and detecting unanswerable cases. We conduct a comprehensive\nevaluation on a variety of extractive question answering datasets ranging from\nsingle-hop to multi-hop and from text-only to table-only to hybrid that require\nvarious reasoning capabilities and show that ReasonBert achieves remarkable\nimprovement over an array of strong baselines. Few-shot experiments further\ndemonstrate that our pre-training method substantially improves sample\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lees_A/0/1/0/all/0/1\">Alyssa Lees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion in Task-Oriented Dialogue Systems. (arXiv:2109.04919v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04919","description":"<p>The ability to recognise emotions lends a conversational artificial\nintelligence a human touch. While emotions in chit-chat dialogues have received\nsubstantial attention, emotions in task-oriented dialogues have been largely\noverlooked despite having an equally important role, such as to signal failure\nor success. Existing emotion-annotated task-oriented corpora are limited in\nsize, label richness, and public availability, creating a bottleneck for\ndownstream tasks. To lay a foundation for studies on emotions in task-oriented\ndialogues, we introduce EmoWOZ, a large-scale manually emotion-annotated corpus\nof task-oriented dialogues. EmoWOZ is based on MultiWOZ, a multi-domain\ntask-oriented dialogue dataset. It contains more than 11K dialogues with more\nthan 83K emotion annotations of user utterances. In addition to Wizzard-of-Oz\ndialogues from MultiWOZ, we collect human-machine dialogues within the same set\nof domains to sufficiently cover the space of various emotions that can happen\nduring the lifetime of a data-driven dialogue system. To the best of our\nknowledge, this is the first large-scale open-source corpus of its kind. We\npropose a novel emotion labelling scheme, which is tailored to task-oriented\ndialogues. We report a set of experimental results to show the usability of\nthis corpus for emotion recognition and state tracking in task-oriented\ndialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes. (arXiv:2109.04921v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04921","description":"<p>State-of-the-art contextual embeddings are obtained from large language\nmodels available only for a few languages. For others, we need to learn\nrepresentations using a multilingual model. There is an ongoing debate on\nwhether multilingual embeddings can be aligned in a space shared across many\nlanguages. The novel Orthogonal Structural Probe (Limisiewicz and Mare\\v{c}ek,\n2021) allows us to answer this question for specific linguistic features and\nlearn a projection based only on mono-lingual annotated datasets. We evaluate\nsyntactic (UD) and lexical (WordNet) structural information encoded inmBERT's\ncontextual representations for nine diverse languages. We observe that for\nlanguages closely related to English, no transformation is needed. The\nevaluated information is encoded in a shared cross-lingual embedding space. For\nother languages, it is beneficial to apply orthogonal transformation learned\nseparately for each language. We successfully apply our findings to zero-shot\nand few-shot cross-lingual parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1\">David Mare&#x10d;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers. (arXiv:2109.04922v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04922","description":"<p>As large-scale, pre-trained language models achieve human-level and\nsuperhuman accuracy on existing language understanding tasks, statistical bias\nin benchmark data and probing studies have recently called into question their\ntrue capabilities. For a more informative evaluation than accuracy on text\nclassification tasks can offer, we propose evaluating systems through a novel\nmeasure of prediction coherence. We apply our framework to two existing\nlanguage understanding benchmarks with different properties to demonstrate its\nversatility. Our experimental results show that this evaluation framework,\nalthough simple in ideas and implementation, is a quick, effective, and\nversatile measure to provide insight into the coherence of machines'\npredictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04939","description":"<p>In computational linguistics, it has been shown that hierarchical structures\nmake language models (LMs) more human-like. However, the previous literature\nhas been agnostic about a parsing strategy of the hierarchical models. In this\npaper, we investigated whether hierarchical structures make LMs more\nhuman-like, and if so, which parsing strategy is most cognitively plausible. In\norder to address this question, we evaluated three LMs against human reading\ntimes in Japanese with head-final left-branching structures: Long Short-Term\nMemory (LSTM) as a sequential model and Recurrent Neural Network Grammars\n(RNNGs) with top-down and left-corner parsing strategies as hierarchical\nmodels. Our computational modeling demonstrated that left-corner RNNGs\noutperformed top-down RNNGs and LSTM, suggesting that hierarchical and\nleft-corner architectures are more cognitively plausible than top-down or\nsequential architectures. In addition, the relationships between the cognitive\nplausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be\ndiscussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1\">Ryo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noji_H/0/1/0/all/0/1\">Hiroshi Noji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04947","description":"<p>Large-scale, pre-trained language models (LMs) have achieved human-level\nperformance on a breadth of language understanding tasks. However, evaluations\nonly based on end task performance shed little light on machines' true ability\nin language understanding and reasoning. In this paper, we highlight the\nimportance of evaluating the underlying reasoning process in addition to end\nperformance. Toward this goal, we introduce Tiered Reasoning for Intuitive\nPhysics (TRIP), a novel commonsense reasoning dataset with dense annotations\nthat enable multi-tiered evaluation of machines' reasoning process. Our\nempirical results show that while large LMs can achieve high end performance,\nthey struggle to support their predictions with valid supporting evidence. The\nTRIP dataset and our baseline results will motivate verifiable evaluation of\ncommonsense reasoning and facilitate future research toward developing better\nlanguage understanding and reasoning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"We went to look for meaning and all we got were these lousy representations: aspects of meaning representation for computational semantics. (arXiv:2109.04949v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04949","description":"<p>In this paper we examine different meaning representations that are commonly\nused in different natural language applications today and discuss their limits,\nboth in terms of the aspects of the natural language meaning they are modelling\nand in terms of the aspects of the application for which they are used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobnik_S/0/1/0/all/0/1\">Simon Dobnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_R/0/1/0/all/0/1\">Robin Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ek_A/0/1/0/all/0/1\">Adam Ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_B/0/1/0/all/0/1\">Bill Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_S/0/1/0/all/0/1\">Staffan Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilinykh_N/0/1/0/all/0/1\">Nikolai Ilinykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maraev_V/0/1/0/all/0/1\">Vladislav Maraev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somashekarappa_V/0/1/0/all/0/1\">Vidya Somashekarappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Pretraining for Summarization Require Knowledge Transfer?. (arXiv:2109.04953v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04953","description":"<p>Pretraining techniques leveraging enormous datasets have driven recent\nadvances in text summarization. While folk explanations suggest that knowledge\ntransfer accounts for pretraining's benefits, little is known about why it\nworks or what makes a pretraining task or dataset suitable. In this paper, we\nchallenge the knowledge transfer story, showing that pretraining on documents\nconsisting of character n-grams selected at random, we can nearly match the\nperformance of models pretrained on real corpora. This work holds the promise\nof eliminating upstream corpora, which may alleviate some concerns over\noffensive language, bias, and copyright issues. To see whether the small\nresidual benefit of using real data could be accounted for by the structure of\nthe pretraining task, we design several tasks motivated by a qualitative study\nof summarization corpora. However, these tasks confer no appreciable benefit,\nleaving open the possibility of a small role for knowledge transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kundan Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey Bigham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Neural Sentence-Level Reframing of News Articles. (arXiv:2109.04957v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04957","description":"<p>Framing a news article means to portray the reported event from a specific\nperspective, e.g., from an economic or a health perspective. Reframing means to\nchange this perspective. Depending on the audience or the submessage, reframing\ncan become necessary to achieve the desired effect on the readers. Reframing is\nrelated to adapting style and sentiment, which can be tackled with neural text\ngeneration techniques. However, it is more challenging since changing a frame\nrequires rewriting entire sentences rather than single phrases. In this paper,\nwe study how to computationally reframe sentences in news articles while\nmaintaining their coherence to the context. We treat reframing as a\nsentence-level fill-in-the-blank task for which we train neural models on an\nexisting media frame corpus. To guide the training, we propose three\nstrategies: framed-language pretraining, named-entity preservation, and\nadversarial learning. We evaluate respective models automatically and manually\nfor topic consistency, coherence, and successful reframing. Our results\nindicate that generating properly-framed text works well but with tradeoffs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04993","description":"<p>Pre-training visual and textual representations from large-scale image-text\npairs is becoming a standard approach for many downstream vision-language\ntasks. The transformer-based models learn inter and intra-modal attention\nthrough a list of self-supervised learning tasks. This paper proposes LAViTeR,\na novel architecture for visual and textual representation learning. The main\nmodule, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\nGAN-based image synthesis and Image Captioning. We also propose a new\nevaluation metric measuring the similarity between the learnt visual and\ntextual embedding. The experimental results on two public datasets, CUB and\nMS-COCO, demonstrate superior visual and textual representation alignment in\nthe joint feature embedding space\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization. (arXiv:2109.04994v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04994","description":"<p>Unlike well-structured text, such as news reports and encyclopedia articles,\ndialogue content often comes from two or more interlocutors, exchanging\ninformation with each other. In such a scenario, the topic of a conversation\ncan vary upon progression and the key information for a certain topic is often\nscattered across multiple utterances of different speakers, which poses\nchallenges to abstractly summarize dialogues. To capture the various topic\ninformation of a conversation and outline salient facts for the captured\ntopics, this work proposes two topic-aware contrastive learning objectives,\nnamely coherence detection and sub-summary generation objectives, which are\nexpected to implicitly model the topic change and handle information scattering\nchallenges for the dialogue summarization task. The proposed contrastive\nobjectives are framed as auxiliary tasks for the primary dialogue summarization\ntask, united via an alternative parameter updating strategy. Extensive\nexperiments on benchmark datasets demonstrate that the proposed simple method\nsignificantly outperforms strong baselines and achieves new state-of-the-art\nperformance. The code and trained models are publicly available via\n\\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Caixia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box Embeddings: An open-source library for representation learning using geometric structures. (arXiv:2109.04997v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04997","description":"<p>A major factor contributing to the success of modern representation learning\nis the ease of performing various vector operations. Recently, objects with\ngeometric structures (eg. distributions, complex or hyperbolic vectors, or\nregions such as cones, disks, or boxes) have been explored for their\nalternative inductive biases and additional representational capacities. In\nthis work, we introduce Box Embeddings, a Python library that enables\nresearchers to easily apply and extend probabilistic box embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chheda_T/0/1/0/all/0/1\">Tejas Chheda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Purujit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Dhruvesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1\">Michael Boratko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Shib Sankar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. (arXiv:2109.05003v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05003","description":"<p>We study the problem of training named entity recognition (NER) models using\nonly distantly-labeled data, which can be automatically obtained by matching\nentity mentions in the raw text with entity types in a knowledge base. The\nbiggest challenge of distantly-supervised NER is that the distant supervision\nmay induce incomplete and noisy labels, rendering the straightforward\napplication of supervised learning ineffective. In this paper, we propose (1) a\nnoise-robust learning scheme comprised of a new loss function and a noisy label\nremoval step, for training NER models on distantly-labeled data, and (2) a\nself-training method that uses contextualized augmentations created by\npre-trained language models to improve the generalization ability of the NER\nmodel. On three benchmark datasets, our method achieves superior performance,\noutperforming existing distantly-supervised NER models by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiSECT: Learning to Split and Rephrase Sentences with Bitexts. (arXiv:2109.05006v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05006","description":"<p>An important task in NLP applications such as sentence simplification is the\nability to take a long, complex sentence and split it into shorter sentences,\nrephrasing as necessary. We introduce a novel dataset and a new model for this\n`split and rephrase' task. Our BiSECT training data consists of 1 million long\nEnglish sentences paired with shorter, meaning-equivalent English sentences. We\nobtain these by extracting 1-2 sentence alignments in bilingual parallel\ncorpora and then using machine translation to convert both sides of the corpus\ninto the same language. BiSECT contains higher quality training examples than\nprevious Split and Rephrase corpora, with sentence splits that require more\nsignificant modifications. We categorize examples in our corpus, and use these\ncategories in a novel model that allows us to target specific regions of the\ninput sentence to be split and edited. Moreover, we show that models trained on\nBiSECT can perform a wider variety of split operations and improve upon\nprevious state-of-the-art approaches in automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriz_R/0/1/0/all/0/1\">Reno Kriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation Quality and Post-Editing Performance. (arXiv:2109.05016v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05016","description":"<p>We test the natural expectation that using MT in professional translation\nsaves human processing time. The last such study was carried out by\nSanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the\ntranslation quality. In contrast, we focus on neural MT (NMT) of high quality,\nwhich has become the state-of-the-art approach since then and also got adopted\nby most translation companies.\n</p>\n<p>Through an experimental study involving over 30 professional translators for\nEnglish -&gt; Czech translation, we examine the relationship between NMT\nperformance and post-editing time and quality. Across all models, we found that\nbetter MT systems indeed lead to fewer changes in the sentences in this\nindustry setting. The relation between system quality and post-editing time is\nhowever not straightforward and, contrary to the results on phrase-based MT,\nBLEU is definitely not a stable predictor of the time or final output quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamchyna_A/0/1/0/all/0/1\">Ale&#x161; Tamchyna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popel_M/0/1/0/all/0/1\">Martin Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensional Emotion Detection from Categorical Emotion. (arXiv:1911.02499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.02499","description":"<p>We present a model to predict fine-grained emotions along the continuous\ndimensions of valence, arousal, and dominance (VAD) with a corpus with\ncategorical emotion annotations. Our model is trained by minimizing the EMD\n(Earth Mover's Distance) loss between the predicted VAD score distribution and\nthe categorical emotion distributions sorted along VAD, and it can\nsimultaneously classify the emotion categories and predict the VAD scores for a\ngiven sentence. We use pre-trained RoBERTa-Large and fine-tune on three\ndifferent corpora with categorical labels and evaluate on EmoBank corpus with\nVAD scores. We show that our approach reaches comparable performance to that of\nthe state-of-the-art classifiers in categorical emotion classification and\nshows significant positive correlations with the ground truth VAD scores. Also,\nfurther training with supervision of VAD labels leads to improved performance\nespecially when dataset is small. We also present examples of predictions of\nappropriate emotion words that are not part of the original annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jaeyeol Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hee Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Surprising Variability in Word Embedding Stability Across Languages. (arXiv:2004.14876v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14876","description":"<p>Word embeddings are powerful representations that form the foundation of many\nnatural language processing architectures, both in English and in other\nlanguages. To gain further insight into word embeddings, we explore their\nstability (e.g., overlap between the nearest neighbors of a word in different\nembedding spaces) in diverse languages. We discuss linguistic properties that\nare related to stability, drawing out insights about correlations with\naffixing, language gender systems, and other features. This has implications\nfor embedding use, particularly in research that uses them to study language\ntrends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burdick_L/0/1/0/all/0/1\">Laura Burdick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms. (arXiv:2005.00782v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00782","description":"<p>Pre-trained language models (PTLMs) have achieved impressive performance on\ncommonsense inference benchmarks, but their ability to employ commonsense to\nmake robust inferences, which is crucial for effective communications with\nhumans, is debated. In the pursuit of advancing fluid human-AI communication,\nwe propose a new challenge, RICA: Robust Inference capability based on\nCommonsense Axioms, that evaluates robust commonsense inference despite textual\nperturbations. To generate data for this challenge, we develop a systematic and\nscalable procedure using commonsense knowledge bases and probe PTLMs across two\ndifferent evaluation settings. Extensive experiments on our generated probe\nsets with more than 10k statements show that PTLMs perform no better than\nrandom guessing on the zero-shot setting, are heavily impacted by statistical\nbiases, and are not robust to perturbation attacks. We also find that\nfine-tuning on similar statements offer limited gains, as PTLMs still fail to\ngeneralize to unseen inferences. Our new large-scale benchmark exposes a\nsignificant gap between PTLMs and human-level language understanding and offers\na new challenge for PTLMs to demonstrate commonsense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1\">Rahul Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hard Retrieval Decoder Attention for Transformers. (arXiv:2009.14658v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.14658","description":"<p>The Transformer translation model is based on the multi-head attention\nmechanism, which can be parallelized easily. The multi-head attention network\nperforms the scaled dot-product attention function in parallel, empowering the\nmodel by jointly attending to information from different representation\nsubspaces at different positions. In this paper, we present an approach to\nlearning a hard retrieval attention where an attention head only attends to one\ntoken in the sentence rather than all tokens. The matrix multiplication between\nattention probabilities and the value sequence in the standard scaled\ndot-product attention can thus be replaced by a simple and efficient retrieval\noperation. We show that our hard retrieval attention mechanism is 1.43 times\nfaster in decoding, while preserving translation quality on a wide range of\nmachine translation tasks when used in the decoder self- and cross-attention\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiuhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Selection for Cross-Lingual Transfer. (arXiv:2010.06127v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.06127","description":"<p>Transformers that are pre-trained on multilingual corpora, such as, mBERT and\nXLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In\nthe zero-shot transfer setting, only English training data is used, and the\nfine-tuned model is evaluated on another target language. While this works\nsurprisingly well, substantial variance has been observed in target language\nperformance between different fine-tuning runs, and in the zero-shot setup, no\ntarget-language development data is available to select among multiple\nfine-tuned models. Prior work has relied on English dev data to select among\nmodels that are fine-tuned with different learning rates, number of steps and\nother hyperparameters, often resulting in suboptimal choices. In this paper, we\nshow that it is possible to select consistently better models when small\namounts of annotated data are available in auxiliary pivot languages. We\npropose a machine learning approach to model selection that uses the fine-tuned\nmodel's own internal representations to predict its cross-lingual capabilities.\nIn extensive experiments we find that this method consistently selects better\nmodels than English validation data across twenty five languages (including\neight low-resource languages), and often achieves results that are comparable\nto model selection using target language development data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.09697","description":"<p>The capacity of neural networks like the widely adopted transformer is known\nto be very high. Evidence is emerging that they learn successfully due to\ninductive bias in the training routine, typically a variant of gradient descent\n(GD). To better understand this bias, we study the tendency for transformer\nparameters to grow in magnitude ($\\ell_2$ norm) during training, and its\nimplications for the emergent representations within self attention layers.\nEmpirically, we document norm growth in the training of transformer language\nmodels, including T5 during its pretraining. As the parameters grow in\nmagnitude, we prove that the network approximates a discretized network with\nsaturated activation functions. Such \"saturated\" networks are known to have a\nreduced capacity compared to the full network family that can be described in\nterms of formal languages and automata. Our results suggest saturation is a new\ncharacterization of an inductive bias implicit in GD of particular interest for\nNLP. We leverage the emergent discrete structure in a saturated transformer to\nanalyze the role of different attention heads, finding that some focus locally\non a small number of positions, while other heads compute global averages,\nallowing counting. We believe understanding the interplay between these two\ncapabilities may shed further light on the structure of computation within\nlarge transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A scalable framework for learning from implicit user feedback to improve natural language understanding in large-scale conversational AI systems. (arXiv:2010.12251v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12251","description":"<p>Natural Language Understanding (NLU) is an established component within a\nconversational AI or digital assistant system, and it is responsible for\nproducing semantic understanding of a user request. We propose a scalable and\nautomatic approach for improving NLU in a large-scale conversational AI system\nby leveraging implicit user feedback, with an insight that user interaction\ndata and dialog context have rich information embedded from which user\nsatisfaction and intention can be inferred. In particular, we propose a general\ndomain-agnostic framework for curating new supervision data for improving NLU\nfrom live production traffic. With an extensive set of experiments, we show the\nresults of applying the framework and improving NLU for a large-scale\nproduction system and show its impact across 10 domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ameen Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1\">Sidharth Mudgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Bum Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_S/0/1/0/all/0/1\">Spyros Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1\">Ruhi Sarikaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Association Between Labels and Free-Text Rationales. (arXiv:2010.12762v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12762","description":"<p>In interpretable NLP, we require faithful rationales that reflect the model's\ndecision-making process for an explained instance. While prior work focuses on\nextractive rationales (a subset of the input words), we investigate their\nless-studied counterpart: free-text natural language rationales. We demonstrate\nthat pipelines, existing models for faithful extractive rationalization on\ninformation-extraction style tasks, do not extend as reliably to \"reasoning\"\ntasks requiring free-text rationales. We turn to models that jointly predict\nand rationalize, a class of widely used high-performance models for free-text\nrationalization whose faithfulness is not yet established. We define\nlabel-rationale association as a necessary property for faithfulness: the\ninternal mechanisms of the model producing the label and the rationale must be\nmeaningfully correlated. We propose two measurements to test this property:\nrobustness equivalence and feature importance agreement. We find that\nstate-of-the-art T5-based joint models exhibit both properties for\nrationalizing commonsense question-answering and natural language inference,\nindicating their potential for producing faithful free-text rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval. (arXiv:2010.12800v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12800","description":"<p>We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.\nSimilar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,\nQuery Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from\n55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query\nBank and Relevance Set, where the former contains 1,236 human-paraphrased\nqueries while the latter contains ~32 human-annotated FAQ items for each query.\nWe analyze COUGH by testing different FAQ retrieval models built on top of BM25\nand BERT, among which the best model achieves 48.8 under P@5, indicating a\ngreat challenge presented by COUGH and encouraging future research for further\nimprovement. Our COUGH dataset is available at\nhttps://github.com/sunlab-osu/covid-faq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Simon Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Relation Extraction via Incremental Meta Self-Training. (arXiv:2010.16410v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.16410","description":"<p>To alleviate human efforts from obtaining large-scale annotations,\nSemi-Supervised Relation Extraction methods aim to leverage unlabeled data in\naddition to learning from limited samples. Existing self-training methods\nsuffer from the gradual drift problem, where noisy pseudo labels on unlabeled\ndata are incorporated during training. To alleviate the noise in pseudo labels,\nwe propose a method called MetaSRE, where a Relation Label Generation Network\ngenerates quality assessment on pseudo labels by (meta) learning from the\nsuccessful and failed attempts on Relation Classification Network as an\nadditional meta-objective. To reduce the influence of noisy pseudo labels,\nMetaSRE adopts a pseudo label selection and exploitation scheme which assesses\npseudo label quality on unlabeled samples and only exploits high-quality pseudo\nlabels in a self-training fashion to incrementally augment labeled samples for\nboth robustness and accuracy. Experimental results on two public datasets\ndemonstrate the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fukun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases. (arXiv:2011.07497v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2011.07497","description":"<p>Codifying commonsense knowledge in machines is a longstanding goal of\nartificial intelligence. Recently, much progress toward this goal has been made\nwith automatic knowledge base (KB) construction techniques. However, such\ntechniques focus primarily on the acquisition of positive (true) KB statements,\neven though negative (false) statements are often also important for\ndiscriminative reasoning over commonsense KBs. As a first step toward the\nlatter, this paper proposes NegatER, a framework that ranks potential negatives\nin commonsense KBs using a contextual language model (LM). Importantly, as most\nKBs do not contain negatives, NegatER relies only on the positive knowledge in\nthe LM and does not require ground-truth negative examples. Experiments\ndemonstrate that, compared to multiple contrastive data augmentation\napproaches, NegatER yields negatives that are more grammatical, coherent, and\ninformative -- leading to statistically significant accuracy improvements in a\nchallenging KB completion task and confirming that the positive knowledge in\nLMs can be \"re-purposed\" to generate negative knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Sentence Representation Learning with Conditional Masked Language Model. (arXiv:2012.14388v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14388","description":"<p>This paper presents a novel training method, Conditional Masked Language\nModeling (CMLM), to effectively learn sentence representations on large scale\nunlabeled corpora. CMLM integrates sentence representation learning into MLM\ntraining by conditioning on the encoded vectors of adjacent sentences. Our\nEnglish CMLM model achieves state-of-the-art performance on SentEval, even\noutperforming models learned using supervised signals. As a fully unsupervised\nlearning method, CMLM can be conveniently extended to a broad range of\nlanguages and domains. We find that a multilingual CMLM model co-trained with\nbitext retrieval (BR) and natural language inference (NLI) tasks outperforms\nthe previous state-of-the-art multilingual models by a large margin, e.g. 10%\nimprovement upon baseline models on cross-lingual semantic search. We explore\nthe same language bias of the learned representations, and propose a simple,\npost-training and model agnostic approach to remove the language identifying\ninformation from the representation while still retaining sentence semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_J/0/1/0/all/0/1\">Jax Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darve_E/0/1/0/all/0/1\">Eric Darve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts. (arXiv:2012.15562v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15562","description":"<p>Massively multilingual language models such as multilingual BERT offer\nstate-of-the-art cross-lingual transfer performance on a range of NLP tasks.\nHowever, due to limited capacity and large differences in pretraining data\nsizes, there is a profound performance gap between resource-rich and\nresource-poor target languages. The ultimate challenge is dealing with\nunder-resourced languages not covered at all by the models and written in\nscripts unseen during pretraining. In this work, we propose a series of novel\ndata-efficient methods that enable quick and effective adaptation of pretrained\nmultilingual models to such low-resource languages and unseen scripts. Relying\non matrix factorization, our methods capitalize on the existing latent\nknowledge about multiple languages already available in the pretrained model's\nembedding matrix. Furthermore, we show that learning of the new dedicated\nembedding matrix in the target language can be improved by leveraging a small\nnumber of vocabulary items (i.e., the so-called lexically overlapping tokens)\nshared between mBERT's and target language vocabulary. Our adaptation\ntechniques offer substantial performance gains for languages with unseen\nscripts. We also demonstrate that they can yield improvements for low-resource\nlanguages written in scripts covered by the pretrained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Robust Neural Machine Translation: A Transformer Case Study. (arXiv:2012.15710v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15710","description":"<p>Transformers (Vaswani et al., 2017) have brought a remarkable improvement in\nthe performance of neural machine translation (NMT) systems but they could be\nsurprisingly vulnerable to noise. In this work, we try to investigate how noise\nbreaks Transformers and if there exist solutions to deal with such issues.\nThere is a large body of work in the NMT literature on analyzing the behavior\nof conventional models for the problem of noise but Transformers are relatively\nunderstudied in this context. Motivated by this, we introduce a novel\ndata-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate\nnoise during training. This idea is comparable to the well-known fine-tuning\nstrategy. Moreover, we propose two other novel extensions to the original\nTransformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that\nmodify the neural architecture as well as the training process to handle noise.\nOne important characteristic of our techniques is that they only impact the\ntraining phase and do not impose any overhead at inference time. We evaluated\nour techniques to translate the English--German pair in both directions and\nobserved that our models have a higher tolerance to noise. More specifically,\nthey perform with no deterioration where up to 10% of entire test words are\ninfected by noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passban_P/0/1/0/all/0/1\">Peyman Passban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saladi_P/0/1/0/all/0/1\">Puneeth S.M. Saladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging. (arXiv:2012.15781v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.15781","description":"<p>Influence functions approximate the \"influences\" of training data-points for\ntest predictions and have a wide variety of applications. Despite the\npopularity, their computational cost does not scale well with model and\ntraining data size. We present FastIF, a set of simple modifications to\ninfluence functions that significantly improves their run-time. We use\nk-Nearest Neighbors (kNN) to narrow the search space down to a subset of good\ncandidate data points, identify the configurations that best balance the\nspeed-quality trade-off in estimating the inverse Hessian-vector product, and\nintroduce a fast parallel variant. Our proposed method achieves about 80X\nspeedup while being highly correlated with the original influence values. With\nthe availability of the fast influence functions, we demonstrate their\nusefulness in four applications. First, we examine whether influential\ndata-points can \"explain\" test time behavior using the framework of\nsimulatability. Second, we visualize the influence interactions between\ntraining and test data-points. Third, we show that we can correct model errors\nby additional fine-tuning on certain influential data-points, improving the\naccuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we\nexperiment with a similar setup but fine-tuning on datapoints not seen during\ntraining, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI\ndatasets respectively. Overall, our fast influence functions can be efficiently\napplied to large models and datasets, and our experiments demonstrate the\npotential of influence functions in model interpretation and correcting model\nerrors. Code is available at\nhttps://github.com/salesforce/fast-influence-functions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Commonsense Emergence in Few-shot Knowledge Models. (arXiv:2101.00297v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00297","description":"<p>Recently, commonsense knowledge models - pretrained language models (LM)\nfine-tuned on knowledge graph (KG) tuples - showed that considerable amounts of\ncommonsense knowledge can be encoded in the parameters of large language\nmodels. However, as parallel studies show that LMs are poor hypothesizers of\ndeclarative commonsense relationships on their own, it remains unclear whether\nthis knowledge is learned during pretraining or from fine-tuning on KG\nexamples. To investigate this question, we train commonsense knowledge models\nin few-shot settings to study the emergence of their commonsense representation\nabilities. Our results show that commonsense knowledge models can rapidly adapt\nfrom limited examples, indicating that KG fine-tuning serves to learn an\ninterface to encoded knowledge learned during pretraining. Importantly, our\nanalysis of absolute, angular, and distributional parameter changes during\nfew-shot fine-tuning provides novel insights into how this interface is\nlearned.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Da_J/0/1/0/all/0/1\">Jeff Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00433","description":"<p>Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nThis is problematic, as previous work has demonstrated possible trade-offs and\nnegative consequences to disclosive transparency, such as a confusion effect,\nwhere \"too much information\" clouds a reader's understanding of what a system\ndescription means. Disclosive transparency's subjective nature has rendered\ndeep study into these problems and their remedies difficult. To improve this\nstate of affairs, We introduce neural language model-based probabilistic\nmetrics to directly model disclosive transparency, and demonstrate that they\ncorrelate with user and expert opinions of system transparency, making them a\nvalid objective proxy. Finally, we demonstrate the use of these metrics in a\npilot study quantifying the relationships between transparency, confusion, and\nuser perceptions in a corpus of real NLP system descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Spoken Language Modeling from Raw Audio. (arXiv:2102.01192v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.01192","description":"<p>We introduce Generative Spoken Language Modeling, the task of learning the\nacoustic and linguistic characteristics of a language from raw audio (no text,\nno labels), and a set of metrics to automatically evaluate the learned\nrepresentations at acoustic and linguistic levels for both encoding and\ngeneration. We set up baseline systems consisting of a discrete speech encoder\n(returning pseudo-text units), a generative language model (trained on\npseudo-text), and a speech decoder (generating a waveform from pseudo-text) all\ntrained without supervision and validate the proposed metrics with human\nevaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that\nthe number of discrete units (50, 100, or 200) matters in a task-dependent and\nencoder-dependent way, and that some combinations approach text-based systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Evgeny Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_B/0/1/0/all/0/1\">Benjamin Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Adelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Creative Inspiration with Fine-Grained Functional Facets of Ideas. (arXiv:2102.09761v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2102.09761","description":"<p>Large repositories of products, patents and scientific papers offer an\nopportunity for building systems that scour millions of ideas and help users\ndiscover inspirations. However, idea descriptions are typically in the form of\nunstructured text, lacking key structure that is required for supporting\ncreative innovation interactions. Prior work has explored idea representations\nthat were limited in expressivity, required significant manual effort from\nusers, or dependent on curated knowledge bases with poor coverage. We explore a\nnovel representation that automatically breaks up products into fine-grained\nfunctional facets capturing the purposes and mechanisms of ideas, and use it to\nsupport important creative innovation interactions: functional search for\nideas, and exploration of the design space around a focal problem by viewing\nrelated problem perspectives pooled from across many products. In user studies,\nour approach boosts the quality of creative search and inspirations,\noutperforming strong baselines by 50-60%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamari_R/0/1/0/all/0/1\">Ronen Tamari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyeonsu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Joel Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1\">Aniket Kittur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01378","description":"<p>Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Cues and Error Correction for Translation Robustness. (arXiv:2103.07352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07352","description":"<p>Neural Machine Translation models are sensitive to noise in the input texts,\nsuch as misspelled words and ungrammatical constructions. Existing robustness\ntechniques generally fail when faced with unseen types of noise and their\nperformance degrades on clean texts. In this paper, we focus on three types of\nrealistic noise that are commonly generated by humans and introduce the idea of\nvisual context to improve translation robustness for noisy texts. In addition,\nwe describe a novel error correction training regime that can be used as an\nauxiliary task to further improve translation robustness. Experiments on\nEnglish-French and English-German translation show that both multimodal and\nerror correction components improve model robustness to noisy texts, while\nstill retaining translation quality on clean texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources. (arXiv:2103.11320v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11320","description":"<p>Warning: this paper contains content that may be offensive or upsetting.\n</p>\n<p>Numerous natural language processing models have tried injecting commonsense\nby using the ConceptNet knowledge base to improve performance on different\ntasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect\nhuman biases such as \"lawyers are dishonest.\" It is important that these biases\nare not conflated with the notion of commonsense. We study this missing yet\nimportant problem by first defining and quantifying biases in ConceptNet as two\ntypes of representational harms: overgeneralization of polarized perceptions\nand representation disparity. We find that ConceptNet contains severe biases\nand disparities across four demographic categories. In addition, we analyze two\ndownstream models that use ConceptNet as a source for commonsense knowledge and\nfind the existence of biases in those models as well. We further propose a\nfiltered-based bias-mitigation approach and examine its effectiveness. We show\nthat our mitigation approach can reduce the issues in both resource and models\nbut leads to a performance drop, leaving room for future work to build fairer\nand stronger commonsense models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1\">Ninareh Mehrabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Morphosyntactic Well-formedness of Generated Texts. (arXiv:2103.16590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.16590","description":"<p>Text generation systems are ubiquitous in natural language processing\napplications. However, evaluation of these systems remains a challenge,\nespecially in multilingual settings. In this paper, we propose L'AMBRE -- a\nmetric to evaluate the morphosyntactic well-formedness of text using its\ndependency parse and morphosyntactic rules of the language. We present a way to\nautomatically extract various rules governing morphosyntax directly from\ndependency treebanks. To tackle the noisy outputs from text generation systems,\nwe propose a simple methodology to train robust parsers. We show the\neffectiveness of our metric on the task of machine translation through a\ndiachronic study of systems translating into morphologically-rich languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Keyword Spotting in Any Language. (arXiv:2104.01454v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01454","description":"<p>We introduce a few-shot transfer learning method for keyword spotting in any\nlanguage. Leveraging open speech corpora in nine languages, we automate the\nextraction of a large multilingual keyword bank and use it to train an\nembedding model. With just five training examples, we fine-tune the embedding\nmodel for keyword spotting and achieve an average F1 score of 0.75 on keyword\nclassification for 180 new keywords unseen by the embedding model in these nine\nlanguages. This embedding model also generalizes to new languages. We achieve\nan average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13\nnew languages unseen by the embedding model. We investigate streaming accuracy\nfor our 5-shot models in two contexts: keyword spotting and keyword search.\nAcross 440 keywords in 22 languages, we achieve an average streaming keyword\nspotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe\npromising initial results on keyword search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_M/0/1/0/all/0/1\">Mark Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1\">Colby Banbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_J/0/1/0/all/0/1\">Josh Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1\">Pete Warden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Contrastive samples via Summarization for Text Classification with limited annotations. (arXiv:2104.05094v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05094","description":"<p>Contrastive Learning has emerged as a powerful representation learning method\nand facilitates various downstream tasks especially when supervised data is\nlimited. How to construct efficient contrastive samples through data\naugmentation is key to its success. Unlike vision tasks, the data augmentation\nmethod for contrastive learning has not been investigated sufficiently in\nlanguage tasks. In this paper, we propose a novel approach to construct\ncontrastive samples for language tasks using text summarization. We use these\nsamples for supervised contrastive learning to gain better text representations\nwhich greatly benefit text classification tasks with limited annotations. To\nfurther improve the method, we mix up samples from different classes and add an\nextra regularization, named Mixsum, in addition to the cross-entropy-loss.\nExperiments on real-world text classification datasets (Amazon-5, Yelp-5, AG\nNews, and IMDb) demonstrate the effectiveness of the proposed contrastive\nlearning framework with summarization-based data augmentation and Mixsum\nregularization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yangkai Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WHOSe Heritage: Classification of UNESCO World Heritage \"Outstanding Universal Value\" Documents with Soft Labels. (arXiv:2104.05547v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05547","description":"<p>The UNESCO World Heritage List (WHL) includes the exceptionally valuable\ncultural and natural heritage to be preserved for mankind. Evaluating and\njustifying the Outstanding Universal Value (OUV) is essential for each site\ninscribed in the WHL, and yet a complex task, even for experts, since the\nselection criteria of OUV are not mutually exclusive. Furthermore, manual\nannotation of heritage values and attributes from multi-source textual data,\nwhich is currently dominant in heritage studies, is knowledge-demanding and\ntime-consuming, impeding systematic analysis of such authoritative documents in\nterms of their implications on heritage management. This study applies\nstate-of-the-art NLP models to build a classifier on a new dataset containing\nStatements of OUV, seeking an explainable and scalable automation tool to\nfacilitate the nomination, evaluation, research, and monitoring processes of\nWorld Heritage sites. Label smoothing is innovatively adapted to improve the\nmodel performance by adding prior inter-class relationship knowledge to\ngenerate soft labels. The study shows that the best models fine-tuned from BERT\nand ULMFiT can reach 94.3% top-3 accuracy. A human study with expert evaluation\non the model prediction shows that the models are sufficiently generalizable.\nThe study is promising to be further developed and applied in heritage research\nand practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_N/0/1/0/all/0/1\">Nan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nourian_P/0/1/0/all/0/1\">Pirouz Nourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roders_A/0/1/0/all/0/1\">Ana Pereira Roders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational World Knowledge Representation in Contextual Language Models: A Review. (arXiv:2104.05837v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05837","description":"<p>Relational knowledge bases (KBs) are commonly used to represent world\nknowledge in machines. However, while advantageous for their high degree of\nprecision and interpretability, KBs are usually organized according to\nmanually-defined schemas, which limit their expressiveness and require\nsignificant human efforts to engineer and maintain. In this review, we take a\nnatural language processing perspective to these limitations, examining how\nthey may be addressed in part by training deep contextual language models (LMs)\nto internalize and express relational knowledge in more flexible forms. We\npropose to organize knowledge representation strategies in LMs by the level of\nKB supervision provided, from no KB supervision at all to entity- and\nrelation-level supervision. Our contributions are threefold: (1) We provide a\nhigh-level, extensible taxonomy for knowledge representation in LMs; (2) Within\nour taxonomy, we highlight notable models, evaluation tasks, and findings, in\norder to provide an up-to-date review of current knowledge representation\ncapabilities in LMs; and (3) We suggest future research directions that build\nupon the complementary aspects of LMs and KBs as knowledge representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05845","description":"<p>Understanding what sequence of steps are needed to complete a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model\nis given a textual goal and must choose which of four images represents a\nplausible step towards that goal. With a new dataset harvested from wikiHow\nconsisting of 772,277 images representing human actions, we show that our task\nis challenging for state-of-the-art multimodal models. Moreover, the multimodal\nrepresentation learned from our data can be effectively transferred to other\ndatasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task\nwill facilitate multimodal reasoning about procedural events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1\">Artemis Panagopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06022","description":"<p>We propose a parameter sharing method for Transformers (Vaswani et al.,\n2017). The proposed approach relaxes a widely used technique, which shares\nparameters for one layer with all layers such as Universal Transformers\n(Dehghani et al., 2019), to increase the efficiency in the computational time.\nWe propose three strategies: Sequence, Cycle, and Cycle (rev) to assign\nparameters to each layer. Experimental results show that the proposed\nstrategies are efficient in the parameter size and computational time.\nMoreover, we indicate that the proposed strategies are also effective in the\nconfiguration where we use many training data such as the recent WMT\ncompetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. (arXiv:2104.06644v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06644","description":"<p>A possible explanation for the impressive performance of masked language\nmodel (MLM) pre-training is that such models have learned to represent the\nsyntactic structures prevalent in classical NLP pipelines. In this paper, we\npropose a different explanation: MLMs succeed on downstream tasks almost\nentirely due to their ability to model higher-order word co-occurrence\nstatistics. To demonstrate this, we pre-train MLMs on sentences with randomly\nshuffled word order, and show that these models still achieve high accuracy\nafter fine-tuning on many downstream tasks -- including on tasks specifically\ndesigned to be challenging for models that ignore word order. Our models\nperform surprisingly well according to some parametric syntactic probes,\nindicating possible deficiencies in how we test representations for syntactic\ninformation. Overall, our results show that purely distributional information\nlargely explains the success of pre-training, and underscore the importance of\ncurating challenging evaluation datasets that require deeper linguistic\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06979","description":"<p>Learning sentence embeddings often requires a large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong domain adaptation and\npre-training method for sentence embeddings, significantly outperforming other\napproaches like Masked Language Model.\n</p>\n<p>A crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution. (arXiv:2104.07425v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07425","description":"<p>Masked language models (MLMs) have contributed to drastic performance\nimprovements with regard to zero anaphora resolution (ZAR). To further improve\nthis approach, in this study, we made two proposals. The first is a new\npretraining task that trains MLMs on anaphoric relations with explicit\nsupervision, and the second proposal is a new finetuning method that remedies a\nnotorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese\nZAR demonstrated that our two proposals boost the state-of-the-art performance,\nand our detailed analysis provides new insights on the remaining challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konno_R/0/1/0/all/0/1\">Ryuto Konno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubayashi_Y/0/1/0/all/0/1\">Yuichiroh Matsubayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_H/0/1/0/all/0/1\">Hiroki Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning. (arXiv:2104.07637v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07637","description":"<p>Natural languages display a trade-off among different strategies to convey\nsyntactic structure, such as word order or inflection. This trade-off, however,\nhas not appeared in recent simulations of iterated language learning with\nneural network agents (Chaabouni et al., 2019b). We re-evaluate this result in\nlight of three factors that play an important role in comparable experiments\nfrom the Language Evolution field: (i) speaker bias towards efficient\nmessaging, (ii) non systematic input languages, and (iii) learning bottleneck.\nOur simulations show that neural agents mainly strive to maintain the utterance\ntype distribution observed during learning, instead of developing a more\nefficient or systematic language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Yuchen Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhoef_T/0/1/0/all/0/1\">Tessa Verhoef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect and Classify -- Joint Span Detection and Classification for Health Outcomes. (arXiv:2104.07789v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07789","description":"<p>A health outcome is a measurement or an observation used to capture and\nassess the effect of a treatment. Automatic detection of health outcomes from\ntext would undoubtedly speed up access to evidence necessary in healthcare\ndecision making. Prior work on outcome detection has modelled this task as\neither (a) a sequence labelling task, where the goal is to detect which text\nspans describe health outcomes, or (b) a classification task, where the goal is\nto classify a text into a pre-defined set of categories depending on an outcome\nthat is mentioned somewhere in that text. However, this decoupling of span\ndetection and classification is problematic from a modelling perspective and\nignores global structural correspondences between sentence-level and word-level\ninformation present in a given text. To address this, we propose a method that\nuses both word-level and sentence-level information to simultaneously perform\noutcome span detection and outcome type classification. In addition to\ninjecting contextual information to hidden vectors, we use label attention to\nappropriately weight both word and sentence level information. Experimental\nresults on several benchmark datasets for health outcome detection show that\nour proposed method consistently outperforms decoupled methods, reporting\ncompetitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaho_M/0/1/0/all/0/1\">Michael Abaho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_P/0/1/0/all/0/1\">Paula Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodd_S/0/1/0/all/0/1\">Susanna Dodd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What to Pre-Train on? Efficient Intermediate Task Selection. (arXiv:2104.08247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08247","description":"<p>Intermediate task fine-tuning has been shown to culminate in large transfer\ngains across many NLP tasks. With an abundance of candidate datasets as well as\npre-trained language models, it has become infeasible to run the cross-product\nof all combinations to find the best transfer setting. In this work we first\nestablish that similar sequential fine-tuning gains can be achieved in adapter\nsettings, and subsequently consolidate previously proposed methods that\nefficiently identify beneficial tasks for intermediate transfer learning. We\nexperiment with a diverse set of 42 intermediate and 11 target English\nclassification, multiple choice, question answering, and sequence tagging\ntasks. Our results show that efficient embedding based methods that rely solely\non the respective datasets outperform computational expensive few-shot\nfine-tuning approaches. Our best methods achieve an average Regret@3 of less\nthan 1% across all target tasks, demonstrating that we are able to efficiently\nidentify the best datasets for intermediate training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poth_C/0/1/0/all/0/1\">Clifton Poth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Wikily\" Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08384","description":"<p>We present a simple but effective approach for leveraging Wikipedia for\nneural machine translation as well as cross-lingual tasks of image captioning\nand dependency parsing without using any direct supervision from external\nparallel data or supervised models in the target language. We show that first\nsentences and titles of linked Wikipedia pages, as well as cross-lingual image\ncaptions, are strong signals for a seed parallel data to extract bilingual\ndictionaries and cross-lingual word embeddings for mining parallel text from\nWikipedia. Our final model achieves high BLEU scores that are close to or\nsometimes higher than strong supervised baselines in low-resource languages;\ne.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.\nMoreover, we tailor our wikily supervised translation models to unsupervised\nimage captioning, and cross-lingual dependency parser transfer. In image\ncaptioning, we train a multi-tasking machine translation and image captioning\npipeline for Arabic and English from which the Arabic training data is a\ntranslated version of the English captioning data, using our wikily-supervised\ntranslation models. Our captioning results on Arabic are slightly better than\nthat of its supervised model. In dependency parsing, we translate a large\namount of monolingual text, and use it as artificial training data in an\nannotation projection framework. We show that our model outperforms recent work\non cross-lingual transfer of dependency parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Tanti Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment. (arXiv:2104.08597v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08597","description":"<p>Cross-lingual named-entity lexica are an important resource to multilingual\nNLP tasks such as machine translation and cross-lingual wikification. While\nknowledge bases contain a large number of entities in high-resource languages\nsuch as English and French, corresponding entities for lower-resource languages\nare often missing. To address this, we propose Lexical-Semantic-Phonetic Align\n(LSP-Align), a technique to automatically mine cross-lingual entity lexica from\nmined web data. We demonstrate LSP-Align outperforms baselines at extracting\ncross-lingual entity pairs and mine 164 million entity pairs from 120 different\nlanguages aligned with English. We release these cross-lingual entity pairs\nalong with the massively multilingual tagged named entity corpus as a resource\nto the NLP community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1\">Adithya Renduchintala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training. (arXiv:2104.08645v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08645","description":"<p>Pre-trained multilingual language encoders, such as multilingual BERT and\nXLM-R, show great potential for zero-shot cross-lingual transfer. However,\nthese multilingual encoders do not precisely align words and phrases across\nlanguages. Especially, learning alignments in the multilingual embedding space\nusually requires sentence-level or word-level parallel corpora, which are\nexpensive to be obtained for low-resource languages. An alternative is to make\nthe multilingual encoders more robust; when fine-tuning the encoder using\ndownstream task, we train the encoder to tolerate noise in the contextual\nembedding spaces such that even if the representations of different languages\nare not aligned well, the model can still achieve good performance on zero-shot\ncross-lingual transfer. In this work, we propose a learning strategy for\ntraining robust models by drawing connections between adversarial examples and\nthe failure cases of zero-shot cross-lingual transfer. We adopt two widely used\nrobust training methods, adversarial training and randomized smoothing, to\ntrain the desired robust model. The experimental results demonstrate that\nrobust training improves zero-shot cross-lingual transfer on text\nclassification tasks. The improvement is more significant in the generalized\ncross-lingual transfer setting, where the pair of input sentences belong to two\ndifferent languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Dependencies and Statistical Dependence. (arXiv:2104.08685v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08685","description":"<p>Are pairs of words that tend to occur together also likely to stand in a\nlinguistic dependency? This empirical question is motivated by a long history\nof literature in cognitive science, psycholinguistics, and NLP. In this work we\ncontribute an extensive analysis of the relationship between linguistic\ndependencies and statistical dependence between words. Improving on previous\nwork, we introduce the use of large pretrained language models to compute\ncontextualized estimates of the pointwise mutual information between words\n(CPMI). For multiple models and languages, we extract dependency trees which\nmaximize CPMI, and compare to gold standard linguistic dependencies. Overall,\nwe find that CPMI dependencies achieve an unlabelled undirected attachment\nscore of at most $\\approx 0.5$. While far above chance, and consistently above\na non-contextualized PMI baseline, this score is generally comparable to a\nsimple baseline formed by connecting adjacent words. We analyze which kinds of\nlinguistic dependencies are best captured in CPMI dependencies, and also find\nmarked differences between the estimates of the large pretrained language\nmodels, illustrating how their different training schemes affect the type of\ndependencies they capture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoover_J/0/1/0/all/0/1\">Jacob Louis Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning. (arXiv:2104.08799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08799","description":"<p>Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a\nclassical task for capturing the central idea from a given document. Based on\nSeq2Seq models, the previous reinforcement learning framework on KG tasks\nutilizes the evaluation metrics to further improve the well-trained neural\nmodels. However, these KG evaluation metrics such as $F_1@5$ and $F_1@M$ are\nonly aware of the exact correctness of predictions on phrase-level and ignore\nthe semantic similarities between similar predictions and targets, which\ninhibits the model from learning deep linguistic patterns. In response to this\nproblem, we propose a new fine-grained evaluation metric to improve the RL\nframework, which considers different granularities: token-level $F_1$ score,\nedit distance, duplication, and prediction quantities. On the whole, the new\nframework includes two reward functions: the fine-grained evaluation score and\nthe vanilla $F_1$ score. This framework helps the model identifying some\npartial match phrases which can be further optimized as the exact match ones.\nExperiments on KG benchmarks show that our proposed training framework\noutperforms the previous RL training frameworks among all evaluation scores. In\naddition, our method can effectively ease the synonym problem and generate a\nhigher quality prediction. The source code is available at\n\\url{https://github.com/xuyige/FGRL4KG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yichao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yige Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08821","description":"<p>This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances the state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework, by using \"entailment\" pairs\nas positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nprevious best results. We also show -- both theoretically and empirically --\nthat contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09574","description":"<p>Humans use commonsense reasoning (CSR) implicitly to produce natural and\ncoherent responses in conversations. Aiming to close the gap between current\nresponse generation (RG) models and human communication abilities, we want to\nunderstand why RG models respond as they do by probing RG model's understanding\nof commonsense reasoning that elicits proper responses. We formalize the\nproblem by framing commonsense as a latent variable in the RG task and using\nexplanations for responses as textual form of commonsense. We collect 6k\nannotated explanations justifying responses from four dialogue datasets and ask\nhumans to verify them and propose two probing settings to evaluate RG models'\nCSR capabilities. Probing results show that models fail to capture the logical\nrelations between commonsense explanations and responses and fine-tuning on\nin-domain data and increasing model sizes do not lead to understanding of CSR\nfor RG. We hope our study motivates more research in making RG models emulate\nthe human reasoning process in pursuit of smooth human-AI communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Justin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation. (arXiv:2105.03432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03432","description":"<p>Concept-to-text Natural Language Generation is the task of expressing an\ninput meaning representation in natural language. Previous approaches in this\ntask have been able to generalise to rare or unseen instances by relying on a\ndelexicalisation of the input. However, this often requires that the input\nappears verbatim in the output text. This poses challenges in multilingual\nsettings, where the task expands to generate the output text in multiple\nlanguages given the same input. In this paper, we explore the application of\nmultilingual models in concept-to-text and propose Language Agnostic\nDelexicalisation, a novel delexicalisation method that uses multilingual\npretrained embeddings, and employs a character-level post-editing model to\ninflect words in their correct form during relexicalisation. Our experiments\nacross five datasets and five languages show that multilingual models\noutperform monolingual models in concept-to-text and that our framework\noutperforms previous approaches, especially for low resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Giulio Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03824","description":"<p>We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01198","description":"<p>In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00270","description":"<p>Opinion prediction is an emerging research area with diverse real-world\napplications, such as market research and situational awareness. We identify\ntwo lines of approaches to the problem of opinion prediction. One uses\ntopic-based sentiment analysis with time-series modeling, while the other uses\nstatic embedding of text. The latter approaches seek user-specific solutions by\ngenerating user fingerprints. Such approaches are useful in predicting user's\nreactions to unseen content. In this work, we propose a novel dynamic\nfingerprinting method that leverages contextual embedding of user's comments\nconditioned on relevant user's reading history. We integrate BERT variants with\na recurrent neural network to generate predictions. The results show up to 13\\%\nimprovement in micro F1-score compared to previous approaches. Experimental\nresults show novel insights that were previously unknown such as better\npredictions for an increase in dynamic history length, the impact of the nature\nof the article on performance, thereby laying the foundation for further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1\">Kishore Tumarada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_E/0/1/0/all/0/1\">Eduard Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnawali_O/0/1/0/all/0/1\">Omprakash Gnawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arjun Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. (arXiv:2108.04539v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04539","description":"<p>Key information extraction (KIE) from document images requires understanding\nthe contextual and spatial semantics of texts in two-dimensional (2D) space.\nMany recent studies try to solve the task by developing pre-training language\nmodels focusing on combining visual features from document images with texts\nand their layout. On the other hand, this paper tackles the problem by going\nback to the basic: effective combination of text and layout. Specifically, we\npropose a pre-trained language model, named BROS (BERT Relying On Spatiality),\nthat encodes relative positions of texts in 2D space and learns from unlabeled\ndocuments with area-masking strategy. With this optimized training scheme for\nunderstanding texts in 2D space, BROS shows comparable or better performance\ncompared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and\nSciTSR) without relying on visual features. This paper also reveals two\nreal-world challenges in KIE tasks--(1) minimizing the error from incorrect\ntext ordering and (2) efficient learning from fewer downstream examples--and\ndemonstrates the superiority of BROS over previous methods. Our code will be\nopen to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2108.05669","description":"<p>Isolated silos of scientific research and the growing challenge of\ninformation overload limit awareness across the literature and hinder\ninnovation. Algorithmic curation and recommendation, which often prioritize\nrelevance, can further reinforce these informational \"filter bubbles.\" In\nresponse, we describe Bridger, a system for facilitating discovery of scholars\nand their work, to explore design tradeoffs between relevant and novel\nrecommendations. We construct a faceted representation of authors with\ninformation gleaned from their papers and inferred author personas, and use it\nto develop an approach that locates commonalities (\"bridges\") and contrasts\nbetween scientists -- retrieving partially similar authors rather than aiming\nfor strict similarity. In studies with computer science researchers, this\napproach helps users discover authors considered useful for generating novel\nresearch directions, outperforming a state-of-art neural model. In addition to\nrecommending new content, we also demonstrate an approach for displaying it in\na manner that boosts researchers' ability to understand the work of authors\nwith whom they are unfamiliar. Finally, our analysis reveals that Bridger\nconnects authors who have different citation profiles, publish in different\nvenues, and are more distant in social co-authorship networks, raising the\nprospect of bridging diverse communities and facilitating discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portenoy_J/0/1/0/all/0/1\">Jason Portenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radensky_M/0/1/0/all/0/1\">Marissa Radensky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1\">Jevin West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07971","description":"<p>In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Code Generation and Summarization. (arXiv:2108.11601v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2108.11601","description":"<p>Software developers write a lot of source code and documentation during\nsoftware development. Intrinsically, developers often recall parts of source\ncode or code summaries that they had written in the past while implementing\nsoftware or documenting them. To mimic developers' code or summary generation\nbehavior, we propose a retrieval augmented framework, REDCODER, that retrieves\nrelevant code or summaries from a retrieval database and provides them as a\nsupplement to code generation or summarization models. REDCODER has a couple of\nuniqueness. First, it extends the state-of-the-art dense retrieval technique to\nsearch for relevant code or summaries. Second, it can work with retrieval\ndatabases that include unimodal (only code or natural language description) or\nbimodal instances (code-description pairs). We conduct experiments and\nextensive analysis on two benchmark datasets of code generation and\nsummarization in Java and Python, and the promising results endorse the\neffectiveness of our proposed retrieval augmented framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications. For example, intent\nclassification in dialogue systems. The reason is that the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current methods can tackle such problems\nreliably in a realistic scenario where zero OOD training data is available. In\nthis study, we propose ProtoInfoMax, a new architecture that extends\nPrototypical Networks to simultaneously process in-domain and OOD sentences via\nMutual Information Maximization (InfoMax) objective. Experimental results show\nthat our proposed method can substantially improve performance up to 20% for\nOOD detection in low resource settings of text classification. We also show\nthat ProtoInfoMax is less prone to typical overconfidence errors of Neural\nNetworks, leading to more reliable prediction results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummerTime: Text Summarization Toolkit for Non-experts. (arXiv:2108.12738v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12738","description":"<p>Recent advances in summarization provide models that can generate summaries\nof higher quality. Such models now exist for a number of summarization tasks,\nincluding query-based summarization, dialogue summarization, and multi-document\nsummarization. While such models and tasks are rapidly growing in the research\nfield, it has also become challenging for non-experts to keep track of them. To\nmake summarization methods more accessible to a wider audience, we develop\nSummerTime by rethinking the summarization task from the perspective of an NLP\nnon-expert. SummerTime is a complete toolkit for text summarization, including\nvarious models, datasets and evaluation metrics, for a full spectrum of\nsummarization-related tasks. SummerTime integrates with libraries designed for\nNLP researchers, and enables users with easy-to-use APIs. With SummerTime,\nusers can locate pipeline solutions and search for the best model with their\nown data, and visualize the differences, all with a few lines of code. We also\nprovide explanations for models and evaluation metrics to help users understand\nthe model behaviors and select models that best suit their needs. Our library,\nalong with a notebook demo, is available at\nhttps://github.com/Yale-LILY/SummerTime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutuma_M/0/1/0/all/0/1\">Mutethia Mutuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Troy Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2108.12971","description":"<p>A smart contract is a program executed on a blockchain, based on which many\ncryptocurrencies are implemented, and is being used for automating\ntransactions. Due to the large amount of money that smart contracts deal with,\nthere is a surging demand for a method that can statically and formally verify\nthem.\n</p>\n<p>This article describes our type-based static verification tool HELMHOLTZ for\nMichelson, which is a statically typed stack-based language for writing smart\ncontracts that are executed on the blockchain platform Tezos. HELMHOLTZ is\ndesigned on top of our extension of Michelson's type system with refinement\ntypes. HELMHOLTZ takes a Michelson program annotated with a user-defined\nspecification written in the form of a refinement type as input; it then\ntypechecks the program against the specification based on the refinement type\nsystem, discharging the generated verification conditions with the SMT solver\nZ3. We briefly introduce our refinement type system for the core calculus\nMini-Michelson of Michelson, which incorporates the characteristic features\nsuch as compound datatypes (e.g., lists and pairs), higher-order functions, and\ninvocation of another contract. \\HELMHOLTZ{} successfully verifies several\npractical Michelson programs, including one that transfers money to an account\nand that checks a digital signature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_Y/0/1/0/all/0/1\">Yuki Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiromasa Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawata_A/0/1/0/all/0/1\">Akira Kawata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuse_J/0/1/0/all/0/1\">Jun Furuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_A/0/1/0/all/0/1\">Atsushi Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13751","description":"<p>Keeping track of scientific challenges, advances and emerging directions is a\nfundamental part of research. However, researchers face a flood of papers that\nhinders discovery of important knowledge. In biomedicine, this directly impacts\nhuman lives. To address this problem, we present a novel task of extraction and\nsearch of scientific challenges and directions, to facilitate rapid knowledge\ndiscovery. We construct and release an expert-annotated corpus of texts sampled\nfrom full-length papers, labeled with novel semantic categories that generalize\nacross many types of challenges and directions. We focus on a large corpus of\ninterdisciplinary work relating to the COVID-19 pandemic, ranging from\nbiomedicine to areas such as AI and economics. We apply a model trained on our\ndata to identify challenges and directions across the corpus and build a\ndedicated search engine. In experiments with 19 researchers and clinicians\nusing our system, we outperform a popular scientific search engine in assisting\nknowledge discovery. Finally, we show that models trained on our resource\ngeneralize to the wider biomedical domain and to AI papers, highlighting its\nbroad utility. We make our data, model and search engine publicly available.\nhttps://challenges.apps.allenai.org/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_J/0/1/0/all/0/1\">Jon Saad Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shomron_N/0/1/0/all/0/1\">Noam Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01850","description":"<p>As the digital news industry becomes the main channel of information\ndissemination, the adverse impact of fake news is explosively magnified. The\ncredibility of a news report should not be considered in isolation. Rather,\npreviously published news articles on the similar event could be used to assess\nthe credibility of a news report. Inspired by this, we propose a BERT-based\nmultimodal unreliable news detection framework, which captures both textual and\nvisual information from unreliable articles utilising the contrastive learning\nstrategy. The contrastive learner interacts with the unreliable news classifier\nto push similar credible news (or similar unreliable news) closer while moving\nnews articles with similar content but opposite credibility labels away from\neach other in the multimodal embedding space. Experimental results on a\nCOVID-19 related dataset, ReCOVery, show that our model outperforms a number of\ncompetitive baseline in unreliable news detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02972","description":"<p>Despite constant improvements in machine translation quality, automatic\npoetry translation remains a challenging problem due to the lack of\nopen-sourced parallel poetic corpora, and to the intrinsic complexities\ninvolved in preserving the semantics, style, and figurative nature of poetry.\nWe present an empirical investigation for poetry translation along several\ndimensions: 1) size and style of training data (poetic vs. non-poetic),\nincluding a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)\nlanguage-family-specific models vs. mixed-multilingual models. To accomplish\nthis, we contribute a parallel dataset of poetry translations for several\nlanguage pairs. Our results show that multilingual fine-tuning on poetic text\nsignificantly outperforms multilingual fine-tuning on non-poetic text that is\n35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and\nhuman evaluation metrics such as faithfulness (meaning and poetic style).\nMoreover, multilingual fine-tuning on poetic data outperforms \\emph{bilingual}\nfine-tuning on poetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03158","description":"<p>An individual's variation in writing style is often a function of both social\nand personal attributes. While structured social variation has been extensively\nstudied, e.g., gender based variation, far less is known about how to\ncharacterize individual styles due to their idiosyncratic nature. We introduce\na new approach to studying idiolects through a massive cross-author comparison\nto identify and encode stylistic features. The neural model achieves strong\nperformance at authorship identification on short texts and through an\nanalogy-based probing task, showing that the learned representations exhibit\nsurprising regularities that encode qualitative and quantitative shifts of\nidiolectal styles. Through text perturbation, we quantify the relative\ncontributions of different linguistic elements to idiolectal variation.\nFurthermore, we provide a description of idiolects through measuring inter- and\nintra-author variation, showing that variation in idiolects is often\ndistinctive yet consistent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2109.03848","description":"<p>Cyber intelligence is widely and abundantly available in numerous open online\nsources with reports on vulnerabilities and incidents. This constant stream of\nnoisy information requires new tools and techniques if it is to be used for the\nbenefit of analysts and investigators in various organizations. In this paper\nwe present and implement a novel knowledge graph and knowledge mining framework\nfor extracting relevant information from free-form text about incidents in the\ncyber domain. Our framework includes a machine learning based pipeline as well\nas crawling methods for generating graphs of entities, attackers and the\nrelated information with our non-technical cyber ontology. We test our\nframework on publicly available cyber incident datasets to evaluate the\naccuracy of our knowledge mining methods as well as the usefulness of the\nframework in the use of cyber analysts. Our results show analyzing the\nknowledge graph constructed using the novel framework, an analyst can infer\nadditional information from the current cyber landscape in terms of risk to\nvarious entities and the propagation of risk between industries and countries.\nExpanding the framework to accommodate more technical and operational level\ninformation can increase the accuracy and explainability of trends and risk in\nthe knowledge graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takko_T/0/1/0/all/0/1\">Tuomas Takko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kunal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehto_M/0/1/0/all/0/1\">Martti Lehto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalasvirta_P/0/1/0/all/0/1\">Pertti Jalasvirta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cederberg_A/0/1/0/all/0/1\">Aapo Cederberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation. (arXiv:2109.03858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03858","description":"<p>Recent works have found evidence of gender bias in models of machine\ntranslation and coreference resolution using mostly synthetic diagnostic\ndatasets. While these quantify bias in a controlled experiment, they often do\nso on a small scale and consist mostly of artificial, out-of-distribution\nsentences. In this work, we find grammatical patterns indicating stereotypical\nand non-stereotypical gender-role assignments (e.g., female nurses versus male\ndancers) in corpora from three domains, resulting in a first large-scale gender\nbias dataset of 108K diverse real-world English sentences. We manually verify\nthe quality of our corpus and use it to evaluate gender bias in various\ncoreference resolution and machine translation models. We find that all tested\nmodels tend to over-rely on gender stereotypes when presented with natural\ninputs, which may be especially harmful when deployed in commercial systems.\nFinally, we show that our dataset lends itself to finetuning a coreference\nresolution model, finding it mitigates bias on a held out set. Our dataset and\nmodels are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will\nspur future research into gender bias evaluation mitigation techniques in\nrealistic settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Shahar Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer. (arXiv:2109.03969v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03969","description":"<p>Transformers have recently become very popular for sequence-to-sequence\napplications such as machine translation and speech recognition. In this work,\nwe propose a multi-task learning-based transformer model for low-resource\nmultilingual speech recognition for Indian languages. Our proposed model\nconsists of a conformer [1] encoder and two parallel transformer decoders. We\nuse a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme\ndecoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme\nrecognition task as an auxiliary task for our multi-task learning framework. We\njointly optimize the network for both phoneme and grapheme recognition tasks\nusing Joint CTC-Attention [2] training. We use a conditional decoding scheme to\ninject the language information into the model before predicting the grapheme\nsequence. Our experiments show that our proposed approach can obtain\nsignificant improvement over previous approaches [4]. We also show that our\nconformer-based dual-decoder approach outperforms both the transformer-based\ndual-decoder approach and single decoder approach. Finally, We compare\nmonolingual ASR models with our proposed multilingual ASR approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+N_K/0/1/0/all/0/1\">Krishna D N</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04400","description":"<p>In cross-lingual text classification, it is required that task-specific\ntraining data in high-resource source languages are available, where the task\nis identical to that of a low-resource target language. However, collecting\nsuch training data can be infeasible because of the labeling cost, task\ncharacteristics, and privacy concerns. This paper proposes an alternative\nsolution that uses only task-independent word embeddings of high-resource\nlanguages and bilingual dictionaries. First, we construct a dictionary-based\nheterogeneous graph (DHG) from bilingual dictionaries. This opens the\npossibility to use graph neural networks for cross-lingual transfer. The\nremaining challenge is the heterogeneity of DHG because multiple languages are\nconsidered. To address this challenge, we propose dictionary-based\nheterogeneous graph neural network (DHGNet) that effectively handles the\nheterogeneity of DHG by two-step aggregations, which are word-level and\nlanguage-level aggregations. Experimental results demonstrate that our method\noutperforms pretrained models even though it does not access to large corpora.\nFurthermore, it can perform well even though dictionaries contain many\nincorrect translations. Its robustness allows the usage of a wider range of\ndictionaries such as an automatically constructed dictionary and crowdsourced\ndictionary, which are convenient for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chairatanakul_N/0/1/0/all/0/1\">Nuttapong Chairatanakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriwatanasakdi_N/0/1/0/all/0/1\">Noppayut Sriwatanasakdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charoenphakdee_N/0/1/0/all/0/1\">Nontawat Charoenphakdee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1\">Tsuyoshi Murata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization. (arXiv:2109.04527v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04527","description":"<p>Visual localization is the problem of estimating the position and orientation\nfrom which a given image (or a sequence of images) is taken in a known scene.\nIt is an important part of a wide range of computer vision and robotics\napplications, from self-driving cars to augmented/virtual reality systems.\nVisual localization techniques should work reliably and robustly under a wide\nrange of conditions, including seasonal, weather, illumination and man-made\nchanges. Recent benchmarking efforts model this by providing images under\ndifferent conditions, and the community has made rapid progress on these\ndatasets since their inception. However, they are limited to a few geographical\nregions and often recorded with a single device. We propose a new benchmark for\nvisual localization in outdoor scenes, using crowd-sourced data to cover a wide\nrange of geographical regions and camera devices with a focus on the failure\ncases of current algorithms. Experiments with state-of-the-art localization\napproaches show that our dataset is very challenging, with all evaluated\nmethods failing on its hardest parts. As part of the dataset release, we\nprovide the tooling used to generate it, enabling efficient and effective 2D\ncorrespondence annotation to obtain reference poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafarzadeh_A/0/1/0/all/0/1\">Ara Jafarzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antequera_M/0/1/0/all/0/1\">Manuel Lopez Antequera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gargallo_P/0/1/0/all/0/1\">Pau Gargallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1\">Yubin Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toft_C/0/1/0/all/0/1\">Carl Toft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1\">Fredrik Kahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Attention Better Than Matrix Decomposition?. (arXiv:2109.04553v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04553","description":"<p>As an essential ingredient of modern deep learning, attention mechanism,\nespecially self-attention, plays a vital role in the global correlation\ndiscovery. However, is hand-crafted attention irreplaceable when modeling the\nglobal context? Our intriguing finding is that self-attention is not better\nthan the matrix decomposition (MD) model developed 20 years ago regarding the\nperformance and computational cost for encoding the long-distance dependencies.\nWe model the global context issue as a low-rank recovery problem and show that\nits optimization algorithms can help design global information blocks. This\npaper then proposes a series of Hamburgers, in which we employ the optimization\nalgorithms for solving MDs to factorize the input representations into\nsub-matrices and reconstruct a low-rank embedding. Hamburgers with different\nMDs can perform favorably against the popular global context module\nself-attention when carefully coping with gradients back-propagated through\nMDs. Comprehensive experiments are conducted in the vision tasks where it is\ncrucial to learn the global context, including semantic segmentation and image\ngeneration, demonstrating significant improvements over self-attention and its\nvariants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongxu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Ke Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S3G-ARM: Highly Compressive Visual Self-localization from Sequential Semantic Scene Graph Using Absolute and Relative Measurements. (arXiv:2109.04569v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04569","description":"<p>In this paper, we address the problem of image sequence-based\nself-localization (ISS) from a new highly compressive scene representation\ncalled sequential semantic scene graph (S3G). Recent developments in deep graph\nconvolutional neural networks (GCNs) have enabled a highly compressive visual\nplace classifier (VPC) that can use a scene graph as the input modality.\nHowever, in such a highly compressive application, the amount of information\nlost in the image-to-graph mapping is significant and can damage the\nclassification performance. To address this issue, we propose a pair of\nsimilarity-preserving mappings, image-to-nodes and image-to-edges, such that\nthe nodes and edges act as absolute and relative features, respectively, that\ncomplement each other. Moreover, the proposed GCN-VPC is applied to a new task\nof viewpoint planning (VP) of the query image sequence, which contributes to\nfurther improvement in the VPC performance. Experiments using the public NCLT\ndataset validated the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_M/0/1/0/all/0/1\">Mitsuki Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_R/0/1/0/all/0/1\">Ryogo Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1\">Kanji Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object recognition for robotics from tactile time series data utilising different neural network architectures. (arXiv:2109.04573v1 [cs.RO])","link":"http://arxiv.org/abs/2109.04573","description":"<p>Robots need to exploit high-quality information on grasped objects to\ninteract with the physical environment. Haptic data can therefore be used for\nsupplementing the visual modality. This paper investigates the use of\nConvolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) neural\nnetwork architectures for object classification on Spatio-temporal tactile\ngrasping data. Furthermore, we compared these methods using data from two\ndifferent fingertip sensors (namely the BioTac SP and WTS-FT) in the same\nphysical setup, allowing for a realistic comparison across methods and sensors\nfor the same tactile object classification dataset. Additionally, we propose a\nway to create more training examples from the recorded data. The results show\nthat the proposed method improves the maximum accuracy from 82.4% (BioTac SP\nfingertips) and 90.7% (WTS-FT fingertips) with complete time-series data to\nabout 94% for both sensor types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bottcher_W/0/1/0/all/0/1\">Wolfgang Bottcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lama_N/0/1/0/all/0/1\">Nikesh Lama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGinnity_T/0/1/0/all/0/1\">T.M. McGinnity</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Portrait Video Matting via Context Motion Network. (arXiv:2109.04598v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04598","description":"<p>Our automatic portrait video matting method does not require extra inputs.\nMost state-of-the-art matting methods rely on semantic segmentation methods to\nautomatically generate the trimap. Their performance is compromised due to the\nlack of temporal information. Our method exploits semantic information as well\nas temporal information from optical flow and produces high-quality results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiqi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Charlie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04600","description":"<p>Temporal grounding aims to predict a time interval of a video clip\ncorresponding to a natural language query input. In this work, we present\nEVOQUER, a temporal grounding framework incorporating an existing text-to-video\ngrounding model and a video-assisted query generation network. Given a query\nand an untrimmed video, the temporal grounding model predicts the target\ninterval, and the predicted video clip is fed into a video translation task by\ngenerating a simplified version of the input query. EVOQUER forms closed-loop\nlearning by incorporating loss functions from both temporal grounding and query\ngeneration serving as feedback. Our experiments on two widely used datasets,\nCharades-STA and ActivityNet, show that EVOQUER achieves promising improvements\nby 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could\nfacilitate error analysis by explaining temporal grounding model behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lulu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jason Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiently Identifying Task Groupings for Multi-Task Learning. (arXiv:2109.04617v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04617","description":"<p>Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naively training all tasks\ntogether in one model often degrades performance, and exhaustively searching\nthrough combinations of task groupings can be prohibitively expensive. As a\nresult, efficiently identifying the tasks that would benefit from co-training\nremains a challenging design question without a clear solution. In this paper,\nwe suggest an approach to select which tasks should train together in\nmulti-task learning models. Our method determines task groupings in a single\ntraining run by co-training all tasks together and quantifying the effect to\nwhich one task's gradient would affect another task's loss. On the large-scale\nTaskonomy computer vision dataset, we find this method can decrease test loss\nby 10.0\\% compared to simply training all tasks together while operating 11.6\ntimes faster than a state-of-the-art task grouping method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fifty_C/0/1/0/all/0/1\">Christopher Fifty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1\">Ehsan Amid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACFNet: Adaptively-Cooperative Fusion Network for RGB-D Salient Object Detection. (arXiv:2109.04627v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04627","description":"<p>The reasonable employment of RGB and depth data show great significance in\npromoting the development of computer vision tasks and robot-environment\ninteraction. However, there are different advantages and disadvantages in the\nearly and late fusion of the two types of data. Besides, due to the diversity\nof object information, using a single type of data in a specific scenario tends\nto result in semantic misleading. Based on the above considerations, we propose\nan adaptively-cooperative fusion network (ACFNet) with ResinRes structure for\nsalient object detection. This structure is designed to flexibly utilize the\nadvantages of feature fusion in early and late stages. Secondly, an\nadaptively-cooperative semantic guidance (ACG) scheme is designed to suppress\ninaccurate features in the guidance phase. Further, we proposed a type-based\nattention module (TAM) to optimize the network and enhance the multi-scale\nperception of different objects. For different objects, the features generated\nby different types of convolution are enhanced or suppressed by the gated\nmechanism for segmentation optimization. ACG and TAM optimize the transfer of\nfeature streams according to their data attributes and convolution attributes,\nrespectively. Sufficient experiments conducted on RGB-D SOD datasets illustrate\nthat the proposed network performs favorably against 18 state-of-the-art\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Per Garment Capture and Synthesis for Real-time Virtual Try-on. (arXiv:2109.04654v1 [cs.GR])","link":"http://arxiv.org/abs/2109.04654","description":"<p>Virtual try-on is a promising application of computer graphics and human\ncomputer interaction that can have a profound real-world impact especially\nduring this pandemic. Existing image-based works try to synthesize a try-on\nimage from a single image of a target garment, but it inherently limits the\nability to react to possible interactions. It is difficult to reproduce the\nchange of wrinkles caused by pose and body size change, as well as pulling and\nstretching of the garment by hand. In this paper, we propose an alternative per\ngarment capture and synthesis workflow to handle such rich interactions by\ntraining the model with many systematically captured images. Our workflow is\ncomposed of two parts: garment capturing and clothed person image synthesis. We\ndesigned an actuated mannequin and an efficient capturing process that collects\nthe detailed deformations of the target garments under diverse body sizes and\nposes. Furthermore, we proposed to use a custom-designed measurement garment,\nand we captured paired images of the measurement garment and the target\ngarments. We then learn a mapping between the measurement garment and the\ntarget garments using deep image-to-image translation. The customer can then\ntry on the target garments interactively during online shopping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_T/0/1/0/all/0/1\">Toby Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_I/0/1/0/all/0/1\">I-Chao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umetani_N/0/1/0/all/0/1\">Nobuyuki Umetani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_T/0/1/0/all/0/1\">Takeo Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIP: Physical Interaction Prediction via Mental Imagery with Span Selection. (arXiv:2109.04683v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04683","description":"<p>To align advanced artificial intelligence (AI) with human values and promote\nsafe AI, it is important for AI to predict the outcome of physical\ninteractions. Even with the ongoing debates on how humans predict the outcomes\nof physical interactions among objects in the real world, there are works\nattempting to tackle this task via cognitive-inspired AI approaches. However,\nthere is still a lack of AI approaches that mimic the mental imagery humans use\nto predict physical interactions in the real world. In this work, we propose a\nnovel PIP scheme: Physical Interaction Prediction via Mental Imagery with Span\nSelection. PIP utilizes a deep generative model to output future frames of\nphysical interactions among objects before extracting crucial information for\npredicting physical interactions by focusing on salient frames using span\nselection. To evaluate our model, we propose a large-scale SPACE+ dataset of\nsynthetic video frames, including three physical interaction events in a 3D\nenvironment. Our experiments show that PIP outperforms baselines and human\nperformance in physical interaction prediction for both seen and unseen\nobjects. Furthermore, PIP's span selection scheme can effectively identify the\nframes where physical interactions among objects occur within the generated\nframes, allowing for added interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samson Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual 3D Scene Flow Learning with Context-Aware Feature Extraction. (arXiv:2109.04685v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04685","description":"<p>Scene flow estimation is the task to predict the point-wise 3D displacement\nvector between two consecutive frames of point clouds, which has important\napplication in fields such as service robots and autonomous driving. Although\nmany previous works have explored greatly on scene flow estimation based on\npoint clouds, we point out two problems that have not been noticed or well\nsolved before: 1) Points of adjacent frames in repetitive patterns may be\nwrongly associated due to similar spatial structure in their neighbourhoods; 2)\nScene flow between adjacent frames of point clouds with long-distance movement\nmay be inaccurately estimated. To solve the first problem, we propose a novel\ncontext-aware set conv layer to exploit contextual structure information of\nEuclidean space and learn soft aggregation weights for local point features.\nOur design is inspired by human perception of contextual structure information\nduring scene understanding. We incorporate the context-aware set conv layer in\na context-aware point feature pyramid module of 3D point clouds for scene flow\nestimation. For the second problem, we propose an explicit residual flow\nlearning structure in the residual flow refinement layer to cope with\nlong-distance movement. The experiments and ablation study on FlyingThings3D\nand KITTI scene flow datasets demonstrate the effectiveness of each proposed\ncomponent and that we solve problem of ambiguous inter-frame association and\nlong-distance movement estimation. Quantitative results on both FlyingThings3D\nand KITTI scene flow datasets show that our method achieves state-of-the-art\nperformance, surpassing all other previous works to the best of our knowledge\nby at least 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunzhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinrui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face-NMS: A Core-set Selection Approach for Efficient Face Recognition. (arXiv:2109.04698v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04698","description":"<p>Recently, face recognition in the wild has achieved remarkable success and\none key engine is the increasing size of training data. For example, the\nlargest face dataset, WebFace42M contains about 2 million identities and 42\nmillion faces. However, a massive number of faces raise the constraints in\ntraining time, computing resources, and memory cost. The current research on\nthis problem mainly focuses on designing an efficient Fully-connected layer\n(FC) to reduce GPU memory consumption caused by a large number of identities.\nIn this work, we relax these constraints by resolving the redundancy problem of\nthe up-to-date face datasets caused by the greedily collecting operation (i.e.\nthe core-set selection perspective). As the first attempt in this perspective\non the face recognition problem, we find that existing methods are limited in\nboth performance and efficiency. For superior cost-efficiency, we contribute a\nnovel filtering strategy dubbed Face-NMS. Face-NMS works on feature space and\nsimultaneously considers the local and global sparsity in generating core sets.\nIn practice, Face-NMS is analogous to Non-Maximum Suppression (NMS) in the\nobject detection community. It ranks the faces by their potential contribution\nto the overall sparsity and filters out the superfluous face in the pairs with\nhigh similarity for local sparsity. With respect to the efficiency aspect,\nFace-NMS accelerates the whole pipeline by applying a smaller but sufficient\nproxy dataset in training the proxy model. As a result, with Face-NMS, we\nsuccessfully scale down the WebFace42M dataset to 60% while retaining its\nperformance on the main benchmarks, offering a 40% resource-saving and 1.64\ntimes acceleration. The code is publicly available for reference at\nhttps://github.com/HuangJunJie2017/Face-NMS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiagang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dalong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04699","description":"<p>While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering. (arXiv:2109.04735v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04735","description":"<p>Video question answering (VideoQA) is challenging given its multimodal\ncombination of visual understanding and natural language understanding. While\nexisting approaches seldom leverage the appearance-motion information in the\nvideo at multiple temporal scales, the interaction between the question and the\nvisual information for textual semantics extraction is frequently ignored.\nTargeting these issues, this paper proposes a novel Temporal Pyramid\nTransformer (TPT) model with multimodal interaction for VideoQA. The TPT model\ncomprises two modules, namely Question-specific Transformer (QT) and Visual\nInference (VI). Given the temporal pyramid constructed from a video, QT builds\nthe question semantics from the coarse-to-fine multimodal co-occurrence between\neach word and the visual content. Under the guidance of such question-specific\nsemantics, VI infers the visual clues from the local-to-global multi-level\ninteractions between the question and the video. Within each module, we\nintroduce a multimodal attention mechanism to aid the extraction of\nquestion-video interactions, with residual connections adopted for the\ninformation passing across different levels. Through extensive experiments on\nthree VideoQA datasets, we demonstrate better performances of the proposed\nmethod in comparison with the state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Min Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang-Dong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Line as a Visual Sentence: Context-aware Line Descriptor for Visual Localization. (arXiv:2109.04753v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04753","description":"<p>Along with feature points for image matching, line features provide\nadditional constraints to solve visual geometric problems in robotics and\ncomputer vision (CV). Although recent convolutional neural network (CNN)-based\nline descriptors are promising for viewpoint changes or dynamic environments,\nwe claim that the CNN architecture has innate disadvantages to abstract\nvariable line length into the fixed-dimensional descriptor. In this paper, we\neffectively introduce Line-Transformers dealing with variable lines. Inspired\nby natural language processing (NLP) tasks where sentences can be understood\nand abstracted well in neural nets, we view a line segment as a sentence that\ncontains points (words). By attending to well-describable points on aline\ndynamically, our descriptor performs excellently on variable line length. We\nalso propose line signature networks sharing the line's geometric attributes to\nneighborhoods. Performing as group descriptors, the networks enhance line\ndescriptors by understanding lines' relative geometries. Finally, we present\nthe proposed line descriptor and matching in a Point and Line Localization\n(PL-Loc). We show that the visual localization with feature points can be\nimproved using our line features. We validate the proposed method for\nhomography estimation and visual localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungho Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Ayoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReconfigISP: Reconfigurable Camera Image Processing Pipeline. (arXiv:2109.04760v1 [eess.IV])","link":"http://arxiv.org/abs/2109.04760","description":"<p>Image Signal Processor (ISP) is a crucial component in digital cameras that\ntransforms sensor signals into images for us to perceive and understand.\nExisting ISP designs always adopt a fixed architecture, e.g., several\nsequential modules connected in a rigid order. Such a fixed ISP architecture\nmay be suboptimal for real-world applications, where camera sensors, scenes and\ntasks are diverse. In this study, we propose a novel Reconfigurable ISP\n(ReconfigISP) whose architecture and parameters can be automatically tailored\nto specific data and tasks. In particular, we implement several ISP modules,\nand enable backpropagation for each module by training a differentiable proxy,\nhence allowing us to leverage the popular differentiable neural architecture\nsearch and effectively search for the optimal ISP architecture. A proxy tuning\nmechanism is adopted to maintain the accuracy of proxy networks in all cases.\nExtensive experiments conducted on image restoration and object detection, with\ndifferent sensors, light conditions and efficiency constraints, validate the\neffectiveness of ReconfigISP. Only hundreds of parameters need tuning for every\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1\">Ke Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zexian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yue Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1\">Jinwei Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04797","description":"<p>Computational fluid dynamics (CFD) is a valuable tool for personalised,\nnon-invasive evaluation of hemodynamics in arteries, but its complexity and\ntime-consuming nature prohibit large-scale use in practice. Recently, the use\nof deep learning for rapid estimation of CFD parameters like wall shear stress\n(WSS) on surface meshes has been investigated. However, existing approaches\ntypically depend on a hand-crafted re-parametrisation of the surface mesh to\nmatch convolutional neural network architectures. In this work, we propose to\ninstead use mesh convolutional neural networks that directly operate on the\nsame finite-element surface mesh as used in CFD. We train and evaluate our\nmethod on two datasets of synthetic coronary artery models with and without\nbifurcation, using a ground truth obtained from CFD simulation. We show that\nour flexible deep learning model can accurately predict 3D WSS vectors on this\nsurface mesh. Our method processes new meshes in less than 5 [s], consistently\nachieves a normalised mean absolute error of $\\leq$ 1.6 [%], and peaks at 90.5\n[%] median approximation accuracy over the held-out test set, comparing\nfavorably to previously published work. This shows the feasibility of CFD\nsurrogate modelling using mesh convolutional neural networks for hemodynamic\nparameter estimation in artery models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1\">Julian Suk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Pim de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1\">Christoph Brune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolterink_J/0/1/0/all/0/1\">Jelmer M. Wolterink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TADA: Taxonomy Adaptive Domain Adaptation. (arXiv:2109.04813v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04813","description":"<p>Traditional domain adaptation addresses the task of adapting a model to a\nnovel target domain under limited or no additional supervision. While tackling\nthe input domain gap, the standard domain adaptation settings assume no domain\nchange in the output space. In semantic prediction tasks, different datasets\nare often labeled according to different semantic taxonomies. In many\nreal-world settings, the target domain task requires a different taxonomy than\nthe one imposed by the source domain. We therefore introduce the more general\ntaxonomy adaptive domain adaptation (TADA) problem, allowing for inconsistent\ntaxonomies between the two domains. We further propose an approach that jointly\naddresses the image-level and label-level domain adaptation. On the\nlabel-level, we employ a bilateral mixed sampling strategy to augment the\ntarget domain, and a relabelling method to unify and align the label spaces. We\naddress the image-level domain gap by proposing an uncertainty-rectified\ncontrastive learning method, leading to more domain-invariant and class\ndiscriminative features. We extensively evaluate the effectiveness of our\nframework under different TADA settings: open taxonomy, coarse-to-fine\ntaxonomy, and partially-overlapping taxonomy. Our framework outperforms\nprevious state-of-the-art by a large margin, while capable of adapting to new\ntarget domain taxonomies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally Coherent Person Matting Trained on Fake-Motion Dataset. (arXiv:2109.04843v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04843","description":"<p>We propose a novel neural-network-based method to perform matting of videos\ndepicting people that does not require additional user input such as trimaps.\nOur architecture achieves temporal stability of the resulting alpha mattes by\nusing motion-estimation-based smoothing of image-segmentation algorithm\noutputs, combined with convolutional-LSTM modules on U-Net skip connections.\n</p>\n<p>We also propose a fake-motion algorithm that generates training clips for the\nvideo-matting network given photos with ground-truth alpha mattes and\nbackground videos. We apply random motion to photos and their mattes to\nsimulate movement one would find in real videos and composite the result with\nthe background clips. It lets us train a deep neural network operating on\nvideos in an absence of a large annotated video dataset and provides\nground-truth training-clip foreground optical flow for use in loss functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molodetskikh_I/0/1/0/all/0/1\">Ivan Molodetskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_M/0/1/0/all/0/1\">Mikhail Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moskalenko_A/0/1/0/all/0/1\">Andrey Moskalenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitry Vatolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emerging AI Security Threats for Autonomous Cars -- Case Studies. (arXiv:2109.04865v1 [cs.CR])","link":"http://arxiv.org/abs/2109.04865","description":"<p>Artificial Intelligence has made a significant contribution to autonomous\nvehicles, from object detection to path planning. However, AI models require a\nlarge amount of sensitive training data and are usually computationally\nintensive to build. The commercial value of such models motivates attackers to\nmount various attacks. Adversaries can launch model extraction attacks for\nmonetization purposes or step-ping-stone towards other attacks like model\nevasion. In specific cases, it even results in destroying brand reputation,\ndifferentiation, and value proposition. In addition, IP laws and AI-related\nlegalities are still evolving and are not uniform across countries. We discuss\nmodel extraction attacks in detail with two use-cases and a generic kill-chain\nthat can compromise autonomous cars. It is essential to investigate strategies\nto manage and mitigate the risk of model theft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lekkala_S/0/1/0/all/0/1\">Shanthi Lekkala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motwani_T/0/1/0/all/0/1\">Tanya Motwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Manojkumar Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phadke_A/0/1/0/all/0/1\">Amit Phadke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation. (arXiv:2109.04871v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04871","description":"<p>Event camera has offered promising alternative for visual perception,\nespecially in high speed and high dynamic range scenes. Recently, many deep\nlearning methods have shown great success in providing model-free solutions to\nmany event-based problems, such as optical flow estimation. However, existing\ndeep learning methods did not address the importance of temporal information\nwell from the perspective of architecture design and cannot effectively extract\nspatio-temporal features. Another line of research that utilizes Spiking Neural\nNetwork suffers from training issues for deeper architecture. To address these\npoints, a novel input representation is proposed that captures the events\ntemporal distribution for signal enhancement. Moreover, we introduce a\nspatio-temporal recurrent encoding-decoding neural network architecture for\nevent-based optical flow estimation, which utilizes Convolutional Gated\nRecurrent Units to extract feature maps from a series of event images. Besides,\nour architecture allows some traditional frame-based core modules, such as\ncorrelation layer and iterative residual refine scheme, to be incorporated. The\nnetwork is end-to-end trained with self-supervised learning on the\nMulti-Vehicle Stereo Event Camera dataset. We have shown that it outperforms\nall the existing state-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Ziluo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianxiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding. (arXiv:2109.04872v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04872","description":"<p>Temporal grounding aims to temporally localize a video moment in the video\nwhose semantics are related to a given natural language query. Existing methods\ntypically apply a detection or regression pipeline on the fused representation\nwith a focus on designing complicated heads and fusion strategies. Instead,\nfrom a perspective on temporal grounding as a metric-learning problem, we\npresent a Dual Matching Network (DMN), to directly model the relations between\nlanguage queries and video moments in a joint embedding space. This new\nmetric-learning framework enables fully exploiting negative samples from two\nnew aspects: constructing negative cross-modal pairs from a dual matching\nscheme and mining negative pairs across different videos. These new negative\nsamples could enhance the joint representation learning of two modalities via\ncross-modal pair discrimination to maximize their mutual information.\nExperiments show that DMN achieves highly competitive performance compared with\nstate-of-the-art methods on four video grounding benchmarks. Based on DMN, we\npresent a winner solution for STVG challenge of the 3rd PIC workshop. This\nsuggests that metric-learning is still a promising method for temporal\ngrounding via capturing the essential cross-modal correlation in a joint\nembedding space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving gas bubbles ascending in liquid metal from low-SNR neutron radiography images. (arXiv:2109.04883v1 [physics.flu-dyn])","link":"http://arxiv.org/abs/2109.04883","description":"<p>We demonstrate a new image processing methodology for resolving gas bubbles\ntravelling through liquid metal from dynamic neutron radiography images with\nintrinsically low signal-to-noise ratio. Image pre-processing, denoising and\nbubble segmentation are described in detail, with practical recommendations.\nExperimental validation is presented - stationary and moving reference bodies\nwith neutron-transparent cavities are radiographed with imaging conditions\nsimilar to the cases with bubbles in liquid metal. The new methods are applied\nto our experimental data from previous and recent imaging campaigns, and the\nperformance of the methods proposed in this paper is compared against our\npreviously developed methods. Significant improvements are observed as well as\nthe capacity to reliably extract physically meaningful information from\nmeasurements performed under highly adverse imaging conditions. The showcased\nimage processing solution and separate elements thereof are readily extendable\nbeyond the present application, and have been made open-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Birjukovs_M/0/1/0/all/0/1\">Mihails Birjukovs</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Trtik_P/0/1/0/all/0/1\">Pavel Trtik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kaestner_A/0/1/0/all/0/1\">Anders Kaestner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hovind_J/0/1/0/all/0/1\">Jan Hovind</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Klevs_M/0/1/0/all/0/1\">Martins Klevs</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Thomsen_K/0/1/0/all/0/1\">Knud Thomsen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jakovics_A/0/1/0/all/0/1\">Andris Jakovics</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LibFewShot: A Comprehensive Library for Few-shot Learning. (arXiv:2109.04898v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04898","description":"<p>Few-shot learning, especially few-shot image classification, has received\nincreasing attention and witnessed significant advances in recent years. Some\nrecent studies implicitly show that many generic techniques or ``tricks'', such\nas data augmentation, pre-training, knowledge distillation, and\nself-supervision, may greatly boost the performance of a few-shot learning\nmethod. Moreover, different works may employ different software platforms,\ndifferent training schedules, different backbone architectures and even\ndifferent input image sizes, making fair comparisons difficult and\npractitioners struggle with reproducibility. To address these situations, we\npropose a comprehensive library for few-shot learning (LibFewShot) by\nre-implementing seventeen state-of-the-art few-shot learning methods in a\nunified framework with the same single codebase in PyTorch. Furthermore, based\non LibFewShot, we provide comprehensive evaluations on multiple benchmark\ndatasets with multiple backbone architectures to evaluate common pitfalls and\neffects of different training tricks. In addition, given the recent doubts on\nthe necessity of meta- or episodic-training mechanism, our evaluation results\nshow that such kind of mechanism is still necessary especially when combined\nwith pre-training. We hope our work can not only lower the barriers for\nbeginners to work on few-shot learning but also remove the effects of the\nnontrivial tricks to facilitate intrinsic research on few-shot learning. The\nsource code is available from https://github.com/RL-VIG/LibFewShot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chuanqi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pinzhuo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tiexin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuesong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Guided Experience Packing for Replay in Continual Learning. (arXiv:2109.04954v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04954","description":"<p>Artificial learning systems aspire to mimic human intelligence by continually\nlearning from a stream of tasks without forgetting past knowledge. One way to\nenable such learning is to store past experiences in the form of input examples\nin episodic memory and replay them when learning new tasks. However,\nperformance of such method suffers as the size of the memory becomes smaller.\nIn this paper, we propose a new approach for experience replay, where we select\nthe past experiences by looking at the saliency maps which provide visual\nexplanations for the model's decision. Guided by these saliency maps, we pack\nthe memory with only the parts or patches of the input images important for the\nmodel's prediction. While learning a new task, we replay these memory patches\nwith appropriate zero-padding to remind the model about its past decisions. We\nevaluate our algorithm on diverse image classification datasets and report\nbetter performance than the state-of-the-art approaches. With qualitative and\nquantitative analyses we show that our method captures richer summary of past\nexperiences without any memory increase, and hence performs well with small\nepisodic memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_G/0/1/0/all/0/1\">Gobinda Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Displacement and Vibration Measurement in Laboratory Experiments with A Deep Learning Method. (arXiv:2109.04960v1 [eess.IV])","link":"http://arxiv.org/abs/2109.04960","description":"<p>This paper proposes a pipeline to automatically track and measure\ndisplacement and vibration of structural specimens during laboratory\nexperiments. The latest Mask Regional Convolutional Neural Network (Mask R-CNN)\ncan locate the targets and monitor their movement from videos recorded by a\nstationary camera. To improve precision and remove the noise, techniques such\nas Scale-invariant Feature Transform (SIFT) and various filters for signal\nprocessing are included. Experiments on three small-scale reinforced concrete\nbeams and a shaking table test are utilized to verify the proposed method.\nResults show that the proposed deep learning method can achieve the goal to\nautomatically and precisely measure the motion of tested structural members\nduring laboratory experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bai_Y/0/1/0/all/0/1\">Yongsheng Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abduallah_R/0/1/0/all/0/1\">Ramzi M. Abduallah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezen_H/0/1/0/all/0/1\">Halil Sezen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"View Blind-spot as Inpainting: Self-Supervised Denoising with Mask Guided Residual Convolution. (arXiv:2109.04970v1 [eess.IV])","link":"http://arxiv.org/abs/2109.04970","description":"<p>In recent years, self-supervised denoising methods have shown impressive\nperformance, which circumvent painstaking collection procedure of noisy-clean\nimage pairs in supervised denoising methods and boost denoising applicability\nin real world. One of well-known self-supervised denoising strategies is the\nblind-spot training scheme. However, a few works attempt to improve blind-spot\nbased self-denoiser in the aspect of network architecture. In this paper, we\ntake an intuitive view of blind-spot strategy and consider its process of using\nneighbor pixels to predict manipulated pixels as an inpainting process.\nTherefore, we propose a novel Mask Guided Residual Convolution (MGRConv) into\ncommon convolutional neural networks, e.g. U-Net, to promote blind-spot based\ndenoising. Our MGRConv can be regarded as soft partial convolution and find a\ntrade-off among partial convolution, learnable attention maps, and gated\nconvolution. It enables dynamic mask learning with appropriate mask constrain.\nDifferent from partial convolution and gated convolution, it provides moderate\nfreedom for network learning. It also avoids leveraging external learnable\nparameters for mask activation, unlike learnable attention maps. The\nexperiments show that our proposed plug-and-play MGRConv can assist blind-spot\nbased denoising network to reach promising results on both existing\nsingle-image based and dataset-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhongze Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic Narrative Grounding. (arXiv:2109.04988v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04988","description":"<p>This paper proposes Panoptic Narrative Grounding, a spatially fine and\ngeneral formulation of the natural language visual grounding problem. We\nestablish an experimental framework for the study of this new task, including\nnew ground truth and metrics, and we propose a strong baseline method to serve\nas stepping stone for future work. We exploit the intrinsic semantic richness\nin an image by including panoptic categories, and we approach visual grounding\nat a fine-grained level by using segmentations. In terms of ground truth, we\npropose an algorithm to automatically transfer Localized Narratives annotations\nto specific regions in the panoptic segmentations of the MS COCO dataset. To\nguarantee the quality of our annotations, we take advantage of the semantic\nstructure contained in WordNet to exclusively incorporate noun phrases that are\ngrounded to a meaningfully related panoptic segmentation region. The proposed\nbaseline achieves a performance of 55.4 absolute Average Recall points. This\nresult is a suitable foundation to push the envelope further in the development\nof methods for Panoptic Narrative Grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">C. Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1\">N. Ayobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_I/0/1/0/all/0/1\">I. Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1\">J. Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">J. Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">P. Arbel&#xe1;ez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Change Detection in Hyperspectral Images using Feature Fusion Deep Convolutional Autoencoders. (arXiv:2109.04990v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04990","description":"<p>Binary change detection in bi-temporal co-registered hyperspectral images is\na challenging task due to a large number of spectral bands present in the data.\nResearchers, therefore, try to handle it by reducing dimensions. The proposed\nwork aims to build a novel feature extraction system using a feature fusion\ndeep convolutional autoencoder for detecting changes between a pair of such\nbi-temporal co-registered hyperspectral images. The feature fusion considers\nfeatures across successive levels and multiple receptive fields and therefore\nadds a competitive edge over the existing feature extraction methods. The\nchange detection technique described is completely unsupervised and is much\nmore elegant than other supervised or semi-supervised methods which require\nsome amount of label information. Different methods have been applied to the\nextracted features to find the changes in the two images and it is found that\nthe proposed method clearly outperformed the state of the art methods in\nunsupervised change detection for all the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1\">Debasrita Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ashish Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of GAN-synthesized street videos. (arXiv:2109.04991v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04991","description":"<p>Research on the detection of AI-generated videos has focused almost\nexclusively on face videos, usually referred to as deepfakes. Manipulations\nlike face swapping, face reenactment and expression manipulation have been the\nsubject of an intense research with the development of a number of efficient\ntools to distinguish artificial videos from genuine ones. Much less attention\nhas been paid to the detection of artificial non-facial videos. Yet, new tools\nfor the generation of such kind of videos are being developed at a fast pace\nand will soon reach the quality level of deepfake videos. The goal of this\npaper is to investigate the detectability of a new kind of AI-generated videos\nframing driving street sequences (here referred to as DeepStreets videos),\nwhich, by their nature, can not be analysed with the same tools used for facial\ndeepfakes. Specifically, we present a simple frame-based detector, achieving\nvery good performance on state-of-the-art DeepStreets videos generated by the\nVid2vid architecture. Noticeably, the detector retains very good performance on\ncompressed videos, even when the compression level used during training does\nnot match that used for the test videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alamayreh_O/0/1/0/all/0/1\">Omran Alamayreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1\">Mauro Barni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04993","description":"<p>Pre-training visual and textual representations from large-scale image-text\npairs is becoming a standard approach for many downstream vision-language\ntasks. The transformer-based models learn inter and intra-modal attention\nthrough a list of self-supervised learning tasks. This paper proposes LAViTeR,\na novel architecture for visual and textual representation learning. The main\nmodule, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\nGAN-based image synthesis and Image Captioning. We also propose a new\nevaluation metric measuring the similarity between the learnt visual and\ntextual embedding. The experimental results on two public datasets, CUB and\nMS-COCO, demonstrate superior visual and textual representation alignment in\nthe joint feature embedding space\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA. (arXiv:2109.05014v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05014","description":"<p>Knowledge-based visual question answering (VQA) involves answering questions\nthat require external knowledge not present in the image. Existing methods\nfirst retrieve knowledge from external resources, then reason over the selected\nknowledge, the input image, and question for answer prediction. However, this\ntwo-step approach could lead to mismatches that potentially limit the VQA\nperformance. For example, the retrieved knowledge might be noisy and irrelevant\nto the question, and the re-embedded knowledge features during reasoning might\ndeviate from their original meanings in the knowledge base (KB). To address\nthis challenge, we propose PICa, a simple yet effective method that Prompts\nGPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by\nGPT-3's power in knowledge retrieval and question answering, instead of using\nstructured KBs as in previous work, we treat GPT-3 as an implicit and\nunstructured KB that can jointly acquire and process relevant knowledge.\nSpecifically, we first convert the image into captions (or tags) that GPT-3 can\nunderstand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just\nproviding a few in-context VQA examples. We further boost performance by\ncarefully investigating: (i) what text formats best describe the image content,\nand (ii) how in-context examples can be better selected and used. PICa unlocks\nthe first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa\nsurpasses the supervised state of the art by an absolute +8.6 points on the\nOK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent\nfew-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yumao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Conditional Dependence Hidden Markov Models for Skeleton-Based Action Recognition. (arXiv:2002.05809v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2002.05809","description":"<p>Hidden Markov Models (HMMs) comprise a powerful generative approach for\nmodeling sequential data and time-series in general. However, the commonly\nemployed assumption of the dependence of the current time frame to a single or\nmultiple immediately preceding frames is unrealistic; more complicated dynamics\npotentially exist in real world scenarios. This paper revisits conventional\nsequential modeling approaches, aiming to address the problem of capturing\ntime-varying temporal dependency patterns. To this end, we propose a different\nformulation of HMMs, whereby the dependence on past frames is dynamically\ninferred from the data. Specifically, we introduce a hierarchical extension by\npostulating an additional latent variable layer; therein, the (time-varying)\ntemporal dependence patterns are treated as latent variables over which\ninference is performed. We leverage solid arguments from the Variational Bayes\nframework and derive a tractable inference algorithm based on the\nforward-backward algorithm. As we experimentally show, our approach can model\nhighly complex sequential data and can effectively handle data with missing\nvalues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1\">Sergios Theodoridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Lung Nodules Segmentation with Detailed Representation Transfer and Soft Mask Supervision. (arXiv:2007.14556v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.14556","description":"<p>Accurate lung nodules segmentation from Computed Tomography (CT) images is\ncrucial to the analysis and diagnosis of lung diseases such as COVID-19 and\nlung cancer. However, due to the smallness and variety of lung nodules and the\nlack of high-quality labeling, accurate lung nodule segmentation is still a\nchallenging problem. To address these issues, we propose a complete paradigm\nfor accurate lung nodules segmentation. First, we introduce a new segmentation\nmask named Soft Mask which has richer and more accurate edge details\ndescription and better visualization. Correspondingly, we develop a universal\nsemi-automatic Soft Mask annotation pipeline to deal with different datasets.\nSecond, a novel Network with Detailed representation transfer and Soft Mask\nsupervision (DSNet) is proposed to process the input low-resolution images of\nlung nodules into high-quality segmentation results. In our DSNet, we design a\nnovel Selective Detailed Representation Fusion Module to reconstruct the\ndetailed representation to alleviate the small size of lung nodules images. In\naddition, the adversarial training framework with Soft Mask is proposed to\nfurther improve the accuracy of segmentation. Extensive experiments validate\nthat our DSNet outperforms the state-of-the-art methods for accurate lung\nnodules segmentation. And our method also demonstrates competitive results in\nother accurate medical segmentation tasks. Besides, we provide a new\nchallenging lung nodules segmentation dataset for further studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changwei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Rongtao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shibiao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_W/0/1/0/all/0/1\">Weiliang Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Q/0/1/0/all/0/1\">Qimin Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Estimation for Robot Manipulators via Keypoint Optimization and Sim-to-Real Transfer. (arXiv:2010.08054v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2010.08054","description":"<p>Keypoint detection is an essential building block for many robotic\napplications like motion capture and pose estimation. Historically, keypoints\nare detected using uniquely engineered markers such as checkerboards or\nfiducials. More recently, deep learning methods have been explored as they have\nthe ability to detect user-defined keypoints in a marker-less manner. However,\ndifferent manually selected keypoints can have uneven performance when it comes\nto detection and localization. An example of this can be found on symmetric\nrobotic tools where DNN detectors cannot solve the correspondence problem\ncorrectly. In this work, we propose a new and autonomous way to define the\nkeypoint locations that overcomes these challenges. The approach involves\nfinding the optimal set of keypoints on robotic manipulators for robust visual\ndetection and localization. Using a robotic simulator as a medium, our\nalgorithm utilizes synthetic data for DNN training, and the proposed algorithm\nis used to optimize the selection of keypoints through an iterative approach.\nThe results show that when using the optimized keypoints, the detection\nperformance of the DNNs improved significantly. We further use the optimized\nkeypoints for real robotic applications by using domain randomization to bridge\nthe reality gap between the simulator and the physical world. The physical\nworld experiments show how the proposed method can be applied to the\nwide-breadth of robotic applications that require visual feedback, such as\ncamera-to-robot calibration, robotic tool tracking, and end-effector pose\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingpei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_F/0/1/0/all/0/1\">Florian Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1\">Michael Yip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"View-Invariant, Occlusion-Robust Probabilistic Embedding for Human Pose. (arXiv:2010.13321v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.13321","description":"<p>Recognition of human poses and actions is crucial for autonomous systems to\ninteract smoothly with people. However, cameras generally capture human poses\nin 2D as images and videos, which can have significant appearance variations\nacross viewpoints that make the recognition tasks challenging. To address this,\nwe explore recognizing similarity in 3D human body poses from 2D information,\nwhich has not been well-studied in existing works. Here, we propose an approach\nto learning a compact view-invariant embedding space from 2D body joint\nkeypoints, without explicitly predicting 3D poses. Input ambiguities of 2D\nposes from projection and occlusion are difficult to represent through a\ndeterministic mapping, and therefore we adopt a probabilistic formulation for\nour embedding space. Experimental results show that our embedding model\nachieves higher accuracy when retrieving similar poses across different camera\nviews, in comparison with 3D pose estimation models. We also show that by\ntraining a simple temporal embedding model, we achieve superior performance on\npose sequence retrieval and largely reduce the embedding dimension from\nstacking frame-based embeddings for efficient large-scale retrieval.\nFurthermore, in order to enable our embeddings to work with partially visible\ninput, we further investigate different keypoint occlusion augmentation\nstrategies during training. We demonstrate that these occlusion augmentations\nsignificantly improve retrieval performance on partial 2D input poses. Results\non action recognition and video alignment demonstrate that using our embeddings\nwithout any additional training achieves competitive performance relative to\nother models specifically trained for each task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1\">Florian Schroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring and Harnessing Transference in Multi-Task Learning. (arXiv:2010.15413v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.15413","description":"<p>Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naive formulations often\ndegrade performance and in particular, identifying the tasks that would benefit\nfrom co-training remains a challenging design question. In this paper, we\nanalyze the dynamics of information transfer, or transference, across tasks\nthroughout training. Specifically, we develop a similarity measure that can\nquantify transference among tasks and use this quantity to both better\nunderstand the optimization dynamics of multi-task learning as well as improve\noverall learning performance. In the latter case, we propose two methods to\nleverage our transference metric. The first operates at a macro-level by\nselecting which tasks should train together while the second functions at a\nmicro-level by determining how to combine task gradients at each training step.\nWe find these methods can lead to significant improvement over prior work on\nthree supervised multi-task learning benchmarks and one multi-task\nreinforcement learning paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fifty_C/0/1/0/all/0/1\">Christopher Fifty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1\">Ehsan Amid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Features Guidance Network for partial-to-partial point cloud registration. (arXiv:2011.12079v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12079","description":"<p>To eliminate the problems of large dimensional differences, big semantic gap,\nand mutual interference caused by hybrid features, in this paper, we propose a\nnovel Multi-Features Guidance Network for partial-to-partial point cloud\nregistration(MFG). The proposed network mainly includes four parts: keypoints'\nfeature extraction, correspondences searching, correspondences credibility\ncomputation, and SVD, among which correspondences searching and correspondence\ncredibility computation are the cores of the network. Unlike the previous work,\nwe utilize the shape features and the spatial coordinates to guide\ncorrespondences search independently and fusing the matching results to obtain\nthe final matching matrix. In the correspondences credibility computation\nmodule, based on the conflicted relationship between the features matching\nmatrix and the coordinates matching matrix, we score the reliability for each\ncorrespondence, which can reduce the impact of mismatched or non-matched\npoints. Experimental results show that our network outperforms the current\nstate-of-the-art while maintaining computational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wen Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qianhao Ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nerfies: Deformable Neural Radiance Fields. (arXiv:2011.12948v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12948","description":"<p>We present the first method capable of photorealistically reconstructing\ndeformable scenes using photos/videos captured casually from mobile phones. Our\napproach augments neural radiance fields (NeRF) by optimizing an additional\ncontinuous volumetric deformation field that warps each observed point into a\ncanonical 5D NeRF. We observe that these NeRF-like deformation fields are prone\nto local minima, and propose a coarse-to-fine optimization method for\ncoordinate-based models that allows for more robust optimization. By adapting\nprinciples from geometry processing and physical simulation to NeRF-like\nmodels, we propose an elastic regularization of the deformation field that\nfurther improves robustness. We show that our method can turn casually captured\nselfie photos/videos into deformable NeRF models that allow for photorealistic\nrenderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We\nevaluate our method by collecting time-synchronized data using a rig with two\nmobile phones, yielding train/validation images of the same pose at different\nviewpoints. We show that our method faithfully reconstructs non-rigidly\ndeforming scenes and reproduces unseen views with high fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Keunhong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1\">Utkarsh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1\">Dan B Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1\">Steven M. Seitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning normal appearance for fetal anomaly screening: Application to the unsupervised detection of Hypoplastic Left Heart Syndrome. (arXiv:2012.03679v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.03679","description":"<p>Congenital heart disease is considered as one the most common groups of\ncongenital malformations which affects $6-11$ per $1000$ newborns. In this\nwork, an automated framework for detection of cardiac anomalies during\nultrasound screening is proposed and evaluated on the example of Hypoplastic\nLeft Heart Syndrome (HLHS), a sub-category of congenital heart disease. We\npropose an unsupervised approach that learns healthy anatomy exclusively from\nclinically confirmed normal control patients. We evaluate a number of known\nanomaly detection frameworks together with a model architecture based on the\n$\\alpha$-GAN network and find evidence that the proposed model performs\nsignificantly better than the state-of-the-art in image-based anomaly\ndetection, yielding average $0.81$ AUC \\emph{and} a better robustness towards\ninitialisation compared to previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chotzoglou_E/0/1/0/all/0/1\">Elisa Chotzoglou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lloyd_D/0/1/0/all/0/1\">David Lloyd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Generalization in Visual Representation Learning. (arXiv:2012.05649v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05649","description":"<p>Measuring concept generalization, i.e., the extent to which models trained on\na set of (seen) visual concepts can be leveraged to recognize a new set of\n(unseen) concepts, is a popular way of evaluating visual representations,\nespecially in a self-supervised learning framework. Nonetheless, the choice of\nunseen concepts for such an evaluation is usually made arbitrarily, and\nindependently from the seen concepts used to train representations, thus\nignoring any semantic relationships between the two. In this paper, we argue\nthat the semantic relationships between seen and unseen concepts affect\ngeneralization performance and propose ImageNet-CoG, a novel benchmark on the\nImageNet-21K (IN-21K) dataset that enables measuring concept generalization in\na principled way. Our benchmark leverages expert knowledge that comes from\nWordNet in order to define a sequence of unseen IN-21K concept sets that are\nsemantically more and more distant from the ImageNet-1K (IN-1K) subset, a\nubiquitous training set. This allows us to benchmark visual representations\nlearned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31\nconvolution and transformer-based models and show how different architectures,\nlevels of supervision, regularization techniques and use of web data impact the\nconcept generalization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sariyildiz_M/0/1/0/all/0/1\">Mert Bulent Sariyildiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantidis_Y/0/1/0/all/0/1\">Yannis Kalantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISD: Self-Supervised Learning by Iterative Similarity Distillation. (arXiv:2012.09259v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.09259","description":"<p>Recently, contrastive learning has achieved great results in self-supervised\nlearning, where the main idea is to push two augmentations of an image\n(positive pairs) closer compared to other random images (negative pairs). We\nargue that not all random images are equal. Hence, we introduce a self\nsupervised learning algorithm where we use a soft similarity for the negative\nimages rather than a binary distinction between positive and negative pairs. We\niteratively distill a slowly evolving teacher model to the student model by\ncapturing the similarity of a query image to some random images and\ntransferring that knowledge to the student. We argue that our method is less\nconstrained compared to recent contrastive learning methods, so it can learn\nbetter features. Specifically, our method should handle unbalanced and\nunlabeled data better than existing contrastive learning methods, because the\nrandomly chosen negative set might include many samples that are semantically\nsimilar to the query image. In this case, our method labels them as highly\nsimilar while standard contrastive methods label them as negative pairs. Our\nmethod achieves comparable results to the state-of-the-art models. We also show\nthat our method performs better in the settings where the unlabeled data is\nunbalanced. Our code is available here: https://github.com/UMBCvision/ISD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_V/0/1/0/all/0/1\">Vipin Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solid Texture Synthesis using Generative Adversarial Networks. (arXiv:2102.03973v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03973","description":"<p>Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a\n3D solid volume, exhibits advantages in numerous application domains. However,\nexisting methods generally synthesize solid texture with specific features,\nwhich may result in the failure of capturing diversified textural information.\nIn this paper, we propose a novel generative adversarial nets-based approach\n(STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our\nmulti-scale discriminators evaluate the similarity between patch from exemplar\nand slice from the generated volume, promoting the generator to synthesize\nrealistic solid textures. Experimental results demonstrate that the proposed\nmethod can generate high-quality solid textures with similar visual\ncharacteristics to the exemplar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fanqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Junteng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Modelling of BRDF Textures from Flash Images. (arXiv:2102.11861v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2102.11861","description":"<p>We learn a latent space for easy capture, consistent interpolation, and\nefficient reproduction of visual material appearance. When users provide a\nphoto of a stationary natural material captured under flashlight illumination,\nfirst it is converted into a latent material code. Then, in the second step,\nconditioned on the material code, our method produces an infinite and diverse\nspatial field of BRDF model parameters (diffuse albedo, normals, roughness,\nspecular albedo) that subsequently allows rendering in complex scenes and\nilluminations, matching the appearance of the input photograph. Technically, we\njointly embed all flash images into a latent space using a convolutional\nencoder, and -- conditioned on these latent codes -- convert random spatial\nfields into fields of BRDF parameters using a convolutional neural network\n(CNN). We condition these BRDF parameters to match the visual characteristics\n(statistics and spectra of visual features) of the input under matching light.\nA user study compares our approach favorably to previous work, even those with\naccess to BRDF supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henzler_P/0/1/0/all/0/1\">Philipp Henzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1\">Valentin Deschaintre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Morphological and Histogram based Text Line Segmentation in the OCR Context. (arXiv:2103.08922v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08922","description":"<p>Text line segmentation is one of the pre-stages of modern optical character\nrecognition systems. The algorithmic approach proposed by this paper has been\ndesigned for this exact purpose. Its main characteristic is the combination of\ntwo different techniques, morphological image operations and horizontal\nhistogram projections. The method was developed to be applied on a historic\ndata collection that commonly features quality issues, such as degraded paper,\nblurred text, or presence of noise. For that reason, the segmenter in question\ncould be of particular interest for cultural institutions, that want access to\nrobust line bounding boxes for a given historic document. Because of the\npromising segmentation results that are joined by low computational cost, the\nalgorithm was incorporated into the OCR pipeline of the National Library of\nLuxembourg, in the context of the initiative of reprocessing their historic\nnewspaper collection. The general contribution of this paper is to outline the\napproach and to evaluate the gains in terms of accuracy and speed, comparing it\nto the segmentation algorithm bundled with the used open source OCR software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization. (arXiv:2103.16874v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16874","description":"<p>The task of image-based virtual try-on aims to transfer a target clothing\nitem onto the corresponding region of a person, which is commonly tackled by\nfitting the item to the desired body part and fusing the warped item with the\nperson. While an increasing number of studies have been conducted, the\nresolution of synthesized images is still limited to low (e.g., 256x192), which\nacts as the critical limitation against satisfying online consumers. We argue\nthat the limitation stems from several challenges: as the resolution increases,\nthe artifacts in the misaligned areas between the warped clothes and the\ndesired clothing regions become noticeable in the final results; the\narchitectures used in existing methods have low performance in generating\nhigh-quality body parts and maintaining the texture sharpness of the clothes.\nTo address the challenges, we propose a novel virtual try-on method called\nVITON-HD that successfully synthesizes 1024x768 virtual try-on images.\nSpecifically, we first prepare the segmentation map to guide our virtual try-on\nsynthesis, and then roughly fit the target clothing item to a given person's\nbody. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS\ngenerator to handle the misaligned areas and preserve the details of 1024x768\ninputs. Through rigorous comparison with existing methods, we demonstrate that\nVITON-HD highly surpasses the baselines in terms of synthesized image quality\nboth qualitatively and quantitatively. Code is available at\nhttps://github.com/shadow2496/VITON-HD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seunghwan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AAformer: Auto-Aligned Transformer for Person Re-Identification. (arXiv:2104.00921v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00921","description":"<p>In person re-identification, extracting part-level features from person\nimages has been verified to be crucial. Most of existing CNN-based methods only\nlocate the human parts coarsely, or rely on pre-trained human parsing models\nand fail in locating the identifiable non-human parts (e.g., knapsack). In this\npaper, we introduce an alignment scheme in Transformer architecture for the\nfirst time and propose the Auto-Aligned Transformer (AAformer) to automatically\nlocate both the human parts and non-human ones at patch-level. We introduce the\n\"part tokens\", which are learnable vectors, to extract part features in\nTransformer. A part token only interacts with a local subset of patches in\nself-attention and learns to be the part representation. To adaptively group\nthe image patches into different subsets, we design the Auto-Alignment.\nAuto-Alignment employs a fast variant of Optimal Transport algorithm to online\ncluster the patch embeddings into several groups with the part tokens as their\nprototypes. We harmoniously integrate the part alignment into the\nself-attention and the output part tokens can be directly used for retrieval.\nExtensive experiments validate the effectiveness of part tokens and the\nsuperiority of AAformer over various state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haiyun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gaopan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1\">Honglin Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis. (arXiv:2104.04767v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04767","description":"<p>In recent years, the use of Generative Adversarial Networks (GANs) has become\nvery popular in generative image modeling. While style-based GAN architectures\nyield state-of-the-art results in high-fidelity image synthesis,\ncomputationally, they are highly complex. In our work, we focus on the\nperformance optimization of style-based generative models. We analyze the most\ncomputationally hard parts of StyleGAN2, and propose changes in the generator\nnetwork to make it possible to deploy style-based generative networks in the\nedge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer\nparameters and is x9.5 less computationally complex than StyleGAN2, while\nproviding comparable quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belousov_S/0/1/0/all/0/1\">Sergei Belousov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05845","description":"<p>Understanding what sequence of steps are needed to complete a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model\nis given a textual goal and must choose which of four images represents a\nplausible step towards that goal. With a new dataset harvested from wikiHow\nconsisting of 772,277 images representing human actions, we show that our task\nis challenging for state-of-the-art multimodal models. Moreover, the multimodal\nrepresentation learned from our data can be effectively transferred to other\ndatasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task\nwill facilitate multimodal reasoning about procedural events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1\">Artemis Panagopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAM: A Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.07566","description":"<p>Recovering texture information from the aliasing regions has always been a\nmajor challenge for Single Image Super Resolution (SISR) task. These regions\nare often submerged in noise so that we have to restore texture details while\nsuppressing noise. To address this issue, we propose a Balanced Attention\nMechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and\nMaxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to\nsuppress extreme noise in the large scale feature maps while MSAM preserves\nhigh-frequency texture details. Thanks to the parallel structure, these two\nmodules not only conduct self-optimization, but also mutual optimization to\nobtain the balance of noise reduction and high-frequency texture restoration\nduring the back propagation process, and the parallel structure makes the\ninference faster. To verify the effectiveness and robustness of BAM, we applied\nit to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently\nimprove the networks performance, and for those originally with attention\nmechanism, the substitution with BAM further reduces the amount of parameters\nand increases the inference speed. Moreover, we present a dataset with rich\ntexture aliasing regions in real scenes, named realSR7. Experiments prove that\nBAM achieves better super-resolution results on the aliasing area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Wikily\" Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08384","description":"<p>We present a simple but effective approach for leveraging Wikipedia for\nneural machine translation as well as cross-lingual tasks of image captioning\nand dependency parsing without using any direct supervision from external\nparallel data or supervised models in the target language. We show that first\nsentences and titles of linked Wikipedia pages, as well as cross-lingual image\ncaptions, are strong signals for a seed parallel data to extract bilingual\ndictionaries and cross-lingual word embeddings for mining parallel text from\nWikipedia. Our final model achieves high BLEU scores that are close to or\nsometimes higher than strong supervised baselines in low-resource languages;\ne.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.\nMoreover, we tailor our wikily supervised translation models to unsupervised\nimage captioning, and cross-lingual dependency parser transfer. In image\ncaptioning, we train a multi-tasking machine translation and image captioning\npipeline for Arabic and English from which the Arabic training data is a\ntranslated version of the English captioning data, using our wikily-supervised\ntranslation models. Our captioning results on Arabic are slightly better than\nthat of its supervised model. In dependency parsing, we translate a large\namount of monolingual text, and use it as artificial training data in an\nannotation projection framework. We show that our model outperforms recent work\non cross-lingual transfer of dependency parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Tanti Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mean Shift for Self-Supervised Learning. (arXiv:2105.07269v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07269","description":"<p>Most recent self-supervised learning (SSL) algorithms learn features by\ncontrasting between instances of images or by clustering the images and then\ncontrasting between the image clusters. We introduce a simple mean-shift\nalgorithm that learns representations by grouping images together without\ncontrasting between them or adopting much of prior on the structure of the\nclusters. We simply \"shift\" the embedding of each image to be close to the\n\"mean\" of its neighbors. Since in our setting, the closest neighbor is always\nanother augmentation of the same image, our model will be identical to BYOL\nwhen using only one nearest neighbor instead of 5 as used in our experiments.\nOur model achieves 72.4% on ImageNet linear evaluation with ResNet50 at 200\nepochs outperforming BYOL. Our code is available here:\nhttps://github.com/UMBCvision/MSF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FineAction: A Fine-Grained Video Dataset for Temporal Action Localization. (arXiv:2105.11107v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11107","description":"<p>Temporal action localization (TAL) is an important and challenging problem in\nvideo understanding. However, most existing TAL benchmarks are built upon the\ncoarse granularity of action classes, which exhibits two major limitations in\nthis task. First, coarse-level actions can make the localization models overfit\nin high-level context information, and ignore the atomic action details in the\nvideo. Second, the coarse action classes often lead to the ambiguous\nannotations of temporal boundaries, which are inappropriate for temporal action\nlocalization. To tackle these problems, we develop a novel large-scale and\nfine-grained video dataset, coined as FineAction, for temporal action\nlocalization. In total, FineAction contains 103K temporal instances of 106\naction categories, annotated in 17K untrimmed videos. FineAction introduces new\nopportunities and challenges for temporal action localization, thanks to its\ndistinct characteristics of fine action classes with rich diversity, dense\nannotations of multiple instances, and co-occurring actions of different\nclasses. To benchmark FineAction, we systematically investigate the performance\nof several popular temporal localization methods on it, and deeply analyze the\ninfluence of short-duration and fine-grained instances in temporal action\nlocalization. We believe that FineAction can advance research of temporal\naction localization and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.07806","description":"<p>Machine learning is revolutionizing image-based diagnostics in pathology and\nradiology. ML models have shown promising results in research settings, but\ntheir lack of interoperability has been a major barrier for clinical\nintegration and evaluation. The DICOM a standard specifies Information Object\nDefinitions and Services for the representation and communication of digital\nimages and related information, including image-derived annotations and\nanalysis results. However, the complexity of the standard represents an\nobstacle for its adoption in the ML community and creates a need for software\nlibraries and tools that simplify working with data sets in DICOM format. Here\nwe present the highdicom library, which provides a high-level application\nprogramming interface for the Python programming language that abstracts\nlow-level details of the standard and enables encoding and decoding of\nimage-derived information in DICOM format in a few lines of Python code. The\nhighdicom library ties into the extensive Python ecosystem for image processing\nand machine learning. Simultaneously, by simplifying creation and parsing of\nDICOM-compliant files, highdicom achieves interoperability with the medical\nimaging systems that hold the data used to train and run ML models, and\nultimately communicate and store model outputs for clinical use. We demonstrate\nthrough experiments with slide microscopy and computed tomography imaging,\nthat, by bridging these two ecosystems, highdicom enables developers to train\nand evaluate state-of-the-art ML models in pathology and radiology while\nremaining compliant with the DICOM standard and interoperable with clinical\nsystems at all stages. To promote standardization of ML research and streamline\nthe ML model development and deployment process, we made the library available\nfree and open-source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1\">Christopher P. Bridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1\">Chris Gorman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1\">Steven Pieper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1\">Sean W. Doyle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1\">Jochen K. Lennerz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1\">David A. Clunie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1\">Andriy Y. Fedorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1\">Markus D. Herrmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11858","description":"<p>Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising patches\nextracted from full image, in each acquisition step. The problem is framed in\nan exploration-exploitation framework by combining an embedding based on\nUniform Manifold Approximation to model representativeness with entropy as\nuncertainty measure to model informativeness. We applied our proposed method to\nthe autonomous driving datasets CamVid and Cityscapes and performed a\nquantitative comparison with state-of-the-art baselines. We find that our\nactive learning method achieves better performance compared to previous\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1\">Johannes Otterbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. (arXiv:2106.13228v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13228","description":"<p>Neural Radiance Fields (NeRF) are able to reconstruct scenes with\nunprecedented fidelity, and various recent works have extended NeRF to handle\ndynamic scenes. A common approach to reconstruct such non-rigid scenes is\nthrough the use of a learned deformation field mapping from coordinates in each\ninput image into a canonical template coordinate space. However, these\ndeformation-based approaches struggle to model changes in topology, as\ntopological changes require a discontinuity in the deformation field, but these\ndeformation fields are necessarily continuous. We address this limitation by\nlifting NeRFs into a higher dimensional space, and by representing the 5D\nradiance field corresponding to each individual input image as a slice through\nthis \"hyper-space\". Our method is inspired by level set methods, which model\nthe evolution of surfaces as slices through a higher dimensional surface. We\nevaluate our method on two tasks: (i) interpolating smoothly between \"moments\",\ni.e., configurations of the scene, seen in the input images while maintaining\nvisual plausibility, and (ii) novel-view synthesis at fixed moments. We show\nthat our method, which we dub HyperNeRF, outperforms existing methods on both\ntasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for\ninterpolation and 8.6% for novel-view synthesis, as measured by LPIPS.\nAdditional videos, results, and visualizations are available at\nhttps://hypernerf.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Keunhong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1\">Utkarsh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Peter Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1\">Dan B Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1\">Steven M. Seitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes for Hierarchical Vision Transformer?. (arXiv:2107.02174v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02174","description":"<p>Recent studies indicate that hierarchical Vision Transformer with a macro\narchitecture of interleaved non-overlapped window-based self-attention \\&amp;\nshifted-window operation is able to achieve state-of-the-art performance in\nvarious visual recognition tasks, and challenges the ubiquitous convolutional\nneural networks (CNNs) using densely slid kernels. Most follow-up works attempt\nto replace the shifted-window operation with other kinds of cross-window\ncommunication paradigms, while treating self-attention as the de-facto standard\nfor window-based information aggregation. In this manuscript, we question\nwhether self-attention is the only choice for hierarchical Vision Transformer\nto attain strong performance, and the effects of different kinds of\ncross-window communication. To this end, we replace self-attention layers with\nembarrassingly simple linear mapping layers, and the resulting proof-of-concept\narchitecture termed as LinMapper can achieve very strong performance in\nImageNet-1k image recognition. Moreover, we find that LinMapper is able to\nbetter leverage the pre-trained representations from image recognition and\ndemonstrates excellent transfer learning properties on downstream dense\nprediction tasks such as object detection and instance segmentation. We also\nexperiment with other alternatives to self-attention for content aggregation\ninside each non-overlapped window under different cross-window communication\napproaches, which all give similar competitive results. Our study reveals that\nthe \\textbf{macro architecture} of Swin model families, other than specific\naggregation layers or specific means of cross-window communication, may be more\nresponsible for its strong performance and is the real challenger to the\nubiquitous CNN's dense sliding window paradigm. Code and models will be\npublicly available to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. (arXiv:2108.01390v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01390","description":"<p>Vision transformers (ViTs) have recently received explosive popularity, but\nthe huge computational cost is still a severe issue. Since the computation\ncomplexity of ViT is quadratic with respect to the input sequence length, a\nmainstream paradigm for computation reduction is to reduce the number of\ntokens. Existing designs include structured spatial compression that uses a\nprogressive shrinking pyramid to reduce the computations of large feature maps,\nand unstructured token pruning that dynamically drops redundant tokens.\nHowever, the limitation of existing token pruning lies in two folds: 1) the\nincomplete spatial structure caused by pruning is not compatible with\nstructured spatial compression that is commonly used in modern deep-narrow\ntransformers; 2) it usually requires a time-consuming pre-training procedure.\nTo tackle the limitations and expand the applicable scenario of token pruning,\nwe present Evo-ViT, a self-motivated slow-fast token evolution approach for\nvision transformers. Specifically, we conduct unstructured instance-wise token\nselection by taking advantage of the simple and effective global class\nattention that is native to vision transformers. Then, we propose to update the\nselected informative tokens and uninformative tokens with different computation\npaths, namely, slow-fast updating. Since slow-fast updating mechanism maintains\nthe spatial structure and information flow, Evo-ViT can accelerate vanilla\ntransformers of both flat and deep-narrow structures from the very beginning of\nthe training process. Experimental results demonstrate that our method\nsignificantly reduces the computational cost of vision transformers while\nmaintaining comparable performance on image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach. (arXiv:2108.05060v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05060","description":"<p>Multitask learning is a common approach in machine learning, which allows to\ntrain multiple objectives with a shared architecture. It has been shown that by\ntraining multiple tasks together inference time and compute resources can be\nsaved, while the objectives performance remains on a similar or even higher\nlevel. However, in perception related multitask networks only closely related\ntasks can be found, such as object detection, instance and semantic\nsegmentation or depth estimation. Multitask networks with diverse tasks and\ntheir effects with respect to efficiency on one another are not well studied.\nIn this paper we augment the CenterNet anchor-free approach for training\nmultiple diverse perception related tasks together, including the task of\nobject detection and semantic segmentation as well as human pose estimation. We\nrefer to this DNN as Multitask-CenterNet (MCN). Additionally, we study\ndifferent MCN settings for efficiency. The MCN can perform several tasks at\nonce while maintaining, and in some cases even exceeding, the performance\nvalues of its corresponding single task networks. More importantly, the MCN\narchitecture decreases inference time and reduces network size when compared to\na composition of single task networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heuer_F/0/1/0/all/0/1\">Falk Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantowsky_S/0/1/0/all/0/1\">Sven Mantowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Saqib Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Georg Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-imaging real-time detection and tracking of fast-moving objects using a single-pixel detector. (arXiv:2108.06009v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.06009","description":"<p>Detection and tracking of fast-moving objects have widespread utility in many\nfields. However, fulfilling this demand for fast and efficient detecting and\ntracking using image-based techniques is problematic, owing to the complex\ncalculations and limited data processing capabilities. To tackle this problem,\nwe propose an image-free method to achieve real-time detection and tracking of\nfast-moving objects. It employs the Hadamard pattern to illuminate the\nfast-moving object by a spatial light modulator, in which the resulting light\nsignal is collected by a single-pixel detector. The single-pixel measurement\nvalues are directly used to reconstruct the position information without image\nreconstruction. Furthermore, a new sampling method is used to optimize the\npattern projection way for achieving an ultra-low sampling rate. Compared with\nthe state-of-the-art methods, our approach is not only capable of handling\nreal-time detection and tracking, but also it has a small amount of calculation\nand high efficiency. We experimentally demonstrate that the proposed method,\nusing a 22kHz digital micro-mirror device, can implement a 105fps frame rate at\na 1.28% sampling rate when tracks. Our method breaks through the traditional\ntracking ways, which can implement the object real-time tracking without image\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_F/0/1/0/all/0/1\">Fengming Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xuelei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_T/0/1/0/all/0/1\">Tianhang Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yiguang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Backbone for Hyperspectral Image Reconstruction. (arXiv:2108.07739v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.07739","description":"<p>The study of 3D hyperspectral image (HSI) reconstruction refers to the\ninverse process of snapshot compressive imaging, during which the optical\nsystem, e.g., the coded aperture snapshot spectral imaging (CASSI) system,\ncaptures the 3D spatial-spectral signal and encodes it to a 2D measurement.\nWhile numerous sophisticated neural networks have been elaborated for\nend-to-end reconstruction, trade-offs still need to be made among performance,\nefficiency (training and inference time), and feasibility (the ability of\nrestoring high resolution HSI on limited GPU memory). This raises a challenge\nto design a new baseline to conjointly meet the above requirements. In this\npaper, we fill in this blank by proposing a Spatial/Spectral Invariant Residual\nU-Net, namely SSI-ResU-Net. It differentiates with U-Net in three folds--1)\nscale/spectral-invariant learning, 2) nested residual learning, and 3)\ncomputational efficiency. Benefiting from these three modules, the proposed\nSSI-ResU-Net outperforms the current state-of-the-art method TSA-Net by over 3\ndB in PSNR and 0.036 in SSIM while only using 2.82% trainable parameters. To\nthe greatest extent, SSI-ResU-Net achieves competing performance with over\n77.3% reduction in terms of floating-point operations (FLOPs), which for the\nfirst time, makes high-resolution HSI reconstruction feasible under practical\napplication scenarios. Code and pre-trained models are made available at\nhttps://github.com/Jiamian-Wang/HSI_baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Self-training for Image Classification through Self-supervision. (arXiv:2109.00778v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00778","description":"<p>Self-training is a simple semi-supervised learning approach: Unlabelled\nexamples that attract high-confidence predictions are labelled with their\npredictions and added to the training set, with this process being repeated\nmultiple times. Recently, self-supervision -- learning without manual\nsupervision by solving an automatically-generated pretext task -- has gained\nprominence in deep learning. This paper investigates three different ways of\nincorporating self-supervision into self-training to improve accuracy in image\nclassification: self-supervision as pretraining only, self-supervision\nperformed exclusively in the first iteration of self-training, and\nself-supervision added to every iteration of self-training. Empirical results\non the SVHN, CIFAR-10, and PlantVillage datasets, using both training from\nscratch, and Imagenet-pretrained weights, show that applying self-supervision\nonly in the first iteration of self-training can greatly improve accuracy, for\na modest increase in computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer of Pretrained Model Weights Substantially Improves Semi-Supervised Image Classification. (arXiv:2109.00788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00788","description":"<p>Deep neural networks produce state-of-the-art results when trained on a large\nnumber of labeled examples but tend to overfit when small amounts of labeled\nexamples are used for training. Creating a large number of labeled examples\nrequires considerable resources, time, and effort. If labeling new data is not\nfeasible, so-called semi-supervised learning can achieve better generalisation\nthan purely supervised learning by employing unlabeled instances as well as\nlabeled ones. The work presented in this paper is motivated by the observation\nthat transfer learning provides the opportunity to potentially further improve\nperformance by exploiting models pretrained on a similar domain. More\nspecifically, we explore the use of transfer learning when performing\nsemi-supervised learning using self-learning. The main contribution is an\nempirical evaluation of transfer learning using different combinations of\nsimilarity metric learning methods and label propagation algorithms in\nsemi-supervised learning. We find that transfer learning always substantially\nimproves the model's accuracy when few labeled examples are available,\nregardless of the type of loss used for training the neural network. This\nfinding is obtained by performing extensive experiments on the SVHN, CIFAR10,\nand Plant Village image classification datasets and applying pretrained weights\nfrom Imagenet for transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning using Siamese Networks. (arXiv:2109.00794v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.00794","description":"<p>Neural networks have been successfully used as classification models yielding\nstate-of-the-art results when trained on a large number of labeled samples.\nThese models, however, are more difficult to train successfully for\nsemi-supervised problems where small amounts of labeled instances are available\nalong with a large number of unlabeled instances. This work explores a new\ntraining method for semi-supervised learning that is based on similarity\nfunction learning using a Siamese network to obtain a suitable embedding. The\nlearned representations are discriminative in Euclidean space, and hence can be\nused for labeling unlabeled instances using a nearest-neighbor classifier.\nConfident predictions of unlabeled instances are used as true labels for\nretraining the Siamese network on the expanded training set. This process is\napplied iteratively. We perform an empirical study of this iterative\nself-training algorithm. For improving unlabeled predictions, local learning\nwith global consistency [22] is also evaluated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting isocitrate dehydrogenase mutation status in glioma using structural brain networks and graph neural networks. (arXiv:2109.01854v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.01854","description":"<p>Glioma is a common malignant brain tumor with distinct survival among\npatients. The isocitrate dehydrogenase (IDH) gene mutation provides critical\ndiagnostic and prognostic value for glioma. It is of crucial significance to\nnon-invasively predict IDH mutation based on pre-treatment MRI. Machine\nlearning/deep learning models show reasonable performance in predicting IDH\nmutation using MRI. However, most models neglect the systematic brain\nalterations caused by tumor invasion, where widespread infiltration along white\nmatter tracts is a hallmark of glioma. Structural brain network provides an\neffective tool to characterize brain organisation, which could be captured by\nthe graph neural networks (GNN) to more accurately predict IDH mutation.\n</p>\n<p>Here we propose a method to predict IDH mutation using GNN, based on the\nstructural brain network of patients. Specifically, we firstly construct a\nnetwork template of healthy subjects, consisting of atlases of edges (white\nmatter tracts) and nodes (cortical/subcortical brain regions) to provide\nregions of interest (ROIs). Next, we employ autoencoders to extract the latent\nmulti-modal MRI features from the ROIs of edges and nodes in patients, to train\na GNN architecture for predicting IDH mutation. The results show that the\nproposed method outperforms the baseline models using the 3D-CNN and\n3D-DenseNet. In addition, model interpretation suggests its ability to identify\nthe tracts infiltrated by tumor, corresponding to clinical prior knowledge. In\nconclusion, integrating brain networks with GNN offers a new avenue to study\nbrain lesions using computational neuroscience and computer vision approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yonghao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1\">Stephen J. Price</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02860","description":"<p>Graph convolutional networks (GCNs) achieve promising performance for\nskeleton-based action recognition. However, in most GCN-based methods, the\nspatial-temporal graph convolution is strictly restricted by the graph topology\nwhile only captures the short-term temporal context, thus lacking the\nflexibility of feature extraction. In this work, we present a novel\narchitecture, named Graph Convolutional skeleton Transformer (GCsT), which\naddresses limitations in GCNs by introducing Transformer. Our GCsT employs all\nthe benefits of Transformer (i.e. dynamical attention and global context) while\nkeeps the advantages of GCNs (i.e. hierarchy and local topology structure). In\nGCsT, the spatial-temporal GCN forces the capture of local dependencies while\nTransformer dynamically extracts global spatial-temporal relationships.\nFurthermore, the proposed GCsT shows stronger expressive capability by adding\nadditional information present in skeleton sequences. Incorporating the\nTransformer allows that information to be introduced into the model almost\neffortlessly. We validate the proposed GCsT by conducting extensive\nexperiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU\nRGB+D 120 and Northwestern-UCLA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1\">Ruwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengfa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Miao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02862","description":"<p>Image classification is a major application domain for conventional deep\nlearning (DL). Quantum machine learning (QML) has the potential to\nrevolutionize image classification. In any typical DL-based image\nclassification, we use convolutional neural network (CNN) to extract features\nfrom the image and multi-layer perceptron network (MLP) to create the actual\ndecision boundaries. On one hand, QML models can be useful in both of these\ntasks. Convolution with parameterized quantum circuits (Quanvolution) can\nextract rich features from the images. On the other hand, quantum neural\nnetwork (QNN) models can create complex decision boundaries. Therefore,\nQuanvolution and QNN can be used to create an end-to-end QML model for image\nclassification. Alternatively, we can extract image features separately using\nclassical dimension reduction techniques such as, Principal Components Analysis\n(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to\ntrain a QNN. We review two proposals on quantum-classical hybrid ML models for\nimage classification namely, Quanvolutional Neural Network and dimension\nreduction using a classical algorithm followed by QNN. Particularly, we make a\ncase for trainable filters in Quanvolution and CAE-based feature extraction for\nimage datasets (instead of dimension reduction using linear transformations\nsuch as, PCA). We discuss various design choices, potential opportunities, and\ndrawbacks of these models. We also release a Python-based framework to create\nand explore these hybrid models with a variety of design choices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mahabubul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Satwik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaloglu_R/0/1/0/all/0/1\">Rasit Onur Topaloglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Swaroop Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03805","description":"<p>Panoptic scene understanding and tracking of dynamic agents are essential for\nrobots and automated vehicles to navigate in urban environments. As LiDARs\nprovide accurate illumination-independent geometric depictions of the scene,\nperforming these tasks using LiDAR point clouds provides reliable predictions.\nHowever, existing datasets lack diversity in the type of urban scenes and have\na limited number of dynamic object instances which hinders both learning of\nthese tasks as well as credible benchmarking of the developed methods. In this\npaper, we introduce the large-scale Panoptic nuScenes benchmark dataset that\nextends our popular nuScenes dataset with point-wise groundtruth annotations\nfor semantic segmentation, panoptic segmentation, and panoptic tracking tasks.\nTo facilitate comparison, we provide several strong baselines for each of these\ntasks on our proposed dataset. Moreover, we analyze the drawbacks of the\nexisting metrics for panoptic tracking and propose the novel instance-centric\nPAT metric that addresses the concerns. We present exhaustive experiments that\ndemonstrate the utility of Panoptic nuScenes compared to existing datasets and\nmake the online evaluation server available at nuScenes.org. We believe that\nthis extension will accelerate the research of novel methods for scene\nunderstanding of dynamic urban environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1\">Rohit Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Juana Valeria Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lubing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Event-Line Constraint for Closed-Form Velocity Initialization. (arXiv:2109.04313v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04313","description":"<p>Event cameras trigger events asynchronously and independently upon a\nsufficient change of the logarithmic brightness level. The neuromorphic sensor\nhas several advantages over standard cameras including low latency, absence of\nmotion blur, and high dynamic range. Event cameras are particularly well suited\nto sense motion dynamics in agile scenarios. We propose the continuous\nevent-line constraint, which relies on a constant-velocity motion assumption as\nwell as trifocal tensor geometry in order to express a relationship between\nline observations given by event clusters as well as first-order camera\ndynamics. Our core result is a closed-form solver for up-to-scale linear camera\nvelocity {with known angular velocity}. Nonlinear optimization is adopted to\nimprove the performance of the algorithm. The feasibility of the approach is\ndemonstrated through a careful analysis on both simulated and real data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_P/0/1/0/all/0/1\">Peng Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanting_X/0/1/0/all/0/1\">Xu Wanting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiaqi_Y/0/1/0/all/0/1\">Yang Jiaqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_K/0/1/0/all/0/1\">Kneip Laurent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IFBiD: Inference-Free Bias Detection. (arXiv:2109.04374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04374","description":"<p>This paper is the first to explore an automatic way to detect bias in deep\nconvolutional neural networks by simply looking at their weights. Furthermore,\nit is also a step towards understanding neural networks and how they work. We\nshow that it is indeed possible to know if a model is biased or not simply by\nlooking at its weights, without the model inference for an specific input. We\nanalyze how bias is encoded in the weights of deep networks through a toy\nexample using the Colored MNIST database and we also provide a realistic case\nstudy in gender detection from face images using state-of-the-art methods and\nexperimental resources. To do so, we generated two databases with 36K and 48K\nbiased models each. In the MNIST models we were able to detect whether they\npresented a strong or low bias with more than 99% accuracy, and we were also\nable to classify between four levels of bias with more than 70% accuracy. For\nthe face models, we achieved 90% accuracy in distinguishing between models\nbiased towards Asian, Black, or Caucasian ethnicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing and grounding narrated instructional videos in 3D. (arXiv:2109.04409v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04409","description":"<p>Narrated instructional videos often show and describe manipulations of\nsimilar objects, e.g., repairing a particular model of a car or laptop. In this\nwork we aim to reconstruct such objects and to localize associated narrations\nin 3D. Contrary to the standard scenario of instance-level 3D reconstruction,\nwhere identical objects or scenes are present in all views, objects in\ndifferent instructional videos may have large appearance variations given\nvarying conditions and versions of the same product. Narrations may also have\nlarge variation in natural language expressions. We address these challenges by\nthree contributions. First, we propose an approach for correspondence\nestimation combining learnt local features and dense flow. Second, we design a\ntwo-step divide and conquer reconstruction approach where the initial 3D\nreconstructions of individual videos are combined into a 3D alignment graph.\nFinally, we propose an unsupervised approach to ground natural language in\nobtained 3D reconstructions. We demonstrate the effectiveness of our approach\nfor the domain of car maintenance. Given raw instructional videos and no manual\nsupervision, our method successfully reconstructs engines of different car\nmodels and associates textual descriptions with corresponding objects in 3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_D/0/1/0/all/0/1\">Dimitri Zhukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocco_I/0/1/0/all/0/1\">Ignacio Rocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonberger_J/0/1/0/all/0/1\">Johannes L. Sch&#xf6;nberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekin_B/0/1/0/all/0/1\">Bugra Tekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Web image search engine based on LSH index and CNN Resnet50. (arXiv:2108.13301v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2108.13301","description":"<p>To implement a good Content Based Image Retrieval (CBIR) system, it is\nessential to adopt efficient search methods. One way to achieve this results is\nby exploiting approximate search techniques. In fact, when we deal with very\nlarge collections of data, using an exact search method makes the system very\nslow. In this project, we adopt the Locality Sensitive Hashing (LSH) index to\nimplement a CBIR system that allows us to perform fast similarity search on\ndeep features. Specifically, we exploit transfer learning techniques to extract\ndeep features from images; this phase is done using two famous Convolutional\nNeural Networks (CNNs) as features extractors: Resnet50 and Resnet50v2, both\npre-trained on ImageNet. Then we try out several fully connected deep neural\nnetworks, built on top of both of the previously mentioned CNNs in order to\nfine-tuned them on our dataset. In both of previous cases, we index the\nfeatures within our LSH index implementation and within a sequential scan, to\nbetter understand how much the introduction of the index affects the results.\nFinally, we carry out a performance analysis: we evaluate the relevance of the\nresult set, computing the mAP (mean Average Precision) value obtained during\nthe different experiments with respect to the number of done comparison and\nvarying the hyper-parameter values of the LSH index.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parola_M/0/1/0/all/0/1\">Marco Parola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nannini_A/0/1/0/all/0/1\">Alice Nannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poleggi_S/0/1/0/all/0/1\">Stefano Poleggi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}