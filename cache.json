{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"OPERA: Harmonizing Task-Oriented Dialogs and Information Seeking Experience. (arXiv:2206.12449v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12449","description":"<p>Existing studies in conversational AI mostly treat task-oriented dialog (TOD)\nand question answering (QA) as separate tasks. Towards the goal of constructing\na conversational agent that can complete user tasks and support information\nseeking, it is important to build a system that handles both TOD and QA with\naccess to various external knowledge. In this work, we propose a new task,\nOpen-Book TOD (OB-TOD), which combines TOD with QA task and expand external\nknowledge sources to include both explicit knowledge sources (e.g., the Web)\nand implicit knowledge sources (e.g., pre-trained language models). We create a\nnew dataset OB-MultiWOZ, where we enrich TOD sessions with QA-like information\nseeking experience grounded on external knowledge. We propose a unified model\nOPERA (Open-book End-to-end Task-oriented Dialog) which can appropriately\naccess explicit and implicit external knowledge to tackle the defined task.\nExperimental results demonstrate OPERA's superior performance compared to\nclosed-book baselines and illustrate the value of both knowledge types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Miaoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Baolin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion, Age, and Origin from Vocal Bursts. (arXiv:2206.12469v1 [cs.SD])","link":"http://arxiv.org/abs/2206.12469","description":"<p>We present Burst2Vec, our multi-task learning approach to predict emotion,\nage, and origin (i.e., native country/language) from vocal bursts. Burst2Vec\nutilises pre-trained speech representations to capture acoustic information\nfrom raw waveforms and incorporates the concept of model debiasing via\nadversarial training. Our models achieve a relative 30 % performance gain over\nbaselines using pre-extracted features and score the highest amongst all\nparticipants in the ICML ExVo 2022 Multi-Task Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anuchitanukul_A/0/1/0/all/0/1\">Atijit Anuchitanukul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The syntax-lexicon tradeoff in writing. (arXiv:2206.12485v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12485","description":"<p>As speakers turn their thoughts into sentences, they maintain a balance\nbetween the complexity of words and syntax. However, it is unclear whether this\nsyntax-lexicon tradeoff is unique to the spoken language production that is\nunder the pressure of rapid online processing. Alternatively, it is possible\nthat the tradeoff is a basic property of language irrespective of the modality\nof production. This work evaluates the relationship between the complexity of\nwords and syntactic rules in the written language of neurotypical individuals\non three different topics. We found that similar to speaking, constructing\nsentences in writing involves a tradeoff between the complexity of the lexical\nand syntactic items. We also show that the reduced online processing demands\nduring writing allows for retrieving more complex words at the cost of\nincorporating simpler syntax. This work further highlights the role of\naccessibility of the elements of a sentence as the driving force in the\nemergence of the syntax-lexicon tradeoff.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaii_N/0/1/0/all/0/1\">Neguine Rezaii</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DetIE: Multilingual Open Information Extraction Inspired by Object Detection. (arXiv:2206.12514v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12514","description":"<p>State of the art neural methods for open information extraction (OpenIE)\nusually extract triplets (or tuples) iteratively in an autoregressive or\npredicate-based manner in order not to produce duplicates. In this work, we\npropose a different approach to the problem that can be equally or more\nsuccessful. Namely, we present a novel single-pass method for OpenIE inspired\nby object detection algorithms from computer vision. We use an order-agnostic\nloss based on bipartite matching that forces unique predictions and a\nTransformer-based encoder-only architecture for sequence labeling. The proposed\napproach is faster and shows superior or similar performance in comparison with\nstate of the art models on standard benchmarks in terms of both quality metrics\nand inference time. Our model sets the new state of the art performance of\n67.7% F1 on CaRB evaluated as OIE2016 while being 3.35x faster at inference\nthan previous state of the art. We also evaluate the multilingual version of\nour model in the zero-shot setting for two languages and introduce a strategy\nfor generating synthetic multilingual data to fine-tune the model for each\nspecific language. In this setting, we show performance improvement 15% on\nmultilingual Re-OIE2016, reaching 75% F1 for both Portuguese and Spanish\nlanguages. Code and models are available at\nhttps://github.com/sberbank-ai/DetIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasilkovsky_M/0/1/0/all/0/1\">Michael Vasilkovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alekseev_A/0/1/0/all/0/1\">Anton Alekseev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malykh_V/0/1/0/all/0/1\">Valentin Malykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenbin_I/0/1/0/all/0/1\">Ilya Shenbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutubalina_E/0/1/0/all/0/1\">Elena Tutubalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salikhov_D/0/1/0/all/0/1\">Dmitriy Salikhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepnov_M/0/1/0/all/0/1\">Mikhail Stepnov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chertok_A/0/1/0/all/0/1\">Andrey Chertok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolenko_S/0/1/0/all/0/1\">Sergey Nikolenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Causes of Hallucinations in Neural Machine Translations. (arXiv:2206.12529v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12529","description":"<p>Hallucination, one kind of pathological translations that bothers Neural\nMachine Translation, has recently drawn much attention. In simple terms,\nhallucinated translations are fluent sentences but barely related to source\ninputs. Arguably, it remains an open problem how hallucination occurs. In this\npaper, we propose to use probing methods to investigate the causes of\nhallucinations from the perspective of model architecture, aiming to avoid such\nproblems in future architecture designs. By conducting experiments over various\nNMT datasets, we find that hallucination is often accompanied by the deficient\nencoder, especially embeddings, and vulnerable cross-attentions, while,\ninterestingly, cross-attention mitigates some errors caused by the encoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConcreteGraph: A Data Augmentation Method Leveraging the Properties of Concept Relatedness Estimation. (arXiv:2206.12556v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12556","description":"<p>The concept relatedness estimation (CRE) task is to determine whether two\ngiven concepts are related. Although existing methods for the semantic textual\nsimilarity (STS) task can be easily adapted to this task, the CRE task has some\nunique properties that can be leveraged to augment the datasets for addressing\nits data scarcity problem. In this paper, we construct a graph named\nConcreteGraph (Concept relatedness estimation Graph) to take advantage of the\nCRE properties. For the sampled new concept pairs from the ConcreteGraph, we\nadd an additional step of filtering out the new concept pairs with low quality\nbased on simple yet effective quality thresholding. We apply the ConcreteGraph\ndata augmentation on three Transformer-based models to show its efficacy.\nDetailed ablation study for quality thresholding further shows that even a\nlimited amount of high-quality data is more beneficial than a large quantity of\nunthresholded data. This paper is the first one to work on the WORD dataset and\nthe proposed ConcreteGraph can boost the accuracy of the Transformers by more\nthan 2%. All three Transformers, with the help of ConcreteGraph, can outperform\nthe current state-of-theart method, Concept Interaction Graph (CIG), on the\nCNSE and CNSS datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yueen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zixing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chirui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Context-aware Style Representation for Expressive Speech Synthesis. (arXiv:2206.12559v1 [cs.SD])","link":"http://arxiv.org/abs/2206.12559","description":"<p>Expressive speech synthesis, like audiobook synthesis, is still challenging\nfor style representation learning and prediction. Deriving from reference audio\nor predicting style tags from text requires a huge amount of labeled data,\nwhich is costly to acquire and difficult to define and annotate accurately. In\nthis paper, we propose a novel framework for learning style representation from\nabundant plain text in a self-supervised manner. It leverages an emotion\nlexicon and uses contrastive learning and deep clustering. We further integrate\nthe style representation as a conditioned embedding in a multi-style\nTransformer TTS. Comparing with multi-style TTS by predicting style tags\ntrained on the same dataset but with human annotations, our method achieves\nimproved results according to subjective evaluations on both in-domain and\nout-of-domain test sets in audiobook speech. Moreover, with implicit\ncontext-aware style representation, the emotion transition of synthesized audio\nin a long paragraph appears more natural. The audio samples are available on\nthe demo web.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yihan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construct a Sentence with Multiple Specified Words. (arXiv:2206.12565v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12565","description":"<p>This paper demonstrates a task to finetune a BART model so it can construct a\nsentence from an arbitrary set of words, which used to be a difficult NLP task.\nThe training task is making sentences with four words, but the trained model\ncan generate sentences when fewer or more words are provided. The output\nsentences have high quality in general. The model can have some real-world\napplications, and this task can be used as an evaluation mechanism for any\nlanguage model as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuanliang Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Self-Attention for Language Understanding. (arXiv:2206.12608v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12608","description":"<p>An ultimate language system aims at the high generalization and robustness\nwhen adapting to diverse scenarios. Unfortunately, the recent white hope\npre-trained language models (PrLMs) barely escape from stacking excessive\nparameters to the over-parameterized Transformer architecture to achieve higher\nperformances. This paper thus proposes \\textit{Adversarial Self-Attention}\nmechanism (ASA), which adversarially reconstructs the Transformer attentions\nand facilitates model training from contaminated model structures, coupled with\na fast and simple implementation for better PrLM building. We conduct\ncomprehensive evaluation across a wide range of tasks on both pre-training and\nfine-tuning stages. For pre-training, ASA unfolds remarkable performance gain\ncompared to regular training for longer periods. For fine-tuning, ASA-empowered\nmodels consistently outweigh naive models by a large margin considering both\ngeneralization and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as Knowledge Embeddings. (arXiv:2206.12617v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12617","description":"<p>Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding\nentities and relations into continuous vector spaces. Existing methods are\nmainly structure-based or description-based. Structure-based methods learn\nrepresentations that preserve the inherent structure of KGs. They cannot well\nrepresent abundant long-tail entities in real-world KGs with limited structural\ninformation. Description-based methods leverage textual information and\nlanguage models. Prior approaches in this direction barely outperform\nstructure-based ones, and suffer from problems like expensive negative sampling\nand restrictive description demand. In this paper, we propose LMKE, which\nadopts Language Models to derive Knowledge Embeddings, aiming at both enriching\nrepresentations of long-tail entities and solving problems of prior\ndescription-based methods. We formulate description-based KE learning with a\ncontrastive learning framework to improve efficiency in training and\nevaluation. Experimental results show that LMKE achieves state-of-the-art\nperformance on KE benchmarks of link prediction and triple classification,\nespecially for long-tail entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qianyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling a Pretrained Language Model to a Multilingual ASR Model. (arXiv:2206.12638v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12638","description":"<p>Multilingual speech data often suffer from long-tailed language distribution,\nresulting in performance degradation. However, multilingual text data is much\neasier to obtain, yielding a more useful general language model. Hence, we are\nmotivated to distill the rich knowledge embedded inside a well-trained teacher\ntext model to the student speech model. We propose a novel method called the\nDistilling a Language model to a Speech model (Distill-L2S), which aligns the\nlatent representations of two different modalities. The subtle differences are\nhandled by the shrinking mechanism, nearest-neighbor interpolation, and a\nlearnable linear projection layer. We demonstrate the effectiveness of our\ndistillation method by applying it to the multilingual automatic speech\nrecognition (ASR) task. We distill the transformer-based cross-lingual language\nmodel (InfoXLM) while fine-tuning the large-scale multilingual ASR model\n(XLSR-wav2vec 2.0) for each language. We show the superiority of our method on\n20 low-resource languages of the CommonVoice dataset with less than 100 hours\nof speech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kwanghee Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyung-Min Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis with R: Natural Language Processing for Semi-Automated Assessments of Qualitative Data. (arXiv:2206.12649v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12649","description":"<p>Sentiment analysis is a sub-discipline in the field of natural language\nprocessing and computational linguistics and can be used for automated or\nsemi-automated analyses of text documents. One of the aims of these analyses is\nto recognize an expressed attitude as positive or negative as it can be\ncontained in comments on social media platforms or political documents and\nspeeches as well as fictional and nonfictional texts. Regarding analyses of\ncomments on social media platforms, this is an extension of the previous\ntutorial on semi-automated screenings of social media network data. A\nlongitudinal perspective regarding social media comments as well as\ncross-sectional perspectives regarding fictional and nonfictional texts, e.g.\nentire books and libraries, can lead to extensive text documents. Their\nanalyses can be simplified and accelerated by using sentiment analysis with\nacceptable inter-rater reliability. Therefore, this tutorial introduces the\nbasic functions for performing a sentiment analysis with R and explains how\ntext documents can be analysed step by step - regardless of their underlying\nformatting. All prerequisites and steps are described in detail and associated\ncodes are available on GitHub. A comparison of two political speeches\nillustrates a possible use case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klinkhammer_D/0/1/0/all/0/1\">Dennis Klinkhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesizing Personalized Non-speech Vocalization from Discrete Speech Representations. (arXiv:2206.12662v1 [cs.SD])","link":"http://arxiv.org/abs/2206.12662","description":"<p>We formulated non-speech vocalization (NSV) modeling as a text-to-speech task\nand verified its viability. Specifically, we evaluated the phonetic\nexpressivity of HUBERT speech units on NSVs and verified our model's ability to\ncontrol over speaker timbre even though the training data is speaker few-shot.\nIn addition, we substantiated that the heterogeneity in recording conditions is\nthe major obstacle for NSV modeling. Finally, we discussed five improvements\nover our method for future research. Audio samples of synthesized NSVs are\navailable on our demo page: https://resemble-ai.github.io/reLaugh.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chin-Cheng Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Semantic Answer Similarity Metrics. (arXiv:2206.12664v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12664","description":"<p>There are several issues with the existing general machine translation or\nnatural language generation evaluation metrics, and question-answering (QA)\nsystems are indifferent in that context. To build robust QA systems, we need\nthe ability to have equivalently robust evaluation systems to verify whether\nmodel predictions to questions are similar to ground-truth annotations. The\nability to compare similarity based on semantics as opposed to pure string\noverlap is important to compare models fairly and to indicate more realistic\nacceptance criteria in real-life applications. We build upon the first to our\nknowledge paper that uses transformer-based model metrics to assess semantic\nanswer similarity and achieve higher correlations to human judgement in the\ncase of no lexical overlap. We propose cross-encoder augmented bi-encoder and\nBERTScore models for semantic answer similarity, trained on a new dataset\nconsisting of name pairs of US-American public figures. As far as we are\nconcerned, we provide the first dataset of co-referent name string pairs along\nwith their similarities, which can be used for training.\n</p>\n<p>Machine Learning &amp; Applications 4th International Conference on Machine\nLearning &amp; Applications (CMLA 2022) June 25~26, 2022, Copenhagen, Denmark\nVolume Editors : David C. Wyld, Dhinaharan Nagamalai (Eds) ISBN :\n978-1-925953-69-5\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mustafazade_F/0/1/0/all/0/1\">Farida Mustafazade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebbinghaus_P/0/1/0/all/0/1\">Peter Ebbinghaus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEVR: Improving Speech Recognition by Token Entropy Variance Reduction. (arXiv:2206.12693v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12693","description":"<p>This paper presents TEVR, a speech recognition model designed to minimize the\nvariation in token entropy w.r.t. to the language model. This takes advantage\nof the fact that if the language model will reliably and accurately predict a\ntoken anyway, then the acoustic model doesn't need to be accurate in\nrecognizing it. We train German ASR models with 900 million parameters and show\nthat on CommonVoice German, TEVR scores a very competitive 3.64% word error\nrate, which outperforms the best reported results by a relative 16.89%\nreduction in word error rate. We hope that releasing our fully trained speech\nrecognition pipeline to the community will lead to privacy-preserving offline\nvirtual assistants in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krabbenhoft_H/0/1/0/all/0/1\">Hajo Nils Krabbenh&#xf6;ft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_E/0/1/0/all/0/1\">Erhardt Barth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protoformer: Embedding Prototypes for Transformers. (arXiv:2206.12710v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12710","description":"<p>Transformers have been widely applied in text classification. Unfortunately,\nreal-world data contain anomalies and noisy labels that cause challenges for\nstate-of-art Transformers. This paper proposes Protoformer, a novel\nself-learning framework for Transformers that can leverage problematic samples\nfor text classification. Protoformer features a selection mechanism for\nembedding samples that allows us to efficiently extract and utilize anomalies\nprototypes and difficult class prototypes. We demonstrated such capabilities on\ndatasets with diverse textual structures (e.g., Twitter, IMDB, ArXiv). We also\napplied the framework to several models. The results indicate that Protoformer\ncan improve current Transformers in various empirical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farhangi_A/0/1/0/all/0/1\">Ashkan Farhangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_N/0/1/0/all/0/1\">Ning Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haiyan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Arthur Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhishan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective. (arXiv:2206.12759v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12759","description":"<p>Accented speech recognition and accent classification are relatively\nunder-explored research areas in speech technology. Recently, deep\nlearning-based methods and Transformer-based pretrained models have achieved\nsuperb performances in both areas. However, most accent classification tasks\nfocused on classifying different kinds of English accents and little attention\nwas paid to geographically-proximate accent classification, especially under a\nlow-resource setting where forensic speech science tasks usually encounter. In\nthis paper, we explored three main accent modelling methods combined with two\ndifferent classifiers based on 105 speaker recordings retrieved from five urban\nvarieties in Northern England. Although speech representations generated from\npretrained models generally have better performances in downstream\nclassification, traditional methods like Mel Frequency Cepstral Coefficients\n(MFCCs) and formant measurements are equipped with specific strengths. These\nresults suggest that in forensic phonetics scenario where data are relatively\nscarce, a simple modelling method and classifier could be competitive with\nstate-of-the-art pretrained speech models as feature extractors, which could\nenhance a sooner estimation for the accent information in practices. Besides,\nour findings also cross-validated a new methodology in quantifying\nsociophonetic changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qingcheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Dading Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Auxiliary Learning for Low-resource Spoken Language Understanding. (arXiv:2206.12774v1 [eess.AS])","link":"http://arxiv.org/abs/2206.12774","description":"<p>Spoken language understanding (SLU) treats automatic speech recognition (ASR)\nand natural language understanding (NLU) as a unified task and usually suffers\nfrom data scarcity. We exploit an ASR and NLU joint training method based on\nmeta auxiliary learning to improve the performance of low-resource SLU task by\nonly taking advantage of abundant manual transcriptions of speech data. One\nobvious advantage of such method is that it provides a flexible framework to\nimplement a low-resource SLU training task without requiring access to any\nfurther semantic annotations. In particular, a NLU model is taken as label\ngeneration network to predict intent and slot tags from texts; a multi-task\nnetwork trains ASR task and SLU task synchronously from speech; and the\npredictions of label generation network are delivered to the multi-task network\nas semantic targets. The efficiency of the proposed algorithm is demonstrated\nwith experiments on the public CATSLU dataset, which produces more suitable ASR\nhypotheses for the downstream NLU task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yingying Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shilei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Guided Multi-View Multi-Domain Fake News Detection. (arXiv:2206.12808v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12808","description":"<p>The wide spread of fake news is increasingly threatening both individuals and\nsociety. Great efforts have been made for automatic fake news detection on a\nsingle domain (e.g., politics). However, correlations exist commonly across\nmultiple news domains, and thus it is promising to simultaneously detect fake\nnews of multiple domains. Based on our analysis, we pose two challenges in\nmulti-domain fake news detection: 1) domain shift, caused by the discrepancy\namong domains in terms of words, emotions, styles, etc. 2) domain labeling\nincompleteness, stemming from the real-world categorization that only outputs\none single domain label, regardless of topic diversity of a news piece. In this\npaper, we propose a Memory-guided Multi-view Multi-domain Fake News Detection\nFramework (M$^3$FEND) to address these two challenges. We model news pieces\nfrom a multi-view perspective, including semantics, emotion, and style.\nSpecifically, we propose a Domain Memory Bank to enrich domain information\nwhich could discover potential domain labels based on seen news pieces and\nmodel domain characteristics. Then, with enriched domain information as input,\na Domain Adapter could adaptively aggregate discriminative information from\nmultiple views for news in various domains. Extensive offline experiments on\nEnglish and Chinese datasets demonstrate the effectiveness of M$^3$FEND, and\nonline tests verify its superiority in practice. Our code is available at\nhttps://github.com/ICTMCG/M3FEND.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_Q/0/1/0/all/0/1\">Qiong Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Comparison of Encoders for Attention based End to End Speech Recognition in Standalone and Rescoring Mode. (arXiv:2206.12829v1 [cs.SD])","link":"http://arxiv.org/abs/2206.12829","description":"<p>The streaming automatic speech recognition (ASR) models are more popular and\nsuitable for voice-based applications. However, non-streaming models provide\nbetter performance as they look at the entire audio context. To leverage the\nbenefits of the non-streaming model in streaming applications like voice\nsearch, it is commonly used in second pass re-scoring mode. The candidate\nhypothesis generated using steaming models is re-scored using a non-streaming\nmodel. In this work, we evaluate the non-streaming attention-based end-to-end\nASR models on the Flipkart voice search task in both standalone and re-scoring\nmodes. These models are based on Listen-Attend-Spell (LAS) encoder-decoder\narchitecture. We experiment with different encoder variations based on LSTM,\nTransformer, and Conformer. We compare the latency requirements of these models\nalong with their performance. Overall we show that the Transformer model offers\nacceptable WER with the lowest latency requirements. We report a relative WER\nimprovement of around 16% with the second pass LAS re-scoring with latency\noverhead under 5ms. We also highlight the importance of CNN front-end with\nTransformer architecture to achieve comparable word error rates (WER).\nMoreover, we observe that in the second pass re-scoring mode all the encoders\nprovide similar benefits whereas the difference in performance is prominent in\nstandalone text generation mode.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Subodh Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Your Autoregressive Generative Model Can be Better If You Treat It as an Energy-Based One. (arXiv:2206.12840v1 [cs.LG])","link":"http://arxiv.org/abs/2206.12840","description":"<p>Autoregressive generative models are commonly used, especially for those\ntasks involving sequential data. They have, however, been plagued by a slew of\ninherent flaws due to the intrinsic characteristics of chain-style conditional\nmodeling (e.g., exposure bias or lack of long-range coherence), severely\nlimiting their ability to model distributions properly. In this paper, we\npropose a unique method termed E-ARM for training autoregressive generative\nmodels that takes advantage of a well-designed energy-based learning objective.\nBy leveraging the extra degree of freedom of the softmax operation, we are\nallowed to make the autoregressive model itself be an energy-based model for\nmeasuring the likelihood of input without introducing any extra parameters.\nFurthermore, we show that E-ARM can be trained efficiently and is capable of\nalleviating the exposure bias problem and increase temporal coherence for\nautoregressive generative models. Extensive empirical results, covering\nbenchmarks like language modeling, neural machine translation, and image\ngeneration, demonstrate the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1\">Tong Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1\">Hengzhi Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual embedding and model weighting by fusing domain knowledge on Biomedical Question Answering. (arXiv:2206.12866v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12866","description":"<p>Biomedical Question Answering aims to obtain an answer to the given question\nfrom the biomedical domain. Due to its high requirement of biomedical domain\nknowledge, it is difficult for the model to learn domain knowledge from limited\ntraining data. We propose a contextual embedding method that combines\nopen-domain QA model \\aoa and \\biobert model pre-trained on biomedical domain\ndata. We adopt unsupervised pre-training on large biomedical corpus and\nsupervised fine-tuning on biomedical question answering dataset. Additionally,\nwe adopt an MLP-based model weighting layer to automatically exploit the\nadvantages of two models to provide the correct answer. The public dataset\n\\biomrc constructed from PubMed corpus is used to evaluate our method.\nExperimental results show that our model outperforms state-of-the-art system by\na large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jingya Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhixuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zhongzheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yongping Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Dementia Detection in Spoken Language. (arXiv:2206.12879v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12879","description":"<p>Dementia is a growing problem as our society ages, and detection methods are\noften invasive and expensive. Recent deep-learning techniques can offer a\nfaster diagnosis and have shown promising results. However, they require large\namounts of labelled data which is not easily available for the task of dementia\ndetection. One effective solution to sparse data problems is data augmentation,\nthough the exact methods need to be selected carefully. To date, there has been\nno empirical study of data augmentation on Alzheimer's disease (AD) datasets\nfor NLP and speech processing. In this work, we investigate data augmentation\ntechniques for the task of AD detection and perform an empirical evaluation of\nthe different approaches on two kinds of models for both the text and audio\ndomains. We use a transformer-based model for both domains, and SVM and Random\nForest models for the text and audio domains, respectively. We generate\nadditional samples using traditional as well as deep learning based methods and\nshow that data augmentation improves performance for both the text- and\naudio-based models and that such results are comparable to state-of-the-art\nresults on the popular ADReSS set, with carefully crafted architectures and\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hledikova_A/0/1/0/all/0/1\">Anna Hl&#xe9;dikov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woszczyk_D/0/1/0/all/0/1\">Dominika Woszczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acman_A/0/1/0/all/0/1\">Alican Acman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetriou_S/0/1/0/all/0/1\">Soteris Demetriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotated Speech Corpus for Low Resource Indian Languages: Awadhi, Bhojpuri, Braj and Magahi. (arXiv:2206.12931v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12931","description":"<p>In this paper we discuss an in-progress work on the development of a speech\ncorpus for four low-resource Indo-Aryan languages -- Awadhi, Bhojpuri, Braj and\nMagahi using the field methods of linguistic data collection. The total size of\nthe corpus currently stands at approximately 18 hours (approx. 4-5 hours each\nlanguage) and it is transcribed and annotated with grammatical information such\nas part-of-speech tags, morphological features and Universal dependency\nrelationships. We discuss our methodology for data collection in these\nlanguages, most of which was done in the middle of the COVID-19 pandemic, with\none of the aims being to generate some additional income for low-income groups\nspeaking these languages. In the paper, we also discuss the results of the\nbaseline experiments for automatic speech recognition system in these\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ritesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratan_S/0/1/0/all/0/1\">Shyam Ratan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_M/0/1/0/all/0/1\">Mohit Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sonal Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+lahiri_b/0/1/0/all/0/1\">bornini lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_V/0/1/0/all/0/1\">Vivek Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Atul Kr. Ojha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Training Recipe for a Robust Conformer-based Hybrid Model. (arXiv:2206.12955v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12955","description":"<p>Speaker adaptation is important to build robust automatic speech recognition\n(ASR) systems. In this work, we investigate various methods for speaker\nadaptive training (SAT) based on feature-space approaches for a conformer-based\nacoustic model (AM) on the Switchboard 300h dataset. We propose a method,\ncalled Weighted-Simple-Add, which adds weighted speaker information vectors to\nthe input of the multi-head self-attention module of the conformer AM. Using\nthis method for SAT, we achieve 3.5% and 4.5% relative improvement in terms of\nWER on the CallHome part of Hub5'00 and Hub5'01 respectively. Moreover, we\nbuild on top of our previous work where we proposed a novel and competitive\ntraining recipe for a conformer-based hybrid AM. We extend and improve this\nrecipe where we achieve 11% relative improvement in terms of word-error-rate\n(WER) on Switchboard 300h Hub5'00 dataset. We also make this recipe efficient\nby reducing the total number of parameters by 34% relative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable and High-Performance Hate and Offensive Speech Detection. (arXiv:2206.12983v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12983","description":"<p>The spread of information through social media platforms can create\nenvironments possibly hostile to vulnerable communities and silence certain\ngroups in society. To mitigate such instances, several models have been\ndeveloped to detect hate and offensive speech. Since detecting hate and\noffensive speech in social media platforms could incorrectly exclude\nindividuals from social media platforms, which can reduce trust, there is a\nneed to create explainable and interpretable models. Thus, we build an\nexplainable and interpretable high performance model based on the XGBoost\nalgorithm, trained on Twitter data. For unbalanced Twitter data, XGboost\noutperformed the LSTM, AutoGluon, and ULMFiT models on hate speech detection\nwith an F1 score of 0.75 compared to 0.38 and 0.37, and 0.38 respectively. When\nwe down-sampled the data to three separate classes of approximately 5000\ntweets, XGBoost performed better than LSTM, AutoGluon, and ULMFiT; with F1\nscores for hate speech detection of 0.79 vs 0.69, 0.77, and 0.66 respectively.\nXGBoost also performed better than LSTM, AutoGluon, and ULMFiT in the\ndown-sampled version for offensive speech detection with F1 score of 0.83 vs\n0.88, 0.82, and 0.79 respectively. We use Shapley Additive Explanations (SHAP)\non our XGBoost models' outputs to makes it explainable and interpretable\ncompared to LSTM, AutoGluon and ULMFiT that are black-box models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Babaeianjelodar_M/0/1/0/all/0/1\">Marzieh Babaeianjelodar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prudhvi_G/0/1/0/all/0/1\">Gurram Poorna Prudhvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_S/0/1/0/all/0/1\">Stephen Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1\">Sumona Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Soumyabrata Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Navin Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Intention Understanding in a Head-final Language: A Disambiguation Utilizing Intonation-dependency. (arXiv:1811.04231v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1811.04231","description":"<p>For a large portion of real-life utterances, the intention cannot be solely\ndecided by either their semantic or syntactic characteristics. Although not all\nthe sociolinguistic and pragmatic information can be digitized, at least\nphonetic features are indispensable in understanding the spoken language.\nEspecially in head-final languages such as Korean, sentence-final prosody has\ngreat importance in identifying the speaker's intention. This paper suggests a\nsystem which identifies the inherent intention of a spoken utterance given its\ntranscript, in some cases using auxiliary acoustic features. The main point\nhere is a separate distinction for cases where discrimination of intention\nrequires an acoustic cue. Thus, the proposed classification system decides\nwhether the given utterance is a fragment, statement, question, command, or a\nrhetorical question/command, utilizing the intonation-dependency coming from\nthe head-finality. Based on an intuitive understanding of the Korean language\nthat is engaged in the data annotation, we construct a network which identifies\nthe intention of a speech, and validate its utility with the test sentences.\nThe system, if combined with up-to-date speech recognizers, is expected to be\nflexibly inserted into various language understanding modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1\">Won Ik Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyeon Seung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Ji Won Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seok Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1\">Nam Soo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions Attributes. (arXiv:2007.06954v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.06954","description":"<p>This paper describes a large global dataset on people's discourse and\nresponses to the COVID-19 pandemic over the Twitter platform. From 28 January\n2020 to 1 June 2022, we collected and processed over 252 million Twitter posts\nfrom more than 29 million unique users using four keywords: \"corona\", \"wuhan\",\n\"nCov\" and \"covid\". Leveraging probabilistic topic modelling and pre-trained\nmachine learning-based emotion recognition algorithms, we labelled each tweet\nwith seventeen attributes, including a) ten binary attributes indicating the\ntweet's relevance (1) or irrelevance (0) to the top ten detected topics, b)\nfive quantitative emotion attributes indicating the degree of intensity of the\nvalence or sentiment (from 0: extremely negative to 1: extremely positive) and\nthe degree of intensity of fear, anger, sadness and happiness emotions (from 0:\nnot at all to 1: extremely intense), and c) two categorical attributes\nindicating the sentiment (very negative, negative, neutral or mixed, positive,\nvery positive) and the dominant emotion (fear, anger, sadness, happiness, no\nspecific emotion) the tweet is mainly expressing. We discuss the technical\nvalidity and report the descriptive statistics of these attributes, their\ntemporal distribution, and geographic representation. The paper concludes with\na discussion of the dataset's usage in communication, psychology, public\nhealth, economics, and epidemiology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raj Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwanath_A/0/1/0/all/0/1\">Ajay Vishwanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing conversational agents' overconfidence through linguistic calibration. (arXiv:2012.14983v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14983","description":"<p>While improving neural dialogue agents' factual accuracy is the object of\nmuch research, another important aspect of communication, less studied in the\nsetting of neural dialogue, is transparency about ignorance. In this work, we\nanalyze to what extent state-of-the-art chit-chat models are linguistically\ncalibrated in the sense that their verbalized expression of doubt (or\nconfidence) matches the likelihood that the model's responses are factually\nincorrect (or correct). We find that these models are poorly calibrated, yet we\nshow that likelihood of correctness can accurately be predicted. By\nincorporating such metacognitive features into the training of a controllable\ngeneration model, we obtain a dialogue agent with greatly improved linguistic\ncalibration. While improving neural dialogue agents' factual accuracy is the\nobject of much research, another important aspect of communication, less\nstudied in the setting of neural dialogue, is transparency about ignorance. In\nthis work, we analyze to what extent state-of-the-art chit-chat models are\nlinguistically calibrated in the sense that their verbalized expression of\ndoubt (or confidence) matches the likelihood that the model's responses are\nfactually incorrect (or correct). We find that these models are poorly\ncalibrated, yet we show that likelihood of correctness can accurately be\npredicted. By incorporating such metacognitive features into the training of a\ncontrollable generation model, we obtain a dialogue agent with greatly improved\nlinguistic calibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mielke_S/0/1/0/all/0/1\">Sabrina J. Mielke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1\">Emily Dinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.06450","description":"<p>We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sumeet S. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karayev_S/0/1/0/all/0/1\">Sergey Karayev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Groundedness in Dialogue Systems: The BEGIN Benchmark. (arXiv:2105.00071v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00071","description":"<p>Knowledge-grounded dialogue systems powered by large language models often\ngenerate responses that, while fluent, are not attributable to a relevant\nsource of information. Progress towards models that do not exhibit this issue\nrequires evaluation metrics that can quantify its prevalence. To this end, we\nintroduce the Benchmark for Evaluation of Grounded INteraction (BEGIN),\ncomprised of 12k dialogue turns generated by neural dialogue systems trained on\nthree knowledge-grounded dialogue corpora. We collect human annotations\nassessing the extent to which the models' responses can be attributed to the\ngiven background information. We then use BEGIN to analyze eight evaluation\nmetrics. We find that these metrics rely on spurious correlations, do not\nreliably distinguish attributable abstractive responses from unattributable\nones, and perform substantially worse when the knowledge source is longer. Our\nfindings underscore the need for more sophisticated and robust evaluation\nmetrics for knowledge-grounded dialogue. We make BEGIN publicly available at\nhttps://github.com/google/BEGIN-dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1\">Hannah Rashkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07847","description":"<p>While unbiased machine learning models are essential for many applications,\nbias is a human-defined concept that can vary across tasks. Given only\ninput-label pairs, algorithms may lack sufficient information to distinguish\nstable (causal) features from unstable (spurious) features. However, related\ntasks often share similar biases -- an observation we may leverage to develop\nstable classifiers in the transfer setting. In this work, we explicitly inform\nthe target classifier about unstable features in the source tasks.\nSpecifically, we derive a representation that encodes the unstable features by\ncontrasting different data environments in the source task. We achieve\nrobustness by clustering data of the target task according to this\nrepresentation and minimizing the worst-case risk across these clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask for both synthetically generated environments and real-world environments.\nOur code is available at https://github.com/YujiaBao/Tofu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"gaBERT -- an Irish Language Model. (arXiv:2107.12930v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12930","description":"<p>The BERT family of neural language models have become highly popular due to\ntheir ability to provide sequences of text with rich context-sensitive token\nencodings which are able to generalise well to many NLP tasks. We introduce\ngaBERT, a monolingual BERT model for the Irish language. We compare our gaBERT\nmodel to multilingual BERT and the monolingual Irish WikiBERT, and we show that\ngaBERT provides better representations for a downstream parsing task. We also\nshow how different filtering criteria, vocabulary size and the choice of\nsubword tokenisation model affect downstream performance. We compare the\nresults of fine-tuning a gaBERT model with an mBERT model for the task of\nidentifying verbal multiword expressions, and show that the fine-tuned gaBERT\nmodel also performs better at this task. We release gaBERT and related code to\nthe community\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barry_J/0/1/0/all/0/1\">James Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Joachim Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassidy_L/0/1/0/all/0/1\">Lauren Cassidy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowap_A/0/1/0/all/0/1\">Alan Cowap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynn_T/0/1/0/all/0/1\">Teresa Lynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_A/0/1/0/all/0/1\">Abigail Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meachair_M/0/1/0/all/0/1\">M&#xed;che&#xe1;l J. &#xd3; Meachair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution. (arXiv:2108.13530v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13530","description":"<p>We consider the task of document-level entity linking (EL), where it is\nimportant to make consistent decisions for entity mentions over the full\ndocument jointly. We aim to leverage explicit \"connections\" among mentions\nwithin the document itself: we propose to join the EL task with that of\ncoreference resolution (coref). This is complementary to related works that\nexploit either (i) implicit document information (e.g., latent relations among\nentity mentions, or general language models) or (ii) connections between the\ncandidate links (e.g, as inferred from the external knowledge base).\nSpecifically, we cluster mentions that are linked via coreference, and enforce\na single EL for all of the clustered mentions together. The latter constraint\nhas the added benefit of increased coverage by joining EL candidate lists for\nthe thus clustered mentions. We formulate the coref+EL problem as a structured\nprediction task over directed trees and use a globally normalized model to\nsolve it. Experimental results on two datasets show a boost of up to +5%\nF1-score on both coref and EL tasks, compared to their standalone counterparts.\nFor a subset of hard cases, with individual mentions lacking the correct EL in\ntheir candidate entity list, we obtain a +50% increase in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaporojets_K/0/1/0/all/0/1\">Klim Zaporojets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese. (arXiv:2109.09701v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09701","description":"<p>We present BARTpho with two versions, BARTpho-syllable and BARTpho-word,\nwhich are the first public large-scale monolingual sequence-to-sequence models\npre-trained for Vietnamese. BARTpho uses the \"large\" architecture and the\npre-training scheme of the sequence-to-sequence denoising autoencoder BART,\nthus it is especially suitable for generative NLP tasks. We conduct experiments\nto compare our BARTpho with its competitor mBART on a downstream task of\nVietnamese text summarization and show that: in both automatic and human\nevaluations, BARTpho outperforms the strong baseline mBART and improves the\nstate-of-the-art. We further evaluate and compare BARTpho and mBART on the\nVietnamese capitalization and punctuation restoration tasks and also find that\nBARTpho is more effective than mBART on these two tasks. We publicly release\nBARTpho to facilitate future research and applications of generative Vietnamese\nNLP tasks. Our BARTpho models are available at\nhttps://github.com/VinAIResearch/BARTpho\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nguyen Luong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duong Minh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation Approaches in Natural Language Processing: A Survey. (arXiv:2110.01852v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01852","description":"<p>As an effective strategy, data augmentation (DA) alleviates data scarcity\nscenarios where deep learning techniques may fail. It is widely applied in\ncomputer vision then introduced to natural language processing and achieves\nimprovements in many tasks. One of the main focuses of the DA methods is to\nimprove the diversity of training data, thereby helping the model to better\ngeneralize to unseen testing data. In this survey, we frame DA methods into\nthree categories based on the diversity of augmented data, including\nparaphrasing, noising, and sampling. Our paper sets out to analyze DA methods\nin detail according to the above categories. Further, we also introduce their\napplications in NLP tasks as well as the challenges. Some helpful resources are\nprovided in the appendix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTC Variations Through New WFST Topologies. (arXiv:2110.03098v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.03098","description":"<p>This paper presents novel Weighted Finite-State Transducer (WFST) topologies\nto implement Connectionist Temporal Classification (CTC)-like algorithms for\nautomatic speech recognition. Three new CTC variants are proposed: (1) the\n\"compact-CTC\", in which direct transitions between units are replaced with\n&lt;epsilon&gt; back-off transitions; (2) the \"minimal-CTC\", that only adds &lt;blank&gt;\nself-loops when used in WFST-composition; and (3) the \"selfless-CTC\" variants,\nwhich disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times\nsmaller WFST decoding graphs and reduces memory consumption by two times when\ntraining CTC models with the LF-MMI objective without hurting the recognition\naccuracy. Minimal-CTC reduces graph size and memory consumption by two and four\ntimes for the cost of a small accuracy drop. Using selfless-CTC can improve the\naccuracy for wide context window models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Laptev_A/0/1/0/all/0/1\">Aleksandr Laptev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majumdar_S/0/1/0/all/0/1\">Somshubra Majumdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05354","description":"<p>Text-only adaptation of an end-to-end (E2E) model remains a challenging task\nfor automatic speech recognition (ASR). Language model (LM) fusion-based\napproaches require an additional external LM during inference, significantly\nincreasing the computation cost. To overcome this, we propose an internal LM\nadaptation (ILMA) of the E2E model using text-only data. Trained with\naudio-transcript pairs, an E2E model implicitly learns an internal LM that\ncharacterizes the token sequence probability which is approximated by the E2E\nmodel output after zeroing out the encoder contribution. During ILMA, we\nfine-tune the internal LM, i.e., the E2E components excluding the encoder, to\nminimize a cross-entropy loss. To make ILMA effective, it is essential to train\nthe E2E model with an internal LM loss besides the standard E2E loss.\nFurthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler\ndivergence between the output distributions of the adapted and unadapted\ninternal LMs. ILMA is the most effective when we update only the last linear\nlayer of the joint network. ILMA enables a fast text-only adaptation of the E2E\nmodel without increasing the run-time computational cost. Experimented with\n30K-hour trained transformer transducer models, ILMA achieves up to 34.9%\nrelative word error rate reduction from the unadapted baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ITA: Image-Text Alignments for Multi-Modal Named Entity Recognition. (arXiv:2112.06482v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06482","description":"<p>Recently, Multi-modal Named Entity Recognition (MNER) has attracted a lot of\nattention. Most of the work utilizes image information through region-level\nvisual representations obtained from a pretrained object detector and relies on\nan attention mechanism to model the interactions between image and text\nrepresentations. However, it is difficult to model such interactions as image\nand text representations are trained separately on the data of their respective\nmodality and are not aligned in the same space. As text representations take\nthe most important role in MNER, in this paper, we propose {\\bf I}mage-{\\bf\nt}ext {\\bf A}lignments (ITA) to align image features into the textual space, so\nthat the attention mechanism in transformer-based pretrained textual embeddings\ncan be better utilized. ITA first aligns the image into regional object tags,\nimage-level captions and optical characters as visual contexts, concatenates\nthem with the input texts as a new cross-modal input, and then feeds it into a\npretrained textual embedding model. This makes it easier for the attention\nmodule of a pretrained textual embedding model to model the interaction between\nthe two modalities since they are both represented in the textual space. ITA\nfurther aligns the output distributions predicted from the cross-modal input\nand textual input views so that the MNER model can be more practical in dealing\nwith text-only inputs and robust to noises from images. In our experiments, we\nshow that ITA models can achieve state-of-the-art accuracy on multi-modal Named\nEntity Recognition datasets, even without image information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1\">Min Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zixia Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1\">Nguyen Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black-Box Tuning for Language-Model-as-a-Service. (arXiv:2201.03514v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03514","description":"<p>Extremely large pre-trained language models (PTMs) such as GPT-3 are usually\nreleased as a service. It allows users to design task-specific prompts to query\nthe PTMs through some black-box APIs. In such a scenario, which we call\nLanguage-Model-as-a-Service (LMaaS), the gradients of PTMs are usually\nunavailable. Can we optimize the task prompts by only accessing the model\ninference APIs? This paper proposes the black-box tuning framework to optimize\nthe continuous prompt prepended to the input text via derivative-free\noptimization. Instead of optimizing in the original high-dimensional prompt\nspace, which is intractable for traditional derivative-free optimization, we\nperform optimization in a randomly generated subspace due to the low intrinsic\ndimensionality of large PTMs. The experimental results show that the black-box\ntuning with RoBERTa on a few labeled samples not only significantly outperforms\nmanual prompt and GPT-3's in-context learning, but also surpasses the\ngradient-based counterparts, i.e., prompt tuning and full model tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hong Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. (arXiv:2201.03713v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03713","description":"<p>We introduce CVSS, a massively multilingual-to-English speech-to-speech\ntranslation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21\nlanguages into English. CVSS is derived from the Common Voice speech corpus and\nthe CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the\ntranslation text from CoVoST 2 into speech using state-of-the-art TTS systems.\nTwo versions of translation speeches are provided: 1) CVSS-C: All the\ntranslation speeches are in a single high-quality canonical voice; 2) CVSS-T:\nThe translation speeches are in voices transferred from the corresponding\nsource speeches. In addition, CVSS provides normalized translation text which\nmatches the pronunciation in the translation speech. On each version of CVSS,\nwe built baseline multilingual direct S2ST models and cascade S2ST models,\nverifying the effectiveness of the corpus. To build strong cascade S2ST\nbaselines, we trained an ST model on CoVoST 2, which outperforms the previous\nstate-of-the-art trained on the corpus without extra data by 5.8 BLEU.\nNevertheless, the performance of the direct S2ST models approaches the strong\ncascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU\ndifference on ASR transcribed translation when initialized from matching ST\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05955","description":"<p>A recurring challenge of crowdsourcing NLP datasets at scale is that human\nwriters often rely on repetitive patterns when crafting examples, leading to a\nlack of linguistic diversity. We introduce a novel approach for dataset\ncreation based on worker and AI collaboration, which brings together the\ngenerative strength of language models and the evaluative strength of humans.\nStarting with an existing dataset, MultiNLI for natural language inference\n(NLI), our approach uses dataset cartography to automatically identify examples\nthat demonstrate challenging reasoning patterns, and instructs GPT-3 to compose\nnew examples with similar patterns. Machine generated examples are then\nautomatically filtered, and finally revised and labeled by human crowdworkers.\nThe resulting dataset, WANLI, consists of 107,885 NLI examples and presents\nunique empirical strengths over existing NLI datasets. Remarkably, training a\nmodel on WANLI instead of MultiNLI (which is $4$ times larger) improves\nperformance on seven out-of-domain test sets we consider, including by 11% on\nHANS and 9% on Adversarial NLI. Moreover, combining MultiNLI with WANLI is more\neffective than combining it with other NLI augmentation sets. Our results\ndemonstrate the potential of natural language generation techniques to curate\nNLP datasets of enhanced quality and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Quality in Linear Time. (arXiv:2202.10447v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.10447","description":"<p>We revisit the design choices in Transformers, and propose methods to address\ntheir weaknesses in handling long sequences. First, we propose a simple layer\nnamed gated attention unit, which allows the use of a weaker single-head\nattention with minimal quality loss. We then propose a linear approximation\nmethod complementary to this new layer, which is accelerator-friendly and\nhighly competitive in quality. The resulting model, named FLASH, matches the\nperplexity of improved Transformers over both short (512) and long (8K) context\nlengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and\n12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on\nC4 for masked language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Weizhe Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition. (arXiv:2203.00545v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00545","description":"<p>The MultiCoNER shared task aims at detecting semantically ambiguous and\ncomplex named entities in short and low-context settings for multiple\nlanguages. The lack of contexts makes the recognition of ambiguous named\nentities challenging. To alleviate this issue, our team DAMO-NLP proposes a\nknowledge-based system, where we build a multilingual knowledge base based on\nWikipedia to provide related context information to the named entity\nrecognition (NER) model. Given an input sentence, our system effectively\nretrieves related contexts from the knowledge base. The original input\nsentences are then augmented with such context information, allowing\nsignificantly better contextualized token representations to be captured. Our\nsystem wins 10 out of 13 tracks in the MultiCoNER shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v5 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.10750","description":"<p>In this paper, we develop a new multi-singer Chinese neural singing voice\nsynthesis (SVS) system named WeSinger. To improve the accuracy and naturalness\nof synthesized singing voice, we design several specifical modules and\ntechniques: 1) A deep bi-directional LSTM-based duration model with multi-scale\nrhythm loss and post-processing step; 2) A Transformer-alike acoustic model\nwith progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet\nneural vocoder to produce high-quality singing waveforms; 4) A novel data\naugmentation method with multi-singer pre-training for stronger robustness and\nnaturalness. To our knowledge, WeSinger is the first SVS system to adopt 24 kHz\nLPCNet and multi-singer pre-training simultaneously. Both quantitative and\nqualitative evaluation results demonstrate the effectiveness of WeSinger in\nterms of accuracy and naturalness, and WeSinger achieves state-of-the-art\nperformance on the recent public Chinese singing corpus\nOpencpop\\footnote{https://wenet.org.cn/opencpop/}. Some synthesized singing\nsamples are available online\\footnote{https://zzw922cn.github.io/wesinger/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zewang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning. (arXiv:2203.13628v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13628","description":"<p>Inspired by the recent progress in self-supervised learning for computer\nvision, in this paper we introduce DeLoRes, a new general-purpose audio\nrepresentation learning approach. Our main objective is to make our network\nlearn representations in a resource-constrained setting (both data and\ncompute), that can generalize well across a diverse set of downstream tasks.\nInspired from the Barlow Twins objective function, we propose to learn\nembeddings that are invariant to distortions of an input audio sample, while\nmaking sure that they contain non-redundant information about the sample. To\nachieve this, we measure the cross-correlation matrix between the outputs of\ntwo identical networks fed with distorted versions of an audio segment sampled\nfrom an audio file and make it as close to the identity matrix as possible. We\nuse a combination of a small subset of the large-scale AudioSet dataset and\nFSD50K for self-supervised learning and are able to learn with less than half\nthe parameters compared to state-of-the-art algorithms. For evaluation, we\ntransfer these learned representations to 9 downstream classification tasks,\nincluding speech, music, and animal sounds, and show competitive results under\ndifferent evaluation setups. In addition to being simple and intuitive, our\npre-training algorithm is amenable to compute through its inherent nature of\nconstruction and does not require careful implementation details to avoid\ntrivial or degenerate solutions. Furthermore, we conduct ablation studies on\nour results and make all our code and pre-trained models publicly available\nhttps://github.com/Speech-Lab-IITM/DeLoRes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_a/0/1/0/all/0/1\">and Deepak Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.15081","description":"<p>We present a method for visually-grounded spoken term discovery. After\ntraining either a HuBERT or wav2vec2.0 model to associate spoken captions with\nnatural images, we show that powerful word segmentation and clustering\ncapability emerges within the model's self-attention heads. Our experiments\nreveal that this ability is not present to nearly the same extent in the base\nHuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a\ncrucial component of the word discovery capability we observe. We also evaluate\nour method on the Buckeye word segmentation and ZeroSpeech spoken term\ndiscovery tasks, where we outperform all currently published methods on several\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Contrast Stretching on Target Feature for Speech Enhancement. (arXiv:2203.17152v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.17152","description":"<p>Speech enhancement (SE) performance has improved considerably owing to the\nuse of deep learning models as a base function. Herein, we propose a perceptual\ncontrast stretching (PCS) approach to further improve SE performance. The PCS\nis derived based on the critical band importance function and is applied to\nmodify the targets of the SE model. Specifically, the contrast of target\nfeatures is stretched based on perceptual importance, thereby improving the\noverall SE performance. Compared with post-processing-based implementations,\nincorporating PCS into the training phase preserves performance and reduces\nonline computation. Notably, PCS can be combined with different SE model\narchitectures and training criteria. Furthermore, PCS does not affect the\ncausality or convergence of SE model training. Experimental results on the\nVoiceBank-DEMAND dataset show that the proposed method can achieve\nstate-of-the-art performance on both causal (PESQ score = 3.07) and noncausal\n(PESQ score = 3.35) SE tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chao_R/0/1/0/all/0/1\">Rong Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Szu-Wei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xugang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00763","description":"<p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline\nsetting or through human evaluation. The evaluation is often limited to\nsingle-turn or very time-intensive. As an alternative, user simulators that\nmimic user behavior allow us to consider a broad set of user goals to generate\nhuman-like conversations for simulated evaluation. Employing existing user\nsimulators to evaluate TDSs is challenging as user simulators are primarily\ndesigned to optimize dialogue policies for TDSs and have limited evaluation\ncapabilities. Moreover, the evaluation of user simulators is an open challenge.\nIn this work, we proposes a metaphorical user simulator for end-to-end TDS\nevaluation, where we define a simulator to be metaphorical if it simulates\nuser's analogical thinking in interactions with systems. We also propose a\ntester-based evaluation framework to generate variants, i.e., dialogue systems\nwith different capabilities. Our user simulator constructs a metaphorical user\nmodel that assists the simulator in reasoning by referring to prior knowledge\nwhen encountering new items. We estimate the quality of simulators by checking\nthe simulated interactions between simulators and variants. Our experiments are\nconducted using three TDS datasets. The metaphorical user simulator\ndemonstrates better consistency with manual evaluation than an agenda-based\nsimulator and a seq2seq model on three datasets; our tester framework\ndemonstrates efficiency, and better generalization and scalability because it\ncan be adapted for dialogues in multiple domains and for multiple tasks, such\nas conversational recommendation and e-commerce dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment. (arXiv:2204.04016v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2204.04016","description":"<p>Speech intelligibility assessment plays an important role in the therapy of\npatients suffering from pathological speech disorders. Automatic and objective\nmeasures are desirable to assist therapists in their traditionally subjective\nand labor-intensive assessments. In this work, we investigate a novel approach\nfor obtaining such a measure using the divergence in disentangled latent speech\nrepresentations of a parallel utterance pair, obtained from a healthy reference\nand a pathological speaker. Experiments on an English database of Cerebral\nPalsy patients, using all available utterances per speaker, show high and\nsignificant correlation values (R = -0.9) with subjective intelligibility\nmeasures, while having only minimal deviation (+-0.01) across four different\nreference speaker pairs. We also demonstrate the robustness of the proposed\nmethod (R = -0.89 deviating +-0.02 over 1000 iterations) by considering a\nsignificantly smaller amount of utterances per speaker. Our results are among\nthe first to show that disentangled speech representations can be used for\nautomatic pathological speech intelligibility assessment, resulting in a\nreference speaker pair invariant method, applicable in scenarios with only few\nutterances available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Weise_T/0/1/0/all/0/1\">Tobias Weise</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klumpp_P/0/1/0/all/0/1\">Philipp Klumpp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demir_K/0/1/0/all/0/1\">Kubilay Can Demir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noeth_E/0/1/0/all/0/1\">Elmar Noeth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heismann_B/0/1/0/all/0/1\">Bjoern Heismann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schuster_M/0/1/0/all/0/1\">Maria Schuster</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Language Model Size in Cross-Device Federated Learning. (arXiv:2204.09715v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09715","description":"<p>Most studies in cross-device federated learning focus on small models, due to\nthe server-client communication and on-device computation bottlenecks. In this\nwork, we leverage various techniques for mitigating these bottlenecks to train\nlarger language models in cross-device federated learning. With systematic\napplications of partial model training, quantization, efficient transfer\nlearning, and communication-efficient optimizers, we are able to train a $21$M\nparameter Transformer and $20.2$M parameter Conformer that achieve the same or\nbetter perplexity as that of a similarly sized LSTM with $\\sim10\\times$ smaller\nclient-to-server communication cost and $11\\%$ lower perplexity than smaller\nLSTMs commonly studied in literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1\">Jae Hun Ro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breiner_T/0/1/0/all/0/1\">Theresa Breiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McConnaughey_L/0/1/0/all/0/1\">Lara McConnaughey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Ananda Theertha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Training of Neural Transducer for Speech Recognition. (arXiv:2204.10586v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10586","description":"<p>As one of the most popular sequence-to-sequence modeling approaches for\nspeech recognition, the RNN-Transducer has achieved evolving performance with\nmore and more sophisticated neural network models of growing size and\nincreasing training epochs. While strong computation resources seem to be the\nprerequisite of training superior models, we try to overcome it by carefully\ndesigning a more efficient training pipeline. In this work, we propose an\nefficient 3-stage progressive training pipeline to build highly-performing\nneural transducer models from scratch with very limited computation resources\nin a reasonable short time period. The effectiveness of each stage is\nexperimentally verified on both Librispeech and Switchboard corpora. The\nproposed pipeline is able to train transducer models approaching\nstate-of-the-art performance with a single GPU in just 2-3 weeks. Our best\nconformer transducer achieves 4.1% WER on Librispeech test-other with only 35\nepochs of training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1\">Wilfried Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?. (arXiv:2204.12765v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12765","description":"<p>Recently, self-supervised learning (SSL) has demonstrated strong performance\nin speaker recognition, even if the pre-training objective is designed for\nspeech recognition. In this paper, we study which factor leads to the success\nof self-supervised learning on speaker-related tasks, e.g. speaker verification\n(SV), through a series of carefully designed experiments. Our empirical results\non the Voxceleb-1 dataset suggest that the benefit of SSL to SV task is from a\ncombination of mask speech prediction loss, data scale, and model size, while\nthe SSL quantizer has a minor impact. We further employ the integrated\ngradients attribution method and loss landscape visualization to understand the\neffectiveness of self-supervised learning for speaker recognition performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiangzhan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Parallelize in a Shared-Memory Environment with Transformers. (arXiv:2204.12835v3 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2204.12835","description":"<p>In past years, the world has switched to many-core and multi-core shared\nmemory architectures. As a result, there is a growing need to utilize these\narchitectures by introducing shared memory parallelization schemes to software\napplications. OpenMP is the most comprehensive API that implements such\nschemes, characterized by a readable interface. Nevertheless, introducing\nOpenMP into code is challenging due to pervasive pitfalls in management of\nparallel shared memory. To facilitate the performance of this task, many\nsource-to-source (S2S) compilers have been created over the years, tasked with\ninserting OpenMP directives into code automatically. In addition to having\nlimited robustness to their input format, these compilers still do not achieve\nsatisfactory coverage and precision in locating parallelizable code and\ngenerating appropriate directives. In this work, we propose leveraging recent\nadvances in ML techniques, specifically in natural language processing (NLP),\nto replace S2S compilers altogether. We create a database (corpus), Open-OMP,\nspecifically for this goal. Open-OMP contains over 28,000 code snippets, half\nof which contain OpenMP directives while the other half do not need\nparallelization at all with high probability. We use the corpus to train\nsystems to automatically classify code segments in need of parallelization, as\nwell as suggest individual OpenMP clauses. We train several transformer models,\nnamed PragFormer, for these tasks, and show that they outperform\nstatistically-trained baselines and automatic S2S parallelization compilers in\nboth classifying the overall need for an OpenMP directive and the introduction\nof private and reduction clauses.\n</p>\n<p>Our source code and database are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/PragFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harel_R/0/1/0/all/0/1\">Re&#x27;em Harel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1\">Gal Oren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding. (arXiv:2205.00693v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00693","description":"<p>Spoken language understanding (SLU) is an essential task for machines to\nunderstand human speech for better interactions. However, errors from the\nautomatic speech recognizer (ASR) usually hurt the understanding performance.\nIn reality, ASR systems may not be easy to adjust for the target scenarios.\nTherefore, this paper focuses on learning utterance representations that are\nrobust to ASR errors using a contrastive objective, and further strengthens the\ngeneralization ability by combining supervised contrastive learning and\nself-distillation in model fine-tuning. Experiments on three benchmark datasets\ndemonstrate the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Ya-Hsin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Word Embeddings in Hyperbolic Space. (arXiv:2205.01907v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01907","description":"<p>Cross-lingual word embeddings can be applied to several natural language\nprocessing applications across multiple languages. Unlike prior works that use\nword embeddings based on the Euclidean space, this short paper presents a\nsimple and effective cross-lingual Word2Vec model that adapts to the Poincar\\'e\nball model of hyperbolic space to learn unsupervised cross-lingual word\nrepresentations from a German-English parallel corpus. It has been shown that\nhyperbolic embeddings can capture and preserve hierarchical relationships. We\nevaluate the model on both hypernymy and analogy tasks. The proposed model\nachieves comparable performance with the vanilla Word2Vec model on the\ncross-lingual analogy task, the hypernymy task shows that the cross-lingual\nPoincar\\'e Word2Vec model can capture latent hierarchical structure from free\ntext across languages, which are absent from the Euclidean-based Word2Vec\nrepresentations. Our results show that by preserving the latent hierarchical\ninformation, hyperbolic spaces can offer better representations for\ncross-lingual embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Mudit Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifiers are Better Experts for Controllable Text Generation. (arXiv:2205.07276v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.07276","description":"<p>This paper proposes a simple method for controllable text generation based on\nweighting logits with a free-form classifier, namely CAIF sampling. Using an\narbitrary text classifier, we adjust a small part of a language model's logits\nand guide text generation towards or away from classifier prediction. We\nexperimented with toxicity avoidance and sentiment control tasks and showed\nthat the proposed method significantly outperforms recent PPLM, GeDi, and\nDExperts on PPL and task accuracy metrics based on the external classifier of\ngenerated texts. In addition, compared to other approaches, it is easier to\nimplement and tune and has significantly fewer restrictions and requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitdikov_A/0/1/0/all/0/1\">Askhat Sitdikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_A/0/1/0/all/0/1\">Alexander Markov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Connectivity Reveals Generalization Strategies. (arXiv:2205.12411v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.12411","description":"<p>It is widely accepted in the mode connectivity literature that when two\nneural networks are trained similarly on the same data, they are connected by a\npath through parameter space over which test set accuracy is maintained. Under\nsome circumstances, including transfer learning from pretrained models, these\npaths are presumed to be linear. In contrast to existing results, we find that\namong text classifiers (trained on MNLI, QQP, and CoLA), some pairs of\nfinetuned models have large barriers of increasing loss on the linear paths\nbetween them. On each task, we find distinct clusters of models which are\nlinearly connected on the test loss surface, but are disconnected from models\noutside the cluster -- models that occupy separate basins on the surface. By\nmeasuring performance on specially-crafted diagnostic datasets, we find that\nthese clusters correspond to different generalization strategies: one cluster\nbehaves like a bag of words model under domain shift, while another cluster\nuses syntactic heuristics. Our work demonstrates how the geometry of the loss\nsurface can guide models towards different heuristic functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1\">Jeevesh Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rachit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rites de Passage: Elucidating Displacement to Emplacement of Refugees on Twitter. (arXiv:2206.03248v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2206.03248","description":"<p>Social media deliberations allow to explore refugee-related is-sues. AI-based\nstudies have investigated refugee issues mostly around a specific event and\nconsidered unimodal approaches. Contrarily, we have employed a multimodal\narchitecture for probing the refugee journeys from their home to host nations.\nWe draw insights from Arnold van Gennep's anthropological work 'Les Rites de\nPassage', which systematically analyzed an individual's transition from one\ngroup or society to another. Based on Gennep's\nseparation-transition-incorporation framework, we have identified four phases\nof refugee journeys: Arrival of Refugees, Temporal stay at Asylums,\nRehabilitation, and Integration of Refugees into the host nation. We collected\n0.23 million multimodal tweets from April 2020 to March 2021 for testing this\nproposed frame-work. We find that a combination of transformer-based language\nmodels and state-of-the-art image recognition models, such as fusion of\nBERT+LSTM and InceptionV4, can out-perform unimodal models. Subsequently, to\ntest the practical implication of our proposed model in real-time, we have\nconsidered 0.01 million multimodal tweets related to the 2022 Ukrainian refugee\ncrisis. An F1-score of 71.88 % for this 2022 crisis confirms the\ngeneralizability of our proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khatua_A/0/1/0/all/0/1\">Aparup Khatua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejdl_W/0/1/0/all/0/1\">Wolfgang Nejdl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Zero Oracle Word Error Rate on the Switchboard Benchmark. (arXiv:2206.06192v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.06192","description":"<p>The \"Switchboard benchmark\" is a very well-known test set in automatic speech\nrecognition (ASR) research, establishing record-setting performance for systems\nthat claim human-level transcription accuracy. This work highlights\nlesser-known practical considerations of this evaluation, demonstrating major\nimprovements in word error rate (WER) by correcting the reference\ntranscriptions and deviating from the official scoring methodology. In this\nmore detailed and reproducible scheme, even commercial ASR systems can score\nbelow 5% WER and the established record for a research system is lowered to\n2.3%. An alternative metric of transcript precision is proposed, which does not\npenalize deletions and appears to be more discriminating for human vs. machine\nperformance. While commercial ASR systems are still below this threshold, a\nresearch system is shown to clearly surpass the accuracy of commercial human\nspeech recognition. This work also explores using standardized scoring tools to\ncompute oracle WER by selecting the best among a list of alternatives. A phrase\nalternatives representation is compared to utterance-level N-best lists and\nword-level data structures; using dense lattices and adding out-of-vocabulary\nwords, this achieves an oracle WER of 0.18%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Faria_A/0/1/0/all/0/1\">Arlo Faria</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Janin_A/0/1/0/all/0/1\">Adam Janin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adkoli_S/0/1/0/all/0/1\">Sidhi Adkoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Narratives through Dimensions of Analogy. (arXiv:2206.07167v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2206.07167","description":"<p>Analogical reasoning is a powerful qualitative reasoning tool that enables\nhumans to connect two situations, and to generalize their knowledge from\nfamiliar to novel situations. Cognitive Science research provides valuable\ninsights into the richness and complexity of analogical reasoning, together\nwith implementations of expressive analogical reasoners with limited\nscalability. Modern scalable AI techniques with the potential to reason by\nanalogy have been only applied to the special case of proportional analogy, and\nnot to understanding higher-order analogies. In this paper, we aim to bridge\nthe gap by: 1) formalizing six dimensions of analogy based on mature insights\nfrom Cognitive Science research, 2) annotating a corpus of fables with each of\nthese dimensions, and 3) defining four tasks with increasing complexity that\nenable scalable evaluation of AI techniques. Experiments with language models\nand neuro-symbolic AI reasoners on these tasks reveal that state-of-the-art\nmethods can be applied to reason by analogy with a limited success, motivating\nthe need for further research towards comprehensive and scalable analogical\nreasoning by AI. We make all our code and data available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagarajah_T/0/1/0/all/0/1\">Thiloshon Nagarajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What can Speech and Language Tell us About the Working Alliance in Psychotherapy. (arXiv:2206.08835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.08835","description":"<p>We are interested in the problem of conversational analysis and its\napplication to the health domain. Cognitive Behavioral Therapy is a structured\napproach in psychotherapy, allowing the therapist to help the patient to\nidentify and modify the malicious thoughts, behavior, or actions. This\ncooperative effort can be evaluated using the Working Alliance Inventory\nObserver-rated Shortened - a 12 items inventory covering task, goal, and\nrelationship - which has a relevant influence on therapeutic outcomes. In this\nwork, we investigate the relation between this alliance inventory and the\nspoken conversations (sessions) between the patient and the psychotherapist. We\nhave delivered eight weeks of e-therapy, collected their audio and video call\nsessions, and manually transcribed them. The spoken conversations have been\nannotated and evaluated with WAI ratings by professional therapists. We have\ninvestigated speech and language features and their association with WAI items.\nThe feature types include turn dynamics, lexical entrainment, and\nconversational descriptors extracted from the speech and language signals. Our\nfindings provide strong evidence that a subset of these features are strong\nindicators of working alliance. To the best of our knowledge, this is the first\nand a novel study to exploit speech and language for characterising working\nalliance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roccabruna_G/0/1/0/all/0/1\">Gabriel Roccabruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciulli_T/0/1/0/all/0/1\">Tommaso Ciulli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danieli_M/0/1/0/all/0/1\">Morena Danieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riccardi_G/0/1/0/all/0/1\">Giuseppe Riccardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting a French Dictionary from the Beginning of the 20th Century to Wikidata. (arXiv:2206.11022v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.11022","description":"<p>The \\textit{Petit Larousse illustr\\'e} is a French dictionary first published\nin 1905. Its division in two main parts on language and on history and\ngeography corresponds to a major milestone in French lexicography as well as a\nrepository of general knowledge from this period. Although the value of many\nentries from 1905 remains intact, some descriptions now have a dimension that\nis more historical than contemporary. They are nonetheless significant to\nanalyze and understand cultural representations from this time. A comparison\nwith more recent information or a verification of these entries would require a\ntedious manual work. In this paper, we describe a new lexical resource, where\nwe connected all the dictionary entries of the history and geography part to\ncurrent data sources. For this, we linked each of these entries to a wikidata\nidentifier. Using the wikidata links, we can automate more easily the\nidentification, comparison, and verification of historically-situated\nrepresentations. We give a few examples on how to process wikidata identifiers\nand we carried out a small analysis of the entities described in the dictionary\nto outline possible applications. The resource, i.e. the annotation of 20,245\ndictionary entries with wikidata links, is available from GitHub\nurl{https://github.com/pnugues/petit_larousse_1905/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nugues_P/0/1/0/all/0/1\">Pierre Nugues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Programming Exercises and Code Explanations using Large Language Models. (arXiv:2206.11861v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2206.11861","description":"<p>This article explores the natural language generation capabilities of large\nlanguage models with application to the production of two types of learning\nresources common in programming courses. Using OpenAI Codex as the large\nlanguage model, we create programming exercises (including sample solutions and\ntest cases) and code explanations, assessing these qualitatively and\nquantitatively. Our results suggest that the majority of the automatically\ngenerated content is both novel and sensible, and in some cases ready to use as\nis. When creating exercises we find that it is remarkably easy to influence\nboth the programming concepts and the contextual themes they contain, simply by\nsupplying keywords as input to the model. Our analysis suggests that there is\nsignificant value in massive generative machine learning models as a tool for\ninstructors, although there remains a need for some oversight to ensure the\nquality of the generated content before it is delivered to students. We further\ndiscuss the implications of OpenAI Codex and similar tools for introductory\nprogramming education and highlight future research streams that have the\npotential to improve the quality of the educational experience for both\nteachers and students alike.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarsa_S/0/1/0/all/0/1\">Sami Sarsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1\">Paul Denny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellas_A/0/1/0/all/0/1\">Arto Hellas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1\">Juho Leinonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Independent evaluation of state-of-the-art deep networks for mammography. (arXiv:2206.12407v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12407","description":"<p>Deep neural models have shown remarkable performance in image recognition\ntasks, whenever large datasets of labeled images are available. The largest\ndatasets in radiology are available for screening mammography. Recent reports,\nincluding in high impact journals, document performance of deep models at or\nabove that of trained radiologists. What is not yet known is whether\nperformance of these trained models is robust and replicates across datasets.\nHere we evaluate performance of five published state-of-the-art models on four\npublicly available mammography datasets. The limited size of public datasets\nprecludes retraining the model and so we are limited to evaluate those models\nthat have been made available with pre-trained parameters. Where test data was\navailable, we replicated published results. However, the trained models\nperformed poorly on out-of-sample data, except when based on all four standard\nviews of a mammographic exam. We conclude that future progress will depend on a\nconcerted effort to make more diverse and larger mammography datasets publicly\navailable. Meanwhile, results that are not accompanied by a release of trained\nmodels for independent validation should be judged cautiously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Velarde_O/0/1/0/all/0/1\">Osvaldo Matias Velarde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parrra_L/0/1/0/all/0/1\">Lucas Parrra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep embedded clustering algorithm for clustering PACS repositories. (arXiv:2206.12417v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12417","description":"<p>Creating large datasets of medical radiology images from several sources can\nbe challenging because of the differences in the acquisition and storage\nstandards. One possible way of controlling and/or assessing the image selection\nprocess is through medical image clustering. This, however, requires an\nefficient method for learning latent image representations. In this paper, we\ntackle the problem of fully-unsupervised clustering of medical images using\npixel data only. We test the performance of several contemporary approaches,\nbuilt on top of a convolutional autoencoder (CAE) - convolutional deep embedded\nclustering (CDEC) and convolutional improved deep embedded clustering (CIDEC) -\nand three approaches based on preset feature extraction - histogram of oriented\ngradients (HOG), local binary pattern (LBP) and principal component analysis\n(PCA). CDEC and CIDEC are end-to-end clustering solutions, involving\nsimultaneous learning of latent representations and clustering assignments,\nwhereas the remaining approaches rely on k-means clustering from fixed\nembeddings. We train the models on 30,000 images, and test them using a\nseparate test set consisting of 8,000 images. We sampled the data from the PACS\nrepository archive of the Clinical Hospital Centre Rijeka. For evaluation, we\nuse silhouette score, homogeneity score and normalised mutual information (NMI)\non two target parameters, closely associated with commonly occurring DICOM tags\n- Modality and anatomical region (adjusted BodyPartExamined tag). CIDEC attains\nan NMI score of 0.473 with respect to anatomical region, and CDEC attains an\nNMI score of 0.645 with respect to the tag Modality - both outperforming other\ncommonly used feature descriptors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Manojlovic_T/0/1/0/all/0/1\">Teo Manojlovi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanic_M/0/1/0/all/0/1\">Matija Milani&#x10d;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stajduhar_I/0/1/0/all/0/1\">Ivan &#x160;tajduhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ev-NeRF: Event Based Neural Radiance Field. (arXiv:2206.12455v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12455","description":"<p>We present Ev-NeRF, a Neural Radiance Field derived from event data. While\nevent cameras can measure subtle brightness changes in high frame rates, the\nmeasurements in low lighting or extreme motion suffer from significant domain\ndiscrepancy with complex noise. As a result, the performance of event-based\nvision tasks does not transfer to challenging environments, where the event\ncameras are expected to thrive over normal cameras. We find that the multi-view\nconsistency of NeRF provides a powerful self-supervision signal for eliminating\nthe spurious measurements and extracting the consistent underlying structure\ndespite highly noisy input. Instead of posed images of the original NeRF, the\ninput to Ev-NeRF is the event measurements accompanied by the movements of the\nsensors. Using the loss function that reflects the measurement model of the\nsensor, Ev-NeRF creates an integrated neural volume that summarizes the\nunstructured and sparse data points captured for about 2-4 seconds. The\ngenerated neural volume can also produce intensity images from novel views with\nreasonable depth estimates, which can serve as a high-quality input to various\nvision-based tasks. Our results show that Ev-NeRF achieves competitive\nperformance for intensity image reconstruction under extreme noise conditions\nand high-dynamic-range imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1\">Inwoo Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag of Tricks for Long-Tail Visual Recognition of Animal Species in Camera Trap Images. (arXiv:2206.12458v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12458","description":"<p>Camera traps are a strategy for monitoring wildlife that collects a large\nnumber of pictures. The number of images collected from each species usually\nfollows a long-tail distribution, i.e., a few classes have a large number of\ninstances while a lot of species have just a small percentage. Although in most\ncases these rare species are the classes of interest to ecologists, they are\noften neglected when using deep learning models because these models require a\nlarge number of images for the training. In this work, we systematically\nevaluate recently proposed techniques - namely, square-root re-sampling,\nclass-balanced focal loss, and balanced group softmax - to address the\nlong-tail visual recognition of animal species in camera trap images. To\nachieve a more general conclusion, we evaluated the selected methods on four\nfamilies of computer vision models (ResNet, MobileNetV3, EfficientNetV2, and\nSwin Transformer) and four camera trap datasets with different characteristics.\nInitially, we prepared a robust baseline with the most recent training tricks\nand then we applied the methods for improving long-tail recognition. Our\nexperiments show that the Swin transformer can reach high performance for rare\nclasses without applying any additional method for handling imbalance, with an\noverall accuracy of 88.76% for WCS dataset and 94.97% for Snapshot Serengeti,\nconsidering a location-based train/test split. In general, the square-root\nsampling was the method that most improved the performance for minority classes\nby around 10%, but at the cost of reducing the majority classes accuracy at\nleast 4%. These results motivated us to propose a simple and effective approach\nusing an ensemble combining square-root sampling and the baseline. The proposed\napproach achieved the best trade-off between the performance of the tail class\nand the cost of the head classes' accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cunha_F/0/1/0/all/0/1\">Fagner Cunha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_E/0/1/0/all/0/1\">Eulanda M. dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colonna_J/0/1/0/all/0/1\">Juan G. Colonna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion Estimation for Large Displacements and Deformations. (arXiv:2206.12464v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12464","description":"<p>Large displacement optical flow is an integral part of many computer vision\ntasks. Variational optical flow techniques based on a coarse-to-fine scheme\ninterpolate sparse matches and locally optimize an energy model conditioned on\ncolour, gradient and smoothness, making them sensitive to noise in the sparse\nmatches, deformations, and arbitrarily large displacements. This paper\naddresses this problem and presents HybridFlow, a variational motion estimation\nframework for large displacements and deformations. A multi-scale hybrid\nmatching approach is performed on the image pairs. Coarse-scale clusters formed\nby classifying pixels according to their feature descriptors are matched using\nthe clusters' context descriptors. We apply a multi-scale graph matching on the\nfiner-scale superpixels contained within each matched pair of coarse-scale\nclusters. Small clusters that cannot be further subdivided are matched using\nlocalized feature matching. Together, these initial matches form the flow,\nwhich is propagated by an edge-preserving interpolation and variational\nrefinement. Our approach does not require training and is robust to substantial\ndisplacements and rigid and non-rigid transformations due to motion in the\nscene, making it ideal for large-scale imagery such as Wide-Area Motion Imagery\n(WAMI). More notably, HybridFlow works on directed graphs of arbitrary topology\nrepresenting perceptual groups, which improves motion estimation in the\npresence of significant deformations. We demonstrate HybridFlow's superior\nperformance to state-of-the-art variational techniques on two benchmark\ndatasets and report comparable results with state-of-the-art\ndeep-learning-based techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poullis_C/0/1/0/all/0/1\">Charalambos Poullis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Guided Autoencoder for Automated Progression Prediction of Subjective Cognitive Decline with Structural MRI. (arXiv:2206.12480v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12480","description":"<p>Subjective cognitive decline (SCD) is a preclinical stage of Alzheimer's\ndisease (AD) which occurs even before mild cognitive impairment (MCI).\nProgressive SCD will convert to MCI with the potential of further evolving to\nAD. Therefore, early identification of progressive SCD with neuroimaging\ntechniques (e.g., structural MRI) is of great clinical value for early\nintervention of AD. However, existing MRI-based machine/deep learning methods\nusually suffer the small-sample-size problem which poses a great challenge to\nrelated neuroimaging analysis. The central question we aim to tackle in this\npaper is how to leverage related domains (e.g., AD/NC) to assist the\nprogression prediction of SCD. Meanwhile, we are concerned about which brain\nareas are more closely linked to the identification of progressive SCD. To this\nend, we propose an attention-guided autoencoder model for efficient\ncross-domain adaptation which facilitates the knowledge transfer from AD to\nSCD. The proposed model is composed of four key components: 1) a feature\nencoding module for learning shared subspace representations of different\ndomains, 2) an attention module for automatically locating discriminative brain\nregions of interest defined in brain atlases, 3) a decoding module for\nreconstructing the original input, 4) a classification module for\nidentification of brain diseases. Through joint training of these four modules,\ndomain invariant features can be learned. Meanwhile, the brain disease related\nregions can be highlighted by the attention mechanism. Extensive experiments on\nthe publicly available ADNI dataset and a private CLAS dataset have\ndemonstrated the effectiveness of the proposed method. The proposed model is\nstraightforward to train and test with only 5-10 seconds on CPUs and is\nsuitable for medical tasks with small datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Hao Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_L/0/1/0/all/0/1\">Ling Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozoki_A/0/1/0/all/0/1\">Andrea Bozoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingxia Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Approach For Analysis of Distributed Acoustic Sensing System Based on Deep Transfer Learning. (arXiv:2206.12484v1 [cs.LG])","link":"http://arxiv.org/abs/2206.12484","description":"<p>Distributed acoustic sensors (DAS) are effective apparatus which are widely\nused in many application areas for recording signals of various events with\nvery high spatial resolution along the optical fiber. To detect and recognize\nthe recorded events properly, advanced signal processing algorithms with high\ncomputational demands are crucial. Convolutional neural networks are highly\ncapable tools for extracting spatial information and very suitable for event\nrecognition applications in DAS. Long-short term memory (LSTM) is an effective\ninstrument for processing sequential data. In this study, we proposed a\nmulti-input multi-output, two stage feature extraction methodology that\ncombines the capabilities of these neural network architectures with transfer\nlearning to classify vibrations applied to an optical fiber by a piezo\ntransducer. First, we extracted the differential amplitude and phase\ninformation from the Phase-OTDR recordings and stored them in a\ntemporal-spatial data matrix. Then, we used a state-of-the-art pre-trained CNN\nwithout dense layers as a feature extractor in the first stage. In the second\nstage, we used LSTMs to further analyze the features extracted by the CNN.\nFinally, we used a dense layer to classify the extracted features. To observe\nthe effect of the utilized CNN architecture, we tested our model with five\nstate-of-the art pre-trained models (VGG-16, ResNet-50, DenseNet-121, MobileNet\nand Inception-v3). The results show that using the VGG-16 architecture in our\nframework manages to obtain 100% classification accuracy in 50 trainings and\ngot the best results on our Phase-OTDR dataset. Outcomes of this study indicate\nthat the pre-trained CNNs combined with LSTM are very suitable for the analysis\nof differential amplitude and phase information, represented in a temporal\nspatial data matrix which is promising for event recognition operations in DAS\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kayan_C/0/1/0/all/0/1\">Ceyhun Efe Kayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldogan_K/0/1/0/all/0/1\">Kivilcim Yuksel Aldogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumus_A/0/1/0/all/0/1\">Abdurrahman Gumus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal and Robust Category-level Perception: Object Pose and Shape Estimation from 2D and 3D Semantic Keypoints. (arXiv:2206.12498v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12498","description":"<p>We consider a category-level perception problem, where one is given 2D or 3D\nsensor data picturing an object of a given category (e.g., a car), and has to\nreconstruct the 3D pose and shape of the object despite intra-class variability\n(i.e., different car models have different shapes). We consider an active shape\nmodel, where -- for an object category -- we are given a library of potential\nCAD models describing objects in that category, and we adopt a standard\nformulation where pose and shape are estimated from 2D or 3D keypoints via\nnon-convex optimization. Our first contribution is to develop PACE3D* and\nPACE2D*, the first certifiably optimal solvers for pose and shape estimation\nusing 3D and 2D keypoints, respectively. Both solvers rely on the design of\ntight (i.e., exact) semidefinite relaxations. Our second contribution is to\ndevelop outlier-robust versions of both solvers, named PACE3D# and PACE2D#.\nTowards this goal, we propose ROBIN, a general graph-theoretic framework to\nprune outliers, which uses compatibility hypergraphs to model measurements'\ncompatibility. We show that in category-level perception problems these\nhypergraphs can be built from winding orders of the keypoints (in 2D) or their\nconvex hulls (in 3D), and many outliers can be pruned via maximum hyperclique\ncomputation. The last contribution is an extensive experimental evaluation.\nBesides providing an ablation study on simulated datasets and on the PASCAL\ndataset, we combine our solver with a deep keypoint detector, and show that\nPACE3D# improves over the state of the art in vehicle pose estimation in the\nApolloScape datasets, and its runtime is compatible with practical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jingnan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stain based contrastive co-training for histopathological image analysis. (arXiv:2206.12505v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12505","description":"<p>We propose a novel semi-supervised learning approach for classification of\nhistopathology images. We employ strong supervision with patch-level\nannotations combined with a novel co-training loss to create a semi-supervised\nlearning framework. Co-training relies on multiple conditionally independent\nand sufficient views of the data. We separate the hematoxylin and eosin\nchannels in pathology images using color deconvolution to create two views of\neach slide that can partially fulfill these requirements. Two separate CNNs are\nused to embed the two views into a joint feature space. We use a contrastive\nloss between the views in this feature space to implement co-training. We\nevaluate our approach in clear cell renal cell and prostate carcinomas, and\ndemonstrate improvement over state-of-the-art semi-supervised learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bodong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knudsen_B/0/1/0/all/0/1\">Beatrice Knudsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirohi_D/0/1/0/all/0/1\">Deepika Sirohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrero_A/0/1/0/all/0/1\">Alessandro Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FetReg2021: A Challenge on Placental Vessel Segmentation and Registration in Fetoscopy. (arXiv:2206.12512v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12512","description":"<p>Fetoscopy laser photocoagulation is a widely adopted procedure for treating\nTwin-to-Twin Transfusion Syndrome (TTTS). The procedure involves\nphotocoagulation pathological anastomoses to regulate blood exchange among\ntwins. The procedure is particularly challenging due to the limited field of\nview, poor manoeuvrability of the fetoscope, poor visibility, and variability\nin illumination. These challenges may lead to increased surgery time and\nincomplete ablation. Computer-assisted intervention (CAI) can provide surgeons\nwith decision support and context awareness by identifying key structures in\nthe scene and expanding the fetoscopic field of view through video mosaicking.\nResearch in this domain has been hampered by the lack of high-quality data to\ndesign, develop and test CAI algorithms. Through the Fetoscopic Placental\nVessel Segmentation and Registration (FetReg2021) challenge, which was\norganized as part of the MICCAI2021 Endoscopic Vision challenge, we released\nthe first largescale multicentre TTTS dataset for the development of\ngeneralized and robust semantic segmentation and video mosaicking algorithms.\nFor this challenge, we released a dataset of 2060 images, pixel-annotated for\nvessels, tool, fetus and background classes, from 18 in-vivo TTTS fetoscopy\nprocedures and 18 short video clips. Seven teams participated in this challenge\nand their model performance was assessed on an unseen test dataset of 658\npixel-annotated images from 6 fetoscopic procedures and 6 short clips. The\nchallenge provided an opportunity for creating generalized solutions for\nfetoscopic scene understanding and mosaicking. In this paper, we present the\nfindings of the FetReg2021 challenge alongside reporting a detailed literature\nreview for CAI in TTTS fetoscopy. Through this challenge, its analysis and the\nrelease of multi-centre fetoscopic data, we provide a benchmark for future\nresearch in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Casella_A/0/1/0/all/0/1\">Alessandro Casella</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qayyum_A/0/1/0/all/0/1\">Abdul Qayyum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benzinou_A/0/1/0/all/0/1\">Abdesslam Benzinou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazher_M/0/1/0/all/0/1\">Moona Mazher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meriaudeau_F/0/1/0/all/0/1\">Fabrice Meriaudeau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lena_C/0/1/0/all/0/1\">Chiara Lena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cintorrino_I/0/1/0/all/0/1\">Ilaria Anita Cintorrino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paolis_G/0/1/0/all/0/1\">Gaia Romana De Paolis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biagioli_J/0/1/0/all/0/1\">Jessica Biagioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grechishnikova_D/0/1/0/all/0/1\">Daria Grechishnikova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiao_J/0/1/0/all/0/1\">Jing Jiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_B/0/1/0/all/0/1\">Bizhe Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yanyan Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaire_R/0/1/0/all/0/1\">Rebati Raman Gaire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subedi_R/0/1/0/all/0/1\">Ronast Subedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vazquez_E/0/1/0/all/0/1\">Eduard Vazquez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lisowska_A/0/1/0/all/0/1\">Aneta Lisowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Attilakos_G/0/1/0/all/0/1\">George Attilakos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wimalasundera_R/0/1/0/all/0/1\">Ruwan Wimalasundera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paladini_D/0/1/0/all/0/1\">Dario Paladini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Momi_E/0/1/0/all/0/1\">Elena De Momi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mattos_L/0/1/0/all/0/1\">Leonardo S Mattos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Shallow to Deep: Compositional Reasoning over Graphs for Visual Question Answering. (arXiv:2206.12533v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12533","description":"<p>In order to achieve a general visual question answering (VQA) system, it is\nessential to learn to answer deeper questions that require compositional\nreasoning on the image and external knowledge. Meanwhile, the reasoning process\nshould be explicit and explainable to understand the working mechanism of the\nmodel. It is effortless for human but challenging for machines. In this paper,\nwe propose a Hierarchical Graph Neural Module Network (HGNMN) that reasons over\nmulti-layer graphs with neural modules to address the above issues.\nSpecifically, we first encode the image by multi-layer graphs from the visual,\nsemantic and commonsense views since the clues that support the answer may\nexist in different modalities. Our model consists of several well-designed\nneural modules that perform specific functions over graphs, which can be used\nto conduct multi-step reasoning within and between different graphs. Compared\nto existing modular networks, we extend visual reasoning from one graph to more\ngraphs. We can explicitly trace the reasoning process according to module\nweights and graph attentions. Experiments show that our model not only achieves\nstate-of-the-art performance on the CRIC dataset but also obtains explicit and\nexplainable reasoning procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLIC: Self-Supervised Learning with Iterative Clustering for Human Action Videos. (arXiv:2206.12534v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12534","description":"<p>Self-supervised methods have significantly closed the gap with end-to-end\nsupervised learning for image classification. In the case of human action\nvideos, however, where both appearance and motion are significant factors of\nvariation, this gap remains significant. One of the key reasons for this is\nthat sampling pairs of similar video clips, a required step for many\nself-supervised contrastive learning methods, is currently done conservatively\nto avoid false positives. A typical assumption is that similar clips only occur\ntemporally close within a single video, leading to insufficient examples of\nmotion similarity. To mitigate this, we propose SLIC, a clustering-based\nself-supervised contrastive learning method for human action videos. Our key\ncontribution is that we improve upon the traditional intra-video positive\nsampling by using iterative clustering to group similar video instances. This\nenables our method to leverage pseudo-labels from the cluster assignments to\nsample harder positives and negatives. SLIC outperforms state-of-the-art video\nretrieval baselines by +15.4% on top-1 recall on UCF101 and by +5.7% when\ndirectly transferred to HMDB51. With end-to-end finetuning for action\nclassification, SLIC achieves 83.2% top-1 accuracy (+0.8%) on UCF101 and 54.5%\non HMDB51 (+1.6%). SLIC is also competitive with the state-of-the-art in action\nclassification after self-supervised pretraining on Kinetics400.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khorasgani_S/0/1/0/all/0/1\">Salar Hosseini Khorasgani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shkurti_F/0/1/0/all/0/1\">Florian Shkurti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LFPS-Net: a lightweight fast pulse simulation network for BVP estimation. (arXiv:2206.12558v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12558","description":"<p>Heart rate estimation based on remote photoplethysmography plays an important\nrole in several specific scenarios, such as health monitoring and fatigue\ndetection. Existing well-established methods are committed to taking the\naverage of the predicted HRs of multiple overlapping video clips as the final\nresults for the 30-second facial video. Although these methods with hundreds of\nlayers and thousands of channels are highly accurate and robust, they require\nenormous computational budget and a 30-second wait time, which greatly limits\nthe application of the algorithms to scale. Under these cicumstacnces, We\npropose a lightweight fast pulse simulation network (LFPS-Net), pursuing the\nbest accuracy within a very limited computational and time budget, focusing on\ncommon mobile platforms, such as smart phones. In order to suppress the noise\ncomponent and get stable pulse in a short time, we design a multi-frequency\nmodal signal fusion mechanism, which exploits the theory of time-frequency\ndomain analysis to separate multi-modal information from complex signals. It\nhelps proceeding network learn the effective fetures more easily without adding\nany parameter. In addition, we design a oversampling training strategy to solve\nthe problem caused by the unbalanced distribution of dataset. For the 30-second\nfacial videos, our proposed method achieves the best results on most of the\nevaluation metrics for estimating heart rate or heart rate variability compared\nto the best available papers. The proposed method can still obtain very\ncompetitive results by using a short-time (~15-second) facail video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jialiang Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiujuan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CV 3315 Is All You Need : Semantic Segmentation Competition. (arXiv:2206.12571v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12571","description":"<p>This competition focus on Urban-Sense Segmentation based on the vehicle\ncamera view. Class highly unbalanced Urban-Sense images dataset challenge the\nexisting solutions and further studies. Deep Conventional neural network-based\nsemantic segmentation methods such as encoder-decoder architecture and\nmulti-scale and pyramid-based approaches become flexible solutions applicable\nto real-world applications. In this competition, we mainly review the\nliterature and conduct experiments on transformer-driven methods especially\nSegFormer, to achieve an optimal trade-off between performance and efficiency.\nFor example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,\nand the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple\nfactors, including individual case failure analysis, individual class\nperformance, training pressure and efficiency estimation, the final candidate\nmodel for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU\nevaluated on the testing set. Checkout our code implementation at\nhttps://vmv.re/cv3315.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Akide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RSTAM: An Effective Black-Box Impersonation Attack on Face Recognition using a Mobile and Compact Printer. (arXiv:2206.12590v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12590","description":"<p>Face recognition has achieved considerable progress in recent years thanks to\nthe development of deep neural networks, but it has recently been discovered\nthat deep neural networks are vulnerable to adversarial examples. This means\nthat face recognition models or systems based on deep neural networks are also\nsusceptible to adversarial examples. However, the existing methods of attacking\nface recognition models or systems with adversarial examples can effectively\ncomplete white-box attacks but not black-box impersonation attacks, physical\nattacks, or convenient attacks, particularly on commercial face recognition\nsystems. In this paper, we propose a new method to attack face recognition\nmodels or systems called RSTAM, which enables an effective black-box\nimpersonation attack using an adversarial mask printed by a mobile and compact\nprinter. First, RSTAM enhances the transferability of the adversarial masks\nthrough our proposed random similarity transformation strategy. Furthermore, we\npropose a random meta-optimization strategy for ensembling several pre-trained\nface models to generate more general adversarial masks. Finally, we conduct\nexperiments on the CelebA-HQ, LFW, Makeup Transfer (MT), and CASIA-FaceV5\ndatasets. The performance of the attacks is also evaluated on state-of-the-art\ncommercial face recognition systems: Face++, Baidu, Aliyun, Tencent, and\nMicrosoft. Extensive experiments show that RSTAM can effectively perform\nblack-box impersonation attacks on face recognition models or systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Furao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1\">Changhai Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetric Transfer Hashing with Adaptive Bipartite Graph Learning. (arXiv:2206.12592v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12592","description":"<p>Thanks to the efficient retrieval speed and low storage consumption, learning\nto hash has been widely used in visual retrieval tasks. However, existing\nhashing methods assume that the query and retrieval samples lie in homogeneous\nfeature space within the same domain. As a result, they cannot be directly\napplied to heterogeneous cross-domain retrieval. In this paper, we propose a\nGeneralized Image Transfer Retrieval (GITR) problem, which encounters two\ncrucial bottlenecks: 1) the query and retrieval samples may come from different\ndomains, leading to an inevitable {domain distribution gap}; 2) the features of\nthe two domains may be heterogeneous or misaligned, bringing up an additional\n{feature gap}. To address the GITR problem, we propose an Asymmetric Transfer\nHashing (ATH) framework with its unsupervised/semi-supervised/supervised\nrealizations. Specifically, ATH characterizes the domain distribution gap by\nthe discrepancy between two asymmetric hash functions, and minimizes the\nfeature gap with the help of a novel adaptive bipartite graph constructed on\ncross-domain data. By jointly optimizing asymmetric hash functions and the\nbipartite graph, not only can knowledge transfer be achieved but information\nloss caused by feature alignment can also be avoided. Meanwhile, to alleviate\nnegative transfer, the intrinsic geometrical structure of single-domain data is\npreserved by involving a domain affinity graph. Extensive experiments on both\nsingle-domain and cross-domain benchmarks under different GITR subtasks\nindicate the superiority of our ATH method in comparison with the\nstate-of-the-art hashing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianglin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yudong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1\">Witold Pedrycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_K/0/1/0/all/0/1\">Kwok-Wai Hung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning. (arXiv:2206.12596v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12596","description":"<p>Deformable image registration is a crucial step in medical image analysis for\nfinding a non-linear spatial transformation between a pair of fixed and moving\nimages. Deep registration methods based on Convolutional Neural Networks (CNNs)\nhave been widely used as they can perform image registration in a fast and\nend-to-end manner. However, these methods usually have limited performance for\nimage pairs with large deformations. Recently, iterative deep registration\nmethods have been used to alleviate this limitation, where the transformations\nare iteratively learned in a coarse-to-fine manner. However, iterative methods\ninevitably prolong the registration runtime, and tend to learn separate image\nfeatures for each iteration, which hinders the features from being leveraged to\nfacilitate the registration at later iterations. In this study, we propose a\nNon-Iterative Coarse-to-finE registration Network (NICE-Net) for deformable\nimage registration. In the NICE-Net, we propose: (i) a Single-pass Deep\nCumulative Learning (SDCL) decoder that can cumulatively learn coarse-to-fine\ntransformations within a single pass (iteration) of the network, and (ii) a\nSelectively-propagated Feature Learning (SFL) encoder that can learn common\nimage features for the whole coarse-to-fine registration process and\nselectively propagate the features as needed. Extensive experiments on six\npublic datasets of 3D brain Magnetic Resonance Imaging (MRI) show that our\nproposed NICE-Net can outperform state-of-the-art iterative deep registration\nmethods while only requiring similar runtime to non-iterative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_L/0/1/0/all/0/1\">Lei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dagan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Predict How Humans Manipulate Large-sized Objects from Interactive Motions. (arXiv:2206.12612v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12612","description":"<p>Understanding human intentions during interactions has been a long-lasting\ntheme, that has applications in human-robot interaction, virtual reality and\nsurveillance. In this study, we focus on full-body human interactions with\nlarge-sized daily objects and aim to predict the future states of objects and\nhumans given a sequential observation of human-object interaction. As there is\nno such dataset dedicated to full-body human interactions with large-sized\ndaily objects, we collected a large-scale dataset containing thousands of\ninteractions for training and evaluation purposes. We also observe that an\nobject's intrinsic physical properties are useful for the object motion\nprediction, and thus design a set of object dynamic descriptors to encode such\nintrinsic properties. We treat the object dynamic descriptors as a new modality\nand propose a graph neural network, HO-GCN, to fuse motion data and dynamic\ndescriptors for the prediction task. We show the proposed network that consumes\ndynamic descriptors can achieve state-of-the-art prediction results and help\nthe network better generalize to unseen objects. We also demonstrate the\npredicted results are useful for human-robot collaborations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weilin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuoying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruixing Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yi-King Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BokehMe: When Neural Rendering Meets Classical Rendering. (arXiv:2206.12614v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12614","description":"<p>We propose BokehMe, a hybrid bokeh rendering framework that marries a neural\nrenderer with a classical physically motivated renderer. Given a single image\nand a potentially imperfect disparity map, BokehMe generates high-resolution\nphoto-realistic bokeh effects with adjustable blur size, focal plane, and\naperture shape. To this end, we analyze the errors from the classical\nscattering-based method and derive a formulation to calculate an error map.\nBased on this formulation, we implement the classical renderer by a\nscattering-based method and propose a two-stage neural renderer to fix the\nerroneous areas from the classical renderer. The neural renderer employs a\ndynamic multi-scale scheme to efficiently handle arbitrary blur sizes, and it\nis trained to handle imperfect disparity input. Experiments show that our\nmethod compares favorably against previous methods on both synthetic image data\nand real image data with predicted disparity. A user study is further conducted\nto validate the advantage of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Juewen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiguo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xianrui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_K/0/1/0/all/0/1\">Ke Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAT: Self-adaptive training for fashion compatibility prediction. (arXiv:2206.12622v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12622","description":"<p>This paper presents a self-adaptive training (SAT) model for fashion\ncompatibility prediction. It focuses on the learning of some hard items, such\nas those that share similar color, texture, and pattern features but are\nconsidered incompatible due to the aesthetics or temporal shifts. Specifically,\nwe first design a method to define hard outfits and a difficulty score (DS) is\ndefined and assigned to each outfit based on the difficulty in recommending an\nitem for it. Then, we propose a self-adaptive triplet loss (SATL), where the DS\nof the outfit is considered. Finally, we propose a very simple conditional\nsimilarity network combining the proposed SATL to achieve the learning of hard\nitems in the fashion compatibility prediction. Experiments on the publicly\navailable Polyvore Outfits and Polyvore Outfits-D datasets demonstrate our\nSAT's effectiveness in fashion compatibility prediction. Besides, our SATL can\nbe easily extended to other conditional similarity networks to improve their\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Ling Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamasaki_T/0/1/0/all/0/1\">Toshihiko Yamasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverted Semantic-Index for Image Retrieval. (arXiv:2206.12623v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12623","description":"<p>This paper addresses the construction of inverted index for large-scale image\nretrieval. The inverted index proposed by J. Sivic brings a significant\nacceleration by reducing distance computations with only a small fraction of\nthe database. The state-of-the-art inverted indices aim to build finer\npartitions that produce a concise and accurate candidate list. However,\npartitioning in these frameworks is generally achieved by unsupervised\nclustering methods which ignore the semantic information of images. In this\npaper, we replace the clustering method with image classification, during the\nconstruction of codebook. We then propose a merging and splitting method to\nsolve the problem that the number of partitions is unchangeable in the inverted\nsemantic-index. Next, we combine our semantic-index with the product\nquantization (PQ) so as to alleviate the accuracy loss caused by PQ\ncompression. Finally, we evaluate our model on large-scale image retrieval\nbenchmarks. Experiment results demonstrate that our model can significantly\nimprove the retrieval accuracy by generating high-quality candidate lists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SC-Transformer++: Structured Context Transformer for Generic Event Boundary Detection. (arXiv:2206.12634v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12634","description":"<p>This report presents the algorithm used in the submission of Generic Event\nBoundary Detection (GEBD) Challenge at CVPR 2022. In this work, we improve the\nexisting Structured Context Transformer (SC-Transformer) method for GEBD.\nSpecifically, a transformer decoder module is added after transformer encoders\nto extract high quality frame features. The final classification is performed\njointly on the results of the original binary classifier and a newly introduced\nmulti-class classifier branch. To enrich motion information, optical flow is\nintroduced as a new modality. Finally, model ensemble is used to further boost\nperformance. The proposed method achieves 86.49% F1 score on Kinetics-GEBD test\nset. which improves 2.86% F1 score compared to the previous SOTA method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dexiang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Congcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIMS-PU: Bi-Directional and Multi-Scale Point Cloud Upsampling. (arXiv:2206.12648v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12648","description":"<p>The learning and aggregation of multi-scale features are essential in\nempowering neural networks to capture the fine-grained geometric details in the\npoint cloud upsampling task. Most existing approaches extract multi-scale\nfeatures from a point cloud of a fixed resolution, hence obtain only a limited\nlevel of details. Though an existing approach aggregates a feature hierarchy of\ndifferent resolutions from a cascade of upsampling sub-network, the training is\ncomplex with expensive computation. To address these issues, we construct a new\npoint cloud upsampling pipeline called BIMS-PU that integrates the feature\npyramid architecture with a bi-directional up and downsampling path.\nSpecifically, we decompose the up/downsampling procedure into several\nup/downsampling sub-steps by breaking the target sampling factor into smaller\nfactors. The multi-scale features are naturally produced in a parallel manner\nand aggregated using a fast feature fusion method. Supervision signal is\nsimultaneously applied to all upsampled point clouds of different scales.\nMoreover, we formulate a residual block to ease the training of our model.\nExtensive quantitative and qualitative experiments on different datasets show\nthat our method achieves superior results to state-of-the-art approaches. Last\nbut not least, we demonstrate that point cloud upsampling can improve robot\nperception by ameliorating the 3D data quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yechao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis with R: Natural Language Processing for Semi-Automated Assessments of Qualitative Data. (arXiv:2206.12649v1 [cs.CL])","link":"http://arxiv.org/abs/2206.12649","description":"<p>Sentiment analysis is a sub-discipline in the field of natural language\nprocessing and computational linguistics and can be used for automated or\nsemi-automated analyses of text documents. One of the aims of these analyses is\nto recognize an expressed attitude as positive or negative as it can be\ncontained in comments on social media platforms or political documents and\nspeeches as well as fictional and nonfictional texts. Regarding analyses of\ncomments on social media platforms, this is an extension of the previous\ntutorial on semi-automated screenings of social media network data. A\nlongitudinal perspective regarding social media comments as well as\ncross-sectional perspectives regarding fictional and nonfictional texts, e.g.\nentire books and libraries, can lead to extensive text documents. Their\nanalyses can be simplified and accelerated by using sentiment analysis with\nacceptable inter-rater reliability. Therefore, this tutorial introduces the\nbasic functions for performing a sentiment analysis with R and explains how\ntext documents can be analysed step by step - regardless of their underlying\nformatting. All prerequisites and steps are described in detail and associated\ncodes are available on GitHub. A comparison of two political speeches\nillustrates a possible use case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klinkhammer_D/0/1/0/all/0/1\">Dennis Klinkhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning-based Biological Ageing Estimation Technologies: A Survey. (arXiv:2206.12650v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12650","description":"<p>In recent years, there are various methods of estimating Biological Age (BA)\nhave been developed. Especially with the development of machine learning (ML),\nthere are more and more types of BA predictions, and the accuracy has been\ngreatly improved. The models for the estimation of BA play an important role in\nmonitoring healthy aging, and could provide new tools to detect health status\nin the general population and give warnings to sub-healthy people. We will\nmainly review three age prediction methods by using ML. They are based on blood\nbiomarkers, facial images, and structural neuroimaging features. For now, the\nmodel using blood biomarkers is the simplest, most direct, and most accurate\nmethod. The face image method is affected by various aspects such as race,\nenvironment, etc., the prediction accuracy is not very good, which cannot make\na great contribution to the medical field. In summary, we are here to track the\nway forward in the era of big data for us and other potential general\npopulations and show ways to leverage the vast amounts of data available today.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaonian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Richard Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crookes_D/0/1/0/all/0/1\">Danny Crookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chazot_P/0/1/0/all/0/1\">Paul Chazot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review on Social Behavior Analysis of Laboratory Animals: From Methodologies to Applications. (arXiv:2206.12651v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12651","description":"<p>As the bridge between genetic and physiological aspects, animal behaviour\nanalysis is one of the most significant topics in biology and ecological\nresearch. However, identifying, tracking and recording animal behaviour are\nlabour intensive works that require professional knowledge. To mitigate the\nspend for annotating data, researchers turn to computer vision techniques for\nautomatic label algorithms, since most of the data are recorded visually. In\nthis work, we explore a variety of behaviour detection algorithms, covering\ntraditional vision methods, statistical methods and deep learning methods. The\nobjective of this work is to provide a thorough investigation of related work,\nfurnishing biologists with a scratch of efficient animal behaviour detection\nmethods. Apart from that, we also discuss the strengths and weaknesses of those\nalgorithms to provide some insights for those who already delve into this\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziping Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chazot_P/0/1/0/all/0/1\">Paul L. Chazot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Richard Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnostic Communication and Visual System based on Vehicle UDS Protocol. (arXiv:2206.12653v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12653","description":"<p>Unified Diagnostic Services (UDS) is a diagnostic communication protocol used\nin electronic control units (ECUs) within automotive electronics, which is\nspecified in the ISO 14229-1. It is derived from ISO 14230-3 (KWP2000) and the\nnow obsolete ISO 15765-3 (Diagnostic Communication over Controller Area Network\n(DoCAN). 'Unified' in this context means that it is an international and not a\ncompany-specific standard. By now this communication protocol is used in all\nnew ECUs made by Tier 1 suppliers of Original Equipment Manufacturer (OEM), and\nis incorporated into other standards, such as AUTOSAR. The ECUs in modern\nvehicles control nearly all functions, including electronic fuel injection\n(EFI), engine control, the transmission, anti-lock braking system, door locks,\nbraking, window operation, and more.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Ding Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Deep Animation Video Interpolation. (arXiv:2206.12657v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12657","description":"<p>Existing learning-based frame interpolation algorithms extract consecutive\nframes from high-speed natural videos to train the model. Compared to natural\nvideos, cartoon videos are usually in a low frame rate. Besides, the motion\nbetween consecutive cartoon frames is typically nonlinear, which breaks the\nlinear motion assumption of interpolation algorithms. Thus, it is unsuitable\nfor generating a training set directly from cartoon videos. For better adapting\nframe interpolation algorithms from nature video to animation video, we present\nAutoFI, a simple and effective method to automatically render training data for\ndeep animation video interpolation. AutoFI takes a layered architecture to\nrender synthetic data, which ensures the assumption of linear motion.\nExperimental results show that AutoFI performs favorably in training both DAIN\nand ANIN. However, most frame interpolation algorithms will still fail in\nerror-prone areas, such as fast motion or large occlusion. Besides AutoFI, we\nalso propose a plug-and-play sketch-based post-processing module, named SktFI,\nto refine the final results using user-provided sketches manually. With AutoFI\nand SktFI, the interpolated animation frames show high perceptual quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_C/0/1/0/all/0/1\">Cheng Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wenbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Infer 3D Shape Programs with Differentiable Renderer. (arXiv:2206.12675v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12675","description":"<p>Given everyday artifacts, such as tables and chairs, humans recognize\nhigh-level regularities within them, such as the symmetries of a table, the\nrepetition of its legs, while possessing low-level priors of their geometries,\ne.g., surfaces are smooth and edges are sharp. This kind of knowledge\nconstitutes an important part of human perceptual understanding and reasoning.\nRepresentations of and how to reason in such knowledge, and the acquisition\nthereof, are still open questions in artificial intelligence (AI) and cognitive\nscience. Building on the previous proposal of the \\emph{3D shape programs}\nrepresentation alone with the accompanying neural generator and executor from\n\\citet{tian2019learning}, we propose an analytical yet differentiable executor\nthat is more faithful and controllable in interpreting shape programs\n(particularly in extrapolation) and more sample efficient (requires no\ntraining). These facilitate the generator's learning when ground truth programs\nare not available, and should be especially useful when new shape-program\ncomponents are enrolled either by human designers or -- in the context of\nlibrary learning -- algorithms themselves. Preliminary experiments on using it\nfor adaptation illustrate the aforesaid advantages of the proposed module,\nencouraging similar methods being explored in building machines that learn to\nreason with the kind of knowledge described above, and even learn this\nknowledge itself.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yichao Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UltraMNIST Classification: A Benchmark to Train CNNs for Very Large Images. (arXiv:2206.12681v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12681","description":"<p>Convolutional neural network (CNN) approaches available in the current\nliterature are designed to work primarily with low-resolution images. When\napplied on very large images, challenges related to GPU memory, smaller\nreceptive field than needed for semantic correspondence and the need to\nincorporate multi-scale features arise. The resolution of input images can be\nreduced, however, with significant loss of critical information. Based on the\noutlined issues, we introduce a novel research problem of training CNN models\nfor very large images, and present 'UltraMNIST dataset', a simple yet\nrepresentative benchmark dataset for this task. UltraMNIST has been designed\nusing the popular MNIST digits with additional levels of complexity added to\nreplicate well the challenges of real-world problems. We present two variants\nof the problem: 'UltraMNIST classification' and 'Budget-aware UltraMNIST\nclassification'. The standard UltraMNIST classification benchmark is intended\nto facilitate the development of novel CNN training methods that make the\neffective use of the best available GPU resources. The budget-aware variant is\nintended to promote development of methods that work under constrained GPU\nmemory. For the development of competitive solutions, we present several\nbaseline models for the standard benchmark and its budget-aware variant. We\nstudy the effect of reducing resolution on the performance and present results\nfor baseline models involving pretrained backbones from among the popular\nstate-of-the-art models. Finally, with the presented benchmark dataset and the\nbaselines, we hope to pave the ground for a new generation of CNN methods\nsuitable for handling large images in an efficient and resource-light manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak K. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamba_U/0/1/0/all/0/1\">Udbhav Bamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhishek Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharan_S/0/1/0/all/0/1\">Suraj Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_E/0/1/0/all/0/1\">Ertugrul Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_D/0/1/0/all/0/1\">Dilip K. Prasad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defense against adversarial attacks on deep convolutional neural networks through nonlocal denoising. (arXiv:2206.12685v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12685","description":"<p>Despite substantial advances in network architecture performance, the\nsusceptibility of adversarial attacks makes deep learning challenging to\nimplement in safety-critical applications. This paper proposes a data-centric\napproach to addressing this problem. A nonlocal denoising method with different\nluminance values has been used to generate adversarial examples from the\nModified National Institute of Standards and Technology database (MNIST) and\nCanadian Institute for Advanced Research (CIFAR-10) data sets. Under\nperturbation, the method provided absolute accuracy improvements of up to 9.3%\nin the MNIST data set and 13% in the CIFAR-10 data set. Training using\ntransformed images with higher luminance values increases the robustness of the\nclassifier. We have shown that transfer learning is disadvantageous for\nadversarial machine learning. The results indicate that simple adversarial\nexamples can improve resilience and make deep learning easier to apply in\nvarious applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1\">Sandhya Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_N/0/1/0/all/0/1\">Nagender Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abas_P/0/1/0/all/0/1\">Pg Emeroylariffion Abas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naim_A/0/1/0/all/0/1\">Abdul Ghani Naim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RandStainNA: Learning Stain-Agnostic Features from Histology Slides by Bridging Stain Augmentation and Normalization. (arXiv:2206.12694v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12694","description":"<p>Stain variations often decrease the generalization ability of deep learning\nbased approaches in digital histopathology analysis. Two separate proposals,\nnamely stain normalization (SN) and stain augmentation (SA), have been\nspotlighted to reduce the generalization error, where the former alleviates the\nstain shift across different medical centers using template image and the\nlatter enriches the accessible stain styles by the simulation of more stain\nvariations. However, their applications are bounded by the selection of\ntemplate images and the construction of unrealistic styles. To address the\nproblems, we unify SN and SA with a novel RandStainNA scheme, which constrains\nvariable stain styles in a practicable range to train a stain agnostic deep\nlearning model. The RandStainNA is applicable to stain normalization in a\ncollection of color spaces i.e. HED, HSV, LAB. Additionally, we propose a\nrandom color space selection scheme to gain extra performance improvement. We\nevaluate our method by two diagnostic tasks i.e. tissue subtype classification\nand nuclei segmentation, with various network backbones. The performance\nsuperiority over both SA and SN yields that the proposed RandStainNA can\nconsistently improve the generalization ability, that our models can cope with\nmore incoming clinical datasets with unpredicted stain styles. The codes is\navailable at https://github.com/yiqings/RandStainNA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yulin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1\">Jing Ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays. (arXiv:2206.12704v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12704","description":"<p>Creating a large-scale dataset of abnormality annotation on medical images is\na labor-intensive and costly task. Leveraging weak supervision from readily\navailable data such as radiology reports can compensate lack of large-scale\ndata for anomaly detection methods. However, most of the current methods only\nuse image-level pathological observations, failing to utilize the relevant\nanatomy mentions in reports. Furthermore, Natural Language Processing\n(NLP)-mined weak labels are noisy due to label sparsity and linguistic\nambiguity. We propose an Anatomy-Guided chest X-ray Network (AGXNet) to address\nthese issues of weak annotation. Our framework consists of a cascade of two\nnetworks, one responsible for identifying anatomical abnormalities and the\nsecond responsible for pathological observations. The critical component in our\nframework is an anatomy-guided attention module that aids the downstream\nobservation network in focusing on the relevant anatomical regions generated by\nthe anatomy network. We use Positive Unlabeled (PU) learning to account for the\nfact that lack of mention does not necessarily mean a negative label. Our\nquantitative and qualitative results on the MIMIC-CXR dataset demonstrate the\neffectiveness of AGXNet in disease and anatomical abnormality localization.\nExperiments on the NIH Chest X-ray dataset show that the learned feature\nrepresentations are transferable and can achieve the state-of-the-art\nperformances in disease classification and competitive disease localization\nresults. Our code is available at https://github.com/batmanlab/AGXNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Ke Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shantanu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhexiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deible_C/0/1/0/all/0/1\">Christopher Deible</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"p-Meta: Towards On-device Deep Model Adaptation. (arXiv:2206.12705v1 [cs.LG])","link":"http://arxiv.org/abs/2206.12705","description":"<p>Data collected by IoT devices are often private and have a large diversity\nacross users. Therefore, learning requires pre-training a model with available\nrepresentative data samples, deploying the pre-trained model on IoT devices,\nand adapting the deployed model on the device with local data. Such an\non-device adaption for deep learning empowered applications demands data and\nmemory efficiency. However, existing gradient-based meta learning schemes fail\nto support memory-efficient adaptation. To this end, we propose p-Meta, a new\nmeta learning method that enforces structure-wise partial parameter updates\nwhile ensuring fast generalization to unseen tasks. Evaluations on few-shot\nimage classification and reinforcement learning tasks show that p-Meta not only\nimproves the accuracy but also substantially reduces the peak dynamic memory by\na factor of 2.5 on average compared to state-of-the-art few-shot adaptation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1\">Zhongnan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zimu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yongxin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1\">Lothar Thiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Multimodal Fusion Models against Single-Source Adversaries. (arXiv:2206.12714v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12714","description":"<p>Beyond achieving high performance across many vision tasks, multimodal models\nare expected to be robust to single-source faults due to the availability of\nredundant information between modalities. In this paper, we investigate the\nrobustness of multimodal neural networks against worst-case (i.e., adversarial)\nperturbations on a single modality. We first show that standard multimodal\nfusion models are vulnerable to single-source adversaries: an attack on any\nsingle modality can overcome the correct information from multiple unperturbed\nmodalities and cause the model to fail. This surprising vulnerability holds\nacross diverse multimodal tasks and necessitates a solution. Motivated by this\nfinding, we propose an adversarially robust fusion strategy that trains the\nmodel to compare information coming from all the input sources, detect\ninconsistencies in the perturbed modality compared to the other modalities, and\nonly allow information from the unperturbed modalities to pass through. Our\napproach significantly improves on state-of-the-art methods in single-source\nrobustness, achieving gains of 7.8-25.2% on action recognition, 19.7-48.2% on\nobject detection, and 1.6-6.7% on sentiment analysis, without degrading\nperformance on unperturbed (i.e., clean) data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Karren Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wan-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barman_M/0/1/0/all/0/1\">Manash Barman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condessa_F/0/1/0/all/0/1\">Filipe Condessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_Z/0/1/0/all/0/1\">Zico Kolter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Evaluation of Physical Adversarial Patch Attacks Against Overhead Object Detection Models. (arXiv:2206.12725v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12725","description":"<p>Adversarial patches are images designed to fool otherwise well-performing\nneural network-based computer vision models. Although these attacks were\ninitially conceived of and studied digitally, in that the raw pixel values of\nthe image were perturbed, recent work has demonstrated that these attacks can\nsuccessfully transfer to the physical world. This can be accomplished by\nprinting out the patch and adding it into scenes of newly captured images or\nvideo footage. In this work we further test the efficacy of adversarial patch\nattacks in the physical world under more challenging conditions. We consider\nobject detection models trained on overhead imagery acquired through aerial or\nsatellite cameras, and we test physical adversarial patches inserted into\nscenes of a desert environment. Our main finding is that it is far more\ndifficult to successfully implement the adversarial patch attacks under these\nconditions than in the previously considered conditions. This has important\nimplications for AI safety as the real-world threat posed by adversarial\nexamples may be overstated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartnett_G/0/1/0/all/0/1\">Gavin S. Hartnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Ang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnell_C/0/1/0/all/0/1\">Caolionn O&#x27;Connell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohn_A/0/1/0/all/0/1\">Andrew J. Lohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguirre_J/0/1/0/all/0/1\">Jair Aguirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised 3D Monocular Object Detection by Recycling Bounding Boxes. (arXiv:2206.12738v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12738","description":"<p>Modern object detection architectures are moving towards employing\nself-supervised learning (SSL) to improve performance detection with related\npretext tasks. Pretext tasks for monocular 3D object detection have not yet\nbeen explored yet in literature. The paper studies the application of\nestablished self-supervised bounding box recycling by labeling random windows\nas the pretext task. The classifier head of the 3D detector is trained to\nclassify random windows containing different proportions of the ground truth\nobjects, thus handling the foreground-background imbalance. We evaluate the\npretext task using the RTM3D detection model as baseline, with and without the\napplication of data augmentation. We demonstrate improvements of between 2-3 %\nin mAP 3D and 0.9-1.5 % BEV scores using SSL over the baseline scores. We\npropose the inverse class frequency re-weighted (ICFW) mAP score that\nhighlights improvements in detection for low frequency classes in a class\nimbalanced dataset with long tails. We demonstrate improvements in ICFW both\nmAP 3D and BEV scores to take into account the class imbalance in the KITTI\nvalidation dataset. We see 4-5 % increase in ICFW metric with the pretext task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+T_S/0/1/0/all/0/1\">Sugirtha T</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_S/0/1/0/all/0/1\">Sridevi M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santhakumar_K/0/1/0/all/0/1\">Khailash Santhakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_B/0/1/0/all/0/1\">B Ravi Kiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_T/0/1/0/all/0/1\">Thomas Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Visual Modality Fall Detection Dataset. (arXiv:2206.12740v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12740","description":"<p>Falls are one of the leading cause of injury-related deaths among the elderly\nworldwide. Effective detection of falls can reduce the risk of complications\nand injuries. Fall detection can be performed using wearable devices or ambient\nsensors; these methods may struggle with user compliance issues or false\nalarms. Video cameras provide a passive alternative; however, regular RGB\ncameras are impacted by changing lighting conditions and privacy concerns. From\na machine learning perspective, developing an effective fall detection system\nis challenging because of the rarity and variability of falls. Many existing\nfall detection datasets lack important real-world considerations, such as\nvaried lighting, continuous activities of daily living (ADLs), and camera\nplacement. The lack of these considerations makes it difficult to develop\npredictive models that can operate effectively in the real world. To address\nthese limitations, we introduce a novel multi-modality dataset (MUVIM) that\ncontains four visual modalities: infra-red, depth, RGB and thermal cameras.\nThese modalities offer benefits such as obfuscated facial features and improved\nperformance in low-light conditions. We formulated fall detection as an anomaly\ndetection problem, in which a customized spatio-temporal convolutional\nautoencoder was trained only on ADLs so that a fall would increase the\nreconstruction error. Our results showed that infra-red cameras provided the\nhighest level of performance (AUC ROC=0.94), followed by thermal (AUC\nROC=0.87), depth (AUC ROC=0.86) and RGB (AUC ROC=0.83). This research provides\na unique opportunity to analyze the utility of camera modalities in detecting\nfalls in a home setting while balancing performance, passiveness, and privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Denkovski_S/0/1/0/all/0/1\">Stefan Denkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shehroz S. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malamis_B/0/1/0/all/0/1\">Brandon Malamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Sae Young Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1\">Bing Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihailidis_A/0/1/0/all/0/1\">Alex Mihailidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential image recovery using joint hierarchical Bayesian learning. (arXiv:2206.12745v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12745","description":"<p>Recovering temporal image sequences (videos) based on indirect, noisy, or\nincomplete data is an essential yet challenging task. We specifically consider\nthe case where each data set is missing vital information, which prevents the\naccurate recovery of the individual images. Although some recent (variational)\nmethods have demonstrated high-resolution image recovery based on jointly\nrecovering sequential images, there remain robustness issues due to parameter\ntuning and restrictions on the type of the sequential images. Here, we present\na method based on hierarchical Bayesian learning for the joint recovery of\nsequential images that incorporates prior intra- and inter-image information.\nOur method restores the missing information in each image by \"borrowing\" it\nfrom the other images. As a result, \\emph{all} of the individual\nreconstructions yield improved accuracy. Our method can be used for various\ndata acquisitions and allows for uncertainty quantification. Some preliminary\nresults indicate its potential use for sequential deblurring and magnetic\nresonance imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaubitz_J/0/1/0/all/0/1\">Jan Glaubitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Data Mining: A Survey. (arXiv:2206.12753v1 [cs.DB])","link":"http://arxiv.org/abs/2206.12753","description":"<p>Spatiotemporal data mining aims to discover interesting, useful but\nnon-trivial patterns in big spatial and spatiotemporal data. They are used in\nvarious application domains such as public safety, ecology, epidemiology, earth\nscience, etc. This problem is challenging because of the high societal cost of\nspurious patterns and exorbitant computational cost. Recent surveys of\nspatiotemporal data mining need update due to rapid growth. In addition, they\ndid not adequately survey parallel techniques for spatiotemporal data mining.\nThis paper provides a more up-to-date survey of spatiotemporal data mining\nmethods. Furthermore, it has a detailed survey of parallel formulations of\nspatiotemporal data mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arun Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shashi Shekhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Your Sparse Neural Network Better with Any Mask. (arXiv:2206.12755v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12755","description":"<p>Pruning large neural networks to create high-quality, independently trainable\nsparse masks, which can maintain similar performance to their dense\ncounterparts, is very desirable due to the reduced space and time complexity.\nAs research effort is focused on increasingly sophisticated pruning methods\nthat leads to sparse subnetworks trainable from the scratch, we argue for an\northogonal, under-explored theme: improving training techniques for pruned\nsub-networks, i.e. sparse training. Apart from the popular belief that only the\nquality of sparse masks matters for sparse training, in this paper we\ndemonstrate an alternative opportunity: one can \\textit{carefully customize the\nsparse training techniques to deviate from the default dense network training\nprotocols}, consisting of introducing ``ghost\" neurons and skip connections at\nthe early stage of training, and strategically modifying the initialization as\nwell as labels. Our new sparse training recipe is generally applicable to\nimproving training from scratch with various sparse masks. By adopting our\nnewly curated techniques, we demonstrate significant performance gains across\nvarious popular datasets (CIFAR-10, CIFAR-100, TinyImageNet), architectures\n(ResNet-18/32/104, Vgg16, MobileNet), and sparse mask options (lottery ticket,\nSNIP/GRASP, SynFlow, or even randomly pruning), compared to the default\ntraining protocols, especially at high sparsity levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation. (arXiv:2206.12772v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12772","description":"<p>We present a simple yet effective self-supervised framework for audio-visual\nrepresentation learning, to localize the sound source in videos. To understand\nwhat enables to learn useful representations, we systematically investigate the\neffects of data augmentations, and reveal that (1) composition of data\naugmentations plays a critical role, {\\em i.e.}~explicitly encouraging the\naudio-visual representations to be invariant to various transformations~({\\em\ntransformation invariance}); (2) enforcing geometric consistency substantially\nimproves the quality of learned representations, {\\em i.e.}~the detected sound\nsource should follow the same transformation applied on input video\nframes~({\\em transformation equivariance}). Extensive experiments demonstrate\nthat our model significantly outperforms previous methods on two sound\nlocalization benchmarks, namely, Flickr-SoundNet and VGG-Sound. Additionally,\nwe also evaluate audio retrieval and cross-modal retrieval tasks. In both\ncases, our self-supervised models demonstrate superior retrieval performances,\neven competitive with the supervised approach in audio retrieval. This reveals\nthe proposed framework learns strong multi-modal representations that are\nbeneficial to sound localisation and generalization to further applications.\n\\textit{All codes will be available}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chen Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation with Representative Teacher Keys Based on Attention Mechanism for Image Classification Model Compression. (arXiv:2206.12788v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12788","description":"<p>With the improvement of AI chips (e.g., GPU, TPU, and NPU) and the fast\ndevelopment of internet of things (IoTs), some powerful deep neural networks\n(DNNs) are usually composed of millions or even hundreds of millions of\nparameters, which may not be suitable to be directly deployed on low\ncomputation and low capacity units (e.g., edge devices). Recently, knowledge\ndistillation (KD) has been recognized as one of the effective method of model\ncompression to decrease the model parameters. The main concept of KD is to\nextract useful information from the feature maps of a large model (i.e.,\nteacher model) as a reference to successfully train a small model (i.e.,\nstudent model) which model size is much smaller than the teacher one. Although\nmany KD-based methods have been proposed to utilize the information from the\nfeature maps of intermediate layers in teacher model, however, most of them did\nnot consider the similarity of feature maps between teacher model and student\nmodel, which may let student model learn useless information. Inspired by\nattention mechanism, we propose a novel KD method called representative teacher\nkey (RTK) that not only consider the similarity of feature maps but also filter\nout the useless information to improve the performance of the target student\nmodel. In the experiments, we validate our proposed method with several\nbackbone networks (e.g., ResNet and WideResNet) and datasets (e.g., CIFAR10,\nCIFAR100, SVHN, and CINIC10). The results show that our proposed RTK can\neffectively improve the classification accuracy of the state-of-the-art\nattention-based KD method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jun-Teng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_S/0/1/0/all/0/1\">Sheng-Che Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Scott C.-H. Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTMQ: Cyclic Training of Convolutional Neural Networks with Multiple Quantization Steps. (arXiv:2206.12794v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12794","description":"<p>This paper proposes a training method having multiple cyclic training for\nachieving enhanced performance in low-bit quantized convolutional neural\nnetworks (CNNs). Quantization is a popular method for obtaining lightweight\nCNNs, where the initialization with a pretrained model is widely used to\novercome degraded performance in low-resolution quantization. However, large\nquantization errors between real values and their low-bit quantized ones cause\ndifficulties in achieving acceptable performance for complex networks and large\ndatasets. The proposed training method softly delivers the knowledge of\npretrained models to low-bit quantized models in multiple quantization steps.\nIn each quantization step, the trained weights of a model are used to\ninitialize the weights of the next model with the quantization bit depth\nreduced by one. With small change of the quantization bit depth, the\nperformance gap can be bridged, thus providing better weight initialization. In\ncyclic training, after training a low-bit quantized model, its trained weights\nare used in the initialization of its accurate model to be trained. By using\nbetter training ability of the accurate model in an iterative manner, the\nproposed method can produce enhanced trained weights for the low-bit quantized\nmodel in each cycle. Notably, the training method can advance Top-1 and Top-5\naccuracies of the binarized ResNet-18 on the ImageNet dataset by 5.80% and\n6.85%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyunJin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jungwoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrio_A/0/1/0/all/0/1\">Alberto A. Del Barrio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Instance Learning with Mixed Supervision in Gleason Grading. (arXiv:2206.12798v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12798","description":"<p>With the development of computational pathology, deep learning methods for\nGleason grading through whole slide images (WSIs) have excellent prospects.\nSince the size of WSIs is extremely large, the image label usually contains\nonly slide-level label or limited pixel-level labels. The current mainstream\napproach adopts multi-instance learning to predict Gleason grades. However,\nsome methods only considering the slide-level label ignore the limited\npixel-level labels containing rich local information. Furthermore, the method\nof additionally considering the pixel-level labels ignores the inaccuracy of\npixel-level labels. To address these problems, we propose a mixed supervision\nTransformer based on the multiple instance learning framework. The model\nutilizes both slide-level label and instance-level labels to achieve more\naccurate Gleason grading at the slide level. The impact of inaccurate\ninstance-level labels is further reduced by introducing an efficient random\nmasking strategy in the mixed supervision training process. We achieve the\nstate-of-the-art performance on the SICAPv2 dataset, and the visual analysis\nshows the accurate prediction results of instance level. The source code is\navailable at https://github.com/bianhao123/Mixed_supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_H/0/1/0/all/0/1\">Hao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhuchen Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongbing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of AIS, X-Band Marine Radar Systems and Camera Surveillance Systems in the Collection of Tracking Data. (arXiv:2206.12809v1 [eess.SP])","link":"http://arxiv.org/abs/2206.12809","description":"<p>Maritime traffic has increased in recent years, especially in terms of\nseaborne trade. To ensure safety, security, and protection of the marine\nenvironment, several systems have been deployed. To overcome some of their\ninconveniences, the collected data is typically fused. The fused data is used\nfor various purposes, one of our interest is target tracking. The most relevant\nsystems in that context are AIS and X-band marine radar. Many works consider\nthat visual data provided by camera surveillance systems enable additional\nadvantages. Therefore, many tracking algorithms using visual data (images) have\nbeen developed. Yet, there is little emphasis on the reasons making the\nintegration of camera systems important. Thus, our main aim in this paper is to\nanalyze the aforementioned surveillance systems for target tracking and\nconclude some of the maritime security improvements resulted from the\nintegration of cameras to the overall maritime surveillance system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zardoua_Y/0/1/0/all/0/1\">Yassir Zardoua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Astito_A/0/1/0/all/0/1\">Abdelali Astito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boulaala_M/0/1/0/all/0/1\">Mohammed Boulaala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breast Cancer Classification using Deep Learned Features Boosted with Handcrafted Features. (arXiv:2206.12815v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12815","description":"<p>Breast cancer is one of the leading causes of death among women across the\nglobe. It is difficult to treat if detected at advanced stages, however, early\ndetection can significantly increase chances of survival and improves lives of\nmillions of women. Given the widespread prevalence of breast cancer, it is of\nutmost importance for the research community to come up with the framework for\nearly detection, classification and diagnosis. Artificial intelligence research\ncommunity in coordination with medical practitioners are developing such\nframeworks to automate the task of detection. With the surge in research\nactivities coupled with availability of large datasets and enhanced\ncomputational powers, it expected that AI framework results will help even more\nclinicians in making correct predictions. In this article, a novel framework\nfor classification of breast cancer using mammograms is proposed. The proposed\nframework combines robust features extracted from novel Convolutional Neural\nNetwork (CNN) features with handcrafted features including HOG (Histogram of\nOriented Gradients) and LBP (Local Binary Pattern). The obtained results on\nCBIS-DDSM dataset exceed state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sajid_U/0/1/0/all/0/1\">Unaiza Sajid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_D/0/1/0/all/0/1\">Dr. Rizwan Ahmed Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_D/0/1/0/all/0/1\">Dr. Shahid Munir Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arif_D/0/1/0/all/0/1\">Dr. Sheeraz Arif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer. (arXiv:2206.12837v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12837","description":"<p>This paper reports our solution for MultiMedia ViCo 2022 Conversational Head\nGeneration Challenge, which aims to generate vivid face-to-face conversation\nvideos based on audio and reference images. Our solution focuses on training a\ngeneralized audio-to-head driver using regularization and assembling a high\nvisual quality renderer. We carefully tweak the audio-to-behavior model and\npost-process the generated video using our foreground-background fusion module.\nWe get first place in the listening head generation track and second place in\nthe talking head generation track in the official ranking. Our code will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Ailin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhewei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval. (arXiv:2206.12845v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12845","description":"<p>Seas of videos are uploaded daily with the popularity of social channels;\nthus, retrieving the most related video contents with user textual queries\nplays a more crucial role. Most methods consider only one joint embedding space\nbetween global visual and textual features without considering the local\nstructures of each modality. Some other approaches consider multiple embedding\nspaces consisting of global and local features separately, ignoring rich\ninter-modality correlations.\n</p>\n<p>We propose a novel mixture-of-expert transformer RoME that disentangles the\ntext and the video into three levels; the roles of spatial contexts, temporal\ncontexts, and object contexts. We utilize a transformer-based attention\nmechanism to fully exploit visual and text embeddings at both global and local\nlevels with mixture-of-experts for considering inter-modalities and structures'\ncorrelations. The results indicate that our method outperforms the\nstate-of-the-art methods on the YouCook2 and MSR-VTT datasets, given the same\nvisual backbone without pre-training. Finally, we conducted extensive ablation\nstudies to elucidate our design choices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Satar_B/0/1/0/all/0/1\">Burak Satar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joo Hwee Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Role Aware Correlation Transformer for Text to Video Retrieval. (arXiv:2206.12849v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12849","description":"<p>With the emergence of social media, voluminous video clips are uploaded every\nday, and retrieving the most relevant visual content with a language query\nbecomes critical. Most approaches aim to learn a joint embedding space for\nplain textual and visual contents without adequately exploiting their\nintra-modality structures and inter-modality correlations. This paper proposes\na novel transformer that explicitly disentangles the text and video into\nsemantic roles of objects, spatial contexts and temporal contexts with an\nattention scheme to learn the intra- and inter-role correlations among the\nthree roles to discover discriminative features for matching at different\nlevels. The preliminary results on popular YouCook2 indicate that our approach\nsurpasses a current state-of-the-art method, with a high margin in all metrics.\nIt also overpasses two SOTA methods in terms of two metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Satar_B/0/1/0/all/0/1\">Burak Satar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bresson_X/0/1/0/all/0/1\">Xavier Bresson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joo Hwee Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Aesthetics Assessment Using Graph Attention Network. (arXiv:2206.12869v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12869","description":"<p>Aspect ratio and spatial layout are two of the principal factors determining\nthe aesthetic value of a photograph. But, incorporating these into the\ntraditional convolution-based frameworks for the task of image aesthetics\nassessment is problematic. The aspect ratio of the photographs gets distorted\nwhile they are resized/cropped to a fixed dimension to facilitate training\nbatch sampling. On the other hand, the convolutional filters process\ninformation locally and are limited in their ability to model the global\nspatial layout of a photograph. In this work, we present a two-stage framework\nbased on graph neural networks and address both these problems jointly. First,\nwe propose a feature-graph representation in which the input image is modelled\nas a graph, maintaining its original aspect ratio and resolution. Second, we\npropose a graph neural network architecture that takes this feature-graph and\ncaptures the semantic relationship between the different regions of the input\nimage using visual attention. Our experiments show that the proposed framework\nadvances the state-of-the-art results in aesthetic score regression on the\nAesthetic Visual Analysis (AVA) benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_K/0/1/0/all/0/1\">Koustav Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolic_A/0/1/0/all/0/1\">Aljosa Smolic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FingerGAN: A Constrained Fingerprint Generation Scheme for Latent Fingerprint Enhancement. (arXiv:2206.12885v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12885","description":"<p>Latent fingerprint enhancement is an essential pre-processing step for latent\nfingerprint identification. Most latent fingerprint enhancement methods try to\nrestore corrupted gray ridges/valleys. In this paper, we propose a new method\nthat formulates the latent fingerprint enhancement as a constrained fingerprint\ngeneration problem within a generative adversarial network (GAN) framework. We\nname the proposed network as FingerGAN. It can enforce its generated\nfingerprint (i.e, enhanced latent fingerprint) indistinguishable from the\ncorresponding ground-truth instance in terms of the fingerprint skeleton map\nweighted by minutia locations and the orientation field regularized by the\nFOMFE model. Because minutia is the primary feature for fingerprint recognition\nand minutia can be retrieved directly from the fingerprint skeleton map, we\noffer a holistic framework which can perform latent fingerprint enhancement in\nthe context of directly optimizing minutia information. This will help improve\nlatent fingerprint identification performance significantly. Experimental\nresults on two public latent fingerprint databases demonstrate that our method\noutperforms the state of the arts significantly. The codes will be available\nfor non-commercial purposes from\n\\url{https://github.com/HubYZ/LatentEnhancement}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuefei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiankun Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Woodscape Fisheye Object Detection for Autonomous Driving -- CVPR 2022 OmniCV Workshop Challenge. (arXiv:2206.12912v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12912","description":"<p>Object detection is a comprehensively studied problem in autonomous driving.\nHowever, it has been relatively less explored in the case of fisheye cameras.\nThe strong radial distortion breaks the translation invariance inductive bias\nof Convolutional Neural Networks. Thus, we present the WoodScape fisheye object\ndetection challenge for autonomous driving which was held as part of the CVPR\n2022 Workshop on Omnidirectional Computer Vision (OmniCV). This is one of the\nfirst competitions focused on fisheye camera object detection. We encouraged\nthe participants to design models which work natively on fisheye images without\nrectification. We used CodaLab to host the competition based on the publicly\navailable WoodScape fisheye dataset. In this paper, we provide a detailed\nanalysis on the competition which attracted the participation of 120 global\nteams and a total of 1492 submissions. We briefly discuss the details of the\nwinning methods and analyze their qualitative and quantitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_S/0/1/0/all/0/1\">Saravanabalagi Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1\">Ganesh Sistu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1\">John McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Anomaly Detection via Prediction Network with Enhanced Spatio-Temporal Memory Exchange. (arXiv:2206.12914v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12914","description":"<p>Video anomaly detection is a challenging task because most anomalies are\nscarce and non-deterministic. Many approaches investigate the reconstruction\ndifference between normal and abnormal patterns, but neglect that anomalies do\nnot necessarily correspond to large reconstruction errors. To address this\nissue, we design a Convolutional LSTM Auto-Encoder prediction framework with\nenhanced spatio-temporal memory exchange using bi-directionalilty and a\nhigher-order mechanism. The bi-directional structure promotes learning the\ntemporal regularity through forward and backward predictions. The unique\nhigher-order mechanism further strengthens spatial information interaction\nbetween the encoder and the decoder. Considering the limited receptive fields\nin Convolutional LSTMs, we also introduce an attention module to highlight\ninformative features for prediction. Anomalies are eventually identified by\ncomparing the frames with their corresponding predictions. Evaluations on three\npopular benchmarks show that our framework outperforms most existing\nprediction-based anomaly detection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1\">Guodong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Y/0/1/0/all/0/1\">Yuqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Style Transfer. (arXiv:2206.12921v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12921","description":"<p>Recent feed-forward neural methods of arbitrary image style transfer mainly\nutilized encoded feature map upto its second-order statistics, i.e., linearly\ntransformed the encoded feature map of a content image to have the same mean\nand variance (or covariance) of a target style feature map. In this work, we\nextend the second-order statistical feature matching into a general\ndistribution matching based on the understanding that style of an image is\nrepresented by the distribution of responses from receptive fields. For this\ngeneralization, first, we propose a new feature transform layer that exactly\nmatches the feature map distribution of content image into that of target style\nimage. Second, we analyze the recent style losses consistent with our new\nfeature transform layer to train a decoder network which generates a style\ntransferred image from the transformed feature map. Based on our experimental\nresults, it is proven that the stylized images obtained with our method are\nmore similar with the target style images in all existing style measures\nwithout losing content clearness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeong-Sik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyun-Chul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Activity Localisation with Uncertainties in Temporal Boundary. (arXiv:2206.12923v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12923","description":"<p>Current methods for video activity localisation over time assume implicitly\nthat activity temporal boundaries labelled for model training are determined\nand precise. However, in unscripted natural videos, different activities mostly\ntransit smoothly, so that it is intrinsically ambiguous to determine in\nlabelling precisely when an activity starts and ends over time. Such\nuncertainties in temporal labelling are currently ignored in model training,\nresulting in learning mis-matched video-text correlation with poor\ngeneralisation in test. In this work, we solve this problem by introducing\nElastic Moment Bounding (EMB) to accommodate flexible and adaptive activity\ntemporal boundaries towards modelling universally interpretable video-text\ncorrelation with tolerance to underlying temporal uncertainties in pre-fixed\nannotations. Specifically, we construct elastic boundaries adaptively by mining\nand discovering frame-wise temporal endpoints that can maximise the alignment\nbetween video segments and query sentences. To enable both more robust matching\n(segment content attention) and more accurate localisation (segment elastic\nboundaries), we optimise the selection of frame-wise endpoints subject to\nsegment-wise contents by a novel Guided Attention mechanism. Extensive\nexperiments on three video activity localisation benchmarks demonstrate\ncompellingly the EMB's advantages over existing methods without modelling\nuncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiabo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer for Contrastive Clustering. (arXiv:2206.12925v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12925","description":"<p>Vision Transformer (ViT) has shown its advantages over the convolutional\nneural network (CNN) with its ability to capture global long-range dependencies\nfor visual representation learning. Besides ViT, contrastive learning is\nanother popular research topic recently. While previous contrastive learning\nworks are mostly based on CNNs, some latest studies have attempted to jointly\nmodel the ViT and the contrastive learning for enhanced self-supervised\nlearning. Despite the considerable progress, these combinations of ViT and\ncontrastive learning mostly focus on the instance-level contrastiveness, which\noften overlook the contrastiveness of the global clustering structures and also\nlack the ability to directly learn the clustering result (e.g., for images). In\nview of this, this paper presents an end-to-end deep image clustering approach\ntermed Vision Transformer for Contrastive Clustering (VTCC), which for the\nfirst time, to the best of our knowledge, unifies the Transformer and the\ncontrastive learning for the image clustering task. Specifically, with two\nrandom augmentations performed on each image in a mini-batch, we utilize a ViT\nencoder with two weight-sharing views as the backbone to learn the\nrepresentations for the augmented samples. To remedy the potential instability\nof the ViT, we incorporate a convolutional stem, which uses multiple stacked\nsmall convolutions instead of a big convolution in the patch projection layer,\nto split each augmented sample into a sequence of patches. With representations\nlearned via the backbone, an instance projector and a cluster projector are\nfurther utilized for the instance-level contrastive learning and the global\nclustering structure learning, respectively. Extensive experiments on eight\nimage datasets demonstrate the stability (during the training-from-scratch) and\nthe superiority (in clustering performance) of VTCC over the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Hua-Bao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bowen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Ding-Hua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVBR-NET: A Non-Blind Spatially Varying Defocus Blur Removal Network. (arXiv:2206.12930v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12930","description":"<p>Defocus blur is a physical consequence of the optical sensors used in most\ncameras. Although it can be used as a photographic style, it is commonly viewed\nas an image degradation modeled as the convolution of a sharp image with a\nspatially-varying blur kernel. Motivated by the advance of blur estimation\nmethods in the past years, we propose a non-blind approach for image deblurring\nthat can deal with spatially-varying kernels. We introduce two encoder-decoder\nsub-networks that are fed with the blurry image and the estimated blur map,\nrespectively, and produce as output the deblurred (deconvolved) image. Each\nsub-network presents several skip connections that allow data propagation from\nlayers spread apart, and also inter-subnetwork skip connections that ease the\ncommunication between the modules. The network is trained with synthetically\nblur kernels that are augmented to emulate blur maps produced by existing blur\nestimation methods, and our experimental results show that our method works\nwell when combined with a variety of blur estimation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karaali_A/0/1/0/all/0/1\">Ali Karaali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1\">Claudio Rosito Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection and Tracking with Autonomous UAV. (arXiv:2206.12941v1 [cs.RO])","link":"http://arxiv.org/abs/2206.12941","description":"<p>In this paper, a combat Unmanned Air Vehicle (UAV) is modeled in the\nsimulation environment. The rotary wing UAV is successfully performed various\ntasks such as locking on the targets, tracking, and sharing the relevant data\nwith surrounding vehicles. Different software technologies such as API\ncommunication, ground control station configuration, autonomous movement\nalgorithms, computer vision, and deep learning are employed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demir_A/0/1/0/all/0/1\">A. Huzeyfe Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavas_B/0/1/0/all/0/1\">Berke Yavas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazici_M/0/1/0/all/0/1\">Mehmet Yazici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksu_D/0/1/0/all/0/1\">Dogukan Aksu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aydin_M/0/1/0/all/0/1\">M. Ali Aydin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Feature Augmentation with Adaptive Class Activation Mapping. (arXiv:2206.12943v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12943","description":"<p>We propose an end-to-end-trainable feature augmentation module built for\nimage classification that extracts and exploits multi-view local features to\nboost model performance. Different from using global average pooling (GAP) to\nextract vectorized features from only the global view, we propose to sample and\nensemble diverse multi-view local features to improve model robustness. To\nsample class-representative local features, we incorporate a simple auxiliary\nclassifier head (comprising only one 1$\\times$1 convolutional layer) which\nefficiently and adaptively attends to class-discriminative local regions of\nfeature maps via our proposed AdaCAM (Adaptive Class Activation Mapping).\nExtensive experiments demonstrate consistent and noticeable performance gains\nachieved by our multi-view feature augmentation module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingjie Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFT-VO: Asynchronous Fusion Transformers for Multi-View Visual Odometry Estimation. (arXiv:2206.12946v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12946","description":"<p>Motion estimation approaches typically employ sensor fusion techniques, such\nas the Kalman Filter, to handle individual sensor failures. More recently, deep\nlearning-based fusion approaches have been proposed, increasing the performance\nand requiring less model-specific implementations. However, current deep fusion\napproaches often assume that sensors are synchronised, which is not always\npractical, especially for low-cost hardware. To address this limitation, in\nthis work, we propose AFT-VO, a novel transformer-based sensor fusion\narchitecture to estimate VO from multiple sensors. Our framework combines\npredictions from asynchronous multi-view cameras and accounts for the time\ndiscrepancies of measurements coming from different sources.\n</p>\n<p>Our approach first employs a Mixture Density Network (MDN) to estimate the\nprobability distributions of the 6-DoF poses for every camera in the system.\nThen a novel transformer-based fusion module, AFT-VO, is introduced, which\ncombines these asynchronous pose estimations, along with their confidences.\nMore specifically, we introduce Discretiser and Source Encoding techniques\nwhich enable the fusion of multi-source asynchronous signals.\n</p>\n<p>We evaluate our approach on the popular nuScenes and KITTI datasets. Our\nexperiments demonstrate that multi-view fusion for VO estimation provides\nrobust and accurate trajectories, outperforming the state of the art in both\nchallenging weather and lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaygusuz_N/0/1/0/all/0/1\">Nimet Kaygusuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1\">Oscar Mendez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonwatertight Mesh Reconstruction. (arXiv:2206.12952v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12952","description":"<p>Reconstructing 3D non-watertight mesh from an unoriented point cloud is an\nunexplored area in computer vision and computer graphics. In this project, we\ntried to tackle this problem by extending the learning-based watertight mesh\nreconstruction pipeline presented in the paper 'Shape as Points'. The core of\nour approach is to cast the problem as a semantic segmentation problem that\nidentifies the region in the 3D volume where the mesh surface lies and extracts\nthe surfaces from the detected regions. Our approach achieves compelling\nresults compared to the baseline techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Partha Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Szloca: towards a framework for full 3D tracking through a single camera in context of interactive arts. (arXiv:2206.12958v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12958","description":"<p>Realtime virtual data of objects and human presence in a large area holds a\nvaluable key in enabling many experiences and applications in various\nindustries and with exponential rise in the technological development of\nartificial intelligence, computer vision has expanded the possibilities of\ntracking and classifying things through just video inputs, which is also\nsurpassing the limitations of most popular and common hardware setups known\ntraditionally to detect human pose and position, such as low field of view and\nlimited tracking capacity. The benefits of using computer vision in application\ndevelopment is large as it augments traditional input sources (like video\nstreams) and can be integrated in many environments and platforms. In the\ncontext of new media interactive arts, based on physical movements and\nexpanding over large areas or gallaries, this research presents a novel way and\na framework towards obtaining data and virtual representation of objects/people\n- such as three-dimensional positions, skeltons/pose and masks from a single\nrgb camera. Looking at the state of art through some recent developments and\nbuilding on prior research in the field of computer vision, the paper also\nproposes an original method to obtain three dimensional position data from\nmonocular images, the model does not rely on complex training of computer\nvision systems but combines prior computer vision research and adds a capacity\nto represent z depth, ieto represent a world position in 3 axis from a 2d input\nsource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sahaj Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic PolarGMM: Unsupervised Cluster Learning of Very Noisy Projection Images of Unknown Pose. (arXiv:2206.12959v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12959","description":"<p>A crucial step in single particle analysis (SPA) of cryogenic electron\nmicroscopy (Cryo-EM), 2D classification and alignment takes a collection of\nnoisy particle images to infer orientations and group similar images together.\nAveraging these aligned and clustered noisy images produces a set of clean\nimages, ready for further analysis such as 3D reconstruction. Fourier-Bessel\nsteerable principal component analysis (FBsPCA) enables an efficient,\nadaptable, low-rank rotation operator. We extend the FBsPCA to additionally\nhandle translations. In this extended FBsPCA representation, we use a\nprobabilistic polar-coordinate Gaussian mixture model to learn soft clusters in\nan unsupervised fashion using an expectation maximization (EM) algorithm. The\nobtained rotational clusters are thus additionally robust to the presence of\npairwise alignment imperfections. Multiple benchmarks from simulated Cryo-EM\ndatasets show probabilistic PolarGMM's improved performance in comparisons with\nstandard single-particle Cryo-EM tools, EMAN2 and RELION, in terms of various\nclustering metrics and alignment errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chockchowwat_S/0/1/0/all/0/1\">Supawit Chockchowwat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit L. Bajaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Healing Robust Neural Networks via Closed-Loop Control. (arXiv:2206.12963v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12963","description":"<p>Despite the wide applications of neural networks, there have been increasing\nconcerns about their vulnerability issue. While numerous attack and defense\ntechniques have been developed, this work investigates the robustness issue\nfrom a new angle: can we design a self-healing neural network that can\nautomatically detect and fix the vulnerability issue by itself? A typical\nself-healing mechanism is the immune system of a human body. This\nbiology-inspired idea has been used in many engineering designs but is rarely\ninvestigated in deep learning. This paper considers the post-training\nself-healing of a neural network, and proposes a closed-loop control\nformulation to automatically detect and fix the errors caused by various\nattacks or perturbations. We provide a margin-based analysis to explain how\nthis formulation can improve the robustness of a classifier. To speed up the\ninference of the proposed self-healing network, we solve the control problem\nvia improving the Pontryagin Maximum Principle-based solver. Lastly, we present\nan error estimation of the proposed framework for neural networks with\nnonlinear activation functions. We validate the performance on several network\narchitectures against various perturbations. Since the self-healing method does\nnot need a-priori information about data perturbations/attacks, it can handle a\nbroad class of unforeseen perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuotong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qianxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLCap: Vision-Language with Contrastive Learning for Coherent Video Paragraph Captioning. (arXiv:2206.12972v1 [cs.CV])","link":"http://arxiv.org/abs/2206.12972","description":"<p>In this paper, we leverage the human perceiving process, that involves vision\nand language interaction, to generate a coherent paragraph description of\nuntrimmed videos. We propose vision-language (VL) features consisting of two\nmodalities, i.e., (i) vision modality to capture global visual content of the\nentire scene and (ii) language modality to extract scene elements description\nof both human and non-human objects (e.g. animals, vehicles, etc), visual and\nnon-visual elements (e.g. relations, activities, etc). Furthermore, we propose\nto train our proposed VLCap under a contrastive learning VL loss. The\nexperiments and ablation studies on ActivityNet Captions and YouCookII datasets\nshow that our VLCap outperforms existing SOTA methods on both accuracy and\ndiversity metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Sang Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1\">Khoa Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kidd_M/0/1/0/all/0/1\">Michael Kidd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainwater_C/0/1/0/all/0/1\">Chase Rainwater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Schizophrenia with 3D Structural Brain MRI Using Deep Learning. (arXiv:2206.12980v1 [eess.IV])","link":"http://arxiv.org/abs/2206.12980","description":"<p>Schizophrenia is a chronic neuropsychiatric disorder that causes distinct\nstructural alterations within the brain. We hypothesize that deep learning\napplied to a structural neuroimaging dataset could detect disease-related\nalteration and improve classification and diagnostic accuracy. We tested this\nhypothesis using a single, widely available, and conventional T1-weighted MRI\nscan, from which we extracted the 3D whole-brain structure using standard\npost-processing methods. A deep learning model was then developed, optimized,\nand evaluated on three open datasets with T1-weighted MRI scans of patients\nwith schizophrenia. Our proposed model outperformed the benchmark model, which\nwas also trained with structural MR images using a 3D CNN architecture. Our\nmodel is capable of almost perfectly (area under the ROC curve = 0.987)\ndistinguishing schizophrenia patients from healthy controls on unseen\nstructural MRI scans. Regional analysis localized subcortical regions and\nventricles as the most predictive brain regions. Subcortical structures serve a\npivotal role in cognitive, affective, and social functions in humans, and\nstructural abnormalities of these regions have been associated with\nschizophrenia. Our finding corroborates that schizophrenia is associated with\nwidespread alterations in subcortical brain structure and the subcortical\nstructural information provides prominent features in diagnostic\nclassification. Together, these results further demonstrate the potential of\ndeep learning to improve schizophrenia diagnosis and identify its structural\nneuroimaging signatures from a single, standard T1-weighted brain MRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Junhao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rao_V/0/1/0/all/0/1\">Vishwanatha M. Rao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1\">Ye Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yanting Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acosta_N/0/1/0/all/0/1\">Nicolas Acosta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_Z/0/1/0/all/0/1\">Zihan Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_P/0/1/0/all/0/1\">Pin-Yu Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chloe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kegeles_L/0/1/0/all/0/1\">Lawrence S. Kegeles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Small_S/0/1/0/all/0/1\">Scott A. Small</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-point dimensionality reduction to improve projection layout reliability. (arXiv:2101.06224v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06224","description":"<p>In ordinary Dimensionality Reduction (DR), each data instance in a high\ndimensional space (original space), or on a distance matrix denoting original\nspace distances, is mapped to (projected onto) one point in a low dimensional\nspace (visual space), building a layout of projected points trying to preserve\nas much as possible some property of data such as distances, neighbourhood\nrelationships, and/or topology structures, with the ultimate goal of\napproximating semantic properties of data with preserved geometric properties\nor topology structures in visual space. In this paper, the concept of\nMulti-point Dimensionality Reduction is elaborated on where each data instance\ncan be mapped to (projected onto) possibly more than one point in visual space\nby providing the first general solution (algorithm) for it as a move in the\ndirection of improving reliablity, usability and interpretability of\ndimensionality reduction. Furthermore by allowing the points in visual space to\nbe split into two layers while maintaining the possibility of having more than\none projection (mapping) per data instance , the benefit of separating more\nreliable points from less reliable points is dicussed notwithstanding the\neffort to improve less reliable points. The proposed solution (algorithm) in\nthis paper, named Layered Vertex Splitting Data Embedding (LVSDE), is built\nupon and extends a combination of ordinary DR and graph drawing techniques.\nBased on the experiments of this paper on some data sets, the particular\nproposed algorithm (LVSDE) practically outperforms popular ordinary DR methods\nvisually (semantics, group separation, subgroup detection or combinational\ngroup detection) in a way that is easily explainable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barahimi_F/0/1/0/all/0/1\">Farshad Barahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full Page Handwriting Recognition via Image to Sequence Extraction. (arXiv:2103.06450v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.06450","description":"<p>We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sumeet S. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karayev_S/0/1/0/all/0/1\">Sergey Karayev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Metric Learning for Few-Shot Image Classification: A Review of Recent Developments. (arXiv:2105.08149v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08149","description":"<p>Few-shot image classification is a challenging problem that aims to achieve\nthe human level of recognition based only on a small number of training images.\nOne main solution to few-shot image classification is deep metric learning.\nThese methods, by classifying unseen samples according to their distances to\nfew seen samples in an embedding space learned by powerful deep neural\nnetworks, can avoid overfitting to few training images in few-shot image\nclassification and have achieved the state-of-the-art performance. In this\npaper, we provide an up-to-date review of deep metric learning methods for\nfew-shot image classification from 2018 to 2022 and categorize them into three\ngroups according to three stages of metric learning, namely learning feature\nembeddings, learning class representations, and learning distance measures.\nWith this taxonomy, we identify the novelties of different methods and problems\nthey face. We conclude this review with a discussion on current challenges and\nfuture trends in few-shot image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaochen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.01100","description":"<p>During lung radiotherapy, the position of infrared reflective objects on the\nchest can be recorded to estimate the tumor location. However, radiotherapy\nsystems have a latency inherent to robot control limitations that impedes the\nradiation delivery precision. Prediction with online learning of recurrent\nneural networks (RNN) allows for adaptation to non-stationary respiratory\nsignals, but classical methods such as RTRL and truncated BPTT are respectively\nslow and biased. This study investigates the capabilities of unbiased online\nrecurrent optimization (UORO) to forecast respiratory motion and enhance safety\nin lung radiotherapy.\n</p>\n<p>We used 9 observation records of the 3D position of 3 external markers on the\nchest and abdomen of healthy individuals breathing during intervals from 73s to\n222s. The sampling frequency was 10Hz, and the amplitudes of the recorded\ntrajectories range from 6mm to 40mm in the superior-inferior direction. We\nforecast the 3D location of each marker simultaneously with a horizon value\nbetween 0.1s and 2.0s, using an RNN trained with UORO. We compare its\nperformance with an RNN trained with RTRL, LMS, and offline linear regression.\nWe provide closed-form expressions for quantities involved in the loss gradient\ncalculation in UORO, thereby making its implementation efficient. Training and\ncross-validation were performed during the first minute of each sequence.\n</p>\n<p>On average over the horizon values considered and the 9 sequences, UORO\nachieves the lowest root-mean-square (RMS) error and maximum error among the\ncompared algorithms. These errors are respectively equal to 1.3mm and 8.8mm,\nand the prediction time per time step was lower than 2.8ms (Dell Intel core\ni9-9900K 3.60 GHz). Linear regression has the lowest RMS error for the horizon\nvalues 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,\nand UORO for horizon values greater than 0.6s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pohl_M/0/1/0/all/0/1\">Michel Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uesaka_M/0/1/0/all/0/1\">Mitsuru Uesaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takahashi_H/0/1/0/all/0/1\">Hiroyuki Takahashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demachi_K/0/1/0/all/0/1\">Kazuyuki Demachi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chhatkuli_R/0/1/0/all/0/1\">Ritu Bhusal Chhatkuli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Feature Aggregation by Cross-Scale Pixel-to-Region Relation Operation for Semantic Segmentation. (arXiv:2106.01744v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01744","description":"<p>Exploiting multi-scale features has shown great potential in tackling\nsemantic segmentation problems. The aggregation is commonly done with sum or\nconcatenation (concat) followed by convolutional (conv) layers. However, it\nfully passes down the high-level context to the following hierarchy without\nconsidering their interrelation. In this work, we aim to enable the low-level\nfeature to aggregate the complementary context from adjacent high-level feature\nmaps by a cross-scale pixel-to-region relation operation. We leverage\ncross-scale context propagation to make the long-range dependency capturable\neven by the high-resolution low-level features. To this end, we employ an\nefficient feature pyramid network to obtain multi-scale features. We propose a\nRelational Semantics Extractor (RSE) and Relational Semantics Propagator (RSP)\nfor context extraction and propagation respectively. Then we stack several RSP\ninto an RSP head to achieve the progressive top-down distribution of the\ncontext. Experiment results on two challenging datasets Cityscapes and COCO\ndemonstrate that the RSP head performs competitively on both semantic\nsegmentation and panoptic segmentation with high efficiency. It outperforms\nDeeplabV3 [1] by 0.7% with 75% fewer FLOPs (multiply-adds) in the semantic\nsegmentation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yechao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lyuyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06959","description":"<p>The discovery of the disentanglement properties of the latent space in GANs\nmotivated a lot of research to find the semantically meaningful directions on\nit. In this paper, we suggest that the disentanglement property is closely\nrelated to the geometry of the latent space. In this regard, we propose an\nunsupervised method for finding the semantic-factorizing directions on the\nintermediate latent space of GANs based on the local geometry. Intuitively, our\nproposed method, called Local Basis, finds the principal variation of the\nlatent space in the neighborhood of the base latent variable. Experimental\nresults show that the local principal variation corresponds to the semantic\nfactorization and traversing along it provides strong robustness to image\ntraversal. Moreover, we suggest an explanation for the limited success in\nfinding the global traversal directions in the latent space, especially W-space\nof StyleGAN2. We show that W-space is warped globally by comparing the local\ngeometry, discovered from Local Basis, through the metric on Grassmannian\nManifold. The global warpage implies that the latent space is not well-aligned\nglobally and therefore the global traversal directions are bound to show\nlimited success on it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaewoong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1\">Changyeon Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jung Ho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Geonho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07847","description":"<p>While unbiased machine learning models are essential for many applications,\nbias is a human-defined concept that can vary across tasks. Given only\ninput-label pairs, algorithms may lack sufficient information to distinguish\nstable (causal) features from unstable (spurious) features. However, related\ntasks often share similar biases -- an observation we may leverage to develop\nstable classifiers in the transfer setting. In this work, we explicitly inform\nthe target classifier about unstable features in the source tasks.\nSpecifically, we derive a representation that encodes the unstable features by\ncontrasting different data environments in the source task. We achieve\nrobustness by clustering data of the target task according to this\nrepresentation and minimizing the worst-case risk across these clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask for both synthetically generated environments and real-world environments.\nOur code is available at https://github.com/YujiaBao/Tofu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks. (arXiv:2107.07933v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07933","description":"<p>Unprecedented access to multi-temporal satellite imagery has opened new\nperspectives for a variety of Earth observation tasks. Among them,\npixel-precise panoptic segmentation of agricultural parcels has major economic\nand environmental implications. While researchers have explored this problem\nfor single images, we argue that the complex temporal patterns of crop\nphenology are better addressed with temporal sequences of images. In this\npaper, we present the first end-to-end, single-stage method for panoptic\nsegmentation of Satellite Image Time Series (SITS). This module can be combined\nwith our novel image sequence encoding network which relies on temporal\nself-attention to extract rich and adaptive multi-scale spatio-temporal\nfeatures. We also introduce PASTIS, the first open-access SITS dataset with\npanoptic annotations. We demonstrate the superiority of our encoder for\nsemantic segmentation against multiple competing architectures, and set up the\nfirst state-of-the-art of panoptic segmentation of SITS. Our implementation and\nPASTIS are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garnot_V/0/1/0/all/0/1\">Vivien Sainte Fare Garnot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeoCLR: Georeference Contrastive Learning for Efficient Seafloor Image Interpretation. (arXiv:2108.06421v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06421","description":"<p>This paper describes Georeference Contrastive Learning of visual\nRepresentation (GeoCLR) for efficient training of deep-learning Convolutional\nNeural Networks (CNNs). The method leverages georeference information by\ngenerating a similar image pair using images taken of nearby locations, and\ncontrasting these with an image pair that is far apart. The underlying\nassumption is that images gathered within a close distance are more likely to\nhave similar visual appearance, where this can be reasonably satisfied in\nseafloor robotic imaging applications where image footprints are limited to\nedge lengths of a few metres and are taken so that they overlap along a\nvehicle's trajectory, whereas seafloor substrates and habitats have patch sizes\nthat are far larger. A key advantage of this method is that it is\nself-supervised and does not require any human input for CNN training. The\nmethod is computationally efficient, where results can be generated between\ndives during multi-day AUV missions using computational resources that would be\naccessible during most oceanic field trials. We apply GeoCLR to habitat\nclassification on a dataset that consists of ~86k images gathered using an\nAutonomous Underwater Vehicle (AUV). We demonstrate how the latent\nrepresentations generated by GeoCLR can be used to efficiently guide human\nannotation efforts, where the semi-supervised framework improves classification\naccuracy by an average of 10.2% compared to the state-of-the-art SimCLR using\nthe same CNN and equivalent number of human annotations for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_T/0/1/0/all/0/1\">Takaki Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prugel_Bennett_A/0/1/0/all/0/1\">Adam Pr&#xfc;gel-Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_S/0/1/0/all/0/1\">Stefan B. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizarro_O/0/1/0/all/0/1\">Oscar Pizarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thornton_B/0/1/0/all/0/1\">Blair Thornton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNetFormer: A UNet-like Transformer for Efficient Semantic Segmentation of Remote Sensing Urban Scene Imagery. (arXiv:2109.08937v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08937","description":"<p>Semantic segmentation of remotely sensed urban scene images is required in a\nwide range of practical applications, such as land cover mapping, urban change\ndetection, environmental protection, and economic assessment.Driven by rapid\ndevelopments in deep learning technologies, the convolutional neural network\n(CNN) has dominated semantic segmentation for many years. CNN adopts\nhierarchical feature representation, demonstrating strong capabilities for\nlocal information extraction. However, the local property of the convolution\nlayer limits the network from capturing the global context. Recently, as a hot\ntopic in the domain of computer vision, Transformer has demonstrated its great\npotential in global information modelling, boosting many vision-related tasks\nsuch as image classification, object detection, and particularly semantic\nsegmentation. In this paper, we propose a Transformer-based decoder and\nconstruct a UNet-like Transformer (UNetFormer) for real-time urban scene\nsegmentation. For efficient segmentation, the UNetFormer selects the\nlightweight ResNet18 as the encoder and develops an efficient global-local\nattention mechanism to model both global and local information in the decoder.\nExtensive experiments reveal that our method not only runs faster but also\nproduces higher accuracy compared with state-of-the-art lightweight models.\nSpecifically, the proposed UNetFormer achieved 67.8% and 52.4% mIoU on the\nUAVid and LoveDA datasets, respectively, while the inference speed can achieve\nup to 322.4 FPS with a 512x512 input on a single NVIDIA GTX 3090 GPU. In\nfurther exploration, the proposed Transformer-based decoder combined with a\nSwin Transformer encoder also achieves the state-of-the-art result (91.3% F1\nand 84.1% mIoU) on the Vaihingen dataset. The source code will be freely\navailable at https://github.com/WangLibo1995/GeoSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Libo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shenghui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaoliang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1\">Peter M. Atkinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural network relief: a pruning algorithm based on neural activity. (arXiv:2109.10795v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.10795","description":"<p>Current deep neural networks (DNNs) are overparameterized and use most of\ntheir neuronal connections during inference for each task. The human brain,\nhowever, developed specialized regions for different tasks and performs\ninference with a small fraction of its neuronal connections. We propose an\niterative pruning strategy introducing a simple importance-score metric that\ndeactivates unimportant connections, tackling overparameterization in DNNs and\nmodulating the firing patterns. The aim is to find the smallest number of\nconnections that is still capable of solving a given task with comparable\naccuracy, i.e. a simpler subnetwork. We achieve comparable performance for\nLeNet architectures on MNIST, and significantly higher parameter compression\nthan state-of-the-art algorithms for VGG and ResNet architectures on\nCIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two\ndifferent optimizers considered -- Adam and SGD. The algorithm is not designed\nto minimize FLOPs when considering current hardware and software\nimplementations, although it performs reasonably when compared to the state of\nthe art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dekhovich_A/0/1/0/all/0/1\">Aleksandr Dekhovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tax_D/0/1/0/all/0/1\">David M.J. Tax</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sluiter_M/0/1/0/all/0/1\">Marcel H.F. Sluiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessa_M/0/1/0/all/0/1\">Miguel A. Bessa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRACER: Extreme Attention Guided Salient Object Tracing Network. (arXiv:2112.07380v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07380","description":"<p>Existing studies on salient object detection (SOD) focus on extracting\ndistinct objects with edge information and aggregating multi-level features to\nimprove SOD performance. To achieve satisfactory performance, the methods\nemploy refined edge information and low multi-level discrepancy. However, both\nperformance gain and computational efficiency cannot be attained, which has\nmotivated us to study the inefficiencies in existing encoder-decoder structures\nto avoid this trade-off. We propose TRACER, which detects salient objects with\nexplicit edges by incorporating attention guided tracing modules. We employ a\nmasked edge attention module at the end of the first encoder using a fast\nFourier transform to propagate the refined edge information to the downstream\nfeature extraction. In the multi-level aggregation phase, the union attention\nmodule identifies the complementary channel and important spatial information.\nTo improve the decoder performance and computational efficiency, we minimize\nthe decoder block usage with object attention module. This module extracts\nundetected objects and edge information from refined channels and spatial\nrepresentations. Subsequently, we propose an adaptive pixel intensity loss\nfunction to deal with the relatively important pixels unlike conventional loss\nfunctions which treat all pixels equally. A comparison with 13 existing methods\nreveals that TRACER achieves state-of-the-art performance on five benchmark\ndatasets. We have released TRACER at https://github.com/Karel911/TRACER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Min Seok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Wooseok Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sung Won Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing radiologists' gaze and saliency maps generated by interpretability methods for chest x-rays. (arXiv:2112.11716v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11716","description":"<p>The interpretability of medical image analysis models is considered a key\nresearch field. We use a dataset of eye-tracking data from five radiologists to\ncompare the outputs of interpretability methods and the heatmaps representing\nwhere radiologists looked. We conduct a class-independent analysis of the\nsaliency maps generated by two methods selected from the literature: Grad-CAM\nand attention maps from an attention-gated model. For the comparison, we use\nshuffled metrics, which avoid biases from fixation locations. We achieve scores\ncomparable to an interobserver baseline in one shuffled metric, highlighting\nthe potential of saliency maps from Grad-CAM to mimic a radiologist's attention\nover an image. We also divide the dataset into subsets to evaluate in which\ncases similarities are higher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lanfredi_R/0/1/0/all/0/1\">Ricardo Bigolin Lanfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Ambuj Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drew_T/0/1/0/all/0/1\">Trafton Drew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervision and Multi-Task Learning: Challenges in Fine-Grained COVID-19 Multi-Class Classification from Chest X-rays. (arXiv:2201.06052v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.06052","description":"<p>Quick and accurate diagnosis is of paramount importance to mitigate the\neffects of COVID-19 infection, particularly for severe cases. Enormous effort\nhas been put towards developing deep learning methods to classify and detect\nCOVID-19 infections from chest radiography images. However, recently some\nquestions have been raised surrounding the clinical viability and effectiveness\nof such methods. In this work, we investigate the impact of multi-task learning\n(classification and segmentation) on the ability of CNNs to differentiate\nbetween various appearances of COVID-19 infections in the lung. We also employ\nself-supervised pre-training approaches, namely MoCo and inpainting-CXR, to\neliminate the dependence on expensive ground truth annotations for COVID-19\nclassification. Finally, we conduct a critical evaluation of the models to\nassess their deploy-readiness and provide insights into the difficulties of\nfine-grained COVID-19 multi-class classification from chest X-rays.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ridzuan_M/0/1/0/all/0/1\">Muhammad Ridzuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bawazir_A/0/1/0/all/0/1\">Ameera Ali Bawazir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navarette_I/0/1/0/all/0/1\">Ivo Gollini Navarette</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Almakky_I/0/1/0/all/0/1\">Ibrahim Almakky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs. (arXiv:2201.06374v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06374","description":"<p>Blind face restoration is to recover a high-quality face image from unknown\ndegradations. As face image contains abundant contextual information, we\npropose a method, RestoreFormer, which explores fully-spatial attentions to\nmodel contextual information and surpasses existing works that use local\noperators. RestoreFormer has several benefits compared to prior arts. First,\nunlike the conventional multi-head self-attention in previous Vision\nTransformers (ViTs), RestoreFormer incorporates a multi-head cross-attention\nlayer to learn fully-spatial interactions between corrupted queries and\nhigh-quality key-value pairs. Second, the key-value pairs in ResotreFormer are\nsampled from a reconstruction-oriented high-quality dictionary, whose elements\nare rich in high-quality facial features specifically aimed for face\nreconstruction, leading to superior restoration results. Third, RestoreFormer\noutperforms advanced state-of-the-art methods on one synthetic dataset and\nthree real-world datasets, as well as produces images with better visual\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhouxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swin-Pose: Swin Transformer Based Human Pose Estimation. (arXiv:2201.07384v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07384","description":"<p>Convolutional neural networks (CNNs) have been widely utilized in many\ncomputer vision tasks. However, CNNs have a fixed reception field and lack the\nability of long-range perception, which is crucial to human pose estimation.\nDue to its capability to capture long-range dependencies between pixels,\ntransformer architecture has been adopted to computer vision applications\nrecently and is proven to be a highly effective architecture. We are interested\nin exploring its capability in human pose estimation, and thus propose a novel\nmodel based on transformer architecture, enhanced with a feature pyramid fusion\nstructure. More specifically, we use pre-trained Swin Transformer as our\nbackbone and extract features from input images, we leverage a feature pyramid\nstructure to extract feature maps from different stages. By fusing the features\ntogether, our model predicts the keypoint heatmap. The experiment results of\nour study have demonstrated that the proposed transformer-based model can\nachieve better performance compared to the state-of-the-art CNN-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zinan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Ying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yu Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RePaint: Inpainting using Denoising Diffusion Probabilistic Models. (arXiv:2201.09865v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09865","description":"<p>Free-form inpainting is the task of adding new content to an image in the\nregions specified by an arbitrary binary mask. Most existing approaches train\nfor a certain distribution of masks, which limits their generalization\ncapabilities to unseen mask types. Furthermore, training with pixel-wise and\nperceptual losses often leads to simple textural extensions towards the missing\nareas instead of semantically meaningful generation. In this work, we propose\nRePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting\napproach that is applicable to even extreme masks. We employ a pretrained\nunconditional DDPM as the generative prior. To condition the generation\nprocess, we only alter the reverse diffusion iterations by sampling the\nunmasked regions using the given image information. Since this technique does\nnot modify or condition the original DDPM network itself, the model produces\nhigh-quality and diverse output images for any inpainting form. We validate our\nmethod for both faces and general-purpose image inpainting using standard and\nextreme masks.\n</p>\n<p>RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for\nat least five out of six mask distributions.\n</p>\n<p>Github Repository: git.io/RePaint\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugmayr_A/0/1/0/all/0/1\">Andreas Lugmayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_A/0/1/0/all/0/1\">Andres Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10110","description":"<p>Fitting geometric models onto outlier contaminated data is provably\nintractable. Many computer vision systems rely on random sampling heuristics to\nsolve robust fitting, which do not provide optimality guarantees and error\nbounds. It is therefore critical to develop novel approaches that can bridge\nthe gap between exact solutions that are costly, and fast heuristics that offer\nno quality assurances. In this paper, we propose a hybrid quantum-classical\nalgorithm for robust fitting. Our core contribution is a novel robust fitting\nformulation that solves a sequence of integer programs and terminates with a\nglobal solution or an error bound. The combinatorial subproblems are amenable\nto a quantum annealer, which helps to tighten the bound efficiently. While our\nusage of quantum computing does not surmount the fundamental intractability of\nrobust fitting, by providing error bounds our algorithm is a practical\nimprovement over randomised heuristics. Moreover, our work represents a\nconcrete application of quantum computing in computer vision. We present\nresults obtained using an actual quantum computer (D-Wave Advantage) and via\nsimulation. Source code: https://github.com/dadung/HQC-robust-fitting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1\">Anh-Dzung Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SURDS: Self-Supervised Attention-guided Reconstruction and Dual Triplet Loss for Writer Independent Offline Signature Verification. (arXiv:2201.10138v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10138","description":"<p>Offline Signature Verification (OSV) is a fundamental biometric task across\nvarious forensic, commercial and legal applications. The underlying task at\nhand is to carefully model fine-grained features of the signatures to\ndistinguish between genuine and forged ones, which differ only in minute\ndeformities. This makes OSV more challenging compared to other verification\nproblems. In this work, we propose a two-stage deep learning framework that\nleverages self-supervised representation learning as well as metric learning\nfor writer-independent OSV. First, we train an image reconstruction network\nusing an encoder-decoder architecture that is augmented by a 2D spatial\nattention mechanism using signature image patches. Next, the trained encoder\nbackbone is fine-tuned with a projector head using a supervised metric learning\nframework, whose objective is to optimize a novel dual triplet loss by sampling\nnegative samples from both within the same writer class as well as from other\nwriters. The intuition behind this is to ensure that a signature sample lies\ncloser to its positive counterpart compared to negative samples from both\nintra-writer and cross-writer sets. This results in robust discriminative\nlearning of the embedding space. To the best of our knowledge, this is the\nfirst work of using self-supervised learning frameworks for OSV. The proposed\ntwo-stage framework has been evaluated on two publicly available offline\nsignature datasets and compared with various state-of-the-art methods. It is\nnoted that the proposed method provided promising results outperforming several\nexisting pieces of work. The code is publicly available at:\nhttps://github.com/soumitri2001/SURDS-SSL-OSV\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Soumitri Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manna_S/0/1/0/all/0/1\">Siladittya Manna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RNGDet: Road Network Graph Detection by Transformer in Aerial Images. (arXiv:2202.07824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07824","description":"<p>Road network graphs provide critical information for autonomous-vehicle\napplications, such as drivable areas that can be used for motion planning\nalgorithms. To find road network graphs, manually annotation is usually\ninefficient and labor-intensive. Automatically detecting road network graphs\ncould alleviate this issue, but existing works still have some limitations. For\nexample, segmentation-based approaches could not ensure satisfactory topology\ncorrectness, and graph-based approaches could not present precise enough\ndetection results. To provide a solution to these problems, we propose a novel\napproach based on transformer and imitation learning in this paper. In view of\nthat high-resolution aerial images could be easily accessed all over the world\nnowadays, we make use of aerial images in our approach. Taken as input an\naerial image, our approach iteratively generates road network graphs\nvertex-by-vertex. Our approach can handle complicated intersection points with\nvarious numbers of incident road segments. We evaluate our approach on a\npublicly available dataset. The superiority of our approach is demonstrated\nthrough the comparative experiments. Our work is accompanied with a\ndemonstration video which is available at\n\\url{https://tonyxuqaq.github.io/projects/RNGDet/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Lu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lujia Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-scalable Transformer for Medical Image Segmentation: Architecture, Model Efficiency, and Benchmark. (arXiv:2203.00131v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.00131","description":"<p>Transformer, as a new generation of neural architecture, has demonstrated\nremarkable performance in natural language processing and computer vision.\nHowever, existing vision Transformers struggle to learn with limited medical\ndata and are unable to generalize on diverse medical image tasks. To tackle\nthese challenges, we present MedFormer as a data-scalable Transformer towards\ngeneralizable medical image segmentation. The key designs incorporate desirable\ninductive bias, hierarchical modeling with linear-complexity attention, and\nmulti-scale feature fusion in a spatially and semantically global manner.\nMedFormer can learn across tiny- to large-scale data without pre-training.\nExtensive experiments demonstrate the potential of MedFormer as a general\nsegmentation backbone, outperforming CNNs and vision Transformers on three\npublic datasets with multiple modalities (e.g., CT and MRI) and diverse medical\ntargets (e.g., healthy organ, diseased tissue, and tumor). We make the models\nand evaluation pipeline publicly available, offering solid baselines and\nunbiased comparisons for promoting a wide range of downstream clinical\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zhennan Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods. (arXiv:2203.02486v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02486","description":"<p>In many object recognition applications, the set of possible categories is an\nopen set, and the deployed recognition system will encounter novel objects\nbelonging to categories unseen during training. Detecting such ``novel\ncategory'' objects is usually formulated as an anomaly detection problem.\nAnomaly detection algorithms for feature-vector data identify anomalies as\noutliers, but outlier detection has not worked well in deep learning. Instead,\nmethods based on the computed logits of visual object classifiers give\nstate-of-the-art performance. This paper proposes the Familiarity Hypothesis\nthat these methods succeed because they are detecting the absence of familiar\nlearned features rather than the presence of novelty. This distinction is\nimportant, because familiarity-based detection will fail in many situations\nwhere novelty is present. For example when an image contains both a novel\nobject and a familiar one, the familiarity score will be high, so the novel\nobject will not be noticed. The paper reviews evidence from the literature and\npresents additional evidence from our own experiments that provide strong\nsupport for this hypothesis. The paper concludes with a discussion of whether\nfamiliarity-based detection is an inevitable consequence of representation\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dietterich_T/0/1/0/all/0/1\">Thomas G. Dietterich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyer_A/0/1/0/all/0/1\">Alexander Guyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stepwise Feature Fusion: Local Guides Global. (arXiv:2203.03635v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03635","description":"<p>Colonoscopy, currently the most efficient and recognized colon polyp\ndetection technology, is necessary for early screening and prevention of\ncolorectal cancer. However, due to the varying size and complex morphological\nfeatures of colonic polyps as well as the indistinct boundary between polyps\nand mucosa, accurate segmentation of polyps is still challenging. Deep learning\nhas become popular for accurate polyp segmentation tasks with excellent\nresults. However, due to the structure of polyps image and the varying shapes\nof polyps, it easy for existing deep learning models to overfitting the current\ndataset. As a result, the model may not process unseen colonoscopy data. To\naddress this, we propose a new State-Of-The-Art model for medical image\nsegmentation, the SSFormer, which uses a pyramid Transformer encoder to improve\nthe generalization ability of models. Specifically, our proposed Progressive\nLocality Decoder can be adapted to the pyramid Transformer backbone to\nemphasize local features and restrict attention dispersion. The SSFormer\nachieves statet-of-the-art performance in both learning and generalization\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinfeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1\">Qiming Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_F/0/1/0/all/0/1\">Feilong Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_J/0/1/0/all/0/1\">Jia Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving. (arXiv:2203.05056v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05056","description":"<p>Surround-view cameras are a primary sensor for automated driving, used for\nnear-field perception. It is one of the most commonly used sensors in\ncommercial vehicles primarily used for parking visualization and automated\nparking. Four fisheye cameras with a 190{\\deg} field of view cover the\n360{\\deg} around the vehicle. Due to its high radial distortion, the standard\nalgorithms do not extend easily. Previously, we released the first public\nfisheye surround-view dataset named WoodScape. In this work, we release a\nsynthetic version of the surround-view dataset, covering many of its weaknesses\nand extending it. Firstly, it is not possible to obtain ground truth for\npixel-wise optical flow and depth. Secondly, WoodScape did not have all four\ncameras annotated simultaneously in order to sample diverse frames. However,\nthis means that multi-camera algorithms cannot be designed to obtain a unified\noutput in birds-eye space, which is enabled in the new dataset. We implemented\nsurround-view fisheye geometric projections in CARLA Simulator matching\nWoodScape's configuration and created SynWoodScape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekkat_A/0/1/0/all/0/1\">Ahmed Rida Sekkat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuis_Y/0/1/0/all/0/1\">Yohan Dupuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasseur_P/0/1/0/all/0/1\">Pascal Vasseur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honeine_P/0/1/0/all/0/1\">Paul Honeine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07662","description":"<p>In object detection, false negatives arise when a detector fails to detect a\ntarget object. To understand why object detectors produce false negatives, we\nidentify five 'false negative mechanisms', where each mechanism describes how a\nspecific component inside the detector architecture failed. Focusing on\ntwo-stage and one-stage anchor-box object detector architectures, we introduce\na framework for quantifying these false negative mechanisms. Using this\nframework, we investigate why Faster R-CNN and RetinaNet fail to detect objects\nin benchmark vision datasets and robotics datasets. We show that a detector's\nfalse negative mechanisms differ significantly between computer vision\nbenchmark datasets and robotics deployment scenarios. This has implications for\nthe translation of object detectors developed for benchmark datasets to\nrobotics applications. Code is publicly available at\nhttps://github.com/csiro-robotics/fn_mechanisms\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miller_D/0/1/0/all/0/1\">Dimity Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1\">Peyman Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_M/0/1/0/all/0/1\">Mark Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wildie_M/0/1/0/all/0/1\">Matt Wildie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurdak_R/0/1/0/all/0/1\">Raja Jurdak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-Driven Deep Learning for Computational Magnetic Resonance Imaging. (arXiv:2203.12215v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.12215","description":"<p>Physics-driven deep learning methods have emerged as a powerful tool for\ncomputational magnetic resonance imaging (MRI) problems, pushing reconstruction\nperformance to new limits. This article provides an overview of the recent\ndevelopments in incorporating physics information into learning-based MRI\nreconstruction. We consider inverse problems with both linear and non-linear\nforward models for computational MRI, and review the classical approaches for\nsolving these. We then focus on physics-driven deep learning approaches,\ncovering physics-driven loss functions, plug-and-play methods, generative\nmodels, and unrolled networks. We highlight domain-specific challenges such as\nreal- and complex-valued building blocks of neural networks, and translational\napplications in MRI with linear and non-linear forward models. Finally, we\ndiscuss common issues and open challenges, and draw connections to the\nimportance of physics-driven learning when combined with other downstream tasks\nin the medical imaging pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kustner_T/0/1/0/all/0/1\">Thomas K&#xfc;stner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengnan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knoll_F/0/1/0/all/0/1\">Florian Knoll</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akcakaya_M/0/1/0/all/0/1\">Mehmet Ak&#xe7;akaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAT: Mask-Aware Transformer for Large Hole Image Inpainting. (arXiv:2203.15270v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15270","description":"<p>Recent studies have shown the importance of modeling long-range interactions\nin the inpainting problem. To achieve this goal, existing approaches exploit\neither standalone attention techniques or transformers, but usually under a low\nresolution in consideration of computational cost. In this paper, we present a\nnovel transformer-based model for large hole inpainting, which unifies the\nmerits of transformers and convolutions to efficiently process high-resolution\nimages. We carefully design each component of our framework to guarantee the\nhigh fidelity and diversity of recovered images. Specifically, we customize an\ninpainting-oriented transformer block, where the attention module aggregates\nnon-local information only from partial valid tokens, indicated by a dynamic\nmask. Extensive experiments demonstrate the state-of-the-art performance of the\nnew model on multiple benchmark datasets. Code is released at\nhttps://github.com/fenglinglwb/MAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift. (arXiv:2203.16291v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16291","description":"<p>We introduce AmsterTime: a challenging dataset to benchmark visual place\nrecognition (VPR) in presence of a severe domain shift. AmsterTime offers a\ncollection of 2,500 well-curated images matching the same scene from a street\nview matched to historical archival image data from Amsterdam city. The image\npairs capture the same place with different cameras, viewpoints, and\nappearances. Unlike existing benchmark datasets, AmsterTime is directly\ncrowdsourced in a GIS navigation platform (Mapillary). We evaluate various\nbaselines, including non-learning, supervised and self-supervised methods,\npre-trained on different relevant datasets, for both verification and retrieval\ntasks. Our result credits the best accuracy to the ResNet-101 model pre-trained\non the Landmarks dataset for both verification and retrieval tasks by 84% and\n24%, respectively. Additionally, a subset of Amsterdam landmarks is collected\nfor feature evaluation in a classification task. Classification labels are\nfurther used to extract the visual explanations using Grad-CAM for inspection\nof the learned similar visuals in a deep metric learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yildiz_B/0/1/0/all/0/1\">Burak Yildiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_S/0/1/0/all/0/1\">Seyran Khademi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebes_R/0/1/0/all/0/1\">Ronald Maria Siebes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating High Fidelity Data from Low-density Regions using Diffusion Models. (arXiv:2203.17260v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17260","description":"<p>Our work focuses on addressing sample deficiency from low-density regions of\ndata manifold in common image datasets. We leverage diffusion process based\ngenerative models to synthesize novel images from low-density regions. We\nobserve that uniform sampling from diffusion models predominantly samples from\nhigh-density regions of the data manifold. Therefore, we modify the sampling\nprocess to guide it towards low-density regions while simultaneously\nmaintaining the fidelity of synthetic data. We rigorously demonstrate that our\nprocess successfully generates novel high fidelity samples from low-density\nregions. We further examine generated samples and show that the model does not\nmemorize low-density data and indeed learns to generate novel samples from\nlow-density regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1\">Vikash Sehwag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazirbas_C/0/1/0/all/0/1\">Caner Hazirbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordo_A/0/1/0/all/0/1\">Albert Gordo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgenel_F/0/1/0/all/0/1\">Firat Ozgenel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proper Reuse of Image Classification Features Improves Object Detection. (arXiv:2204.00484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00484","description":"<p>A common practice in transfer learning is to initialize the downstream model\nweights by pre-training on a data-abundant upstream task. In object detection\nspecifically, the feature backbone is typically initialized with Imagenet\nclassifier weights and fine-tuned on the object detection task. Recent works\nshow this is not strictly necessary under longer training regimes and provide\nrecipes for training the backbone from scratch. We investigate the opposite\ndirection of this end-to-end training trend: we show that an extreme form of\nknowledge preservation -- freezing the classifier-initialized backbone --\nconsistently improves many different detection models, and leads to\nconsiderable resource savings. We hypothesize and corroborate experimentally\nthat the remaining detector components capacity and structure is a crucial\nfactor in leveraging the frozen backbone. Immediate applications of our\nfindings include performance improvements on hard cases like detection of\nlong-tail object classes and computational and memory resource savings that\ncontribute to making the field more accessible to researchers with access to\nfewer computational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1\">Vighnesh Birodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Global Shutter: Learn to Restore Video from a Rolling Shutter Camera with Global Reset Feature. (arXiv:2204.00974v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00974","description":"<p>Most computer vision systems assume distortion-free images as inputs. The\nwidely used rolling-shutter (RS) image sensors, however, suffer from geometric\ndistortion when the camera and object undergo motion during capture. Extensive\nresearches have been conducted on correcting RS distortions. However, most of\nthe existing work relies heavily on the prior assumptions of scenes or motions.\nBesides, the motion estimation steps are either oversimplified or\ncomputationally inefficient due to the heavy flow warping, limiting their\napplicability. In this paper, we investigate using rolling shutter with a\nglobal reset feature (RSGR) to restore clean global shutter (GS) videos. This\nfeature enables us to turn the rectification problem into a deblur-like one,\ngetting rid of inaccurate and costly explicit motion estimation. First, we\nbuild an optic system that captures paired RSGR/GS videos. Second, we develop a\nnovel algorithm incorporating spatial and temporal designs to correct the\nspatial-varying RSGR distortion. Third, we demonstrate that existing\nimage-to-image translation algorithms can recover clean GS videos from\ndistorted RSGR inputs, yet our algorithm achieves the best performance with the\nspecific designs. Our rendered results are not only visually appealing but also\nbeneficial to downstream tasks. Compared to the state-of-the-art RS solution,\nour RSGR solution is superior in both effectiveness and efficiency. Considering\nit is easy to realize without changing the hardware, we believe our RSGR\nsolution can potentially replace the RS solution in taking distortion-free\nvideos with low noise and low budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhixiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_S/0/1/0/all/0/1\">Shin&#x27;ichi Satoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Generalizable Dexterous Manipulation from Human Grasp Affordance. (arXiv:2204.02320v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.02320","description":"<p>Dexterous manipulation with a multi-finger hand is one of the most\nchallenging problems in robotics. While recent progress in imitation learning\nhas largely improved the sample efficiency compared to Reinforcement Learning,\nthe learned policy can hardly generalize to manipulate novel objects, given\nlimited expert demonstrations. In this paper, we propose to learn dexterous\nmanipulation using large-scale demonstrations with diverse 3D objects in a\ncategory, which are generated from a human grasp affordance model. This\ngeneralizes the policy to novel object instances within the same category. To\ntrain the policy, we propose a novel imitation learning objective jointly with\na geometric representation learning objective using our demonstrations. By\nexperimenting with relocating diverse objects in simulation, we show that our\napproach outperforms baselines with a large margin when manipulating novel\nobjects. We also ablate the importance on 3D object representation learning for\nmanipulation. We include videos, code, and additional information on the\nproject website - https://kristery.github.io/ILAD/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueh-Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Anticipate Future with Dynamic Context Removal. (arXiv:2204.02587v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02587","description":"<p>Anticipating future events is an essential feature for intelligent systems\nand embodied AI. However, compared to the traditional recognition task, the\nuncertainty of future and reasoning ability requirement make the anticipation\ntask very challenging and far beyond solved. In this filed, previous methods\nusually care more about the model architecture design or but few attention has\nbeen put on how to train an anticipation model with a proper learning policy.\nTo this end, in this work, we propose a novel training scheme called Dynamic\nContext Removal (DCR), which dynamically schedules the visibility of observed\nfuture in the learning procedure. It follows the human-like curriculum learning\nprocess, i.e., gradually removing the event context to increase the\nanticipation difficulty till satisfying the final anticipation target. Our\nlearning scheme is plug-and-play and easy to integrate any reasoning model\nincluding transformer and LSTM, with advantages in both effectiveness and\nefficiency. In extensive experiments, the proposed method achieves\nstate-of-the-art on four widely-used benchmarks. Our code and models are\npublicly released at https://github.com/AllenXuuu/DCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong-Lu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neglectable effect of brain MRI data preprocessing for tumor segmentation. (arXiv:2204.05278v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.05278","description":"<p>Magnetic resonance imaging (MRI) data is heterogeneous due to the differences\nin device manufacturers, scanning protocols, and inter-subject variability. A\nconventional way to mitigate MR image heterogeneity is to apply preprocessing\ntransformations, such as anatomy alignment, voxel resampling, signal intensity\nequalization, image denoising, and localization of regions of interest (ROI).\nAlthough preprocessing pipeline standardizes image appearance, its influence on\nthe quality of image segmentation and other downstream tasks on deep neural\nnetworks (DNN) has never been rigorously studied.\n</p>\n<p>Here we report a comprehensive study of multimodal MRI brain cancer image\nsegmentation on TCIA-GBM open-source dataset. Our results demonstrate that most\npopular standardization steps add no value to artificial neural network\nperformance; moreover, preprocessing can hamper model performance. We suggest\nthat image intensity normalization approaches do not contribute to model\naccuracy because of the reduction of signal variance with image\nstandardization. Finally, we show the contribution of scull-stripping in data\npreprocessing is almost negligible if measured in terms of clinically relevant\nmetrics.\n</p>\n<p>We show that the only essential transformation for accurate analysis is the\nunification of voxel spacing across the dataset. In contrast, anatomy alignment\nin form of non-rigid atlas registration is not necessary and most intensity\nequalization steps do not improve model productiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kondrateva_E/0/1/0/all/0/1\">Ekaterina Kondrateva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Druzhinina_P/0/1/0/all/0/1\">Polina Druzhinina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalechina_A/0/1/0/all/0/1\">Alexandra Dalechina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1\">Anvar Kurmukov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DConvCaps: 3DUnet with Convolutional Capsule Encoder for Medical Image Segmentation. (arXiv:2205.09299v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09299","description":"<p>Convolutional Neural Networks (CNNs) have achieved promising results in\nmedical image segmentation. However, CNNs require lots of training data and are\nincapable of handling pose and deformation of objects. Furthermore, their\npooling layers tend to discard important information such as positions as well\nas CNNs are sensitive to rotation and affine transformation. Capsule network is\na recent new architecture that has achieved better robustness in part-whole\nrepresentation learning by replacing pooling layers with dynamic routing and\nconvolutional strides, which has shown potential results on popular tasks such\nas digit classification and object segmentation. In this paper, we propose a 3D\nencoder-decoder network with Convolutional Capsule Encoder (called 3DConvCaps)\nto learn lower-level features (short-range attention) with convolutional layers\nwhile modeling the higher-level features (long-range dependence) with capsule\nlayers. Our experiments on multiple datasets including iSeg-2017, Hippocampus,\nand Cardiac demonstrate that our 3D 3DConvCaps network considerably outperforms\nprevious capsule networks and 3D-UNets. We further conduct ablation studies of\nnetwork efficiency and segmentation performance under various configurations of\nconvolution layers and capsule layers at both contracting and expanding paths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan T.H. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Landslide4Sense: Reference Benchmark Data and Deep Learning Models for Landslide Detection. (arXiv:2206.00515v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00515","description":"<p>This study introduces \\textit{Landslide4Sense}, a reference benchmark for\nlandslide detection from remote sensing. The repository features 3,799 image\npatches fusing optical layers from Sentinel-2 sensors with the digital\nelevation model and slope layer derived from ALOS PALSAR. The added\ntopographical information facilitates an accurate detection of landslide\nborders, which recent researches have shown to be challenging using optical\ndata alone. The extensive data set supports deep learning (DL) studies in\nlandslide detection and the development and validation of methods for the\nsystematic update of landslide inventories. The benchmark data set has been\ncollected at four different times and geographical locations: Iburi (September\n2018), Kodagu (August 2018), Gorkha (April 2015), and Taiwan (August 2009).\nEach image pixel is labelled as belonging to a landslide or not, incorporating\nvarious sources and thorough manual annotation. We then evaluate the landslide\ndetection performance of 11 state-of-the-art DL segmentation models: U-Net,\nResU-Net, PSPNet, ContextNet, DeepLab-v2, DeepLab-v3+, FCN-8s, LinkNet, FRRN-A,\nFRRN-B, and SQNet. All models were trained from scratch on patches from one\nquarter of each study area and tested on independent patches from the other\nthree quarters. Our experiments demonstrate that ResU-Net outperformed the\nother models for the landslide detection task. We make the multi-source\nlandslide benchmark data (Landslide4Sense) and the tested DL models publicly\navailable at \\url{www.landslide4sense.org}, establishing an important resource\nfor remote sensing, computer vision, and machine learning communities in\nstudies of image classification in general and applications to landslide\ndetection in particular.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghorbanzadeh_O/0/1/0/all/0/1\">Omid Ghorbanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yonghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1\">Michael Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreil_D/0/1/0/all/0/1\">David Kreil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supernet Training for Federated Image Classification under System Heterogeneity. (arXiv:2206.01366v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.01366","description":"<p>Efficient deployment of deep neural networks across many devices and resource\nconstraints, especially on edge devices, is one of the most challenging\nproblems in the presence of data-privacy preservation issues. Conventional\napproaches have evolved to either improve a single global model while keeping\neach local training data decentralized (i.e., data-heterogeneity) or to train a\nonce-for-all network that supports diverse architectural settings to address\nheterogeneous systems equipped with different computational capabilities (i.e.,\nmodel-heterogeneity). However, little research has considered both directions\nsimultaneously. In this work, we propose a novel framework to consider both\nscenarios, namely Federation of Supernet Training (FedSup), where clients send\nand receive a supernet whereby it contains all possible architectures sampled\nfrom itself. It is inspired by how averaging parameters in the model\naggregation stage of Federated Learning (FL) is similar to weight-sharing in\nsupernet training. Specifically, in the FedSup framework, a weight-sharing\napproach widely used in the training single shot model is combined with the\naveraging of Federated Learning (FedAvg). Under our framework, we present an\nefficient algorithm (E-FedSup) by sending the sub-model to clients in the\nbroadcast stage for reducing communication costs and training overhead. We\ndemonstrate several strategies to enhance supernet training in the FL\nenvironment and conduct extensive empirical evaluations. The resulting\nframework is shown to pave the way for the robustness of both data- and\nmodel-heterogeneity on several standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drawing out of Distribution with Neuro-Symbolic Generative Models. (arXiv:2206.01829v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.01829","description":"<p>Learning general-purpose representations from perceptual inputs is a hallmark\nof human intelligence. For example, people can write out numbers or characters,\nor even draw doodles, by characterizing these tasks as different instantiations\nof the same generic underlying process -- compositional arrangements of\ndifferent forms of pen strokes. Crucially, learning to do one task, say\nwriting, implies reasonable competence at another, say drawing, on account of\nthis shared process. We present Drawing out of Distribution (DooD), a\nneuro-symbolic generative model of stroke-based drawing that can learn such\ngeneral-purpose representations. In contrast to prior work, DooD operates\ndirectly on images, requires no supervision or expensive test-time inference,\nand performs unsupervised amortised inference with a symbolic stroke model that\nbetter enables both interpretability and generalization. We evaluate DooD on\nits ability to generalise across both data and tasks. We first perform\nzero-shot transfer from one dataset (e.g. MNIST) to another (e.g. Quickdraw),\nacross five different datasets, and show that DooD clearly outperforms\ndifferent baselines. An analysis of the learnt representations further\nhighlights the benefits of adopting a symbolic stroke model. We then adopt a\nsubset of the Omniglot challenge tasks, and evaluate its ability to generate\nnew exemplars (both unconditionally and conditionally), and perform one-shot\nclassification, showing that DooD matches the state of the art. Taken together,\nwe demonstrate that DooD does indeed capture general-purpose representations\nacross both data and task, and takes a further step towards building general\nand robust concept-learning systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yichao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tuan Anh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image. (arXiv:2206.01856v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.01856","description":"<p>Image enhancement approaches often assume that the noise is signal\nindependent, and approximate the degradation model as zero-mean additive\nGaussian. However, this assumption does not hold for biomedical imaging systems\nwhere sensor-based sources of noise are proportional to signal strengths, and\nthe noise is better represented as a Poisson process. In this work, we explore\na sparsity and dictionary learning-based approach and present a novel\nself-supervised learning method for single-image denoising where the noise is\napproximated as a Poisson process, requiring no clean ground-truth data.\nSpecifically, we approximate traditional iterative optimization algorithms for\nimage denoising with a recurrent neural network that enforces sparsity with\nrespect to the weights of the network. Since the sparse representations are\nbased on the underlying image, it is able to suppress the spurious components\n(noise) in the image patches, thereby introducing implicit regularization for\ndenoising tasks through the network structure. Experiments on two bio-imaging\ndatasets demonstrate that our method outperforms the state-of-the-art\napproaches in terms of PSNR and SSIM. Our qualitative results demonstrate that,\nin addition to higher performance on standard quantitative metrics, we are able\nto recover much more subtle details than other compared approaches. Our code is\nmade publicly available at https://github.com/tacalvin/Poisson2Sparse\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ta_C/0/1/0/all/0/1\">Calvin-Khang Ta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aich_A/0/1/0/all/0/1\">Abhishek Aich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Domain Adaptation in Crowd Counting. (arXiv:2206.03431v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03431","description":"<p>Self-training crowd counting has not been attentively explored though it is\none of the important challenges in computer vision. In practice, the fully\nsupervised methods usually require an intensive resource of manual annotation.\nIn order to address this challenge, this work introduces a new approach to\nutilize existing datasets with ground truth to produce more robust predictions\non unlabeled datasets, named domain adaptation, in crowd counting. While the\nnetwork is trained with labeled data, samples without labels from the target\ndomain are also added to the training process. In this process, the entropy map\nis computed and minimized in addition to the adversarial training process\ndesigned in parallel. Experiments on Shanghaitech, UCF_CC_50, and UCF-QNRF\ndatasets prove a more generalized improvement of our method over the other\nstate-of-the-arts in the cross-domain setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Pha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Miaoqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CO^3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving. (arXiv:2206.04028v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04028","description":"<p>Unsupervised contrastive learning for indoor-scene point clouds has achieved\ngreat successes. However, unsupervised learning point clouds in outdoor scenes\nremains challenging because previous methods need to reconstruct the whole\nscene and capture partial views for the contrastive objective. This is\ninfeasible in outdoor scenes with moving objects, obstacles, and sensors. In\nthis paper, we propose CO^3, namely Cooperative Contrastive Learning and\nContextual Shape Prediction, to learn 3D representation for outdoor-scene point\nclouds in an unsupervised manner. CO^3 has several merits compared to existing\nmethods. (1) It utilizes LiDAR point clouds from vehicle-side and\ninfrastructure-side to build views that differ enough but meanwhile maintain\ncommon semantic information for contrastive learning, which are more\nappropriate than views built by previous methods. (2) Alongside the contrastive\nobjective, shape context prediction is proposed as pre-training goal and brings\nmore task-relevant information for unsupervised 3D point cloud representation\nlearning, which are beneficial when transferring the learned representation to\ndownstream detection tasks. (3) As compared to previous methods, representation\nlearned by CO^3 is able to be transferred to different outdoor scene dataset\ncollected by different type of LiDAR sensors. (4) CO^3 improves current\nstate-of-the-art methods on both Once and KITTI datasets by up to 2.58 mAP.\nCodes and models will be released. We believe CO^3 will facilitate\nunderstanding LiDAR point clouds in outdoor scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yao Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenhan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04046","description":"<p>Domain generalization (DG) aims at learning generalizable models under\ndistribution shifts to avoid redundantly overfitting massive training data.\nPrevious works with complex loss design and gradient constraint have not yet\nled to empirical success on large-scale benchmarks. In this work, we reveal the\nmixture-of-experts (MoE) model's generalizability on DG by leveraging to\ndistributively handle multiple aspects of the predictive features across\ndomains. To this end, we propose Sparse Fusion Mixture-of-Experts (SF-MoE),\nwhich incorporates sparsity and fusion mechanisms into the MoE framework to\nkeep the model both sparse and predictive. SF-MoE has two dedicated modules: 1)\nsparse block and 2) fusion block, which disentangle and aggregate the diverse\nlearned signals of an object, respectively. Extensive experiments demonstrate\nthat SF-MoE is a domain-generalizable learner on large-scale benchmarks. It\noutperforms state-of-the-art counterparts by more than 2% across 5 large-scale\nDG datasets (e.g., DomainNet), with the same or even lower computational costs.\nWe further reveal the internal mechanism of SF-MoE from distributed\nrepresentation perspective (e.g., visual attributes). We hope this framework\ncould facilitate future research to push generalizable object recognition to\nthe real world. Code and models are released at\nhttps://github.com/Luodian/SF-MoE-DG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiawei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of COVID-19 in Chest X-ray Images Using Fusion of Deep Features and LightGBM. (arXiv:2206.04548v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.04548","description":"<p>The COVID-19 disease was first discovered in Wuhan, China, and spread quickly\nworldwide. After the COVID-19 pandemic, many researchers have begun to identify\na way to diagnose the COVID-19 using chest X-ray images. The early diagnosis of\nthis disease can significantly impact the treatment process. In this article,\nwe propose a new technique that is faster and more accurate than the other\nmethods reported in the literature. The proposed method uses a combination of\nDenseNet169 and MobileNet Deep Neural Networks to extract the features of the\npatient's X-ray images. Using the univariate feature selection algorithm, we\nrefined the features for the most important ones. Then we applied the selected\nfeatures as input to the LightGBM (Light Gradient Boosting Machine) algorithm\nfor classification. To assess the effectiveness of the proposed method, the\nChestX-ray8 dataset, which includes 1125 X-ray images of the patient's chest,\nwas used. The proposed method achieved 98.54% and 91.11% accuracies in the\ntwo-class (COVID-19, Healthy) and multi-class (COVID-19, Healthy, Pneumonia)\nclassification problems, respectively. It is worth mentioning that we have used\nGradient-weighted Class Activation Mapping (Grad-CAM) for further analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nasiri_H/0/1/0/all/0/1\">Hamid Nasiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kheyroddin_G/0/1/0/all/0/1\">Ghazal Kheyroddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorrigiv_M/0/1/0/all/0/1\">Morteza Dorrigiv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esmaeili_M/0/1/0/all/0/1\">Mona Esmaeili</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nafchi_A/0/1/0/all/0/1\">Amir Raeisi Nafchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghorbani_M/0/1/0/all/0/1\">Mohsen Haji Ghorbani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zarkesh_Ha_P/0/1/0/all/0/1\">Payman Zarkesh-Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images. (arXiv:2206.06665v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06665","description":"<p>Developing an AI-assisted gland segmentation method from histology images is\ncritical for automatic cancer diagnosis and prognosis; however, the high cost\nof pixel-level annotations hinders its applications to broader diseases.\nExisting weakly-supervised semantic segmentation methods in computer vision\nachieve degenerative results for gland segmentation, since the characteristics\nand problems of glandular datasets are different from general object datasets.\nWe observe that, unlike natural images, the key problem with histology images\nis the confusion of classes owning to morphological homogeneity and low color\ncontrast among different tissues. To this end, we propose a novel method Online\nEasy Example Mining (OEEM) that encourages the network to focus on credible\nsupervision signals rather than noisy signals, therefore mitigating the\ninfluence of inevitable false predictions in pseudo-masks. According to the\ncharacteristics of glandular datasets, we design a strong framework for gland\nsegmentation. Our results exceed many fully-supervised methods and\nweakly-supervised methods for gland segmentation over 4.4% and 6.04% at mIoU,\nrespectively. Code is available at https://github.com/xmed-lab/OEEM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yiduo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yiwen Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tianqi Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Transformer Variational Autoencoders for Multi-Action Motion Synthesis. (arXiv:2206.06741v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06741","description":"<p>We consider the problem of synthesizing multi-action human motion sequences\nof arbitrary lengths. Existing approaches have mastered motion sequence\ngeneration in single action scenarios, but fail to generalize to multi-action\nand arbitrary-length sequences. We fill this gap by proposing a novel efficient\napproach that leverages expressiveness of Recurrent Transformers and generative\nrichness of conditional Variational Autoencoders. The proposed iterative\napproach is able to generate smooth and realistic human motion sequences with\nan arbitrary number of actions and frames while doing so in linear space and\ntime. We train and evaluate the proposed approach on PROX and Charades\ndatasets, where we augment PROX with ground-truth action labels and Charades\nwith human mesh annotations. Experimental evaluation shows significant\nimprovements in FID score and semantic consistency metrics compared to the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Briq_R/0/1/0/all/0/1\">Rania Briq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1\">Chuhang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pishchulin_L/0/1/0/all/0/1\">Leonid Pishchulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broaddus_C/0/1/0/all/0/1\">Chris Broaddus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites. (arXiv:2206.06813v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.06813","description":"<p>In clinical practice, a segmentation network is often required to continually\nlearn on a sequential data stream from multiple sites rather than a\nconsolidated set, due to the storage cost and privacy restriction. However,\nduring the continual learning process, existing methods are usually restricted\nin either network memorizability on previous sites or generalizability on\nunseen sites. This paper aims to tackle the challenging problem of Synchronous\nMemorizability and Generalizability (SMG) and to simultaneously improve\nperformance on both previous and unseen sites, with a novel proposed\nSMG-learning framework. First, we propose a Synchronous Gradient Alignment\n(SGA) objective, which not only promotes the network memorizability by\nenforcing coordinated optimization for a small exemplar set from previous sites\n(called replay buffer), but also enhances the generalizability by facilitating\nsite-invariance under simulated domain shift. Second, to simplify the\noptimization of SGA objective, we design a Dual-Meta algorithm that\napproximates the SGA objective as dual meta-objectives for optimization without\nexpensive computation overhead. Third, for efficient rehearsal, we configure\nthe replay buffer comprehensively considering additional inter-site diversity\nto reduce redundancy. Experiments on prostate MRI data sequentially acquired\nfrom six institutes demonstrate that our method can simultaneously achieve\nhigher memorizability and generalizability over state-of-the-art methods. Code\nis available at https://github.com/jingyzhang/SMG-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_P/0/1/0/all/0/1\">Peng Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_R/0/1/0/all/0/1\">Ran Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yuning Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1\">Mianxin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yongsheng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiming Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiawei Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation. (arXiv:2206.07117v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07117","description":"<p>3D hand pose estimation methods have made significant progress recently.\nHowever, the estimation accuracy is often far from sufficient for specific\nreal-world applications, and thus there is significant room for improvement.\nThis paper proposes TriHorn-Net, a novel model that uses specific innovations\nto improve hand pose estimation accuracy on depth images. The first innovation\nis the decomposition of the 3D hand pose estimation into the estimation of 2D\njoint locations in the depth image space (UV), and the estimation of their\ncorresponding depths aided by two complementary attention maps. This\ndecomposition prevents depth estimation, which is a more difficult task, from\ninterfering with the UV estimations at both the prediction and feature levels.\nThe second innovation is PixDropout, which is, to the best of our knowledge,\nthe first appearance-based data augmentation method for hand depth images.\nExperimental results demonstrate that the proposed model outperforms the\nstate-of-the-art methods on three public benchmark datasets. Our implementation\nis available at https://github.com/mrezaei92/TriHorn-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1\">Mohammad Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Neural Programs: Variational Learning of Image Grammars and Part-Whole Hierarchies. (arXiv:2206.08462v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08462","description":"<p>Human vision involves parsing and representing objects and scenes using\nstructured representations based on part-whole hierarchies. Computer vision and\nmachine learning researchers have recently sought to emulate this capability\nusing capsule networks, reference frames and active predictive coding, but a\ngenerative model formulation has been lacking. We introduce Recursive Neural\nPrograms (RNPs), which, to our knowledge, is the first neural generative model\nto address the part-whole hierarchy learning problem. RNPs model images as\nhierarchical trees of probabilistic sensory-motor programs that recursively\nreuse learned sensory-motor primitives to model an image within different\nreference frames, forming recursive image grammars. We express RNPs as\nstructured variational autoencoders (sVAEs) for inference and sampling, and\ndemonstrate parts-based parsing, sampling and one-shot transfer learning for\nMNIST, Omniglot and Fashion-MNIST datasets, demonstrating the model's\nexpressive power. Our results show that RNPs provide an intuitive and\nexplainable way of composing objects and scenes, allowing rich compositionality\nand intuitive interpretations of objects in terms of part-whole hierarchies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fisher_A/0/1/0/all/0/1\">Ares Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rajesh P.N. Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot AutoML with Pretrained Models. (arXiv:2206.08476v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08476","description":"<p>Given a new dataset D and a low compute budget, how should we choose a\npre-trained model to fine-tune to D, and set the fine-tuning hyperparameters\nwithout risking overfitting, particularly if D is small? Here, we extend\nautomated machine learning (AutoML) to best make these choices. Our\ndomain-independent meta-learning approach learns a zero-shot surrogate model\nwhich, at test time, allows to select the right deep learning (DL) pipeline\n(including the pre-trained model and fine-tuning hyperparameters) for a new\ndataset D given only trivial meta-features describing D such as image\nresolution or the number of classes. To train this zero-shot model, we collect\nperformance data for many DL pipelines on a large collection of datasets and\nmeta-train on this data to minimize a pairwise ranking objective. We evaluate\nour approach under the strict time limit of the vision track of the ChaLearn\nAutoDL challenge benchmark, clearly outperforming all challenge contenders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozturk_E/0/1/0/all/0/1\">Ekrem &#xd6;zt&#xfc;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1\">Fabio Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jomaa_H/0/1/0/all/0/1\">Hadi S. Jomaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabocka_J/0/1/0/all/0/1\">Josif Grabocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images. (arXiv:2206.08549v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.08549","description":"<p>Evaluation metrics in image synthesis play a key role to measure performances\nof generative models. However, most metrics mainly focus on image fidelity.\nExisting diversity metrics are derived by comparing distributions, and thus\nthey cannot quantify the diversity or rarity degree of each generated image. In\nthis work, we propose a new evaluation metric, called `rarity score', to\nmeasure the individual rarity of each image synthesized by generative models.\nWe first show empirical observation that common samples are close to each other\nand rare samples are far from each other in nearest-neighbor distances of\nfeature space. We then use our metric to demonstrate that the extent to which\ndifferent generative models produce rare images can be effectively compared. We\nalso propose a method to compare rarities between datasets that share the same\nconcept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in\ndifferent designs of feature spaces to better understand the relationship\nbetween feature spaces and resulting sparse images. Code will be publicly\navailable online for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiyeon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hwanil Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaesik Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is Where by Looking: Weakly-Supervised Open-World Phrase-Grounding without Text Inputs. (arXiv:2206.09358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09358","description":"<p>Given an input image, and nothing else, our method returns the bounding boxes\nof objects in the image and phrases that describe the objects. This is achieved\nwithin an open world paradigm, in which the objects in the input image may not\nhave been encountered during the training of the localization mechanism.\nMoreover, training takes place in a weakly supervised setting, where no\nbounding boxes are provided. To achieve this, our method combines two\npre-trained networks: the CLIP image-to-text matching score and the BLIP image\ncaptioning tool. Training takes place on COCO images and their captions and is\nbased on CLIP. Then, during inference, BLIP is used to generate a hypothesis\nregarding various regions of the current image. Our work generalizes weakly\nsupervised segmentation and phrase grounding and is shown empirically to\noutperform the state of the art in both domains. It also shows very convincing\nresults in the novel task of weakly-supervised open-world purely visual\nphrase-grounding presented in our work. For example, on the datasets used for\nbenchmarking phrase-grounding, our method results in a very modest degradation\nin comparison to methods that employ human captions as an additional input. Our\ncode is available at https://github.com/talshaharabany/what-is-where-by-looking\nand a live demo can be found at\nhttps://replicate.com/talshaharabany/what-is-where-by-looking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaharabany_T/0/1/0/all/0/1\">Tal Shaharabany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewel_Y/0/1/0/all/0/1\">Yoad Tewel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalizable Person Re-identification with a Bi-stream Generative Model. (arXiv:2206.09362v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09362","description":"<p>Generalizable person re-identification (re-ID) has attracted growing\nattention due to its powerful adaptation capability in the unseen data domain.\nHowever, existing solutions often neglect either crossing cameras (e.g.,\nillumination and resolution differences) or pedestrian misalignments (e.g.,\nviewpoint and pose discrepancies), which easily leads to poor generalization\ncapability when adapted to the new domain. In this paper, we formulate these\ndifficulties as: 1) Camera-Camera (CC) problem, which denotes the various human\nappearance changes caused by different cameras; 2) Camera-Person (CP) problem,\nwhich indicates the pedestrian misalignments caused by the same identity person\nunder different camera viewpoints or changing pose. To solve the above issues,\nwe propose a Bi-stream Generative Model (BGM) to learn the fine-grained\nrepresentations fused with camera-invariant global feature and\npedestrian-aligned local feature, which contains an encoding network and two\nstream decoding sub-networks. Guided by original pedestrian images, one stream\nis employed to learn a camera-invariant global feature for the CC problem via\nfiltering cross-camera interference factors. For the CP problem, another stream\nlearns a pedestrian-aligned local feature for pedestrian alignment using\ninformation-complete densely semantically aligned part maps. Moreover, a\npart-weighted loss function is presented to reduce the influence of missing\nparts on pedestrian alignment. Extensive experiments demonstrate that our\nmethod outperforms the state-of-the-art methods on the large-scale\ngeneralizable re-ID benchmarks, involving domain generalization setting and\ncross-domain setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C-SENN: Contrastive Self-Explaining Neural Network. (arXiv:2206.09575v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09575","description":"<p>In this study, we use a self-explaining neural network (SENN), which learns\nunsupervised concepts, to acquire concepts that are easy for people to\nunderstand automatically. In concept learning, the hidden layer retains\nverbalizable features relevant to the output, which is crucial when adapting to\nreal-world environments where explanations are required. However, it is known\nthat the interpretability of concepts output by SENN is reduced in general\nsettings, such as autonomous driving scenarios. Thus, this study combines\ncontrastive learning with concept learning to improve the readability of\nconcepts and the accuracy of tasks. We call this model Contrastive\nSelf-Explaining Neural Network (C-SENN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sawada_Y/0/1/0/all/0/1\">Yoshihide Sawada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1\">Keigo Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology. (arXiv:2206.09769v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09769","description":"<p>Histopathology whole slide images (WSIs) can reveal significant\ninter-hospital variability such as illumination, color or optical artifacts.\nThese variations, caused by the use of different scanning protocols across\nmedical centers (staining, scanner), can strongly harm algorithms\ngeneralization on unseen protocols. This motivates development of new methods\nto limit such drop of performances. In this paper, to enhance robustness on\nunseen target protocols, we propose a new test-time data augmentation based on\nmulti domain image-to-image translation. It allows to project images from\nunseen protocol into each source domain before classifying them and ensembling\nthe predictions. This test-time augmentation method results in a significant\nboost of performances for domain generalization. To demonstrate its\neffectiveness, our method has been evaluated on 2 different histopathology\ntasks where it outperforms conventional domain generalization, standard H&amp;E\nspecific color augmentation/normalization and standard test-time augmentation\ntechniques. Our code is publicly available at\nhttps://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1\">Marin Scalbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1\">Florent Couzini&#xe9;-Devy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel-MAE: Masked Autoencoders for Pre-training Large-scale Point Clouds. (arXiv:2206.09900v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09900","description":"<p>Mask-based pre-training has achieved great success for self-supervised\nlearning in image, video, and language, without manually annotated supervision.\nHowever, it has not yet been studied about large-scale point clouds with\nredundant spatial information in autonomous driving. As the number of\nlarge-scale point clouds is huge, it is impossible to reconstruct the input\npoint clouds. In this paper, we propose a mask voxel classification network for\nlarge-scale point clouds pre-training. Our key idea is to divide the point\nclouds into voxel representations and classify whether the voxel contains point\nclouds. This simple strategy makes the network to be voxel-aware of the object\nshape, thus improving the performance of the downstream tasks, such as 3D\nobject detection. Our Voxel-MAE with even a 90% masking ratio can still learn\nrepresentative features for the high spatial redundancy of large-scale point\nclouds. We also validate the effectiveness of Voxel-MAE in unsupervised domain\nadaptative tasks, which proves the generalization ability of Voxel-MAE. Our\nVoxel-MAE proves that it is feasible to pre-train large-scale point clouds\nwithout data annotations to enhance the perception ability of the autonomous\nvehicle. Extensive experiments show great effectiveness of our pre-trained\nmodel with 3D object detectors (SECOND, CenterPoint, and PV-RCNN) on two\npopular datasets (KITTI, Waymo). Codes are publicly available at\nhttps://github.com/chaytonmin/Voxel-MAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chen Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Liang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yiming Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bin Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORFD: A Dataset and Benchmark for Off-Road Freespace Detection. (arXiv:2206.09907v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09907","description":"<p>Freespace detection is an essential component of autonomous driving\ntechnology and plays an important role in trajectory planning. In the last\ndecade, deep learning-based free space detection methods have been proved\nfeasible. However, these efforts were focused on urban road environments and\nfew deep learning-based methods were specifically designed for off-road free\nspace detection due to the lack of off-road benchmarks. In this paper, we\npresent the ORFD dataset, which, to our knowledge, is the first off-road free\nspace detection dataset. The dataset was collected in different scenes\n(woodland, farmland, grassland, and countryside), different weather conditions\n(sunny, rainy, foggy, and snowy), and different light conditions (bright light,\ndaylight, twilight, darkness), which totally contains 12,198 LiDAR point cloud\nand RGB image pairs with the traversable area, non-traversable area and\nunreachable area annotated in detail. We propose a novel network named OFF-Net,\nwhich unifies Transformer architecture to aggregate local and global\ninformation, to meet the requirement of large receptive fields for free space\ndetection tasks. We also propose the cross-attention to dynamically fuse LiDAR\nand RGB image information for accurate off-road free space detection. Dataset\nand code are publicly available athttps://github.com/chaytonmin/OFF-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chen Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Weizhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiaolong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Liang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yiming Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bin Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test Time Transform Prediction for Open Set Histopathological Image Recognition. (arXiv:2206.10033v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10033","description":"<p>Tissue typology annotation in Whole Slide histological images is a complex\nand tedious, yet necessary task for the development of computational pathology\nmodels. We propose to address this problem by applying Open Set Recognition\ntechniques to the task of jointly classifying tissue that belongs to a set of\nannotated classes, e.g. clinically relevant tissue categories, while rejecting\nin test time Open Set samples, i.e. images that belong to categories not\npresent in the training set. To this end, we introduce a new approach for Open\nSet histopathological image recognition based on training a model to accurately\nidentify image categories and simultaneously predict which data augmentation\ntransform has been applied. In test time, we measure model confidence in\npredicting this transform, which we expect to be lower for images in the Open\nSet. We carry out comprehensive experiments in the context of colorectal cancer\nassessment from histological images, which provide evidence on the strengths of\nour approach to automatically identify samples from unknown categories. Code is\nreleased at https://github.com/agaldran/t3po .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galdran_A/0/1/0/all/0/1\">Adrian Galdran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_K/0/1/0/all/0/1\">Katherine J. Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_N/0/1/0/all/0/1\">Narmin L. Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kather_J/0/1/0/all/0/1\">Jakob N. Kather</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballester_M/0/1/0/all/0/1\">Miguel A. Gonz&#xe1;lez Ballester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders. (arXiv:2206.10207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10207","description":"<p>Recently, significant progress has been made in masked image modeling to\ncatch up to masked language modeling. However, unlike words in NLP, the lack of\nsemantic decomposition of images still makes masked autoencoding (MAE)\ndifferent between vision and language. In this paper, we explore a potential\nvisual analogue of words, i.e., semantic parts, and we integrate semantic\ninformation into the training process of MAE by proposing a Semantic-Guided\nMasking strategy. Compared to widely adopted random masking, our masking\nstrategy can gradually guide the network to learn various information, i.e.,\nfrom intra-part patterns to inter-part relations. In particular, we achieve\nthis in two steps. 1) Semantic part learning: we design a self-supervised part\nlearning method to obtain semantic parts by leveraging and refining the\nmulti-head attention of a ViT-based encoder. 2) Semantic-guided MAE (SemMAE)\ntraining: we design a masking strategy that varies from masking a portion of\npatches in each part to masking a portion of (whole) parts in an image.\nExtensive experiments on various vision tasks show that SemMAE can learn better\nimage representation by integrating semantic information. In particular, SemMAE\nachieves 84.5% fine-tuning accuracy on ImageNet-1k, which outperforms the\nvanilla MAE by 1.4%. In the semantic segmentation and fine-grained recognition\ntasks, SemMAE also brings significant improvements and yields the\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Heliang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation. (arXiv:2206.10892v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10892","description":"<p>In this paper, we present the Intra- and Inter-Human Relation Networks\n(I^2R-Net) for Multi-Person Pose Estimation. It involves two basic modules.\nFirst, the Intra-Human Relation Module operates on a single person and aims to\ncapture Intra-Human dependencies. Second, the Inter-Human Relation Module\nconsiders the relation between multiple instances and focuses on capturing\nInter-Human interactions. The Inter-Human Relation Module can be designed very\nlightweight by reducing the resolution of feature map, yet learn useful\nrelation information to significantly boost the performance of the Intra-Human\nRelation Module. Even without bells and whistles, our method can compete or\noutperform current competition winners. We conduct extensive experiments on\nCOCO, CrowdPose, and OCHuman datasets. The results demonstrate that the\nproposed model surpasses all the state-of-the-art methods. Concretely, the\nproposed method achieves 77.4% AP on CrowPose dataset and 67.8% AP on OCHuman\ndataset respectively, outperforming existing methods by a large margin.\nAdditionally, the ablation study and visualization analysis also prove the\neffectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yiwei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Wenjin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinglin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer. (arXiv:2206.11053v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11053","description":"<p>Visual question answering (VQA) in surgery is largely unexplored. Expert\nsurgeons are scarce and are often overloaded with clinical and academic\nworkloads. This overload often limits their time answering questionnaires from\npatients, medical students or junior residents related to surgical procedures.\nAt times, students and junior residents also refrain from asking too many\nquestions during classes to reduce disruption. While computer-aided simulators\nand recording of past surgical procedures have been made available for them to\nobserve and improve their skills, they still hugely rely on medical experts to\nanswer their questions. Having a Surgical-VQA system as a reliable 'second\nopinion' could act as a backup and ease the load on the medical experts in\nanswering these questions. The lack of annotated medical data and the presence\nof domain-specific terms has limited the exploration of VQA for surgical\nprocedures. In this work, we design a Surgical-VQA task that answers\nquestionnaires on surgical procedures based on the surgical scene. Extending\nthe MICCAI endoscopic vision challenge 2018 dataset and workflow recognition\ndataset further, we introduce two Surgical-VQA datasets with classification and\nsentence-based answers. To perform Surgical-VQA, we employ vision-text\ntransformers models. We further introduce a residual MLP-based VisualBert\nencoder model that enforces interaction between visual and text tokens,\nimproving performance in classification-based answering. Furthermore, we study\nthe influence of the number of input image patches and temporal visual features\non the model performance in both classification and sentence-based answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seenivasan_L/0/1/0/all/0/1\">Lalithkumar Seenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Adithya K Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation. (arXiv:2206.11474v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11474","description":"<p>Denoising Diffusion Probabilistic Model (DDPM) is able to make flexible\nconditional image generation from prior noise to real data, by introducing an\nindependent noise-aware classifier to provide conditional gradient guidance at\neach time step of denoising process. However, due to the ability of classifier\nto easily discriminate an incompletely generated image only with high-level\nstructure, the gradient, which is a kind of class information guidance, tends\nto vanish early, leading to the collapse from conditional generation process\ninto the unconditional process. To address this problem, we propose two simple\nbut effective approaches from two perspectives. For sampling procedure, we\nintroduce the entropy of predicted distribution as the measure of guidance\nvanishing level and propose an entropy-aware scaling method to adaptively\nrecover the conditional semantic guidance. For training stage, we propose the\nentropy-aware optimization objectives to alleviate the overconfident prediction\nfor noisy data.On ImageNet1000 256x256, with our proposed sampling scheme and\ntrained classifier, the pretrained conditional and unconditional DDPM model can\nachieve 10.89% (4.59 to 4.09) and 43.5% (12 to 6.78) FID improvement\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guangcong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shoudong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel adversarial learning strategy for medical image classification. (arXiv:2206.11501v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.11501","description":"<p>Deep learning (DL) techniques have been extensively utilized for medical\nimage classification. Most DL-based classification networks are generally\nstructured hierarchically and optimized through the minimization of a single\nloss function measured at the end of the networks. However, such a single loss\ndesign could potentially lead to optimization of one specific value of interest\nbut fail to leverage informative features from intermediate layers that might\nbenefit classification performance and reduce the risk of overfitting.\nRecently, auxiliary convolutional neural networks (AuxCNNs) have been employed\non top of traditional classification networks to facilitate the training of\nintermediate layers to improve classification performance and robustness. In\nthis study, we proposed an adversarial learning-based AuxCNN to support the\ntraining of deep neural networks for medical image classification. Two main\ninnovations were adopted in our AuxCNN classification framework. First, the\nproposed AuxCNN architecture includes an image generator and an image\ndiscriminator for extracting more informative image features for medical image\nclassification, motivated by the concept of generative adversarial network\n(GAN) and its impressive ability in approximating target data distribution.\nSecond, a hybrid loss function is designed to guide the model training by\nincorporating different objectives of the classification network and AuxCNN to\nreduce overfitting. Comprehensive experimental studies demonstrated the\nsuperior classification performance of the proposed model. The effect of the\nnetwork-related factors on classification performance was investigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_Z/0/1/0/all/0/1\">Zong Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gasienica_J/0/1/0/all/0/1\">Jacob A. Gasienica</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Potts_J/0/1/0/all/0/1\">Jennifer Potts</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thorstad_W/0/1/0/all/0/1\">Wade Thorstad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gay_H/0/1/0/all/0/1\">Hiram Gay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaowei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022). (arXiv:2206.11610v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11610","description":"<p>This report presents the methods of the winning entry of the RxR-Habitat\nCompetition in CVPR 2022. The competition addresses the problem of\nVision-and-Language Navigation in Continuous Environments (VLN-CE), which\nrequires an agent to follow step-by-step natural language instructions to reach\na target. We present a modular plan-and-control approach for the task. Our\nmodel consists of three modules: the candidate waypoints predictor (CWP), the\nhistory enhanced planner and the tryout controller. In each decision loop, CWP\nfirst predicts a set of candidate waypoints based on depth observations from\nmultiple views. It can reduce the complexity of the action space and facilitate\nplanning. Then, a history-enhanced planner is adopted to select one of the\ncandidate waypoints as the subgoal. The planner additionally encodes historical\nmemory to track the navigation progress, which is especially effective for\nlong-horizon navigation. Finally, we propose a non-parametric heuristic\ncontroller named tryout to execute low-level actions to reach the planned\nsubgoal. It is based on the trial-and-error mechanism which can help the agent\nto avoid obstacles and escape from getting stuck. All three modules work\nhierarchically until the agent stops. We further take several recent advances\nof Vision-and-Language Navigation (VLN) to improve the performance such as\npretraining based on large-scale synthetic in-domain dataset, environment-level\ndata augmentation and snapshot model ensemble. Our model won the RxR-Habitat\nCompetition 2022, with 48% and 90% relative improvements over existing methods\non NDTW and SR metrics respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1\">Dong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidence fusion with contextual discounting for multi-modality medical image segmentation. (arXiv:2206.11739v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11739","description":"<p>As information sources are usually imperfect, it is necessary to take into\naccount their reliability in multi-source information fusion tasks. In this\npaper, we propose a new deep framework allowing us to merge multi-MR image\nsegmentation results using the formalism of Dempster-Shafer theory while taking\ninto account the reliability of different modalities relative to different\nclasses. The framework is composed of an encoder-decoder feature extraction\nmodule, an evidential segmentation module that computes a belief function at\neach voxel for each modality, and a multi-modality evidence fusion module,\nwhich assigns a vector of discount rates to each modality evidence and combines\nthe discounted evidence using Dempster's rule. The whole framework is trained\nby minimizing a new loss function based on a discounted Dice index to increase\nsegmentation accuracy and reliability. The method was evaluated on the BraTs\n2021 database of 1251 patients with brain tumors. Quantitative and qualitative\nresults show that our method outperforms the state of the art, and implements\nan effective new idea for merging multi-information within deep neural\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Ling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoeux_T/0/1/0/all/0/1\">Thierry Denoeux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1\">Pierre Vera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need. (arXiv:2206.11804v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11804","description":"<p>Data diversity and volume are crucial to the success of training deep\nlearning models, while in the medical imaging field, the difficulty and cost of\ndata collection and annotation are especially huge. Specifically in robotic\nsurgery, data scarcity and imbalance have heavily affected the model accuracy\nand limited the design and deployment of deep learning-based surgical\napplications such as surgical instrument segmentation. Considering this, in\nthis paper, we rethink the surgical instrument segmentation task and propose a\none-to-many data generation solution that gets rid of the complicated and\nexpensive process of data collection and annotation from robotic surgery. In\nour method, we only utilize a single surgical background tissue image and a few\nopen-source instrument images as the seed images and apply multiple\naugmentations and blending techniques to synthesize amounts of image\nvariations. In addition, we also introduce the chained augmentation mixing\nduring training to further enhance the data diversities. The proposed approach\nis evaluated on the real datasets of the EndoVis-2018 and EndoVis-2017 surgical\nscene segmentation. Our empirical analysis suggests that without the high cost\nof data collection and annotation, we can achieve decent surgical instrument\nsegmentation performance. Moreover, we also observe that our method can deal\nwith novel instrument prediction in the deployment domain. We hope our\ninspiring results will encourage researchers to emphasize data-centric methods\nto overcome demanding deep learning limitations besides data shortage, such as\nclass imbalance, domain adaptation, and incremental learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">An Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengya Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QReg: On Regularization Effects of Quantization. (arXiv:2206.12372v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12372","description":"<p>In this paper we study the effects of quantization in DNN training. We\nhypothesize that weight quantization is a form of regularization and the amount\nof regularization is correlated with the quantization level (precision). We\nconfirm our hypothesis by providing analytical study and empirical results. By\nmodeling weight quantization as a form of additive noise to weights, we explore\nhow this noise propagates through the network at training time. We then show\nthat the magnitude of this noise is correlated with the level of quantization.\nTo confirm our analytical study, we performed an extensive list of experiments\nsummarized in this paper in which we show that the regularization effects of\nquantization can be seen in various vision tasks and models, over various\ndatasets. Based on our study, we propose that 8-bit quantization provides a\nreliable form of regularization in different vision tasks and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AskariHemmat_M/0/1/0/all/0/1\">MohammadHossein AskariHemmat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemmat_R/0/1/0/all/0/1\">Reyhane Askari Hemmat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_A/0/1/0/all/0/1\">Alex Hoffman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazarevich_I/0/1/0/all/0/1\">Ivan Lazarevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saboori_E/0/1/0/all/0/1\">Ehsan Saboori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastropietro_O/0/1/0/all/0/1\">Olivier Mastropietro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savaria_Y/0/1/0/all/0/1\">Yvon Savaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_J/0/1/0/all/0/1\">Jean-Pierre David</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Driven Stylization of Video Objects. (arXiv:2206.12396v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12396","description":"<p>We tackle the task of stylizing video objects in an intuitive and semantic\nmanner following a user-specified text prompt. This is a challenging task as\nthe resulting video must satisfy multiple properties: (1) it has to be\ntemporally consistent and avoid jittering or similar artifacts, (2) the\nresulting stylization must preserve both the global semantics of the object and\nits fine-grained details, and (3) it must adhere to the user-specified text\nprompt. To this end, our method stylizes an object in a video according to two\ntarget texts. The first target text prompt describes the global semantics and\nthe second target text prompt describes the local semantics. To modify the\nstyle of an object, we harness the representational power of CLIP to get a\nsimilarity score between (1) the local target text and a set of local stylized\nviews, and (2) a global target text and a set of stylized global views. We use\na pretrained atlas decomposition network to propagate the edits in a temporally\nconsistent manner. We demonstrate that our method can generate consistent style\nchanges over time for a variety of objects and videos, that adhere to the\nspecification of the target texts. We also show how varying the specificity of\nthe target texts and augmenting the texts with a set of prefixes results in\nstylizations with different levels of detail. Full results are given on our\nproject webpage:\nhttps://sloeschcke.github.io/Text-Driven-Stylization-of-Video-Objects/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loeschcke_S/0/1/0/all/0/1\">Sebastian Loeschcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counting Phases and Faces Using Bayesian Thermodynamic Integration. (arXiv:2206.07494v1 [cond-mat.stat-mech] CROSS LISTED)","link":"http://arxiv.org/abs/2206.07494","description":"<p>We introduce a new approach to reconstruction of the thermodynamic functions\nand phase boundaries in two-parametric statistical mechanics systems. Our\nmethod is based on expressing the Fisher metric in terms of the posterior\ndistributions over a space of external parameters and approximating the metric\nfield by a Hessian of a convex function. We use the proposed approach to\naccurately reconstruct the partition functions and phase diagrams of the Ising\nmodel and the exactly solvable non-equilibrium TASEP without any a priori\nknowledge about microscopic rules of the models. We also demonstrate how our\napproach can be used to visualize the latent space of StyleGAN models and\nevaluate the variability of the generated images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Lobashev_A/0/1/0/all/0/1\">Alexander Lobashev</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Tamm_M/0/1/0/all/0/1\">Mikhail V. Tamm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sense The Physical, Walkthrough The Virtual, Manage The Metaverse: A Data-centric Perspective. (arXiv:2206.10326v1 [cs.HC] CROSS LISTED)","link":"http://arxiv.org/abs/2206.10326","description":"<p>In the Metaverse, the physical space and the virtual space co-exist, and\ninteract simultaneously. While the physical space is virtually enhanced with\ninformation, the virtual space is continuously refreshed with real-time,\nreal-world information. To allow users to process and manipulate information\nseamlessly between the real and digital spaces, novel technologies must be\ndeveloped. These include smart interfaces, new augmented realities, efficient\nstorage and data management and dissemination techniques. In this paper, we\nfirst discuss some promising co-space applications. These applications offer\nexperiences and opportunities that neither of the spaces can realize on its\nown. We then argue that the database community has much to offer to this field.\nFinally, we present several challenges that we, as a community, can contribute\ntowards managing the Metaverse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1\">Beng Chin Ooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kian-Lee Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_A/0/1/0/all/0/1\">Anthony Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiaokui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}