{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"When classifying grammatical role, BERT doesn't care about word order... except when it matters. (arXiv:2203.06204v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06204","description":"<p>Because meaning can often be inferred from lexical semantics alone, word\norder is often a redundant cue in natural language. For example, the words\nchopped, chef, and onion are more likely used to convey \"The chef chopped the\nonion,\" not \"The onion chopped the chef.\" Recent work has shown large language\nmodels to be surprisingly word order invariant, but crucially has largely\nconsidered natural prototypical inputs, where compositional meaning mostly\nmatches lexical expectations. To overcome this confound, we probe grammatical\nrole representation in English BERT and GPT-2, on instances where lexical\nexpectations are not sufficient, and word order knowledge is necessary for\ncorrect classification. Such non-prototypical instances are naturally occurring\nEnglish sentences with inanimate subjects or animate objects, or sentences\nwhere we systematically swap the arguments to make sentences like \"The onion\nchopped the chef\". We find that, while early layer embeddings are largely\nlexical, word order is in fact crucial in defining the later-layer\nrepresentations of words in semantically non-prototypical positions. Our\nexperiments isolate the effect of word order on the contextualization process,\nand highlight how models use context in the uncommon, but critical, instances\nwhere it matters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1\">Isabel Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Futrell_R/0/1/0/all/0/1\">Richard Futrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Staged Training for Transformer Language Models. (arXiv:2203.06211v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06211","description":"<p>The current standard approach to scaling transformer language models trains\neach model size from a different random initialization. As an alternative, we\nconsider a staged training setup that begins with a small model and\nincrementally increases the amount of compute used for training by applying a\n\"growth operator\" to increase the model depth and width. By initializing each\nstage with the output of the previous one, the training process effectively\nre-uses the compute from prior stages and becomes more efficient. Our growth\noperators each take as input the entire training state (including model\nparameters, optimizer state, learning rate schedule, etc.) and output a new\ntraining state from which training continues. We identify two important\nproperties of these growth operators, namely that they preserve both the loss\nand the \"training dynamics\" after applying the operator. While the\nloss-preserving property has been discussed previously, to the best of our\nknowledge this work is the first to identify the importance of preserving the\ntraining dynamics (the rate of decrease of the loss during training). To find\nthe optimal schedule for stages, we use the scaling laws from (Kaplan et al.,\n2020) to find a precise schedule that gives the most compute saving by starting\na new stage when training efficiency starts decreasing. We empirically validate\nour growth operators and staged training for autoregressive language models,\nshowing up to 22% compute savings compared to a strong baseline trained from\nscratch. Our code is available at https://github.com/allenai/staged-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walsh_P/0/1/0/all/0/1\">Pete Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment. (arXiv:2203.06228v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06228","description":"<p>Pretrained language models (PLMs) have achieved superhuman performance on\nmany benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context\nDefinition Alignment), a challenging benchmark that measures natural language\nunderstanding (NLU) capabilities of PLMs: Given a definition and a context each\nfor k words, but not the words themselves, the task is to align the k\ndefinitions with the k contexts. CoDA21 requires a deep understanding of\ncontexts and definitions, including complex inference and world knowledge. We\nfind that there is a large gap between human and PLM performance, suggesting\nthat CoDA21 measures an aspect of NLU that is not sufficiently covered in\nexisting benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Senel_L/0/1/0/all/0/1\">L&#xfc;tfi Kerem Senel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI agents for facilitating social interactions and wellbeing. (arXiv:2203.06244v1 [cs.CY])","link":"http://arxiv.org/abs/2203.06244","description":"<p>Wellbeing AI has been becoming a new trend in individuals' mental health,\norganizational health, and flourishing our societies. Various applications of\nwellbeing AI have been introduced to our daily lives. While social\nrelationships within groups are a critical factor for wellbeing, the\ndevelopment of wellbeing AI for social interactions remains relatively scarce.\nIn this paper, we provide an overview of the mediative role of AI-augmented\nagents for social interactions. First, we discuss the two-dimensional framework\nfor classifying wellbeing AI: individual/group and analysis/intervention.\nFurthermore, wellbeing AI touches on intervening social relationships between\nhuman-human interactions since positive social relationships are key to human\nwellbeing. This intervention may raise technical and ethical challenges. We\ndiscuss opportunities and challenges of the relational approach with wellbeing\nAI to promote wellbeing in our societies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamada_H/0/1/0/all/0/1\">Hiro Taiyo Hamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanai_R/0/1/0/all/0/1\">Ryota Kanai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Inference with A Chinese Entailment Graph. (arXiv:2203.06264v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06264","description":"<p>Predicate entailment detection is a crucial task for question-answering from\ntext, where previous work has explored unsupervised learning of entailment\ngraphs from typed open relation triples. In this paper, we present the first\npipeline for building Chinese entailment graphs, which involves a novel\nhigh-recall open relation extraction (ORE) method and the first Chinese\nfine-grained entity typing dataset under the FIGER type ontology. Through\nexperiments on the Levy-Holt dataset, we verify the strength of our Chinese\nentailment graph, and reveal the cross-lingual complementarity: on the parallel\nLevy-Holt dataset, an ensemble of Chinese and English entailment graphs\noutperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC\npoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1\">Sabine Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mohammad Javad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Topic Modeling with Deep Mutual Information Estimation. (arXiv:2203.06298v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06298","description":"<p>The emerging neural topic models make topic modeling more easily adaptable\nand extendable in unsupervised text mining. However, the existing neural topic\nmodels is difficult to retain representative information of the documents\nwithin the learnt topic representation. In this paper, we propose a neural\ntopic model which incorporates deep mutual information estimation, i.e., Neural\nTopic Modeling with Deep Mutual Information Estimation(NTM-DMIE). NTM-DMIE is a\nneural network method for topic learning which maximizes the mutual information\nbetween the input documents and their latent topic representation. To learn\nrobust topic representation, we incorporate the discriminator to discriminate\nnegative examples and positive examples via adversarial learning. Moreover, we\nuse both global and local mutual information to preserve the rich information\nof the input documents in the topic representation. We evaluate NTM-DMIE on\nseveral metrics, including accuracy of text clustering, with topic\nrepresentation, topic uniqueness and topic coherence. Compared to the existing\nmethods, the experimental results show that NTM-DMIE can outperform in all the\nmetrics on the four datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoqiu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongtong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Ning Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zheng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Semi-supervised Entity Alignment via Cycle-teaching. (arXiv:2203.06308v1 [cs.AI])","link":"http://arxiv.org/abs/2203.06308","description":"<p>Entity alignment is to find identical entities in different knowledge graphs.\nAlthough embedding-based entity alignment has recently achieved remarkable\nprogress, training data insufficiency remains a critical challenge.\nConventional semi-supervised methods also suffer from the incorrect entity\nalignment in newly proposed training data. To resolve these issues, we design\nan iterative cycle-teaching framework for semi-supervised entity alignment. The\nkey idea is to train multiple entity alignment models (called aligners)\nsimultaneously and let each aligner iteratively teach its successor the\nproposed new entity alignment. We propose a diversity-aware alignment selection\nmethod to choose reliable entity alignment for each aligner. We also design a\nconflict resolution mechanism to resolve the alignment conflict when combining\nthe new alignment of an aligner and that from its teacher. Besides, considering\nthe influence of cycle-teaching order, we elaborately design a strategy to\narrange the optimal order that can maximize the overall performance of multiple\naligners. The cycle-teaching process can break the limitations of each model's\nlearning capability and reduce the noise in new training data, leading to\nimproved performance. Extensive experiments on benchmark datasets demonstrate\nthe effectiveness of the proposed cycle-teaching framework, which significantly\noutperforms the state-of-the-art models when the training data is insufficient\nand the new entity alignment has much noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_K/0/1/0/all/0/1\">Kexuan Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaofang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELLE: Efficient Lifelong Pre-training for Emerging Data. (arXiv:2203.06311v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06311","description":"<p>Current pre-trained language models (PLM) are typically trained with static\ndata, ignoring that in real-world scenarios, streaming data of various sources\nmay continuously grow. This requires PLMs to integrate the information from all\nthe sources in a lifelong manner. Although this goal could be achieved by\nexhaustive pre-training on all the existing data, such a process is known to be\ncomputationally expensive. To this end, we propose ELLE, aiming at efficient\nlifelong pre-training for emerging data. Specifically, ELLE consists of (1)\nfunction preserved model expansion, which flexibly expands an existing PLM's\nwidth and depth to improve the efficiency of knowledge acquisition; and (2)\npre-trained domain prompts, which disentangle the versatile knowledge learned\nduring pre-training and stimulate the proper knowledge for downstream tasks. We\nexperiment ELLE with streaming data from 5 domains on BERT and GPT. The results\nshow the superiority of ELLE over various lifelong learning baselines in both\npre-training efficiency and downstream performances. The codes are publicly\navailable at https://github.com/thunlp/ELLE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Equal Opportunity Fairness through Adversarial Learning. (arXiv:2203.06317v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06317","description":"<p>Adversarial training is a common approach for bias mitigation in natural\nlanguage processing. Although most work on debiasing is motivated by equal\nopportunity, it is not explicitly captured in standard adversarial training. In\nthis paper, we propose an augmented discriminator for adversarial training,\nwhich takes the target class as input to create richer features and more\nexplicitly model equal opportunity. Experimental results over two datasets show\nthat our method substantially improves over standard adversarial debiasing\nmethods, in terms of the performance--fairness trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Reading Comprehension Questions Difficult?. (arXiv:2203.06342v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06342","description":"<p>For a natural language understanding benchmark to be useful in research, it\nhas to consist of examples that are diverse and difficult enough to\ndiscriminate among current and near-future state-of-the-art systems. However,\nwe do not yet know how best to select text sources to collect a variety of\nchallenging examples. In this study, we crowdsource multiple-choice reading\ncomprehension questions for passages taken from seven qualitatively distinct\nsources, analyzing what attributes of passages contribute to the difficulty and\nquestion types of the collected examples. To our surprise, we find that passage\nsource, length, and readability measures do not significantly affect question\ndifficulty. Through our manual annotation of seven reasoning types, we observe\nseveral trends between passage sources and reasoning types, e.g., logical\nreasoning is more often required in questions written for technical passages.\nThese results suggest that when creating a new benchmark dataset, selecting a\ndiverse set of passages can help ensure a diverse range of question types, but\nthat passage difficulty need not be a priority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nangia_N/0/1/0/all/0/1\">Nikita Nangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarkBERT: Marking Word Boundaries Improves Chinese BERT. (arXiv:2203.06378v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06378","description":"<p>We present a Chinese BERT model dubbed MarkBERT that uses word information.\nExisting word-based BERT models regard words as basic units, however, due to\nthe vocabulary limit of BERT, they only cover high-frequency words and fall\nback to character level when encountering out-of-vocabulary (OOV) words.\nDifferent from existing works, MarkBERT keeps the vocabulary being Chinese\ncharacters and inserts boundary markers between contiguous words. Such design\nenables the model to handle any words in the same way, no matter they are OOV\nwords or not. Besides, our model has two additional benefits: first, it is\nconvenient to add word-level learning objectives over markers, which is\ncomplementary to traditional character and sentence-level pre-training tasks;\nsecond, it can easily incorporate richer semantics such as POS tags of words by\nreplacing generic markers with POS tag-specific markers. MarkBERT pushes the\nstate-of-the-art of Chinese named entity recognition from 95.4\\% to 96.5\\% on\nthe MSRA dataset and from 82.8\\% to 84.2\\% on the OntoNotes dataset,\nrespectively. Compared to previous word-based BERT models, MarkBERT achieves\nbetter accuracy on text classification, keyword recognition, and semantic\nsimilarity tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06386","description":"<p>The recent large-scale vision-language pre-training (VLP) of dual-stream\narchitectures (e.g., CLIP) with a tremendous amount of image-text pair data,\nhas shown its superiority on various multimodal alignment tasks. Despite its\nsuccess, the resulting models are not capable of multimodal generative tasks\ndue to the weak text encoder. To tackle this problem, we propose to augment the\ndual-stream VLP model with a textual pre-trained language model (PLM) via\nvision-language knowledge distillation (VLKD), enabling the capability for\nmultimodal generation. VLKD is pretty data- and computation-efficient compared\nto the pre-training from scratch. Experimental results show that the resulting\nmodel has strong zero-shot performance on multimodal generation tasks, such as\nopen-ended visual question answering and image captioning. For example, it\nachieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous\nstate-of-the-art zero-shot model with $7\\times$ fewer parameters. Furthermore,\nthe original textual language understanding and generation ability of the PLM\nis maintained after VLKD, which makes our model versatile for both multimodal\nand unimodal tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiBERT: Accurate Fully Binarized BERT. (arXiv:2203.06390v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06390","description":"<p>The large pre-trained BERT has achieved remarkable performance on Natural\nLanguage Processing (NLP) tasks but is also computation and memory expensive.\nAs one of the powerful compression approaches, binarization extremely reduces\nthe computation and memory consumption by utilizing 1-bit parameters and\nbitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit\nweight, embedding, and activation) usually suffer a significant performance\ndrop, and there is rare study addressing this problem. In this paper, with the\ntheoretical justification and empirical analysis, we identify that the severe\nperformance drop can be mainly attributed to the information degradation and\noptimization direction mismatch respectively in the forward and backward\npropagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate\nthe performance bottlenecks. Specifically, BiBERT introduces an efficient\nBi-Attention structure for maximizing representation information statistically\nand a Direction-Matching Distillation (DMD) scheme to optimize the full\nbinarized BERT accurately. Extensive experiments show that BiBERT outperforms\nboth the straightforward baseline and existing state-of-the-art quantized BERTs\nwith ultra-low bit activations by convincing margins on the NLP benchmark. As\nthe first fully binarized BERT, our method yields impressive 56.3 times and\n31.2 times saving on FLOPs and model size, demonstrating the vast advantages\nand potential of the fully binarized BERT model in real-world\nresource-constrained scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qinghua Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Q/0/1/0/all/0/1\">Qingqing Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A combined approach to the analysis of speech conversations in a contact center domain. (arXiv:2203.06396v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06396","description":"<p>The ever more accurate search for deep analysis in customer data is a really\nstrong technological trend nowadays, quite appealing to both private and public\ncompanies. This is particularly true in the contact center domain, where speech\nanalytics is an extremely powerful methodology for gaining insights from\nunstructured data, coming from customer and human agent conversations. In this\nwork, we describe an experimentation with a speech analytics process for an\nItalian contact center, that deals with call recordings extracted from inbound\nor outbound flows. First, we illustrate in detail the development of an\nin-house speech-to-text solution, based on Kaldi framework, and evaluate its\nperformance (and compare it to Google Cloud Speech API). Then, we evaluate and\ncompare different approaches to the semantic tagging of call transcripts,\nranging from classic regular expressions to machine learning models based on\nngrams and logistic regression, and propose a combination of them, which is\nshown to provide a consistent benefit. Finally, a decision tree inducer, called\nJ48S, is applied to the problem of tagging. Such an algorithm is natively\ncapable of exploiting sequential data, such as texts, for classification\npurposes. The solution is compared with the other approaches and is shown to\nprovide competitive classification performances, while generating highly\ninterpretable models and reducing the complexity of the data preparation phase.\nThe potential operational impact of the whole process is thoroughly examined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brunello_A/0/1/0/all/0/1\">Andrea Brunello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzano_E/0/1/0/all/0/1\">Enrico Marzano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1\">Angelo Montanari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sciavicco_G/0/1/0/all/0/1\">Guido Sciavicco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Proposal to Study \"Is High Quality Data All We Need?\". (arXiv:2203.06404v1 [cs.LG])","link":"http://arxiv.org/abs/2203.06404","description":"<p>Even though deep neural models have achieved superhuman performance on many\npopular benchmarks, they have failed to generalize to OOD or adversarial\ndatasets. Conventional approaches aimed at increasing robustness include\ndeveloping increasingly large models and augmentation with large scale\ndatasets. However, orthogonal to these trends, we hypothesize that a smaller,\nhigh quality dataset is what we need. Our hypothesis is based on the fact that\ndeep neural networks are data driven models, and data is what leads/misleads\nmodels. In this work, we propose an empirical study that examines how to select\na subset of and/or create high quality benchmark data, for a model to learn\neffectively. We seek to answer if big datasets are truly needed to learn a\ntask, and whether a smaller subset of high quality data can replace big\ndatasets. We plan to investigate both data pruning and data creation paradigms\nto generate high quality datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">Anjana Arunkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey in Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06414","description":"<p>In recent years, it has been seen that deep neural networks are lacking\nrobustness and are likely to break in case of adversarial perturbations in\ninput data. Strong adversarial attacks are proposed by various authors for\ncomputer vision and Natural Language Processing (NLP). As a counter-effort,\nseveral defense mechanisms are also proposed to save these networks from\nfailing. In contrast with image data, generating adversarial attacks and\ndefending these models is not easy in NLP because of the discrete nature of the\ntext data. However, numerous methods for adversarial defense are proposed of\nlate, for different NLP tasks such as text classification, named entity\nrecognition, natural language inferencing, etc. These methods are not just used\nfor defending neural networks from adversarial attacks, but also used as a\nregularization mechanism during training, saving the model from overfitting.\nThe proposed survey is an attempt to review different methods proposed for\nadversarial defenses in NLP in the recent past by proposing a novel taxonomy.\nThis survey also highlights the fragility of the advanced deep neural networks\nin NLP and the challenges in defending them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1\">Shreya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M.Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1\">Balaraman Ravindran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues. (arXiv:2203.06419v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06419","description":"<p>Indirect speech such as sarcasm achieves a constellation of discourse goals\nin human communication. While the indirectness of figurative language warrants\nspeakers to achieve certain pragmatic goals, it is challenging for AI agents to\ncomprehend such idiosyncrasies of human communication. Though sarcasm\nidentification has been a well-explored topic in dialogue analysis, for\nconversational systems to truly grasp a conversation's innate meaning and\ngenerate appropriate responses, simply detecting sarcasm is not enough; it is\nvital to explain its underlying sarcastic connotation to capture its true\nessence. In this work, we study the discourse structure of sarcastic\nconversations and propose a novel task - Sarcasm Explanation in Dialogue (SED).\nSet in a multimodal and code-mixed setting, the task aims to generate natural\nlanguage explanations of satirical conversations. To this end, we curate WITS,\na new dataset to support our task. We propose MAF (Modality Aware Fusion), a\nmultimodal context-aware attention and global information fusion module to\ncapture multimodality and use it to benchmark WITS. The proposed attention\nmodule surpasses the traditional multimodal fusion baselines and reports the\nbest performance on almost all metrics. Lastly, we carry out detailed analyses\nboth quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shivani Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Atharva Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice. (arXiv:2203.06462v1 [cs.LG])","link":"http://arxiv.org/abs/2203.06462","description":"<p>Classifiers in natural language processing (NLP) often have a large number of\noutput classes. For example, neural language models (LMs) and machine\ntranslation (MT) models both predict tokens from a vocabulary of thousands. The\nSoftmax output layer of these models typically receives as input a dense\nfeature representation, which has much lower dimensionality than the output. In\ntheory, the result is some words may be impossible to be predicted via argmax,\nirrespective of input features, and empirically, there is evidence this happens\nin small language models. In this paper we ask whether it can happen in\npractical large language models and translation models. To do so, we develop\nalgorithms to detect such \\emph{unargmaxable} tokens in public models. We find\nthat 13 out of 150 models do indeed have such tokens; however, they are very\ninfrequent and unlikely to impact model quality. We release our algorithms and\ncode to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grivas_A/0/1/0/all/0/1\">Andreas Grivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Adam Lopez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging. (arXiv:2203.06482v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06482","description":"<p>Publicly traded companies are required to submit periodic reports with\neXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging\nthe reports is tedious and costly. We, therefore, introduce XBRL tagging as a\nnew entity extraction task for the financial domain and release FiNER-139, a\ndataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction\ndatasets, FiNER-139 uses a much larger label set of 139 entity types. Most\nannotated tokens are numeric, with the correct tag per token depending mostly\non context, rather than the token itself. We show that subword fragmentation of\nnumeric expressions harms BERT's performance, allowing word-level BILSTMs to\nperform better. To improve BERT's performance, we propose two simple and\neffective solutions that replace numeric expressions with pseudo-tokens\nreflecting original token shapes and numeric magnitudes. We also experiment\nwith FIN-BERT, an existing BERT model for the financial domain, and release our\nown BERT (SEC-BERT), pre-trained on financial filings, which performs best.\nThrough data and error analysis, we finally identify possible limitations to\ninspire future work on XBRL tagging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1\">Lefteris Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spyropoulou_E/0/1/0/all/0/1\">Eirini Spyropoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1\">Prodromos Malakasiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chart-to-Text: A Large-Scale Benchmark for Chart Summarization. (arXiv:2203.06486v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06486","description":"<p>Charts are commonly used for exploring data and communicating insights.\nGenerating natural language summaries from charts can be very helpful for\npeople in inferring key insights that would otherwise require a lot of\ncognitive and perceptual efforts. We present Chart-to-text, a large-scale\nbenchmark with two datasets and a total of 44,096 charts covering a wide range\nof topics and chart types. We explain the dataset construction process and\nanalyze the datasets. We also introduce a number of state-of-the-art neural\nmodels as baselines that utilize image captioning and data-to-text generation\ntechniques to tackle two problem variations: one assumes the underlying data\ntable of the chart is available while the other needs to extract data from\nchart images. Our analysis with automatic and human evaluation shows that while\nour best models usually generate fluent summaries and yield reasonable BLEU\nscores, they also suffer from hallucinations and factual errors as well as\ndifficulties in correctly explaining complex patterns and trends in charts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanthara_S/0/1/0/all/0/1\">Shankar Kanthara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_R/0/1/0/all/0/1\">Rixie Tiffany Ko Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Megh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Information Hiding in Natural Language Systems. (arXiv:2203.06512v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06512","description":"<p>With data privacy becoming more of a necessity than a luxury in today's\ndigital world, research on more robust models of privacy preservation and\ninformation security is on the rise. In this paper, we take a look at Natural\nLanguage Steganography (NLS) methods, which perform information hiding in\nnatural language systems, as a means to achieve data security as well as\nconfidentiality. We summarize primary challenges regarding the secrecy and\nimperceptibility requirements of these systems and propose potential directions\nof improvement, specifically targeting steganographic text quality. We believe\nthat this study will act as an appropriate framework to build more resilient\nmodels of Natural Language Steganography, working towards instilling security\nwithin natural language-based neural models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bihani_G/0/1/0/all/0/1\">Geetanjali Bihani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1\">Julia Taylor Rayz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization. (arXiv:2203.06569v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06569","description":"<p>Sequence-to-sequence neural networks have recently achieved great success in\nabstractive summarization, especially through fine-tuning large pre-trained\nlanguage models on the downstream dataset. These models are typically decoded\nwith beam search to generate a unique summary. However, the search space is\nvery large, and with the exposure bias, such decoding is not optimal. In this\npaper, we show that it is possible to directly train a second-stage model\nperforming re-ranking on a set of summary candidates. Our mixture-of-experts\nSummaReranker learns to select a better candidate and consistently improves the\nperformance of the base model. With a base PEGASUS, we push ROUGE scores by\n5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34%\non Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and\ncheckpoints will be available at https://github.com/ntunlp/SummaReranker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravaut_M/0/1/0/all/0/1\">Mathieu Ravaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Impact of COVID-19 on Education by Social Network Mining. (arXiv:2203.06584v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06584","description":"<p>The Covid-19 virus has been one of the most discussed topics on social\nnetworks in 2020 and 2021 and has affected the classic educational paradigm,\nworldwide. In this research, many tweets related to the Covid-19 virus and\neducation are considered and geo-tagged with the help of the GeoNames\ngeographic database, which contains a large number of place names. To detect\nthe feeling of users, sentiment analysis is performed using the RoBERTa\nlanguage-based model. Finally, we obtain the trends of frequency of total,\npositive, and negative tweets for countries with a high number of Covid-19\nconfirmed cases. Investigating the results reveals a correlation between the\ntrends of tweet frequency and the official statistic of confirmed cases for\nseveral countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jamalian_M/0/1/0/all/0/1\">Mohadese Jamalian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_Nejad_H/0/1/0/all/0/1\">Hamed Vahdat-Nejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiabadi_H/0/1/0/all/0/1\">Hamideh Hajiabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informative Causality Extraction from Medical Literature via Dependency-tree based Patterns. (arXiv:2203.06592v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06592","description":"<p>Extracting cause-effect entities from medical literature is an important task\nin medical information retrieval. A solution for solving this task can be used\nfor compilation of various causality relations, such as, causality between\ndisease and symptoms, between medications and side effects, between genes and\ndiseases, etc. Existing solutions for extracting cause-effect entities work\nwell for sentences where the cause and the effect phrases are name entities,\nsingle-word nouns, or noun phrases consisting of two to three words.\nUnfortunately, in medical literature, cause and effect phrases in a sentence\nare not simply nouns or noun phrases, rather they are complex phrases\nconsisting of several words, and existing methods fail to correctly extract the\ncause and effect entities in such sentences. Partial extraction of cause and\neffect entities conveys poor quality, non informative, and often, contradictory\nfacts, comparing to the one intended in the given sentence. In this work, we\nsolve this problem by designing an unsupervised method for cause and effect\nphrase extraction, PatternCausality, which is specifically suitable for the\nmedical literature. Our proposed approach first uses a collection of\ncause-effect dependency patterns as template to extract head words of cause and\neffect phrases and then it uses a novel phrase extraction method to obtain\ncomplete and meaningful cause and effect phrases from a sentence. Experiments\non a cause-effect dataset built from sentences from PubMed articles show that\nfor extracting cause and effect entities, PatternCausality is substantially\nbetter than the existing methods with an order of magnitude improvement in the\nF-score metric over the best of the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md. Ahsanul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almulhim_A/0/1/0/all/0/1\">AlJohara Almulhim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Al Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Study and Analysis of Bengali Folklore with Natural Language Processing Systems. (arXiv:2203.06607v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06607","description":"<p>Folklore, a solid branch of folk literature, is the hallmark of any nation or\nany society. Such as oral tradition; as proverbs or jokes, it also includes\nmaterial culture as well as traditional folk beliefs, and various customs.\nBengali folklore is as rich in-depth as it is amazing. Nevertheless, in the\nwomb of time, it is determined to sustain its existence. Therefore, our aim in\nthis study is to make our rich folklore more comprehensible to everyone in a\nmore sophisticated computational way. Some studies concluded various aspects of\nthe Bengali language with NLP. Our proposed model is to be specific for Bengali\nfolklore. Technically, it will be the first step towards Bengali natural\nlanguage processing for studying and analyzing the folklore of Bengal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Billah_M/0/1/0/all/0/1\">Mustain Billah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mynoddin_M/0/1/0/all/0/1\">Md. Mynoddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhond_M/0/1/0/all/0/1\">Mostafijur Rahman Akhond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adnan_M/0/1/0/all/0/1\">Md. Nasim Adnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galib_S/0/1/0/all/0/1\">Syed Md. Galib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahad_R/0/1/0/all/0/1\">Rizwanur Rahad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">M Nurujjaman Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Prompt Tuning for Dialog State Tracking. (arXiv:2203.06654v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06654","description":"<p>A desirable dialog system should be able to continually learn new skills\nwithout forgetting old ones, and thereby adapt to new domains or tasks in its\nlife cycle. However, continually training a model often leads to a well-known\ncatastrophic forgetting issue. In this paper, we present Continual Prompt\nTuning, a parameter-efficient framework that not only avoids forgetting but\nalso enables knowledge transfer between tasks. To avoid forgetting, we only\nlearn and store a few prompt tokens' embeddings for each task while freezing\nthe backbone pre-trained model. To achieve bi-directional knowledge transfer\namong tasks, we propose several techniques (continual prompt initialization,\nquery fusion, and memory replay) to transfer knowledge from preceding tasks and\na memory-guided technique to transfer knowledge from subsequent tasks.\nExtensive experiments demonstrate the effectiveness and efficiency of our\nproposed method on continual learning for dialog state tracking, compared with\nstate-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the answering frame timeline. Extensive experiments on the medical\ninstructional dataset, namely MedVidQA, show the proposed VPTSL outperforms\nother state-of-the-art methods, which demonstrates the effectiveness of visual\nprompt and the text span predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Personalized Intelligence at Scale. (arXiv:2203.06668v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06668","description":"<p>Personalized Intelligence (PI) is the problem of providing customized AI\nexperiences tailored to each individual user. In many applications, PI is\npreferred or even required. Existing personalization approaches involve\nfine-tuning pre-trained models to create new customized models. However, these\napproaches require a significant amount of computation to train, scaling with\nmodel size and the number of users, inhibiting PI to be realized widely. In\nthis work, we introduce a novel model architecture and training/inference\nframework to enable Personalized Intelligence at scale. We achieve this by\nattaching a Personalization Head (PH) to pre-trained language models (LM).\nDuring training, the base LMs are frozen and only the parameters in PH are\nupdated and are unique per user. This results in significantly smaller overall\nmodel sizes and training cost than traditional fine-tuning approaches when\nscaled across many users. We evaluate PHs on academia and industry-focused\ndatasets and show that the PHs outperform zeroshot baseline in F1 score and are\nsignificantly more scalable than traditional fine-tuning approaches. We\nidentify key factors required for effective PH design and training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yiping Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_A/0/1/0/all/0/1\">Ashish Mahendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarke_C/0/1/0/all/0/1\">Christopher Clarke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lingjia Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mars_J/0/1/0/all/0/1\">Jason Mars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarizing a virtual robot's past actions in natural language. (arXiv:2203.06671v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06671","description":"<p>We propose and demonstrate the task of giving natural language summaries of\nthe actions of a robotic agent in a virtual environment. We explain why such a\ntask is important, what makes it difficult, and discuss how it might be\naddressed. To encourage others to work on this, we show how a popular existing\ndataset that matches robot actions with natural language descriptions designed\nfor an instruction following task can be repurposed to serve as a training\nground for robot action summarization work. We propose and test several methods\nof learning to generate such summaries, starting from either egocentric video\nframes of the robot taking actions or intermediate text representations of the\nactions used by an automatic planner. We provide quantitative and qualitative\nevaluations of our results, which can serve as a baseline for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeChant_C/0/1/0/all/0/1\">Chad DeChant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_D/0/1/0/all/0/1\">Daniel Bauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciNLI: A Corpus for Natural Language Inference on Scientific Text. (arXiv:2203.06728v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06728","description":"<p>Existing Natural Language Inference (NLI) datasets, while being instrumental\nin the advancement of Natural Language Understanding (NLU) research, are not\nrelated to scientific text. In this paper, we introduce SciNLI, a large dataset\nfor NLI that captures the formality in scientific text and contains 107,412\nsentence pairs extracted from scholarly papers on NLP and computational\nlinguistics. Given that the text used in scientific literature differs vastly\nfrom the text used in everyday language both in terms of vocabulary and\nsentence structure, our dataset is well suited to serve as a benchmark for the\nevaluation of scientific NLU models. Our experiments show that SciNLI is harder\nto classify than the existing NLI datasets. Our best performing model with\nXLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23 showing\nthat there is substantial room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadat_M/0/1/0/all/0/1\">Mobashir Sadat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtagonistTagger -- a Tool for Entity Linkage of Persons in Texts from Various Languages and Domains. (arXiv:2203.06746v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06746","description":"<p>Named entities recognition (NER) and disambiguation (NED) can add semantic\ncontext to the recognized named entities in texts. Named entity linkage in\ntexts, regardless of a domain, provides links between the entities mentioned in\nunstructured texts and individual instances of real-world objects. In this\nposter, we present a tool - protagonistTagger - for person NER and NED in\ntexts. The tool was tested on texts extracted from classic English novels and\nPolish Internet news. The tool's performance (both precision and recall)\nfluctuates between 78% and even 88%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lajewska_W/0/1/0/all/0/1\">Weronika Lajewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Anna Wroblewska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruned Graph Neural Network for Short Story Ordering. (arXiv:2203.06778v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06778","description":"<p>Text coherence is a fundamental problem in natural language generation and\nunderstanding. Organizing sentences into an order that maximizes coherence is\nknown as sentence ordering. This paper is proposing a new approach based on the\ngraph neural network approach to encode a set of sentences and learn orderings\nof short stories. We propose a new method for constructing sentence-entity\ngraphs of short stories to create the edges between sentences and reduce noise\nin our graph by replacing the pronouns with their referring entities. We\nimprove the sentence ordering by introducing an aggregation method based on\nmajority voting of state-of-the-art methods and our proposed one. Our approach\nemploys a BERT-based model to learn semantic representations of the sentences.\nThe results demonstrate that the proposed method significantly outperforms\nexisting baselines on a corpus of short stories with a new state-of-the-art\nperformance in terms of Perfect Match Ratio (PMR) and Kendall's Tau (Tau)\nmetrics. More precisely, our method increases PMR and Tau criteria by more than\n5% and 4.3%, respectively. These outcomes highlight the benefit of forming the\nedges between sentences based on their cosine similarity. We also observe that\nreplacing pronouns with their referring entities effectively encodes sentences\nin sentence-entity graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golestani_M/0/1/0/all/0/1\">Melika Golestani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borhanifard_Z/0/1/0/all/0/1\">Zeinab Borhanifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebian_F/0/1/0/all/0/1\">Farnaz Tahmasebian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Heshaam Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can pre-trained Transformers be used in detecting complex sensitive sentences? -- A Monsanto case study. (arXiv:2203.06793v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06793","description":"<p>Each and every organisation releases information in a variety of forms\nranging from annual reports to legal proceedings. Such documents may contain\nsensitive information and releasing them openly may lead to the leakage of\nconfidential information. Detection of sentences that contain sensitive\ninformation in documents can help organisations prevent the leakage of valuable\nconfidential information. This is especially challenging when such sentences\ncontain a substantial amount of information or are paraphrased versions of\nknown sensitive content. Current approaches to sensitive information detection\nin such complex settings are based on keyword-based approaches or standard\nmachine learning models. In this paper, we wish to explore whether pre-trained\ntransformer models are well suited to detect complex sensitive information.\nPre-trained transformers are typically trained on an enormous amount of text\nand therefore readily learn grammar, structure and other linguistic features,\nmaking them particularly attractive for this task. Through our experiments on\nthe Monsanto trial data set, we observe that the fine-tuned Bidirectional\nEncoder Representations from Transformers (BERT) transformer model performs\nbetter than traditional models. We experimented with four different categories\nof documents in the Monsanto dataset and observed that BERT achieves better F2\nscores by 24.13\\% to 65.79\\% for GHOST, 30.14\\% to 54.88\\% for TOXIC, 39.22\\%\nfor CHEMI, 53.57\\% for REGUL compared to existing sensitive information\ndetection models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Timmer_R/0/1/0/all/0/1\">Roelien C. Timmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebowitz_D/0/1/0/all/0/1\">David Liebowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1\">Surya Nepal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanhere_S/0/1/0/all/0/1\">Salil S. Kanhere</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Commands for Autonomous Vehicles via Layer Fusion with Region-specific Dynamic Layer Attention. (arXiv:2203.06822v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06822","description":"<p>Grounding a command to the visual environment is an essential ingredient for\ninteractions between autonomous vehicles and humans. In this work, we study the\nproblem of language grounding for autonomous vehicles, which aims to localize a\nregion in a visual scene according to a natural language command from a\npassenger. Prior work only employs the top layer representations of a\nvision-and-language pre-trained model to predict the region referred to by the\ncommand. However, such a method omits the useful features encoded in other\nlayers, and thus results in inadequate understanding of the input scene and\ncommand. To tackle this limitation, we present the first layer fusion approach\nfor this task. Since different visual regions may require distinct types of\nfeatures to disambiguate them from each other, we further propose the\nregion-specific dynamic (RSD) layer attention to adaptively fuse the multimodal\ninformation across layers for each region. Extensive experiments on the\nTalk2Car benchmark demonstrate that our approach helps predict more accurate\nregions and outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingxi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling. (arXiv:2203.06835v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06835","description":"<p>Currently, Medical Subject Headings (MeSH) are manually assigned to every\nbiomedical article published and subsequently recorded in the PubMed database\nto facilitate retrieving relevant information. With the rapid growth of the\nPubMed database, large-scale biomedical document indexing becomes increasingly\nimportant. MeSH indexing is a challenging task for machine learning, as it\nneeds to assign multiple labels to each article from an extremely large\nhierachically organized collection. To address this challenge, we propose\nKenMeSH, an end-to-end model that combines new text features and a dynamic\n\\textbf{K}nowledge-\\textbf{en}hanced mask attention that integrates document\nfeatures with MeSH label hierarchy and journal correlation features to index\nMeSH terms. Experimental results show the proposed method achieves\nstate-of-the-art performance on a number of measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xindi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercer_R/0/1/0/all/0/1\">Robert E. Mercer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities. (arXiv:2203.06849v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06849","description":"<p>Transfer learning has proven to be crucial in advancing the state of speech\nand natural language processing research in recent years. In speech, a model\npre-trained by self-supervised learning transfers remarkably well on multiple\ntasks. However, the lack of a consistent evaluation methodology is limiting\ntowards a holistic understanding of the efficacy of such models. SUPERB was a\nstep towards introducing a common benchmark to evaluate pre-trained models\nacross various speech tasks. In this paper, we introduce SUPERB-SG, a new\nbenchmark focused on evaluating the semantic and generative capabilities of\npre-trained models by increasing task diversity and difficulty over SUPERB. We\nuse a lightweight methodology to test the robustness of representations learned\nby pre-trained models under shifts in data domain and quality across different\ntypes of tasks. It entails freezing pre-trained model parameters, only using\nsimple task-specific trainable heads. The goal is to be inclusive of all\nresearchers, and encourage efficient use of computational resources. We also\nshow that the task diversity of SUPERB-SG coupled with limited task supervision\nis an effective recipe for evaluating the generalizability of model\nrepresentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_H/0/1/0/all/0/1\">Hsiang-Sheng Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zili Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shuyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_P/0/1/0/all/0/1\">Phil Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsuan-Jui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary and Distance between Sets of Texts based on Topological Data Analysis. (arXiv:1912.09253v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1912.09253","description":"<p>In this paper, we use topological data analysis (TDA) tools such as\npersistent homology, persistent entropy and bottleneck distance, to provide a\n{\\it TDA-based summary} of any given set of texts and a general method for\ncomputing a distance between any two literary styles, authors or periods. To\nthis aim, deep-learning word-embedding techniques are combined with these tools\nin order to study the topological properties of texts embedded in a metric\nspace. As a case of study, we use the written texts of three poets of the\nSpanish Golden Age: Francisco de Quevedo, Luis de G\\'ongora and Lope de Vega.\nAs far as we know, this is the first time that word embedding, bottleneck\ndistance, persistent homology and persistent entropy are used together to\ncharacterize texts and to compare different literary styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paluzo_Hidalgo_E/0/1/0/all/0/1\">Eduardo Paluzo-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Diaz_R/0/1/0/all/0/1\">Rocio Gonzalez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Naranjo_M/0/1/0/all/0/1\">Miguel A. Guti&#xe9;rrez-Naranjo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks. (arXiv:2004.06015v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.06015","description":"<p>Knowledge graph (KG) question generation (QG) aims to generate natural\nlanguage questions from KGs and target answers. Previous works mostly focus on\na simple setting which is to generate questions from a single KG triple. In\nthis work, we focus on a more realistic setting where we aim to generate\nquestions from a KG subgraph and target answers. In addition, most of previous\nworks built on either RNN-based or Transformer-based models to encode a\nlinearized KG sugraph, which totally discards the explicit structure\ninformation of a KG subgraph. To address this issue, we propose to apply a\nbidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we\nenhance our RNN decoder with node-level copying mechanism to allow directly\ncopying node attributes from the KG subgraph to the output question. Both\nautomatic and human evaluation results demonstrate that our model achieves new\nstate-of-the-art scores, outperforming existing methods by a significant margin\non two QG benchmarks. Experimental results also show that our QG model can\nconsistently benefit the Question Answering (QA) task as a mean of data\naugmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammed J. Zaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Inter-Aspect Dependencies with a Non-temporal Mechanism for Aspect-Based Sentiment Analysis. (arXiv:2008.05179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.05179","description":"<p>For multiple aspects scenario of aspect-based sentiment analysis (ABSA),\nexisting approaches typically ignore inter-aspect relations or rely on temporal\ndependencies to process aspect-aware representations of all aspects in a\nsentence. Although multiple aspects of a sentence appear in a non-adjacent\nsequential order, they are not in a strict temporal relationship as natural\nlanguage sequence, thus the aspect-aware sentence representations should not be\ntreated as temporal dependency processing. In this paper, we propose a novel\nnon-temporal mechanism to enhance the ABSA task through modeling inter-aspect\ndependencies. Furthermore, we focus on the well-known class imbalance issue on\nthe ABSA task and address it by down-weighting the loss assigned to\nwell-classified instances. Experiments on two distinct domains of SemEval 2014\ntask 4 demonstrate the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Transformers: A Survey. (arXiv:2009.06732v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.06732","description":"<p>Transformer model architectures have garnered immense interest lately due to\ntheir effectiveness across a range of domains like language, vision and\nreinforcement learning. In the field of natural language processing for\nexample, Transformers have become an indispensable staple in the modern deep\nlearning stack. Recently, a dizzying number of \"X-former\" models have been\nproposed - Reformer, Linformer, Performer, Longformer, to name a few - which\nimprove upon the original Transformer architecture, many of which make\nimprovements around computational and memory efficiency. With the aim of\nhelping the avid researcher navigate this flurry, this paper characterizes a\nlarge and thoughtful selection of recent efficiency-flavored \"X-former\" models,\nproviding an organized and comprehensive overview of existing work and models\nacross multiple domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Document-level Neural Machine Translation. (arXiv:2010.08961v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.08961","description":"<p>This paper does not aim at introducing a novel model for document-level\nneural machine translation. Instead, we head back to the original Transformer\nmodel and hope to answer the following question: Is the capacity of current\nmodels strong enough for document-level translation? Interestingly, we observe\nthat the original Transformer with appropriate training techniques can achieve\nstrong results for document translation, even with a length of 2000 words. We\nevaluate this model and several recent approaches on nine document-level\ndatasets and two sentence-level datasets across six languages. Experiments show\nthat document-level Transformer models outperforms sentence-level ones and many\nprevious methods in a comprehensive set of metrics, including BLEU, four\nlexical indices, three newly proposed assistant linguistic indicators, and\nhuman evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. (arXiv:2012.15409v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15409","description":"<p>Existed pre-training methods either focus on single-modal tasks or\nmulti-modal tasks, and cannot effectively adapt to each other. They can only\nutilize single-modal data (i.e. text or image) or limited multi-modal data\n(i.e. image-text pairs). In this work, we propose a unified-modal pre-training\narchitecture, namely UNIMO, which can effectively adapt to both single-modal\nand multi-modal understanding and generation tasks. Large scale of free text\ncorpus and image collections can be utilized to improve the capability of\nvisual and textual understanding, and cross-modal contrastive learning (CMCL)\nis leveraged to align the textual and visual information into a unified\nsemantic space over a corpus of image-text pairs. As the non-paired\nsingle-modal data is very rich, our model can utilize much larger scale of data\nto learn more generalizable representations. Moreover, the textual knowledge\nand visual knowledge can enhance each other in the unified semantic space. The\nexperimental results show that UNIMO significantly improves the performance of\nseveral single-modal and multi-modal downstream tasks. Our code and pre-trained\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Can Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Guocheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do Your Biomedical Named Entity Recognition Models Generalize to Novel Entities?. (arXiv:2101.00160v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00160","description":"<p>The number of biomedical literature on new biomedical concepts is rapidly\nincreasing, which necessitates a reliable biomedical named entity recognition\n(BioNER) model for identifying new and unseen entity mentions. However, it is\nquestionable whether existing models can effectively handle them. In this work,\nwe systematically analyze the three types of recognition abilities of BioNER\nmodels: memorization, synonym generalization, and concept generalization. We\nfind that although current best models achieve state-of-the-art performance on\nbenchmarks based on overall performance, they have limitations in identifying\nsynonyms and new biomedical concepts, indicating they are overestimated in\nterms of their generalization abilities. We also investigate failure cases of\nmodels and identify several difficulties in recognizing unseen mentions in\nbiomedical literature as follows: (1) models tend to exploit dataset biases,\nwhich hinders the models' abilities to generalize, and (2) several biomedical\nnames have novel morphological patterns with weak name regularity, and models\nfail to recognize them. We apply a statistics-based debiasing method to our\nproblem as a simple remedy and show the improvement in generalization to unseen\nmentions. We hope that our analyses and findings would be able to facilitate\nfurther research into the generalization capabilities of NER models in a domain\nwhere their reliability is of utmost importance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring COVID-19 Biological Pathways from Clinical Phenotypes via Topological Analysis. (arXiv:2101.07417v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.07417","description":"<p>COVID-19 has caused thousands of deaths around the world and also resulted in\na large international economic disruption. Identifying the pathways associated\nwith this illness can help medical researchers to better understand the\nproperties of the condition. This process can be carried out by analyzing the\nmedical records. It is crucial to develop tools and models that can aid\nresearchers with this process in a timely manner. However, medical records are\noften unstructured clinical notes, and this poses significant challenges to\ndeveloping the automated systems. In this article, we propose a pipeline to aid\npractitioners in analyzing clinical notes and revealing the pathways associated\nwith this disease. Our pipeline relies on topological properties and consists\nof three steps: 1) pre-processing the clinical notes to extract the salient\nconcepts, 2) constructing a feature space of the patients to characterize the\nextracted concepts, and finally, 3) leveraging the topological properties to\ndistill the available knowledge and visualize the result. Our experiments on a\npublicly available dataset of COVID-19 clinical notes testify that our pipeline\ncan indeed extract meaningful pathways.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_N/0/1/0/all/0/1\">Negin Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platt_D/0/1/0/all/0/1\">Daniel E. Platt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Saugata Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parida_L/0/1/0/all/0/1\">Laxmi Parida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport-based Adaptation in Dysarthric Speech Tasks. (arXiv:2104.02535v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2104.02535","description":"<p>In many real-world applications, the mismatch between distributions of\ntraining data (source) and test data (target) significantly degrades the\nperformance of machine learning algorithms. In speech data, causes of this\nmismatch include different acoustic environments or speaker characteristics. In\nthis paper, we address this issue in the challenging context of dysarthric\nspeech, by multi-source domain/speaker adaptation (MSDA/MSSA). Specifically, we\npropose the use of an optimal-transport based approach, called MSDA via\nWeighted Joint Optimal Transport (MSDA-WDJOT). We confront the mismatch problem\nin dysarthria detection for which the proposed approach outperforms both the\nBaseline and the state-of-the-art MSDA models, improving the detection accuracy\nof 0.9% over the best competitor method. We then employ MSDA-WJDOT for\ndysarthric speaker adaptation in command speech recognition. This provides a\nCommand Error Rate relative reduction of 16% and 7% over the baseline and the\nbest competitor model, respectively. Interestingly, MSDA-WJDOT provides a\nsimilarity score between the source and the target, i.e. between speakers in\nthis case. We leverage this similarity measure to define a Dysarthric and\nHealthy score of the target speaker and diagnose the dysarthria with an\naccuracy of 95%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turrisi_R/0/1/0/all/0/1\">Rosanna Turrisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badino_L/0/1/0/all/0/1\">Leonardo Badino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on reinforcement learning for language processing. (arXiv:2104.05565v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05565","description":"<p>In recent years some researchers have explored the use of reinforcement\nlearning (RL) algorithms as key components in the solution of various natural\nlanguage processing tasks. For instance, some of these algorithms leveraging\ndeep neural learning have found their way into conversational systems. This\npaper reviews the state of the art of RL methods for their possible use for\ndifferent problems of natural language processing, focusing primarily on\nconversational systems, mainly due to their growing relevance. We provide\ndetailed descriptions of the problems as well as discussions of why RL is\nwell-suited to solve them. Also, we analyze the advantages and limitations of\nthese methods. Finally, we elaborate on promising research directions in\nnatural language processing that might benefit from reinforcement learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uc_Cetina_V/0/1/0/all/0/1\">Victor Uc-Cetina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_Guerrero_N/0/1/0/all/0/1\">Nicolas Navarro-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Gonzalez_A/0/1/0/all/0/1\">Anabel Martin-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08773","description":"<p>Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. Despite the success of the conventional supervised learning on\nindividual datasets, such models often struggle with generalization across\ntasks (e.g., a question-answering system cannot solve classification tasks). A\nlong-standing challenge in AI is to build a model that learns a new task by\nunderstanding the human-readable instructions that define it. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions, and 193k task instances (input-output pairs). The\ninstructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. Using this meta-dataset,\nwe measure cross-task generalization by training models on seen tasks and\nmeasuring generalization to the remaining unseen ones. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks (19%\nbetter for models utilizing instructions). These models, however, are far\nbehind an estimated performance upperbound indicating significant room for more\nprogress in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAPEX: Table Pre-training via Learning a Neural SQL Executor. (arXiv:2107.07653v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07653","description":"<p>Recent progress in language model pre-training has achieved a great success\nvia leveraging large-scale unstructured textual data. However, it is still a\nchallenge to apply pre-training on structured tabular data due to the absence\nof large-scale high-quality tabular data. In this paper, we propose TAPEX to\nshow that table pre-training can be achieved by learning a neural SQL executor\nover a synthetic corpus, which is obtained by automatically synthesizing\nexecutable SQL queries and their execution outputs. TAPEX addresses the data\nscarcity challenge via guiding the language model to mimic a SQL executor on\nthe diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX\non four benchmark datasets. Experimental results demonstrate that TAPEX\noutperforms previous table pre-training approaches by a large margin and\nachieves new state-of-the-art results on all of them. This includes the\nimprovements on the weakly-supervised WikiSQL denotation accuracy to 89.5%\n(+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA\ndenotation accuracy to 74.5% (+3.5%), and the TabFact accuracy to 84.2%\n(+3.2%). To our knowledge, this is the first work to exploit table pre-training\nvia synthetic executable programs and to achieve new state-of-the-art results\non various downstream tasks. Our code can be found at\nhttps://github.com/microsoft/Table-Pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziyadi_M/0/1/0/all/0/1\">Morteza Ziyadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense. (arXiv:2109.02572v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02572","description":"<p>We study how to enhance text representation via textual commonsense. We point\nout that commonsense has the nature of domain discrepancy. Namely, commonsense\nhas different data formats and is domain-independent from the downstream task.\nThis nature brings challenges to introducing commonsense in general text\nunderstanding tasks. A typical method of introducing textual knowledge is\ncontinuing pre-training over the commonsense corpus. However, it will cause\ncatastrophic forgetting to the downstream task due to the domain discrepancy.\nIn addition, previous methods of directly using textual descriptions as extra\ninput information cannot apply to large-scale commonsense.\n</p>\n<p>In this paper, we propose to use large-scale out-of-domain commonsense to\nenhance text representation. In order to effectively incorporate the\ncommonsense, we proposed OK-Transformer (\\underline{O}ut-of-domain\n\\underline{K}nowledge enhanced \\underline{Transformer}). OK-Transformer\neffectively integrates commonsense descriptions and enhances them to the target\ntext representation. In addition, OK-Transformer can adapt to the\nTransformer-based language models (e.g. BERT, RoBERTa) for free, without\npre-training on large-scale unsupervised corpora. We have verified the\neffectiveness of OK-Transformer in multiple applications such as commonsense\nreasoning, general text classification, and low-resource commonsense settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanyun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingran Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04332","description":"<p>Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations. (arXiv:2109.13059v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13059","description":"<p>In NLP, a large volume of tasks involve pairwise comparison between two\nsequences (e.g. sentence similarity and paraphrase identification).\nPredominantly, two formulations are used for sentence-pair tasks: bi-encoders\nand cross-encoders. Bi-encoders produce fixed-dimensional sentence\nrepresentations and are computationally efficient, however, they usually\nunderperform cross-encoders. Cross-encoders can leverage their attention heads\nto exploit inter-sentence interactions for better performance but they require\ntask fine-tuning and are computationally more expensive. In this paper, we\npresent a completely unsupervised sentence representation model termed as\nTrans-Encoder that combines the two learning paradigms into an iterative joint\nframework to simultaneously learn enhanced bi- and cross-encoders.\nSpecifically, on top of a pre-trained Language Model (PLM), we start with\nconverting it to an unsupervised bi-encoder, and then alternate between the bi-\nand cross-encoder task formulations. In each alternation, one task formulation\nwill produce pseudo-labels which are used as learning signals for the other\ntask formulation. We then propose an extension to conduct such\nself-distillation approach on multiple PLMs in parallel and use the average of\ntheir pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best\nof our knowledge, the first completely unsupervised cross-encoder and also a\nstate-of-the-art unsupervised bi-encoder for sentence similarity. Both the\nbi-encoder and cross-encoder formulations of Trans-Encoder outperform recently\nproposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT\nand SimCSE by up to 5% on the sentence similarity benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yunlong Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massiah_J/0/1/0/all/0/1\">Jordan Massiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havrylov_S/0/1/0/all/0/1\">Serhii Havrylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00976","description":"<p>Laws and their interpretations, legal arguments and agreements\\ are typically\nexpressed in writing, leading to the production of vast corpora of legal text.\nTheir analysis, which is at the center of legal practice, becomes increasingly\nelaborate as these collections grow in size. Natural language understanding\n(NLU) technologies can be a valuable tool to support legal practitioners in\nthese endeavors. Their usefulness, however, largely depends on whether current\nstate-of-the-art models can generalize across various tasks in the legal\ndomain. To answer this currently open question, we introduce the Legal General\nLanguage Understanding Evaluation (LexGLUE) benchmark, a collection of datasets\nfor evaluating model performance across a diverse set of legal NLU tasks in a\nstandardized way. We also provide an evaluation and analysis of several generic\nand legal-oriented models demonstrating that the latter consistently offer\nperformance improvements across multiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1\">Abhik Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_D/0/1/0/all/0/1\">Dirk Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael Bommarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeTS: A Benchmark for Translation Suggestion. (arXiv:2110.05151v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05151","description":"<p>Translation Suggestion (TS), which provides alternatives for specific words\nor phrases given the entire documents translated by machine translation (MT)\n\\cite{lee2021intellicat}, has been proven to play a significant role in post\nediting (PE). However, there is still no publicly available data set to support\nin-depth research for this problem, and no reproducible experimental results\ncan be followed by researchers in this community. To break this limitation, we\ncreate a benchmark data set for TS, called \\emph{WeTS}, which contains golden\ncorpus annotated by expert translators on four translation directions. Apart\nfrom the human-annotated golden corpus, we also propose several novel methods\nto generate synthetic corpus which can substantially improve the performance of\nTS. With the corpus we construct, we introduce the Transformer-based model for\nTS, and experimental results show that our model achieves State-Of-The-Art\n(SOTA) results on all four translation directions, including English-to-German,\nGerman-to-English, Chinese-to-English and English-to-Chinese. Codes and corpus\ncan be found at \\url{https://github.com/ZhenYangIACAS/WeTS.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingxue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Ernan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?. (arXiv:2110.06918v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06918","description":"<p>Despite their recent popularity and well-known advantages, dense retrievers\nstill lag behind sparse methods such as BM25 in their ability to reliably match\nsalient phrases and rare entities in the query and to generalize to\nout-of-domain data. It has been argued that this is an inherent limitation of\ndense models. We rebut this claim by introducing the Salient Phrase Aware\nRetriever (SPAR), a dense retriever with the lexical matching capacity of a\nsparse model. We show that a dense Lexical Model {\\Lambda} can be trained to\nimitate a sparse one, and SPAR is built by augmenting a standard dense\nretriever with {\\Lambda}. Empirically, SPAR shows superior performance on a\nrange of tasks including five question answering datasets, MS MARCO passage\nretrieval, as well as the EntityQuestions and BEIR benchmarks for out-of-domain\nevaluation, exceeding the performance of state-of-the-art dense and sparse\nretrievers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIMICause: Representation and automatic extraction of causal relation types from clinical notes. (arXiv:2110.07090v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07090","description":"<p>Understanding causal narratives communicated in clinical notes can help make\nstrides towards personalized healthcare. Extracted causal information from\nclinical notes can be combined with structured EHR data such as patients'\ndemographics, diagnoses, and medications. This will enhance healthcare\nproviders' ability to identify aspects of a patient's story communicated in the\nclinical notes and help make more informed decisions.\n</p>\n<p>In this work, we propose annotation guidelines, develop an annotated corpus\nand provide baseline scores to identify types and direction of causal relations\nbetween a pair of biomedical concepts in clinical notes; communicated\nimplicitly or explicitly, identified either in a single sentence or across\nmultiple sentences.\n</p>\n<p>We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2\nshared task dataset and train four different language model based\narchitectures. Annotation based on our guidelines achieved a high\ninter-annotator agreement i.e. Fleiss' kappa ($\\kappa$) score of 0.72, and our\nmodel for identification of causal relations achieved a macro F1 score of 0.56\non the test data. The high inter-annotator agreement for clinical text shows\nthe quality of our annotation guidelines while the provided baseline F1 score\nsets the direction for future research towards understanding narratives in\nclinical texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_M/0/1/0/all/0/1\">Md Imbesat Hassan Rizvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_J/0/1/0/all/0/1\">Jessica Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_P/0/1/0/all/0/1\">Paige Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacaleanu_B/0/1/0/all/0/1\">Bogdan Sacaleanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings. (arXiv:2110.07385v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07385","description":"<p>Style transfer is the task of rewriting a sentence into a target style while\napproximately preserving content. While most prior literature assumes access to\na large style-labelled corpus, recent work (Riley et al. 2021) has attempted\n\"few-shot\" style transfer using only 3-10 sentences at inference for style\nextraction. In this work we study a relevant low-resource setting: style\ntransfer for languages where no style-labelled corpora are available. We notice\nthat existing few-shot methods perform this task poorly, often copying inputs\nverbatim. We push the state-of-the-art for few-shot style transfer with a new\nmethod modeling the stylistic difference between paraphrases. When compared to\nprior work, our model achieves 2-3x better performance in formality transfer\nand code-mixing addition across seven languages. Moreover, our method is better\nat controlling the style transfer magnitude using an input scalar knob. We\nreport promising qualitative results for several attribute transfer tasks\n(sentiment transfer, simplification, gender neutralization, text anonymization)\nall without retraining the model. Finally, we find model evaluation to be\ndifficult due to the lack of datasets and metrics for many languages. To\nfacilitate future research we crowdsource formality annotations for 4000\nsentence pairs in four Indic languages, and use this data to design our\nautomatic evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nathani_D/0/1/0/all/0/1\">Deepak Nathani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1\">Bidisha Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER. (arXiv:2110.08454v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08454","description":"<p>Recent advances in prompt-based learning have shown strong results on\nfew-shot text classification by using cloze-style templates. Similar attempts\nhave been made on named entity recognition (NER) which manually design\ntemplates to predict entity types for every text span in a sentence. However,\nsuch methods may suffer from error propagation induced by entity span\ndetection, high cost due to enumeration of all possible text spans, and\nomission of inter-dependencies among token labels in a sentence. Here we\npresent a simple demonstration-based learning method for NER, which lets the\ninput be prefaced by task demonstrations for in-context learning. We perform a\nsystematic study on demonstration strategy regarding what to include (entity\nexamples, with or without surrounding context), how to select the examples, and\nwhat templates to use. Results on in-domain learning and domain adaptation show\nthat the model's performance in low-resource settings can be largely improved\nwith a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train\ninstances). We also find that good demonstration can save many labeled examples\nand consistency in demonstration contributes to better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadakia_A/0/1/0/all/0/1\">Akshen Kadakia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kangmin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mahak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xinyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibuya_T/0/1/0/all/0/1\">Takashi Shibuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitani_R/0/1/0/all/0/1\">Ryosuke Mitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekiya_T/0/1/0/all/0/1\">Toshiyuki Sekiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for text-to-image and image-to-text\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation tasks without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial results of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena. (arXiv:2112.07566v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07566","description":"<p>We propose VALSE (Vision And Language Structured Evaluation), a novel\nbenchmark designed for testing general-purpose pretrained vision and language\n(V&amp;L) models for their visio-linguistic grounding capabilities on specific\nlinguistic phenomena. VALSE offers a suite of six tests covering various\nlinguistic constructs. Solving these requires models to ground linguistic\nphenomena in the visual modality, allowing more fine-grained evaluations than\nhitherto possible. We build VALSE using methods that support the construction\nof valid foils, and report results from evaluating five widely-used V&amp;L models.\nOur experiments suggest that current models have considerable difficulty\naddressing most phenomena. Hence, we expect VALSE to serve as an important\nbenchmark to measure future progress of pretrained V&amp;L models from a linguistic\nperspective, complementing the canonical task-centred V&amp;L evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1\">Letitia Parcalabescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muradjan_L/0/1/0/all/0/1\">Lilitta Muradjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebGPT: Browser-assisted question-answering with human feedback. (arXiv:2112.09332v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09332","description":"<p>We fine-tune GPT-3 to answer long-form questions using a text-based\nweb-browsing environment, which allows the model to search and navigate the\nweb. By setting up the task so that it can be performed by humans, we are able\nto train models on the task using imitation learning, and then optimize answer\nquality with human feedback. To make human evaluation of factual accuracy\neasier, models must collect references while browsing in support of their\nanswers. We train and evaluate our models on ELI5, a dataset of questions asked\nby Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior\ncloning, and then performing rejection sampling against a reward model trained\nto predict human preferences. This model's answers are preferred by humans 56%\nof the time to those of our human demonstrators, and 69% of the time to the\nhighest-voted answer from Reddit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakano_R/0/1/0/all/0/1\">Reiichiro Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1\">Suchir Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Christina Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesse_C/0/1/0/all/0/1\">Christopher Hesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shantanu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosaraju_V/0/1/0/all/0/1\">Vineet Kosaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobbe_K/0/1/0/all/0/1\">Karl Cobbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eloundou_T/0/1/0/all/0/1\">Tyna Eloundou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Button_K/0/1/0/all/0/1\">Kevin Button</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1\">Matthew Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chess_B/0/1/0/all/0/1\">Benjamin Chess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02639","description":"<p>As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time -- through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n</p>\n<p>Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining -- even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n</p>\n<p>We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition. (arXiv:2201.03804v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03804","description":"<p>With the rise of deep learning and intelligent vehicle, the smart assistant\nhas become an essential in-car component to facilitate driving and provide\nextra functionalities. In-car smart assistants should be able to process\ngeneral as well as car-related commands and perform corresponding actions,\nwhich eases driving and improves safety. However, there is a data scarcity\nissue for low resource languages, hindering the development of research and\napplications. In this paper, we introduce a new dataset, Cantonese In-car\nAudio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in\nthe Cantonese language with both video and audio data. It consists of 4,984\nsamples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese\nspeakers. Furthermore, we augment our dataset using common in-car background\nnoises to simulate real environments, producing a dataset 10 times larger than\nthe collected one. We provide detailed statistics of both the clean and the\naugmented versions of our dataset. Moreover, we implement two multimodal\nbaselines to demonstrate the validity of CI-AVSR. Experiment results show that\nleveraging the visual signal improves the overall performance of the model.\nAlthough our best model can achieve a considerable quality on the clean test\nset, the speech recognition quality on the noisy data is still inferior and\nremains as an extremely challenging task for real in-car speech recognition\nsystems. The dataset and code will be released at\nhttps://github.com/HLTCHKUST/CI-AVSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_C/0/1/0/all/0/1\">Cheuk Tung Shadow Yiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two heads are better than one: Enhancing medical representations by pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10113","description":"<p>The massive amount of electronic health records (EHRs) has created enormous\npotentials for improving healthcare, among which clinical codes (structured\ndata) and clinical narratives (unstructured data) are two important textual\nmodalities. Most existing EHR-oriented studies, however, either only focus on a\nparticular modality or integrate data from different modalities in a shallow\nmanner, which ignores the intrinsic interactions between them. To address these\nissues, we proposed a Medical Multimodal Pre-trained Language Model, named\nMedM-PLM, to learn enhanced EHR representations over structured and\nunstructured data. In MedM-PLM, two Transformer-based neural networks\ncomponents are firstly adopted to learn representative characteristics from\neach modality. A cross-modal module is then introduced to model their\ninteractions. We pre-trained MedM-PLM on the MIMIC-III dataset and verified the\neffectiveness of the model on three downstream clinical tasks, i.e., medication\nrecommendation, 30-day readmission, and ICD coding. Extensive experiments\ndemonstrate the power of MedM-PLM compared with state-of-the-art methods.\nFurther analyses and visualizations show the robustness of our model which\ncould potentially provide more comprehensive interpretations for clinical\ndecision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yongshuai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-biased image classification: evaluation based on semantic representations. (arXiv:2201.11014v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11014","description":"<p>Humans show language-biased image recognition for a word-embedded image,\nknown as picture-word interference. Such interference depends on hierarchical\nsemantic categories and reflects that human language processing highly\ninteracts with visual processing. Similar to humans, recent artificial models\njointly trained on texts and images, e.g., OpenAI CLIP, show language-biased\nimage classification. Exploring whether the bias leads to interference similar\nto those observed in humans can contribute to understanding how much the model\nacquires hierarchical semantic representations from joint learning of language\nand vision. The present study introduces methodological tools from the\ncognitive science literature to assess the biases of artificial models.\nSpecifically, we introduce a benchmark task to test whether words superimposed\non images can distort the image classification across different category levels\nand, if it can, whether the perturbation is due to the shared semantic\nrepresentation between language and vision. Our dataset is a set of\nword-embedded images and consists of a mixture of natural image datasets and\nhierarchical word labels with superordinate/basic category levels. Using this\nbenchmark test, we evaluate the CLIP model. We show that presenting words\ndistorts the image classification by the model across different category\nlevels, but the effect does not depend on the semantic relationship between\nimages and embedded words. This suggests that the semantic word representation\nin the CLIP visual processing is not shared with the image representation,\nalthough the word representation strongly dominates for word-embedded images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemesle_Y/0/1/0/all/0/1\">Yoann Lemesle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1\">Masataka Sawayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_Perez_G/0/1/0/all/0/1\">Guillermo Valle-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adolphe_M/0/1/0/all/0/1\">Maxime Adolphe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauzeon_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Wikipedia Help Offline Reinforcement Learning?. (arXiv:2201.12122v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12122","description":"<p>Fine-tuning reinforcement learning (RL) models has been challenging because\nof a lack of large scale off-the-shelf datasets as well as high variance in\ntransferability among different environments. Recent work has looked at\ntackling offline RL from the perspective of sequence modeling with improved\nresults as result of the introduction of the Transformer architecture. However,\nwhen the model is trained from scratch, it suffers from slow convergence\nspeeds. In this paper, we look to take advantage of this formulation of\nreinforcement learning as sequence modeling and investigate the transferability\nof pre-trained sequence models on other domains (vision, language) when\nfinetuned on offline RL tasks (control, games). To this end, we also propose\ntechniques to improve transfer between these domains. Results show consistent\nperformance gains in terms of both convergence speed and reward on a variety of\nenvironments, accelerating training by 3-6x and achieving state-of-the-art\nperformance in a variety of tasks using Wikipedia-pretrained and GPT2 language\nmodels. We hope that this work not only brings light to the potentials of\nleveraging generic sequence modeling techniques and pre-trained models for RL,\nbut also inspires future work on sharing knowledge between generative modeling\ntasks of completely different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_Y/0/1/0/all/0/1\">Yutaro Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning. (arXiv:2202.00535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00535","description":"<p>Paraphrase generation is a fundamental and long-standing task in natural\nlanguage processing. In this paper, we concentrate on two contributions to the\ntask: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a\nparameter-efficient method to adapt large pre-trained language models for\nparaphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a\nsimple model-agnostic method of using specialized prompt tokens for controlled\nparaphrase generation with varying levels of lexical novelty. By conducting\nextensive experiments on four datasets, we demonstrate the effectiveness of the\nproposed approaches for retaining the semantic content of the original text\nwhile inducing lexical novelty in the generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuyi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L3Cube-MahaCorpus and MahaBERT: Marathi Monolingual Corpus, Marathi BERT Language Models, and Resources. (arXiv:2202.01159v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01159","description":"<p>We present L3Cube-MahaCorpus a Marathi monolingual data set scraped from\ndifferent internet sources. We expand the existing Marathi monolingual corpus\nwith 24.8M sentences and 289M tokens. We further present, MahaBERT, MahaAlBERT,\nand MahaRoBerta all BERT-based masked language models, and MahaFT, the fast\ntext word embeddings both trained on full Marathi corpus with 752M tokens. We\nshow the effectiveness of these resources on downstream Marathi sentiment\nanalysis, text classification, and named entity recognition (NER) tasks. We\nalso release MahaGPT, a generative Marathi GPT model trained on Marathi corpus.\nMarathi is a popular language in India but still lacks these resources. This\nwork is a step forward in building open resources for the Marathi language. The\ndata and models are available at https://github.com/l3cube-pune/MarathiNLP .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASSIST: Towards Label Noise-Robust Dialogue State Tracking. (arXiv:2202.13024v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13024","description":"<p>The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state\ntracking (DST). However, substantial noise has been discovered in its state\nannotations. Such noise brings about huge challenges for training DST models\nrobustly. Although several refined versions, including MultiWOZ 2.1-2.4, have\nbeen published recently, there are still lots of noisy labels, especially in\nthe training set. Besides, it is costly to rectify all the problematic\nannotations. In this paper, instead of improving the annotation quality\nfurther, we propose a general framework, named ASSIST (lAbel noiSe-robuSt\ndIalogue State Tracking), to train DST models robustly from noisy labels.\nASSIST first generates pseudo labels for each sample in the training set by\nusing an auxiliary model trained on a small clean dataset, then puts the\ngenerated pseudo labels and vanilla noisy labels together to train the primary\nmodel. We show the validity of ASSIST theoretically. Experimental results also\ndemonstrate that ASSIST improves the joint goal accuracy of DST by up to\n$28.16\\%$ on MultiWOZ 2.0 and $8.41\\%$ on MultiWOZ 2.4, compared to using only\nthe vanilla noisy labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fanghua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuoteR: A Benchmark of Quote Recommendation for Writing. (arXiv:2202.13145v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13145","description":"<p>It is very common to use quotations (quotes) to make our writings more\nelegant or convincing. To help people find appropriate quotes efficiently, the\ntask of quote recommendation is presented, aiming to recommend quotes that fit\nthe current context of writing. There have been various quote recommendation\napproaches, but they are evaluated on different unpublished datasets. To\nfacilitate the research on this task, we build a large and fully open quote\nrecommendation dataset called QuoteR, which comprises three parts including\nEnglish, standard Chinese and classical Chinese. Any part of it is larger than\nprevious unpublished counterparts. We conduct an extensive evaluation of\nexisting quote recommendation methods on QuoteR. Furthermore, we propose a new\nquote recommendation model that significantly outperforms previous methods on\nall three parts of QuoteR. All the code and data of this paper are available at\nhttps://github.com/thunlp/QuoteR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanhui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhili Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCR Improves Machine Translation for Low-Resource Languages. (arXiv:2202.13274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13274","description":"<p>We aim to investigate the performance of current OCR systems on low resource\nlanguages and low resource scripts. We introduce and make publicly available a\nnovel benchmark, OCR4MT, consisting of real and synthetic data, enriched with\nnoise, for 60 low-resource languages in low resource scripts. We evaluate\nstate-of-the-art OCR systems on our benchmark and analyse most common errors.\nWe show that OCR monolingual data is a valuable resource that can increase\nperformance of Machine Translation models, when used in backtranslation. We\nthen perform an ablation study to investigate how OCR errors impact Machine\nTranslation performance and determine what is the minimum level of OCR quality\nneeded for the monolingual data to be useful for Machine Translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation. (arXiv:2202.13663v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13663","description":"<p>Most dominant neural machine translation (NMT) models are restricted to make\npredictions only according to the local context of preceding words in a\nleft-to-right manner. Although many previous studies try to incorporate global\ninformation into NMT models, there still exist limitations on how to\neffectively exploit bidirectional global context. In this paper, we propose a\nConfidence Based Bidirectional Global Context Aware (CBBGCA) training framework\nfor NMT, where the NMT model is jointly trained with an auxiliary conditional\nmasked language model (CMLM). The training consists of two stages: (1)\nmulti-task joint training; (2) confidence based knowledge distillation. At the\nfirst stage, by sharing encoder parameters, the NMT model is additionally\nsupervised by the signal from the CMLM decoder that contains bidirectional\nglobal contexts. Moreover, at the second stage, using the CMLM as teacher, we\nfurther pertinently incorporate bidirectional global context to the NMT model\non its unconfidently-predicted target words via knowledge distillation.\nExperimental results show that our proposed CBBGCA training framework\nsignificantly improves the NMT model by +1.02, +1.30 and +0.57 BLEU scores on\nthree large-scale translation datasets, namely WMT'14 English-to-German, WMT'19\nChinese-to-English and WMT'14 English-to-French, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"py-irt: A Scalable Item Response Theory Library for Python. (arXiv:2203.01282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01282","description":"<p>py-irt is a Python library for fitting Bayesian Item Response Theory (IRT)\nmodels. py-irt estimates latent traits of subjects and items, making it\nappropriate for use in IRT tasks as well as ideal-point models. py-irt is built\non top of the Pyro and PyTorch frameworks and uses GPU-accelerated training to\nscale to large data sets. Code, documentation, and examples can be found at\nhttps://github.com/nd-ball/py-irt. py-irt can be installed from the GitHub page\nor the Python Package Index (PyPI).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lalor_J/0/1/0/all/0/1\">John P. Lalor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pedro Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doctor Recommendation in Online Health Forums via Expertise Learning. (arXiv:2203.02932v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02932","description":"<p>Huge volumes of patient queries are daily generated on online health forums,\nrendering manual doctor allocation a labor-intensive task. To better help\npatients, this paper studies a novel task of doctor recommendation to enable\nautomatic pairing of a patient to a doctor with relevant expertise. While most\nprior work in recommendation focuses on modeling target users from their past\nbehavior, we can only rely on the limited words in a query to infer a patient's\nneeds for privacy reasons. For doctor modeling, we study the joint effects of\ntheir profiles and previous dialogues with other patients and explore their\ninteractions via self-learning. The learned doctor embeddings are further\nemployed to estimate their capabilities of handling a patient query with a\nmulti-head attention mechanism. For experiments, a large-scale dataset is\ncollected from Chunyu Yisheng, a Chinese online health forum, where our model\nexhibits the state-of-the-art results, outperforming baselines only consider\nprofiles and past dialogues to characterize a doctor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoxin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records. (arXiv:2203.03540v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03540","description":"<p>Objective: To develop a large pretrained clinical language model from scratch\nusing transformer architecture; systematically examine how transformer models\nof different sizes could help 5 clinical natural language processing (NLP)\ntasks at different linguistic levels. Methods: We created a large corpus with\n&gt;90 billion words from clinical narratives (&gt;82 billion words), scientific\nliterature (6 billion words), and general English text (2.5 billion words). We\ndeveloped GatorTron models from scratch using the BERT architecture of\ndifferent sizes including 345 million, 3.9 billion, and 8.9 billion parameters,\ncompared GatorTron with three existing transformer models in the clinical and\nbiomedical domain on 5 different clinical NLP tasks including clinical concept\nextraction, relation extraction, semantic textual similarity, natural language\ninference, and medical question answering, to examine how large transformer\nmodels could help clinical NLP at different linguistic levels. Results and\nConclusion: GatorTron scaled up transformer-based clinical language models to a\nsize of 8.9 billion parameters and achieved state-of-the-art performance on 5\nclinical NLP tasks of different linguistic levels targeting various healthcare\ninformation documented in unstructured electronic health records (EHRs). The\nproposed GatorTron models performed remarkably better in much complex clinical\nNLP tasks such as natural language inference (9.6% and 7.5% improvements) and\nquestion answering (9.5% and 7.77% improvements) compared with existing smaller\nclinical transformer models (i.e., BioBERT and ClinicalBERT), demonstrating the\npotential of large transformer-based clinical models for advanced medical\nartificial intelligent (AI) applications such as question answering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PourNejatian_N/0/1/0/all/0/1\">Nima PourNejatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Hoo Chang Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kaleb E Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisien_C/0/1/0/all/0/1\">Christopher Parisien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Compas_C/0/1/0/all/0/1\">Colin Compas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1\">Cheryl Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_M/0/1/0/all/0/1\">Mona G Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magoc_T/0/1/0/all/0/1\">Tanja Magoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harle_C/0/1/0/all/0/1\">Christopher A Harle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipori_G/0/1/0/all/0/1\">Gloria Lipori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_D/0/1/0/all/0/1\">Duane A Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William R Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenkman_E/0/1/0/all/0/1\">Elizabeth A Shenkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessment of contextualised representations in detecting outcome phrases in clinical trials. (arXiv:2203.03547v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03547","description":"<p>Automating the recognition of outcomes reported in clinical trials using\nmachine learning has a huge potential of speeding up access to evidence\nnecessary in healthcare decision-making. Prior research has however\nacknowledged inadequate training corpora as a challenge for the Outcome\ndetection (OD) task. Additionally, several contextualized representations like\nBERT and ELMO have achieved unparalleled success in detecting various diseases,\ngenes, proteins, and chemicals, however, the same cannot be emphatically stated\nfor outcomes, because these models have been relatively under-tested and\nstudied for the OD task. We introduce \"EBM-COMET\", a dataset in which 300\nPubMed abstracts are expertly annotated for clinical outcomes. Unlike prior\nrelated datasets that use arbitrary outcome classifications, we use labels from\na taxonomy recently published to standardize outcome classifications. To\nextract outcomes, we fine-tune a variety of pre-trained contextualized\nrepresentations, additionally, we use frozen contextualized and\ncontext-independent representations in our custom neural model augmented with\nclinically informed Part-Of-Speech embeddings and a cost-sensitive loss\nfunction. We adopt strict evaluation for the trained models by rewarding them\nfor correctly identifying full outcome phrases rather than words within the\nentities i.e. given an outcome \"systolic blood pressure\", the models are\nrewarded a classification score only when they predict all 3 words in sequence,\notherwise, they are not rewarded. We observe our best model (BioBERT) achieve\n81.5\\% F1, 81.3\\% sensitivity and 98.0\\% specificity. We reach a consensus on\nwhich contextualized representations are best suited for detecting outcomes\nfrom clinical-trial abstracts. Furthermore, our best model outperforms scores\npublished on the original EBM-NLP dataset leader-board scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaho_M/0/1/0/all/0/1\">Micheal Abaho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_P/0/1/0/all/0/1\">Paula R Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodd_S/0/1/0/all/0/1\">Susanna Dodd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Mixing of Contextual Information in the Transformer. (arXiv:2203.04212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04212","description":"<p>The Transformer architecture aggregates input information through the\nself-attention mechanism, but there is no clear understanding of how this\ninformation is mixed across the entire model. Additionally, recent works have\ndemonstrated that attention weights alone are not enough to describe the flow\nof information. In this paper, we consider the whole attention block --\nmulti-head attention, residual connection, and layer normalization -- and\ndefine a metric to measure token-to-token interactions within each layer,\nconsidering the characteristics of the representation space. Then, we aggregate\nlayer-wise interpretations to provide input attribution scores for model\npredictions. Experimentally, we show that our method, ALTI (Aggregation of\nLayer-wise Token-to-token Interactions), provides faithful explanations and\noutperforms similar aggregation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1\">Javier Ferrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Onception: Active Learning with Expert Advice for Real World Machine Translation. (arXiv:2203.04507v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04507","description":"<p>Active learning can play an important role in low-resource settings (i.e.,\nwhere annotated data is scarce), by selecting which instances may be more\nworthy to annotate. Most active learning approaches for Machine Translation\nassume the existence of a pool of sentences in a source language, and rely on\nhuman annotators to provide translations or post-edits, which can still be\ncostly. In this article, we assume a real world human-in-the-loop scenario in\nwhich: (i) the source sentences may not be readily available, but instead\narrive in a stream; (ii) the automatic translations receive feedback in the\nform of a rating, instead of a correct/edited translation, since the\nhuman-in-the-loop might be a user looking for a translation, but not be able to\nprovide one. To tackle the challenge of deciding whether each incoming pair\nsource-translations is worthy to query for human feedback, we resort to a\nnumber of stream-based active learning query strategies. Moreover, since we not\nknow in advance which query strategy will be the most adequate for a certain\nlanguage pair and set of Machine Translation models, we propose to dynamically\ncombine multiple strategies using prediction with expert advice. Our\nexperiments show that using active learning allows to converge to the best\nMachine Translation systems with fewer human interactions. Furthermore,\ncombining multiple strategies using prediction with expert advice often\noutperforms several individual active learning strategies with even fewer\ninteractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendonca_V/0/1/0/all/0/1\">V&#xe2;nia Mendon&#xe7;a</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a> (1 and 2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Luisa Coheur</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Sardinha_A/0/1/0/all/0/1\">Alberto Sardinha</a> (1 and 2) ((1) INESC-ID Lisboa, (2) Instituto Superior T&#xe9;cnico, (3) Unbabel AI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05598","description":"<p>The study of the applicability of the BERTScore metric was conducted to\ntranslation quality assessment at the sentence level for English -&gt; Russian\ndirection. Experiments were performed with a pre-trained Multilingual BERT as\nwell as with a pair of Monolingual BERT models. To align monolingual\nembeddings, an orthogonal transformation based on anchor tokens was used. It\nwas demonstrated that such transformation helps to prevent mismatching issue\nand shown that this approach gives better results than using embeddings of the\nMultilingual model. To improve the token matching process it is proposed to\ncombine all incomplete WorkPiece tokens into meaningful words and use simple\naveraging of corresponding vectors and to calculate BERTScore based on anchor\ntokens only. Such modifications allowed us to achieve a better correlation of\nthe model predictions with human judgments. In addition to evaluating machine\ntranslation, several versions of human translation were evaluated as well, the\nproblems of this approach were listed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_A/0/1/0/all/0/1\">A.A. Vetrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorn_E/0/1/0/all/0/1\">E.A. Gorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. (arXiv:2203.05797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05797","description":"<p>Most of the open-domain dialogue models tend to perform poorly in the setting\nof long-term human-bot conversations. The possible reason is that they lack the\ncapability of understanding and memorizing long-term dialogue history\ninformation. To address this issue, we present a novel task of Long-term Memory\nConversation (LeMon) and then build a new dialogue dataset DuLeMon and a\ndialogue generation framework with Long-Term Memory (LTM) mechanism (called\nPLATO-LTM). This LTM mechanism enables our system to accurately extract and\ncontinuously update long-term persona memory without requiring multiple-session\ndialogue datasets for model training. To our knowledge, this is the first\nattempt to conduct real-time dynamic management of persona information of both\nparties, including the user and the bot. Results on DuLeMon indicate that\nPLATO-LTM can significantly outperform baselines in terms of long-term dialogue\nconsistency, leading to better dialogue engagingness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_Z/0/1/0/all/0/1\">Zhibin Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenquan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zheng-Yu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shihang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Semi-supervised classification of medical ultrasound images based on generative adversarial network. (arXiv:2203.06184v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06184","description":"<p>Medical ultrasound (US) is one of the most widely used imaging modalities in\nclinical practice. However, its use presents unique challenges such as variable\nimaging quality. Deep learning (DL) can be used as an advanced medical US\nimages analysis tool, while the performance of the DL model is greatly limited\nby the scarcity of big datasets. Here, we develop semi-supervised\nclassification enhancement (SSCE) structures by constructing seven\nconvolutional neural network (CNN) models and one of the most state-of-the-art\ngenerative adversarial network (GAN) models, StyleGAN2-ADA, to address this\nproblem. A breast cancer dataset with 780 images is used as our base dataset.\nThe results show that our SSCE structures obtain an accuracy of up to 97.9%,\nshowing a maximum 21.6% improvement compared with utilizing CNN models alone\nand outperforming the previous methods using the same dataset by up to 23.9%.\nWe believe our proposed state-of-the-art method can be regarded as a potential\nauxiliary tool for on-the-fly diagnoses of medical US images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoshan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chau Hung Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging universality of jet taggers through transfer learning. (arXiv:2203.06210v1 [hep-ph])","link":"http://arxiv.org/abs/2203.06210","description":"<p>A significant challenge in the tagging of boosted objects via\nmachine-learning technology is the prohibitive computational cost associated\nwith training sophisticated models. Nevertheless, the universality of QCD\nsuggests that a large amount of the information learnt in the training is\ncommon to different physical signals and experimental setups. In this article,\nwe explore the use of transfer learning techniques to develop fast and\ndata-efficient jet taggers that leverage such universality. We consider the\ngraph neural networks LundNet and ParticleNet, and introduce two prescriptions\nto transfer an existing tagger into a new signal based either on fine-tuning\nall the weights of a model or alternatively on freezing a fraction of them. In\nthe case of $W$-boson and top-quark tagging, we find that one can obtain\nreliable taggers using an order of magnitude less data with a corresponding\nspeed-up of the training process. Moreover, while keeping the size of the\ntraining data set fixed, we observe a speed-up of the training by up to a\nfactor of three. This offers a promising avenue to facilitate the use of such\ntools in collider physics experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/hep-ph/1/au:+Dreyer_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric A. Dreyer</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Grabarczyk_R/0/1/0/all/0/1\">Rados&#x142;aw Grabarczyk</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Monni_P/0/1/0/all/0/1\">Pier Francesco Monni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can I see an Example? Active Learning the Long Tail of Attributes and Relations. (arXiv:2203.06215v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06215","description":"<p>There has been significant progress in creating machine learning models that\nidentify objects in scenes along with their associated attributes and\nrelationships; however, there is a large gap between the best models and human\ncapabilities. One of the major reasons for this gap is the difficulty in\ncollecting sufficient amounts of annotated relations and attributes for\ntraining these systems. While some attributes and relations are abundant, the\ndistribution in the natural world and existing datasets is long tailed. In this\npaper, we address this problem by introducing a novel incremental active\nlearning framework that asks for attributes and relations in visual scenes.\nWhile conventional active learning methods ask for labels of specific examples,\nwe flip this framing to allow agents to ask for examples from specific\ncategories. Using this framing, we introduce an active sampling method that\nasks for examples from the tail of the data distribution and show that it\noutperforms classical active learning methods on Visual Genome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_M/0/1/0/all/0/1\">Maximilian Nickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1\">Ludovic Denoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Image Segmentation on MRI Images with Missing Modalities: A Review. (arXiv:2203.06217v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06217","description":"<p>Dealing with missing modalities in Magnetic Resonance Imaging (MRI) and\novercoming their negative repercussions is considered a hurdle in biomedical\nimaging. The combination of a specified set of modalities, which is selected\ndepending on the scenario and anatomical part being scanned, will provide\nmedical practitioners with full information about the region of interest in the\nhuman body, hence the missing MRI sequences should be reimbursed. The\ncompensation of the adverse impact of losing useful information owing to the\nlack of one or more modalities is a well-known challenge in the field of\ncomputer vision, particularly for medical image processing tasks including\ntumour segmentation, tissue classification, and image generation. Various\napproaches have been developed over time to mitigate this problem's negative\nimplications and this literature review goes through a significant number of\nthe networks that seek to do so. The approaches reviewed in this work are\nreviewed in detail, including earlier techniques such as synthesis methods as\nwell as later approaches that deploy deep learning, such as common latent space\nmodels, knowledge distillation networks, mutual information maximization, and\ngenerative adversarial networks (GANs). This work discusses the most important\napproaches that have been offered at the time of this writing, examining the\nnovelty, strength, and weakness of each one. Furthermore, the most commonly\nused MRI datasets are highlighted and described. The main goal of this research\nis to offer a performance evaluation of missing modality compensating networks,\nas well as to outline future strategies for dealing with this issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_N/0/1/0/all/0/1\">Nika Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehghanmanshadi_M/0/1/0/all/0/1\">Mohammad Dehghanmanshadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merhof_D/0/1/0/all/0/1\">Dorit Merhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pressure Ulcer Categorisation using Deep Learning: A Clinical Trial to Evaluate Model Performance. (arXiv:2203.06248v1 [cs.LG])","link":"http://arxiv.org/abs/2203.06248","description":"<p>Pressure ulcers are a challenge for patients and healthcare professionals. In\nthe UK, 700,000 people are affected by pressure ulcers each year. Treating them\ncosts the National Health Service {\\pounds}3.8 million every day. Their\netiology is complex and multifactorial. However, evidence has shown a strong\nlink between old age, disease-related sedentary lifestyles and unhealthy eating\nhabits. Pressure ulcers are caused by direct skin contact with a bed or chair\nwithout frequent position changes. Urinary and faecal incontinence, diabetes,\nand injuries that restrict body position and nutrition are also known risk\nfactors. Guidelines and treatments exist but their implementation and success\nvary across different healthcare settings. This is primarily because healthcare\npractitioners have a) minimal experience in dealing with pressure ulcers, and\nb) a general lack of understanding of pressure ulcer treatments. Poorly\nmanaged, pressure ulcers lead to severe pain, poor quality of life, and\nsignificant healthcare costs. In this paper, we report the findings of a\nclinical trial conducted by Mersey Care NHS Foundation Trust that evaluated the\nperformance of a faster region-based convolutional neural network and mobile\nplatform that categorised and documented pressure ulcers. The neural network\nclassifies category I, II, III, and IV pressure ulcers, deep tissue injuries,\nand unstageable pressure ulcers. Photographs of pressure ulcers taken by\ndistrict nurses are transmitted over 4/5G communications to an inferencing\nserver for classification. Classified images are stored and reviewed to assess\nthe model's predictions and relevance as a tool for clinical decision making\nand standardised reporting. The results from the study generated a mean average\nPrecision=0.6796, Recall=0.6997, F1-Score=0.6786 with 45 false positives using\nan @.75 confidence score threshold.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fergus_P/0/1/0/all/0/1\">Paul Fergus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalmers_C/0/1/0/all/0/1\">Carl Chalmers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_W/0/1/0/all/0/1\">William Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1\">Danny Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waraich_A/0/1/0/all/0/1\">Atif Waraich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perception Over Time: Temporal Dynamics for Robust Image Understanding. (arXiv:2203.06254v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06254","description":"<p>While deep learning surpasses human-level performance in narrow and specific\nvision tasks, it is fragile and over-confident in classification. For example,\nminor transformations in perspective, illumination, or object deformation in\nthe image space can result in drastically different labeling, which is\nespecially transparent via adversarial perturbations. On the other hand, human\nvisual perception is orders of magnitude more robust to changes in the input\nstimulus. But unfortunately, we are far from fully understanding and\nintegrating the underlying mechanisms that result in such robust perception. In\nthis work, we introduce a novel method of incorporating temporal dynamics into\nstatic image understanding. We describe a neuro-inspired method that decomposes\na single image into a series of coarse-to-fine images that simulates how\nbiological vision integrates information over time. Next, we demonstrate how\nour novel visual perception framework can utilize this information \"over time\"\nusing a biologically plausible algorithm with recurrent units, and as a result,\nsignificantly improving its accuracy and robustness over standard CNNs. We also\ncompare our proposed approach with state-of-the-art models and explicitly\nquantify our adversarial robustness properties through multiple ablation\nstudies. Our quantitative and qualitative results convincingly demonstrate\nexciting and transformative improvements over the standard computer vision and\ndeep learning architectures used today.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daniali_M/0/1/0/all/0/1\">Maryam Daniali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Edward Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preliminary experiments on automatic gender recognition based on online capital letters. (arXiv:2203.06265v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06265","description":"<p>In this paper we present some experiments to automatically classify online\nhandwritten text based on capital letters. Although handwritten text is not as\ndiscriminative as face or voice, we still found some chance for gender\nclassification based on handwritten text. Accuracies are up to 74%, even in the\nmost challenging case of capital letters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sesa_Nogueras_E/0/1/0/all/0/1\">Enric Sesa-Nogueras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISF: Multi-level Interactive Siamese Filtering for High-Fidelity Image Inpainting. (arXiv:2203.06304v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06304","description":"<p>Although achieving significant progress, existing deep generative inpainting\nmethods are far from real-world applications due to the low generalization\nacross different scenes. As a result, the generated images usually contain\nartifacts or the filled pixels differ greatly from the ground truth.\nImage-level predictive filtering is a widely used image restoration technique,\npredicting suitable kernels adaptively according to different input scenes.\nInspired by this inherent advantage, we explore the possibility of addressing\nimage inpainting as a filtering task. To this end, we first study the\nadvantages and challenges of image-level predictive filtering for image\ninpainting: the method can preserve local structures and avoid artifacts but\nfails to fill large missing areas. Then, we propose semantic filtering by\nconducting filtering on the deep feature level, which fills the missing\nsemantic information but fails to recover the details. To address the issues\nwhile adopting the respective advantages, we propose a novel filtering\ntechnique, i.e., Multilevel Interactive Siamese Filtering (MISF), which\ncontains two branches: kernel prediction branch (KPB) and semantic &amp; image\nfiltering branch (SIFB). These two branches are interactively linked: SIFB\nprovides multi-level features for KPB while KPB predicts dynamic kernels for\nSIFB. As a result, the final method takes the advantage of effective semantic &amp;\nimage-level filling for high-fidelity inpainting. We validate our method on\nthree challenging datasets, i.e., Dunhuang, Places2, and CelebA. Our method\noutperforms state-of-the-art baselines on four metrics, i.e., L1, PSNR, SSIM,\nand LPIPS. Please try the released code and model at\nhttps://github.com/tsingqguo/misf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Di Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Radiomics: Paradigm for Systematic Incorporation of Multi-Flavoured Radiomics Features. (arXiv:2203.06314v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06314","description":"<p>Radiomics features extract quantitative information from medical images,\ntowards the derivation of biomarkers for clinical tasks, such as diagnosis,\nprognosis, or treatment response assessment. Different image discretization\nparameters (e.g. bin number or size), convolutional filters, segmentation\nperturbation, or multi-modality fusion levels can be used to generate radiomics\nfeatures and ultimately signatures. Commonly, only one set of parameters is\nused; resulting in only one value or flavour for a given RF. We propose tensor\nradiomics (TR) where tensors of features calculated with multiple combinations\nof parameters (i.e. flavours) are utilized to optimize the construction of\nradiomics signatures. We present examples of TR as applied to PET/CT, MRI, and\nCT imaging invoking machine learning or deep learning solutions, and\nreproducibility analyses: (1) TR via varying bin sizes on CT images of lung\ncancer and PET-CT images of head &amp; neck cancer (HNC) for overall survival\nprediction. A hybrid deep neural network, referred to as TR-Net, along with two\nML-based flavour fusion methods showed improved accuracy compared to regular\nrediomics features. (2) TR built from different segmentation perturbations and\ndifferent bin sizes for classification of late-stage lung cancer response to\nfirst-line immunotherapy using CT images. TR improved predicted patient\nresponses. (3) TR via multi-flavour generated radiomics features in MR imaging\nshowed improved reproducibility when compared to many single-flavour features.\n(4) TR via multiple PET/CT fusions in HNC. Flavours were built from different\nfusions using methods, such as Laplacian pyramids and wavelet transforms. TR\nimproved overall survival prediction. Our results suggest that the proposed TR\nparadigm has the potential to improve performance capabilities in different\nmedical imaging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahmim_A/0/1/0/all/0/1\">Arman Rahmim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toosi_A/0/1/0/all/0/1\">Amirhosein Toosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salmanpour_M/0/1/0/all/0/1\">Mohammad R. Salmanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubljevic_N/0/1/0/all/0/1\">Natalia Dubljevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janzen_I/0/1/0/all/0/1\">Ian Janzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiri_I/0/1/0/all/0/1\">Isaac Shiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Mohamad A. Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1\">Ren Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Cheryl Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_H/0/1/0/all/0/1\">Habib Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacAulay_C/0/1/0/all/0/1\">Calum MacAulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uribe_C/0/1/0/all/0/1\">Carlos Uribe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousefirizi_F/0/1/0/all/0/1\">Fereshteh Yousefirizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable VisTR: Spatio temporal deformable attention for video instance segmentation. (arXiv:2203.06318v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06318","description":"<p>Video instance segmentation (VIS) task requires classifying, segmenting, and\ntracking object instances over all frames in a video clip. Recently, VisTR has\nbeen proposed as end-to-end transformer-based VIS framework, while\ndemonstrating state-of-the-art performance. However, VisTR is slow to converge\nduring training, requiring around 1000 GPU hours due to the high computational\ncost of its transformer attention module. To improve the training efficiency,\nwe propose Deformable VisTR, leveraging spatio-temporal deformable attention\nmodule that only attends to a small fixed set of key spatio-temporal sampling\npoints around a reference point. This enables Deformable VisTR to achieve\nlinear computation in the size of spatio-temporal feature maps. Moreover, it\ncan achieve on par performance as the original VisTR with 10$\\times$ less GPU\ntraining hours. We validate the effectiveness of our method on the Youtube-VIS\nbenchmark. Code is available at https://github.com/skrya/DefVIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yarram_S/0/1/0/all/0/1\">Sudhir Yarram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jialian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR. (arXiv:2203.06319v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06319","description":"<p>3D object detection plays a fundamental role in enabling autonomous driving,\nwhich is regarded as the significant key to unlocking the bottleneck of\ncontemporary transportation systems from the perspectives of safety, mobility,\nand sustainability. Most of the state-of-the-art (SOTA) object detection\nmethods from point clouds are developed based on a single onboard LiDAR, whose\nperformance will be inevitably limited by the range and occlusion, especially\nin dense traffic scenarios. In this paper, we propose \\textit{PillarGrid}, a\nnovel cooperative perception method fusing information from multiple 3D LiDARs\n(both on-board and roadside), to enhance the situation awareness for connected\nand automated vehicles (CAVs). PillarGrid consists of four main phases: 1)\ncooperative preprocessing of point clouds, 2) pillar-wise voxelization and\nfeature extraction, 3) grid-wise deep fusion of features from multiple sensors,\nand 4) convolutional neural network (CNN)-based augmented 3D object detection.\nA novel cooperative perception platform is developed for model training and\ntesting. Extensive experimentation shows that PillarGrid outperforms the SOTA\nsingle-LiDAR-based 3D object detection methods with respect to both accuracy\nand range by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew J. Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisbot_A/0/1/0/all/0/1\">Akin Sisbot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation. (arXiv:2203.06321v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06321","description":"<p>Remarkable achievements have been attained with Generative Adversarial\nNetworks (GANs) in image-to-image translation. However, due to a tremendous\namount of parameters, state-of-the-art GANs usually suffer from low efficiency\nand bulky memory usage. To tackle this challenge, firstly, this paper\ninvestigates GANs performance from a frequency perspective. The results show\nthat GANs, especially small GANs lack the ability to generate high-quality high\nfrequency information. To address this problem, we propose a novel knowledge\ndistillation method referred to as wavelet knowledge distillation. Instead of\ndirectly distilling the generated images of teachers, wavelet knowledge\ndistillation first decomposes the images into different frequency bands with\ndiscrete wavelet transformation and then only distills the high frequency\nbands. As a result, the student GAN can pay more attention to its learning on\nhigh frequency bands. Experiments demonstrate that our method leads to 7.08\ntimes compression and 6.80 times acceleration on CycleGAN with almost no\nperformance drop. Additionally, we have studied the relation between\ndiscriminators and generators which shows that the compression of\ndiscriminators can promote the performance of compressed generators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xiaobing Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Style Transfer: from Artistic to Photorealistic. (arXiv:2203.06328v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06328","description":"<p>The rapid advancement of deep learning has significantly boomed the\ndevelopment of photorealistic style transfer. In this review, we reviewed the\ndevelopment of photorealistic style transfer starting from artistic style\ntransfer and the contribution of traditional image processing techniques on\nphotorealistic style transfer, including some work that had been completed in\nthe Multimedia lab at the University of Alberta. Many techniques were discussed\nin this review. However, our focus is on VGG-based techniques, whitening and\ncoloring transform (WCTs) based techniques, the combination of deep learning\nwith traditional image processing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenggui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Li Bin Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-FedRL: Federated Hyperparameter Optimization for Multi-institutional Medical Image Segmentation. (arXiv:2203.06338v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06338","description":"<p>Federated learning (FL) is a distributed machine learning technique that\nenables collaborative model training while avoiding explicit data sharing. The\ninherent privacy-preserving property of FL algorithms makes them especially\nattractive to the medical field. However, in case of heterogeneous client data\ndistributions, standard FL methods are unstable and require intensive\nhyperparameter tuning to achieve optimal performance. Conventional\nhyperparameter optimization algorithms are impractical in real-world FL\napplications as they involve numerous training trials, which are often not\naffordable with limited compute budgets. In this work, we propose an efficient\nreinforcement learning~(RL)-based federated hyperparameter optimization\nalgorithm, termed Auto-FedRL, in which an online RL agent can dynamically\nadjust hyperparameters of each client based on the current training progress.\nExtensive experiments are conducted to investigate different search strategies\nand RL agents. The effectiveness of the proposed method is validated on a\nheterogeneous data split of the CIFAR-10 dataset as well as two real-world\nmedical image segmentation datasets for COVID-19 lesion segmentation in chest\nCT and pancreas segmentation in abdominal CT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_A/0/1/0/all/0/1\">An Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_C/0/1/0/all/0/1\">Can Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harmon_S/0/1/0/all/0/1\">Stephanie Harmon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_E/0/1/0/all/0/1\">Evrim Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turkbey_B/0/1/0/all/0/1\">Baris Turkbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wood_B/0/1/0/all/0/1\">Bradford Wood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patella_F/0/1/0/all/0/1\">Francesca Patella</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stellato_E/0/1/0/all/0/1\">Elvira Stellato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carrafiello_G/0/1/0/all/0/1\">Gianpaolo Carrafiello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger R. Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy. (arXiv:2203.06345v1 [cs.LG])","link":"http://arxiv.org/abs/2203.06345","description":"<p>Vision transformers (ViTs) have gained increasing popularity as they are\ncommonly believed to own higher modeling capacity and representation\nflexibility, than traditional convolutional networks. However, it is\nquestionable whether such potential has been fully unleashed in practice, as\nthe learned ViTs often suffer from over-smoothening, yielding likely redundant\nmodels. Recent works made preliminary attempts to identify and alleviate such\nredundancy, e.g., via regularizing embedding similarity or re-injecting\nconvolution-like structures. However, a \"head-to-toe assessment\" regarding the\nextent of redundancy in ViTs, and how much we could gain by thoroughly\nmitigating such, has been absent for this field. This paper, for the first\ntime, systematically studies the ubiquitous existence of redundancy at all\nthree levels: patch embedding, attention map, and weight space. In view of\nthem, we advocate a principle of diversity for training ViTs, by presenting\ncorresponding regularizers that encourage the representation diversity and\ncoverage at each of those levels, that enabling capturing more discriminative\ninformation. Extensive experiments on ImageNet with a number of ViT backbones\nvalidate the effectiveness of our proposals, largely eliminating the observed\nViT redundancy and significantly boosting the model generalization. For\nexample, our diversified DeiT obtains 0.70%~1.76% accuracy boosts on ImageNet\nwith highly reduced similarity. Our codes are fully available in\nhttps://github.com/VITA-Group/Diverse-ViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LesionPaste: One-Shot Anomaly Detection for Medical Images. (arXiv:2203.06354v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06354","description":"<p>Due to the high cost of manually annotating medical images, especially for\nlarge-scale datasets, anomaly detection has been explored through training\nmodels with only normal data. Lacking prior knowledge of true anomalies is the\nmain reason for the limited application of previous anomaly detection methods,\nespecially in the medical image analysis realm. In this work, we propose a\none-shot anomaly detection framework, namely LesionPaste, that utilizes true\nanomalies from a single annotated sample and synthesizes artificial anomalous\nsamples for anomaly detection. First, a lesion bank is constructed by applying\naugmentation to randomly selected lesion patches. Then, MixUp is adopted to\npaste patches from the lesion bank at random positions in normal images to\nsynthesize anomalous samples for training. Finally, a classification network is\ntrained using the synthetic abnormal samples and the true normal data.\nExtensive experiments are conducted on two publicly-available medical image\ndatasets with different types of abnormalities. On both datasets, our proposed\nLesionPaste largely outperforms several state-of-the-art unsupervised and\nsemi-supervised anomaly detection methods, and is on a par with the\nfully-supervised counterpart. To note, LesionPaste is even better than the\nfully-supervised method in detecting early-stage diabetic retinopathy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Weikai Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yijin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventFormer: AU Event Transformer for Facial Action Unit Event Detection. (arXiv:2203.06355v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06355","description":"<p>Facial action units (AUs) play an indispensable role in human emotion\nanalysis. We observe that although AU-based high-level emotion analysis is\nurgently needed by real-world applications, frame-level AU results provided by\nprevious works cannot be directly used for such analysis. Moreover, as AUs are\ndynamic processes, the utilization of global temporal information is important\nbut has been gravely ignored in the literature. To this end, we propose\nEventFormer for AU event detection, which is the first work directly detecting\nAU events from a video sequence by viewing AU event detection as a multiple\nclass-specific sets prediction problem. Extensive experiments conducted on a\ncommonly used AU benchmark dataset, BP4D, show the superiority of EventFormer\nunder suitable metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiarui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Diqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yun Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taking an Emotional Look at Video Paragraph Captioning. (arXiv:2203.06356v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06356","description":"<p>Translating visual data into natural language is essential for machines to\nunderstand the world and interact with humans. In this work, a comprehensive\nstudy is conducted on video paragraph captioning, with the goal to generate\nparagraph-level descriptions for a given video. However, current researches\nmainly focus on detecting objective facts, ignoring the needs to establish the\nlogical associations between sentences and to discover more accurate emotions\nrelated to video contents. Such a problem impairs fluent and abundant\nexpressions of predicted captions, which are far below human language tandards.\nTo solve this problem, we propose to construct a large-scale emotion and logic\ndriven multilingual dataset for this task. This dataset is named EMVPC\n(standing for \"Emotional Video Paragraph Captioning\") and contains 53\nwidely-used emotions in daily life, 376 common scenes corresponding to these\nemotions, 10,291 high-quality videos and 20,582 elaborated paragraph captions\nwith English and Chinese versions. Relevant emotion categories, scene labels,\nemotion word labels and logic word labels are also provided in this new\ndataset. The proposed EMVPC dataset intends to provide full-fledged video\nparagraph captioning in terms of rich emotions, coherent logic and elaborate\nexpressions, which can also benefit other tasks in vision-language fields.\nFurthermore, a comprehensive study is conducted through experiments on existing\nbenchmark video paragraph captioning datasets and the proposed EMVPC. The\nstateof-the-art schemes from different visual captioning tasks are compared in\nterms of 15 popular metrics, and their detailed objective as well as subjective\nresults are summarized. Finally, remaining problems and future directions of\nvideo paragraph captioning are also discussed. The unique perspective of this\nwork is expected to boost further development in video paragraph captioning\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tengpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. (arXiv:2203.06359v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06359","description":"<p>Non-exemplar class-incremental learning is to recognize both the old and new\nclasses when old class samples cannot be saved. It is a challenging task since\nrepresentation optimization and feature retention can only be achieved under\nsupervision from new classes. To address this problem, we propose a novel\nself-sustaining representation expansion scheme. Our scheme consists of a\nstructure reorganization strategy that fuses main-branch expansion and\nside-branch updating to maintain the old features, and a main-branch\ndistillation scheme to transfer the invariant knowledge. Furthermore, a\nprototype selection mechanism is proposed to enhance the discrimination between\nthe old and new classes by selectively incorporating new samples into the\ndistillation process. Extensive experiments on three benchmarks demonstrate\nsignificant incremental performance, outperforming the state-of-the-art methods\nby a margin of 3%, 3% and 6%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDT-Net: Multi-domain Transfer by Perceptual Supervision for Unpaired Images in OCT Scan. (arXiv:2203.06363v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06363","description":"<p>Deep learning models tend to underperform in the presence of domain shifts.\nDomain transfer has recently emerged as a promising approach wherein images\nexhibiting a domain shift are transformed into other domains for augmentation\nor adaptation. However, with the absence of paired and annotated images, most\ndomain transfer methods mainly rely on adversarial networks and weak cycle\nconsistency, which could result in incomplete domain transfer or poor adherence\nto the original image content. In this paper, we introduce MDT-Net to address\nthe limitations above through a multi-domain transfer model based on perceptual\nsupervision. Specifically, our model consists of an encoder-decoder network,\nwhich aims to preserve anatomical structures, and multiple domain-specific\ntransfer modules, which guide the domain transition through feature\ntransformation. During the inference, MDT-Net can directly transfer images from\nthe source domain to multiple target domains at one time without any reference\nimage. To demonstrate the performance of MDT-Net, we evaluate it on RETOUCH\ndataset, comprising OCT scans from three different scanner devices (domains),\nfor multi-domain transfer. We also take the transformed results as additional\ntraining images for fluid segmentation in OCT scans in the tasks of domain\nadaptation and data augmentation. Experimental results show that MDT-Net can\noutperform other domain transfer models qualitatively and quantitatively.\nFurthermore, the significant improvement in dice scores over multiple\nsegmentation models also demonstrates the effectiveness and efficiency of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weinan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fotedar_G/0/1/0/all/0/1\">Gaurav Fotedar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajbakhsh_N/0/1/0/all/0/1\">Nima Tajbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaowei Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiated Relevances Embedding for Group-based Referring Expression Comprehension. (arXiv:2203.06382v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06382","description":"<p>Referring expression comprehension (REC) aims to locate a certain object in\nan image referred by a natural language expression. For joint understanding of\nregions and expressions, existing REC works typically target on modeling the\ncross-modal relevance in each region-expression pair within each single image.\nIn this paper, we explore a new but general REC-related problem, named\nGroup-based REC, where the regions and expressions can come from different\nsubject-related images (images in the same group), e.g., sets of photo albums\nor video frames. Different from REC, Group-based REC involves differentiated\ncross-modal relevances within each group and across different groups, which,\nhowever, are neglected in the existing one-line paradigm. To this end, we\npropose a novel relevance-guided multi-group self-paced learning schema (termed\nRMSL), where the within-group region-expression pairs are adaptively assigned\nwith different priorities according to their cross-modal relevances, and the\nbias of the group priority is balanced via an across-group relevance constraint\nsimultaneously. In particular, based on the visual and textual semantic\nfeatures, RMSL conducts an adaptive learning cycle upon triplet ranking, where\n(1) the target-negative region-expression pairs with low within-group\nrelevances are used preferentially in model training to distinguish the primary\nsemantics of the target objects, and (2) an across-group relevance\nregularization is integrated into model training to balance the bias of group\npriority. The relevances, the pairs, and the model parameters are alternatively\nupdated upon a unified self-paced hinge loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fuhai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation. (arXiv:2203.06386v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06386","description":"<p>The recent large-scale vision-language pre-training (VLP) of dual-stream\narchitectures (e.g., CLIP) with a tremendous amount of image-text pair data,\nhas shown its superiority on various multimodal alignment tasks. Despite its\nsuccess, the resulting models are not capable of multimodal generative tasks\ndue to the weak text encoder. To tackle this problem, we propose to augment the\ndual-stream VLP model with a textual pre-trained language model (PLM) via\nvision-language knowledge distillation (VLKD), enabling the capability for\nmultimodal generation. VLKD is pretty data- and computation-efficient compared\nto the pre-training from scratch. Experimental results show that the resulting\nmodel has strong zero-shot performance on multimodal generation tasks, such as\nopen-ended visual question answering and image captioning. For example, it\nachieves 44.5% zero-shot accuracy on the VQAv2 dataset, surpassing the previous\nstate-of-the-art zero-shot model with $7\\times$ fewer parameters. Furthermore,\nthe original textual language understanding and generation ability of the PLM\nis maintained after VLKD, which makes our model versatile for both multimodal\nand unimodal tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint CNN and Transformer Network via weakly supervised Learning for efficient crowd counting. (arXiv:2203.06388v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06388","description":"<p>Currently, for crowd counting, the fully supervised methods via density map\nestimation are the mainstream research directions. However, such methods need\nlocation-level annotation of persons in an image, which is time-consuming and\nlaborious. Therefore, the weakly supervised method just relying upon the\ncount-level annotation is urgently needed. Since CNN is not suitable for\nmodeling the global context and the interactions between image patches, crowd\ncounting with weakly supervised learning via CNN generally can not show good\nperformance. The weakly supervised model via Transformer was sequentially\nproposed to model the global context and learn contrast features. However, the\ntransformer directly partitions the crowd images into a series of tokens, which\nmay not be a good choice due to each pedestrian being an independent\nindividual, and the parameter number of the network is very large. Hence, we\npropose a Joint CNN and Transformer Network (JCTNet) via weakly supervised\nlearning for crowd counting in this paper. JCTNet consists of three parts: CNN\nfeature extraction module (CFM), Transformer feature extraction module (TFM),\nand counting regression module (CRM). In particular, the CFM extracts crowd\nsemantic information features, then sends their patch partitions to TRM for\nmodeling global context, and CRM is used to predict the number of people.\nExtensive experiments and visualizations demonstrate that JCTNet can\neffectively focus on the crowd regions and obtain superior weakly supervised\ncounting performance on five mainstream datasets. The number of parameters of\nthe model can be reduced by about 67%~73% compared with the pure Transformer\nworks. We also tried to explain the phenomenon that a model constrained only by\ncount-level annotations can still focus on the crowd regions. We believe our\nwork can promote further research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fusen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_F/0/1/0/all/0/1\">Fei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaofeng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jun Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection. (arXiv:2203.06398v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06398","description":"<p>Domain Adaptive Object Detection (DAOD) leverages a labeled domain to learn\nan object detector generalizing to a novel domain free of annotations. Recent\nadvances align class-conditional distributions by narrowing down cross-domain\nprototypes (class centers). Though great success,they ignore the significant\nwithin-class variance and the domain-mismatched semantics within the training\nbatch, leading to a sub-optimal adaptation. To overcome these challenges, we\npropose a novel SemantIc-complete Graph MAtching (SIGMA) framework for DAOD,\nwhich completes mismatched semantics and reformulates the adaptation with graph\nmatching. Specifically, we design a Graph-embedded Semantic Completion module\n(GSC) that completes mismatched semantics through generating hallucination\ngraph nodes in missing categories. Then, we establish cross-image graphs to\nmodel class-conditional distributions and learn a graph-guided memory bank for\nbetter semantic completion in turn. After representing the source and target\ndata as graphs, we reformulate the adaptation as a graph matching problem,\ni.e., finding well-matched node pairs across graphs to reduce the domain gap,\nwhich is solved with a novel Bipartite Graph Matching adaptor (BGM). In a\nnutshell, we utilize graph nodes to establish semantic-aware node affinity and\nleverage graph edges as quadratic constraints in a structure-aware matching\nloss, achieving fine-grained adaptation with a node-to-node graph matching.\nExtensive experiments verify that SIGMA outperforms existing works\nsignificantly. Our codes are available at\nhttps://github.com/CityU-AIM-Group/SIGMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wuyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel Proposal Network for Arbitrary Shape Text Detection. (arXiv:2203.06410v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06410","description":"<p>Segmentation-based methods have achieved great success for arbitrary shape\ntext detection. However, separating neighboring text instances is still one of\nthe most challenging problems due to the complexity of texts in scene images.\nIn this paper, we propose an innovative Kernel Proposal Network (dubbed KPN)\nfor arbitrary shape text detection. The proposed KPN can separate neighboring\ntext instances by classifying different texts into instance-independent feature\nmaps, meanwhile avoiding the complex aggregation process existing in\nsegmentation-based arbitrary shape text detection methods. To be concrete, our\nKPN will predict a Gaussian center map for each text image, which will be used\nto extract a series of candidate kernel proposals (i.e., dynamic convolution\nkernel) from the embedding feature maps according to their corresponding\nkeypoint positions. To enforce the independence between kernel proposals, we\npropose a novel orthogonal learning loss (OLL) via orthogonal constraints.\nSpecifically, our kernel proposals contain important self-information learned\nby network and location information by position embedding. Finally, kernel\nproposals will individually convolve all embedding feature maps for generating\nindividual embedded maps of text instances. In this way, our KPN can\neffectively separate neighboring text instances and improve the robustness\nagainst unclear boundaries. To our knowledge, our work is the first to\nintroduce the dynamic convolution kernel strategy to efficiently and\neffectively tackle the adhesion problem of neighboring text instances in text\ndetection. Experimental results on challenging datasets verify the impressive\nperformance and efficiency of our method. The code and model are available at\nhttps://github.com/GXYM/KPN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jie-Bo Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrence-in-Recurrence Networks for Video Deblurring. (arXiv:2203.06418v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06418","description":"<p>State-of-the-art video deblurring methods often adopt recurrent neural\nnetworks to model the temporal dependency between the frames. While the hidden\nstates play key role in delivering information to the next frame, abrupt motion\nblur tend to weaken the relevance in the neighbor frames. In this paper, we\npropose recurrence-in-recurrence network architecture to cope with the\nlimitations of short-ranged memory. We employ additional recurrent units inside\nthe RNN cell. First, we employ inner-recurrence module (IRM) to manage the\nlong-ranged dependency in a sequence. IRM learns to keep track of the cell\nmemory and provides complementary information to find the deblurred frames.\nSecond, we adopt an attention-based temporal blending strategy to extract the\nnecessary part of the information in the local neighborhood. The adpative\ntemporal blending (ATB) can either attenuate or amplify the features by the\nspatial attention. Our extensive experimental results and analysis validate the\neffectiveness of IRM and ATB on various RNN architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1\">Joonkyu Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nah_S/0/1/0/all/0/1\">Seungjun Nah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-stage Video Instance Segmentation: From Frame-in Frame-out to Clip-in Clip-out. (arXiv:2203.06421v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06421","description":"<p>Many video instance segmentation (VIS) methods partition a video sequence\ninto individual frames to detect and segment objects frame by frame. However,\nsuch a frame-in frame-out (FiFo) pipeline is ineffective to exploit the\ntemporal information. Based on the fact that adjacent frames in a short clip\nare highly coherent in content, we propose to extend the one-stage FiFo\nframework to a clip-in clip-out (CiCo) one, which performs VIS clip by clip.\nSpecifically, we stack FPN features of all frames in a short video clip to\nbuild a spatio-temporal feature cube, and replace the 2D conv layers in the\nprediction heads and the mask branch with 3D conv layers, forming clip-level\nprediction heads (CPH) and clip-level mask heads (CMH). Then the clip-level\nmasks of an instance can be generated by feeding its box-level predictions from\nCPH and clip-level features from CMH into a small fully convolutional network.\nA clip-level segmentation loss is proposed to ensure that the generated\ninstance masks are temporally coherent in the clip. The proposed CiCo strategy\nis free of inter-frame alignment, and can be easily embedded into existing FiFo\nbased VIS approaches. To validate the generality and effectiveness of our CiCo\nstrategy, we apply it to two representative FiFo methods, Yolact\n\\cite{bolya2019yolact} and CondInst \\cite{tian2020conditional}, resulting in\ntwo new one-stage VIS models, namely CiCo-Yolact and CiCo-CondInst, which\nachieve 37.1/37.3\\%, 35.2/35.4\\% and 17.2/18.0\\% mask AP using the ResNet50\nbackbone, and 41.8/41.4\\%, 38.0/38.9\\% and 18.0/18.2\\% mask AP using the Swin\nTransformer tiny backbone on YouTube-VIS 2019, 2021 and OVIS valid sets,\nrespectively, recording new state-of-the-arts. Code and video demos of CiCo can\nbe found at \\url{https://github.com/MinghanLi/CiCo}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VariabilityTrack:Multi-Object Tracking with Variable Speed Object Movement. (arXiv:2203.06424v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06424","description":"<p>Multi-object tracking (MOT) aims at estimating bounding boxes and identities\nof objects in videos. Most methods can be roughly classified as\ntracking-by-detection and joint-detection-association paradigms. Although the\nlatter has elicited more attention and demonstrates comparable performance\nrelative than the former, we claim that the tracking-by-detection paradigm is\nstill the optimal solution in terms of tracking accuracy,such as\nByteTrack,which achieves 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of\nMOT17 with 30 FPS running speed on a single V100 GPU.However, under complex\nperspectives such as vehicle and UAV acceleration, the performance of such a\ntracker using uniform Kalman filter will be greatly affected, resulting in\ntracking loss.In this paper, we propose a variable speed Kalman filter\nalgorithm based on environmental feedback and improve the matching process,\nwhich can greatly improve the tracking effect in complex variable speed scenes\nwhile maintaining high tracking accuracy in relatively static scenes.\nEventually, higher MOTA and IDF1 results can be achieved on MOT17 test set than\nByteTrack\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Run Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">JinLin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qiao Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAFO-Loss: VAscular Feature Optimised Loss Function for Retinal Artery/Vein Segmentation. (arXiv:2203.06425v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06425","description":"<p>Estimating clinically-relevant vascular features following vessel\nsegmentation is a standard pipeline for retinal vessel analysis, which provides\npotential ocular biomarkers for both ophthalmic disease and systemic disease.\nIn this work, we integrate these clinical features into a novel vascular\nfeature optimised loss function (VAFO-Loss), in order to regularise networks to\nproduce segmentation maps, with which more accurate vascular features can be\nderived. Two common vascular features, vessel density and fractal dimension,\nare identified to be sensitive to intra-segment misclassification, which is a\nwell-recognised problem in multi-class artery/vein segmentation particularly\nhindering the estimation of these vascular features. Thus we encode these two\nfeatures into VAFO-Loss. We first show that incorporating our end-to-end\nVAFO-Loss in standard segmentation networks indeed improves vascular feature\nestimation, yielding quantitative improvement in stroke incidence prediction, a\nclinical downstream task. We also report a technically interesting finding that\nthe trained segmentation network, albeit biased by the feature optimised loss\nVAFO-Loss, shows statistically significant improvement in segmentation metrics,\ncompared to those trained with other state-of-the-art segmentation losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yukun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Moucheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B. Blumberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1\">An Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Siegfried K. Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keane_P/0/1/0/all/0/1\">Pearse A. Keane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFTR: Depth-supervised Hierarchical Feature Fusion Transformer for Salient Object Detection. (arXiv:2203.06429v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06429","description":"<p>Automated salient object detection (SOD) plays an increasingly crucial role\nin many computer vision applications. Although existing frameworks achieve\nimpressive SOD performances especially with the development of deep learning\ntechniques, their performances still have room for improvement. In this work,\nwe propose a novel pure Transformer-based SOD framework, namely\nDepth-supervised hierarchical feature Fusion TRansformer (DFTR), to further\nimprove the accuracy of both RGB and RGB-D SOD. The proposed DFTR involves\nthree primary improvements: 1) The backbone of feature encoder is switched from\na convolutional neural network to a Swin Transformer for more effective feature\nextraction; 2) We propose a multi-scale feature aggregation (MFA) module to\nfully exploit the multi-scale features encoded by the Swin Transformer in a\ncoarse-to-fine manner; 3) Following recent studies, we formulate an auxiliary\ntask of depth map prediction and use the ground-truth depth maps as extra\nsupervision signals for network learning. To enable bidirectional information\nflow between saliency and depth branches, a novel multi-task feature fusion\n(MFF) module is integrated into our DFTR. We extensively evaluate the proposed\nDFTR on ten benchmarking datasets. Experimental results show that our DFTR\nconsistently outperforms the existing state-of-the-art methods for both RGB and\nRGB-D SOD tasks. The code and model will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based conditional inpainting for restoration of artifact-affected 4D CT images. (arXiv:2203.06431v1 [physics.med-ph])","link":"http://arxiv.org/abs/2203.06431","description":"<p>4D CT imaging is an essential component of radiotherapy of thoracic/abdominal\ntumors. 4D CT images are, however, often affected by artifacts that compromise\ntreatment planning quality. In this work, deep learning (DL)-based conditional\ninpainting is proposed to restore anatomically correct image information of\nartifact-affected areas. The restoration approach consists of a two-stage\nprocess: DL-based detection of common interpolation (INT) and double structure\n(DS) artifacts, followed by conditional inpainting applied to the artifact\nareas. In this context, conditional refers to a guidance of the inpainting\nprocess by patient-specific image data to ensure anatomically reliable results.\nEvaluation is based on 65 in-house 4D CT data sets of lung cancer patients (48\nwith only slight artifacts, 17 with pronounced artifacts) and the publicly\navailable DIRLab 4D CT data (independent external test set). Automated artifact\ndetection revealed a ROC-AUC of 0.99 for INT and 0.97 for DS artifacts\n(in-house data). The proposed inpainting method decreased the average root mean\nsquared error (RMSE) by 60% (DS) and 42% (INT) for the in-house evaluation data\n(simulated artifacts for the slight artifact data; original data were\nconsidered as ground truth for RMSE computation). For the external DIR-Lab\ndata, the RMSE decreased by 65% and 36%, respectively. Applied to the\npronounced artifact data group, on average 68% of the detectable artifacts were\nremoved. The results highlight the potential of DL-based inpainting for the\nrestoration of artifact-affected 4D CT data. Improved performance of\nconditional inpainting (compared to standard inpainting) illustrates the\nbenefits of exploiting patient-specific prior knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Madesta_F/0/1/0/all/0/1\">Frederic Madesta</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sentker_T/0/1/0/all/0/1\">Thilo Sentker</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gauer_T/0/1/0/all/0/1\">Tobias Gauer</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Werner_R/0/1/0/all/0/1\">Rene Werner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DATR: Domain-adaptive transformer for multi-domain landmark detection. (arXiv:2203.06433v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06433","description":"<p>Accurate anatomical landmark detection plays an increasingly vital role in\nmedical image analysis. Although existing methods achieve satisfying\nperformance, they are mostly based on CNN and specialized for a single domain\nsay associated with a particular anatomical region. In this work, we propose a\nuniversal model for multi-domain landmark detection by taking advantage of\ntransformer for modeling long dependencies and develop a domain-adaptive\ntransformer model, named as DATR, which is trained on multiple mixed datasets\nfrom different anatomies and capable of detecting landmarks of any image from\nthose anatomies. The proposed DATR exhibits three primary features: (i) It is\nthe first universal model which introduces transformer as an encoder for\nmulti-anatomy landmark detection; (ii) We design a domain-adaptive transformer\nfor anatomy-aware landmark detection, which can be effectively extended to any\nother transformer network; (iii) Following previous studies, we employ a\nlight-weighted guidance network, which encourages the transformer network to\ndetect more accurate landmarks. We carry out experiments on three widely used\nX-ray datasets for landmark detection, which have 1,588 images and 62 landmarks\nin total, including three different anatomies (head, hand, and chest).\nExperimental results demonstrate that our proposed DATR achieves\nstate-of-the-art performances by most metrics and behaves much better than any\nprevious convolution-based models. The code will be released publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qingsong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing Rolling Shutter Images Alive with Dual Reversed Distortion. (arXiv:2203.06451v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06451","description":"<p>Rolling shutter (RS) distortion can be interpreted as the result of picking a\nrow of pixels from instant global shutter (GS) frames over time during the\nexposure of the RS camera. This means that the information of each instant GS\nframe is partially, yet sequentially, embedded into the row-dependent\ndistortion. Inspired by this fact, we address the challenging task of reversing\nthis process, i.e., extracting undistorted GS frames from images suffering from\nRS distortion. However, since RS distortion is coupled with other factors such\nas readout settings and the relative velocity of scene elements to the camera,\nmodels that only exploit the geometric correlation between temporally adjacent\nimages suffer from poor generality in processing data with different readout\nsettings and dynamic scenes with both camera motion and object motion. In this\npaper, instead of two consecutive frames, we propose to exploit a pair of\nimages captured by dual RS cameras with reversed RS directions for this highly\nchallenging task. Grounded on the symmetric and complementary nature of dual\nreversed distortion, we develop a novel end-to-end model, IFED, to generate\ndual optical flow sequence through iterative learning of the velocity field\nduring the RS time. Extensive experimental results demonstrate that IFED is\nsuperior to naive cascade schemes, as well as the state-of-the-art which\nutilizes adjacent RS images. Most importantly, although it is trained on a\nsynthetic dataset, IFED is shown to be effective at retrieving GS frame\nsequences from real-world RS distorted images of dynamic scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhongyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Imari Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-GIF: 3D-Controllable Object Generation via Implicit Factorized Representations. (arXiv:2203.06457v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06457","description":"<p>While NeRF-based 3D-aware image generation methods enable viewpoint control,\nlimitations still remain to be adopted to various 3D applications. Due to their\nview-dependent and light-entangled volume representation, the 3D geometry\npresents unrealistic quality and the color should be re-rendered for every\ndesired viewpoint. To broaden the 3D applicability from 3D-aware image\ngeneration to 3D-controllable object generation, we propose the factorized\nrepresentations which are view-independent and light-disentangled, and training\nschemes with randomly sampled light conditions. We demonstrate the superiority\nof our method by visualizing factorized representations, re-lighted images, and\nalbedo-textured meshes. In addition, we show that our approach improves the\nquality of the generated geometry via visualization and quantitative\ncomparison. To the best of our knowledge, this is the first work that extracts\nalbedo-textured meshes with unposed 2D images without any additional labels or\nassumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_C/0/1/0/all/0/1\">Chaeyeon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hojun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minjung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sanghun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factored Attention and Embedding for Unstructured-view Topic-related Ultrasound Report Generation. (arXiv:2203.06458v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06458","description":"<p>Echocardiography is widely used to clinical practice for diagnosis and\ntreatment, e.g., on the common congenital heart defects. The traditional manual\nmanipulation is error-prone due to the staff shortage, excess workload, and\nless experience, leading to the urgent requirement of an automated\ncomputer-aided reporting system to lighten the workload of ultrasonologists\nconsiderably and assist them in decision making. Despite some recent successful\nattempts in automatical medical report generation, they are trapped in the\nultrasound report generation, which involves unstructured-view images and\ntopic-related descriptions. To this end, we investigate the task of the\nunstructured-view topic-related ultrasound report generation, and propose a\nnovel factored attention and embedding model (termed FAE-Gen). The proposed\nFAE-Gen mainly consists of two modules, i.e., view-guided factored attention\nand topic-oriented factored embedding, which 1) capture the homogeneous and\nheterogeneous morphological characteristic across different views, and 2)\ngenerate the descriptions with different syntactic patterns and different\nemphatic contents for different topics. Experimental evaluations are conducted\non a to-be-released large-scale clinical cardiovascular ultrasound dataset\n(CardUltData). Both quantitative comparisons and qualitative analysis\ndemonstrate the effectiveness and the superiority of FAE-Gen over seven\ncommonly-used metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fuhai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1\">Chengpeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xuri Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengchuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Review on Computer Vision-Based Parking Lot Management Applied on Public Datasets. (arXiv:2203.06463v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06463","description":"<p>Computer vision-based parking lot management methods have been extensively\nresearched upon owing to their flexibility and cost-effectiveness. To evaluate\nsuch methods authors often employ publicly available parking lot image\ndatasets. In this study, we surveyed and compared robust publicly available\nimage datasets specifically crafted to test computer vision-based methods for\nparking lot management approaches and consequently present a systematic and\ncomprehensive review of existing works that employ such datasets. The\nliterature review identified relevant gaps that require further research, such\nas the requirement of dataset-independent approaches and methods suitable for\nautonomous detection of position of parking spaces. In addition, we have\nnoticed that several important factors such as the presence of the same cars\nacross consecutive images, have been neglected in most studies, thereby\nrendering unrealistic assessment protocols. Furthermore, the analysis of the\ndatasets also revealed that certain features that should be present when\ndeveloping new benchmarks, such as the availability of video sequences and\nimages taken in more diverse conditions, including nighttime and snow, have not\nbeen incorporated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_P/0/1/0/all/0/1\">Paulo Ricardo Lisboa de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_J/0/1/0/all/0/1\">Jeovane Hon&#xf3;rio Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parpinelli_R/0/1/0/all/0/1\">Rafael Stubs Parpinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barddal_J/0/1/0/all/0/1\">Jean Paul Barddal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Lifelong Person Re-identification via Contrastive Rehearsal. (arXiv:2203.06468v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06468","description":"<p>Existing unsupervised person re-identification (ReID) methods focus on\nadapting a model trained on a source domain to a fixed target domain. However,\nan adapted ReID model usually only works well on a certain target domain, but\ncan hardly memorize the source domain knowledge and generalize to upcoming\nunseen data. In this paper, we propose unsupervised lifelong person ReID, which\nfocuses on continuously conducting unsupervised domain adaptation on new\ndomains without forgetting the knowledge learnt from old domains. To tackle\nunsupervised lifelong ReID, we conduct a contrastive rehearsal on a small\nnumber of stored old samples while sequentially adapting to new domains. We\nfurther set an image-to-image similarity constraint between old and new models\nto regularize the model updates in a way that suits old knowledge. We\nsequentially train our model on several large-scale datasets in an unsupervised\nmanner and test it on all seen domains as well as several unseen domains to\nvalidate the generalizability of our method. Our proposed unsupervised lifelong\nmethod achieves strong generalizability, which significantly outperforms\nprevious lifelong methods on both seen and unseen domains. Code will be made\navailable at https://github.com/chenhao2345/UCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagadec_B/0/1/0/all/0/1\">Benoit Lagadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?. (arXiv:2203.06487v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06487","description":"<p>Being able to explain the prediction to clinical end-users is a necessity to\nleverage the power of artificial intelligence (AI) models for clinical decision\nsupport. For medical images, a feature attribution map, or heatmap, is the most\ncommon form of explanation that highlights important features for AI models'\nprediction. However, it is unknown how well heatmaps perform on explaining\ndecisions on multi-modal medical images, where each image modality or channel\nvisualizes distinct clinical information of the same underlying biomedical\nphenomenon. Understanding such modality-dependent features is essential for\nclinical users' interpretation of AI decisions. To tackle this clinically\nimportant but technically ignored problem, we propose the modality-specific\nfeature importance (MSFI) metric. It encodes clinical image and explanation\ninterpretation patterns of modality prioritization and modality-specific\nfeature localization. We conduct a clinical requirement-grounded, systematic\nevaluation using computational methods and a clinician user study. Results show\nthat the examined 16 heatmap algorithms failed to fulfill clinical requirements\nto correctly indicate AI model decision process or decision quality. The\nevaluation and MSFI metric can guide the design and selection of XAI algorithms\nto meet clinical requirements on multi-modal explanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weina Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEN: Twin Embedding Networks for the Jigsaw Puzzle Problem with Eroded Boundaries. (arXiv:2203.06488v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06488","description":"<p>The jigsaw puzzle problem (JPP) is a well-known research problem, which has\nbeen studied for many years. Solving this problem typically involves a\ntwo-stage scheme, consisting of the computation of a pairwise piece\ncompatibility measure (CM), coupled with a subsequent puzzle reconstruction\nalgorithm. Many effective CMs, which apply a simple distance measure, based\nmerely on the information along the piece edges, have been proposed. However,\nthe practicality of these classical methods is rather doubtful for problem\ninstances harder than pure synthetic images. Specifically, these methods tend\nto break down in more realistic scenarios involving, e.g., monochromatic\npuzzles, eroded boundaries due to piece degradation over long time periods,\nmissing pieces, etc. To overcome this significant deficiency, a few deep\nconvolutional neural network (CNN)-based CMs have been recently introduced.\nDespite their promising accuracy, these models are very computationally\nintensive. Twin Embedding Networks (TEN), to represent a piece with respect to\nits boundary in a latent embedding space. Combining this latent representation\nwith a simple distance measure, we then demonstrate a superior performance, in\nterms of accuracy, of our newly proposed pairwise CM, compared to that of\nvarious classical methods, for the problem domain of eroded tile boundaries, a\ntestbed for a number of real-world JPP variants. Furthermore, we also\ndemonstrate that TEN is faster by a few orders of magnitude, on average, than\nthe recent NN models, i.e., it is as fast as the classical methods. In this\nregard, the paper makes a significant first attempt at bridging the gap between\nthe relatively low accuracy (of classical methods) and the intensive\ncomputational complexity (of NN models), for practical, real-world puzzle-like\nproblems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rika_D/0/1/0/all/0/1\">Daniel Rika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sholomon_D/0/1/0/all/0/1\">Dror Sholomon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1\">Eli David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netanyahu_N/0/1/0/all/0/1\">Nathan S. Netanyahu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Information Bottleneck Guided Joint Source-Channel Coding. (arXiv:2203.06492v1 [cs.IT])","link":"http://arxiv.org/abs/2203.06492","description":"<p>Joint source channel coding (JSCC) has attracted increasing attentions due to\nits robustness and high efficiency. However, the existing research on JSCC\nmainly focuses on minimizing the distortion between the transmitted and\nreceived information, while limiting the required data rate. Therefore, even\nthough the transmitted information is well recovered, the transmitted bits may\nbe far more than the minimal threshold according to the rate-distortion (RD)\ntheory. In this paper, we propose an adaptive Information Bottleneck (IB)\nguided JSCC (AIB-JSCC), which aims at achieving the theoretically maximal\ncompression ratio for a given reconstruction quality. In particular, we first\nderive a mathematically tractable form of loss function for AIB-JSCC. To keep a\nbetter tradeoff between compression and reconstruction quality, we further\npropose an adaptive algorithm that adjusts hyperparameter beta of the proposed\nloss function dynamically according to the distortion during training.\nExperiment results show that AIB-JSCC can significantly reduce the required\namount of the transmitted data and improve the reconstruction quality and\ndownstream artificial-intelligent task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lunan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Caili Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Mixed Quantization Network for Computationally Efficient Mobile Inverse Tone Mapping. (arXiv:2203.06504v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06504","description":"<p>Recovering a high dynamic range (HDR) image from a single low dynamic range\n(LDR) image, namely inverse tone mapping (ITM), is challenging due to the lack\nof information in over- and under-exposed regions. Current methods focus\nexclusively on training high-performing but computationally inefficient ITM\nmodels, which in turn hinder deployment of the ITM models in\nresource-constrained environments with limited computing power such as edge and\nmobile device applications.\n</p>\n<p>To this end, we propose combining efficient operations of deep neural\nnetworks with a novel mixed quantization scheme to construct a well-performing\nbut computationally efficient mixed quantization network (MQN) which can\nperform single image ITM on mobile platforms. In the ablation studies, we\nexplore the effect of using different attention mechanisms, quantization\nschemes, and loss functions on the performance of MQN in ITM tasks. In the\ncomparative analyses, ITM models trained using MQN perform on par with the\nstate-of-the-art methods on benchmark datasets. MQN models provide up to 10\ntimes improvement on latency and 25 times improvement on memory consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borrego_Carazo_J/0/1/0/all/0/1\">Juan Borrego-Carazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozay_M/0/1/0/all/0/1\">Mete Ozay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laboyrie_F/0/1/0/all/0/1\">Frederik Laboyrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisbey_P/0/1/0/all/0/1\">Paul Wisbey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsity and Heterogeneous Dropout for Continual Learning in the Null Space of Neural Activations. (arXiv:2203.06514v1 [cs.LG])","link":"http://arxiv.org/abs/2203.06514","description":"<p>Continual/lifelong learning from a non-stationary input data stream is a\ncornerstone of intelligence. Despite their phenomenal performance in a wide\nvariety of applications, deep neural networks are prone to forgetting their\npreviously learned information upon learning new ones. This phenomenon is\ncalled \"catastrophic forgetting\" and is deeply rooted in the\nstability-plasticity dilemma. Overcoming catastrophic forgetting in deep neural\nnetworks has become an active field of research in recent years. In particular,\ngradient projection-based methods have recently shown exceptional performance\nat overcoming catastrophic forgetting. This paper proposes two\nbiologically-inspired mechanisms based on sparsity and heterogeneous dropout\nthat significantly increase a continual learner's performance over a long\nsequence of tasks. Our proposed approach builds on the Gradient Projection\nMemory (GPM) framework. We leverage K-winner activations in each layer of a\nneural network to enforce layer-wise sparse activations for each task, together\nwith a between-task heterogeneous dropout that encourages the network to use\nnon-overlapping activation patterns between different tasks. In addition, we\nintroduce Continual Swiss Roll as a lightweight and interpretable -- yet\nchallenging -- synthetic benchmark for continual learning. Lastly, we provide\nan in-depth analysis of our proposed method and demonstrate a significant\nperformance boost on various benchmark continual learning problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1\">Ali Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nooralinejad_P/0/1/0/all/0/1\">Parsa Nooralinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1\">Soheil Kolouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning. (arXiv:2203.06541v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06541","description":"<p>Heatmap regression methods have dominated face alignment area in recent years\nwhile they ignore the inherent relation between different landmarks. In this\npaper, we propose a Sparse Local Patch Transformer (SLPT) for learning the\ninherent relation. The SLPT generates the representation of each single\nlandmark from a local patch and aggregates them by an adaptive inherent\nrelation based on the attention mechanism. The subpixel coordinate of each\nlandmark is predicted independently based on the aggregated feature. Moreover,\na coarse-to-fine framework is further introduced to incorporate with the SLPT,\nwhich enables the initial landmarks to gradually converge to the target facial\nlandmarks using fine-grained features from dynamically resized local patches.\nExtensive experiments carried out on three popular benchmarks, including WFLW,\n300W and COFW, demonstrate that the proposed method works at the\nstate-of-the-art level with much less computational complexity by learning the\ninherent relation between facial landmarks. The code is available at the\nproject website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jiahao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+qu_W/0/1/0/all/0/1\">Weiwei qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenjian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Change Detection from Synthetic Aperture Radar Images via Dual Path Denoising Network. (arXiv:2203.06543v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06543","description":"<p>Benefited from the rapid and sustainable development of synthetic aperture\nradar (SAR) sensors, change detection from SAR images has received increasing\nattentions over the past few years. Existing unsupervised deep learning-based\nmethods have made great efforts to exploit robust feature representations, but\nthey consume much time to optimize parameters. Besides, these methods use\nclustering to obtain pseudo-labels for training, and the pseudo-labeled samples\noften involve errors, which can be considered as \"label noise\". To address\nthese issues, we propose a Dual Path Denoising Network (DPDNet) for SAR image\nchange detection. In particular, we introduce the random label propagation to\nclean the label noise involved in preclassification. We also propose the\ndistinctive patch convolution for feature representation learning to reduce the\ntime consumption. Specifically, the attention mechanism is used to select\ndistinctive pixels in the feature maps, and patches around these pixels are\nselected as convolution kernels. Consequently, the DPDNet does not require a\ngreat number of training samples for parameter optimization, and its\ncomputational efficiency is greatly enhanced. Extensive experiments have been\nconducted on five SAR datasets to verify the proposed DPDNet. The experimental\nresults demonstrate that our method outperforms several state-of-the-art\nmethods in change detection results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Heng-Chao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEKD:Cross Ensemble Knowledge Distillation for Augmented Fine-grained Data. (arXiv:2203.06551v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06551","description":"<p>Data augmentation has been proved effective in training deep models. Existing\ndata augmentation methods tackle the fine-grained problem by blending image\npairs and fusing corresponding labels according to the statistics of mixed\npixels, which produces additional noise harmful to the performance of networks.\nMotivated by this, we present a simple yet effective cross ensemble knowledge\ndistillation (CEKD) model for fine-grained feature learning. We innovatively\npropose a cross distillation module to provide additional supervision to\nalleviate the noise problem, and propose a collaborative ensemble module to\novercome the target conflict problem. The proposed model can be trained in an\nend-to-end manner, and only requires image-level label supervision. Extensive\nexperiments on widely used fine-grained benchmarks demonstrate the\neffectiveness of our proposed model. Specifically, with the backbone of\nResNet-101, CEKD obtains the accuracy of 89.59%, 95.96% and 94.56% in three\ndatasets respectively, outperforming state-of-the-art API-Net by 0.99%, 1.06%\nand 1.16%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoli Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yongliang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaofeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1\">Feiwei Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Automotive mmWave Radar Detection Points Based Instance Segmentation. (arXiv:2203.06553v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06553","description":"<p>The automotive mmWave radar plays a key role in advanced driver assistance\nsystems (ADAS) and autonomous driving. Deep learning-based instance\nsegmentation enables real-time object identification from the radar detection\npoints. In the conventional training process, accurate annotation is the key.\nHowever, high-quality annotations of radar detection points are challenging to\nachieve due to their ambiguity and sparsity. To address this issue, we propose\na contrastive learning approach for implementing radar detection points-based\ninstance segmentation. We define the positive and negative samples according to\nthe ground-truth label, apply the contrastive loss to train the model first,\nand then perform training for the following downstream task. In addition, these\ntwo steps can be merged into one, and pseudo labels can be generated for the\nunlabeled data to improve the performance further. Thus, there are four\ndifferent training settings for our method. Experiments show that when the\nground-truth information is only available for 5% of the training data, our\nmethod still achieves a comparable performance to the approach trained in a\nsupervised manner with 100% ground-truth information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weiyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation. (arXiv:2203.06558v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06558","description":"<p>Training a generalizable 3D part segmentation network is quite challenging\nbut of great importance in real-world applications. To tackle this problem,\nsome works design task-specific solutions by translating human understanding of\nthe task to machine's learning process, which faces the risk of missing the\noptimal strategy since machines do not necessarily understand in the exact\nhuman way. Others try to use conventional task-agnostic approaches designed for\ndomain generalization problems with no task prior knowledge considered. To\nsolve the above issues, we propose AutoGPart, a generic method enabling\ntraining generalizable 3D part segmentation networks with the task prior\nconsidered. AutoGPart builds a supervision space with geometric prior knowledge\nencoded, and lets the machine to search for the optimal supervisions from the\nspace for a specific segmentation task automatically. Extensive experiments on\nthree generalizable 3D part segmentation tasks are conducted to demonstrate the\neffectiveness and versatility of AutoGPart. We demonstrate that the performance\nof segmentation networks using simple backbones can be significantly improved\nwhen trained with supervisions searched by our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xueyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaomeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anyi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Efficient Black-box Adversarial Attacks Guided by a Transfer-based Prior. (arXiv:2203.06560v1 [cs.LG])","link":"http://arxiv.org/abs/2203.06560","description":"<p>Adversarial attacks have been extensively studied in recent years since they\ncan identify the vulnerability of deep learning models before deployed. In this\npaper, we consider the black-box adversarial setting, where the adversary needs\nto craft adversarial examples without access to the gradients of a target\nmodel. Previous methods attempted to approximate the true gradient either by\nusing the transfer gradient of a surrogate white-box model or based on the\nfeedback of model queries. However, the existing methods inevitably suffer from\nlow attack success rates or poor query efficiency since it is difficult to\nestimate the gradient in a high-dimensional input space with limited\ninformation. To address these problems and improve black-box attacks, we\npropose two prior-guided random gradient-free (PRGF) algorithms based on biased\nsampling and gradient averaging, respectively. Our methods can take the\nadvantage of a transfer-based prior given by the gradient of a surrogate model\nand the query information simultaneously. Through theoretical analyses, the\ntransfer-based prior is appropriately integrated with model queries by an\noptimal coefficient in each method. Extensive experiments demonstrate that, in\ncomparison with the alternative state-of-the-arts, both of our methods require\nmuch fewer queries to attack black-box models with higher success rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shuyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Worst Case Matters for Few-Shot Recognition. (arXiv:2203.06574v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06574","description":"<p>Few-shot recognition learns a recognition model with very few (e.g., 1 or 5)\nimages per category, and current few-shot learning methods focus on improving\nthe average accuracy over many episodes. We argue that in real-world\napplications we may often only try one episode instead of many, and hence\nmaximizing the worst-case accuracy is more important than maximizing the\naverage accuracy. We empirically show that a high average accuracy not\nnecessarily means a high worst-case accuracy. Since this objective is not\naccessible, we propose to reduce the standard deviation and increase the\naverage accuracy simultaneously. In turn, we devise two strategies from the\nbias-variance tradeoff perspective to implicitly reach this goal: a simple yet\neffective stability regularization (SR) loss together with model ensemble to\nreduce variance during fine-tuning, and an adaptability calibration mechanism\nto reduce the bias. Extensive experiments on benchmark datasets demonstrate the\neffectiveness of the proposed strategies, which outperforms current\nstate-of-the-art methods with a significant margin in terms of not only\naverage, but also worst-case accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_M/0/1/0/all/0/1\">Minghao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yun-Hao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVFNet: Real-time 3D Object Detection by Learning Cross View Features. (arXiv:2203.06585v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06585","description":"<p>In recent years 3D object detection from LiDAR point clouds has made great\nprogress thanks to the development of deep learning technologies. Although\nvoxel or point based methods are popular in 3D object detection, they usually\ninvolve time-consuming operations such as 3D convolutions on voxels or ball\nquery among points, making the resulting network inappropriate for time\ncritical applications. On the other hand, 2D view-based methods feature high\ncomputing efficiency while usually obtaining inferior performance than the\nvoxel or point based methods. In this work, we present a real-time view-based\nsingle stage 3D object detector, namely CVFNet to fulfill this task. To\nstrengthen the cross-view feature learning under the condition of demanding\nefficiency, our framework extracts the features of different views and fuses\nthem in an efficient progressive way. We first propose a novel Point-Range\nfeature fusion module that deeply integrates point and range view features in\nmultiple stages. Then, a special Slice Pillar is designed to well maintain the\n3D geometry when transforming the obtained deep point-view features into bird's\neye view. To better balance the ratio of samples, a sparse pillar detection\nhead is presented to focus the detection on the nonempty grids. We conduct\nexperiments on the popular KITTI and NuScenes benchmark, and state-of-the-art\nperformances are achieved in terms of both accuracy and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tingming Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AugShuffleNet: Improve ShuffleNetV2 via More Information Communication. (arXiv:2203.06589v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06589","description":"<p>Based on ShuffleNetV2, we build a more powerful and efficient model family,\ntermed as AugShuffleNets, by introducing higher frequency of cross-layer\ninformation communication for better model performance. Evaluated on the\nCIFAR-10 and CIFAR-100 datasets, AugShuffleNet consistently outperforms\nShuffleNetV2 in terms of accuracy, with less computational cost, fewer\nparameter count.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1\">Longqing Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Autoencoders for Point Cloud Self-supervised Learning. (arXiv:2203.06604v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06604","description":"<p>As a promising scheme of self-supervised learning, masked autoencoding has\nsignificantly advanced natural language processing and computer vision.\nInspired by this, we propose a neat scheme of masked autoencoders for point\ncloud self-supervised learning, addressing the challenges posed by point\ncloud's properties, including leakage of location information and uneven\ninformation density. Concretely, we divide the input point cloud into irregular\npoint patches and randomly mask them at a high ratio. Then, a standard\nTransformer based autoencoder, with an asymmetric design and a shifting mask\ntokens operation, learns high-level latent features from unmasked point\npatches, aiming to reconstruct the masked point patches. Extensive experiments\nshow that our approach is efficient during pre-training and generalizes well on\nvarious downstream tasks. Specifically, our pre-trained models achieve 84.52\\%\naccuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all\nthe other self-supervised learning methods. We show with our scheme, a simple\narchitecture entirely based on standard Transformers can surpass dedicated\nTransformer models from supervised learning. Our approach also advances\nstate-of-the-art accuracies by 1.5%-2.3% in the few-shot object classification.\nFurthermore, our work inspires the feasibility of applying unified\narchitectures from languages and images to the point cloud.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yatian Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_F/0/1/0/all/0/1\">Francis E.H. Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2203.06605v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06605","description":"<p>Talking head video generation aims to produce a synthetic human face video\nthat contains the identity and pose information respectively from a given\nsource image and a driving video.Existing works for this task heavily rely on\n2D representations (e.g. appearance and motion) learned from the input images.\nHowever, dense 3D facial geometry (e.g. pixel-wise depth) is extremely\nimportant for this task as it is particularly beneficial for us to essentially\ngenerate accurate 3D face structures and distinguish noisy information from the\npossibly cluttered background. Nevertheless, dense 3D geometry annotations are\nprohibitively costly for videos and are typically not available for this video\ngeneration task. In this paper, we first introduce a self-supervised geometry\nlearning method to automatically recover the dense 3D geometry (i.e.depth) from\nthe face videos without the requirement of any expensive 3D annotation data.\nBased on the learned dense depth maps, we further propose to leverage them to\nestimate sparse facial keypoints that capture the critical movement of the\nhuman head. In a more dense way, the depth is also utilized to learn 3D-aware\ncross-modal (i.e. appearance and depth) attention to guide the generation of\nmotion fields for warping source image representations. All these contributions\ncompose a novel depth-aware generative adversarial network (DaGAN) for talking\nhead generation. Extensive experiments conducted demonstrate that our proposed\nmethod can generate highly realistic faces, and achieve significant results on\nthe unseen human faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fa-Ting Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-LSTM: a robust classifier for video detection on UCF101. (arXiv:2203.06610v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06610","description":"<p>Video detection and human action recognition may be computationally\nexpensive, and need a long time to train models. In this paper, we were\nintended to reduce the training time and the GPU memory usage of video\ndetection, and achieved a competitive detection accuracy. Other research works\nsuch as Two-stream, C3D, TSN have shown excellent performance on UCF101. Here,\nwe used a LSTM structure simply for video detection. We used a simple structure\nto perform a competitive top-1 accuracy on the entire validation dataset of\nUCF101. The LSTM structure is named Context-LSTM, since it may process the deep\ntemporal features. The Context-LSTM may simulate the human recognition system.\nWe cascaded the LSTM blocks in PyTorch and connected the cell state flow and\nhidden output flow. At the connection of the blocks, we used ReLU, Batch\nNormalization, and MaxPooling functions. The Context-LSTM could reduce the\ntraining time and the GPU memory usage, while keeping a state-of-the-art top-1\naccuracy on UCF101 entire validation dataset, show a robust performance on\nvideo action detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rujing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Single Correspondence Is Enough: Robust Global Registration to Avoid Degeneracy in Urban Environments. (arXiv:2203.06612v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06612","description":"<p>Global registration using 3D point clouds is a crucial technology for mobile\nplatforms to achieve localization or manage loop-closing situations. In recent\nyears, numerous researchers have proposed global registration methods to\naddress a large number of outlier correspondences. Unfortunately, the\ndegeneracy problem, which represents the phenomenon in which the number of\nestimated inliers becomes lower than three, is still potentially inevitable. To\ntackle the problem, a degeneracy-robust decoupling-based global registration\nmethod is proposed, called Quatro. In particular, our method employs\nquasi-SO(3) estimation by leveraging the Atlanta world assumption in urban\nenvironments to avoid degeneracy in rotation estimation. Thus, the minimum\ndegree of freedom (DoF) of our method is reduced from three to one. As verified\nin indoor and outdoor 3D LiDAR datasets, our proposed method yields robust\nglobal registration performance compared with other global registration\nmethods, even for distant point cloud pairs. Furthermore, the experimental\nresults confirm the applicability of our method as a coarse alignment. Our code\nis available: https://github.com/url-kaist/quatro.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeon_S/0/1/0/all/0/1\">Suyong Yeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1\">Soohyun Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jaeseong Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1\">Euigon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAS-AT: Adversarial Training with Learnable Attack Strategy. (arXiv:2203.06616v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06616","description":"<p>Adversarial training (AT) is always formulated as a minimax problem, of which\nthe performance depends on the inner optimization that involves the generation\nof adversarial examples (AEs). Most previous methods adopt Projected Gradient\nDecent (PGD) with manually specifying attack parameters for AE generation. A\ncombination of the attack parameters can be referred to as an attack strategy.\nSeveral works have revealed that using a fixed attack strategy to generate AEs\nduring the whole training phase limits the model robustness and propose to\nexploit different attack strategies at different training stages to improve\nrobustness. But those multi-stage hand-crafted attack strategies need much\ndomain expertise, and the robustness improvement is limited. In this paper, we\npropose a novel framework for adversarial training by introducing the concept\nof \"learnable attack strategy\", dubbed LAS-AT, which learns to automatically\nproduce attack strategies to improve the model robustness. Our framework is\ncomposed of a target network that uses AEs for training to improve robustness\nand a strategy network that produces attack strategies to control the AE\ngeneration. Experimental evaluations on three benchmark databases demonstrate\nthe superiority of the proposed method. The code is released at\nhttps://github.com/jiaxiaojunQAQ/LAS-AT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaojun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Ke Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Bracket High Dynamic Range Imaging with Event Cameras. (arXiv:2203.06622v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06622","description":"<p>Modern high dynamic range (HDR) imaging pipelines align and fuse multiple low\ndynamic range (LDR) images captured at different exposure times. While these\nmethods work well in static scenes, dynamic scenes remain a challenge since the\nLDR images still suffer from saturation and noise. In such scenarios, event\ncameras would be a valid complement, thanks to their higher temporal resolution\nand dynamic range. In this paper, we propose the first multi-bracket HDR\npipeline combining a standard camera with an event camera. Our results show\nbetter overall robustness when using events, with improvements in PSNR by up to\n5dB on synthetic data and up to 0.7dB on real-world data. We also introduce a\nnew dataset containing bracketed LDR images with aligned events and HDR ground\ntruth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messikommer_N/0/1/0/all/0/1\">Nico Messikommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1\">Stamatios Georgoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Stepan Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbach_J/0/1/0/all/0/1\">Julius Erbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bochicchio_A/0/1/0/all/0/1\">Alfredo Bochicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Deep Semi-supervised Learning: An Empirical Distribution Alignment Framework and Its Generalization Bound. (arXiv:2203.06639v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06639","description":"<p>In this work, we revisit the semi-supervised learning (SSL) problem from a\nnew perspective of explicitly reducing empirical distribution mismatch between\nlabeled and unlabeled samples. Benefited from this new perspective, we first\npropose a new deep semi-supervised learning framework called Semi-supervised\nLearning by Empirical Distribution Alignment (SLEDA), in which existing\ntechnologies from the domain adaptation community can be readily used to\naddress the semi-supervised learning problem through reducing the empirical\ndistribution distance between labeled and unlabeled data. Based on this\nframework, we also develop a new theoretical generalization bound for the\nresearch community to better understand the semi-supervised learning problem,\nin which we show the generalization error of semi-supervised learning can be\neffectively bounded by minimizing the training error on labeled data and the\nempirical distribution distance between labeled and unlabeled data. Building\nupon our new framework and the theoretical bound, we develop a simple and\neffective deep semi-supervised learning method called Augmented Distribution\nAlignment Network (ADA-Net) by simultaneously adopting the well-established\nadversarial training strategy from the domain adaptation community and a simple\nsample interpolation strategy for data augmentation. Additionally, we\nincorporate both strategies in our ADA-Net into two exiting SSL methods to\nfurther improve their generalization capability, which indicates that our new\nframework provides a complementary solution for solving the SSL problem. Our\ncomprehensive experimental results on two benchmark datasets SVHN and CIFAR-10\nfor the semi-supervised image recognition task and another two benchmark\ndatasets ModelNet40 and ShapeNet55 for the semi-supervised point cloud\nrecognition task demonstrate the effectiveness of our proposed framework for\nSSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4. (arXiv:2203.06649v1 [q-bio.NC])","link":"http://arxiv.org/abs/2203.06649","description":"<p>Modern high-scoring models of vision in the brain score competition do not\nstem from Vision Transformers. However, in this short paper, we provide\nevidence against the unexpected trend of Vision Transformers (ViT) being not\nperceptually aligned with human visual representations by showing how a\ndual-stream Transformer, a CrossViT$~\\textit{a la}$ Chen et al. (2021), under a\njoint rotationally-invariant and adversarial optimization procedure yields 2nd\nplace in the aggregate Brain-Score 2022 competition averaged across all visual\ncategories, and currently (March 1st, 2022) holds the 1st place for the highest\nexplainable variance of area V4. In addition, our current Transformer-based\nmodel also achieves greater explainable variance for areas V4, IT and Behaviour\nthan a biologically-inspired CNN (ResNet50) that integrates a frontal V1-like\ncomputation module(Dapello et al.,2020). Our team was also the only entry in\nthe top-5 that shows a positive rank correlation between explained variance per\narea and depth in the visual hierarchy.\n</p>\n<p>Against our initial expectations, these results provide tentative support for\nan $\\textit{\"All roads lead to Rome\"}$ argument enforced via a joint\noptimization rule even for non biologically-motivated models of vision such as\nVision Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Berrios_W/0/1/0/all/0/1\">William Berrios</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deza_A/0/1/0/all/0/1\">Arturo Deza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global2Local: A Joint-Hierarchical Attention for Video Captioning. (arXiv:2203.06663v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06663","description":"<p>Recently, automatic video captioning has attracted increasing attention,\nwhere the core challenge lies in capturing the key semantic items, like objects\nand actions as well as their spatial-temporal correlations from the redundant\nframes and semantic content. To this end, existing works select either the key\nvideo clips in a global level~(across multi frames), or key regions within each\nframe, which, however, neglect the hierarchical order, i.e., key frames first\nand key regions latter. In this paper, we propose a novel joint-hierarchical\nattention model for video captioning, which embeds the key clips, the key\nframes and the key regions jointly into the captioning model in a hierarchical\nmanner. Such a joint-hierarchical attention model first conducts a global\nselection to identify key frames, followed by a Gumbel sampling operation to\nidentify further key regions based on the key frames, achieving an accurate\nglobal-to-local feature representation to guide the captioning. Extensive\nquantitative evaluations on two public benchmark datasets MSVD and MSR-VTT\ndemonstrates the superiority of the proposed method over the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1\">Chengpeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fuhai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderiving from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps in semantics in features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor fail to perform well in the TAGV task. In this work, we\npropose a visual-prompt text span localizing (VPTSL) method, which enhances the\ntext span localization in the pre-trained language model (PLM) with the visual\nhighlight features. Specifically, the context query attention is utilized to\nperform cross-modal modeling between the textual and visual features. Then, the\nhighlight features are obtained through the highlight module with a linear\nlayer to provide the visual prompt. To alleviate the differences in semantics\nand correlations between textual and visual features, we design the text span\npredictor by encoding the question, the subtitles, and the visual prompt in the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the answering frame timeline. Extensive experiments on the medical\ninstructional dataset, namely MedVidQA, show the proposed VPTSL outperforms\nother state-of-the-art methods, which demonstrates the effectiveness of visual\nprompt and the text span predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PNM: Pixel Null Model for General Image Segmentation. (arXiv:2203.06677v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06677","description":"<p>A major challenge in image segmentation is classifying object boundaries.\nRecent efforts propose to refine the segmentation result with boundary masks.\nHowever, models are still prone to misclassifying boundary pixels even when\nthey correctly capture the object contours. In such cases, even a perfect\nboundary map is unhelpful for segmentation refinement. In this paper, we argue\nthat assigning proper prior weights to error-prone pixels such as object\nboundaries can significantly improve the segmentation quality. Specifically, we\npresent the \\textit{pixel null model} (PNM), a prior model that weights each\npixel according to its probability of being correctly classified by a random\nsegmenter. Empirical analysis shows that PNM captures the misclassification\ndistribution of different state-of-the-art (SOTA) segmenters. Extensive\nexperiments on semantic, instance, and panoptic segmentation tasks over three\ndatasets (Cityscapes, ADE20K, MS COCO) confirm that PNM consistently improves\nthe segmentation quality of most SOTA methods (including the vision\ntransformers) and outperforms boundary-based methods by a large margin. We also\nobserve that the widely-used mean IoU (mIoU) metric is insensitive to\nboundaries of different sharpness. As a byproduct, we propose a new metric,\n\\textit{PNM IoU}, which perceives the boundary sharpness and better reflects\nthe model segmentation performance in error-prone regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors. (arXiv:2203.06691v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06691","description":"<p>The main question this work aims at answering is: can morphing attack\ndetection (MAD) solutions be successfully developed based on synthetic data?.\nTowards that, this work introduces the first synthetic-based MAD development\ndataset, namely the Synthetic Morphing Attack Detection Development dataset\n(SMDD). This dataset is utilized successfully to train three MAD backbones\nwhere it proved to lead to high MAD performance, even on completely unknown\nattack types. Additionally, an essential aspect of this work is the detailed\nlegal analyses of the challenges of using and sharing real biometric data,\nrendering our proposed SMDD dataset extremely essential. The SMDD dataset,\nconsisting of 30,000 attack and 50,000 bona fide samples, is made publicly\navailable for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">C&#xe9;sar Augusto Fontanillo L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_N/0/1/0/all/0/1\">No&#xe9;mie Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Vu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Protocol Matters: Towards Accurate Scene Text Recognition via Training Protocol Searching. (arXiv:2203.06696v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06696","description":"<p>The development of scene text recognition (STR) in the era of deep learning\nhas been mainly focused on novel architectures of STR models. However, training\nprotocol (i.e., settings of the hyper-parameters involved in the training of\nSTR models), which plays an equally important role in successfully training a\ngood STR model, is under-explored for scene text recognition. In this work, we\nattempt to improve the accuracy of existing STR models by searching for optimal\ntraining protocol. Specifically, we develop a training protocol search\nalgorithm, based on a newly designed search space and an efficient search\nalgorithm using evolutionary optimization and proxy tasks. Experimental results\nshow that our searched training protocol can improve the recognition accuracy\nof mainstream STR models by 2.7%~3.9%. In particular, with the searched\ntraining protocol, TRBA-Net achieves 2.1% higher accuracy than the\nstate-of-the-art STR model (i.e., EFIFSTR), while the inference speed is 2.3x\nand 3.7x faster on CPU and GPU respectively. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed method and the\ngeneralization ability of the training protocol found by our search method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Long-Range Attention Network for Image Super-resolution. (arXiv:2203.06697v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06697","description":"<p>Recently, transformer-based methods have demonstrated impressive results in\nvarious vision tasks, including image super-resolution (SR), by exploiting the\nself-attention (SA) for feature extraction. However, the computation of SA in\nmost existing transformer based models is very expensive, while some employed\noperations may be redundant for the SR task. This limits the range of SA\ncomputation and consequently the SR performance. In this work, we propose an\nefficient long-range attention network (ELAN) for image SR. Specifically, we\nfirst employ shift convolution (shift-conv) to effectively extract the image\nlocal structural information while maintaining the same level of complexity as\n1x1 convolution, then propose a group-wise multi-scale self-attention (GMSA)\nmodule, which calculates SA on non-overlapped groups of features using\ndifferent window sizes to exploit the long-range image dependency. A highly\nefficient long-range attention block (ELAB) is then built by simply cascading\ntwo shift-conv with a GMSA module, which is further accelerated by using a\nshared attention mechanism. Without bells and whistles, our ELAN follows a\nfairly simple design by sequentially cascading the ELABs. Extensive experiments\ndemonstrate that ELAN obtains even better results against the transformer-based\nSR models but with significantly less complexity. The source code can be found\nat https://github.com/xindongzhang/ELAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xindong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06717","description":"<p>In this paper we revisit large kernel design in modern convolutional neural\nnetworks (CNNs), which is often neglected in the past few years. Inspired by\nrecent advances of vision transformers (ViTs), we point out that using a few\nlarge kernels instead of a stack of small convolutions could be a more powerful\nparadigm. We therefore summarize 5 guidelines, e.g., applying re-parameterized\nlarge depth-wise convolutions, to design efficient high-performance\nlarge-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN\narchitecture whose kernel size is as large as 31x31. RepLKNet greatly bridges\nthe performance gap between CNNs and ViTs, e.g., achieving comparable or better\nresults than Swin Transformer on ImageNet and downstream tasks, while the\nlatency of RepLKNet is much lower. Moreover, RepLKNet also shows feasible\nscalability to big data and large models, obtaining 87.8% top-1 accuracy on\nImageNet and 56.0%} mIoU on ADE20K. At last, our study further suggests\nlarge-kernel CNNs share several nice properties with ViTs, e.g., much larger\neffective receptive fields than conventional CNNs, and higher shape bias rather\nthan texture bias. Code &amp; models at\nhttps://github.com/megvii-research/RepLKNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yizhuang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Food Recipe Recommendation Based on Ingredients Detection Using Deep Learning. (arXiv:2203.06721v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06721","description":"<p>Food is essential for human survival, and people always try to taste\ndifferent types of delicious recipes. Frequently, people choose food\ningredients without even knowing their names or pick up some food ingredients\nthat are not obvious to them from a grocery store. Knowing which ingredients\ncan be mixed to make a delicious food recipe is essential. Selecting the right\nrecipe by choosing a list of ingredients is very difficult for a beginner cook.\nHowever, it can be a problem even for experts. One such example is recognising\nobjects through image processing. Although this process is complex due to\ndifferent food ingredients, traditional approaches will lead to an inaccuracy\nrate. These problems can be solved by machine learning and deep learning\napproaches. In this paper, we implemented a model for food ingredients\nrecognition and designed an algorithm for recommending recipes based on\nrecognised ingredients. We made a custom dataset consisting of 9856 images\nbelonging to 32 different food ingredients classes. Convolution Neural Network\n(CNN) model was used to identify food ingredients, and for recipe\nrecommendations, we have used machine learning. We achieved an accuracy of 94\npercent, which is quite impressive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rokon_M/0/1/0/all/0/1\">Md. Shafaat Jamil Rokon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morol_M/0/1/0/all/0/1\">Md Kishor Morol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_I/0/1/0/all/0/1\">Ishra Binte Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saif_A/0/1/0/all/0/1\">A. M. Saif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Rafid Hussain Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature space reduction as data preprocessing for the anomaly detection. (arXiv:2203.06747v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06747","description":"<p>In this paper, we present two pipelines in order to reduce the feature space\nfor anomaly detection using the One Class SVM. As a first stage of both\npipelines, we compare the performance of three convolutional autoencoders. We\nuse the PCA method together with t-SNE as the first pipeline and the\nreconstruction errors based method as the second. Both methods have potential\nfor the anomaly detection, but the reconstruction error metrics prove to be\nmore robust for this task. We show that the convolutional autoencoder\narchitecture doesn't have a significant effect for this task and we prove the\npotential of our approach on the real world dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bilik_S/0/1/0/all/0/1\">Simon Bilik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horak_K/0/1/0/all/0/1\">Karel Horak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance. (arXiv:2203.06749v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06749","description":"<p>In May 2021, the site runnersworld.com published that participation in\nultra-distance races has increased by 1,676% in the last 23 years. Moreover,\nnearly 41% of those runners participate in more than one race per year. The\ndevelopment of wearable devices has undoubtedly contributed to motivating\nparticipants by providing performance measures in real-time. However, we\nbelieve there is room for improvement, particularly from the organizers point\nof view. This work aims to determine how the runners performance can be\nquantified and predicted by considering a non-invasive technique focusing on\nthe ultra-running scenario. In this sense, participants are captured when they\npass through a set of locations placed along the race track. Each footage is\nconsidered an input to an I3D ConvNet to extract the participant's running gait\nin our work. Furthermore, weather and illumination capture conditions or\nocclusions may affect these footages due to the race staff and other runners.\nTo address this challenging task, we have tracked and codified the\nparticipant's running gait at some RPs and removed the context intending to\nensure a runner-of-interest proper evaluation. The evaluation suggests that the\nfeatures extracted by an I3D ConvNet provide enough information to estimate the\nparticipant's performance along the different race tracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freire_Obregon_D/0/1/0/all/0/1\">David Freire-Obreg&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzo_Navarro_J/0/1/0/all/0/1\">Javier Lorenzo-Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castrillon_Santana_M/0/1/0/all/0/1\">Modesto Castrill&#xf3;n-Santana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TurbuGAN: An Adversarial Learning Approach to Spatially-Varying Multiframe Blind Deconvolution with Applications to Imaging Through Turbulence. (arXiv:2203.06764v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06764","description":"<p>We present a self-supervised and self-calibrating multi-shot approach to\nimaging through atmospheric turbulence, called TurbuGAN. Our approach requires\nno paired training data, adapts itself to the distribution of the turbulence,\nleverages domain-specific data priors, outperforms existing approaches, and can\ngeneralize from tens to tens of thousands of measurements. We achieve such\nfunctionality through an adversarial sensing framework adapted from CryoGAN,\nwhich uses a discriminator network to match the distributions of captured and\nsimulated measurements. Our framework builds on CryoGAN by (1) generalizing the\nforward measurement model to incorporate physically accurate and\ncomputationally efficient models for light propagation through anisoplanatic\nturbulence, (2) enabling adaptation to slightly misspecified forward models,\nand (3) leveraging domain-specific prior knowledge using pretrained generative\nnetworks, when available. We validate TurbuGAN in simulation using realistic\nmodels for atmospheric turbulence-induced distortion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Brandon Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mingyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_C/0/1/0/all/0/1\">Christopher A. Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity Equivariant Linear Transformation of Joint Orientation-Scale Space Representations. (arXiv:2203.06786v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06786","description":"<p>Convolution is conventionally defined as a linear operation on functions of\none or more variables which commutes with shifts. Group convolution generalizes\nthe concept to linear operations on functions of group elements representing\nmore general geometric transformations and which commute with those\ntransformations. Since similarity transformation is the most general geometric\ntransformation on images that preserves shape, the group convolution that is\nequivariant to similarity transformation is the most general shape preserving\nlinear operator. Because similarity transformations have four free parameters,\ngroup convolutions are defined on four-dimensional, joint orientation-scale\nspaces. Although prior work on equivariant linear operators has been limited to\ndiscrete groups, the similarity group is continuous. In this paper, we describe\nlinear operators on discrete representations that are equivariant to continuous\nsimilarity transformation. This is achieved by using a basis of functions that\nis it joint shiftable-twistable-scalable. These it pinwheel functions use\nFourier series in the orientation dimension and Laplace transform in the\nlog-scale dimension to form a basis of spatially localized functions that can\nbe continuously interpolated in position, orientation and scale. Although this\nresult is potentially significant with respect to visual computation generally,\nwe present an initial demonstration of its utility by using it to compute a\nshape equivariant distribution of closed contours traced by particles\nundergoing Brownian motion in velocity. The contours are constrained by sets of\npoints and line endings representing well known bistable illusory contour\ninducing patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1\">Lance R. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Euclidean Invariant Recognition of 2D Shapes Using Histograms of Magnitudes of Local Fourier-Mellin Descriptors. (arXiv:2203.06787v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06787","description":"<p>Because the magnitude of inner products with its basis functions are\ninvariant to rotation and scale change, the Fourier-Mellin transform has long\nbeen used as a component in Euclidean invariant 2D shape recognition systems.\nYet Fourier-Mellin transform magnitudes are only invariant to rotation and\nscale changes about a known center point, and full Euclidean invariant shape\nrecognition is not possible except when this center point can be consistently\nand accurately identified. In this paper, we describe a system where a\nFourier-Mellin transform is computed at every point in the image. The spatial\nsupport of the Fourier-Mellin basis functions is made local by multiplying them\nwith a polynomial envelope. Significantly, the magnitudes of convolutions with\nthese complex filters at isolated points are not (by themselves) used as\nfeatures for Euclidean invariant shape recognition because reliable\ndiscrimination would require filters with spatial support large enough to fully\nencompass the shapes. Instead, we rely on the fact that normalized histograms\nof magnitudes are fully Euclidean invariant. We demonstrate a system based on\nthe VLAD machine learning method that performs Euclidean invariant recognition\nof 2D shapes and requires an order of magnitude less training data than\ncomparable methods based on convolutional neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1\">Lance R. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Learning for Deformable Medical Image Registration by Jointly Optimizing Network Architectures and Objective Functions. (arXiv:2203.06810v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06810","description":"<p>Deformable image registration plays a critical role in various tasks of\nmedical image analysis. A successful registration algorithm, either derived\nfrom conventional energy optimization or deep networks requires tremendous\nefforts from computer experts to well design registration energy or to\ncarefully tune network architectures for the specific type of medical data. To\ntackle the aforementioned problems, this paper proposes an automated learning\nregistration algorithm (AutoReg) that cooperatively optimizes both\narchitectures and their corresponding training objectives, enabling\nnon-computer experts, e.g., medical/clinical users, to conveniently find\noff-the-shelf registration algorithms for diverse scenarios. Specifically, we\nestablish a triple-level framework to deduce registration network architectures\nand objectives with an auto-searching mechanism and cooperating optimization.\nWe conduct image registration experiments on multi-site volume datasets and\nvarious registration tasks. Extensive results demonstrate that our AutoReg may\nautomatically learn an optimal deep registration network for given volumes and\nachieve state-of-the-art performance, also significantly improving computation\nefficiency than the mainstream UNet architectures (from 0.558 to 0.270 seconds\nfor a 3D image pair on the same configuration).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhongxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADAS: A Direct Adaptation Strategy for Multi-Target Domain Adaptive Semantic Segmentation. (arXiv:2203.06811v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06811","description":"<p>In this paper, we present a direct adaptation strategy (ADAS), which aims to\ndirectly adapt a single model to multiple target domains in a semantic\nsegmentation task without pretrained domain-specific models. To do so, we\ndesign a multi-target domain transfer network (MTDT-Net) that aligns visual\nattributes across domains by transferring the domain distinctive features\nthrough a new target adaptive denormalization (TAD) module. Moreover, we\npropose a bi-directional adaptive region selection (BARS) that reduces the\nattribute ambiguity among the class labels by adaptively selecting the regions\nwith consistent feature statistics. We show that our single MTDT-Net can\nsynthesize visually pleasing domain transferred images with complex driving\ndatasets, and BARS effectively filters out the unnecessary region of training\nimages for each target domain. With the collaboration of MTDT-Net and BARS, our\nADAS achieves state-of-the-art performance for multi-target domain adaptation\n(MTDA). To the best of our knowledge, our method is the first MTDA method that\ndirectly adapts to multiple domains in semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Wonhyeok Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1\">Minwoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1\">Sunghoon Im</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Commands for Autonomous Vehicles via Layer Fusion with Region-specific Dynamic Layer Attention. (arXiv:2203.06822v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06822","description":"<p>Grounding a command to the visual environment is an essential ingredient for\ninteractions between autonomous vehicles and humans. In this work, we study the\nproblem of language grounding for autonomous vehicles, which aims to localize a\nregion in a visual scene according to a natural language command from a\npassenger. Prior work only employs the top layer representations of a\nvision-and-language pre-trained model to predict the region referred to by the\ncommand. However, such a method omits the useful features encoded in other\nlayers, and thus results in inadequate understanding of the input scene and\ncommand. To tackle this limitation, we present the first layer fusion approach\nfor this task. Since different visual regions may require distinct types of\nfeatures to disambiguate them from each other, we further propose the\nregion-specific dynamic (RSD) layer attention to adaptively fuse the multimodal\ninformation across layers for each region. Extensive experiments on the\nTalk2Car benchmark demonstrate that our approach helps predict more accurate\nregions and outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingxi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SKM-TEA: A Dataset for Accelerated MRI Reconstruction with Dense Image Labels for Quantitative Clinical Evaluation. (arXiv:2203.06823v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06823","description":"<p>Magnetic resonance imaging (MRI) is a cornerstone of modern medical imaging.\nHowever, long image acquisition times, the need for qualitative expert\nanalysis, and the lack of (and difficulty extracting) quantitative indicators\nthat are sensitive to tissue health have curtailed widespread clinical and\nresearch studies. While recent machine learning methods for MRI reconstruction\nand analysis have shown promise for reducing this burden, these techniques are\nprimarily validated with imperfect image quality metrics, which are discordant\nwith clinically-relevant measures that ultimately hamper clinical deployment\nand clinician trust. To mitigate this challenge, we present the Stanford Knee\nMRI with Multi-Task Evaluation (SKM-TEA) dataset, a collection of quantitative\nknee MRI (qMRI) scans that enables end-to-end, clinically-relevant evaluation\nof MRI reconstruction and analysis tools. This 1.6TB dataset consists of\nraw-data measurements of ~25,000 slices (155 patients) of anonymized patient\nMRI scans, the corresponding scanner-generated DICOM images, manual\nsegmentations of four tissues, and bounding box annotations for sixteen\nclinically relevant pathologies. We provide a framework for using qMRI\nparameter maps, along with image reconstructions and dense image labels, for\nmeasuring the quality of qMRI biomarker estimates extracted from MRI\nreconstruction, segmentation, and detection techniques. Finally, we use this\nframework to benchmark state-of-the-art baselines on this dataset. We hope our\nSKM-TEA dataset and code can enable a broad spectrum of research for modular\nimage reconstruction and image analysis in a clinically informed manner.\nDataset access, code, and benchmarks are available at\nhttps://github.com/StanfordMIMI/skm-tea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Desai_A/0/1/0/all/0/1\">Arjun D Desai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_A/0/1/0/all/0/1\">Andrew M Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubin_E/0/1/0/all/0/1\">Elka B Rubin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandino_C/0/1/0/all/0/1\">Christopher M Sandino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Black_M/0/1/0/all/0/1\">Marianne S Black</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazzoli_V/0/1/0/all/0/1\">Valentina Mazzoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stevens_K/0/1/0/all/0/1\">Kathryn J Stevens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boutin_R/0/1/0/all/0/1\">Robert Boutin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gold_G/0/1/0/all/0/1\">Garry E Gold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hargreaves_B/0/1/0/all/0/1\">Brian A Hargreaves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay S Chaudhari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness Evaluation in Deepfake Detection Models using Metamorphic Testing. (arXiv:2203.06825v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06825","description":"<p>Fairness of deepfake detectors in the presence of anomalies are not well\ninvestigated, especially if those anomalies are more prominent in either male\nor female subjects. The primary motivation for this work is to evaluate how\ndeepfake detection model behaves under such anomalies. However, due to the\nblack-box nature of deep learning (DL) and artificial intelligence (AI)\nsystems, it is hard to predict the performance of a model when the input data\nis modified. Crucially, if this defect is not addressed properly, it will\nadversely affect the fairness of the model and result in discrimination of\ncertain sub-population unintentionally. Therefore, the objective of this work\nis to adopt metamorphic testing to examine the reliability of the selected\ndeepfake detection model, and how the transformation of input variation places\ninfluence on the output. We have chosen MesoInception-4, a state-of-the-art\ndeepfake detection model, as the target model and makeup as the anomalies.\nMakeups are applied through utilizing the Dlib library to obtain the 68 facial\nlandmarks prior to filling in the RGB values. Metamorphic relations are derived\nbased on the notion that realistic perturbations of the input images, such as\nmakeup, involving eyeliners, eyeshadows, blushes, and lipsticks (which are\ncommon cosmetic appearance) applied to male and female images, should not alter\nthe output of the model by a huge margin. Furthermore, we narrow down the scope\nto focus on revealing potential gender biases in DL and AI systems.\nSpecifically, we are interested to examine whether MesoInception-4 model\nproduces unfair decisions, which should be considered as a consequence of\nrobustness issues. The findings from our work have the potential to pave the\nway for new research directions in the quality assurance and fairness in DL and\nAI systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_M/0/1/0/all/0/1\">Muxin Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuan_M/0/1/0/all/0/1\">Meng Yi Kuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_N/0/1/0/all/0/1\">Nyee Thoang Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_C/0/1/0/all/0/1\">Chun Yong Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1\">Mei Kuan Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bures Joint Distribution Alignment with Dynamic Margin for Unsupervised Domain Adaptation. (arXiv:2203.06836v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06836","description":"<p>Unsupervised domain adaptation (UDA) is one of the prominent tasks of\ntransfer learning, and it provides an effective approach to mitigate the\ndistribution shift between the labeled source domain and the unlabeled target\ndomain. Prior works mainly focus on aligning the marginal distributions or the\nestimated class-conditional distributions. However, the joint dependency among\nthe feature and the label is crucial for the adaptation task and is not fully\nexploited. To address this problem, we propose the Bures Joint Distribution\nAlignment (BJDA) algorithm which directly models the joint distribution shift\nbased on the optimal transport theory in the infinite-dimensional kernel\nspaces. Specifically, we propose a novel alignment loss term that minimizes the\nkernel Bures-Wasserstein distance between the joint distributions. Technically,\nBJDA can effectively capture the nonlinear structures underlying the data. In\naddition, we introduce a dynamic margin in contrastive learning phase to\nflexibly characterize the class separability and improve the discriminative\nability of representations. It also avoids the cross-validation procedure to\ndetermine the margin parameter in traditional triplet loss based methods.\nExtensive experiments show that BJDA is very effective for the UDA tasks, as it\noutperforms state-of-the-art algorithms in most experimental settings. In\nparticular, BJDA improves the average accuracy of UDA tasks by 2.8% on\nAdaptiope, 1.4% on Office-Caltech10, and 1.1% on ImageCLEF-DA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao-Lin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Ke-Kun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STDAN: Deformable Attention Network for Space-Time Video Super-Resolution. (arXiv:2203.06841v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06841","description":"<p>The target of space-time video super-resolution (STVSR) is to increase the\nspatial-temporal resolution of low-resolution (LR) and low frame rate (LFR)\nvideos. Recent approaches based on deep learning have made significant\nimprovements, but most of them only use two adjacent frames, that is,\nshort-term features, to synthesize the missing frame embedding, which suffers\nfrom fully exploring the information flow of consecutive input LR frames. In\naddition, existing STVSR models hardly exploit the temporal contexts explicitly\nto assist high-resolution (HR) frame reconstruction. To address these issues,\nin this paper, we propose a deformable attention network called STDAN for\nSTVSR. First, we devise a long-short term feature interpolation (LSTFI) module,\nwhich is capable of excavating abundant content from more neighboring input\nframes for the interpolation process through a bidirectional RNN structure.\nSecond, we put forward a spatial-temporal deformable feature aggregation\n(STDFA) module, in which spatial and temporal contexts in dynamic video frames\nare adaptively captured and aggregated to enhance SR reconstruction.\nExperimental results on several datasets demonstrate that our approach\noutperforms state-of-the-art STVSR methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_X/0/1/0/all/0/1\">Xiaoyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RecursiveMix: Mixed Learning with History. (arXiv:2203.06844v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06844","description":"<p>Mix-based augmentation has been proven fundamental to the generalization of\ndeep vision models. However, current augmentations only mix samples at the\ncurrent data batch during training, which ignores the possible knowledge\naccumulated in the learning history. In this paper, we propose a recursive\nmixed-sample learning paradigm, termed \"RecursiveMix\" (RM), by exploring a\nnovel training strategy that leverages the historical input-prediction-label\ntriplets. More specifically, we iteratively resize the input image batch from\nthe previous iteration and paste it into the current batch while their labels\nare fused proportionally to the area of the operated patches. Further, a\nconsistency loss is introduced to align the identical image semantics across\nthe iterations, which helps the learning of scale-invariant feature\nrepresentations. Based on ResNet-50, RM largely improves classification\naccuracy by $\\sim$3.2\\% on CIFAR100 and $\\sim$2.8\\% on ImageNet with negligible\nextra computation/storage costs. In the downstream object detection task, the\nRM pretrained model outperforms the baseline by 2.1 AP points and surpasses\nCutMix by 1.4 AP points under the ATSS detector on COCO. In semantic\nsegmentation, RM also surpasses the baseline and CutMix by 1.9 and 1.1 mIoU\npoints under UperNet on ADE20K, respectively. Codes and pretrained models are\navailable at \\url{https://github.com/megvii-research/RecursiveMix}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Borui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Renjie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Scale Open-Set Deep Logo Detection. (arXiv:1911.07440v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.07440","description":"<p>We present an open-set logo detection (OSLD) system, which can detect\n(localize and recognize) any number of unseen logo classes without re-training;\nit only requires a small set of canonical logo images for each logo class. We\nachieve this using a two-stage approach: (1) Generic logo detection to detect\ncandidate logo regions in an image. (2) Logo matching for matching the detected\nlogo regions to a set of canonical logo images to recognize them.\n</p>\n<p>We constructed an open-set logo detection dataset with 12.1k logo classes and\nreleased it for research purposes.We demonstrate the effectiveness of OSLD on\nour dataset and on the standard Flickr-32 logo dataset, outperforming the\nstate-of-the-art open-set and closed-set logo detection methods by a large\nmargin. OSLD is scalable to millions of logo classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Muhammet Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tian Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kota_B/0/1/0/all/0/1\">Bhargava Kota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tek_M/0/1/0/all/0/1\">Mehmet Tek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Split and Expand: An inference-time improvement for Weakly Supervised Cell Instance Segmentation. (arXiv:2007.10817v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.10817","description":"<p>We consider the problem of segmenting cell nuclei instances from Hematoxylin\nand Eosin (H&amp;E) stains with weak supervision. While most recent works focus on\nimproving the segmentation quality, this is usually insufficient for instance\nsegmentation of cell instances clumped together or with a small size. In this\nwork, we propose a two-step post-processing procedure, Split and Expand, that\ndirectly improves the conversion of segmentation maps to instances. In the\nSplit step, we split clumps of cells from the segmentation map into individual\ncell instances with the guidance of cell-center predictions through Gaussian\nMixture Model clustering. In the Expand step, we find missing small cells using\nthe cell-center predictions (which tend to capture small cells more\nconsistently as they are trained using reliable point annotations), and utilize\nLayer-wise Relevance Propagation (LRP) explanation results to expand those\ncell-center predictions into cell instances. Our Split and Expand\npost-processing procedure is training-free and is executed at inference-time\nonly. To further improve the performance of our method, a feature re-weighting\nloss based on LRP is proposed. We test our procedure on the MoNuSeg and TNBC\ndatasets and show that our proposed method provides statistically significant\nimprovements on object-level metrics. Our code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1\">Lin Geng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_R/0/1/0/all/0/1\">Rui En Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Transformers: A Survey. (arXiv:2009.06732v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.06732","description":"<p>Transformer model architectures have garnered immense interest lately due to\ntheir effectiveness across a range of domains like language, vision and\nreinforcement learning. In the field of natural language processing for\nexample, Transformers have become an indispensable staple in the modern deep\nlearning stack. Recently, a dizzying number of \"X-former\" models have been\nproposed - Reformer, Linformer, Performer, Longformer, to name a few - which\nimprove upon the original Transformer architecture, many of which make\nimprovements around computational and memory efficiency. With the aim of\nhelping the avid researcher navigate this flurry, this paper characterizes a\nlarge and thoughtful selection of recent efficiency-flavored \"X-former\" models,\nproviding an organized and comprehensive overview of existing work and models\nacross multiple domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Guided Learning: Towards Open Domain Egocentric Action Recognition with Zero Supervision. (arXiv:2009.07470v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.07470","description":"<p>Advances in deep learning have enabled the development of models that have\nexhibited a remarkable tendency to recognize and even localize actions in\nvideos. However, they tend to experience errors when faced with scenes or\nexamples beyond their initial training environment. Hence, they fail to adapt\nto new domains without significant retraining with large amounts of annotated\ndata. In this paper, we propose to overcome these limitations by moving to an\nopen-world setting by decoupling the ideas of recognition and reasoning.\nBuilding upon the compositional representation offered by Grenander's Pattern\nTheory formalism, we show that attention and commonsense knowledge can be used\nto enable the self-supervised discovery of novel actions in egocentric videos\nin an open-world setting, where data from the observed environment (the target\ndomain) is open i.e., the vocabulary is partially known and training examples\n(both labeled and unlabeled) are not available. We show that our approach can\ninfer and learn novel classes for open vocabulary classification in egocentric\nvideos and novel object detection with zero supervision. Extensive experiments\nshow its competitive performance on two publicly available egocentric action\nrecognition datasets (GTEA Gaze and GTEA Gaze+) under open-world conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aakur_S/0/1/0/all/0/1\">Sathyanarayanan N. Aakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Sanjoy Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunti_N/0/1/0/all/0/1\">Nikhil Gunti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWIPENET: Object detection in noisy underwater images. (arXiv:2010.10006v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.10006","description":"<p>In recent years, deep learning based object detection methods have achieved\npromising performance in controlled environments. However, these methods lack\nsufficient capabilities to handle underwater object detection due to these\nchallenges: (1) images in the underwater datasets and real applications are\nblurry whilst accompanying severe noise that confuses the detectors and (2)\nobjects in real applications are usually small. In this paper, we propose a\nnovel Sample-WeIghted hyPEr Network (SWIPENET), and a robust training paradigm\nnamed Curriculum Multi-Class Adaboost (CMA), to address these two problems at\nthe same time. Firstly, the backbone of SWIPENET produces multiple high\nresolution and semantic-rich Hyper Feature Maps, which significantly improve\nsmall object detection. Secondly, a novel sample-weighted detection loss\nfunction is designed for SWIPENET, which focuses on learning high weight\nsamples and ignore learning low weight samples. Moreover, inspired by the human\neducation process that drives the learning from easy to hard concepts, we here\npropose the CMA training paradigm that first trains a clean detector which is\nfree from the influence of noisy data. Then, based on the clean detector,\nmultiple detectors focusing on learning diverse noisy data are trained and\nincorporated into a unified deep ensemble of strong noise immunity. Experiments\non two underwater robot picking contest datasets (URPC2017 and URPC2018) show\nthat the proposed SWIPENET+CMA framework achieves better accuracy in object\ndetection against several state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feixiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Ning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haiping Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery. (arXiv:2011.13913v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13913","description":"<p>Magnetic resonance imaging (MRI) offers the flexibility to image a given\nanatomic volume under a multitude of tissue contrasts. Yet, scan time\nconsiderations put stringent limits on the quality and diversity of MRI data.\nThe gold-standard approach to alleviate this limitation is to recover\nhigh-quality images from data undersampled across various dimensions, most\ncommonly the Fourier domain or contrast sets. A primary distinction among\nrecovery methods is whether the anatomy is processed per volume or per\ncross-section. Volumetric models offer enhanced capture of global contextual\ninformation, but they can suffer from suboptimal learning due to elevated model\ncomplexity. Cross-sectional models with lower complexity offer improved\nlearning behavior, yet they ignore contextual information across the\nlongitudinal dimension of the volume. Here, we introduce a novel progressive\nvolumetrization strategy for generative models (ProvoGAN) that serially\ndecomposes complex volumetric image recovery tasks into successive\ncross-sectional mappings task-optimally ordered across individual rectilinear\ndimensions. ProvoGAN effectively captures global context and recovers\nfine-structural details across all dimensions, while maintaining low model\ncomplexity and improved learning behaviour. Comprehensive demonstrations on\nmainstream MRI reconstruction and synthesis tasks show that ProvoGAN yields\nsuperior performance to state-of-the-art volumetric and cross-sectional models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dar_S/0/1/0/all/0/1\">Salman Ul Hassan Dar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinaz_B/0/1/0/all/0/1\">Berk T&#x131;naz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_K/0/1/0/all/0/1\">Kader Karl&#x131; O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Graph Attention Network with Conditional Kernels for Pixel-Wise Prediction. (arXiv:2101.02843v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.02843","description":"<p>Multi-scale representations deeply learned via convolutional neural networks\nhave shown tremendous importance for various pixel-level prediction problems.\nIn this paper we present a novel approach that advances the state of the art on\npixel-level prediction in a fundamental aspect, i.e. structured multi-scale\nfeatures learning and fusion. In contrast to previous works directly\nconsidering multi-scale feature maps obtained from the inner layers of a\nprimary CNN architecture, and simply fusing the features with weighted\naveraging or concatenation, we propose a probabilistic graph attention network\nstructure based on a novel Attention-Gated Conditional Random Fields (AG-CRFs)\nmodel for learning and fusing multi-scale representations in a principled\nmanner. In order to further improve the learning capacity of the network\nstructure, we propose to exploit feature dependant conditional kernels within\nthe deep probabilistic framework. Extensive experiments are conducted on four\npublicly available datasets (i.e. BSDS500, NYUD-V2, KITTI, and Pascal-Context)\nand on three challenging pixel-wise prediction problems involving both discrete\nand continuous labels (i.e. monocular depth estimation, object contour\nprediction, and semantic segmentation). Quantitative and qualitative results\ndemonstrate the effectiveness of the proposed latent AG-CRF model and the\noverall probabilistic graph attention network with feature conditional kernels\nfor structured feature learning and pixel-wise prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Underwater Image Enhancement via Learning Water Type Desensitized Representations. (arXiv:2102.00676v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.00676","description":"<p>We present a novel underwater image enhancement method termed SCNet to\nimprove the image quality meanwhile cope with the degradation diversity caused\nby the water. SCNet is based on normalization schemes across both spatial and\nchannel dimensions with the key idea of learning water type desensitized\nfeatures. Specifically, we apply whitening to de-correlate activations across\nspatial dimensions for each instance in a mini-batch. We also eliminate\nchannel-wise correlation by standardizing and re-injecting the first two\nmoments of the activations across channels. The normalization schemes of\nspatial and channel dimensions are performed at each scale of the U-Net to\nobtain multi-scale representations. With such water type irrelevant encodings,\nthe decoder can easily reconstruct the clean signal and be unaffected by the\ndistortion types. Experimental results on two real-world underwater image\ndatasets show that our approach can successfully enhance images with diverse\nwater types, and achieves competitive performance in visual quality\nimprovement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhenqi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaopeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Learn Once: Universal Anatomical Landmark Detection. (arXiv:2103.04657v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04657","description":"<p>Detecting anatomical landmarks in medical images plays an essential role in\nunderstanding the anatomy and planning automated processing. In recent years, a\nvariety of deep neural network methods have been developed to detect landmarks\nautomatically. However, all of those methods are unary in the sense that a\nhighly specialized network is trained for a single task say associated with a\nparticular anatomical region. In this work, for the first time, we investigate\nthe idea of \"You Only Learn Once (YOLO)\" and develop a universal anatomical\nlandmark detection model to realize multiple landmark detection tasks with\nend-to-end training based on mixed datasets. The model consists of a local\nnetwork and a global network: The local network is built upon the idea of\nuniversal U-Net to learn multi-domain local features and the global network is\na parallelly-duplicated sequential of dilated convolutions that extract global\nfeatures to further disambiguate the landmark locations. It is worth mentioning\nthat the new model design requires much fewer parameters than models with\nstandard convolutions to train. We evaluate our YOLO model on three X-ray\ndatasets of 1,588 images on the head, hand, and chest, collectively\ncontributing 62 landmarks. The experimental results show that our proposed\nuniversal model behaves largely better than any previous models trained on\nmultiple datasets. It even beats the performance of the model that is trained\nseparately for every single dataset. The code is available at\nhttps://github.com/MIRACLE-Center/YOLO_Universal_Anatomical_Landmark_Detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heqin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qingsong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R-PointHop: A Green, Accurate, and Unsupervised Point Cloud Registration Method. (arXiv:2103.08129v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08129","description":"<p>Inspired by the recent PointHop classification method, an unsupervised 3D\npoint cloud registration method, called R-PointHop, is proposed in this work.\nR-PointHop first determines a local reference frame (LRF) for every point using\nits nearest neighbors and finds local attributes. Next, R-PointHop obtains\nlocal-to-global hierarchical features by point downsampling, neighborhood\nexpansion, attribute construction and dimensionality reduction steps. Thus,\npoint correspondences are built in hierarchical feature space using the nearest\nneighbor rule. Afterwards, a subset of salient points with good correspondence\nis selected to estimate the 3D transformation. The use of the LRF allows for\ninvariance of the hierarchical features of points with respect to rotation and\ntranslation, thus making R-PointHop more robust at building point\ncorrespondence, even when the rotation angles are large. Experiments are\nconducted on the 3DMatch, ModelNet40, and Stanford Bunny datasets, which\ndemonstrate the effectiveness of R-PointHop for 3D point cloud registration.\nR-PointHop's model size and training time are an order of magnitude smaller\nthan those of deep learning methods, and its registration errors are smaller,\nmaking it a green and accurate solution. Our codes are available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadam_P/0/1/0/all/0/1\">Pranav Kadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08773","description":"<p>Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. Despite the success of the conventional supervised learning on\nindividual datasets, such models often struggle with generalization across\ntasks (e.g., a question-answering system cannot solve classification tasks). A\nlong-standing challenge in AI is to build a model that learns a new task by\nunderstanding the human-readable instructions that define it. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions, and 193k task instances (input-output pairs). The\ninstructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. Using this meta-dataset,\nwe measure cross-task generalization by training models on seen tasks and\nmeasuring generalization to the remaining unseen ones. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks (19%\nbetter for models utilizing instructions). These models, however, are far\nbehind an estimated performance upperbound indicating significant room for more\nprogress in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Explanations for Model Inversion Attacks. (arXiv:2104.12669v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12669","description":"<p>The successful deployment of artificial intelligence (AI) in many domains\nfrom healthcare to hiring requires their responsible use, particularly in model\nexplanations and privacy. Explainable artificial intelligence (XAI) provides\nmore information to help users to understand model decisions, yet this\nadditional knowledge exposes additional risks for privacy attacks. Hence,\nproviding explanation harms privacy. We study this risk for image-based model\ninversion attacks and identified several attack architectures with increasing\nperformance to reconstruct private image data from model explanations. We have\ndeveloped several multi-modal transposed CNN architectures that achieve\nsignificantly higher inversion performance than using the target model\nprediction only. These XAI-aware inversion models were designed to exploit the\nspatial knowledge in image explanations. To understand which explanations have\nhigher privacy risk, we analyzed how various explanation types and factors\ninfluence inversion performance. In spite of some models not providing\nexplanations, we further demonstrate increased inversion performance even for\nnon-explainable target models by exploiting explanations of surrogate models\nthrough attention transfer. This method first inverts an explanation from the\ntarget prediction, then reconstructs the target image. These threats highlight\nthe urgent and significant privacy risks of explanations and calls attention\nfor new privacy preservation techniques that balance the dual-requirement for\nAI explainability and privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xuejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wencan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiaokui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1\">Brian Y. Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Grounding with Transformers. (arXiv:2105.04281v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04281","description":"<p>In this paper, we propose a transformer based approach for visual grounding.\nUnlike previous proposal-and-rank frameworks that rely heavily on pretrained\nobject detectors or proposal-free frameworks that upgrade an off-the-shelf\none-stage detector by fusing textual embeddings, our approach is built on top\nof a transformer encoder-decoder and is independent of any pretrained detectors\nor word embedding models. Termed VGTR -- Visual Grounding with TRansformers,\nour approach is designed to learn semantic-discriminative visual features under\nthe guidance of the textual description without harming their location ability.\nThis information flow enables our VGTR to have a strong capability in capturing\ncontext-level semantics of both vision and language modalities, rendering us to\naggregate accurate visual clues implied by the description to locate the\ninterested object instance. Experiments show that our method outperforms\nstate-of-the-art proposal-free approaches by a considerable margin on five\nbenchmarks while maintaining fast inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Ye Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zehua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Coreset Selection for Rehearsal-based Continual Learning. (arXiv:2106.01085v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01085","description":"<p>A dataset is a shred of crucial evidence to describe a task. However, each\ndata point in the dataset does not have the same potential, as some of the data\npoints can be more representative or informative than others. This unequal\nimportance among the data points may have a large impact in rehearsal-based\ncontinual learning, where we store a subset of the training examples (coreset)\nto be replayed later to alleviate catastrophic forgetting. In continual\nlearning, the quality of the samples stored in the coreset directly affects the\nmodel's effectiveness and efficiency. The coreset selection problem becomes\neven more important under realistic settings, such as imbalanced continual\nlearning or noisy data scenarios. To tackle this problem, we propose Online\nCoreset Selection (OCS), a simple yet effective method that selects the most\nrepresentative and informative coreset at each iteration and trains them in an\nonline manner. Our proposed method maximizes the model's adaptation to a\ncurrent dataset while selecting high-affinity samples to past tasks, which\ndirectly inhibits catastrophic forgetting. We validate the effectiveness of our\ncoreset selection mechanism over various standard, imbalanced, and noisy\ndatasets against strong continual learning baselines, demonstrating that it\nimproves task adaptation and prevents catastrophic forgetting in a\nsample-efficient manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1\">Divyam Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations. (arXiv:2106.01548v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01548","description":"<p>Vision Transformers (ViTs) and MLPs signal further efforts on replacing\nhand-wired features or inductive biases with general-purpose neural\narchitectures. Existing works empower the models by massive data, such as\nlarge-scale pre-training and/or repeated strong data augmentations, and still\nreport optimization-related problems (e.g., sensitivity to initialization and\nlearning rates). Hence, this paper investigates ViTs and MLP-Mixers from the\nlens of loss geometry, intending to improve the models' data efficiency at\ntraining and generalization at inference. Visualization and Hessian reveal\nextremely sharp local minima of converged models. By promoting smoothness with\na recently proposed sharpness-aware optimizer, we substantially improve the\naccuracy and robustness of ViTs and MLP-Mixers on various tasks spanning\nsupervised, adversarial, contrastive, and transfer learning (e.g., +5.3\\% and\n+11.0\\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,\nwith the simple Inception-style preprocessing). We show that the improved\nsmoothness attributes to sparser active neurons in the first few layers. The\nresultant ViTs outperform ResNets of similar size and throughput when trained\nfrom scratch on ImageNet without large-scale pre-training or strong data\naugmentations. Model checkpoints are available at\n\\url{https://github.com/google-research/vision_transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Memorization in Adversarial Training. (arXiv:2106.01606v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01606","description":"<p>Deep learning models have a propensity for fitting the entire training set\neven with random labels, which requires memorization of every training sample.\nIn this paper, we explore the memorization effect in adversarial training (AT)\nfor promoting a deeper understanding of model capacity, convergence,\ngeneralization, and especially robust overfitting of the adversarially trained\nmodels. We first demonstrate that deep networks have sufficient capacity to\nmemorize adversarial examples of training data with completely random labels,\nbut not all AT algorithms can converge under the extreme circumstance. Our\nstudy of AT with random labels motivates further analyses on the convergence\nand generalization of AT. We find that some AT approaches suffer from a\ngradient instability issue and most recently suggested complexity measures\ncannot explain robust generalization by considering models trained on random\nlabels. Furthermore, we identify a significant drawback of memorization in AT\nthat it could result in robust overfitting. We then propose a new mitigation\nalgorithm motivated by detailed memorization analyses. Extensive experiments on\nvarious datasets validate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhijie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental False Negative Detection for Contrastive Learning. (arXiv:2106.03719v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03719","description":"<p>Self-supervised learning has recently shown great potential in vision tasks\nthrough contrastive learning, which aims to discriminate each image, or\ninstance, in the dataset. However, such instance-level learning ignores the\nsemantic relationship among instances and sometimes undesirably repels the\nanchor from the semantically similar samples, termed as \"false negatives\". In\nthis work, we show that the unfavorable effect from false negatives is more\nsignificant for the large-scale datasets with more semantic concepts. To\naddress the issue, we propose a novel self-supervised contrastive learning\nframework that incrementally detects and explicitly removes the false negative\nsamples. Specifically, following the training process, our method dynamically\ndetects increasing high-quality false negatives considering that the encoder\ngradually improves and the embedding space becomes more semantically\nstructural. Next, we discuss two strategies to explicitly remove the detected\nfalse negatives during contrastive learning. Extensive experiments show that\nour framework outperforms other self-supervised contrastive learning methods on\nmultiple benchmarks in a limited resource setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tsai-Shien Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResIST: Layer-Wise Decomposition of ResNets for Distributed Training. (arXiv:2107.00961v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00961","description":"<p>We propose ResIST, a novel distributed training protocol for Residual\nNetworks (ResNets). ResIST randomly decomposes a global ResNet into several\nshallow sub-ResNets that are trained independently in a distributed manner for\nseveral local iterations, before having their updates synchronized and\naggregated into the global model. In the next round, new sub-ResNets are\nrandomly generated and the process repeats until convergence. By construction,\nper iteration, ResIST communicates only a small portion of network parameters\nto each machine and never uses the full model during training. Thus, ResIST\nreduces the per-iteration communication, memory, and time requirements of\nResNet training to only a fraction of the requirements of full-model training.\nIn comparison to common protocols, like data-parallel training and\ndata-parallel training with local SGD, ResIST yields a decrease in\ncommunication and compute requirements, while being competitive with respect to\nmodel performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Christopher M. Jermaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Score-Based Point Cloud Denoising (Learning Implicit Gradient Fields for Point Cloud Denoising). (arXiv:2107.10981v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10981","description":"<p>Point clouds acquired from scanning devices are often perturbed by noise,\nwhich affects downstream tasks such as surface reconstruction and analysis. The\ndistribution of a noisy point cloud can be viewed as the distribution of a set\nof noise-free samples $p(x)$ convolved with some noise model $n$, leading to\n$(p * n)(x)$ whose mode is the underlying clean surface. To denoise a noisy\npoint cloud, we propose to increase the log-likelihood of each point from $p *\nn$ via gradient ascent -- iteratively updating each point's position. Since $p\n* n$ is unknown at test-time, and we only need the score (i.e., the gradient of\nthe log-probability function) to perform gradient ascent, we propose a neural\nnetwork architecture to estimate the score of $p * n$ given only noisy point\nclouds as input. We derive objective functions for training the network and\ndevelop a denoising algorithm leveraging on the estimated scores. Experiments\ndemonstrate that the proposed model outperforms state-of-the-art methods under\na variety of noise models, and shows the potential to be applied in other tasks\nsuch as point cloud upsampling. The code is available at\n\\url{https://github.com/luost26/score-denoise}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shitong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Human Cell Classification in Sparse Datasets using Few-Shot Learning. (arXiv:2107.13093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13093","description":"<p>Classifying and analyzing human cells is a lengthy procedure, often involving\na trained professional. In an attempt to expedite this process, an active area\nof research involves automating cell classification through use of deep\nlearning-based techniques. In practice, a large amount of data is required to\naccurately train these deep learning models. However, due to the sparse human\ncell datasets currently available, the performance of these models is typically\nlow. This study investigates the feasibility of using few-shot learning-based\ntechniques to mitigate the data requirements for accurate training. The study\nis comprised of three parts: First, current state-of-the-art few-shot learning\ntechniques are evaluated on human cell classification. The selected techniques\nare trained on a non-medical dataset and then tested on two out-of-domain,\nhuman cell datasets. The results indicate that, overall, the test accuracy of\nstate-of-the-art techniques decreased by at least 30% when transitioning from a\nnon-medical dataset to a medical dataset. Second, this study evaluates the\npotential benefits, if any, to varying the backbone architecture and training\nschemes in current state-of-the-art few-shot learning techniques when used in\nhuman cell classification. Even with these variations, the overall test\naccuracy decreased from 88.66% on non-medical datasets to 44.13% at best on the\nmedical datasets. Third, this study presents future directions for using\nfew-shot learning in human cell classification. In general, few-shot learning\nin its current state performs poorly on human cell classification. The study\nproves that attempts to modify existing network architectures are not effective\nand concludes that future research effort should be focused on improving\nrobustness towards out-of-domain testing using optimization-based or\nself-supervised few-shot learning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walsh_R/0/1/0/all/0/1\">Reece Walsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelpakey_M/0/1/0/all/0/1\">Mohamed H. Abdelpakey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1\">Mohamed S. Shehata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_M/0/1/0/all/0/1\">Mostafa M.Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WSDesc: Weakly Supervised 3D Local Descriptor Learning for Point Cloud Registration. (arXiv:2108.02740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02740","description":"<p>In this work, we present a novel method called WSDesc to learn 3D local\ndescriptors in a weakly supervised manner for robust point cloud registration.\nOur work builds upon recent 3D CNN-based descriptor extractors, which leverage\na voxel-based representation to parameterize local geometry of 3D points.\nInstead of using a predefined fixed-size local support in voxelization, we\npropose to learn the optimal support in a data-driven manner. To this end, we\ndesign a novel differentiable voxelization layer that can back-propagate the\ngradient to the support size optimization. To train the extracted descriptors,\nwe propose a novel registration loss based on the deviation from rigidity of 3D\ntransformations, and the loss is weakly supervised by the prior knowledge that\nthe input point clouds have partial overlap, without requiring ground-truth\nalignment information. Through extensive experiments, we show that our learned\ndescriptors yield superior performance on existing geometric registration\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-resolution quality assessment for pansharpening. (arXiv:2108.06144v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06144","description":"<p>A reliable quality assessment procedure for pansharpening methods is of\ncritical importance for the development of the related solutions.\nUnfortunately, the lack of ground-truths to be used as guidance for an\nobjective evaluation has pushed the community to resort to reference-based\nreduced-resolution indexes to assess synthesis ability and to no-reference\nsubjective quality indexes to be applied on full-resolution datasets to assess\nspectral and spatial consistency. Both reference and no-reference indexes\npresent critical shortcomings which motivate the community to explore new\nsolutions. In this work, we propose an alternative no-reference full-resolution\nassessment framework. On one side we introduce a protocol, namely the\nreprojection protocol, to take care of the spectral consistency issue. On the\nother side, a new index of the spatial consistency between the pansharpened\nimage and the panchromatic band at full resolution is also proposed.\nExperimental results carried out on different datasets/sensors demonstrate the\neffectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1\">Giuseppe Scarpa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciotola_M/0/1/0/all/0/1\">Matteo Ciotola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards unconstrained joint hand-object reconstruction from RGB videos. (arXiv:2108.07044v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07044","description":"<p>Our work aims to obtain 3D reconstruction of hands and manipulated objects\nfrom monocular videos. Reconstructing hand-object manipulations holds a great\npotential for robotics and learning from human demonstrations. The supervised\nlearning approach to this problem, however, requires 3D supervision and remains\nlimited to constrained laboratory settings and simulators for which 3D ground\ntruth is available. In this paper we first propose a learning-free fitting\napproach for hand-object reconstruction which can seamlessly handle two-hand\nobject interactions. Our method relies on cues obtained with common methods for\nobject detection, hand pose estimation and instance segmentation. We\nquantitatively evaluate our approach and show that it can be applied to\ndatasets with varying levels of difficulty for which training data is\nunavailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasson_Y/0/1/0/all/0/1\">Yana Hasson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_G/0/1/0/all/0/1\">G&#xfc;l Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.12056","description":"<p>Existing machines are functionally specific tools that were made for easy\nprediction and control. Tomorrow's machines may be closer to biological systems\nin their mutability, resilience, and autonomy. But first they must be capable\nof learning, and retaining, new information without repeated exposure to it.\nPast efforts to engineer such systems have sought to build or regulate\nartificial neural networks using task-specific modules with constrained\ncircumstances of application. This has not yet enabled continual learning over\nlong sequences of previously unseen data without corrupting existing knowledge:\na problem known as catastrophic forgetting. In this paper, we introduce a\nsystem that can learn sequentially over previously unseen datasets (ImageNet,\nCIFAR-100) with little forgetting over time. This is accomplished by regulating\nthe activity of weights in a convolutional neural network on the basis of\ninputs using top-down modulation generated by a second feed-forward neural\nnetwork. We find that our method learns continually under domain transfer with\nsparse bursts of activity in weights that are recycled across tasks, rather\nthan by maintaining task-specific modules. Sparse synaptic bursting is found to\nbalance enhanced and diminished activity in a way that facilitates adaptation\nto new inputs without corrupting previously acquired functions. This behavior\nemerges during a prior meta-learning phase in which regulated synapses are\nselectively disinhibited, or grown, from an initial state of uniform\nsuppression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1\">Shawn L. Beaulieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1\">Nick Cheney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Deep Image Deraining Using Dual Contrastive Learning. (arXiv:2109.02973v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02973","description":"<p>Learning single image deraining (SID) networks from an unpaired set of clean\nand rainy images is practical and valuable as acquiring paired real-world data\nis almost infeasible. However, without the paired data as the supervision,\nlearning a SID network is challenging. Moreover, simply using existing unpaired\nlearning methods (e.g., unpaired adversarial learning and cycle-consistency\nconstraints) in the SID task is insufficient to learn the underlying\nrelationship from rainy inputs to clean outputs as there exists significant\ndomain gap between the rainy and clean images. In this paper, we develop an\neffective unpaired SID adversarial framework which explores mutual properties\nof the unpaired exemplars by a dual contrastive learning manner in a deep\nfeature space, named as DCD-GAN. The proposed method mainly consists of two\ncooperative branches: Bidirectional Translation Branch (BTB) and Contrastive\nGuidance Branch (CGB). Specifically, BTB exploits full advantage of the\ncirculatory architecture of adversarial consistency to generate abundant\nexemplar pairs and excavates latent feature distributions between two domains\nby equipping it with bidirectional mapping. Simultaneously, CGB implicitly\nconstrains the embeddings of different exemplars in the deep feature space by\nencouraging the similar feature distributions closer while pushing the\ndissimilar further away, in order to better facilitate rain removal and help\nimage restoration. Extensive experiments demonstrate that our method performs\nfavorably against existing unpaired deraining approaches on both synthetic and\nreal-world datasets, and generates comparable results against several\nfully-supervised or semi-supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Caihua Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Longgang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhentao Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04275","description":"<p>Despite the potential of multi-modal pre-training to learn highly\ndiscriminative feature representations from complementary data modalities,\ncurrent progress is being slowed by the lack of large-scale modality-diverse\ndatasets. By leveraging the natural suitability of E-commerce, where different\nmodalities capture complementary semantic information, we contribute a\nlarge-scale multi-modal pre-training dataset M5Product. The dataset comprises 5\nmodalities (image, text, table, video, and audio), covers over 6,000 categories\nand 5,000 attributes, and is 500 larger than the largest publicly available\ndataset with a similar number of modalities. Furthermore, M5Product contains\nincomplete modality pairs and noise while also having a long-tailed\ndistribution, resembling most real-world problems. We further propose\nSelf-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework\nthat integrates the different modalities into a unified model through an\nadaptive feature fusion mechanism, where the importance of each modality is\nlearned directly from the modality embeddings and impacts the inter-modality\ncontrastive learning and masked tasks within a multi-modal transformer model.\nWe evaluate the current multi-modal pre-training state-of-the-art approaches\nand benchmark their ability to learn from unlabeled data when faced with the\nlarge number of modalities in the M5Product dataset. We conduct extensive\nexperiments on four downstream tasks and demonstrate the superiority of our\nSCALE model, providing insights into the importance of dataset scale and\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael C. Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated Construction of Multimodal Atlases with Structural Connectomes in the Space of Riemannian Metrics. (arXiv:2109.09808v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09808","description":"<p>The structural network of the brain, or structural connectome, can be\nrepresented by fiber bundles generated by a variety of tractography methods.\nWhile such methods give qualitative insights into brain structure, there is\ncontroversy over whether they can provide quantitative information, especially\nat the population level. In order to enable population-level statistical\nanalysis of the structural connectome, we propose representing a connectome as\na Riemannian metric, which is a point on an infinite-dimensional manifold. We\nequip this manifold with the Ebin metric, a natural metric structure for this\nspace, to get a Riemannian manifold along with its associated geometric\nproperties. We then use this Riemannian framework to apply object-oriented\nstatistical analysis to define an atlas as the Fr\\'echet mean of a population\nof Riemannian metrics. This formulation ties into the existing framework for\ndiffeomorphic construction of image atlases, allowing us to construct a\nmultimodal atlas by simultaneously integrating complementary white matter\nstructure details from DWMRI and cortical details from T1-weighted MRI. We\nillustrate our framework with 2D data examples of connectome registration and\natlas formation. Finally, we build an example 3D multimodal atlas using T1\nimages and connectomes derived from diffusion tensors estimated from a subset\nof subjects from the Human Connectome Project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campbell_K/0/1/0/all/0/1\">Kristen M. Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haocheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhe Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1\">Martin Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_P/0/1/0/all/0/1\">P. Thomas Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sarang C. Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Dense Video Grounding via Parallel Regression. (arXiv:2109.11265v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11265","description":"<p>Video grounding aims to localize the corresponding video moment in an\nuntrimmed video given a language query. Existing methods often address this\ntask in an indirect way, by casting it as a proposal-and-match or\nfusion-and-detection problem. Solving these surrogate problems often requires\nsophisticated label assignment during training and hand-crafted removal of\nnear-duplicate results. Meanwhile, existing works typically focus on sparse\nvideo grounding with a single sentence as input, which could result in\nambiguous localization due to its unclear description. In this paper, we tackle\na new problem of dense video grounding, by simultaneously localizing multiple\nmoments with a paragraph as input. From a perspective on video grounding as\nlanguage conditioned regression, we present an end-to-end parallel decoding\nparadigm by re-purposing a Transformer-alike architecture (PRVG). The key\ndesign in our PRVG is to use languages as queries, and directly regress the\nmoment boundaries based on language-modulated visual representations. Thanks to\nits simplicity in design, our PRVG framework can be applied in different\ntesting schemes (sparse or dense grounding) and allows for efficient inference\nwithout any post-processing technique. In addition, we devise a robust\nproposal-level attention loss to guide the training of PRVG, which is invariant\nto moment duration and contributes to model convergence. We perform experiments\non two video grounding benchmarks of ActivityNet Captions and TACoS,\ndemonstrating that our PRVG can significantly outperform previous methods. We\nalso perform in-depth studies to investigate the effectiveness of parallel\nregression paradigm on video grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Fengyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing Generalization to Deep Multi-View Pedestrian Detection. (arXiv:2109.12227v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12227","description":"<p>Multi-view Detection (MVD) is highly effective for occlusion reasoning in a\ncrowded environment. While recent works using deep learning have made\nsignificant advances in the field, they have overlooked the generalization\naspect, which makes them impractical for real-world deployment. The key novelty\nof our work is to formalize three critical forms of generalization and propose\nexperiments to evaluate them: generalization with i) a varying number of\ncameras, ii) varying camera positions, and finally, iii) to new scenes. We find\nthat existing state-of-the-art models show poor generalization by overfitting\nto a single scene and camera configuration. To address the concerns: (a) we\npropose a novel Generalized MVD (GMVD) dataset, assimilating diverse scenes\nwith changing daytime, camera configurations, varying number of cameras, and\n(b) we discuss the properties essential to bring generalization to MVD and\npropose a barebones model to incorporate them. We perform a comprehensive set\nof experiments on the WildTrack, MultiViewX, and the GMVD datasets to motivate\nthe necessity to evaluate the generalization abilities of MVD methods and to\ndemonstrate the efficacy of the proposed approach. The code and the proposed\ndataset can be found at https://github.com/jeetv/GMVD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vora_J/0/1/0/all/0/1\">Jeet Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Swetanjal Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_K/0/1/0/all/0/1\">Kanishk Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Motion-aware Contrastive Video Representation Learning via Foreground-background Merging. (arXiv:2109.15130v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15130","description":"<p>In light of the success of contrastive learning in the image domain, current\nself-supervised video representation learning methods usually employ\ncontrastive loss to facilitate video representation learning. When naively\npulling two augmented views of a video closer, the model however tends to learn\nthe common static background as a shortcut but fails to capture the motion\ninformation, a phenomenon dubbed as background bias. Such bias makes the model\nsuffer from weak generalization ability, leading to worse performance on\ndownstream tasks such as action recognition. To alleviate such bias, we propose\n\\textbf{F}oreground-b\\textbf{a}ckground \\textbf{Me}rging (FAME) to deliberately\ncompose the moving foreground region of the selected video onto the static\nbackground of others. Specifically, without any off-the-shelf detector, we\nextract the moving foreground out of background regions via the frame\ndifference and color statistics, and shuffle the background regions among the\nvideos. By leveraging the semantic consistency between the original clips and\nthe fused ones, the model focuses more on the motion patterns and is debiased\nfrom the background shortcut. Extensive experiments demonstrate that FAME can\neffectively resist background cheating and thus achieve the state-of-the-art\nperformance on downstream tasks across UCF101, HMDB51, and Diving48 datasets.\nThe code and configurations are released at https://github.com/Mark12Ding/FAME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuangrui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maomao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haohang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hongkai Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.02636","description":"<p>Diffusion-based inpainting is a powerful tool for the reconstruction of\nimages from sparse data. Its quality strongly depends on the choice of known\ndata. Optimising their spatial location -- the inpainting mask -- is\nchallenging. A commonly used tool for this task are stochastic optimisation\nstrategies. However, they are slow as they compute multiple inpainting results.\nWe provide a remedy in terms of a learned mask generation model. By emulating\nthe complete inpainting pipeline with two networks for mask generation and\nneural surrogate inpainting, we obtain a model for highly efficient adaptive\nmask generation. Experiments indicate that our model can achieve competitive\nquality with an acceleration by as much as four orders of magnitude. Our\nfindings serve as a basis for making diffusion-based inpainting more attractive\nfor applications such as image compression, where fast encoding is highly\ndesirable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alt_T/0/1/0/all/0/1\">Tobias Alt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peter_P/0/1/0/all/0/1\">Pascal Peter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weickert_J/0/1/0/all/0/1\">Joachim Weickert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPSN: Motion-aware Pseudo Siamese Network for Indoor Video Head Detection in Buildings. (arXiv:2110.03302v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03302","description":"<p>Head detection in the indoor video is an essential component of building\noccupancy detection. While deep models have achieved remarkable progress in\ngeneral object detection, they are not satisfying enough in complex indoor\nscenes. The indoor surveillance video often includes cluttered background\nobjects, among which heads have small scales and diverse poses. In this paper,\nwe propose Motion-aware Pseudo Siamese Network (MPSN), an end-to-end approach\nthat leverages head motion information to guide the deep model to extract\neffective head features in indoor scenarios. By taking the pixel-wise\ndifference of adjacent frames as the auxiliary input, MPSN effectively enhances\nhuman head motion information and removes the irrelevant objects in the\nbackground. Compared with prior methods, it achieves superior performance on\nthe two indoor video datasets. Our experiments show that MPSN successfully\nsuppresses static background objects and highlights the moving instances,\nespecially human heads in indoor videos. We also compare different methods to\ncapture head motion, which demonstrates the simplicity and flexibility of MPSN.\nFinally, to validate the robustness of MPSN, we conduct adversarial experiments\nwith a mathematical solution of small perturbations for robust model selection.\nCode is available at https://github.com/pl-share/MPSN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kailai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoteng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qianchuan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm. (arXiv:2110.05208v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05208","description":"<p>Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has\nattracted unprecedented attention for its impressive zero-shot recognition\nability and excellent transferability to downstream tasks. However, CLIP is\nquite data-hungry and requires 400M image-text pairs for pre-training, thereby\nrestricting its adoption. This work proposes a novel training paradigm, Data\nefficient CLIP (DeCLIP), to alleviate this limitation. We demonstrate that by\ncarefully utilizing the widespread supervision among the image-text pairs, our\nDe-CLIP can learn generic visual features more efficiently. Instead of using\nthe single image-text contrastive supervision, we fully exploit data potential\nthrough the use of (1) self-supervision within each modality; (2) multi-view\nsupervision across modalities; (3) nearest-neighbor supervision from other\nsimilar pairs. Benefiting from intrinsic supervision, our DeCLIP-ResNet50 can\nachieve 60.4% zero-shot top1 accuracy on ImageNet, which is 0.8% above the\nCLIP-ResNet50 while using 7.1 x fewer data. Our DeCLIP-ResNet50 outperforms its\ncounterpart in 8 out of 11 visual datasets when transferred to downstream\ntasks. Moreover, Scaling up the model and computing also works well in our\nframework.Our code, dataset and models are released at:\nhttps://github.com/Sense-GVT/DeCLIP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yufeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Supervised Pre-training for Better Downstream Transferring. (arXiv:2110.06014v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06014","description":"<p>The pretrain-finetune paradigm has shown outstanding performance on many\napplications of deep learning, where a model is pre-trained on a upstream large\ndataset (e.g. ImageNet), and is then fine-tuned to different downstream tasks.\nThough for most cases, the pre-training stage is conducted based on supervised\nmethods, recent works on self-supervised pre-training have shown powerful\ntransferability and even outperform supervised pre-training on multiple\ndownstream tasks. It thus remains an open question how to better generalize\nsupervised pre-training model to downstream tasks. In this paper, we argue that\nthe worse transferability of existing supervised pre-training methods arise\nfrom the negligence of valuable intra-class semantic difference. This is\nbecause these methods tend to push images from the same class close to each\nother despite of the large diversity in their visual contents, a problem to\nwhich referred as \"overfit of upstream tasks\". To alleviate this problem, we\npropose a new supervised pre-training method based on Leave-One-Out\nK-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting\nupstream tasks by only requiring each image to share its class label with most\nof its k nearest neighbors, thus allowing each class to exhibit a multi-mode\ndistribution and consequentially preserving part of intra-class difference for\nbetter transferring to downstream tasks. We developed efficient implementation\nof the proposed method that scales well to large datasets. Experimental studies\non multiple downstream tasks show that LOOK outperforms other state-of-the-art\nmethods for supervised and self-supervised pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video. (arXiv:2110.07058v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07058","description":"<p>We introduce Ego4D, a massive-scale egocentric video dataset and benchmark\nsuite. It offers 3,670 hours of daily-life activity video spanning hundreds of\nscenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique\ncamera wearers from 74 worldwide locations and 9 different countries. The\napproach to collection is designed to uphold rigorous privacy and ethics\nstandards with consenting participants and robust de-identification procedures\nwhere relevant. Ego4D dramatically expands the volume of diverse egocentric\nvideo footage publicly available to the research community. Portions of the\nvideo are accompanied by audio, 3D meshes of the environment, eye gaze, stereo,\nand/or synchronized videos from multiple egocentric cameras at the same event.\nFurthermore, we present a host of new benchmark challenges centered around\nunderstanding the first-person visual experience in the past (querying an\nepisodic memory), present (analyzing hand-object manipulation, audio-visual\nconversation, and social interactions), and future (forecasting activities). By\npublicly sharing this massive annotated dataset and benchmark suite, we aim to\npush the frontier of first-person perception. Project page:\nhttps://ego4d-data.org/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westbury_A/0/1/0/all/0/1\">Andrew Westbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_E/0/1/0/all/0/1\">Eugene Byrne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavis_Z/0/1/0/all/0/1\">Zachary Chavis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamburger_J/0/1/0/all/0/1\">Jackson Hamburger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Miao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1\">Miguel Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1\">Tushar Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radosavovic_I/0/1/0/all/0/1\">Ilija Radosavovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Kumar Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_F/0/1/0/all/0/1\">Fiona Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jayant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_M/0/1/0/all/0/1\">Michael Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_E/0/1/0/all/0/1\">Eric Zhongcong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Siddhant Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cartillier_V/0/1/0/all/0/1\">Vincent Cartillier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_S/0/1/0/all/0/1\">Sean Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulaty_M/0/1/0/all/0/1\">Morrie Doulaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erapalli_A/0/1/0/all/0/1\">Akshay Erapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragomeni_A/0/1/0/all/0/1\">Adriano Fragomeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebreselasie_A/0/1/0/all/0/1\">Abrham Gebreselasie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Cristina Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillis_J/0/1/0/all/0/1\">James Hillis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenqi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoo_W/0/1/0/all/0/1\">Weslie Khoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolar_J/0/1/0/all/0/1\">Jachym Kolar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1\">Satwik Kottur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landini_F/0/1/0/all/0/1\">Federico Landini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modhugu_R/0/1/0/all/0/1\">Raghava Modhugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munro_J/0/1/0/all/0/1\">Jonathan Munro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murrell_T/0/1/0/all/0/1\">Tullie Murrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishiyasu_T/0/1/0/all/0/1\">Takumi Nishiyasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Will Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puentes_P/0/1/0/all/0/1\">Paola Ruiz Puentes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramazanova_M/0/1/0/all/0/1\">Merey Ramazanova</a>, et al. (33 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Semantic Segmentation by Pixel-to-Prototype Contrast. (arXiv:2110.07110v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07110","description":"<p>Though image-level weakly supervised semantic segmentation (WSSS) has\nachieved great progress with Class Activation Maps (CAMs) as the cornerstone,\nthe large supervision gap between classification and segmentation still hampers\nthe model to generate more complete and precise pseudo masks for segmentation.\nIn this study, we propose weakly-supervised pixel-to-prototype contrast that\ncan provide pixel-level supervisory signals to narrow the gap. Guided by two\nintuitive priors, our method is executed across different views and within per\nsingle view of an image, aiming to impose cross-view feature semantic\nconsistency regularization and facilitate intra(inter)-class\ncompactness(dispersion) of the feature space. Our method can be seamlessly\nincorporated into existing WSSS models without any changes to the base networks\nand does not incur any extra inference burden. Extensive experiments manifest\nthat our method consistently improves two strong baselines by large margins,\ndemonstrating the effectiveness. Specifically, built on top of SEAM, we improve\nthe initial seed mIoU on PASCAL VOC 2012 from 55.4% to 61.5%. Moreover, armed\nwith our method, we increase the segmentation mIoU of EPS from 70.8% to 73.6%,\nachieving new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Ye Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zehua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Translation using Texture Co-occurrence and Spatial Self-Similarity for Texture Debiasing. (arXiv:2110.07920v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07920","description":"<p>Classification models trained on datasets with texture bias usually perform\npoorly on out-of-distribution samples since biased representations are embedded\ninto the model. Recently, various debiasing methods have attempted to\ndisentangle biased representations, but discarding texture biased features\nwithout altering other relevant information is still a challenging task. In\nthis paper, we propose a novel texture debiasing approach to generate\nadditional training images using the content of a source image and the texture\nof a target image with a different semantic label to explicitly mitigate\ntexture biases when training a classifier. Our model ensures texture similarity\nbetween the target and generated images via a texture co-occurrence loss while\npreserving the content details from the source image with a spatial\nself-similarity loss. Both the generated and original training images are\ncombined to train an improved classifier that is robust against inconsistent\ntexture bias representations. We employ five datasets with known texture biases\nto demonstrate the ability of our method to mitigate texture bias. In all\ncases, our method outperformed existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myeongkyun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_D/0/1/0/all/0/1\">Dongkyu Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luna_M/0/1/0/all/0/1\">Miguel Luna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chikontwe_P/0/1/0/all/0/1\">Philip Chikontwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kyung Soo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">June Hong Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sang Hyun Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal-Boost: Multimodal Medical Image Super-Resolution using Multi-Attention Network with Wavelet Transform. (arXiv:2110.11684v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.11684","description":"<p>Deep learning based single image super resolution (SISR) algorithms has\nrevolutionized the overall diagnosis framework by continually improving the\narchitectural components and training strategies associated with convolutional\nneural networks (CNN) on low-resolution images. However, existing work lacks in\ntwo ways: i) the SR output produced exhibits poor texture details, and often\nproduce blurred edges, ii) most of the models have been developed for a single\nmodality, hence, require modification to adapt to a new one. This work\naddresses (i) by proposing generative adversarial network (GAN) with deep\nmulti-attention modules to learn high-frequency information from low-frequency\ndata. Existing approaches based on the GAN have yielded good SR results;\nhowever, the texture details of their SR output have been experimentally\nconfirmed to be deficient for medical images particularly. The integration of\nwavelet transform (WT) and GANs in our proposed SR model addresses the\naforementioned limitation concerning textons. While the WT divides the LR image\ninto multiple frequency bands, the transferred GAN uses multi-attention and\nupsample blocks to predict high-frequency components. Additionally, we present\na learning method for training domain-specific classifiers as perceptual loss\nfunctions. Using a combination of multi-attention GAN loss and a perceptual\nloss function results in an efficient and reliable performance. Applying the\nsame model for medical images from diverse modalities is challenging, our work\naddresses (ii) by training and performing on several modalities via transfer\nlearning. Using two medical datasets, we validate our proposed SR network\nagainst existing state-of-the-art approaches and achieve promising results in\nterms of SSIM and PSNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dharejo_F/0/1/0/all/0/1\">Fayaz Ali Dharejo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zawish_M/0/1/0/all/0/1\">Muhammad Zawish</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_F/0/1/0/all/0/1\">Farah Deeba Yuanchun Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dev_K/0/1/0/all/0/1\">Kapal Dev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khowaja_S/0/1/0/all/0/1\">Sunder Ali Khowaja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qureshi_N/0/1/0/all/0/1\">Nawab Muhammad Faseeh Qureshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Longitudinal Analysis of Mask and No-Mask on Child Face Recognition. (arXiv:2111.00121v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00121","description":"<p>Face is one of the most widely employed traits for person recognition, even\nin many large-scale applications. Despite technological advancements in face\nrecognition systems, they still face obstacles caused by pose, expression,\nocclusion, and aging variations. Owing to the COVID-19 pandemic, contactless\nidentity verification has become exceedingly vital. Recently, few studies have\nbeen conducted on the effect of face mask on adult face recognition systems\n(FRS). However, the impact of aging with face mask on child subject recognition\nhas not been adequately explored. Thus, the main objective of this study is\nanalyzing the child longitudinal impact together with face mask and other\ncovariates on FRS. Specifically, we performed a comparative investigation of\nthree top performing publicly available face matchers and a post-COVID-19\ncommercial-off-the-shelf (COTS) system under child cross-age verification and\nidentification settings using our generated synthetic mask and no-mask samples.\nFurthermore, we investigated the longitudinal consequence of eyeglasses with\nmask and no-mask. The study exploited no-mask longitudinal child face dataset\n(i.e., extended Indian Child Longitudinal Face Dataset) that contains 26,258\nface images of 7,473 subjects in the age group of [2, 18] over an average time\nspan of 3.35 years. Due to the combined effects of face mask and face aging,\nthe FaceNet, PFE, ArcFace, and COTS face verification system accuracies\ndecrease approximately 25%, 22%, 18%, 12%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandaliya_P/0/1/0/all/0/1\">Praveen Kumar Chandaliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_Z/0/1/0/all/0/1\">Zahid Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nain_N/0/1/0/all/0/1\">Neeta Nain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation. (arXiv:2111.08557v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08557","description":"<p>In keypoint estimation tasks such as human pose estimation, heatmap-based\nregression is the dominant approach despite possessing notable drawbacks:\nheatmaps intrinsically suffer from quantization error and require excessive\ncomputation to generate and post-process. Motivated to find a more efficient\nsolution, we propose to model individual keypoints and sets of spatially\nrelated keypoints (i.e., poses) as objects within a dense single-stage\nanchor-based detection framework. Hence, we call our method KAPAO (pronounced\n\"Ka-Pow\"), for Keypoints And Poses As Objects. KAPAO is applied to the problem\nof single-stage multi-person human pose estimation by simultaneously detecting\nhuman pose and keypoint objects and fusing the detections to exploit the\nstrengths of both object representations. In experiments, we observe that KAPAO\nis faster and more accurate than previous methods, which suffer greatly from\nheatmap post-processing. The accuracy-speed trade-off is especially favourable\nin the practical setting when not using test-time augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McNally_W/0/1/0/all/0/1\">William McNally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vats_K/0/1/0/all/0/1\">Kanav Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1\">John McPhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection. (arXiv:2111.09099v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09099","description":"<p>Anomaly detection is commonly pursued as a one-class classification problem,\nwhere models can only learn from normal training samples, while being evaluated\non both normal and abnormal test samples. Among the successful approaches for\nanomaly detection, a distinguished category of methods relies on predicting\nmasked information (e.g. patches, future frames, etc.) and leveraging the\nreconstruction error with respect to the masked information as an abnormality\nscore. Different from related methods, we propose to integrate the\nreconstruction-based functionality into a novel self-supervised predictive\narchitectural building block. The proposed self-supervised block is generic and\ncan easily be incorporated into various state-of-the-art anomaly detection\nmethods. Our block starts with a convolutional layer with dilated filters,\nwhere the center area of the receptive field is masked. The resulting\nactivation maps are passed through a channel attention module. Our block is\nequipped with a loss that minimizes the reconstruction error with respect to\nthe masked area in the receptive field. We demonstrate the generality of our\nblock by integrating it into several state-of-the-art frameworks for anomaly\ndetection on image and video, providing empirical evidence that shows\nconsiderable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We\nrelease our code as open source at https://github.com/ristea/sspcab.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1\">Neelu Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1\">Kamal Nasrollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for text-to-image and image-to-text\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation tasks without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial results of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition. (arXiv:2111.12994v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12994","description":"<p>Recently, Vision Transformers (ViT), with the self-attention (SA) as the de\nfacto ingredients, have demonstrated great potential in the computer vision\ncommunity. For the sake of trade-off between efficiency and performance, a\ngroup of works merely perform SA operation within local patches, whereas the\nglobal contextual information is abandoned, which would be indispensable for\nvisual recognition tasks. To solve the issue, the subsequent global-local ViTs\ntake a stab at marrying local SA with global one in parallel or alternative way\nin the model. Nevertheless, the exhaustively combined local and global context\nmay exist redundancy for various visual data, and the receptive field within\neach layer is fixed. Alternatively, a more graceful way is that global and\nlocal context can adaptively contribute per se to accommodate different visual\ndata. To achieve this goal, we in this paper propose a novel ViT architecture,\ntermed NomMer, which can dynamically Nominate the synergistic global-local\ncontext in vision transforMer. By investigating the working pattern of our\nproposed NomMer, we further explore what context information is focused.\nBeneficial from this \"dynamic nomination\" mechanism, without bells and\nwhistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy\non ImageNet with only 73M parameters, but also show promising performance on\ndense prediction tasks, i.e., object detection and semantic segmentation. The\ncode and models will be made publicly available at\nhttps://github.com/TencentYoutuResearch/VisualRecognition-NomMer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinghua Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhimin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Deqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Real Image Super-resolution via Distortion-Relation Guided Transfer Learning. (arXiv:2111.13078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13078","description":"<p>Collecting large clean-distorted training image pairs in real world is\nnon-trivial, which seriously limits the practical applications of these\nsupervised learning based image super-resolution (SR) methods. Previous works\nattempt to address this problem by leveraging unsupervised learning\ntechnologies to alleviate the dependency for paired training samples. However,\nthese methods typically suffer from unsatisfactory textures synthesis due to\nthe lack of clean image supervision. Compared with purely unsupervised\nsolution, the under-explored scheme with Few-Shot clean images (FS-RSR) is more\nfeasible to tackle this challenging Real Image Super-Resolution task. In this\npaper, we are the first to investigate the few-shot real image super-resolution\nand propose a Distortion-Relation guided Transfer Learning (termed as DRTL)\nframework. DRTL assigns a knowledge graph to capture the distortion relation\nbetween auxiliary tasks (i.e., synthetic distortions) and target tasks (i.e.,\nreal distortions with few images), and then adopt a gradient weighting strategy\nto guide the knowledge transfer from auxiliary task to target task. In this\nway, DRTL could quickly learn the most relevant knowledge from the prior\ndistortions for target distortion. We instantiate DRTL integrated with\npre-training and meta-learning pipelines as an embodiment to realize a\ndistortion-relation aware FS-RSR. Extensive experiments on multiple benchmarks\ndemonstrate the effectiveness of DRTL on few-shot real image super-resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_B/0/1/0/all/0/1\">Bei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Integration of Self-Attention and Convolution. (arXiv:2111.14556v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14556","description":"<p>Convolution and self-attention are two powerful techniques for representation\nlearning, and they are usually considered as two peer approaches that are\ndistinct from each other. In this paper, we show that there exists a strong\nunderlying relation between them, in the sense that the bulk of computations of\nthese two paradigms are in fact done with the same operation. Specifically, we\nfirst show that a traditional convolution with kernel size k x k can be\ndecomposed into k^2 individual 1x1 convolutions, followed by shift and\nsummation operations. Then, we interpret the projections of queries, keys, and\nvalues in self-attention module as multiple 1x1 convolutions, followed by the\ncomputation of attention weights and aggregation of the values. Therefore, the\nfirst stage of both two modules comprises the similar operation. More\nimportantly, the first stage contributes a dominant computation complexity\n(square of the channel size) comparing to the second stage. This observation\nnaturally leads to an elegant integration of these two seemingly distinct\nparadigms, i.e., a mixed model that enjoys the benefit of both self-Attention\nand Convolution (ACmix), while having minimum computational overhead compared\nto the pure convolution or self-attention counterpart. Extensive experiments\nshow that our model achieves consistently improved results over competitive\nbaselines on image recognition and downstream tasks. Code and pre-trained\nmodels will be released at https://github.com/LeapLabTHU/ACmix and\nhttps://gitee.com/mindspore/models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xuran Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chunjiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Rui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanfu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AssistSR: Task-oriented Question-driven Video Segment Retrieval. (arXiv:2111.15050v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15050","description":"<p>It is still a pipe dream that AI assistants on phone and AR glasses can\nassist our daily life in addressing our questions like \"how to adjust the date\nfor this watch?\" and \"how to set its heating duration? (while pointing at an\noven)\". The queries used in conventional tasks (i.e. Video Question Answering,\nVideo Retrieval, Moment Localization) are often factoid and based on pure text.\nIn contrast, we present a new task called Task-oriented Question-driven Video\nSegment Retrieval (TQVSR). Each of our questions is an image-box-text query\nthat focuses on affordance of items in our daily life and expects relevant\nanswer segments to be retrieved from a corpus of instructional video-transcript\nsegments. To support the study of this TQVSR task, we construct a new dataset\ncalled AssistSR. We design novel guidelines to create high-quality samples.\nThis dataset contains 1.4k multimodal questions on 1k video segments from\ninstructional videos on diverse daily-used items. To address TQVSR, we develop\na straightforward yet effective model called Dual Multimodal Encoders (DME)\nthat significantly outperforms several baseline methods while still having\nlarge room for improvement in the future. Moreover, we present detailed\nablation analyses. Our codes and data are available\nat~\\url{https://github.com/StanLei52/AQVSR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1\">Dongxing Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirObject: A Temporally Evolving Graph Embedding for Object Identification. (arXiv:2111.15150v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15150","description":"<p>Object encoding and identification are vital for robotic tasks such as\nautonomous exploration, semantic scene understanding, and re-localization.\nPrevious approaches have attempted to either track objects or generate\ndescriptors for object identification. However, such systems are limited to a\n\"fixed\" partial object representation from a single viewpoint. In a robot\nexploration setup, there is a requirement for a temporally \"evolving\" global\nobject representation built as the robot observes the object from multiple\nviewpoints. Furthermore, given the vast distribution of unknown novel objects\nin the real world, the object identification process must be class-agnostic. In\nthis context, we propose a novel temporal 3D object encoding approach, dubbed\nAirObject, to obtain global keypoint graph-based embeddings of objects.\nSpecifically, the global 3D object embeddings are generated using a temporal\nconvolutional network across structural information of multiple frames obtained\nfrom a graph attention-based encoding method. We demonstrate that AirObject\nachieves the state-of-the-art performance for video object identification and\nis robust to severe occlusion, perceptual aliasing, viewpoint shift,\ndeformation, and scale transform, outperforming the state-of-the-art\nsingle-frame and sequential descriptors. To the best of our knowledge,\nAirObject is one of the first temporal object encoding methods. Source code is\navailable at https://github.com/Nik-V9/AirObject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keetha_N/0/1/0/all/0/1\">Nikhil Varma Keetha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yuheng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via Regularized Domain Concatenation. (arXiv:2111.15242v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15242","description":"<p>Transferring knowledge learned from the labeled source domain to the raw\ntarget domain for unsupervised domain adaptation (UDA) is essential to the\nscalable deployment of an autonomous driving system. State-of-the-art methods\nin UDA often employ a key idea: utilizing joint supervision signals from both\nof the source domain (with ground-truth) and target domain (with pseudo-labels)\nfor self-training. In this work, we improve and extend on this aspect. We\npresent ConDA, a concatenation-based domain adaptation framework for LiDAR\nsegmentation that: 1) constructs an intermediate domain consisting of\nfine-grained interchange signals from both source and target domains without\ndestabilizing the semantic coherency of objects and background around the\nego-vehicle; and 2) utilizes the intermediate domain for self-training. To\nimprove both the network training on the source domain and self-training on the\nintermediate domain, we propose an anti-aliasing regularizer and an entropy\naggregator to reduce the negative effect caused by the aliasing artifacts and\nnoisy pseudo labels. Through extensive experiments, we demonstrate that ConDA\nsignificantly outperforms prior arts in mitigating the domain gap not only in\nUDA, but also in other DAs with minimum supervisions, such as\nsemi-/weakly-supervised DAs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingdong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quader_N/0/1/0/all/0/1\">Niamul Quader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liong_V/0/1/0/all/0/1\">Venice Erin Liong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Transferability of Supervised Pretraining: an MLP Perspective. (arXiv:2112.00496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00496","description":"<p>The pretrain-finetune paradigm is a classical pipeline in visual learning.\nRecent progress on unsupervised pretraining methods shows superior transfer\nperformance to their supervised counterparts. This paper revisits this\nphenomenon and sheds new light on understanding the transferability gap between\nunsupervised and supervised pretraining from a multilayer perceptron (MLP)\nperspective. While previous works focus on the effectiveness of MLP on\nunsupervised image classification where pretraining and evaluation are\nconducted on the same dataset, we reveal that the MLP projector is also the key\nfactor to better transferability of unsupervised pretraining methods than\nsupervised pretraining methods. Based on this observation, we attempt to close\nthe transferability gap between supervised and unsupervised pretraining by\nadding an MLP projector before the classifier in supervised pretraining. Our\nanalysis indicates that the MLP projector can help retain intra-class variation\nof visual features, decrease the feature distribution distance between\npretraining and evaluation datasets, and reduce feature redundancy. Extensive\nexperiments on public benchmarks demonstrate that the added MLP projector\nsignificantly boosts the transferability of supervised pretraining, e.g. +7.2\\%\ntop-1 accuracy on the concept generalization task, +5.8\\% top-1 accuracy for\nlinear evaluation on 12-domain classification tasks, and +0.8\\% AP on COCO\nobject detection task, making supervised pretraining comparable or even better\nthan unsupervised pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_D/0/1/0/all/0/1\">Donglian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extrapolating from a Single Image to a Thousand Classes using Distillation. (arXiv:2112.00725v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00725","description":"<p>What can neural networks learn about the visual world from a single image?\nWhile it obviously cannot contain the multitudes of possible objects, scenes\nand lighting conditions that exist - within the space of all possible\n256^(3x224x224) 224-sized square images, it might still provide a strong prior\nfor natural images. To analyze this hypothesis, we develop a framework for\ntraining neural networks from scratch using a single image by means of\nknowledge distillation from a supervisedly pretrained teacher. With this, we\nfind that the answer to the above question is: 'surprisingly, a lot'. In\nquantitative terms, we find top-1 accuracies of 94%/74% on CIFAR-10/100, 66% on\nImageNet, and by extending this method to video and audio, 77% on UCF-101 and\n84% on SpeechCommands. In extensive analyses we disentangle the effect of\naugmentations, choice of source image and network architectures and also\ndiscover \"panda neurons\" in networks that have never seen a panda. This work\nshows that one image can be used to extrapolate to thousands of object classes\nand motivates a renewed research agenda on the fundamental interplay of\naugmentations and image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Aaqib Saeed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Neural Representations for Direct Volume Rendering. (arXiv:2112.01579v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2112.01579","description":"<p>Despite the potential of neural scene representations to effectively compress\n3D scalar fields at high reconstruction quality, the computational complexity\nof the training and data reconstruction step using scene representation\nnetworks limits their use in practical applications. In this paper, we analyze\nwhether scene representation networks can be modified to reduce these\nlimitations and whether such architectures can also be used for temporal\nreconstruction tasks. We propose a novel design of scene representation\nnetworks using GPU tensor cores to integrate the reconstruction seamlessly into\non-chip raytracing kernels, and compare the quality and performance of this\nnetwork to alternative network- and non-network-based compression schemes. The\nresults indicate competitive quality of our design at high compression rates,\nand significantly faster decoding times and lower memory consumption during\ndata reconstruction. We investigate how density gradients can be computed using\nthe network and show an extension where density, gradient and curvature are\npredicted jointly. As an alternative to spatial super-resolution approaches for\ntime-varying fields, we propose a solution that builds upon latent-space\ninterpolation to enable random access reconstruction at arbitrary granularity.\nWe summarize our findings in the form of an assessment of the strengths and\nlimitations of scene representation networks \\changed{for compression domain\nvolume rendering, and outline future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_S/0/1/0/all/0/1\">Sebastian Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermuller_P/0/1/0/all/0/1\">Philipp Herm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westermann_R/0/1/0/all/0/1\">R&#xfc;diger Westermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Data Augmentation Using Feature Interpolation for Low-Shot Image Generation. (arXiv:2112.02450v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02450","description":"<p>Training of generative models especially Generative Adversarial Networks can\neasily diverge in low-data setting. To mitigate this issue, we propose a novel\nimplicit data augmentation approach which facilitates stable training and\nsynthesize high-quality samples without need of label information.\nSpecifically, we view the discriminator as a metric embedding of the real data\nmanifold, which offers proper distances between real data points. We then\nutilize information in the feature space to develop a data-driven augmentation\nmethod. Experiments on few-shot generation tasks show the proposed method\nsignificantly improve results from strong baselines, and allows generating\nhigh-quality images with around 100 training samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Mengyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1\">Haibin Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoyang Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification-Then-Grounding: Reformulating Video Scene Graphs as Temporal Bipartite Graphs. (arXiv:2112.04222v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04222","description":"<p>Today's VidSGG models are all proposal-based methods, i.e., they first\ngenerate numerous paired subject-object snippets as proposals, and then conduct\npredicate classification for each proposal. In this paper, we argue that this\nprevalent proposal-based framework has three inherent drawbacks: 1) The\nground-truth predicate labels for proposals are partially correct. 2) They\nbreak the high-order relations among different predicate instances of a same\nsubject-object pair. 3) VidSGG performance is upper-bounded by the quality of\nthe proposals. To this end, we propose a new classification-then-grounding\nframework for VidSGG, which can avoid all the three overlooked drawbacks.\nMeanwhile, under this framework, we reformulate the video scene graphs as\ntemporal bipartite graphs, where the entities and predicates are two types of\nnodes with time slots, and the edges denote different semantic roles between\nthese nodes. This formulation takes full advantage of our new framework.\nAccordingly, we further propose a novel BIpartite Graph based SGG model: BIG.\nIt consists of a classification stage and a grounding stage, where the former\naims to classify the categories of all the nodes and the edges, and the latter\ntries to localize the temporal location of each relation instance. Extensive\nablations on two VidSGG datasets have attested to the effectiveness of our\nframework and BIG. Code is available at https://github.com/Dawn-LX/VidSGG-BIG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kaifeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceFormer: Speech-Driven 3D Facial Animation with Transformers. (arXiv:2112.05329v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05329","description":"<p>Speech-driven 3D facial animation is challenging due to the complex geometry\nof human faces and the limited availability of 3D audio-visual data. Prior\nworks typically focus on learning phoneme-level features of short audio windows\nwith limited context, occasionally resulting in inaccurate lip movements. To\ntackle this limitation, we propose a Transformer-based autoregressive model,\nFaceFormer, which encodes the long-term audio context and autoregressively\npredicts a sequence of animated 3D face meshes. To cope with the data scarcity\nissue, we integrate the self-supervised pre-trained speech representations.\nAlso, we devise two biased attention mechanisms well suited to this specific\ntask, including the biased cross-modal multi-head (MH) attention and the biased\ncausal MH self-attention with a periodic positional encoding strategy. The\nformer effectively aligns the audio-motion modalities, whereas the latter\noffers abilities to generalize to longer audio sequences. Extensive experiments\nand a perceptual user study show that our approach outperforms the existing\nstate-of-the-arts. The code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yingruo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Overlooked Classifier in Human-Object Interaction Recognition. (arXiv:2112.06392v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06392","description":"<p>Human-Object Interaction (HOI) recognition is challenging due to two factors:\n(1) significant imbalance across classes and (2) requiring multiple labels per\nimage. This paper shows that these two challenges can be effectively addressed\nby improving the classifier with the backbone architecture untouched. Firstly,\nwe encode the semantic correlation among classes into the classification head\nby initializing the weights with language embeddings of HOIs. As a result, the\nperformance is boosted significantly, especially for the few-shot subset.\nSecondly, we propose a new loss named LSE-Sign to enhance multi-label learning\non a long-tailed dataset. Our simple yet effective method enables\ndetection-free HOI classification, outperforming the state-of-the-arts that\nrequire object detection and human pose by a clear margin. Moreover, we\ntransfer the classification model to instance-level HOI detection by connecting\nit with an off-the-shelf object detector. We achieve state-of-the-art without\nadditional fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena. (arXiv:2112.07566v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07566","description":"<p>We propose VALSE (Vision And Language Structured Evaluation), a novel\nbenchmark designed for testing general-purpose pretrained vision and language\n(V&amp;L) models for their visio-linguistic grounding capabilities on specific\nlinguistic phenomena. VALSE offers a suite of six tests covering various\nlinguistic constructs. Solving these requires models to ground linguistic\nphenomena in the visual modality, allowing more fine-grained evaluations than\nhitherto possible. We build VALSE using methods that support the construction\nof valid foils, and report results from evaluating five widely-used V&amp;L models.\nOur experiments suggest that current models have considerable difficulty\naddressing most phenomena. Hence, we expect VALSE to serve as an important\nbenchmark to measure future progress of pretrained V&amp;L models from a linguistic\nperspective, complementing the canonical task-centred V&amp;L evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parcalabescu_L/0/1/0/all/0/1\">Letitia Parcalabescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muradjan_L/0/1/0/all/0/1\">Lilitta Muradjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calixto_I/0/1/0/all/0/1\">Iacer Calixto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of class imbalance on chest x-ray classifiers: towards better evaluation practices for discrimination and calibration performance. (arXiv:2112.12843v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12843","description":"<p>This work aims to analyze standard evaluation practices adopted by the\nresearch community when assessing chest x-ray classifiers, particularly\nfocusing on the impact of class imbalance in such appraisals. Our analysis\nconsiders a comprehensive definition of model performance, covering not only\ndiscriminative performance but also model calibration, a topic of research that\nhas received increasing attention during the last years within the machine\nlearning community. Firstly, we conducted a literature study to analyze common\nscientific practices and confirmed that: (1) even when dealing with highly\nimbalanced datasets, the community tends to use metrics that are dominated by\nthe majority class; and (2) it is still uncommon to include calibration studies\nfor chest x-ray classifiers, albeit its importance in the context of\nhealthcare. Secondly, we perform a systematic experiment on two major chest\nx-ray datasets to explore the behavior of several performance metrics under\ndifferent class ratios and show that widely adopted metrics can conceal the\nperformance in the minority class. Finally, we recommend the inclusion of\ncomplementary metrics to better reflect the system's performance in such\nscenarios. Our study indicates that current evaluation practices adopted by the\nresearch community for chest x-ray computer-aided diagnosis systems may not\nreflect their performance in real clinical scenarios, and suggest alternatives\nto improve this situation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosquera_C/0/1/0/all/0/1\">Candelaria Mosquera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1\">Luciana Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milone_D/0/1/0/all/0/1\">Diego Milone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luna_D/0/1/0/all/0/1\">Daniel Luna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRIME: A few primitives can boost robustness to common corruptions. (arXiv:2112.13547v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13547","description":"<p>Despite their impressive performance on image classification tasks, deep\nnetworks have a hard time generalizing to unforeseen corruptions of their data.\nTo fix this vulnerability, prior works have built complex data augmentation\nstrategies, combining multiple methods to enrich the training data. However,\nintroducing intricate design choices or heuristics makes it hard to understand\nwhich elements of these methods are indeed crucial for improving robustness. In\nthis work, we take a step back and follow a principled approach to achieve\nrobustness to common corruptions. We propose PRIME, a general data augmentation\nscheme that relies on simple yet rich families of max-entropy image\ntransformations. PRIME outperforms the prior art in terms of corruption\nrobustness, while its simplicity and plug-and-play nature enable combination\nwith other methods to further boost their robustness. We analyze PRIME to shed\nlight on the importance of the mixing strategy on synthesizing corrupted\nimages, and to reveal the robustness-accuracy trade-offs arising in the context\nof common corruptions. Finally, we show that the computational efficiency of\nour method allows it to be easily used in both on-line and off-line data\naugmentation schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1\">Apostolos Modas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1\">Rahul Rade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_Jimenez_G/0/1/0/all/0/1\">Guillermo Ortiz-Jim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Fine-grained Class Clustering via Generative Adversarial Networks. (arXiv:2112.14971v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14971","description":"<p>Unsupervised fine-grained class clustering is a practical yet challenging\ntask due to the difficulty of feature representations learning of subtle object\ndetails. We introduce C3-GAN, a method that leverages the categorical inference\npower of InfoGAN with contrastive learning. We aim to learn feature\nrepresentations that encourage a dataset to form distinct cluster boundaries in\nthe embedding space, while also maximizing the mutual information between the\nlatent code and its image observation. Our approach is to train a\ndiscriminator, which is also used for inferring clusters, to optimize the\ncontrastive loss, where image-latent pairs that maximize the mutual information\nare considered as positive pairs and the rest as negative pairs. Specifically,\nwe map the input of a generator, which was sampled from the categorical\ndistribution, to the embedding space of the discriminator and let them act as a\ncluster centroid. In this way, C3-GAN succeeded in learning a\nclustering-friendly embedding space where each cluster is distinctively\nseparable. Experimental results show that C3-GAN achieved the state-of-the-art\nclustering performance on four fine-grained image datasets, while also\nalleviating the mode collapse phenomenon. Code is available at\nhttps://github.com/naver-ai/c3-gan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yunji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language as Queries for Referring Video Object Segmentation. (arXiv:2201.00487v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00487","description":"<p>Referring video object segmentation (R-VOS) is an emerging cross-modal task\nthat aims to segment the target object referred by a language expression in all\nvideo frames. In this work, we propose a simple and unified framework built\nupon Transformer, termed ReferFormer. It views the language as queries and\ndirectly attends to the most relevant regions in the video frames. Concretely,\nwe introduce a small set of object queries conditioned on the language as the\ninput to the Transformer. In this manner, all the queries are obligated to find\nthe referred objects only. They are eventually transformed into dynamic kernels\nwhich capture the crucial object-level information, and play the role of\nconvolution filters to generate the segmentation masks from feature maps. The\nobject tracking is achieved naturally by linking the corresponding queries\nacross frames. This mechanism greatly simplifies the pipeline and the\nend-to-end framework is significantly different from the previous methods.\nExtensive experiments on Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and\nJHMDB-Sentences show the effectiveness of ReferFormer. On Ref-Youtube-VOS,\nRefer-Former achieves 55.6J&amp;F with a ResNet-50 backbone without bells and\nwhistles, which exceeds the previous state-of-the-art performance by 8.4\npoints. In addition, with the strong Swin-Large backbone, ReferFormer achieves\nthe best J&amp;F of 64.2 among all existing methods. Moreover, we show the\nimpressive results of 55.0 mAP and 43.7 mAP on A2D-Sentences andJHMDB-Sentences\nrespectively, which significantly outperforms the previous methods by a large\nmargin. Code is publicly available at https://github.com/wjn922/ReferFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiannan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peize Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enabling Verification of Deep Neural Networks in Perception Tasks Using Fuzzy Logic and Concept Embeddings. (arXiv:2201.00572v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00572","description":"<p>One major drawback of deep convolutional neural networks (CNNs) for use in\nsafety critical applications is their black-box nature. This makes it hard to\nverify or monitor complex, symbolic requirements on already trained computer\nvision CNNs. In this work, we present a simple, yet effective, approach to\nverify that a CNN complies with symbolic predicate logic rules which relate\nvisual concepts. It is the first that (1) does not modify the CNN, (2) may use\nvisual concepts that are no CNN in- or output feature, and (3) can leverage\ncontinuous CNN confidence outputs. To achieve this, we newly combine methods\nfrom explainable artificial intelligence and logic: First, using supervised\nconcept embedding analysis, the output of a CNN is post-hoc enriched by concept\noutputs. Second, rules from prior knowledge are modelled as truth functions\nthat accept the CNN outputs, and can be evaluated with little computational\noverhead. We here investigate the use of fuzzy logic, i.e., continuous truth\nvalues, and of proper output calibration, which both theoretically and\npractically show slight benefits. Applicability is demonstrated on\nstate-of-the-art object detectors for three verification use-cases, where\nmonitoring of rule breaches can reveal detection errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwalbe_G/0/1/0/all/0/1\">Gesina Schwalbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirth_C/0/1/0/all/0/1\">Christian Wirth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_U/0/1/0/all/0/1\">Ute Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Autoencoder for Point Cloud Self-supervised Representation Learning. (arXiv:2201.00785v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00785","description":"<p>Many 3D representations (e.g., point clouds) are discrete samples of the\nunderlying continuous 3D surface. This process inevitably introduces sampling\nvariations on the underlying 3D shapes. In learning 3D representation, the\nvariations should be disregarded while transferable knowledge of the underlying\n3D shape should be captured. This poses a grand challenge to existing\nrepresentation learning paradigms. For example, the standard autoencoding\nparadigm forces the encoder to capture such sampling variations as the decoder\nhas to reconstruct the original point cloud. We introduce Implicit\nAutoencoder(IAE), a simple yet effective method that addresses this challenge\nby replacing the point cloud decoder with an implicit decoder. The implicit\ndecoder outputs a continuous representation that is shared among different\npoint cloud samplings of the same model. Reconstructing under the implicit\nrepresentation can prioritize that the encoder discards sampling variations,\nintroducing more space to learn useful features. We theoretically justify this\nclaim under a simple linear autoencoder. Moreover, our implicit decoder offers\na rich space to design suitable implicit representations for different tasks.\nWe demonstrate the usefulness of IAE across various self-supervised learning\ntasks for both 3D objects and 3D scenes. Experimental results show that IAE\nconsistently outperforms the state-of-the-art in each task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Siming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenpei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1\">Li Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction. (arXiv:2201.02184v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2201.02184","description":"<p>Video recordings of speech contain correlated audio and visual information,\nproviding a strong signal for speech representation learning from the speaker's\nlip movements and the produced sound. We introduce Audio-Visual Hidden Unit\nBERT (AV-HuBERT), a self-supervised representation learning framework for\naudio-visual speech, which masks multi-stream video input and predicts\nautomatically discovered and iteratively refined multimodal hidden units.\nAV-HuBERT learns powerful audio-visual speech representation benefiting both\nlip-reading and automatic speech recognition. On the largest public lip-reading\nbenchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of\nlabeled data, outperforming the former state-of-the-art approach (33.6%)\ntrained with a thousand times more transcribed video data (31K hours). The\nlip-reading WER is further reduced to 26.9% when using all 433 hours of labeled\ndata from LRS3 and combined with self-training. Using our audio-visual\nrepresentation on the same benchmark for audio-only speech recognition leads to\na 40% relative WER reduction over the state-of-the-art performance (1.3% vs\n2.3%). Our code and models are available at\nhttps://github.com/facebookresearch/av_hubert\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02639","description":"<p>As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time -- through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n</p>\n<p>Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining -- even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n</p>\n<p>We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-biased image classification: evaluation based on semantic representations. (arXiv:2201.11014v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11014","description":"<p>Humans show language-biased image recognition for a word-embedded image,\nknown as picture-word interference. Such interference depends on hierarchical\nsemantic categories and reflects that human language processing highly\ninteracts with visual processing. Similar to humans, recent artificial models\njointly trained on texts and images, e.g., OpenAI CLIP, show language-biased\nimage classification. Exploring whether the bias leads to interference similar\nto those observed in humans can contribute to understanding how much the model\nacquires hierarchical semantic representations from joint learning of language\nand vision. The present study introduces methodological tools from the\ncognitive science literature to assess the biases of artificial models.\nSpecifically, we introduce a benchmark task to test whether words superimposed\non images can distort the image classification across different category levels\nand, if it can, whether the perturbation is due to the shared semantic\nrepresentation between language and vision. Our dataset is a set of\nword-embedded images and consists of a mixture of natural image datasets and\nhierarchical word labels with superordinate/basic category levels. Using this\nbenchmark test, we evaluate the CLIP model. We show that presenting words\ndistorts the image classification by the model across different category\nlevels, but the effect does not depend on the semantic relationship between\nimages and embedded words. This suggests that the semantic word representation\nin the CLIP visual processing is not shared with the image representation,\nalthough the word representation strongly dominates for word-embedded images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemesle_Y/0/1/0/all/0/1\">Yoann Lemesle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1\">Masataka Sawayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_Perez_G/0/1/0/all/0/1\">Guillermo Valle-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adolphe_M/0/1/0/all/0/1\">Maxime Adolphe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauzeon_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (arXiv:2201.11528v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11528","description":"<p>Adversarial examples have posed a severe threat to deep neural networks due\nto their transferable nature. Currently, various works have paid great efforts\nto enhance the cross-model transferability, which mostly assume the substitute\nmodel is trained in the same domain as the target model. However, in reality,\nthe relevant information of the deployed model is unlikely to leak. Hence, it\nis vital to build a more practical black-box threat model to overcome this\nlimitation and evaluate the vulnerability of deployed models. In this paper,\nwith only the knowledge of the ImageNet domain, we propose a Beyond ImageNet\nAttack (BIA) to investigate the transferability towards black-box domains\n(unknown classification tasks). Specifically, we leverage a generative model to\nlearn the adversarial function for disrupting low-level features of input\nimages. Based on this framework, we further propose two variants to narrow the\ngap between the source and target domains from the data and model perspectives,\nrespectively. Extensive experiments on coarse-grained and fine-grained domains\ndemonstrate the effectiveness of our proposed methods. Notably, our methods\noutperform state-of-the-art approaches by up to 7.71\\% (towards coarse-grained\ndomains) and 25.91\\% (towards fine-grained domains) on average. Our code is\navailable at \\url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAP: An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks. (arXiv:2201.11808v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11808","description":"<p>Despite the state-of-the-art performance of deep convolutional neural\nnetworks, they are susceptible to bias and malfunction in unseen situations.\nThe complex computation behind their reasoning is not sufficiently\nhuman-understandable to develop trust. External explainer methods have tried to\ninterpret the network decisions in a human-understandable way, but they are\naccused of fallacies due to their assumptions and simplifications. On the other\nside, the inherent self-interpretability of models, while being more robust to\nthe mentioned fallacies, cannot be applied to the already trained models. In\nthis work, we propose a new attention-based pooling layer, called Local\nAttention Pooling (LAP), that accomplishes self-interpretability and the\npossibility for knowledge injection while improving the model's performance.\nMoreover, several weakly-supervised knowledge injection methodologies are\nprovided to enhance the process of training. We verified our claims by\nevaluating several LAP-extended models on three different datasets, including\nImagenet. The proposed framework offers more valid human-understandable and\nmore faithful-to-the-model interpretations than the commonly used white-box\nexplainer methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modegh_R/0/1/0/all/0/1\">Rassa Ghavami Modegh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_A/0/1/0/all/0/1\">Ahmad Salimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabiee_H/0/1/0/all/0/1\">Hamid R. Rabiee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Servoing for Pose Control of Soft Continuum Arm in a Structured Environment. (arXiv:2202.05200v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.05200","description":"<p>For soft continuum arms, visual servoing is a popular control strategy that\nrelies on visual feedback to close the control loop. However, robust visual\nservoing is challenging as it requires reliable feature extraction from the\nimage, accurate control models and sensors to perceive the shape of the arm,\nboth of which can be hard to implement in a soft robot. This letter circumvents\nthese challenges by presenting a deep neural network-based method to perform\nsmooth and robust 3D positioning tasks on a soft arm by visual servoing using a\ncamera mounted at the distal end of the arm. A convolutional neural network is\ntrained to predict the actuations required to achieve the desired pose in a\nstructured environment. Integrated and modular approaches for estimating the\nactuations from the image are proposed and are experimentally compared. A\nproportional control law is implemented to reduce the error between the desired\nand current image as seen by the camera. The model together with the\nproportional feedback control makes the described approach robust to several\nvariations such as new targets, lighting, loads, and diminution of the soft\narm. Furthermore, the model lends itself to be transferred to a new environment\nwith minimal effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamtikar_S/0/1/0/all/0/1\">Shivani Kamtikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marri_S/0/1/0/all/0/1\">Samhita Marri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walt_B/0/1/0/all/0/1\">Benjamin Walt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uppalapati_N/0/1/0/all/0/1\">Naveen Kumar Uppalapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_G/0/1/0/all/0/1\">Girish Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entroformer: A Transformer-based Entropy Model for Learned Image Compression. (arXiv:2202.05492v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.05492","description":"<p>One critical component in lossy deep image compression is the entropy model,\nwhich predicts the probability distribution of the quantized latent\nrepresentation in the encoding and decoding modules. Previous works build\nentropy models upon convolutional neural networks which are inefficient in\ncapturing global dependencies. In this work, we propose a novel\ntransformer-based entropy model, termed Entroformer, to capture long-range\ndependencies in probability distribution estimation effectively and\nefficiently. Different from vision transformers in image classification, the\nEntroformer is highly optimized for image compression, including a top-k\nself-attention and a diamond relative position encoding. Meanwhile, we further\nexpand this architecture with a parallel bidirectional context model to speed\nup the decoding process. The experiments show that the Entroformer achieves\nstate-of-the-art performance on image compression while being time-efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yichen Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Transformer for Fast and Robust Point Cloud Registration. (arXiv:2202.06688v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06688","description":"<p>We study the problem of extracting accurate correspondences for point cloud\nregistration. Recent keypoint-free methods bypass the detection of repeatable\nkeypoints which is difficult in low-overlap scenarios, showing great potential\nin registration. They seek correspondences over downsampled superpoints, which\nare then propagated to dense points. Superpoints are matched based on whether\ntheir neighboring patches overlap. Such sparse and loose matching requires\ncontextual features capturing the geometric structure of the point clouds. We\npropose Geometric Transformer to learn geometric feature for robust superpoint\nmatching. It encodes pair-wise distances and triplet-wise angles, making it\nrobust in low-overlap cases and invariant to rigid transformation. The\nsimplistic design attains surprisingly high matching accuracy such that no\nRANSAC is required in the estimation of alignment transformation, leading to\n$100$ times acceleration. Our method improves the inlier ratio by $17{\\sim}30$\npercentage points and the registration recall by over $7$ points on the\nchallenging 3DLoMatch benchmark. Our code and models are available at\n\\url{https://github.com/qinzheng93/GeoTransformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zheng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changjian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuxing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification. (arXiv:2202.07570v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07570","description":"<p>Progress in digital pathology is hindered by high-resolution images and the\nprohibitive cost of exhaustive localized annotations. The commonly used\nparadigm to categorize pathology images is patch-based processing, which often\nincorporates multiple instance learning (MIL) to aggregate local patch-level\nrepresentations yielding image-level prediction. Nonetheless, diagnostically\nrelevant regions may only take a small fraction of the whole tissue, and\ncurrent MIL-based approaches often process images uniformly, discarding the\ninter-patches interactions. To alleviate these issues, we propose ScoreNet, a\nnew efficient transformer that exploits a differentiable recommendation stage\nto extract discriminative image regions and dedicate computational resources\naccordingly. The proposed transformer leverages the local and global attention\nof a few dynamically recommended high-resolution regions at an efficient\ncomputational cost. We further introduce a novel Mixup-based data-augmentation,\nnamely ScoreMix, by leveraging the image's semantic distribution to guide the\ndata mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly\nsimple and mitigates the pitfalls of previous augmentations, which assume a\nuniform semantic distribution and risk mislabeling the samples. Thorough\nexperiments and ablation studies on three breast cancer histology datasets of\nHaematoxylin &amp; Eosin (H&amp;E) have validated the superiority of our approach over\nprior arts, including transformer-based models on tumour regions-of-interest\n(TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation\ndemonstrates better generalization capabilities and achieves new\nstate-of-the-art (SOTA) results with only 50% of the data compared to other\nMixup augmentation variants. Finally, ScoreNet yields high efficacy and\noutperforms SOTA efficient transformers, namely TransPath and SwinTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stegmuller_T/0/1/0/all/0/1\">Thomas Stegm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1\">Behzad Bozorgtabar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spahr_A/0/1/0/all/0/1\">Antoine Spahr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1\">Jean-Philippe Thiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Movies2Scenes: Learning Scene Representations Using Movie Similarities. (arXiv:2202.10650v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10650","description":"<p>Labeling movie-scenes is a time-consuming process which makes applying\nend-to-end supervised methods for scene-understanding a challenging problem.\nMoreover, directly using image-based visual representations for\nscene-understanding tasks does not prove to be effective given the large gap\nbetween the two domains. To address these challenges, we propose a novel\ncontrastive learning approach that uses commonly available movie-level\ninformation (e.g., co-watch, genre, synopsis) to learn a general-purpose\nscene-level representation. Our learned representation comfortably outperforms\nexisting state-of-the-art approaches on eleven downstream tasks evaluated using\nmultiple benchmark datasets. To further demonstrate generalizability of our\nlearned representation, we present its comparative results on a set of\nvideo-moderation tasks evaluated using a newly collected large-scale internal\nmovie dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shixing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xiaohan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamid_R/0/1/0/all/0/1\">Raffay Hamid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Network Architecture Attribution. (arXiv:2202.13843v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13843","description":"<p>With the rapid progress of generation technology, it has become necessary to\nattribute the origin of fake images. Existing works on fake image attribution\nperform multi-class classification on several Generative Adversarial Network\n(GAN) models and obtain high accuracies. While encouraging, these works are\nrestricted to model-level attribution, only capable of handling images\ngenerated by seen models with a specific seed, loss and dataset, which is\nlimited in real-world scenarios when fake images may be generated by privately\ntrained models. This motivates us to ask whether it is possible to attribute\nfake images to the source models' architectures even if they are finetuned or\nretrained under different configurations. In this work, we present the first\nstudy on Deepfake Network Architecture Attribution to attribute fake images on\narchitecture-level. Based on an observation that GAN architecture is likely to\nleave globally consistent fingerprints while traces left by model weights vary\nin different regions, we provide a simple yet effective solution named DNA-Det\nfor this problem. Extensive experiments on multiple cross-test setups and a\nlarge-scale dataset demonstrate the effectiveness of DNA-Det.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectiveness of Delivered Information Trade Study. (arXiv:2203.00116v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00116","description":"<p>The sensor to shooter timeline is affected by two main variables: satellite\npositioning and asset positioning. Speeding up satellite positioning by adding\nmore sensors or by decreasing processing time is important only if there is a\nprepared shooter, otherwise the main source of time is getting the shooter into\nposition. However, the intelligence community should work towards the\nexploitation of sensors to the highest speed and effectiveness possible.\nAchieving a high effectiveness while keeping speed high is a tradeoff that must\nbe considered in the sensor to shooter timeline. In this paper we investigate\ntwo main ideas, increasing the effectiveness of satellite imagery through image\nmanipulation and how on-board image manipulation would affect the sensor to\nshooter timeline. We cover these ideas in four scenarios: Discrete Event\nSimulation of onboard processing versus ground station processing, quality of\ninformation with cloud cover removal, information improvement with super\nresolution, and data reduction with image to caption. This paper will show how\nimage manipulation techniques such as Super Resolution, Cloud Removal, and\nImage to Caption will improve the quality of delivered information in addition\nto showing how those processes effect the sensor to shooter timeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciolino_M/0/1/0/all/0/1\">Matthew Ciolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hambrick_D/0/1/0/all/0/1\">Dominick Hambrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision. (arXiv:2203.01475v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01475","description":"<p>Curating a large set of fully annotated training data can be costly,\nespecially for the tasks of medical image segmentation. Scribble, a weaker form\nof annotation, is more obtainable in practice, but training segmentation models\nfrom limited supervision of scribbles is still challenging. To address the\ndifficulties, we propose a new framework for scribble learning-based medical\nimage segmentation, which is composed of mix augmentation and cycle consistency\nand thus is referred to as CycleMix. For augmentation of supervision, CycleMix\nadopts the mixup strategy with a dedicated design of random occlusion, to\nperform increments and decrements of scribbles. For regularization of\nsupervision, CycleMix intensifies the training objective with consistency\nlosses to penalize inconsistent segmentation, which results in significant\nimprovement of segmentation performance. Results on two open datasets, i.e.,\nACDC and MSCMRseg, showed that the proposed method achieved exhilarating\nperformance, demonstrating comparable or even better accuracy than the\nfully-supervised methods. The code and expert-made scribble annotations for\nMSCMRseg are publicly available at https://github.com/BWGZK/CycleMix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection. (arXiv:2203.01562v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01562","description":"<p>Face Presentation Attack Detection (PAD) is an important measure to prevent\nspoof attacks for face biometric systems. Many works based on Convolution\nNeural Networks (CNNs) for face PAD formulate the problem as an image-level\nbinary classification task without considering the context. Alternatively,\nVision Transformers (ViT) using self-attention to attend the context of an\nimage become the mainstreams in face PAD. Inspired by ViT, we propose a\nVideo-based Transformer for face PAD (ViTransPAD) with short/long-range\nspatio-temporal attention which can not only focus on local details with short\nattention within a frame but also capture long-range dependencies over frames.\nInstead of using coarse image patches with single-scale as in ViT, we propose\nthe Multi-scale Multi-Head Self-Attention (MsMHSA) architecture to accommodate\nmulti-scale patch partitions of Q, K, V feature maps to the heads of\ntransformer in a coarse-to-fine manner, which enables to learn a fine-grained\nrepresentation to perform pixel-level discrimination for face PAD. Due to lack\ninductive biases of convolutions in pure transformers, we also introduce\nconvolutions to the proposed ViTransPAD to integrate the desirable properties\nof CNNs by using convolution patch embedding and convolution projection. The\nextensive experiments show the effectiveness of our proposed ViTransPAD with a\npreferable accuracy-computation balance, which can serve as a new backbone for\nface PAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zuheng Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ghadi_M/0/1/0/all/0/1\">Musab Al-Ghadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visani_M/0/1/0/all/0/1\">Muriel Visani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MuzzamilLuqman_M/0/1/0/all/0/1\">Muhammad MuzzamilLuqman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burie_J/0/1/0/all/0/1\">Jean-Christophe Burie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Object Localization as Domain Adaption. (arXiv:2203.01714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01714","description":"<p>Weakly supervised object localization (WSOL) focuses on localizing objects\nonly with the supervision of image-level classification masks. Most previous\nWSOL methods follow the classification activation map (CAM) that localizes\nobjects based on the classification structure with the multi-instance learning\n(MIL) mechanism. However, the MIL mechanism makes CAM only activate\ndiscriminative object parts rather than the whole object, weakening its\nperformance for localizing objects. To avoid this problem, this work provides a\nnovel perspective that models WSOL as a domain adaption (DA) task, where the\nscore estimator trained on the source/image domain is tested on the\ntarget/pixel domain to locate objects. Under this perspective, a DA-WSOL\npipeline is designed to better engage DA approaches into WSOL to enhance\nlocalization performance. It utilizes a proposed target sampling strategy to\nselect different types of target samples. Based on these types of target\nsamples, domain adaption localization (DAL) loss is elaborated. It aligns the\nfeature distribution between the two domains by DA and makes the estimator\nperceive target domain cues by Universum regularization. Experiments show that\nour pipeline outperforms SOTA methods on multi benchmarks. Code are released at\n\\url{https://github.com/zh460045050/DA-WSOL_CVPR2022}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yunfei You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02925","description":"<p>Weakly supervised temporal action localization aims to localize temporal\nboundaries of actions and simultaneously identify their categories with only\nvideo-level category labels. Many existing methods seek to generate pseudo\nlabels for bridging the discrepancy between classification and localization,\nbut usually only make use of limited contextual information for pseudo label\ngeneration. To alleviate this problem, we propose a representative snippet\nsummarization and propagation framework. Our method seeks to mine the\nrepresentative snippets in each video for propagating information between video\nsnippets to generate better pseudo labels. For each video, its own\nrepresentative snippets and the representative snippets from a memory bank are\npropagated to update the input features in an intra- and inter-video manner.\nThe pseudo labels are generated from the temporal class activation maps of the\nupdated features to rectify the predictions of the main branch. Our method\nobtains superior performance in comparison to the existing methods on two\nbenchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in\nterms of average mAP on THUMOS14.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linjiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signature and Log-signature for the Study of Empirical Distributions Generated with GANs. (arXiv:2203.03226v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03226","description":"<p>In this paper, we develop a new and systematic method to explore and analyze\nsamples taken by NASA Perseverance on the surface of the planet Mars. A novel\nin this context PCA adaptive t-SNE is proposed, as well as the introduction of\nstatistical measures to study the goodness of fit of the sample distribution.\nWe go beyond visualization by generating synthetic imagery using Stylegan2-ADA\nthat resemble the original terrain distribution. We also conduct synthetic\nimage generation using the recently introduced Scored-based Generative\nModeling. We bring forward the use of the recently developed Signature\nTransform as a way to measure the similarity between image distributions and\nprovide detailed acquaintance and extensive evaluations. We are the first to\npioneer RMSE and MAE Signature and log-signature as an alternative to measure\nGAN convergence. Insights on state-of-the-art instance segmentation of the\nsamples by the use of a model DeepLabv3 are also given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Curto_J/0/1/0/all/0/1\">J. de Curt&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarza_I/0/1/0/all/0/1\">I. de Zarz&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels. (arXiv:2203.03884v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03884","description":"<p>The crux of semi-supervised semantic segmentation is to assign adequate\npseudo-labels to the pixels of unlabeled images. A common practice is to select\nthe highly confident predictions as the pseudo ground-truth, but it leads to a\nproblem that most pixels may be left unused due to their unreliability. We\nargue that every pixel matters to the model training, even its prediction is\nambiguous. Intuitively, an unreliable prediction may get confused among the top\nclasses (i.e., those with the highest probabilities), however, it should be\nconfident about the pixel not belonging to the remaining classes. Hence, such a\npixel can be convincingly treated as a negative sample to those most unlikely\ncategories. Based on this insight, we develop an effective pipeline to make\nsufficient use of unlabeled data. Concretely, we separate reliable and\nunreliable pixels via the entropy of predictions, push each unreliable pixel to\na category-wise queue that consists of negative samples, and manage to train\nthe model with all candidate pixels. Considering the training evolution, where\nthe prediction becomes more and more accurate, we adaptively adjust the\nthreshold for the reliable-unreliable partition. Experimental results on\nvarious benchmarks and training settings demonstrate the superiority of our\napproach over the state-of-the-art alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Jingjing Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1\">Guoqiang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_X/0/1/0/all/0/1\">Xinyi Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs. (arXiv:2203.04050v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04050","description":"<p>Semantic segmentation in bird's eye view (BEV) is an important task for\nautonomous driving. Though this task has attracted a large amount of research\nefforts, it is still challenging to flexibly cope with arbitrary (single or\nmultiple) camera sensors equipped on the autonomous vehicle. In this paper, we\npresent BEVSegFormer, an effective transformer-based method for BEV semantic\nsegmentation from arbitrary camera rigs. Specifically, our method first encodes\nimage features from arbitrary cameras with a shared backbone. These image\nfeatures are then enhanced by a deformable transformer-based encoder. Moreover,\nwe introduce a BEV transformer decoder module to parse BEV semantic\nsegmentation results. An efficient multi-camera deformable attention unit is\ndesigned to carry out the BEV-to-image view transformation. Finally, the\nqueries are reshaped according the layout of grids in the BEV, and upsampled to\nproduce the semantic segmentation result in a supervised manner. We evaluate\nthe proposed algorithm on the public nuScenes dataset and a self-collected\ndataset. Experimental results show that our method achieves promising\nperformance on BEV semantic segmentation from arbitrary camera rigs. We also\ndemonstrate the effectiveness of each component via ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhirong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhangjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Pengpeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1\">Erkang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment. (arXiv:2203.04121v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04121","description":"<p>Training a generative adversarial network (GAN) with limited data has been a\nchallenging task. A feasible solution is to start with a GAN well-trained on a\nlarge scale source domain and adapt it to the target domain with a few samples,\ntermed as few shot generative model adaption. However, existing methods are\nprone to model overfitting and collapse in extremely few shot setting (less\nthan 10). To solve this problem, we propose a relaxed spatial structural\nalignment method to calibrate the target generative models during the adaption.\nWe design a cross-domain spatial structural consistency loss comprising the\nself-correlation and disturbance correlation consistency loss. It helps align\nthe spatial structural information between the synthesis image pairs of the\nsource and target domains. To relax the cross-domain alignment, we compress the\noriginal latent space of generative models to a subspace. Image pairs generated\nfrom the subspace are pulled closer. Qualitative and quantitative experiments\nshow that our method consistently surpasses the state-of-the-art methods in few\nshot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiayu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaofei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Mosquito Habitat Detection Using Satellite Imagery and Convolutional Neural Networks for Disease Risk Mapping. (arXiv:2203.04463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04463","description":"<p>Mosquitoes are known vectors for disease transmission that cause over one\nmillion deaths globally each year. The majority of natural mosquito habitats\nare areas containing standing water that are challenging to detect using\nconventional ground-based technology on a macro scale. Contemporary approaches,\nsuch as drones, UAVs, and other aerial imaging technology are costly when\nimplemented and are only most accurate on a finer spatial scale whereas the\nproposed convolutional neural network(CNN) approach can be applied for disease\nrisk mapping and further guide preventative efforts on a more global scale. By\nassessing the performance of autonomous mosquito habitat detection technology,\nthe transmission of mosquito-borne diseases can be prevented in a\ncost-effective manner. This approach aims to identify the spatiotemporal\ndistribution of mosquito habitats in extensive areas that are difficult to\nsurvey using ground-based technology by employing computer vision on satellite\nimagery for proof of concept. The research presents an evaluation and the\nresults of 3 different CNN models to determine their accuracy of predicting\nlarge-scale mosquito habitats. For this approach, a dataset was constructed\ncontaining a variety of geographical features. Larger land cover variables such\nas ponds/lakes, inlets, and rivers were utilized to classify mosquito habitats\nwhile minute sites were omitted for higher accuracy on a larger scale. Using\nthe dataset, multiple CNN networks were trained and evaluated for accuracy of\nhabitat prediction. Utilizing a CNN-based approach on readily available\nsatellite imagery is cost-effective and scalable, unlike most aerial imaging\ntechnology. Testing revealed that YOLOv4 obtained greater accuracy in mosquito\nhabitat detection for identifying large-scale mosquito habitats.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elango_S/0/1/0/all/0/1\">Sriram Elango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandran_N/0/1/0/all/0/1\">Nandini Ramachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Low_R/0/1/0/all/0/1\">Russanne Low</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Combinatorial Brain Surgeon: Pruning Weights That Cancel One Another in Neural Networks. (arXiv:2203.04466v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.04466","description":"<p>Neural networks tend to achieve better accuracy with training if they are\nlarger -- even if the resulting models are overparameterized. Nevertheless,\ncarefully removing such excess parameters before, during, or after training may\nalso produce models with similar or even improved accuracy. In many cases, that\ncan be curiously achieved by heuristics as simple as removing a percentage of\nthe weights with the smallest absolute value -- even though magnitude is not a\nperfect proxy for weight relevance. With the premise that obtaining\nsignificantly better performance from pruning depends on accounting for the\ncombined effect of removing multiple weights, we revisit one of the classic\napproaches for impact-based pruning: the Optimal Brain Surgeon(OBS). We propose\na tractable heuristic for solving the combinatorial extension of OBS, in which\nwe select weights for simultaneous removal, as well as a systematic update of\nthe remaining weights. Our selection method outperforms other methods under\nhigh sparsity, and the weight update is advantageous even when combined with\nthe other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_T/0/1/0/all/0/1\">Thiago Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_S/0/1/0/all/0/1\">Shandian Zhe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need is LUV: Unsupervised Collection of Labeled Images using Invisible UV Fluorescent Indicators. (arXiv:2203.04566v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04566","description":"<p>Large-scale semantic image annotation is a significant challenge for\nlearning-based perception systems in robotics. Current approaches often rely on\nhuman labelers, which can be expensive, or simulation data, which can visually\nor physically differ from real data. This paper proposes Labels from\nUltraViolet (LUV), a novel framework that enables rapid, labeled data\ncollection in real manipulation environments without human labeling. LUV uses\ntransparent, ultraviolet-fluorescent paint with programmable ultraviolet LEDs\nto collect paired images of a scene in standard lighting and UV lighting to\nautonomously extract segmentation masks and keypoints via color segmentation.\nWe apply LUV to a suite of diverse robot perception tasks to evaluate its\nlabeling quality, flexibility, and data collection rate. Results suggest that\nLUV is 180-2500 times faster than a human labeler across the tasks. We show\nthat LUV provides labels consistent with human annotations on unpainted test\nimages. The networks trained on these labels are used to smooth and fold\ncrumpled towels with 83% success rate and achieve 1.7mm position error with\nrespect to human labels on a surgical needle pose estimation task. The low cost\nof LUV makes it ideal as a lightweight replacement for human labeling systems,\nwith the one-time setup costs at $300 equivalent to the cost of collecting\naround 200 semantic segmentation labels on Amazon Mechanical Turk. Code,\ndatasets, visualizations, and supplementary material can be found at\nhttps://sites.google.com/berkeley.edu/luv\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerr_J/0/1/0/all/0/1\">Justin Kerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1\">Ken Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04568","description":"<p>The success of Transformer in computer vision has attracted increasing\nattention in the medical imaging community. Especially for medical image\nsegmentation, many excellent hybrid architectures based on convolutional neural\nnetworks (CNNs) and Transformer have been presented and achieve impressive\nperformance. However, most of these methods, which embed modular Transformer\ninto CNNs, struggle to reach their full potential. In this paper, we propose a\nnovel hybrid architecture for medical image segmentation called PHTrans, which\nparallelly hybridizes Transformer and CNN in main building blocks to produce\nhierarchical representations from global and local features and adaptively\naggregate them, aiming to fully exploit their strengths to obtain better\nsegmentation performance. Specifically, PHTrans follows the U-shaped\nencoder-decoder design and introduces the parallel hybird module in deep\nstages, where convolution blocks and the modified 3D Swin Transformer learn\nlocal features and global dependencies separately, then a sequence-to-volume\noperation unifies the dimensions of the outputs to achieve feature aggregation.\nExtensive experimental results on both Multi-Atlas Labeling Beyond the Cranial\nVault and Automated Cardiac Diagnosis Challeng datasets corroborate its\neffectiveness, consistently outperforming state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_T/0/1/0/all/0/1\">Tong Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1\">Weijin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huihua Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification. (arXiv:2203.04614v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04614","description":"<p>A large-scale labeled dataset is a key factor for the success of supervised\ndeep learning in computer vision. However, a limited number of annotated data\nis very common, especially in ophthalmic image analysis, since manual\nannotation is time-consuming and labor-intensive. Self-supervised learning\n(SSL) methods bring huge opportunities for better utilizing unlabeled data, as\nthey do not need massive annotations. With an attempt to use as many as\npossible unlabeled ophthalmic images, it is necessary to break the dimension\nbarrier, simultaneously making use of both 2D and 3D images. In this paper, we\npropose a universal self-supervised Transformer framework, named Uni4Eye, to\ndiscover the inherent image property and capture domain-specific feature\nembedding in ophthalmic images. Uni4Eye can serve as a global feature\nextractor, which builds its basis on a Masked Image Modeling task with a Vision\nTransformer (ViT) architecture. We employ a Unified Patch Embedding module to\nreplace the origin patch embedding module in ViT for jointly processing both 2D\nand 3D input images. Besides, we design a dual-branch multitask decoder module\nto simultaneously perform two reconstruction tasks on the input image and its\ngradient map, delivering discriminative representations for better convergence.\nWe evaluate the performance of our pre-trained Uni4Eye encoder by fine-tuning\nit on six downstream ophthalmic image classification tasks. The superiority of\nUni4Eye is successfully established through comparisons to other\nstate-of-the-art SSL pre-training methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiyuan Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1\">Huaqing He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEU-Net: Ensemble Semantic Segmentation of Hyperspectral Images Using Clustering. (arXiv:2203.04873v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04873","description":"<p>Most semantic segmentation approaches of Hyperspectral images (HSIs) use and\nrequire preprocessing steps in the form of patching to accurately classify\ndiversified land cover in remotely sensed images. These approaches use patching\nto incorporate the rich neighborhood information in images and exploit the\nsimplicity and segmentability of the most common HSI datasets. In contrast,\nmost landmasses in the world consist of overlapping and diffused classes,\nmaking neighborhood information weaker than what is seen in common HSI\ndatasets. To combat this issue and generalize the segmentation models to more\ncomplex and diverse HSI datasets, in this work, we propose our novel flagship\nmodel: Clustering Ensemble U-Net (CEU-Net). CEU-Net uses the ensemble method to\ncombine spectral information extracted from convolutional neural network (CNN)\ntraining on a cluster of landscape pixels. Our CEU-Net model outperforms\nexisting state-of-the-art HSI semantic segmentation methods and gets\ncompetitive performance with and without patching when compared to baseline\nmodels. We highlight CEU-Net's high performance across Botswana, KSC, and\nSalinas datasets compared to HybridSN and AeroRIT methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soucy_N/0/1/0/all/0/1\">Nicholas Soucy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekeh_S/0/1/0/all/0/1\">Salimeh Yasaei Sekeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Map Learning for Vision and Language Navigation. (arXiv:2203.05137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05137","description":"<p>We consider the problem of Vision-and-Language Navigation (VLN). The majority\nof current methods for VLN are trained end-to-end using either unstructured\nmemory such as LSTM, or using cross-modal attention over the egocentric\nobservations of the agent. In contrast to other works, our key insight is that\nthe association between language and vision is stronger when it occurs in\nexplicit spatial representations. In this work, we propose a cross-modal map\nlearning model for vision-and-language navigation that first learns to predict\nthe top-down semantics on an egocentric map for both observed and unobserved\nregions, and then predicts a path towards the goal as a set of waypoints. In\nboth cases, the prediction is informed by the language through cross-modal\nattention mechanisms. We experimentally test the basic hypothesis that\nlanguage-driven navigation can be solved given a map, and then show competitive\nresults on the full VLN-CE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1\">Georgios Georgakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanchoo_K/0/1/0/all/0/1\">Karan Wanchoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Soham Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltsakaki_E/0/1/0/all/0/1\">Eleni Miltsakaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Background Matting Using Background Matching. (arXiv:2203.05193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05193","description":"<p>Due to the difficulty of solving the matting problem, lots of methods use\nsome kinds of assistance to acquire high quality alpha matte. Green screen\nmatting methods rely on physical equipment. Trimap-based methods take manual\ninteractions as external input. Background-based methods require a\npre-captured, static background. The methods are not flexible and convenient\nenough to use widely. Trimap-free methods are flexible but not stable in\ncomplicated video applications. To be stable and flexible in real applications,\nwe propose an adaptive background matting method. The user first captures their\nvideos freely, moving the cameras. Then the user captures the background video\nafterwards, roughly covering the previous captured regions. We use dynamic\nbackground video instead of static background for accurate matting. The\nproposed method is convenient to use in any scenes as the static camera and\nbackground is no more the limitation. To achieve this goal, we use background\nmatching network to find the best-matched background frame by frame from\ndynamic backgrounds. Then, robust semantic estimation network is used to\nestimate the coarse alpha matte. Finally, we crop and zoom the target region\naccording to the coarse alpha matte, and estimate the final accurate alpha\nmatte. In experiments, the proposed method is able to perform comparably\nagainst the state-of-the-art matting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinlin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-generative Generalized Zero-shot Learning via Task-correlated Disentanglement and Controllable Samples Synthesis. (arXiv:2203.05335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05335","description":"<p>Synthesizing pseudo samples is currently the most effective way to solve the\nGeneralized Zero Shot Learning (GZSL) problem. Most models achieve competitive\nperformance but still suffer from two problems: (1) Feature confounding, the\noverall representations confound task-correlated and task-independent features,\nand existing models disentangle them in a generative way, but they are\nunreasonable to synthesize reliable pseudo samples with limited samples; (2)\nDistribution uncertainty, that massive data is needed when existing models\nsynthesize samples from the uncertain distribution, which causes poor\nperformance in limited samples of seen classes. In this paper, we propose a\nnon-generative model to address these problems correspondingly in two modules:\n(1) Task-correlated feature disentanglement, to exclude the task-correlated\nfeatures from task-independent ones by adversarial learning of domain adaption\ntowards reasonable synthesis; (2) Controllable pseudo sample synthesis, to\nsynthesize edge-pseudo and center-pseudo samples with certain characteristics\ntowards more diversity generated and intuitive transfer. In addation, to\ndescribe the new scene that is the limit seen class samples in the training\nprocess, we further formulate a new ZSL task named the 'Few-shot Seen class and\nZero-shot Unseen class learning' (FSZU). Extensive experiments on four\nbenchmarks verify that the proposed method is competitive in the GZSL and the\nFSZU tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yaogong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Pengbo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Commonsense Graph for Object Localisation in Partial Scenes. (arXiv:2203.05380v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05380","description":"<p>We solve object localisation in partial scenes, a new problem of estimating\nthe unknown position of an object (e.g. where is the bag?) given a partial 3D\nscan of a scene. The proposed solution is based on a novel scene graph model,\nthe Spatial Commonsense Graph (SCG), where objects are the nodes and edges\ndefine pairwise distances between them, enriched by concept nodes and\nrelationships from a commonsense knowledge base. This allows SCG to better\ngeneralise its spatial inference over unknown 3D scenes. The SCG is used to\nestimate the unknown position of the target object in two steps: first, we feed\nthe SCG into a novel Proximity Prediction Network, a graph neural network that\nuses attention to perform distance prediction between the node representing the\ntarget object and the nodes representing the observed objects in the SCG;\nsecond, we propose a Localisation Module based on circular intersection to\nestimate the object position using all the predicted pairwise distances in\norder to be independent of any reference system. We create a new dataset of\npartially reconstructed scenes to benchmark our method and baselines for object\nlocalisation in partial scenes, where our proposed method achieves the best\nlocalisation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giuliari_F/0/1/0/all/0/1\">Francesco Giuliari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Investigation of 3D Anomaly Detection and Segmentation. (arXiv:2203.05550v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05550","description":"<p>Anomaly detection and segmentation in images has made tremendous progress in\nrecent years while 3D information has often been ignored. The objective of this\npaper is to further understand the benefit and role of 3D as opposed to color\nin image anomaly detection. Our study begins by presenting a surprising\nfinding: standard color-only anomaly segmentation methods, when applied to 3D\ndatasets, significantly outperform all current methods. On the other hand, we\nobserve that color-only methods are insufficient for images containing\ngeometric anomalies where shape cannot be unambiguously inferred from 2D. This\nsuggests that better 3D methods are needed. We investigate different\nrepresentations for 3D anomaly detection and discover that handcrafted\norientation-invariant representations are unreasonably effective on this task.\nWe uncover a simple 3D-only method that outperforms all recent approaches while\nnot using deep learning, external pretraining datasets, or color information.\nAs the 3D-only method cannot detect color and texture anomalies, we combine it\nwith 2D color features, granting us the best current results by a large margin\n(Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by\ndiscussing future challenges for 3D anomaly detection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horwitz_E/0/1/0/all/0/1\">Eliahu Horwitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Domain Reconstruction Networks with V-Net and K-Net for fast MRI. (arXiv:2203.05725v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05725","description":"<p>Purpose: To introduce a dual-domain reconstruction network with V-Net and\nK-Net for accurate MR image reconstruction from undersampled k-space data.\nMethods: Most state-of-the-art reconstruction methods apply U-Net or cascaded\nU-Nets in image domain and/or k-space domain. Nevertheless, these methods have\nfollowing problems: (1) Directly applying U-Net in k-space domain is not\noptimal for extracting features in k-space domain; (2) Classical image-domain\noriented U-Net is heavy-weight and hence is inefficient to be cascaded many\ntimes for yielding good reconstruction accuracy; (3) Classical image-domain\noriented U-Net does not fully make use information of encoder network for\nextracting features in decoder network; and (4) Existing methods are\nineffective in simultaneously extracting and fusing features in image domain\nand its dual k-space domain. To tackle these problems, we propose in this paper\n(1) an image-domain encoder-decoder sub-network called V-Net which is more\nlight-weight for cascading and effective in fully utilizing features in the\nencoder for decoding, (2) a k-space domain sub-network called K-Net which is\nmore suitable for extracting hierarchical features in k-space domain, and (3) a\ndual-domain reconstruction network where V-Nets and K-Nets are parallelly and\neffectively combined and cascaded. Results: Extensive experimental results on\nthe challenging fastMRI dataset demonstrate that the proposed KV-Net can\nreconstruct high-quality images and outperform current state-of-the-art\napproaches with fewer parameters. Conclusions: To reconstruct images\neffectively and efficiently from incomplete k-space data, we have presented a\nparallel dual-domain KV-Net to combine K-Nets and V-Nets. The KV-Net is more\nlightweight than state-of-the-art methods but achieves better reconstruction\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yanwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruiqi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenchang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}