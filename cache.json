{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"How Adults Understand What Young Children Say. (arXiv:2206.07807v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07807","description":"<p>Children's early speech often bears little resemblance to adult speech in\nform or content, and yet caregivers often find meaning in young children's\nutterances. Precisely how caregivers are able to do this remains poorly\nunderstood. We propose that successful early communication (an essential\nbuilding block of language development) relies not just on children's growing\nlinguistic knowledge, but also on adults' sophisticated inferences. These\ninferences, we further propose, are optimized for fine-grained details of how\nchildren speak. We evaluate these ideas using a set of candidate computational\nmodels of spoken word recognition based on deep learning and Bayesian\ninference, which instantiate competing hypotheses regarding the information\nsources used by adults to understand children. We find that the best-performing\nmodels (evaluated on datasets of adult interpretations of child speech) are\nthose that have strong prior expectations about what children are likely to\nwant to communicate, rather than the actual phonetic contents of what children\nsay. We further find that adults' behavior is best characterized as well-tuned\nto specific children: the more closely a word recognition model is tuned to the\nparticulars of an individual child's actual linguistic behavior, the better it\npredicts adults' inferences about what the child has said. These results offer\na comprehensive investigation into the role of caregivers as child-directed\nlisteners, with broader consequences for theories of language acquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meylan_S/0/1/0/all/0/1\">Stephan C. Meylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foushee_R/0/1/0/all/0/1\">Ruthe Foushee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Nicole H. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergelson_E/0/1/0/all/0/1\">Elika Bergelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems. (arXiv:2206.07808v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07808","description":"<p>We present results from a large-scale experiment on pretraining encoders with\nnon-embedding parameter counts ranging from 700M to 9.3B, their subsequent\ndistillation into smaller models ranging from 17M-170M parameters, and their\napplication to the Natural Language Understanding (NLU) component of a virtual\nassistant system. Though we train using 70% spoken-form data, our teacher\nmodels perform comparably to XLM-R and mT5 when evaluated on the written-form\nCross-lingual Natural Language Inference (XNLI) corpus. We perform a second\nstage of pretraining on our teacher models using in-domain data from our\nsystem, improving error rates by 3.86% relative for intent classification and\n7.01% relative for slot filling. We find that even a 170M-parameter model\ndistilled from our Stage 2 teacher model has 2.88% better intent classification\nand 7.69% better slot filling error rates when compared to the 2.3B-parameter\nteacher trained only on public data (Stage 1), emphasizing the importance of\nin-domain data for pretraining. When evaluated offline using labeled NLU data,\nour 17M-parameter Stage 2 distilled model outperforms both XLM-R Base (85M\nparams) and DistillBERT (42M params) by 4.23% to 6.14%, respectively. Finally,\nwe present results from a full virtual assistant experimentation platform,\nwhere we find that models trained using our pretraining and distillation\npipeline outperform models distilled from 85M-parameter teachers by 3.74%-4.91%\non an automatic measurement of full-system user dissatisfaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_J/0/1/0/all/0/1\">Jack FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ananthakrishnan_S/0/1/0/all/0/1\">Shankar Ananthakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardi_D/0/1/0/all/0/1\">Davide Bernardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagia_A/0/1/0/all/0/1\">Abhishek Bhagia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovi_C/0/1/0/all/0/1\">Claudio Delli Bovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1\">Amit Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Luoxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwarakanath_A/0/1/0/all/0/1\">Anurag Dwarakanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dwivedi_S/0/1/0/all/0/1\">Satyam Dwivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojayev_T/0/1/0/all/0/1\">Turan Gojayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gueudre_T/0/1/0/all/0/1\">Thomas Gueudre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamza_W/0/1/0/all/0/1\">Wael Hamza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hueser_J/0/1/0/all/0/1\">Jonathan Hueser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_K/0/1/0/all/0/1\">Kevin Martin Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Beiye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianhua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzotti_A/0/1/0/all/0/1\">Alessandro Manzotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owczarzak_K/0/1/0/all/0/1\">Karolina Owczarzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oz_G/0/1/0/all/0/1\">Gokmen Oz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palumbo_E/0/1/0/all/0/1\">Enrico Palumbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_C/0/1/0/all/0/1\">Chandana Satya Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawls_S/0/1/0/all/0/1\">Stephen Rawls</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenbaum_A/0/1/0/all/0/1\">Andy Rosenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1\">Anjali Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltan_S/0/1/0/all/0/1\">Saleh Soltan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_M/0/1/0/all/0/1\">Mukund Harakere Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liz Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triefenbach_F/0/1/0/all/0/1\">Fabian Triefenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personal Entity, Concept, and Named Entity Linking in Conversations. (arXiv:2206.07836v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07836","description":"<p>Building conversational agents that can have natural and knowledge-grounded\ninteractions with humans requires understanding user utterances. Entity Linking\n(EL) is an effective and widely used method for understanding natural language\ntext and connecting it to external knowledge. It is, however, shown that\nexisting EL methods developed for annotating documents are suboptimal for\nconversations, where personal entities (e.g., \"my cars\") and concepts are\nessential for understanding user utterances. In this paper, we introduce a\ncollection and a tool for entity linking in conversations. We collect EL\nannotations for 1327 conversational utterances, consisting of links to named\nentities, concepts, and personal entities. The dataset is used for training our\ntoolkit for conversational entity linking, CREL. Unlike existing EL methods,\nCREL is developed to identify both named entities and concepts. It also\nutilizes coreference resolution techniques to identify personal entities and\nreferences to the explicit entity mentions in the conversations. We compare\nCREL with state-of-the-art techniques and show that it outperforms all existing\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joko_H/0/1/0/all/0/1\">Hideaki Joko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasibi_F/0/1/0/all/0/1\">Faegheh Hasibi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOKEN is a MASK: Few-shot Named Entity Recognition with Pre-trained Language Models. (arXiv:2206.07841v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07841","description":"<p>Transferring knowledge from one domain to another is of practical importance\nfor many tasks in natural language processing, especially when the amount of\navailable data in the target domain is limited. In this work, we propose a\nnovel few-shot approach to domain adaptation in the context of Named Entity\nRecognition (NER). We propose a two-step approach consisting of a variable base\nmodule and a template module that leverages the knowledge captured in\npre-trained language models with the help of simple descriptive patterns. Our\napproach is simple yet versatile and can be applied in few-shot and zero-shot\nsettings. Evaluating our lightweight approach across a number of different\ndatasets shows that it can boost the performance of state-of-the-art baselines\nby 2-5% F1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davody_A/0/1/0/all/0/1\">Ali Davody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1\">Thomas Kleinbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text normalization for endangered languages: the case of Ligurian. (arXiv:2206.07861v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07861","description":"<p>Text normalization is a crucial technology for low-resource languages which\nlack rigid spelling conventions. Low-resource text normalization has so far\nrelied upon hand-crafted rules, which are perceived to be more data efficient\nthan neural methods.\n</p>\n<p>In this paper we examine the case of text normalization for Ligurian, an\nendangered Romance language. We collect 4,394 Ligurian sentences paired with\ntheir normalized versions, as well as the first monolingual corpus for\nLigurian. We show that, in spite of the small amounts of data available, a\ncompact transformer-based model can be trained to achieve very low error rates\nby the use of backtranslation and appropriate tokenization. Our datasets are\nreleased to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lusito_S/0/1/0/all/0/1\">Stefano Lusito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1\">Edoardo Ferrante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maillard_J/0/1/0/all/0/1\">Jean Maillard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization. (arXiv:2206.07882v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07882","description":"<p>We report on aggressive quantization strategies that greatly accelerate\ninference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit\ninteger representation for both weights and activations and apply Quantization\nAware Training (QAT) to retrain the full model (acoustic encoder and language\nmodel) and achieve near-iso-accuracy. We show that customized quantization\nschemes that are tailored to the local properties of the network are essential\nto achieve good performance while limiting the computational overhead of QAT.\n</p>\n<p>Density ratio Language Model fusion has shown remarkable accuracy gains on\nRNN-T workloads but it severely increases the computational cost of inference.\nWe show that our quantization strategies enable using large beam widths for\nhypothesis search while achieving streaming-compatible runtimes and a full\nmodel compression ratio of 7.6$\\times$ compared to the full precision model.\n</p>\n<p>Via hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to\nINT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a\nReal Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03\ntest sets, we retain most of the gains associated with LM fusion, improving the\naverage WER by $&gt;$1.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fasoli_A/0/1/0/all/0/1\">Andrea Fasoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chia-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_M/0/1/0/all/0/1\">Mauricio Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataramani_S/0/1/0/all/0/1\">Swagath Venkataramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaodong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kailash Gopalakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dialogue State Tracking. (arXiv:2206.07898v1 [cs.AI])","link":"http://arxiv.org/abs/2206.07898","description":"<p>Designed for tracking user goals in dialogues, a dialogue state tracker is an\nessential component in a dialogue system. However, the research of dialogue\nstate tracking has largely been limited to unimodality, in which slots and slot\nvalues are limited by knowledge domains (e.g. restaurant domain with slots of\nrestaurant name and price range) and are defined by specific database schema.\nIn this paper, we propose to extend the definition of dialogue state tracking\nto multimodality. Specifically, we introduce a novel dialogue state tracking\ntask to track the information of visual objects that are mentioned in\nvideo-grounded dialogues. Each new dialogue utterance may introduce a new video\nsegment, new visual objects, or new object attributes, and a state tracker is\nrequired to update these information slots accordingly. We created a new\nsynthetic benchmark and designed a novel baseline, Video-Dialogue Transformer\nNetwork (VDTN), for this task. VDTN combines both object-level features and\nsegment-level features and learns contextual dependencies between videos and\ndialogues to generate multimodal dialogue states. We optimized VDTN for a state\ngeneration task as well as a self-supervised video understanding task which\nrecovers video segment or object representations. Finally, we trained VDTN to\nuse the decoded states in a response prediction task. Together with\ncomprehensive ablation and qualitative analysis, we discovered interesting\ninsights towards building more capable multimodal dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PInKS: Preconditioned Commonsense Inference with Minimal Supervision. (arXiv:2206.07920v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07920","description":"<p>Reasoning with preconditions such as \"glass can be used for drinking water\nunless the glass is shattered\" remains an open problem for language models. The\nmain challenge lies in the scarcity of preconditions data and the model's lack\nof support for such reasoning. We present PInKS, Preconditioned Commonsense\nInference with WeaK Supervision, an improved model for reasoning with\npreconditions through minimum supervision. We show, both empirically and\ntheoretically, that PInKS improves the results on benchmarks focused on\nreasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1\nscores). We further investigate PInKS through PAC-Bayesian informativeness\nanalysis, precision measures, and ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qasemi_E/0/1/0/all/0/1\">Ehsan Qasemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_P/0/1/0/all/0/1\">Piyush Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qiang Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Prosody Annotation with Pre-Trained Text-Speech Model. (arXiv:2206.07956v1 [cs.SD])","link":"http://arxiv.org/abs/2206.07956","description":"<p>Prosodic boundary plays an important role in text-to-speech synthesis (TTS)\nin terms of naturalness and readability. However, the acquisition of prosodic\nboundary labels relies on manual annotation, which is costly and\ntime-consuming. In this paper, we propose to automatically extract prosodic\nboundary labels from text-audio data via a neural text-speech model with\npre-trained audio encoders. This model is pre-trained on text and speech data\nseparately and jointly fine-tuned on TTS data in a triplet format: {speech,\ntext, prosody}. The experimental results on both automatic evaluation and human\nevaluation demonstrate that: 1) the proposed text-speech prosody annotation\nframework significantly outperforms text-only baselines; 2) the quality of\nautomatic prosodic boundary annotations is comparable to human annotations; 3)\nTTS systems trained with model-annotated boundaries are slightly better than\nsystems that use manual ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Ziqian Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yanyao Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Better Understanding with Uniformity and Explicit Regularization of Embeddings in Embedding-based Neural Topic Models. (arXiv:2206.07960v1 [cs.CL])","link":"http://arxiv.org/abs/2206.07960","description":"<p>Embedding-based neural topic models could explicitly represent words and\ntopics by embedding them to a homogeneous feature space, which shows higher\ninterpretability. However, there are no explicit constraints for the training\nof embeddings, leading to a larger optimization space. Also, a clear\ndescription of the changes in embeddings and the impact on model performance is\nstill lacking. In this paper, we propose an embedding regularized neural topic\nmodel, which applies the specially designed training constraints on word\nembedding and topic embedding to reduce the optimization space of parameters.\nTo reveal the changes and roles of embeddings, we introduce \\textbf{uniformity}\ninto the embedding-based neural topic model as the evaluation metric of\nembedding space. On this basis, we describe how embeddings tend to change\nduring training via the changes in the uniformity of embeddings. Furthermore,\nwe demonstrate the impact of changes in embeddings in embedding-based neural\ntopic models through ablation studies. The results of experiments on two\nmainstream datasets indicate that our model significantly outperforms baseline\nmodels in terms of the harmony between topic quality and document modeling.\nThis work is the first attempt to exploit uniformity to explore changes in\nembeddings of embedding-based neural topic models and their impact on model\nperformance to the best of our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shihua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Global Semantic Similarities in Knowledge Graphs by Relational Prototype Entities. (arXiv:2206.08021v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08021","description":"<p>Knowledge graph (KG) embedding aims at learning the latent representations\nfor entities and relations of a KG in continuous vector spaces. An empirical\nobservation is that the head (tail) entities connected by the same relation\noften share similar semantic attributes -- specifically, they often belong to\nthe same category -- no matter how far away they are from each other in the KG;\nthat is, they share global semantic similarities. However, many existing\nmethods derive KG embeddings based on the local information, which fail to\neffectively capture such global semantic similarities among entities. To\naddress this challenge, we propose a novel approach, which introduces a set of\nvirtual nodes called \\textit{\\textbf{relational prototype entities}} to\nrepresent the prototypes of the head and tail entities connected by the same\nrelations. By enforcing the entities' embeddings close to their associated\nprototypes' embeddings, our approach can effectively encourage the global\nsemantic similarities of entities -- that can be far away in the KG --\nconnected by the same relation. Experiments on the entity alignment and KG\ncompletion tasks demonstrate that our approach significantly outperforms recent\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIALOG-22 RuATD Generated Text Detection. (arXiv:2206.08029v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08029","description":"<p>Text Generation Models (TGMs) succeed in creating text that matches human\nlanguage style reasonably well. Detectors that can distinguish between\nTGM-generated text and human-written ones play an important role in preventing\nabuse of TGM.\n</p>\n<p>In this paper, we describe our pipeline for the two DIALOG-22 RuATD tasks:\ndetecting generated text (binary task) and classification of which model was\nused to generate text (multiclass task). We achieved 1st place on the binary\nclassification task with an accuracy score of 0.82995 on the private test set\nand 4th place on the multiclass classification task with an accuracy score of\n0.62856 on the private test set. We proposed an ensemble method of different\npre-trained models based on the attention mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maloyan_N/0/1/0/all/0/1\">Narek Maloyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nutfullin_B/0/1/0/all/0/1\">Bulat Nutfullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilyushin_E/0/1/0/all/0/1\">Eugene Ilyushin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History. (arXiv:2206.08039v1 [cs.SD])","link":"http://arxiv.org/abs/2206.08039","description":"<p>We propose an end-to-end empathetic dialogue speech synthesis (DSS) model\nthat considers both the linguistic and prosodic contexts of dialogue history.\nEmpathy is the active attempt by humans to get inside the interlocutor in\ndialogue, and empathetic DSS is a technology to implement this act in spoken\ndialogue systems. Our model is conditioned by the history of linguistic and\nprosody features for predicting appropriate dialogue context. As such, it can\nbe regarded as an extension of the conventional linguistic-feature-based\ndialogue history modeling. To train the empathetic DSS model effectively, we\ninvestigate 1) a self-supervised learning model pretrained with large speech\ncorpora, 2) a style-guided training using a prosody embedding of the current\nutterance to be predicted by the dialogue context embedding, 3) a cross-modal\nattention to combine text and speech modalities, and 4) a sentence-wise\nembedding to achieve fine-grained prosody modeling rather than utterance-wise\nmodeling. The evaluation results demonstrate that 1) simply considering\nprosodic contexts of the dialogue history does not improve the quality of\nspeech in empathetic DSS and 2) introducing style-guided training and\nsentence-wise embedding modeling achieves higher speech quality than that by\nthe conventional method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_Y/0/1/0/all/0/1\">Yuto Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1\">Yuki Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamichi_S/0/1/0/all/0/1\">Shinnosuke Takamichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tachibana_K/0/1/0/all/0/1\">Kentaro Tachibana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saruwatari_H/0/1/0/all/0/1\">Hiroshi Saruwatari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Open-Domain QA System for e-Governance. (arXiv:2206.08046v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08046","description":"<p>The paper presents an open-domain Question Answering system for Romanian,\nanswering COVID-19 related questions. The QA system pipeline involves automatic\nquestion processing, automatic query generation, web searching for the top 10\nmost relevant documents and answer extraction using a fine-tuned BERT model for\nExtractive QA, trained on a COVID-19 data set that we have manually created.\nThe paper will present the QA system and its integration with the Romanian\nlanguage technologies portal RELATE, the COVID-19 data set and different\nevaluations of the QA performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ion_R/0/1/0/all/0/1\">Radu Ion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile P&#x103;i&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrofan_M/0/1/0/all/0/1\">Maria Mitrofan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mititelu_V/0/1/0/all/0/1\">Verginica Barbu Mititelu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irimia_E/0/1/0/all/0/1\">Elena Irimia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badea_V/0/1/0/all/0/1\">Valentin Badea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JU_NLP at HinglishEval: Quality Evaluation of the Low-Resource Code-Mixed Hinglish Text. (arXiv:2206.08053v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08053","description":"<p>In this paper we describe a system submitted to the INLG 2022 Generation\nChallenge (GenChal) on Quality Evaluation of the Low-Resource Synthetically\nGenerated Code-Mixed Hinglish Text. We implement a Bi-LSTM-based neural network\nmodel to predict the Average rating score and Disagreement score of the\nsynthetic Hinglish dataset. In our models, we used word embeddings for English\nand Hindi data, and one hot encodings for Hinglish data. We achieved a F1 score\nof 0.11, and mean squared error of 6.0 in the average rating score prediction\ntask. In the task of Disagreement score prediction, we achieve a F1 score of\n0.18, and mean squared error of 5.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guha_P/0/1/0/all/0/1\">Prantik Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhar_R/0/1/0/all/0/1\">Rudra Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipankar Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonwords Pronunciation Classification in Language Development Tests for Preschool Children. (arXiv:2206.08058v1 [eess.AS])","link":"http://arxiv.org/abs/2206.08058","description":"<p>This work aims to automatically evaluate whether the language development of\nchildren is age-appropriate. Validated speech and language tests are used for\nthis purpose to test the auditory memory. In this work, the task is to\ndetermine whether spoken nonwords have been uttered correctly. We compare\ndifferent approaches that are motivated to model specific language structures:\nLow-level features (FFT), speaker embeddings (ECAPA-TDNN), grapheme-motivated\nembeddings (wav2vec 2.0), and phonetic embeddings in form of senones (ASR\nacoustic model). Each of the approaches provides input for VGG-like 5-layer CNN\nclassifiers. We also examine the adaptation per nonword. The evaluation of the\nproposed systems was performed using recordings from different kindergartens of\nspoken nonwords. ECAPA-TDNN and low-level FFT features do not explicitly model\nphonetic information; wav2vec2.0 is trained on grapheme labels, our ASR\nacoustic model features contain (sub-)phonetic information. We found that the\nmore granular the phonetic modeling is, the higher are the achieved recognition\nrates. The best system trained on ASR acoustic model features with VTLN\nachieved an accuracy of 89.4% and an area under the ROC (Receiver Operating\nCharacteristic) curve (AUC) of 0.923. This corresponds to an improvement in\naccuracy of 20.2% and AUC of 0.309 relative compared to the FFT-baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baumann_I/0/1/0/all/0/1\">Ilja Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bocklet_T/0/1/0/all/0/1\">Tobias Bocklet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Ranker for Text Retrieval. (arXiv:2206.08063v1 [cs.IR])","link":"http://arxiv.org/abs/2206.08063","description":"<p>A ranker plays an indispensable role in the de facto 'retrieval &amp; rerank'\npipeline, but its training still lags behind -- learning from moderate\nnegatives or/and serving as an auxiliary module for a retriever. In this work,\nwe first identify two major barriers to a robust ranker, i.e., inherent label\nnoises caused by a well-trained retriever and non-ideal negatives sampled for a\nhigh-capable ranker. Thereby, we propose multiple retrievers as negative\ngenerators improve the ranker's robustness, where i) involving extensive\nout-of-distribution label noises renders the ranker against each noise\ndistribution, and ii) diverse hard negatives from a joint distribution are\nrelatively close to the ranker's negative distribution, leading to more\nchallenging thus effective training. To evaluate our robust ranker (dubbed\nR$^2$anker), we conduct experiments in various settings on the popular passage\nretrieval benchmark, including BM25-reranking, full-ranking, retriever\ndistillation, etc. The empirical results verify the new state-of-the-art\neffectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransDrift: Modeling Word-Embedding Drift using Transformer. (arXiv:2206.08081v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08081","description":"<p>In modern NLP applications, word embeddings are a crucial backbone that can\nbe readily shared across a number of tasks. However as the text distributions\nchange and word semantics evolve over time, the downstream applications using\nthe embeddings can suffer if the word representations do not conform to the\ndata drift. Thus, maintaining word embeddings to be consistent with the\nunderlying data distribution is a key problem. In this work, we tackle this\nproblem and propose TransDrift, a transformer-based prediction model for word\nembeddings. Leveraging the flexibility of transformer, our model accurately\nlearns the dynamics of the embedding drift and predicts the future embedding.\nIn experiments, we compare with existing methods and show that our model makes\nsignificantly more accurate predictions of the word embedding than the\nbaselines. Crucially, by applying the predicted embeddings as a backbone for\ndownstream classification tasks, we show that our embeddings lead to superior\nperformance compared to the previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_N/0/1/0/all/0/1\">Nishtha Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_P/0/1/0/all/0/1\">Prateek Chaudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nishant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1\">Srikanta Bedathur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator. (arXiv:2206.08082v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08082","description":"<p>Large-scale pre-trained language models (PLMs) are well-known for being\ncapable of solving a task simply by conditioning a few input-label pairs dubbed\ndemonstrations on a prompt without being explicitly tuned for the desired\ndownstream task. Such a process (i.e., in-context learning), however, naturally\nleads to high reliance on the demonstrations which are usually selected from\nexternal datasets. In this paper, we propose self-generated in-context learning\n(SG-ICL), which generates demonstrations for in-context learning from PLM\nitself to minimize the reliance on the external demonstration. We conduct\nexperiments on four different text classification tasks and show SG-ICL\nsignificantly outperforms zero-shot learning and is generally worth\napproximately 0.6 gold training samples. Moreover, our generated demonstrations\nshow more consistent performance with low variance compared to randomly\nselected demonstrations from the training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuhng Joon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunsoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junyeob Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. (arXiv:2206.08155v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08155","description":"<p>Video question answering (VideoQA) is a complex task that requires diverse\nmulti-modal data for training. Manual annotation of question and answers for\nvideos, however, is tedious and prohibits scalability. To tackle this problem,\nrecent methods consider zero-shot settings with no manual annotation of visual\nquestion-answer. In particular, a promising approach adapts frozen\nautoregressive language models pretrained on Web-scale text-only data to\nmulti-modal inputs. In contrast, we here build on frozen bidirectional language\nmodels (BiLM) and show that such an approach provides a stronger and cheaper\nalternative for zero-shot VideoQA. In particular, (i) we combine visual inputs\nwith the frozen BiLM using light trainable modules, (ii) we train such modules\nusing Web-scraped multi-modal data, and finally (iii) we perform zero-shot\nVideoQA inference through masked language modeling, where the masked text is\nthe answer to a given question. Our proposed approach, FrozenBiLM, outperforms\nthe state of the art in zero-shot VideoQA by a significant margin on a variety\nof datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA,\nTGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in\nthe few-shot and fully-supervised setting. Our code and models will be made\npublicly available at https://antoyang.github.io/frozenbilm.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All the World's a (Hyper)Graph: A Data Drama. (arXiv:2206.08225v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08225","description":"<p>We introduce Hyperbard, a dataset of diverse relational data representations\nderived from Shakespeare's plays. Our representations range from simple graphs\ncapturing character co-occurrence in single scenes to hypergraphs encoding\ncomplex communication settings and character contributions as hyperedges with\nedge-specific node weights. By making multiple intuitive representations\nreadily available for experimentation, we facilitate rigorous representation\nrobustness checks in graph learning, graph mining, and network analysis,\nhighlighting the advantages and drawbacks of specific representations.\nLeveraging the data released in Hyperbard, we demonstrate that many solutions\nto popular graph mining problems are highly dependent on the representation\nchoice, thus calling current graph curation practices into question. As an\nhomage to our data source, and asserting that science can also be art, we\npresent all our points in the form of a play.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coupette_C/0/1/0/all/0/1\">Corinna Coupette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1\">Jilles Vreeken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1\">Bastian Rieck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Architecture for Automatic Essay Scoring. (arXiv:2206.08232v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08232","description":"<p>Automatic evaluation of essay (AES) and also called automatic essay scoring\nhas become a severe problem due to the rise of online learning and evaluation\nplatforms such as Coursera, Udemy, Khan academy, and so on. Researchers have\nrecently proposed many techniques for automatic evaluation. However, many of\nthese techniques use hand-crafted features and thus are limited from the\nfeature representation point of view. Deep learning has emerged as a new\nparadigm in machine learning which can exploit the vast data and identify the\nfeatures useful for essay evaluation. To this end, we propose a novel\narchitecture based on recurrent networks (RNN) and convolution neural network\n(CNN). In the proposed architecture, the multichannel convolutional layer\nlearns and captures the contextual features of the word n-gram from the word\nembedding vectors and the essential semantic concepts to form the feature\nvector at essay level using max-pooling operation. A variant of RNN called\nBi-gated recurrent unit (BGRU) is used to access both previous and subsequent\ncontextual representations. The experiment was carried out on eight data sets\navailable on Kaggle for the task of AES. The experimental results show that our\nproposed system achieves significantly higher grading accuracy than other deep\nlearning-based AES systems and also other state-of-the-art AES systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tashu_T/0/1/0/all/0/1\">Tsegaye Misikir Tashu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurya_C/0/1/0/all/0/1\">Chandresh Kumar Maurya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_T/0/1/0/all/0/1\">Tomas Horvath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"'John ate 5 apples' != 'John ate some apples': Self-Supervised Paraphrase Quality Detection for Algebraic Word Problems. (arXiv:2206.08263v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08263","description":"<p>This paper introduces the novel task of scoring paraphrases for Algebraic\nWord Problems (AWP) and presents a self-supervised method for doing so. In the\ncurrent online pedagogical setting, paraphrasing these problems is helpful for\nacademicians to generate multiple syntactically diverse questions for\nassessments. It also helps induce variation to ensure that the student has\nunderstood the problem instead of just memorizing it or using unfair means to\nsolve it. The current state-of-the-art paraphrase generation models often\ncannot effectively paraphrase word problems, losing a critical piece of\ninformation (such as numbers or units) which renders the question unsolvable.\nThere is a need for paraphrase scoring methods in the context of AWP to enable\nthe training of good paraphrasers. Thus, we propose ParaQD, a self-supervised\nparaphrase quality detection method using novel data augmentations that can\nlearn latent representations to separate a high-quality paraphrase of an\nalgebraic question from a poor one by a wide margin. Through extensive\nexperimentation, we demonstrate that our method outperforms existing\nstate-of-the-art self-supervised methods by up to 32% while also demonstrating\nimpressive zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rishabh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1\">Venktesh V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohania_M/0/1/0/all/0/1\">Mukesh Mohania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Generation of Musical Explanations with GPT-3. (arXiv:2206.08264v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08264","description":"<p>Open AI's language model, GPT-3, has shown great potential for many NLP\ntasks, with applications in many different domains. In this work we carry out a\nfirst study on GPT-3's capability to communicate musical decisions through\ntextual explanations when prompted with a textual representation of a piece of\nmusic. Enabling a dialogue in human-AI music partnerships is an important step\ntowards more engaging and creative human-AI interactions. Our results show that\nGPT-3 lacks the necessary intelligence to really understand musical decisions.\nA major barrier to reach a better performance is the lack of data that includes\nexplanations of the creative process carried out by artists for musical pieces.\nWe believe such a resource would aid the understanding and collaboration with\nAI music systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krol_S/0/1/0/all/0/1\">Stephen James Krol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llano_M/0/1/0/all/0/1\">Maria Teresa Llano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCormack_J/0/1/0/all/0/1\">Jon McCormack</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANGLEr: A Next-Generation Natural Language Exploratory Framework. (arXiv:2206.08266v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08266","description":"<p>Natural language processing is used for solving a wide variety of problems.\nSome scholars and interest groups working with language resources are not well\nversed in programming, so there is a need for a good graphical framework that\nallows users to quickly design and test natural language processing pipelines\nwithout the need for programming. The existing frameworks do not satisfy all\nthe requirements for such a tool. We, therefore, propose a new framework that\nprovides a simple way for its users to build language processing pipelines. It\nalso allows a simple programming language agnostic way for adding new modules,\nwhich will help the adoption by natural language processing developers and\nresearchers. The main parts of the proposed framework consist of (a) a\npluggable Docker-based architecture, (b) a general data model, and (c) APIs\ndescription along with the graphical user interface. The proposed design is\nbeing used for implementation of a new natural language processing framework,\ncalled ANGLEr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knez_T/0/1/0/all/0/1\">Timotej Knez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajec_M/0/1/0/all/0/1\">Marko Bajec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnik_S/0/1/0/all/0/1\">Slavko &#x17d;itnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ratatouille: A tool for Novel Recipe Generation. (arXiv:2206.08267v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08267","description":"<p>Due to availability of a large amount of cooking recipes online, there is a\ngrowing interest in using this as data to create novel recipes. Novel Recipe\nGeneration is a problem in the field of Natural Language Processing in which\nour main interest is to generate realistic, novel cooking recipes. To come up\nwith such novel recipes, we trained various Deep Learning models such as LSTMs\nand GPT-2 with a large amount of recipe data. We present Ratatouille\n(https://cosylab.iiitd.edu.in/ratatouille2/), a web based application to\ngenerate novel recipes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_M/0/1/0/all/0/1\">Mansi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pallab Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponnaganti_V/0/1/0/all/0/1\">Vijay Ponnaganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Minnet Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tatipamala_S/0/1/0/all/0/1\">Sritanaya Tatipamala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_A/0/1/0/all/0/1\">Aakanksha Saini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagler_G/0/1/0/all/0/1\">Ganesh Bagler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Cost and Quality: An Exploration of Human-in-the-loop Frameworks for Automated Short Answer Scoring. (arXiv:2206.08288v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08288","description":"<p>Short answer scoring (SAS) is the task of grading short text written by a\nlearner. In recent years, deep-learning-based approaches have substantially\nimproved the performance of SAS models, but how to guarantee high-quality\npredictions still remains a critical issue when applying such models to the\neducation field. Towards guaranteeing high-quality predictions, we present the\nfirst study of exploring the use of human-in-the-loop framework for minimizing\nthe grading cost while guaranteeing the grading quality by allowing a SAS model\nto share the grading task with a human grader. Specifically, by introducing a\nconfidence estimation method for indicating the reliability of the model\npredictions, one can guarantee the scoring quality by utilizing only\npredictions with high reliability for the scoring results and casting\npredictions with low reliability to human graders. In our experiments, we\ninvestigate the feasibility of the proposed framework using multiple confidence\nestimation methods and multiple SAS datasets. We find that our\nhuman-in-the-loop framework allows automatic scoring models and human graders\nto achieve the target scoring quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Funayama_H/0/1/0/all/0/1\">Hiroaki Funayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Tasuku Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubayashi_Y/0/1/0/all/0/1\">Yuichiroh Matsubayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizumoto_T/0/1/0/all/0/1\">Tomoya Mizumoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition. (arXiv:2206.08317v1 [cs.SD])","link":"http://arxiv.org/abs/2206.08317","description":"<p>Transformers have recently dominated the ASR field. Although able to yield\ngood performance, they involve an autoregressive (AR) decoder to generate\ntokens one by one, which is computationally inefficient. To speed up inference,\nnon-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to\nenable parallel generation. However, due to an independence assumption within\nthe output tokens, performance of single-step NAR is inferior to that of AR\nmodels, especially with a large-scale corpus. There are two challenges to\nimproving single-step NAR: Firstly to accurately predict the number of output\ntokens and extract hidden variables; secondly, to enhance modeling of\ninterdependence between output tokens. To tackle both challenges, we propose a\nfast and accurate parallel transformer, termed Paraformer. This utilizes a\ncontinuous integrate-and-fire based predictor to predict the number of tokens\nand generate hidden variables. A glancing language model (GLM) sampler then\ngenerates semantic embeddings to enhance the NAR decoder's ability to model\ncontext interdependence. Finally, we design a strategy to generate negative\nsamples for minimum word error rate training to further improve performance.\nExperiments using the public AISHELL-1, AISHELL-2 benchmark, and an\nindustrial-level 20,000 hour task demonstrate that the proposed Paraformer can\nattain comparable performance to the state-of-the-art AR transformer, with more\nthan 10x speedup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLoughlin_I/0/1/0/all/0/1\">Ian McLoughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhijie Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models. (arXiv:2206.08325v1 [cs.CL])","link":"http://arxiv.org/abs/2206.08325","description":"<p>Large language models produce human-like text that drive a growing number of\napplications. However, recent literature and, increasingly, real world\nobservations, have demonstrated that these models can generate language that is\ntoxic, biased, untruthful or otherwise harmful. Though work to evaluate\nlanguage model harms is under way, translating foresight about which harms may\narise into rigorous benchmarks is not straightforward. To facilitate this\ntranslation, we outline six ways of characterizing harmful text which merit\nexplicit consideration when designing new benchmarks. We then use these\ncharacteristics as a lens to identify trends and gaps in existing benchmarks.\nFinally, we apply them in a case study of the Perspective API, a toxicity\nclassifier that is widely used in harm benchmarks. Our characteristics provide\none piece of the bridge that translates between foresight and effective\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rauh_M/0/1/0/all/0/1\">Maribeth Rauh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1\">John Mellor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uesato_J/0/1/0/all/0/1\">Jonathan Uesato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Sen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welbl_J/0/1/0/all/0/1\">Johannes Welbl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidinger_L/0/1/0/all/0/1\">Laura Weidinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dathathri_S/0/1/0/all/0/1\">Sumanth Dathathri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1\">Amelia Glaese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irving_G/0/1/0/all/0/1\">Geoffrey Irving</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_I/0/1/0/all/0/1\">Iason Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isaac_W/0/1/0/all/0/1\">William Isaac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Know your audience: specializing grounded language models with the game of Dixit. (arXiv:2206.08349v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08349","description":"<p>Effective communication requires adapting to the idiosyncratic common ground\nshared with each communicative partner. We study a particularly challenging\ninstantiation of this problem: the popular game Dixit. We formulate a round of\nDixit as a multi-agent image reference game where a (trained) speaker model is\nrewarded for describing a target image such that one (pretrained) listener\nmodel can correctly identify it from a pool of distractors, but another\nlistener cannot. To adapt to this setting, the speaker must exploit differences\nin the common ground it shares with the different listeners. We show that\nfinetuning an attention-based adapter between a CLIP vision encoder and a large\nlanguage model in this contrastive, multi-agent setting gives rise to\ncontext-dependent natural language specialization from rewards only, without\ndirect supervision. In a series of controlled experiments, we show that the\nspeaker can adapt according to the idiosyncratic strengths and weaknesses of\nvarious pairs of different listeners. Furthermore, we show zero-shot transfer\nof the speaker's specialization to unseen real-world data. Our experiments\noffer a step towards adaptive communication in complex multi-partner settings\nand highlight the interesting research challenges posed by games like Dixit. We\nhope that our work will inspire creative new approaches to adapting pretrained\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya K. Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">David Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxe_A/0/1/0/all/0/1\">Andrew Saxe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v13 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Broader terms curriculum mapping: Using natural language processing and visual-supported communication to create representative program planning experiences. (arXiv:2102.04811v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.04811","description":"<p>Accreditation bodies call for curriculum development processes open to all\nstakeholders, reflecting viewpoints of students, industry, university faculty\nand society. However, communication difficulties between faculty and\nnon-faculty groups leave unexplored an immense collaboration potential. Using\nclassification of learning objectives, natural language processing, and data\nvisualization, this paper presents a method to deliver program plan\nrepresentations that are universal, self-explanatory, and empowering. A simple\nexample shows how the method contributes to representative program planning\nexperiences and a case study is used to confirm the method's accuracy and\nutility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duarte_R/0/1/0/all/0/1\">Rog&#xe9;rio Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nobre_A/0/1/0/all/0/1\">&#xc2;ngela Lacerda Nobre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_F/0/1/0/all/0/1\">Fernando Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacquinet_M/0/1/0/all/0/1\">Marc Jacquinet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.13973","description":"<p>Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightSeq2: Accelerated Training for Transformer-based Models on GPUs. (arXiv:2110.05722v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05722","description":"<p>Transformer-based neural models are used in many AI applications. Training\nthese models is expensive, as it takes huge GPU resources and long duration. It\nis challenging because typical data like sentences have variable lengths, and\nTransformer's computation patterns are more complex than convolutional neural\nnetworks. Existing systems either only focus on model inference or optimization\nfor only BERT-like encoder models. In this paper, we present LightSeq2, a\nsystem to accelerate training for a general family of Transformer models on\nGPUs. We propose a series of GPU optimization techniques tailored to the\nspecific computation flow and memory access patterns of Transformer models.\nLightSeq2 supports many model architectures, including BERT (encoder-only), GPT\n(decoder-only), Transformer (encoder-decoder), and vision Transformer. Our\nexperiments for a variety of models and benchmarks show that LightSeq2 is\nconsistently faster (1.4-3.5x) than previous systems on different GPUs. In\nparticular, it gains 308% training speedup compared with existing systems on a\nlarge public machine translation benchmark (WMT14 English-German).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Ying Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yufei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Interpretation of Neural Text Classification. (arXiv:2202.09792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09792","description":"<p>Recent years have witnessed increasing interests in developing interpretable\nmodels in Natural Language Processing (NLP). Most existing models aim at\nidentifying input features such as words or phrases important for model\npredictions. Neural models developed in NLP however often compose word\nsemantics in a hierarchical manner and text classification requires\nhierarchical modelling to aggregate local information in order to deal with\ntopic and label shifts more effectively. As such, interpretation by words or\nphrases only cannot faithfully explain model decisions in text classification.\nThis paper proposes a novel Hierarchical INTerpretable neural text classifier,\ncalled Hint, which can automatically generate explanations of model predictions\nin the form of label-associated topics in a hierarchical manner. Model\ninterpretation is no longer at the word level, but built on topics as the basic\nsemantic unit. Experimental results on both review datasets and news datasets\nshow that our proposed approach achieves text classification results on par\nwith existing state-of-the-art text classifiers, and generates interpretations\nmore faithful to model predictions and better understood by humans than other\ninterpretable neural text classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanqi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset of Stuttering. (arXiv:2203.05383v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.05383","description":"<p>Stuttering is a complex speech disorder that negatively affects an\nindividual's ability to communicate effectively. Persons who stutter (PWS)\noften suffer considerably under the condition and seek help through therapy.\nFluency shaping is a therapy approach where PWSs learn to modify their speech\nto help them to overcome their stutter. Mastering such speech techniques takes\ntime and practice, even after therapy. Shortly after therapy, success is\nevaluated highly, but relapse rates are high. To be able to monitor speech\nbehavior over a long time, the ability to detect stuttering events and\nmodifications in speech could help PWSs and speech pathologists to track the\nlevel of fluency. Monitoring could create the ability to intervene early by\ndetecting lapses in fluency. To the best of our knowledge, no public dataset is\navailable that contains speech from people who underwent stuttering therapy\nthat changed the style of speaking. This work introduces the Kassel State of\nFluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The\nclips were labeled with six stuttering-related event types: blocks,\nprolongations, sound repetitions, word repetitions, interjections, and -\nspecific to therapy - speech modifications. The audio was recorded during\ntherapy sessions at the Institut der Kasseler Stottertherapie. The data will be\nmade available for research purposes upon request.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gudenberg_A/0/1/0/all/0/1\">Alexander Wolff von Gudenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honig_F/0/1/0/all/0/1\">Florian H&#xf6;nig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noth_E/0/1/0/all/0/1\">Elmar N&#xf6;th</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph-Enabled Text-Based Automatic Personality Prediction. (arXiv:2203.09103v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09103","description":"<p>How people think, feel, and behave, primarily is a representation of their\npersonality characteristics. By being conscious of personality characteristics\nof individuals whom we are dealing with or decided to deal with, one can\ncompetently ameliorate the relationship, regardless of its type. With the rise\nof Internet-based communication infrastructures (social networks, forums,\netc.), a considerable amount of human communications take place there. The most\nprominent tool in such communications, is the language in written and spoken\nform that adroitly encodes all those essential personality characteristics of\nindividuals. Text-based Automatic Personality Prediction (APP) is the automated\nforecasting of the personality of individuals based on the generated/exchanged\ntext contents. This paper presents a novel knowledge graph-enabled approach to\ntext-based APP that relies on the Big Five personality traits. To this end,\ngiven a text a knowledge graph which is a set of interlinked descriptions of\nconcepts, was built through matching the input text's concepts with DBpedia\nknowledge base entries. Then, due to achieving more powerful representation the\ngraph was enriched with the DBpedia ontology, NRC Emotion Intensity Lexicon,\nand MRC psycholinguistic database information. Afterwards, the knowledge graph\nwhich is now a knowledgeable alternative for the input text was embedded to\nyield an embedding matrix. Finally, to perform personality predictions the\nresulting embedding matrix was fed to four suggested deep learning models\nindependently, which are based on convolutional neural network (CNN), simple\nrecurrent neural network (RNN), long short term memory (LSTM) and bidirectional\nlong short term memory (BiLSTM). The results indicated a considerable\nimprovements in prediction accuracies in all of the suggested classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohammad-Ali Balafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent. (arXiv:2203.14757v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.14757","description":"<p>We present STUDIES, a new speech corpus for developing a voice agent that can\nspeak in a friendly manner. Humans naturally control their speech prosody to\nempathize with each other. By incorporating this \"empathetic dialogue\" behavior\ninto a spoken dialogue system, we can develop a voice agent that can respond to\na user more naturally. We designed the STUDIES corpus to include a speaker who\nspeaks with empathy for the interlocutor's emotion explicitly. We describe our\nmethodology to construct an empathetic dialogue speech corpus and report the\nanalysis results of the STUDIES corpus. We conducted a text-to-speech\nexperiment to initially investigate how we can develop more natural voice agent\nthat can tune its speaking style corresponding to the interlocutor's emotion.\nThe results show that the use of interlocutor's emotion label and\nconversational context embedding can produce speech with the same degree of\nnaturalness as that synthesized by using the agent's emotion label. Our project\npage of the STUDIES corpus is <a href=\"http://sython.org/Corpus/STUDIES.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1\">Yuki Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_Y/0/1/0/all/0/1\">Yuto Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamichi_S/0/1/0/all/0/1\">Shinnosuke Takamichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tachibana_K/0/1/0/all/0/1\">Kentaro Tachibana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saruwatari_H/0/1/0/all/0/1\">Hiroshi Saruwatari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0. (arXiv:2204.03417v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2204.03417","description":"<p>Stuttering is a varied speech disorder that harms an individual's\ncommunication ability. Persons who stutter (PWS) often use speech therapy to\ncope with their condition. Improving speech recognition systems for people with\nsuch non-typical speech or tracking the effectiveness of speech therapy would\nrequire systems that can detect dysfluencies while at the same time being able\nto detect speech techniques acquired in therapy. This paper shows that\nfine-tuning wav2vec 2.0 [1] for the classification of stuttering on a sizeable\nEnglish corpus containing stuttered speech, in conjunction with multi-task\nlearning, boosts the effectiveness of the general-purpose wav2vec 2.0 features\nfor detecting stuttering in speech; both within and across languages. We\nevaluate our method on FluencyBank , [2] and the German therapy-centric Kassel\nState of Fluency (KSoF) [3] dataset by training Support Vector Machine\nclassifiers using features extracted from the finetuned models for six\ndifferent stuttering-related event types: blocks, prolongations, sound\nrepetitions, word repetitions, interjections, and - specific to therapy -\nspeech modifications. Using embeddings from the fine-tuned models leads to\nrelative classification performance gains up to 27% w.r.t. F1-score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_D/0/1/0/all/0/1\">Dominik Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noth_E/0/1/0/all/0/1\">Elmar N&#xf6;th</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling sign language transcription system with the computer-friendly numerical multilabels. (arXiv:2204.06924v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06924","description":"<p>This paper presents our recent developments in the automatic processing of\nsign language corpora using the Hamburg Sign Language Annotation System\n(HamNoSys). We designed an automated tool to convert HamNoSys annotations into\nnumerical labels for defined initial features of body and hand positions. Our\nproposed numerical multilabels greatly simplify annotations' structure without\nsignificant loss of gloss meaning. These numerical multilabels can potentially\nbe used to feed the machine learning models, which would accelerate the\ndevelopment of vision-based sign language recognition. In addition, this tool\ncan assist experts in the annotation process and help identify semantic errors.\nThe code and sample annotations are publicly available at\n\\url{https://github.com/hearai/parse-hamnosys}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1\">Sylwia Majchrowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plantykow_M/0/1/0/all/0/1\">Marta Plantykow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olech_M/0/1/0/all/0/1\">Milena Olech</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DDXPlus: A New Dataset For Automatic Medical Diagnosis. (arXiv:2205.09148v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09148","description":"<p>There has been a rapidly growing interest in Automatic Symptom Detection\n(ASD) and Automatic Diagnosis (AD) systems in the machine learning research\nliterature, aiming to assist doctors in telemedicine services. These systems\nare designed to interact with patients, collect evidence about their symptoms\nand relevant antecedents, and possibly make predictions about the underlying\ndiseases. Doctors would review the interactions, including the evidence and the\npredictions, collect if necessary additional information from patients, before\ndeciding on next steps. Despite recent progress in this area, an important\npiece of doctors' interactions with patients is missing in the design of these\nsystems, namely the differential diagnosis. Its absence is largely due to the\nlack of datasets that include such information for models to train on. In this\nwork, we present a large-scale synthetic dataset of roughly 1.3 million\npatients that includes a differential diagnosis, along with the ground truth\npathology, symptoms and antecedents, for each patient. Unlike existing datasets\nwhich only contain binary symptoms and antecedents, this dataset also contains\ncategorical and multi-choice symptoms and antecedents useful for efficient data\ncollection. Moreover, some symptoms are organized in a hierarchy, making it\npossible to design systems able to interact with patients in a logical way. As\na proof-of-concept, we extend two existing AD and ASD systems to incorporate\nthe differential diagnosis, and provide empirical evidence that using\ndifferentials as training signals is essential for the efficiency of such\nsystems. The dataset is available at\n\\href{https://figshare.com/articles/dataset/DDXPlus_Dataset/20043374}{https://figshare.com/articles/dataset/DDXPlus\\_Dataset/20043374}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tchango_A/0/1/0/all/0/1\">Arsene Fansi Tchango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rishab Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_J/0/1/0/all/0/1\">Julien Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosn_J/0/1/0/all/0/1\">Joumana Ghosn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation. (arXiv:2206.05807v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05807","description":"<p>Simultaneous speech translation (SimulST) systems aim at generating their\noutput with the lowest possible latency, which is normally computed in terms of\nAverage Lagging (AL). In this paper we highlight that, despite its widespread\nadoption, AL provides underestimated scores for systems that generate longer\npredictions compared to the corresponding references. We also show that this\nproblem has practical relevance, as recent SimulST systems have indeed a\ntendency to over-generate. As a solution, we propose LAAL (Length-Adaptive\nAverage Lagging), a modified version of the metric that takes into account the\nover-generation phenomenon and allows for unbiased evaluation of both\nunder-/over-generating systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v2 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/2206.06924","description":"<p>The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping\n$\\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers\nthat maximizes $D_{\\pi}(G)=\\sum_{uv\\in E(G)}|\\pi(u) - \\pi(v)|$. In this\nsetting, vertices are considered to lie on a horizontal line and edges are\ndrawn as semicircles above the line. There exist variants of MaxLA in which the\narrangements are constrained. In the planar variant edge crossings are\nforbidden. In the projective variant for rooted trees arrangements are planar\nand the root cannot be covered by any edge. Here we present $O(n)$-time and\n$O(n)$-space algorithms that solve Planar and Projective MaxLA for trees. We\nalso prove several properties of maximum projective and planar arrangements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation. (arXiv:2206.07359v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07359","description":"<p>In emotion recognition in conversation (ERC), the emotion of the current\nutterance is predicted by considering the previous context, which can be\nutilized in many natural language processing tasks. Although multiple emotions\ncan coexist in a given sentence, most previous approaches take the perspective\nof a classification task to predict only a given label. However, it is\nexpensive and difficult to label the emotion of a sentence with confidence or\nmulti-label. In this paper, we automatically construct a grayscale label\nconsidering the correlation between emotions and use it for learning. That is,\ninstead of using a given label as a one-hot encoding, we construct a grayscale\nlabel by measuring scores for different emotions. We introduce several methods\nfor constructing grayscale labels and confirm that each method improves the\nemotion recognition performance. Our method is simple, effective, and\nuniversally applicable to previous systems. The experiments show a significant\nimprovement in the performance of baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Improving Diversity with Adversarially Learned Transformations for Domain Generalization. (arXiv:2206.07736v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07736","description":"<p>To be successful in single source domain generalization, maximizing diversity\nof synthesized domains has emerged as one of the most effective strategies.\nMany of the recent successes have come from methods that pre-specify the types\nof diversity that a model is exposed to during training, so that it can\nultimately generalize well to new domains. However, na\\\"ive diversity based\naugmentations do not work effectively for domain generalization either because\nthey cannot model large domain shift, or because the span of transforms that\nare pre-specified do not cover the types of shift commonly occurring in domain\ngeneralization. To address this issue, we present a novel framework that uses\nadversarially learned transformations (ALT) using a neural network to model\nplausible, yet hard image transformations that fool the classifier. This\nnetwork is randomly initialized for each batch and trained for a fixed number\nof steps to maximize classification error. Further, we enforce consistency\nbetween the classifier's predictions on the clean and transformed images. With\nextensive empirical analysis, we find that this new form of adversarial\ntransformations achieve both objectives of diversity and hardness\nsimultaneously, outperforming all existing techniques on competitive benchmarks\nfor single source domain generalization. We also show that ALT can naturally\nwork with existing diversity modules to produce highly distinct, and large\ntransformations of the source domain leading to state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks. (arXiv:2206.07741v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07741","description":"<p>The large computing and memory cost of deep neural networks (DNNs) often\nprecludes their use in resource-constrained devices. Quantizing the parameters\nand operations to lower bit-precision offers substantial memory and energy\nsavings for neural network inference, facilitating the use of DNNs on edge\ncomputing platforms. Recent efforts at quantizing DNNs have employed a range of\ntechniques encompassing progressive quantization, step-size adaptation, and\ngradient scaling. This paper proposes a new quantization approach for mixed\nprecision convolutional neural networks (CNNs) targeting edge-computing. Our\nmethod establishes a new pareto frontier in model accuracy and memory footprint\ndemonstrating a range of quantized models, delivering best-in-class accuracy\nbelow 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions\nare: (i) hardware-aware heterogeneous differentiable quantization with\ntensor-sliced learned precision, (ii) targeted gradient modification for wgts.\nand acts. to mitigate quantization errors, and (iii) a multi-phase learning\nschedule to address instability in learning arising from updates to the learned\nquantizer and model parameters. We demonstrate the effectiveness of our\ntechniques on the ImageNet dataset across a range of models including\nEfficientNet-Lite0 (e.g., 4.14MB of wgts. and acts. at 67.66% accuracy) and\nMobileNetV2 (e.g., 3.51MB wgts. and acts. at 65.39% accuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_C/0/1/0/all/0/1\">Clemens JS Schaefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Siddharth Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blazquez_R/0/1/0/all/0/1\">Raul Blazquez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing Training Data from Trained Neural Networks. (arXiv:2206.07758v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07758","description":"<p>Understanding to what extent neural networks memorize training data is an\nintriguing question with practical and theoretical implications. In this paper\nwe show that in some cases a significant fraction of the training data can in\nfact be reconstructed from the parameters of a trained neural network\nclassifier. We propose a novel reconstruction scheme that stems from recent\ntheoretical results about the implicit bias in training neural networks with\ngradient-based methods. To the best of our knowledge, our results are the first\nto show that reconstructing a large portion of the actual training samples from\na trained neural network classifier is generally possible. This has negative\nimplications on privacy, as it can be used as an attack for revealing sensitive\ntraining data. We demonstrate our method for binary MLP classifiers on a few\nstandard computer vision datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haim_N/0/1/0/all/0/1\">Niv Haim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1\">Gal Vardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1\">Gilad Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAVi++: Towards End-to-End Object-Centric Learning from Real-World Videos. (arXiv:2206.07764v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07764","description":"<p>The visual world can be parsimoniously characterized in terms of distinct\nentities with sparse interactions. Discovering this compositional structure in\ndynamic visual scenes has proven challenging for end-to-end computer vision\napproaches unless explicit instance-level supervision is provided. Slot-based\nmodels leveraging motion cues have recently shown great promise in learning to\nrepresent, segment, and track objects without direct supervision, but they\nstill fail to scale to complex real-world multi-object videos. In an effort to\nbridge this gap, we take inspiration from human development and hypothesize\nthat information about scene geometry in the form of depth signals can\nfacilitate object-centric learning. We introduce SAVi++, an object-centric\nvideo model which is trained to predict depth signals from a slot-based video\nrepresentation. By further leveraging best practices for model scaling, we are\nable to train SAVi++ to segment complex dynamic scenes recorded with moving\ncameras, containing both static and moving objects of diverse appearance on\nnaturalistic backgrounds, without the need for segmentation supervision.\nFinally, we demonstrate that by using sparse depth signals obtained from LiDAR,\nSAVi++ is able to learn emergent object segmentation and tracking from videos\nin the real-world Waymo Open dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1\">Gamaleldin F. Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendran_A/0/1/0/all/0/1\">Aravindh Mahendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenkiste_S/0/1/0/all/0/1\">Sjoerd van Steenkiste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greff_K/0/1/0/all/0/1\">Klaus Greff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1\">Michael C. Mozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation. (arXiv:2206.07771v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07771","description":"<p>Diffusion probabilistic models (DPMs) have become a popular approach to\nconditional generation, due to their promising results and support for\ncross-modal synthesis. A key desideratum in conditional synthesis is to achieve\nhigh correspondence between the conditioning input and generated output. Most\nexisting methods learn such relationships implicitly, by incorporating the\nprior into the variational lower bound. In this work, we take a different route\n-- we enhance input-output connections by maximizing their mutual information\nusing contrastive learning. To this end, we introduce a Conditional Discrete\nContrastive Diffusion (CDCD) loss and design two contrastive diffusion\nmechanisms to effectively incorporate it into the denoising process. We\nformulate CDCD by connecting it with the conventional variational objectives.\nWe demonstrate the efficacy of our approach in evaluations with three diverse,\nmultimodal conditional synthesis tasks: dance-to-music generation,\ntext-to-image synthesis, and class-conditioned image synthesis. On each, we\nachieve state-of-the-art or higher synthesis quality and improve the\ninput-output correspondence. Furthermore, the proposed approach improves the\nconvergence of diffusion models, reducing the number of required diffusion\nsteps by more than 35% on two benchmarks, significantly increasing the\ninference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Calibrated Model Uncertainty in Deep Learning. (arXiv:2206.07795v1 [cs.LG])","link":"http://arxiv.org/abs/2206.07795","description":"<p>Estimated uncertainty by approximate posteriors in Bayesian neural networks\nare prone to miscalibration, which leads to overconfident predictions in\ncritical tasks that have a clear asymmetric cost or significant losses. Here,\nwe extend the approximate inference for the loss-calibrated Bayesian framework\nto dropweights based Bayesian neural networks by maximising expected utility\nover a model posterior to calibrate uncertainty in deep learning. Furthermore,\nwe show that decisions informed by loss-calibrated uncertainty can improve\ndiagnostic performance to a greater extent than straightforward alternatives.\nWe propose Maximum Uncertainty Calibration Error (MUCE) as a metric to measure\ncalibrated confidence, in addition to its prediction especially for high-risk\napplications, where the goal is to minimise the worst-case deviation between\nerror and estimated uncertainty. In experiments, we show the correlation\nbetween error in prediction and estimated uncertainty by interpreting\nWasserstein distance as the accuracy of prediction. We evaluated the\neffectiveness of our approach to detecting Covid-19 from X-Ray images.\nExperimental results show that our method reduces miscalibration considerably,\nwithout impacting the models accuracy and improves reliability of\ncomputer-based diagnostics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghoshal_B/0/1/0/all/0/1\">Biraja Ghoshal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_A/0/1/0/all/0/1\">Allan Tucker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What makes domain generalization hard?. (arXiv:2206.07802v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07802","description":"<p>While several methodologies have been proposed for the daunting task of\ndomain generalization, understanding what makes this task challenging has\nreceived little attention. Here we present SemanticDG (Semantic Domain\nGeneralization): a benchmark with 15 photo-realistic domains with the same\ngeometry, scene layout and camera parameters as the popular 3D ScanNet dataset,\nbut with controlled domain shifts in lighting, materials, and viewpoints. Using\nthis benchmark, we investigate the impact of each of these semantic shifts on\ngeneralization independently. Visual recognition models easily generalize to\nnovel lighting, but struggle with distribution shifts in materials and\nviewpoints. Inspired by human vision, we hypothesize that scene context can\nserve as a bridge to help models generalize across material and viewpoint\ndomain shifts and propose a context-aware vision transformer along with a\ncontrastive loss over material and viewpoint changes to address these domain\nshifts. Our approach (dubbed as CDCNet) outperforms existing domain\ngeneralization methods by over an 18% margin. As a critical benchmark, we also\nconduct psychophysics experiments and find that humans generalize equally well\nacross lighting, materials and viewpoints. The benchmark and computational\nmodel introduced here help understand the challenges associated with\ngeneralization across domains and provide initial steps towards extrapolation\nto semantic distribution shifts. We include all data and source code in the\nsupplement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_L/0/1/0/all/0/1\">Li You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling visual and written concepts in CLIP. (arXiv:2206.07835v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07835","description":"<p>The CLIP network measures the similarity between natural text and images; in\nthis work, we investigate the entanglement of the representation of word images\nand natural images in its image encoder. First, we find that the image encoder\nhas an ability to match word images with natural images of scenes described by\nthose words. This is consistent with previous research that suggests that the\nmeaning and the spelling of a word might be entangled deep within the network.\nOn the other hand, we also find that CLIP has a strong ability to match\nnonsense words, suggesting that processing of letters is separated from\nprocessing of their meaning. To explicitly determine whether the spelling\ncapability of CLIP is separable, we devise a procedure for identifying\nrepresentation subspaces that selectively isolate or eliminate spelling\ncapabilities. We benchmark our methods against a range of retrieval tasks, and\nwe also test them by measuring the appearance of text in CLIP-guided generated\nimages. We find that our methods are able to cleanly separate spelling\ncapabilities of CLIP from the visual processing of natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1\">Joanna Materzynska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action Spotting using Dense Detection Anchors Revisited: Submission to the SoccerNet Challenge 2022. (arXiv:2206.07846v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07846","description":"<p>This technical report describes our submission to the Action Spotting\nSoccerNet Challenge 2022. The challenge is part of the CVPR 2022 ActivityNet\nWorkshop. Our submission is based on a method that we proposed recently, which\nfocuses on increasing temporal precision via a densely sampled set of detection\nanchors. Due to its emphasis on temporal precision, this approach is able to\nproduce competitive results on the tight average-mAP metric, which uses small\ntemporal evaluation tolerances. This recently proposed metric is the evaluation\ncriterion used for the challenge. In order to further improve results, here we\nintroduce small changes in the pre- and post-processing steps, and also combine\ndifferent input feature types via late fusion. This report describes the\nresulting overall approach, focusing on the modifications introduced. We also\ndescribe the training procedures used, and present our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soares_J/0/1/0/all/0/1\">Jo&#xe3;o V. B. Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Avijit Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved surface reconstruction using high-frequency details. (arXiv:2206.07850v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07850","description":"<p>Neural rendering can be used to reconstruct implicit representations of\nshapes without 3D supervision. However, current neural surface reconstruction\nmethods have difficulty learning high-frequency details of shapes, so that the\nreconstructed shapes are often oversmoothed. We propose a novel method to\nimprove the quality of surface reconstruction in neural rendering. We follow\nrecent work to model surfaces as signed distance fields. First, we offer a\nderivation to analyze the relationship between the signed distance function,\nthe volume density, the transparency function, and the weighting function used\nin the volume rendering equation. Second, we observe that attempting to jointly\nencode high-frequency and low frequency components in a single signed distance\nfunction leads to unstable optimization. We propose to decompose the signed\ndistance function in a base function and a displacement function together with\na coarse-to-fine strategy to gradually increase the high-frequency details.\nFinally, we propose to use an adaptive strategy that enables the optimization\nto focus on improving certain regions near the surface where the signed\ndistance fields have artifacts. Our qualitative and quantitative results show\nthat our method can reconstruct high-frequency surface details and obtain\nbetter surface reconstruction quality than the current state of the art. Code\nwill be released at https://github.com/yiqun-wang/HFS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PeQuENet: Perceptual Quality Enhancement of Compressed Video with Adaptation- and Attention-based Network. (arXiv:2206.07893v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07893","description":"<p>In this paper we propose a generative adversarial network (GAN) framework to\nenhance the perceptual quality of compressed videos. Our framework includes\nattention and adaptation to different quantization parameters (QPs) in a single\nmodel. The attention module exploits global receptive fields that can capture\nand align long-range correlations between consecutive frames, which can be\nbeneficial for enhancing perceptual quality of videos. The frame to be enhanced\nis fed into the deep network together with its neighboring frames, and in the\nfirst stage features at different depths are extracted. Then extracted features\nare fed into attention blocks to explore global temporal correlations, followed\nby a series of upsampling and convolution layers. Finally, the resulting\nfeatures are processed by the QP-conditional adaptation module which leverages\nthe corresponding QP information. In this way, a single model can be used to\nenhance adaptively to various QPs without requiring multiple models specific\nfor every QP value, while having similar performance. Experimental results\ndemonstrate the superior performance of the proposed PeQuENet compared with the\nstate-of-the-art compressed video quality enhancement algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Saiping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanch_M/0/1/0/all/0/1\">Marc Gorriz Blanch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Shuai Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fuzheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Contrastive Attributed Graph Clustering Network. (arXiv:2206.07897v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07897","description":"<p>Attributed graph clustering is one of the most important tasks in graph\nanalysis field, the goal of which is to group nodes with similar\nrepresentations into the same cluster without manual guidance. Recent studies\nbased on graph contrastive learning have achieved impressive results in\nprocessing graph-structured data. However, existing graph contrastive learning\nbased methods 1) do not directly address the clustering task, since the\nrepresentation learning and clustering process are separated; 2) depend too\nmuch on graph data augmentation, which greatly limits the capability of\ncontrastive learning; 3) ignore the contrastive message for subspace\nclustering. To accommodate the aforementioned issues, we propose a generic\nframework called Dual Contrastive Attributed Graph Clustering Network (DCAGC).\nIn DCAGC, by leveraging Neighborhood Contrast Module, the similarity of the\nneighbor nodes will be maximized and the quality of the node representation\nwill be improved. Meanwhile, the Contrastive Self-Expression Module is built by\nminimizing the node representation before and after the reconstruction of the\nself-expression layer to obtain a discriminative self-expression matrix for\nspectral clustering. All the modules of DCAGC are trained and optimized in a\nunified framework, so the learned node representation contains\nclustering-oriented messages. Extensive experimental results on four attributed\ngraph datasets show the superiority of DCAGC compared with 16 state-of-the-art\nclustering methods. The code of this paper is available at\nhttps://github.com/wangtong627/Dual-Contrastive-Attributed-Graph-Clustering-Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qijia He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenquan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dialogue State Tracking. (arXiv:2206.07898v1 [cs.AI])","link":"http://arxiv.org/abs/2206.07898","description":"<p>Designed for tracking user goals in dialogues, a dialogue state tracker is an\nessential component in a dialogue system. However, the research of dialogue\nstate tracking has largely been limited to unimodality, in which slots and slot\nvalues are limited by knowledge domains (e.g. restaurant domain with slots of\nrestaurant name and price range) and are defined by specific database schema.\nIn this paper, we propose to extend the definition of dialogue state tracking\nto multimodality. Specifically, we introduce a novel dialogue state tracking\ntask to track the information of visual objects that are mentioned in\nvideo-grounded dialogues. Each new dialogue utterance may introduce a new video\nsegment, new visual objects, or new object attributes, and a state tracker is\nrequired to update these information slots accordingly. We created a new\nsynthetic benchmark and designed a novel baseline, Video-Dialogue Transformer\nNetwork (VDTN), for this task. VDTN combines both object-level features and\nsegment-level features and learns contextual dependencies between videos and\ndialogues to generate multimodal dialogue states. We optimized VDTN for a state\ngeneration task as well as a self-supervised video understanding task which\nrecovers video segment or object representations. Finally, we trained VDTN to\nuse the decoded states in a response prediction task. Together with\ncomprehensive ablation and qualitative analysis, we discovered interesting\ninsights towards building more capable multimodal dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Wandering: A realistic few-shot online continual learning setting. (arXiv:2206.07932v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07932","description":"<p>Online few-shot learning describes a setting where models are trained and\nevaluated on a stream of data while learning emerging classes. While prior work\nin this setting has achieved very promising performance on instance\nclassification when learning from data-streams composed of a single indoor\nenvironment, we propose to extend this setting to consider object\nclassification on a series of several indoor environments, which is likely to\noccur in applications such as robotics. Importantly, our setting, which we\nrefer to as online few-shot continual learning, injects the well-studied issue\nof catastrophic forgetting into the few-shot online learning paradigm. In this\nwork, we benchmark several existing methods and adapted baselines within our\nsetting, and show there exists a trade-off between catastrophic forgetting and\nonline performance. Our findings motivate the need for future work in this\nsetting, which can achieve better online performance without catastrophic\nforgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lunayach_M/0/1/0/all/0/1\">Mayank Lunayach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">James Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical Report for Argoverse2 Challenge 2022 -- Motion Forecasting Task. (arXiv:2206.07934v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07934","description":"<p>We propose a motion forecasting model called BANet, which means\nBoundary-Aware Network, and it is a variant of LaneGCN. We believe that it is\nnot enough to use only the lane centerline as input to obtain the embedding\nfeatures of the vector map nodes. The lane centerline can only provide the\ntopology of the lanes, and other elements of the vector map also contain rich\ninformation. For example, the lane boundary can provide traffic rule constraint\ninformation such as whether it is possible to change lanes which is very\nimportant. Therefore, we achieved better performance by encoding more vector\nmap elements in the motion forecasting model.We report our results on the 2022\nArgoverse2 Motion Forecasting challenge and rank 2nd on the test leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Honglin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis and Extensions of Adversarial Training for Video Classification. (arXiv:2206.07953v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07953","description":"<p>Adversarial training (AT) is a simple yet effective defense against\nadversarial attacks to image classification systems, which is based on\naugmenting the training set with attacks that maximize the loss. However, the\neffectiveness of AT as a defense for video classification has not been\nthoroughly studied. Our first contribution is to show that generating optimal\nattacks for video requires carefully tuning the attack parameters, especially\nthe step size. Notably, we show that the optimal step size varies linearly with\nthe attack budget. Our second contribution is to show that using a smaller\n(sub-optimal) attack budget at training time leads to a more robust performance\nat test time. Based on these findings, we propose three defenses against\nattacks with variable attack budgets. The first one, Adaptive AT, is a\ntechnique where the attack budget is drawn from a distribution that is adapted\nas training iterations proceed. The second, Curriculum AT, is a technique where\nthe attack budget is increased as training iterations proceed. The third,\nGenerative AT, further couples AT with a denoising generative adversarial\nnetwork to boost robust performance. Experiments on the UCF101 dataset\ndemonstrate that the proposed methods improve adversarial robustness against\nmultiple attack types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kinfu_K/0/1/0/all/0/1\">Kaleab A. Kinfu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Ren&#xe9; Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Baseline for BEV Perception Without LiDAR. (arXiv:2206.07959v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07959","description":"<p>Building 3D perception systems for autonomous vehicles that do not rely on\nLiDAR is a critical research problem because of the high expense of LiDAR\nsystems compared to cameras and other sensors. Current methods use multi-view\nRGB data collected from cameras around the vehicle and neurally \"lift\" features\nfrom the perspective images to the 2D ground plane, yielding a \"bird's eye\nview\" (BEV) feature representation of the 3D space around the vehicle. Recent\nresearch focuses on the way the features are lifted from images to the BEV\nplane. We instead propose a simple baseline model, where the \"lifting\" step\nsimply averages features from all projected image locations, and find that it\noutperforms the current state-of-the-art in BEV vehicle segmentation. Our\nablations show that batch size, data augmentation, and input resolution play a\nlarge part in performance. Additionally, we reconsider the utility of radar\ninput, which has previously been either ignored or found non-helpful by recent\nworks. With a simple RGB-radar fusion module, we obtain a sizable boost in\nperformance, approaching the accuracy of a LiDAR-enabled system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhaoyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DreamNet: A Deep Riemannian Network based on SPD Manifold Learning for Visual Classification. (arXiv:2206.07967v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07967","description":"<p>Image set-based visual classification methods have achieved remarkable\nperformance, via characterising the image set in terms of a non-singular\ncovariance matrix on a symmetric positive definite (SPD) manifold. To adapt to\ncomplicated visual scenarios better, several Riemannian networks (RiemNets) for\nSPD matrix nonlinear processing have recently been studied. However, it is\npertinent to ask, whether greater accuracy gains can be achieved by simply\nincreasing the depth of RiemNets. The answer appears to be negative, as deeper\nRiemNets tend to lose generalization ability. To explore a possible solution to\nthis issue, we propose a new architecture for SPD matrix learning.\nSpecifically, to enrich the deep representations, we adopt SPDNet [1] as the\nbackbone, with a stacked Riemannian autoencoder (SRAE) built on the tail. The\nassociated reconstruction error term can make the embedding functions of both\nSRAE and of each RAE an approximate identity mapping, which helps to prevent\nthe degradation of statistical information. We then insert several\nresidual-like blocks with shortcut connections to augment the representational\ncapacity of SRAE, and to simplify the training of a deeper network. The\nexperimental evidence demonstrates that our DreamNet can achieve improved\naccuracy with increased depth of the network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos. (arXiv:2206.07981v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07981","description":"<p>Multimodal sentiment analysis in videos is a key task in many real-world\napplications, which usually requires integrating multimodal streams including\nvisual, verbal and acoustic behaviors. To improve the robustness of multimodal\nfusion, some of the existing methods let different modalities communicate with\neach other and modal the crossmodal interaction via transformers. However,\nthese methods only use the single-scale representations during the interaction\nbut forget to exploit multi-scale representations that contain different levels\nof semantic information. As a result, the representations learned by\ntransformers could be biased especially for unaligned multimodal data. In this\npaper, we propose a multi-scale cooperative multimodal transformer (MCMulT)\narchitecture for multimodal sentiment analysis. On the whole, the \"multi-scale\"\nmechanism is capable of exploiting the different levels of semantic information\nof each modality which are used for fine-grained crossmodal interactions.\nMeanwhile, each modality learns its feature hierarchies via integrating the\ncrossmodal interactions from multiple level features of its source modality. In\nthis way, each pair of modalities progressively builds feature hierarchies\nrespectively in a cooperative manner. The empirical results illustrate that our\nMCMulT model not only outperforms existing approaches on unaligned multimodal\nsequences but also has strong performance on aligned multimodal sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lianyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Captioning based on Feature Refinement and Reflective Decoding. (arXiv:2206.07986v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07986","description":"<p>Automatically generating a description of an image in natural language is\ncalled image captioning. It is an active research topic that lies at the\nintersection of two major fields in artificial intelligence, computer vision,\nand natural language processing. Image captioning is one of the significant\nchallenges in image understanding since it requires not only recognizing\nsalient objects in the image but also their attributes and the way they\ninteract. The system must then generate a syntactically and semantically\ncorrect caption that describes the image content in natural language. With the\nsignificant progress in deep learning models and their ability to effectively\nencode large sets of images and generate correct sentences, several\nneural-based captioning approaches have been proposed recently, each trying to\nachieve better accuracy and caption quality. This paper introduces an\nencoder-decoder-based image captioning system in which the encoder extracts\nspatial and global features for each region in the image using the Faster R-CNN\nwith ResNet-101 as a backbone. This stage is followed by a refining model,\nwhich uses an attention-on-attention mechanism to extract the visual features\nof the target image objects, then determine their interactions. The decoder\nconsists of an attention-based recurrent module and a reflective attention\nmodule, which collaboratively apply attention to the visual and textual\nfeatures to enhance the decoder's ability to model long-term sequential\ndependencies. Extensive experiments performed on two benchmark datasets, MSCOCO\nand Flickr30K, show the effectiveness the proposed approach and the high\nquality of the generated captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alabduljabbar_G/0/1/0/all/0/1\">Ghadah Alabduljabbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhidour_H/0/1/0/all/0/1\">Hafida Benhidour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerrache_S/0/1/0/all/0/1\">Said Kerrache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07990","description":"<p>Recent self-supervised learning (SSL) methods have shown impressive results\nin learning visual representations from unlabeled images. This paper aims to\nimprove their performance further by utilizing the architectural advantages of\nthe underlying neural network, as the current state-of-the-art visual pretext\ntasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.\nIn particular, we focus on Vision Transformers (ViTs), which have gained much\nattention recently as a better architectural choice, often outperforming\nconvolutional networks for various visual tasks. The unique characteristic of\nViT is that it takes a sequence of disjoint patches from an image and processes\npatch-level representations internally. Inspired by this, we design a simple\nyet effective visual pretext task, coined SelfPatch, for learning better\npatch-level representations. To be specific, we enforce invariance against each\npatch and its neighbors, i.e., each patch treats similar neighboring patches as\npositive samples. Consequently, training ViTs with SelfPatch learns more\nsemantically meaningful relations among patches (without using human-annotated\nlabels), which can be beneficial, in particular, to downstream tasks of a dense\nprediction type. Despite its simplicity, we demonstrate that it can\nsignificantly improve the performance of existing SSL methods for various\nvisual tasks, including object detection and semantic segmentation.\nSpecifically, SelfPatch significantly improves the recent self-supervised ViT,\nDINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance\nsegmentation, and +2.9 mIoU on ADE20K semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sukmin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hankook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Class-Affinity Loss Correction for Robust Medical Image Segmentation with Noisy Labels. (arXiv:2206.07994v1 [cs.CV])","link":"http://arxiv.org/abs/2206.07994","description":"<p>Noisy labels collected with limited annotation cost prevent medical image\nsegmentation algorithms from learning precise semantic correlations. Previous\nsegmentation arts of learning with noisy labels merely perform a pixel-wise\nmanner to preserve semantics, such as pixel-wise label correction, but neglect\nthe pair-wise manner. In fact, we observe that the pair-wise manner capturing\naffinity relations between pixels can greatly reduce the label noise rate.\nMotivated by this observation, we present a novel perspective for noisy\nmitigation by incorporating both pixel-wise and pair-wise manners, where\nsupervisions are derived from noisy class and affinity labels, respectively.\nUnifying the pixel-wise and pair-wise manners, we propose a robust Joint\nClass-Affinity Segmentation (JCAS) framework to combat label noise issues in\nmedical image segmentation. Considering the affinity in pair-wise manner\nincorporates contextual dependencies, a differentiated affinity reasoning (DAR)\nmodule is devised to rectify the pixel-wise segmentation prediction by\nreasoning about intra-class and inter-class affinity relations. To further\nenhance the noise resistance, a class-affinity loss correction (CALC) strategy\nis designed to correct supervision signals via the modeled noise label\ndistributions in class and affinity labels. Meanwhile, CALC strategy interacts\nthe pixel-wise and pair-wise manners through the theoretically derived\nconsistency regularization. Extensive experiments under both synthetic and\nreal-world noisy labels corroborate the efficacy of the proposed JCAS framework\nwith a minimum gap towards the upper bound performance. The source code is\navailable at \\url{https://github.com/CityU-AIM-Group/JCAS}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoqing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Discriminability and Transferability for Source-Free Domain Adaptation. (arXiv:2206.08009v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08009","description":"<p>Conventional domain adaptation (DA) techniques aim to improve domain\ntransferability by learning domain-invariant representations; while\nconcurrently preserving the task-discriminability knowledge gathered from the\nlabeled source data. However, the requirement of simultaneous access to labeled\nsource and unlabeled target renders them unsuitable for the challenging\nsource-free DA setting. The trivial solution of realizing an effective original\nto generic domain mapping improves transferability but degrades task\ndiscriminability. Upon analyzing the hurdles from both theoretical and\nempirical standpoints, we derive novel insights to show that a mixup between\noriginal and corresponding translated generic samples enhances the\ndiscriminability-transferability trade-off while duly respecting the\nprivacy-oriented source-free setting. A simple but effective realization of the\nproposed insights on top of the existing source-free DA approaches yields\nstate-of-the-art performance with faster convergence. Beyond single-source, we\nalso outperform multi-source prior-arts across both classification and semantic\nsegmentation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_J/0/1/0/all/0/1\">Jogendra Nath Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Akshay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1\">Suvaansh Bhambri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Deepesh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1\">Shreyas Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoDi: Unconditional Motion Synthesis from Diverse Data. (arXiv:2206.08010v1 [cs.GR])","link":"http://arxiv.org/abs/2206.08010","description":"<p>The emergence of neural networks has revolutionized the field of motion\nsynthesis. Yet, learning to unconditionally synthesize motions from a given\ndistribution remains a challenging task, especially when the motions are highly\ndiverse. We present MoDi, an unconditional generative model that synthesizes\ndiverse motions. Our model is trained in a completely unsupervised setting from\na diverse, unstructured and unlabeled motion dataset and yields a well-behaved,\nhighly semantic latent space. The design of our model follows the prolific\narchitecture of StyleGAN and adapts two of its key technical components into\nthe motion domain: a set of style-codes injected into each level of the\ngenerator hierarchy and a mapping function that learns and forms a disentangled\nlatent space. We show that despite the lack of any structure in the dataset,\nthe latent space can be semantically clustered, and facilitates semantic\nediting and motion interpolation. In addition, we propose a technique to invert\nunseen motions into the latent space, and demonstrate latent-based motion\nediting operations that otherwise cannot be achieved by naive manipulation of\nexplicit motion representations. Our qualitative and quantitative experiments\nshow that our framework achieves state-of-the-art synthesis quality that can\nfollow the distribution of highly diverse motion datasets. Code and trained\nmodels will be released at https://sigal-raab.github.io/MoDi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raab_S/0/1/0/all/0/1\">Sigal Raab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibovitch_I/0/1/0/all/0/1\">Inbal Leibovitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peizhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkine_Hornung_O/0/1/0/all/0/1\">Olga Sorkine-Hornung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backbones-Review: Feature Extraction Networks for Deep Learning and Deep Reinforcement Learning Approaches. (arXiv:2206.08016v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08016","description":"<p>To understand the real world using various types of data, Artificial\nIntelligence (AI) is the most used technique nowadays. While finding the\npattern within the analyzed data represents the main task. This is performed by\nextracting representative features step, which is proceeded using the\nstatistical algorithms or using some specific filters. However, the selection\nof useful features from large-scale data represented a crucial challenge. Now,\nwith the development of convolution neural networks (CNNs), the feature\nextraction operation has become more automatic and easier. CNNs allow to work\non large-scale size of data, as well as cover different scenarios for a\nspecific task. For computer vision tasks, convolutional networks are used to\nextract features also for the other parts of a deep learning model. The\nselection of a suitable network for feature extraction or the other parts of a\nDL model is not random work. So, the implementation of such a model can be\nrelated to the target task as well as the computational complexity of it. Many\nnetworks have been proposed and become the famous networks used for any DL\nmodels in any AI task. These networks are exploited for feature extraction or\nat the beginning of any DL model which is named backbones. A backbone is a\nknown network trained in many other tasks before and demonstrates its\neffectiveness. In this paper, an overview of the existing backbones, e.g. VGGs,\nResNets, DenseNet, etc, is given with a detailed description. Also, a couple of\ncomputer vision tasks are discussed by providing a review of each task\nregarding the backbones used. In addition, a comparison in terms of performance\nis also provided, based on the backbone used for each task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elharrouss_O/0/1/0/all/0/1\">Omar Elharrouss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_Y/0/1/0/all/0/1\">Younes Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almaadeed_N/0/1/0/all/0/1\">Noor Almaadeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Maadeed_S/0/1/0/all/0/1\">Somaya Al-Maadeed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Imputation and Cross-Attention Network Based on Incomplete Longitudinal and Multi-Modal Data for Alzheimer's Disease Prediction. (arXiv:2206.08019v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08019","description":"<p>Longitudinal variations and complementary information inherent in\nlongitudinal and multi-modal data play an important role in Alzheimer's disease\n(AD) prediction, particularly in identifying subjects with mild cognitive\nimpairment who are about to have AD. However, longitudinal and multi-modal data\nmay have missing data, which hinders the effective application of these data.\nAdditionally, previous longitudinal studies require existing longitudinal data\nto achieve prediction, but AD prediction is expected to be conducted at\npatients' baseline visit (BL) in clinical practice. Thus, we proposed a\nmulti-view imputation and cross-attention network (MCNet) to integrate data\nimputation and AD prediction in a unified framework and achieve accurate AD\nprediction. First, a multi-view imputation method combined with adversarial\nlearning, which can handle a wide range of missing data situations and reduce\nimputation errors, was presented. Second, two cross-attention blocks were\nintroduced to exploit the potential associations in longitudinal and\nmulti-modal data. Finally, a multi-task learning model was built for data\nimputation, longitudinal classification, and AD prediction tasks. When the\nmodel was properly trained, the disease progression information learned from\nlongitudinal data can be leveraged by BL data to improve AD prediction. The\nproposed method was tested on two independent testing sets and single-model\ndata at BL to verify its effectiveness and flexibility on AD prediction.\nResults showed that MCNet outperformed several state-of-the-art methods.\nMoreover, the interpretability of MCNet was presented. Thus, our MCNet is a\ntool with a great application potential in longitudinal and multi-modal data\nanalysis for AD prediction. Codes are available at\nhttps://github.com/Meiyan88/MCNET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1\">Meiyan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiumei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoling Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shuoling Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_Q/0/1/0/all/0/1\">Qianjin Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation. (arXiv:2206.08023v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08023","description":"<p>Despite the considerable progress in automatic abdominal multi-organ\nsegmentation from CT/MRI scans in recent years, a comprehensive evaluation of\nthe models' capabilities is hampered by the lack of a large-scale benchmark\nfrom diverse clinical scenarios. Constraint by the high cost of collecting and\nlabeling 3D medical data, most of the deep learning models to date are driven\nby datasets with a limited number of organs of interest or samples, which still\nlimits the power of modern deep models and makes it difficult to provide a\nfully comprehensive and fair estimate of various methods. To mitigate the\nlimitations, we present AMOS, a large-scale, diverse, clinical dataset for\nabdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected\nfrom multi-center, multi-vendor, multi-modality, multi-phase, multi-disease\npatients, each with voxel-level annotations of 15 abdominal organs, providing\nchallenging examples and test-bed for studying robust segmentation algorithms\nunder diverse targets and scenarios. We further benchmark several\nstate-of-the-art medical segmentation models to evaluate the status of the\nexisting methods on this new challenging dataset. We have made our datasets,\nbenchmark servers, and baselines publicly available, and hope to inspire future\nresearch. Information can be found at https://amos22.grand-challenge.org.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ji_Y/0/1/0/all/0/1\">Yuanfeng Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1\">Haotian Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_C/0/1/0/all/0/1\">Chongjian Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lingyan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1\">Wanling Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFormableTag: End-to-end Generation and Recognition of Deformable Fiducial Markers. (arXiv:2206.08026v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08026","description":"<p>Fiducial markers have been broadly used to identify objects or embed messages\nthat can be detected by a camera. Primarily, existing detection methods assume\nthat markers are printed on ideally planar surfaces. Markers often fail to be\nrecognized due to various imaging artifacts of optical/perspective distortion\nand motion blur. To overcome these limitations, we propose a novel deformable\nfiducial marker system that consists of three main parts: First, a fiducial\nmarker generator creates a set of free-form color patterns to encode\nsignificantly large-scale information in unique visual codes. Second, a\ndifferentiable image simulator creates a training dataset of photorealistic\nscene images with the deformed markers, being rendered during optimization in a\ndifferentiable manner. The rendered images include realistic shading with\nspecular reflection, optical distortion, defocus and motion blur, color\nalteration, imaging noise, and shape deformation of markers. Lastly, a trained\nmarker detector seeks the regions of interest and recognizes multiple marker\npatterns simultaneously via inverse deformation transformation. The deformable\nmarker creator and detector networks are jointly optimized via the\ndifferentiable photorealistic renderer in an end-to-end manner, allowing us to\nrobustly recognize a wide range of deformable markers with high accuracy. Our\ndeformable marker system is capable of decoding 36-bit messages successfully at\n~29 fps with severe shape deformation. Results validate that our system\nsignificantly outperforms the traditional and data-driven marker methods. Our\nlearning-based marker system opens up new interesting applications of fiducial\nmarkers, including cost-effective motion capture of the human body, active 3D\nscanning using our fiducial markers' array as structured light patterns, and\nrobust augmented reality rendering of virtual objects on dynamic surfaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yaldiz_M/0/1/0/all/0/1\">Mustafa B. Yaldiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuleman_A/0/1/0/all/0/1\">Andreas Meuleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyeonjoong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1\">Hyunho Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min H. Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Effect of Lay People in Gesture-Based Locomotion in Virtual Reality. (arXiv:2206.08076v1 [cs.HC])","link":"http://arxiv.org/abs/2206.08076","description":"<p>Locomotion in Virtual Reality (VR) is an important part of VR applications.\nMany scientists are enriching the community with different variations that\nenable locomotion in VR. Some of the most promising methods are gesture-based\nand do not require additional handheld hardware. Recent work focused mostly on\nuser preference and performance of the different locomotion techniques. This\nignores the learning effect that users go through while new methods are being\nexplored. In this work, it is investigated whether and how quickly users can\nadapt to a hand gesture-based locomotion system in VR. Four different\nlocomotion techniques are implemented and tested by participants. The goal of\nthis paper is twofold: First, it aims to encourage researchers to consider the\nlearning effect in their studies. Second, this study aims to provide insight\ninto the learning effect of users in gesture-based systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schafer_A/0/1/0/all/0/1\">Alexander Sch&#xe4;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_G/0/1/0/all/0/1\">Gerd Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Scene Representation for Locomotion on Structured Terrain. (arXiv:2206.08077v1 [cs.RO])","link":"http://arxiv.org/abs/2206.08077","description":"<p>We propose a learning-based method to reconstruct the local terrain for\nlocomotion with a mobile robot traversing urban environments. Using a stream of\ndepth measurements from the onboard cameras and the robot's trajectory, the\nalgorithm estimates the topography in the robot's vicinity. The raw\nmeasurements from these cameras are noisy and only provide partial and occluded\nobservations that in many cases do not show the terrain the robot stands on.\nTherefore, we propose a 3D reconstruction model that faithfully reconstructs\nthe scene, despite the noisy measurements and large amounts of missing data\ncoming from the blind spots of the camera arrangement. The model consists of a\n4D fully convolutional network on point clouds that learns the geometric priors\nto complete the scene from the context and an auto-regressive feedback to\nleverage spatio-temporal consistency and use evidence from the past. The\nnetwork can be solely trained with synthetic data, and due to extensive\naugmentation, it is robust in the real world, as shown in the validation on a\nquadrupedal robot, ANYmal, traversing challenging settings. We run the pipeline\non the robot's onboard low-power computer using an efficient sparse tensor\nimplementation and show that the proposed method outperforms classical map\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoeller_D/0/1/0/all/0/1\">David Hoeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudin_N/0/1/0/all/0/1\">Nikita Rudin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1\">Christopher Choy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Animashree Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1\">Marco Hutter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-PET: MRI-based Dementia Detection with Joint Generation of Synthetic FDG-PET Images. (arXiv:2206.08078v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08078","description":"<p>Alzheimer's disease (AD) is the most common cause of dementia. An early\ndetection is crucial for slowing down the disease and mitigating risks related\nto the progression. While the combination of MRI and FDG-PET is the best\nimage-based tool for diagnosis, FDG-PET is not always available. The reliable\ndetection of Alzheimer's disease with only MRI could be beneficial, especially\nin regions where FDG-PET might not be affordable for all patients. To this end,\nwe propose a multi-task method based on U-Net that takes T1-weighted MR images\nas an input to generate synthetic FDG-PET images and classifies the dementia\nprogression of the patient into cognitive normal (CN), cognitive impairment\n(MCI), and AD. The attention gates used in both task heads can visualize the\nmost relevant parts of the brain, guiding the examiner and adding\ninterpretability. Results show the successful generation of synthetic FDG-PET\nimages and a performance increase in disease classification over the naive\nsingle-task baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kollovieh_M/0/1/0/all/0/1\">Marcel Kollovieh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keicher_M/0/1/0/all/0/1\">Matthias Keicher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wunderlich_S/0/1/0/all/0/1\">Stephan Wunderlich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burwinkel_H/0/1/0/all/0/1\">Hendrik Burwinkel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wendler_T/0/1/0/all/0/1\">Thomas Wendler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains. (arXiv:2206.08083v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08083","description":"<p>Unsupervised Domain Adaptation demonstrates great potential to mitigate\ndomain shifts by transferring models from labeled source domains to unlabeled\ntarget domains. While Unsupervised Domain Adaptation has been applied to a wide\nvariety of complex vision tasks, only few works focus on lane detection for\nautonomous driving. This can be attributed to the lack of publicly available\ndatasets. To facilitate research in these directions, we propose CARLANE, a\n3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE\nencompasses the single-target datasets MoLane and TuLane and the multi-target\ndataset MuLane. These datasets are built from three different domains, which\ncover diverse scenes and contain a total of 163K unique images, 118K of which\nare annotated. In addition we evaluate and report systematic baselines,\nincluding our own method, which builds upon Prototypical Cross-domain\nSelf-supervised Learning. We find that false positive and false negative rates\nof the evaluated domain adaptation methods are high compared to those of fully\nsupervised baselines. This affirms the need for benchmarks such as CARLANE to\nfurther strengthen research in Unsupervised Domain Adaptation for lane\ndetection. CARLANE, all evaluated models and the corresponding implementations\nare publicly available at https://carlanebenchmark.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gebele_J/0/1/0/all/0/1\">Julian Gebele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhr_B/0/1/0/all/0/1\">Bonifaz Stuhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haselberger_J/0/1/0/all/0/1\">Johann Haselberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Improved Normed-Deformable Convolution for Crowd Counting. (arXiv:2206.08084v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08084","description":"<p>In recent years, crowd counting has become an important issue in computer\nvision. In most methods, the density maps are generated by convolving with a\nGaussian kernel from the ground-truth dot maps which are marked around the\ncenter of human heads. Due to the fixed geometric structures in CNNs and\nindistinct head-scale information, the head features are obtained incompletely.\nDeformable convolution is proposed to exploit the scale-adaptive capabilities\nfor CNN features in the heads. By learning the coordinate offsets of the\nsampling points, it is tractable to improve the ability to adjust the receptive\nfield. However, the heads are not uniformly covered by the sampling points in\nthe deformable convolution, resulting in loss of head information. To handle\nthe non-uniformed sampling, an improved Normed-Deformable Convolution\n(\\textit{i.e.,}NDConv) implemented by Normed-Deformable loss\n(\\textit{i.e.,}NDloss) is proposed in this paper. The offsets of the sampling\npoints which are constrained by NDloss tend to be more even. Then, the features\nin the heads are obtained more completely, leading to better performance.\nEspecially, the proposed NDConv is a light-weight module which shares similar\ncomputation burden with Deformable Convolution. In the extensive experiments,\nour method outperforms state-of-the-art methods on ShanghaiTech A, ShanghaiTech\nB, UCF\\_QNRF, and UCF\\_CC\\_50 dataset, achieving 61.4, 7.8, 91.2, and 167.2\nMAE, respectively. The code is available at\nhttps://github.com/bingshuangzhuzi/NDConv\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhaoyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weigang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Baseline for Adversarial Domain Adaptation-based Unsupervised Flood Forecasting. (arXiv:2206.08105v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08105","description":"<p>Flood disasters cause enormous social and economic losses. However, both\ntraditional physical models and learning-based flood forecasting models require\nmassive historical flood data to train the model parameters. When come to some\nnew site that does not have sufficient historical data, the model performance\nwill drop dramatically due to overfitting. This technical report presents a\nFlood Domain Adaptation Network (FloodDAN), a baseline of applying Unsupervised\nDomain Adaptation (UDA) to the flood forecasting problem. Specifically,\ntraining of FloodDAN includes two stages: in the first stage, we train a\nrainfall encoder and a prediction head to learn general transferable\nhydrological knowledge on large-scale source domain data; in the second stage,\nwe transfer the knowledge in the pretrained encoder into the rainfall encoder\nof target domain through adversarial domain alignment. During inference, we\nutilize the target domain rainfall encoder trained in the second stage and the\nprediction head trained in the first stage to get flood forecasting\npredictions. Experimental results on Tunxi and Changhua flood dataset show that\nFloodDAN can perform flood forecasting effectively with zero target domain\nsupervision. The performance of the FloodDAN is on par with supervised models\nthat uses 450-500 hours of supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ruizhi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yanling Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel Importance Matters in Few-Shot Image Classification. (arXiv:2206.08126v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08126","description":"<p>Few-Shot Learning (FSL) requires vision models to quickly adapt to brand-new\nclassification tasks with a shift in task distribution. Understanding the\ndifficulties posed by this task distribution shift is central to FSL. In this\npaper, we show that a simple channel-wise feature transformation may be the key\nto unraveling this secret from a channel perspective. When facing novel\nfew-shot tasks in the test-time datasets, this transformation can greatly\nimprove the generalization ability of learned image representations, while\nbeing agnostic to the choice of training algorithms and datasets. Through an\nin-depth analysis of this transformation, we find that the difficulty of\nrepresentation transfer in FSL stems from the severe channel bias problem of\nimage representations: channels may have different importance in different\ntasks, while convolutional neural networks are likely to be insensitive, or\nrespond incorrectly to such a shift. This points out a core problem of the\ngeneralization ability of modern vision systems and needs further attention in\nthe future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline. (arXiv:2206.08129v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08129","description":"<p>Current end-to-end autonomous driving methods either run a controller based\non a planned trajectory or perform control prediction directly, which have\nspanned two separately studied lines of research. Seeing their potential mutual\nbenefits to each other, this paper takes the initiative to explore the\ncombination of these two well-developed worlds. Specifically, our integrated\napproach has two branches for trajectory planning and direct control,\nrespectively. The trajectory branch predicts the future trajectory, while the\ncontrol branch involves a novel multi-step prediction scheme such that the\nrelationship between current actions and future states can be reasoned. The two\nbranches are connected so that the control branch receives corresponding\nguidance from the trajectory branch at each time step. The outputs from two\nbranches are then fused to achieve complementary advantages. Our results are\nevaluated in the closed-loop urban driving setting with challenging scenarios\nusing the CARLA simulator. Even with a monocular camera input, the proposed\napproach ranks $first$ on the official CARLA Leaderboard, outperforming other\ncomplex candidates with multiple sensors or fusion mechanisms by a large\nmargin. The source code and data will be made publicly available at\nhttps://github.com/OpenPerceptionX/TCP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Penghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaosong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone fine-tuning without episodic meta-learning dominates for few-shot learning image classification. (arXiv:2206.08138v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08138","description":"<p>Although deep neural networks are capable of achieving performance superior\nto humans on various tasks, they are notorious for requiring large amounts of\ndata and computing resources, restricting their success to domains where such\nresources are available. Metalearning methods can address this problem by\ntransferring knowledge from related tasks, thus reducing the amount of data and\ncomputing resources needed to learn new tasks. We organize the MetaDL\ncompetition series, which provide opportunities for research groups all over\nthe world to create and experimentally assess new meta-(deep)learning solutions\nfor real problems. In this paper, authored collaboratively between the\ncompetition organizers and the top-ranked participants, we describe the design\nof the competition, the datasets, the best experimental results, as well as the\ntop-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active\nteams who made it to the final phase (by outperforming the baseline), making\nover 100 code submissions during the feedback phase. The solutions of the top\nparticipants have been open-sourced. The lessons learned include that learning\ngood representations is essential for effective transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baz_A/0/1/0/all/0/1\">Adrian El Baz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1\">Andr&#xe9; Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1\">Fabio Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouk_H/0/1/0/all/0/1\">Henry Gouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shell Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohr_F/0/1/0/all/0/1\">Felix Mohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_J/0/1/0/all/0/1\">Jan van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a> (TAU, LISN)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adaptive Label Augmentation for Semi-supervised Few-shot Classification. (arXiv:2206.08150v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08150","description":"<p>Few-shot classification aims to learn a model that can generalize well to new\ntasks when only a few labeled samples are available. To make use of unlabeled\ndata that are more abundantly available in real applications, Ren et al.\n\\shortcite{ren2018meta} propose a semi-supervised few-shot classification\nmethod that assigns an appropriate label to each unlabeled sample by a manually\ndefined metric. However, the manually defined metric fails to capture the\nintrinsic property in data. In this paper, we propose a\n\\textbf{S}elf-\\textbf{A}daptive \\textbf{L}abel \\textbf{A}ugmentation approach,\ncalled \\textbf{SALA}, for semi-supervised few-shot classification. A major\nnovelty of SALA is the task-adaptive metric, which can learn the metric\nadaptively for different tasks in an end-to-end fashion. Another appealing\nfeature of SALA is a progressive neighbor selection strategy, which selects\nunlabeled data with high confidence progressively through the training phase.\nExperiments demonstrate that SALA outperforms several state-of-the-art methods\nfor semi-supervised few-shot classification on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. (arXiv:2206.08155v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08155","description":"<p>Video question answering (VideoQA) is a complex task that requires diverse\nmulti-modal data for training. Manual annotation of question and answers for\nvideos, however, is tedious and prohibits scalability. To tackle this problem,\nrecent methods consider zero-shot settings with no manual annotation of visual\nquestion-answer. In particular, a promising approach adapts frozen\nautoregressive language models pretrained on Web-scale text-only data to\nmulti-modal inputs. In contrast, we here build on frozen bidirectional language\nmodels (BiLM) and show that such an approach provides a stronger and cheaper\nalternative for zero-shot VideoQA. In particular, (i) we combine visual inputs\nwith the frozen BiLM using light trainable modules, (ii) we train such modules\nusing Web-scraped multi-modal data, and finally (iii) we perform zero-shot\nVideoQA inference through masked language modeling, where the masked text is\nthe answer to a given question. Our proposed approach, FrozenBiLM, outperforms\nthe state of the art in zero-shot VideoQA by a significant margin on a variety\nof datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA,\nTGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in\nthe few-shot and fully-supervised setting. Our code and models will be made\npublicly available at https://antoyang.github.io/frozenbilm.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Volumetric Supervised Contrastive Learning for Seismic Semantic Segmentation. (arXiv:2206.08158v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08158","description":"<p>In seismic interpretation, pixel-level labels of various rock structures can\nbe time-consuming and expensive to obtain. As a result, there oftentimes exists\na non-trivial quantity of unlabeled data that is left unused simply because\ntraditional deep learning methods rely on access to fully labeled volumes. To\nrectify this problem, contrastive learning approaches have been proposed that\nuse a self-supervised methodology in order to learn useful representations from\nunlabeled data. However, traditional contrastive learning approaches are based\non assumptions from the domain of natural images that do not make use of\nseismic context. In order to incorporate this context within contrastive\nlearning, we propose a novel positive pair selection strategy based on the\nposition of slices within a seismic volume. We show that the learnt\nrepresentations from our method out-perform a state of the art contrastive\nlearning methodology in a semantic segmentation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kokilepersaud_K/0/1/0/all/0/1\">Kiran Kokilepersaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhushankar_M/0/1/0/all/0/1\">Mohit Prabhushankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1\">Ghassan AlRegib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Radar: 4D Radar Object Detection Dataset and Benchmark for Autonomous Driving in Various Weather Conditions. (arXiv:2206.08171v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08171","description":"<p>Unlike RGB cameras that use visible light bands (384$\\sim$769 THz) and Lidar\nthat use infrared bands (361$\\sim$331 THz), Radars use relatively longer\nwavelength radio bands (77$\\sim$81 GHz), resulting in robust measurements in\nadverse weathers. Unfortunately, existing Radar datasets only contain a\nrelatively small number of samples compared to the existing camera and Lidar\ndatasets. This may hinder the development of sophisticated data-driven deep\nlearning techniques for Radar-based perception. Moreover, most of the existing\nRadar datasets only provide 3D Radar tensor (3DRT) data that contain power\nmeasurements along the Doppler, range, and azimuth dimensions. As there is no\nelevation information, it is challenging to estimate the 3D bounding box of an\nobject from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel\nlarge-scale object detection dataset and benchmark that contains 35K frames of\n4D Radar tensor (4DRT) data with power measurements along the Doppler, range,\nazimuth, and elevation dimensions, together with carefully annotated 3D\nbounding box labels of objects on the roads. K-Radar includes challenging\ndriving conditions such as adverse weathers (fog, rain, and snow) on various\nroad structures (urban, suburban roads, alleyways, and highways). In addition\nto the 4DRT, we provide auxiliary measurements from carefully calibrated\nhigh-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide\n4DRT-based object detection baseline neural networks (baseline NNs) and show\nthat the height information is crucial for 3D object detection. And by\ncomparing the baseline NN with a similarly-structured Lidar-based neural\nnetwork, we demonstrate that 4D Radar is a more robust sensor for adverse\nweather conditions. All codes are available at\nhttps://github.com/kaist-avelab/k-radar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paek_D/0/1/0/all/0/1\">Dong-Hee Paek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Seung-Hyun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_K/0/1/0/all/0/1\">Kevin Tirta Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RefCrowd: Grounding the Target in Crowd with Referring Expressions. (arXiv:2206.08172v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08172","description":"<p>Crowd understanding has aroused the widespread interest in vision domain due\nto its important practical significance. Unfortunately, there is no effort to\nexplore crowd understanding in multi-modal domain that bridges natural language\nand computer vision. Referring expression comprehension (REF) is such a\nrepresentative multi-modal task. Current REF studies focus more on grounding\nthe target object from multiple distinctive categories in general scenarios. It\nis difficult to applied to complex real-world crowd understanding. To fill this\ngap, we propose a new challenging dataset, called RefCrowd, which towards\nlooking for the target person in crowd with referring expressions. It not only\nrequires to sufficiently mine the natural language information, but also\nrequires to carefully focus on subtle differences between the target and a\ncrowd of persons with similar appearance, so as to realize the fine-grained\nmapping from language to vision. Furthermore, we propose a Fine-grained\nMulti-modal Attribute Contrastive Network (FMAC) to deal with REF in crowd\nunderstanding. It first decomposes the intricate visual and language features\ninto attribute-aware multi-modal features, and then captures discriminative but\nrobustness fine-grained attribute features to effectively distinguish these\nsubtle differences between similar persons. The proposed method outperforms\nexisting state-of-the-art (SoTA) methods on our RefCrowd dataset and existing\nREF datasets. In addition, we implement an end-to-end REF toolbox for the\ndeeper research in multi-modal domain. Our dataset and code can be available\nat: \\url{https://qiuheqian.github.io/datasets/refcrowd/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Heqian Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Taijin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingbo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanman Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Level 2 Autonomous Driving on a Single Device: Diving into the Devils of Openpilot. (arXiv:2206.08176v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08176","description":"<p>Equipped with a wide span of sensors, predominant autonomous driving\nsolutions are becoming more modular-oriented for safe system design. Though\nthese sensors have laid a solid foundation, most massive-production solutions\nup to date still fall into L2 phase. Among these, Comma.ai comes to our sight,\nclaiming one $999 aftermarket device mounted with a single camera and board\ninside owns the ability to handle L2 scenarios. Together with open-sourced\nsoftware of the entire system released by Comma.ai, the project is named\nOpenpilot. Is it possible? If so, how is it made possible? With curiosity in\nmind, we deep-dive into Openpilot and conclude that its key to success is the\nend-to-end system design instead of a conventional modular framework. The model\nis briefed as Supercombo, and it can predict the ego vehicle's future\ntrajectory and other road semantics on the fly from monocular input.\nUnfortunately, the training process and massive amount of data to make all\nthese work are not publicly available. To achieve an intensive investigation,\nwe try to reimplement the training details and test the pipeline on public\nbenchmarks. The refactored network proposed in this work is referred to as\nOP-Deepdive. For a fair comparison of our version to the original Supercombo,\nwe introduce a dual-model deployment scheme to test the driving performance in\nthe real world. Experimental results on nuScenes, Comma2k19, CARLA, and\nin-house realistic scenarios verify that a low-cost device can indeed achieve\nmost L2 functionalities and be on par with the original Supercombo model. In\nthis report, we would like to share our latest findings, shed some light on the\nnew perspective of end-to-end autonomous driving from an industrial\nproduct-level side, and potentially inspire the community to continue improving\nthe performance. Our code, benchmarks are at\nhttps://github.com/OpenPerceptionX/Openpilot-Deepdive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tutian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhitian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Penghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nucleus Segmentation and Analysis in Breast Cancer with the MIScnn Framework. (arXiv:2206.08182v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08182","description":"<p>The NuCLS dataset contains over 220.000 annotations of cell nuclei in breast\ncancers. We show how to use these data to create a multi-rater model with the\nMIScnn Framework to automate the analysis of cell nuclei. For the model\ncreation, we use the widespread U-Net approach embedded in a pipeline. This\npipeline provides besides the high performance convolution neural network,\nseveral preprocessor techniques and a extended data exploration. The final\nmodel is tested in the evaluation phase using a wide variety of metrics with a\nsubsequent visualization. Finally, the results are compared and interpreted\nwith the results of the NuCLS study. As an outlook, indications are given which\nare important for the future development of models in the context of cell\nnuclei.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfleiderer_A/0/1/0/all/0/1\">Adrian Pfleiderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_D/0/1/0/all/0/1\">Dominik M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymptotic Soft Cluster Pruning for Deep Neural Networks. (arXiv:2206.08186v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08186","description":"<p>Filter pruning method introduces structural sparsity by removing selected\nfilters and is thus particularly effective for reducing complexity. Previous\nworks empirically prune networks from the point of view that filter with\nsmaller norm contributes less to the final results. However, such criteria has\nbeen proven sensitive to the distribution of filters, and the accuracy may hard\nto recover since the capacity gap is fixed once pruned. In this paper, we\npropose a novel filter pruning method called Asymptotic Soft Cluster Pruning\n(ASCP), to identify the redundancy of network based on the similarity of\nfilters. Each filter from over-parameterized network is first distinguished by\nclustering, and then reconstructed to manually introduce redundancy into it.\nSeveral guidelines of clustering are proposed to better preserve feature\nextraction ability. After reconstruction, filters are allowed to be updated to\neliminate the effect caused by mistakenly selected. Besides, various decaying\nstrategies of the pruning rate are adopted to stabilize the pruning process and\nimprove the final performance as well. By gradually generating more identical\nfilters within each cluster, ASCP can remove them through channel addition\noperation with almost no accuracy drop. Extensive experiments on CIFAR-10 and\nImageNet datasets show that our method can achieve competitive results compared\nwith many state-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tao Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yinglei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_P/0/1/0/all/0/1\">Panpan Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Segmentation of LiDAR Sequences: Dataset and Algorithm. (arXiv:2206.08194v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08194","description":"<p>Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles,\ndriving the need for real-time processing of 3D point sequences. However, most\nLiDAR semantic segmentation datasets and algorithms split these acquisitions\ninto $360^\\circ$ frames, leading to acquisition latency that is incompatible\nwith realistic real-time applications and evaluations. We address this issue\nwith two key contributions. First, we introduce HelixNet, a $10$ billion point\ndataset with fine-grained labels, timestamps, and sensor rotation information\nthat allows an accurate assessment of real-time readiness of segmentation\nalgorithms. Second, we propose Helix4D, a compact and efficient spatio-temporal\ntransformer architecture specifically designed for rotating LiDAR point\nsequences. Helix4D operates on acquisition slices that correspond to a fraction\nof a full rotation of the sensor, significantly reducing the total latency. We\npresent an extensive benchmark of the performance and real-time readiness of\nseveral state-of-the-art models on HelixNet and SemanticKITTI. Helix4D reaches\naccuracy on par with the best segmentation algorithms with a reduction of more\nthan $5\\times$ in terms of latency and $50\\times$ in model size. Code and data\nare available at: https://romainloiseau.fr/helixnet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loiseau_R/0/1/0/all/0/1\">Romain Loiseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Lo&#xef;c Landrieu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Multi-Scale Learning for Object Detection. (arXiv:2206.08206v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08206","description":"<p>Pyramidal networks are standard methods for multi-scale object detection.\nCurrent researches on feature pyramid networks usually adopt layer connections\nto collect features from certain levels of the feature hierarchy, and do not\nconsider the significant differences among them. We propose a better\narchitecture of feature pyramid networks, named selective multi-scale learning\n(SMSL), to address this issue. SMSL is efficient and general, which can be\nintegrated in both single-stage and two-stage detectors to boost detection\nperformance, with nearly no extra inference cost. RetinaNet combined with SMSL\nobtains 1.8\\% improvement in AP (from 39.1\\% to 40.9\\%) on COCO dataset. When\nintegrated with SMSL, two-stage detectors can get around 1.0\\% improvement in\nAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weizeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Smoothness in Domain Adversarial Training. (arXiv:2206.08213v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08213","description":"<p>Domain adversarial training has been ubiquitous for achieving invariant\nrepresentations and is used widely for various domain adaptation tasks. In\nrecent times, methods converging to smooth optima have shown improved\ngeneralization for supervised learning tasks like classification. In this work,\nwe analyze the effect of smoothness enhancing formulations on domain\nadversarial training, the objective of which is a combination of task loss (eg.\nclassification, regression, etc.) and adversarial terms. We find that\nconverging to a smooth minima with respect to (w.r.t.) task loss stabilizes the\nadversarial training leading to better performance on target domain. In\ncontrast to task loss, our analysis shows that converging to smooth minima\nw.r.t. adversarial loss leads to sub-optimal generalization on the target\ndomain. Based on the analysis, we introduce the Smooth Domain Adversarial\nTraining (SDAT) procedure, which effectively enhances the performance of\nexisting domain adversarial methods for both classification and object\ndetection tasks. Our analysis also provides insight into the extensive usage of\nSGD over Adam in the community for domain adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rangwani_H/0/1/0/all/0/1\">Harsh Rangwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aithal_S/0/1/0/all/0/1\">Sumukh K Aithal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1\">Mayank Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Arihant Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HaGRID - HAnd Gesture Recognition Image Dataset. (arXiv:2206.08219v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08219","description":"<p>In this paper, we introduce an enormous dataset HaGRID (HAnd Gesture\nRecognition Image Dataset) for hand gesture recognition (HGR) systems. This\ndataset contains 552,992 samples divided into 18 classes of gestures. The\nannotations consist of bounding boxes of hands with gesture labels and markups\nof leading hands. The proposed dataset allows for building HGR systems, which\ncan be used in video conferencing services, home automation systems, the\nautomotive sector, services for people with speech and hearing impairments,\netc. We are especially focused on interaction with devices to manage them. That\nis why all 18 chosen gestures are functional, familiar to the majority of\npeople, and may be an incentive to take some action. In addition, we used\ncrowdsourcing platforms to collect the dataset and took into account various\nparameters to ensure data diversity. We describe the challenges of using\nexisting HGR datasets for our task and provide a detailed overview of them.\nFurthermore, the baselines for the hand detection and gesture classification\ntasks are proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kapitanov_A/0/1/0/all/0/1\">Alexander Kapitanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makhlyarchuk_A/0/1/0/all/0/1\">Andrew Makhlyarchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kvanchiani_K/0/1/0/all/0/1\">Karina Kvanchiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Self-Supervised Vision Transformers by Probing Attention-Conditioned Masking Consistency. (arXiv:2206.08222v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08222","description":"<p>Visual domain adaptation (DA) seeks to transfer trained models to unseen,\nunlabeled domains across distribution shift, but approaches typically focus on\nadapting convolutional neural network architectures initialized with supervised\nImageNet representations. In this work, we shift focus to adapting modern\narchitectures for object recognition -- the increasingly popular Vision\nTransformer (ViT) -- and modern pretraining based on self-supervised learning\n(SSL). Inspired by the design of recent SSL approaches based on learning from\npartial image inputs generated via masking or cropping -- either by learning to\npredict the missing pixels, or learning representational invariances to such\naugmentations -- we propose PACMAC, a simple two-stage adaptation algorithm for\nself-supervised ViTs. PACMAC first performs in-domain SSL on pooled source and\ntarget data to learn task-discriminative features, and then probes the model's\npredictive consistency across a set of partial target inputs generated via a\nnovel attention-conditioned masking strategy, to identify reliable candidates\nfor self-training. Our simple approach leads to consistent performance gains\nover competing methods that use ViTs and self-supervised initializations on\nstandard object recognition benchmarks. Code available at\nhttps://github.com/virajprabhu/PACMAC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1\">Viraj Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yenamandra_S/0/1/0/all/0/1\">Sriram Yenamandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aaditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1\">Judy Hoffman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi scale Feature Extraction and Fusion for Online Knowledge Distillation. (arXiv:2206.08224v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08224","description":"<p>Online knowledge distillation conducts knowledge transfer among all student\nmodels to alleviate the reliance on pre-trained models. However, existing\nonline methods rely heavily on the prediction distributions and neglect the\nfurther exploration of the representational knowledge. In this paper, we\npropose a novel Multi-scale Feature Extraction and Fusion method (MFEF) for\nonline knowledge distillation, which comprises three key components:\nMulti-scale Feature Extraction, Dual-attention and Feature Fusion, towards\ngenerating more informative feature maps for distillation. The multiscale\nfeature extraction exploiting divide-and-concatenate in channel dimension is\nproposed to improve the multi-scale representation ability of feature maps. To\nobtain more accurate information, we design a dual-attention to strengthen the\nimportant channel and spatial regions adaptively. Moreover, we aggregate and\nfuse the former processed feature maps via feature fusion to assist the\ntraining of student models. Extensive experiments on CIF AR-10, CIF AR-100, and\nCINIC-10 show that MFEF transfers more beneficial representational knowledge\nfor distillation and outperforms alternative methods among various network\narchitectures\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_P/0/1/0/all/0/1\">Panpan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1\">Yinglei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tao Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving into the Scale Variance Problem in Object Detection. (arXiv:2206.08227v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08227","description":"<p>Object detection has made substantial progress in the last decade, due to the\ncapability of convolution in extracting local context of objects. However, the\nscales of objects are diverse and current convolution can only process\nsingle-scale input. The capability of traditional convolution with a fixed\nreceptive field in dealing with such a scale variance problem, is thus limited.\nMulti-scale feature representation has been proven to be an effective way to\nmitigate the scale variance problem. Recent researches mainly adopt partial\nconnection with certain scales, or aggregate features from all scales and focus\non the global information across the scales. However, the information across\nspatial and depth dimensions is ignored. Inspired by this, we propose the\nmulti-scale convolution (MSConv) to handle this problem. Taking into\nconsideration scale, spatial and depth information at the same time, MSConv is\nable to process multi-scale input more comprehensively. MSConv is effective and\ncomputationally efficient, with only a small increase of computational cost.\nFor most of the single-stage object detectors, replacing the traditional\nconvolutions with MSConvs in the detection head can bring more than 2.5\\%\nimprovement in AP (on COCO 2017 dataset), with only 3\\% increase of FLOPs.\nMSConv is also flexible and effective for two-stage object detectors. When\nextended to the mainstream two-stage object detectors, MSConv can bring up to\n3.0\\% improvement in AP. Our best model under single-scale testing achieves\n48.9\\% AP on COCO 2017 \\textit{test-dev} split, which surpasses many\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaodong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Set Recognition with Gradient-Based Representations. (arXiv:2206.08229v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08229","description":"<p>Neural networks for image classification tasks assume that any given image\nduring inference belongs to one of the training classes. This closed-set\nassumption is challenged in real-world applications where models may encounter\ninputs of unknown classes. Open-set recognition aims to solve this problem by\nrejecting unknown classes while classifying known classes correctly. In this\npaper, we propose to utilize gradient-based representations obtained from a\nknown classifier to train an unknown detector with instances of known classes\nonly. Gradients correspond to the amount of model updates required to properly\nrepresent a given sample, which we exploit to understand the model's capability\nto characterize inputs with its learned features. Our approach can be utilized\nwith any classifier trained in a supervised manner on known classes without the\nneed to model the distribution of unknown samples explicitly. We show that our\ngradient-based approach outperforms state-of-the-art methods by up to 11.6% in\nopen-set classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinsol Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1\">Ghassan AlRegib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Efficient Architectures for Semantic Segmentation. (arXiv:2206.08236v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08236","description":"<p>Though the state-of-the architectures for semantic segmentation, such as\nHRNet, demonstrate impressive accuracy, the complexity arising from their\nsalient design choices hinders a range of model acceleration tools, and further\nthey make use of operations that are inefficient on current hardware. This\npaper demonstrates that a simple encoder-decoder architecture with a\nResNet-like backbone and a small multi-scale head, performs on-par or better\nthan complex semantic segmentation architectures such as HRNet, FANet and\nDDRNets. Naively applying deep backbones designed for Image Classification to\nthe task of Semantic Segmentation leads to sub-par results, owing to a much\nsmaller effective receptive field of these backbones. Implicit among the\nvarious design choices put forth in works like HRNet, DDRNet, and FANet are\nnetworks with a large effective receptive field. It is natural to ask if a\nsimple encoder-decoder architecture would compare favorably if comprised of\nbackbones that have a larger effective receptive field, though without the use\nof inefficient operations like dilated convolutions. We show that with minor\nand inexpensive modifications to ResNets, enlarging the receptive field, very\nsimple and competitive baselines can be created for Semantic Segmentation. We\npresent a family of such simple architectures for desktop as well as mobile\ntargets, which match or exceed the performance of complex models on the\nCityscapes dataset. We hope that our work provides simple yet effective\nbaselines for practitioners to develop efficient semantic segmentation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Dushyant Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skliar_A/0/1/0/all/0/1\">Andrii Skliar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahia_H/0/1/0/all/0/1\">Haitam Ben Yahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borse_S/0/1/0/all/0/1\">Shubhankar Borse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habibian_A/0/1/0/all/0/1\">Amirhossein Habibian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1\">Tijmen Blankevoort</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Catastrophic overfitting is a bug but also a feature. (arXiv:2206.08242v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08242","description":"<p>Despite clear computational advantages in building robust neural networks,\nadversarial training (AT) using single-step methods is unstable as it suffers\nfrom catastrophic overfitting (CO): Networks gain non-trivial robustness during\nthe first stages of adversarial training, but suddenly reach a breaking point\nwhere they quickly lose all robustness in just a few iterations. Although some\nworks have succeeded at preventing CO, the different mechanisms that lead to\nthis remarkable failure mode are still poorly understood. In this work,\nhowever, we find that the interplay between the structure of the data and the\ndynamics of AT plays a fundamental role in CO. Specifically, through active\ninterventions on typical datasets of natural images, we establish a causal link\nbetween the structure of the data and the onset of CO in single-step AT\nmethods. This new perspective provides important insights into the mechanisms\nthat lead to CO and paves the way towards a better understanding of the general\ndynamics of robust model construction. The code to reproduce the experiments of\nthis paper can be found at https://github.com/gortizji/co_features .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_Jimenez_G/0/1/0/all/0/1\">Guillermo Ortiz-Jim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorge_P/0/1/0/all/0/1\">Pau de Jorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Amartya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogez_G/0/1/0/all/0/1\">Gregory Rog&#xe9;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-Based Adversarial and Out-of-Distribution Detection. (arXiv:2206.08255v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08255","description":"<p>We propose to utilize gradients for detecting adversarial and\nout-of-distribution samples. We introduce confounding labels -- labels that\ndiffer from normal labels seen during training -- in gradient generation to\nprobe the effective expressivity of neural networks. Gradients depict the\namount of change required for a model to properly represent given inputs,\nproviding insight into the representational power of the model established by\nnetwork architectural properties as well as training data. By introducing a\nlabel of different design, we remove the dependency on ground truth labels for\ngradient generation during inference. We show that our gradient-based approach\nallows for capturing the anomaly in inputs based on the effective expressivity\nof the models with no hyperparameter tuning or additional processing, and\noutperforms state-of-the-art methods for adversarial and out-of-distribution\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinsol Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhushankar_M/0/1/0/all/0/1\">Mohit Prabhushankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1\">Ghassan AlRegib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Longitudinal detection of new MS lesions using Deep Learning. (arXiv:2206.08272v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08272","description":"<p>The detection of new multiple sclerosis (MS) lesions is an important marker\nof the evolution of the disease. The applicability of learning-based methods\ncould automate this task efficiently. However, the lack of annotated\nlongitudinal data with new-appearing lesions is a limiting factor for the\ntraining of robust and generalizing models. In this work, we describe a\ndeep-learning-based pipeline addressing the challenging task of detecting and\nsegmenting new MS lesions. First, we propose to use transfer-learning from a\nmodel trained on a segmentation task using single time-points. Therefore, we\nexploit knowledge from an easier task and for which more annotated datasets are\navailable. Second, we propose a data synthesis strategy to generate realistic\nlongitudinal time-points with new lesions using single time-point scans. In\nthis way, we pretrain our detection model on large synthetic annotated\ndatasets. Finally, we use a data-augmentation technique designed to simulate\ndata diversity in MRI. By doing that, we increase the size of the available\nsmall annotated longitudinal datasets. Our ablation study showed that each\ncontribution lead to an enhancement of the segmentation accuracy. Using the\nproposed pipeline, we obtained the best score for the segmentation and the\ndetection of new MS lesions in the MSSEG2 MICCAI challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kamraoui_R/0/1/0/all/0/1\">Reda Abdellah Kamraoui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mansencal_B/0/1/0/all/0/1\">Boris Mansencal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manjon_J/0/1/0/all/0/1\">Jos&#xe9; V Manjon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coupe_P/0/1/0/all/0/1\">Pierrick Coup&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rank the triplets: A ranking-based multiple instance learning framework for detecting HPV infection in head and neck cancers using routine H&E images. (arXiv:2206.08275v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08275","description":"<p>The aetiology of head and neck squamous cell carcinoma (HNSCC) involves\nmultiple carcinogens such as alcohol, tobacco and infection with human\npapillomavirus (HPV). As the HPV infection influences the prognosis, treatment\nand survival of patients with HNSCC, it is important to determine the HPV\nstatus of these tumours. In this paper, we propose a novel triplet-ranking loss\nfunction and a multiple instance learning pipeline for HPV status prediction.\nThis achieves a new state-of-the-art performance in HPV detection using only\nthe routine H&amp;E stained WSIs on two HNSCC cohorts. Furthermore, a comprehensive\ntumour microenvironment profiling was performed, which characterised the unique\npatterns between HPV+/- HNSCC from genomic, immunology and cellular\nperspectives. Positive correlations of the proposed score with different\nsubtypes of T cells (e.g. T cells follicular helper, CD8+ T cells), and\nnegative correlations with macrophages and connective cells (e.g. fibroblast)\nwere identified, which is in line with clinical findings. Unique gene\nexpression profiles were also identified with respect to HPV infection status,\nand is in line with existing findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khurram_S/0/1/0/all/0/1\">Syed Ali Khurram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_A/0/1/0/all/0/1\">Amina Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_L/0/1/0/all/0/1\">Lawrence Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Capsule Endoscopy Classification using Focal Modulation Guided Convolutional Neural Network. (arXiv:2206.08298v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08298","description":"<p>Video capsule endoscopy is a hot topic in computer vision and medicine. Deep\nlearning can have a positive impact on the future of video capsule endoscopy\ntechnology. It can improve the anomaly detection rate, reduce physicians' time\nfor screening, and aid in real-world clinical analysis. CADx classification\nsystem for video capsule endoscopy has shown a great promise for further\nimprovement. For example, detection of cancerous polyp and bleeding can lead to\nswift medical response and improve the survival rate of the patients. To this\nend, an automated CADx system must have high throughput and decent accuracy. In\nthis paper, we propose FocalConvNet, a focal modulation network integrated with\nlightweight convolutional layers for the classification of small bowel\nanatomical landmarks and luminal findings. FocalConvNet leverages focal\nmodulation to attain global context and allows global-local spatial\ninteractions throughout the forward pass. Moreover, the convolutional block\nwith its intrinsic inductive/learning bias and capacity to extract hierarchical\nfeatures allows our FocalConvNet to achieve favourable results with high\nthroughput. We compare our FocalConvNet with other SOTA on Kvasir-Capsule, a\nlarge-scale VCE dataset with 44,228 frames with 13 classes of different\nanomalies. Our proposed method achieves the weighted F1-score, recall and MCC}\nof 0.6734, 0.6373 and 0.2974, respectively outperforming other SOTA\nmethodologies. Furthermore, we report the highest throughput of 148.02\nimages/second rate to establish the potential of FocalConvNet in a real-time\nclinical environment. The code of the proposed FocalConvNet is available at\nhttps://github.com/NoviceMAn-prog/FocalConvNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Srivastava_A/0/1/0/all/0/1\">Abhishek Srivastava</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomar_N/0/1/0/all/0/1\">Nikhil Kumar Tomar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1\">Debesh Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Patch Attacks and Defences in Vision-Based Tasks: A Survey. (arXiv:2206.08304v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08304","description":"<p>Adversarial attacks in deep learning models, especially for safety-critical\nsystems, are gaining more and more attention in recent years, due to the lack\nof trust in the security and robustness of AI models. Yet the more primitive\nadversarial attacks might be physically infeasible or require some resources\nthat are hard to access like the training data, which motivated the emergence\nof patch attacks. In this survey, we provide a comprehensive overview to cover\nexisting techniques of adversarial patch attacks, aiming to help interested\nresearchers quickly catch up with the progress in this field. We also discuss\nexisting techniques for developing detection and defences against adversarial\npatches, aiming to help the community better understand this field and its\napplications in the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abhijith Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yijun Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munz_P/0/1/0/all/0/1\">Phil Munz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1\">Apurva Narayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake histological images for enhancing digital pathology. (arXiv:2206.08308v1 [eess.IV])","link":"http://arxiv.org/abs/2206.08308","description":"<p>An optical microscopic examination of thinly cut stained tissue on glass\nslides prepared from a FFPE tissue blocks is the gold standard for tissue\ndiagnostics. In addition, the diagnostic abilities and expertise of any\npathologist is dependent on their direct experience with common as well as\nrarer variant morphologies. Recently, deep learning approaches have been used\nto successfully show a high level of accuracy for such tasks. However,\nobtaining expert-level annotated images is an expensive and time-consuming task\nand artificially synthesized histological images can prove greatly beneficial.\nHere, we present an approach to not only generate histological images that\nreproduce the diagnostic morphologic features of common disease but also\nprovide a user ability to generate new and rare morphologies. Our approach\ninvolves developing a generative adversarial network model that synthesizes\npathology images constrained by class labels. We investigated the ability of\nthis framework in synthesizing realistic prostate and colon tissue images and\nassessed the utility of these images in augmenting diagnostic ability of\nmachine learning methods as well as their usability by a panel of experienced\nanatomic pathologists. Synthetic data generated by our framework performed\nsimilar to real data in training a deep learning model for diagnosis.\nPathologists were not able to distinguish between real and synthetic images and\nshowed a similar level of inter-observer agreement for prostate cancer grading.\nWe extended the approach to significantly more complex images from colon\nbiopsies and showed that the complex microenvironment in such tissues can also\nbe reproduced. Finally, we present the ability for a user to generate deepfake\nhistological images via a simple markup of sematic labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Falahkheirkhah_K/0/1/0/all/0/1\">Kianoush Falahkheirkhah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiwari_S/0/1/0/all/0/1\">Saumya Tiwari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeh_K/0/1/0/all/0/1\">Kevin Yeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sounak Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herrera_Hernandez_L/0/1/0/all/0/1\">Loren Herrera-Hernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McCarthy_M/0/1/0/all/0/1\">Michael R. McCarthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jimenez_R/0/1/0/all/0/1\">Rafael E. Jimenez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheville_J/0/1/0/all/0/1\">John C. Cheville</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhargava_R/0/1/0/all/0/1\">Rohit Bhargava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning. (arXiv:2206.08312v1 [cs.SD])","link":"http://arxiv.org/abs/2206.08312","description":"<p>We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio\nrendering for 3D environments. Given a 3D mesh of a real-world environment,\nSoundSpaces can generate highly realistic acoustics for arbitrary sounds\ncaptured from arbitrary microphone locations. Together with existing 3D visual\nassets, it supports an array of audio-visual research tasks, such as\naudio-visual navigation, mapping, source localization and separation, and\nacoustic matching. Compared to existing resources, SoundSpaces 2.0 has the\nadvantages of allowing continuous spatial sampling, generalization to novel\nenvironments, and configurable microphone and material properties. To our best\nknowledge, this is the first geometry-based acoustic simulation that offers\nhigh fidelity and realism while also being fast enough to use for embodied\nlearning. We showcase the simulator's properties and benchmark its performance\nagainst real-world audio measurements. In addition, through two downstream\ntasks covering embodied navigation and far-field automatic speech recognition,\nhighlighting sim2real performance for the latter. SoundSpaces 2.0 is publicly\navailable to facilitate wider research for perceptual systems that can both see\nand hear.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schissler_C/0/1/0/all/0/1\">Carl Schissler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sanchit Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobernik_P/0/1/0/all/0/1\">Philip Kobernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1\">Alexander Clegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calamia_P/0/1/0/all/0/1\">Paul Calamia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_P/0/1/0/all/0/1\">Philip W Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting the Adversarial Transferability of Surrogate Model with Dark Knowledge. (arXiv:2206.08316v1 [cs.LG])","link":"http://arxiv.org/abs/2206.08316","description":"<p>Deep neural networks (DNNs) for image classification are known to be\nvulnerable to adversarial examples. And, the adversarial examples have\ntransferability, which means an adversarial example for a DNN model can fool\nanother black-box model with a non-trivial probability. This gave birth of the\ntransfer-based adversarial attack where the adversarial examples generated by a\npretrained or known model (called surrogate model) are used to conduct\nblack-box attack. There are some work on how to generate the adversarial\nexamples from a given surrogate model to achieve better transferability.\nHowever, training a special surrogate model to generate adversarial examples\nwith better transferability is relatively under-explored. In this paper, we\npropose a method of training a surrogate model with abundant dark knowledge to\nboost the adversarial transferability of the adversarial examples generated by\nthe surrogate model. This trained surrogate model is named dark surrogate model\n(DSM), and the proposed method to train DSM consists of two key components: a\nteacher model extracting dark knowledge and providing soft labels, and the\nmixing augmentation skill which enhances the dark knowledge of training data.\nExtensive experiments have been conducted to show that the proposed method can\nsubstantially improve the adversarial transferability of surrogate model across\ndifferent architectures of surrogate model and optimizers for generating\nadversarial examples. We also show that the proposed method can be applied to\nother scenarios of transfer-based attack that contain dark knowledge, like face\nverification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingcheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenjian Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iBoot: Image-bootstrapped Self-Supervised Video Representation Learning. (arXiv:2206.08339v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08339","description":"<p>Learning visual representations through self-supervision is an extremely\nchallenging task as the network needs to sieve relevant patterns from spurious\ndistractors without the active guidance provided by supervision. This is\nachieved through heavy data augmentation, large-scale datasets and prohibitive\namounts of compute. Video self-supervised learning (SSL) suffers from added\nchallenges: video datasets are typically not as large as image datasets,\ncompute is an order of magnitude larger, and the amount of spurious patterns\nthe optimizer has to sieve through is multiplied several fold. Thus, directly\nlearning self-supervised representations from video data might result in\nsub-optimal performance. To address this, we propose to utilize a strong\nimage-based model, pre-trained with self- or language supervision, in a video\nrepresentation learning framework, enabling the model to learn strong spatial\nand temporal information without relying on the video labeled data. To this\nend, we modify the typical video-based SSL design and objective to encourage\nthe video encoder to \\textit{subsume} the semantic content of an image-based\nmodel trained on a general domain. The proposed algorithm is shown to learn\nmuch more efficiently (i.e. in less epochs and with a smaller batch) and\nresults in a new state-of-the-art performance on standard downstream tasks\namong single-modality SSL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_F/0/1/0/all/0/1\">Fatemeh Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fuwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1\">Brais Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic One-shot Mesh-based Head Avatars. (arXiv:2206.08343v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08343","description":"<p>We present a system for realistic one-shot mesh-based human head avatars\ncreation, ROME for short. Using a single photograph, our model estimates a\nperson-specific head mesh and the associated neural texture, which encodes both\nlocal photometric and geometric details. The resulting avatars are rigged and\ncan be rendered using a neural network, which is trained alongside the mesh and\ntexture estimators on a dataset of in-the-wild videos. In the experiments, we\nobserve that our system performs competitively both in terms of head geometry\nrecovery and the quality of renders, especially for the cross-person\nreenactment. See results https://samsunglabs.github.io/rome/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khakhulin_T/0/1/0/all/0/1\">Taras Khakhulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sklyarova_V/0/1/0/all/0/1\">Vanessa Sklyarova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1\">Victor Lempitsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_E/0/1/0/all/0/1\">Egor Zakharov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-World Single Image Super-Resolution Under Rainy Condition. (arXiv:2206.08345v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08345","description":"<p>Image super-resolution is an important research area in computer vision that\nhas a wide variety of applications including surveillance, medical imaging etc.\nReal-world signal image super-resolution has become very popular now-a-days due\nto its real-time application. There are still a lot of scopes to improve\nreal-world single image super-resolution specially during challenging weather\nscenarios. In this paper, we have proposed a new algorithm to perform\nreal-world single image super-resolution during rainy condition. Our proposed\nmethod can mitigate the influence of rainy conditions during image\nsuper-resolution. Our experiment results show that our proposed algorithm can\nperform image super-resolution decreasing the negative effects of the rain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1\">Mohammad Shahab Uddin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Supervised vs. Unsupervised: Representative Benchmarking and Analysis of Image Representation Learning. (arXiv:2206.08347v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08347","description":"<p>By leveraging contrastive learning, clustering, and other pretext tasks,\nunsupervised methods for learning image representations have reached impressive\nresults on standard benchmarks. The result has been a crowded field - many\nmethods with substantially different implementations yield results that seem\nnearly identical on popular benchmarks, such as linear evaluation on ImageNet.\nHowever, a single result does not tell the whole story. In this paper, we\ncompare methods using performance-based benchmarks such as linear evaluation,\nnearest neighbor classification, and clustering for several different datasets,\ndemonstrating the lack of a clear front-runner within the current\nstate-of-the-art. In contrast to prior work that performs only supervised vs.\nunsupervised comparison, we compare several different unsupervised methods\nagainst each other. To enrich this comparison, we analyze embeddings with\nmeasurements such as uniformity, tolerance, and centered kernel alignment\n(CKA), and propose two new metrics of our own: nearest neighbor graph\nsimilarity and linear prediction overlap. We reveal through our analysis that\nin isolation, single popular methods should not be treated as though they\nrepresent the field as a whole, and that future work ought to consider how to\nleverage the complimentary nature of these methods. We also leverage CKA to\nprovide a framework to robustly quantify augmentation invariance, and provide a\nreminder that certain types of invariance will be undesirable for downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FWD: Real-time Novel View Synthesis with Forward Warping and Depth. (arXiv:2206.08355v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08355","description":"<p>Novel view synthesis (NVS) is a challenging task requiring systems to\ngenerate photorealistic images of scenes from new viewpoints, where both\nquality and speed are important for applications. Previous image-based\nrendering (IBR) methods are fast, but have poor quality when input views are\nsparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give\nimpressive results but are not real-time. In our paper, we propose a\ngeneralizable NVS method with sparse inputs, called FWD, which gives\nhigh-quality synthesis in real-time. With explicit depth and differentiable\nrendering, it achieves competitive results to the SOTA methods with 130-1000x\nspeedup and better perceptual quality. If available, we can seamlessly\nintegrate sensor depth during either training or inference to improve image\nquality while retaining real-time speed. With the growing prevalence of depths\nsensors, we hope that methods making use of depth will become increasingly\nuseful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_A/0/1/0/all/0/1\">Ang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rockwell_C/0/1/0/all/0/1\">Chris Rockwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniMAE: Single Model Masked Pretraining on Images and Videos. (arXiv:2206.08356v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08356","description":"<p>Transformer-based architectures have become competitive across a variety of\nvisual domains, most notably images and videos. While prior work has studied\nthese modalities in isolation, having a common architecture suggests that one\ncan train a single unified model for multiple visual modalities. Prior attempts\nat unified modeling typically use architectures tailored for vision tasks, or\nobtain worse performance compared to single modality models. In this work, we\nshow that masked autoencoding can be used to train a simple Vision Transformer\non images and videos, without requiring any labeled data. This single model\nlearns visual representations that are comparable to or better than\nsingle-modality representations on both image and video benchmarks, while using\na much simpler architecture. In particular, our single pretrained model can be\nfinetuned to achieve 86.5% on ImageNet and 75.3% on the challenging Something\nSomething-v2 video benchmark. Furthermore, this model can be learned by\ndropping 90% of the image and 95% of the video patches, enabling extremely fast\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Nouby_A/0/1/0/all/0/1\">Alaaeldin El-Nouby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alwala_K/0/1/0/all/0/1\">Kalyan Vasudev Alwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing. (arXiv:2206.08357v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08357","description":"<p>Existing GAN inversion and editing methods work well for aligned objects with\na clean background, such as portraits and animal faces, but often struggle for\nmore difficult categories with complex scene layouts and object occlusions,\nsuch as cars, animals, and outdoor images. We propose a new method to invert\nand edit such complex images in the latent space of GANs, such as StyleGAN2.\nOur key idea is to explore inversion with a collection of layers, spatially\nadapting the inversion process to the difficulty of the image. We learn to\npredict the \"invertibility\" of different image segments and project each\nsegment into a latent layer. Easier regions can be inverted into an earlier\nlayer in the generator's latent space, while more challenging regions can be\ninverted into a later feature space. Experiments show that our method obtains\nbetter inversion results compared to the recent approaches on complex\ncategories, while maintaining downstream editability. Please refer to our\nproject page at https://www.cs.cmu.edu/~SAMInversion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_G/0/1/0/all/0/1\">Gaurav Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yijun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Krishna Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixGen: A New Multi-Modal Data Augmentation. (arXiv:2206.08358v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08358","description":"<p>Data augmentation is a necessity to enhance data efficiency in deep learning.\nFor vision-language pre-training, data is only augmented either for images or\nfor text in previous works. In this paper, we present MixGen: a joint data\naugmentation for vision-language representation learning to further improve\ndata efficiency. It generates new image-text pairs with semantic relationships\npreserved by interpolating images and concatenating text. It's simple, and can\nbe plug-and-played into existing pipelines. We evaluate MixGen on four\narchitectures, including CLIP, ViLT, ALBEF and TCL, across five downstream\nvision-language tasks to show its versatility and effectiveness. For example,\nadding MixGen in ALBEF pre-training leads to absolute performance improvements\non downstream tasks: image-text retrieval (+6.2% on COCO fine-tuned and +5.3%\non Flicker30K zero-shot), visual grounding (+0.9% on RefCOCO+), visual\nreasoning (+0.9% on NLVR$^{2}$), visual question answering (+0.3% on VQA2.0),\nand visual entailment (+0.4% on SNLI-VE).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xiaoshuai Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aston Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanqian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable 3D Face Synthesis with Conditional Generative Occupancy Fields. (arXiv:2206.08361v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08361","description":"<p>Capitalizing on the recent advances in image generation models, existing\ncontrollable face image synthesis methods are able to generate high-fidelity\nimages with some levels of controllability, e.g., controlling the shapes,\nexpressions, textures, and poses of the generated face images. However, these\nmethods focus on 2D image generative models, which are prone to producing\ninconsistent face images under large expression and pose changes. In this\npaper, we propose a new NeRF-based conditional 3D face synthesis framework,\nwhich enables 3D controllability over the generated face images by imposing\nexplicit 3D conditions from 3D face priors. At its core is a conditional\nGenerative Occupancy Field (cGOF) that effectively enforces the shape of the\ngenerated face to commit to a given 3D Morphable Model (3DMM) mesh. To achieve\naccurate control over fine-grained 3D face shapes of the synthesized image, we\nadditionally incorporate a 3D landmark loss as well as a volume warping loss\ninto our synthesis algorithm. Experiments validate the effectiveness of the\nproposed method, which is able to generate high-fidelity face images and shows\nmore precise 3D controllability than state-of-the-art 2D-based controllable\nface synthesis methods. Find code and demo at\nhttps://keqiangsun.github.io/projects/cgof.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Keqiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">HongSheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces. (arXiv:2206.08362v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08362","description":"<p>We introduce a unified framework for group equivariant networks on\nhomogeneous spaces derived from a Fourier perspective. We address the case of\nfeature fields being tensor valued before and after a convolutional layer. We\npresent a unified derivation of kernels via the Fourier domain by taking\nadvantage of the sparsity of Fourier coefficients of the lifted feature fields.\nThe sparsity emerges when the stabilizer subgroup of the homogeneous space is a\ncompact Lie group. We further introduce an activation method via an elementwise\nnonlinearity on the regular representation after lifting and projecting back to\nthe field through an equivariant convolution. We show that other methods\ntreating features as the Fourier coefficients in the stabilizer subgroup are\nspecial cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show\nstate-of-the-art performance in spherical vector field regression, point cloud\nclassification, and molecular completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinshuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiahui Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Correspondence: Humans as a Cue for Extreme-View Geometry. (arXiv:2206.08365v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08365","description":"<p>Recovering the spatial layout of the cameras and the geometry of the scene\nfrom extreme-view images is a longstanding challenge in computer vision.\nPrevailing 3D reconstruction algorithms often adopt the image matching paradigm\nand presume that a portion of the scene is co-visible across images, yielding\npoor performance when there is little overlap among inputs. In contrast, humans\ncan associate visible parts in one image to the corresponding invisible\ncomponents in another image via prior knowledge of the shapes. Inspired by this\nfact, we present a novel concept called virtual correspondences (VCs). VCs are\na pair of pixels from two images whose camera rays intersect in 3D. Similar to\nclassic correspondences, VCs conform with epipolar geometry; unlike classic\ncorrespondences, VCs do not need to be co-visible across views. Therefore VCs\ncan be established and exploited even if images do not overlap. We introduce a\nmethod to find virtual correspondences based on humans in the scene. We\nshowcase how VCs can be seamlessly integrated with classic bundle adjustment to\nrecover camera poses across extreme views. Experiments show that our method\nsignificantly outperforms state-of-the-art camera pose estimation methods in\nchallenging scenarios and is comparable in the traditional densely captured\nsetup. Our approach also unleashes the potential of multiple downstream tasks\nsuch as scene reconstruction from multi-view stereo and novel view synthesis in\nextreme-view scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wei-Chiu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Anqi Joyce Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shenlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation. (arXiv:2206.08367v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08367","description":"<p>Adapting to a continuously evolving environment is a safety-critical\nchallenge inevitably faced by all autonomous driving systems. Existing image\nand video driving datasets, however, fall short of capturing the mutable nature\nof the real world. In this paper, we introduce the largest multi-task synthetic\ndataset for autonomous driving, SHIFT. It presents discrete and continuous\nshifts in cloudiness, rain and fog intensity, time of day, and vehicle and\npedestrian density. Featuring a comprehensive sensor suite and annotations for\nseveral mainstream perception tasks, SHIFT allows investigating the degradation\nof a perception system performance at increasing levels of domain shift,\nfostering the development of continuous adaptation strategies to mitigate this\nproblem and assess model robustness and generality. Our dataset and benchmark\ntoolkit are publicly available at www.vis.xyz/shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1\">Mattia Segu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1\">Janis Postels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased 4D: Monocular 4D Reconstruction with a Neural Deformation Model. (arXiv:2206.08368v1 [cs.CV])","link":"http://arxiv.org/abs/2206.08368","description":"<p>Capturing general deforming scenes is crucial for many computer graphics and\nvision applications, and it is especially challenging when only a monocular RGB\nvideo of the scene is available. Competing methods assume dense point tracks,\n3D templates, large-scale training datasets, or only capture small-scale\ndeformations. In contrast to those, our method, Ub4D, makes none of these\nassumptions while outperforming the previous state of the art in challenging\nscenarios. Our technique includes two new, in the context of non-rigid 3D\nreconstruction, components, i.e., 1) A coordinate-based and implicit neural\nrepresentation for non-rigid scenes, which enables an unbiased reconstruction\nof dynamic scenes, and 2) A novel dynamic scene flow loss, which enables the\nreconstruction of larger deformations. Results on our new dataset, which will\nbe made publicly available, demonstrate the clear improvement over the state of\nthe art in terms of surface reconstruction accuracy and robustness to large\ndeformations. Visit the project page https://4dqv.mpi-inf.mpg.de/Ub4D/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Johnson_E/0/1/0/all/0/1\">Erik C.M. Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust and Reproducible Active Learning Using Neural Networks. (arXiv:2002.09564v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2002.09564","description":"<p>Active learning (AL) is a promising ML paradigm that has the potential to\nparse through large unlabeled data and help reduce annotation cost in domains\nwhere labeling data can be prohibitive. Recently proposed neural network based\nAL methods use different heuristics to accomplish this goal. In this study, we\ndemonstrate that under identical experimental settings, different types of AL\nalgorithms (uncertainty based, diversity based, and committee based) produce an\ninconsistent gain over random sampling baseline. Through a variety of\nexperiments, controlling for sources of stochasticity, we show that variance in\nperformance metrics achieved by AL algorithms can lead to results that are not\nconsistent with the previously reported results. We also found that under\nstrong regularization, AL methods show marginal or no advantage over the random\nsampling baseline under a variety of experimental conditions. Finally, we\nconclude with a set of recommendations on how to assess the results using a new\nAL algorithm to ensure results are reproducible and robust under changes in\nexperimental conditions. We share our codes to facilitate AL evaluations. We\nbelieve our findings and recommendations will help advance reproducible\nresearch in AL using neural networks. We open source our code at\nhttps://github.com/PrateekMunjal/TorchAL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munjal_P/0/1/0/all/0/1\">Prateek Munjal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_N/0/1/0/all/0/1\">Nasir Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sourati_J/0/1/0/all/0/1\">Jamshid Sourati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shadab Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Anti-Spoofing by Learning Polarization Cues in a Real-World Scenario. (arXiv:2003.08024v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.08024","description":"<p>Face anti-spoofing is the key to preventing security breaches in biometric\nrecognition applications. Existing software-based and hardware-based face\nliveness detection methods are effective in constrained environments or\ndesignated datasets only. Deep learning method using RGB and infrared images\ndemands a large amount of training data for new attacks. In this paper, we\npresent a face anti-spoofing method in a real-world scenario by automatic\nlearning the physical characteristics in polarization images of a real face\ncompared to a deceptive attack. A computational framework is developed to\nextract and classify the unique face features using convolutional neural\nnetworks and SVM together. Our real-time polarized face anti-spoofing (PAAS)\ndetection method uses a on-chip integrated polarization imaging sensor with\noptimized processing algorithms. Extensive experiments demonstrate the\nadvantages of the PAAS technique to counter diverse face spoofing attacks\n(print, replay, mask) in uncontrolled indoor and outdoor conditions by learning\npolarized face images of 33 people. A four-directional polarized face image\ndataset is released to inspire future applications within biometric\nanti-spoofing field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pointly-Supervised Instance Segmentation. (arXiv:2104.06404v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06404","description":"<p>We propose an embarrassingly simple point annotation scheme to collect weak\nsupervision for instance segmentation. In addition to bounding boxes, we\ncollect binary labels for a set of points uniformly sampled inside each\nbounding box. We show that the existing instance segmentation models developed\nfor full mask supervision can be seamlessly trained with point-based\nsupervision collected via our scheme. Remarkably, Mask R-CNN trained on COCO,\nPASCAL VOC, Cityscapes, and LVIS with only 10 annotated random points per\nobject achieves 94%--98% of its fully-supervised performance, setting a strong\nbaseline for weakly-supervised instance segmentation. The new point annotation\nscheme is approximately 5 times faster than annotating full object masks,\nmaking high-quality instance segmentation more accessible in practice.\n</p>\n<p>Inspired by the point-based annotation form, we propose a modification to\nPointRend instance segmentation module. For each object, the new architecture,\ncalled Implicit PointRend, generates parameters for a function that makes the\nfinal point-level mask prediction. Implicit PointRend is more straightforward\nand uses a single point-level mask loss. Our experiments show that the new\nmodule is more suitable for the point-based supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bowen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parkhi_O/0/1/0/all/0/1\">Omkar Parkhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEEMD: Drug Efficacy Estimation against SARS-CoV-2 based on cell Morphology with Deep multiple instance learning. (arXiv:2105.05758v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.05758","description":"<p>Drug repurposing can accelerate the identification of effective compounds for\nclinical use against SARS-CoV-2, with the advantage of pre-existing clinical\nsafety data and an established supply chain. RNA viruses such as SARS-CoV-2\nmanipulate cellular pathways and induce reorganization of subcellular\nstructures to support their life cycle. These morphological changes can be\nquantified using bioimaging techniques. In this work, we developed DEEMD: a\ncomputational pipeline using deep neural network models within a multiple\ninstance learning framework, to identify putative treatments effective against\nSARS-CoV-2 based on morphological analysis of the publicly available RxRx19a\ndataset. This dataset consists of fluorescence microscopy images of SARS-CoV-2\nnon-infected cells and infected cells, with and without drug treatment. DEEMD\nfirst extracts discriminative morphological features to generate cell\nmorphological profiles from the non-infected and infected cells. These\nmorphological profiles are then used in a statistical model to estimate the\napplied treatment efficacy on infected cells based on similarities to\nnon-infected cells. DEEMD is capable of localizing infected cells via weak\nsupervision without any expensive pixel-level annotations. DEEMD identifies\nknown SARS-CoV-2 inhibitors, such as Remdesivir and Aloxistatin, supporting the\nvalidity of our approach. DEEMD can be explored for use on other emerging\nviruses and datasets to rapidly identify candidate antiviral treatments in the\nfuture}. Our implementation is available online at\nhttps://www.github.com/Sadegh-Saberian/DEEMD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saberian_M/0/1/0/all/0/1\">M.Sadegh Saberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriarty_K/0/1/0/all/0/1\">Kathleen P. Moriarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olmstead_A/0/1/0/all/0/1\">Andrea D. Olmstead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallgrimson_C/0/1/0/all/0/1\">Christian Hallgrimson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jean_F/0/1/0/all/0/1\">Fran&#xe7;ois Jean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_I/0/1/0/all/0/1\">Ivan R. Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libbrecht_M/0/1/0/all/0/1\">Maxwell W. Libbrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenCoS: Contrastive Semi-supervised Learning for Handling Open-set Unlabeled Data. (arXiv:2107.08943v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08943","description":"<p>Semi-supervised learning (SSL) is one of the most promising paradigms to\ncircumvent the expensive labeling cost for building a high-performance model.\nMost existing SSL methods conventionally assume both labeled and unlabeled data\nare drawn from the same (class) distribution. However, unlabeled data may\ninclude out-of-class samples in practice; those that cannot have one-hot\nencoded labels from a closed-set of classes in label data, i.e. unlabeled data\nis an open-set. In this paper, we introduce OpenCoS, a method for handling this\nrealistic semi-supervised learning scenario based upon a recent framework of\nself-supervised visual representation learning. Specifically, we first observe\nthat the out-of-class samples in the open-set unlabeled dataset can be\nidentified effectively via self-supervised contrastive learning. Then, OpenCoS\nutilizes this information to overcome the failure modes in the existing\nstate-of-the-art semi-supervised methods, by utilizing one-hot pseudo-labels\nand soft-labels for the identified in- and out-of-class unlabeled data,\nrespectively. Our extensive experimental results show the effectiveness of\nOpenCoS, fixing up the state-of-the-art semi-supervised methods to be suitable\nfor diverse scenarios involving open-set unlabeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongjin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sukmin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jongheon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-Domain Generalization from a Single Source: An Uncertainty Quantification Approach. (arXiv:2108.02888v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02888","description":"<p>We are concerned with a worst-case scenario in model generalization, in the\nsense that a model aims to perform well on many unseen domains while there is\nonly one single domain available for training. We propose Meta-Learning based\nAdversarial Domain Augmentation to solve this Out-of-Domain generalization\nproblem. The key idea is to leverage adversarial training to create\n\"fictitious\" yet \"challenging\" populations, from which a model can learn to\ngeneralize with theoretical guarantees. To facilitate fast and desirable domain\naugmentation, we cast the model training in a meta-learning scheme and use a\nWasserstein Auto-Encoder to relax the widely used worst-case constraint. We\nfurther improve our method by integrating uncertainty quantification for\nefficient domain generalization. Extensive experiments on multiple benchmark\ndatasets indicate its superior performance in tackling single domain\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1\">Fengchun Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09818","description":"<p>Convolutional Neural Networks have demonstrated dermatologist-level\nperformance in the classification of melanoma from skin lesion images, but\nprediction irregularities due to biases seen within the training data are an\nissue that should be addressed before widespread deployment is possible. In\nthis work, we robustly remove bias and spurious variation from an automated\nmelanoma classification pipeline using two leading bias unlearning techniques.\nWe show that the biases introduced by surgical markings and rulers presented in\nprevious studies can be reasonably mitigated using these bias removal methods.\nWe also demonstrate the generalisation benefits of unlearning spurious\nvariation relating to the imaging instrument used to capture lesion images. Our\nexperimental results provide evidence that the effects of each of the\naforementioned biases are notably reduced, with different debiasing techniques\nexcelling at different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.09960v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09960","description":"<p>In this paper, we propose a novel mutual consistency network (MC-Net+) to\neffectively exploit the unlabeled data for semi-supervised medical image\nsegmentation. The MC-Net+ model is motivated by the observation that deep\nmodels trained with limited annotations are prone to output highly uncertain\nand easily mis-classified predictions in the ambiguous regions (e.g., adhesive\nedges or thin branches) for medical image segmentation. Leveraging these\nchallenging samples can make the semi-supervised segmentation model training\nmore effective. Therefore, our proposed MC-Net+ model consists of two new\ndesigns. First, the model contains one shared encoder and multiple slightly\ndifferent decoders (i.e., using different up-sampling strategies). The\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, we\napply a novel mutual consistency constraint between one decoder's probability\noutput and other decoders' soft pseudo labels. In this way, we minimize the\ndiscrepancy of multiple outputs (i.e., the model uncertainty) during training\nand force the model to generate invariant results in such challenging regions,\naiming at regularizing the model training. We compared the segmentation results\nof our MC-Net+ model with five state-of-the-art semi-supervised approaches on\nthree public medical datasets. Extension experiments with two standard\nsemi-supervised settings demonstrate the superior performance of our model over\nother methods, which sets a new state of the art for semi-supervised medical\nimage segmentation. Our code is released publicly at\nhttps://github.com/ycwu1997/MC-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation. (arXiv:2110.13939v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13939","description":"<p>Generating safety-critical scenarios, which are crucial yet difficult to\ncollect, provides an effective way to evaluate the robustness of autonomous\ndriving systems. However, the diversity of scenarios and efficiency of\ngeneration methods are heavily restricted by the rareness and structure of\nsafety-critical scenarios. Therefore, existing generative models that only\nestimate distributions from observational data are not satisfying to solve this\nproblem. In this paper, we integrate causality as a prior into the scenario\ngeneration and propose a flow-based generative framework, Causal Autoregressive\nFlow (CausalAF). CausalAF encourages the generative model to uncover and follow\nthe causal relationship among generated objects via novel causal masking\noperations instead of searching the sample only from observational data. By\nlearning the cause-and-effect mechanism of how the generated scenario causes\nrisk situations rather than just learning correlations from data, CausalAF\nsignificantly improves learning efficiency. Extensive experiments on three\nheterogeneous traffic scenarios illustrate that CausalAF requires much fewer\noptimization resources to effectively generate safety-critical scenarios. We\nalso show that using generated scenarios as additional training samples\nempirically improves the robustness of autonomous driving algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haohong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Inverse Problems in Medical Imaging with Score-Based Generative Models. (arXiv:2111.08005v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.08005","description":"<p>Reconstructing medical images from partial measurements is an important\ninverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging\n(MRI). Existing solutions based on machine learning typically train a model to\ndirectly map measurements to medical images, leveraging a training dataset of\npaired images and measurements. These measurements are typically synthesized\nfrom images using a fixed physical model of the measurement process, which\nhinders the generalization capability of models to unknown measurement\nprocesses. To address this issue, we propose a fully unsupervised technique for\ninverse problem solving, leveraging the recently introduced score-based\ngenerative models. Specifically, we first train a score-based generative model\non medical images to capture their prior distribution. Given measurements and a\nphysical model of the measurement process at test time, we introduce a sampling\nmethod to reconstruct an image consistent with both the prior and the observed\nmeasurements. Our method does not assume a fixed measurement process during\ntraining, and can thus be flexibly adapted to different measurement processes\nat test time. Empirically, we observe comparable or better performance to\nsupervised learning techniques in several medical imaging tasks in CT and MRI,\nwhile demonstrating significantly better generalization to unknown measurement\nprocesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Liyue Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeCGAN: Parallel Conditional Generative Adversarial Networks for Face Editing via Semantic Consistency. (arXiv:2111.09298v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09298","description":"<p>Semantically guided conditional Generative Adversarial Networks (cGANs) have\nbecome a popular approach for face editing in recent years. However, most\nexisting methods introduce semantic masks as direct conditional inputs to the\ngenerator and often require the target masks to perform the corresponding\ntranslation in the RGB space. We propose SeCGAN, a novel label-guided cGAN for\nediting face images utilising semantic information without the need to specify\ntarget semantic masks. During training, SeCGAN has two branches of generators\nand discriminators operating in parallel, with one trained to translate RGB\nimages and the other for semantic masks. To bridge the two branches in a\nmutually beneficial manner, we introduce a semantic consistency loss which\nconstrains both branches to have consistent semantic outputs. Whilst both\nbranches are required during training, the RGB branch is our primary network\nand the semantic branch is not needed for inference. Our results on CelebA and\nCelebA-HQ demonstrate that our approach is able to generate facial images with\nmore accurate attributes, outperforming competitive baselines in terms of\nTarget Attribute Recognition Rate whilst maintaining quality metrics such as\nself-supervised Fr\\'{e}chet Inception Distance and Inception Score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiaze Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae-Kyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked-attention Mask Transformer for Universal Image Segmentation. (arXiv:2112.01527v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01527","description":"<p>Image segmentation is about grouping pixels with different semantics, e.g.,\ncategory or instance membership, where each choice of semantics defines a task.\nWhile only the semantics of each task differ, current research focuses on\ndesigning specialized architectures for each task. We present Masked-attention\nMask Transformer (Mask2Former), a new architecture capable of addressing any\nimage segmentation task (panoptic, instance or semantic). Its key components\ninclude masked attention, which extracts localized features by constraining\ncross-attention within predicted mask regions. In addition to reducing the\nresearch effort by at least three times, it outperforms the best specialized\narchitectures by a significant margin on four popular datasets. Most notably,\nMask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on\nCOCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7\nmIoU on ADE20K).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bowen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wild ToFu: Improving Range and Quality of Indirect Time-of-Flight Depth with RGB Fusion in Challenging Environments. (arXiv:2112.03750v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03750","description":"<p>Indirect Time-of-Flight (I-ToF) imaging is a widespread way of depth\nestimation for mobile devices due to its small size and affordable price.\nPrevious works have mainly focused on quality improvement for I-ToF imaging\nespecially curing the effect of Multi Path Interference (MPI). These\ninvestigations are typically done in specifically constrained scenarios at\nclose distance, indoors and under little ambient light. Surprisingly little\nwork has investigated I-ToF quality improvement in real-life scenarios where\nstrong ambient light and far distances pose difficulties due to an extreme\namount of induced shot noise and signal sparsity, caused by the attenuation\nwith limited sensor power and light scattering. In this work, we propose a new\nlearning based end-to-end depth prediction network which takes noisy raw I-ToF\nsignals as well as an RGB image and fuses their latent representation based on\na multi step approach involving both implicit and explicit alignment to predict\na high quality long range depth map aligned to the RGB viewpoint. We test our\napproach on challenging real-world scenes and show more than 40% RMSE\nimprovement on the final depth map compared to the baseline approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">HyunJun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brasch_N/0/1/0/all/0/1\">Nikolas Brasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View. (arXiv:2112.11790v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11790","description":"<p>Autonomous driving perceives its surroundings for decision making, which is\none of the most complex scenarios in visual perception. The success of paradigm\ninnovation in solving the 2D object detection task inspires us to seek an\nelegant, feasible, and scalable paradigm for fundamentally pushing the\nperformance boundary in this area. To this end, we contribute the BEVDet\nparadigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View\n(BEV), where most target values are defined and route planning can be handily\nperformed. We merely reuse existing modules to build its framework but\nsubstantially develop its performance by constructing an exclusive data\naugmentation strategy and upgrading the Non-Maximum Suppression strategy. In\nthe experiment, BEVDet offers an excellent trade-off between accuracy and\ntime-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS\non the nuScenes val set. It is comparable with FCOS3D, but requires just 11%\ncomputational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS.\nAnother high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2%\nNDS, significantly exceeding all published results. With a comparable inference\nspeed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The\nsource code is publicly available for further research at\nhttps://github.com/HuangJunJie2017/BEVDet .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dalong Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Synthetic InSAR with Vision Transformers: The case of volcanic unrest detection. (arXiv:2201.03016v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.03016","description":"<p>The detection of early signs of volcanic unrest preceding an eruption, in the\nform of ground deformation in Interferometric Synthetic Aperture Radar (InSAR)\ndata is critical for assessing volcanic hazard. In this work we treat this as a\nbinary classification problem of InSAR images, and propose a novel deep\nlearning methodology that exploits a rich source of synthetically generated\ninterferograms to train quality classifiers that perform equally well in real\ninterferograms. The imbalanced nature of the problem, with orders of magnitude\nfewer positive samples, coupled with the lack of a curated database with\nlabeled InSAR data, sets a challenging task for conventional deep learning\narchitectures. We propose a new framework for domain adaptation, in which we\nlearn class prototypes from synthetic data with vision transformers. We report\ndetection accuracy that amounts to the highest reported accuracy on a large\ntest set for volcanic unrest detection. Moreover, we built upon this knowledge\nby learning a new, non-linear, projection between the learnt representations\nand prototype space, using pseudo labels produced by our model from an\nunlabeled real InSAR dataset. This leads to the new state of the art with 97.1%\naccuracy on our test set. We demonstrate the robustness of our approach by\ntraining a simple ResNet-18 Convolutional Neural Network on the unlabeled real\nInSAR dataset with pseudo-labels generated from our top transformer-prototype\nmodel. Our methodology provides a significant improvement in performance\nwithout the need of manually labeling any sample, opening the road for further\nexploitation of synthetic InSAR data in various remote sensing applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bountos_N/0/1/0/all/0/1\">Nikolaos Ioannis Bountos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michail_D/0/1/0/all/0/1\">Dimitrios Michail</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papoutsis_I/0/1/0/all/0/1\">Ioannis Papoutsis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?. (arXiv:2201.06346v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06346","description":"<p>Even though Generative Adversarial Networks (GANs) have shown a remarkable\nability to generate high-quality images, GANs do not always guarantee the\ngeneration of photorealistic images. Occasionally, they generate images that\nhave defective or unnatural objects, which are referred to as 'artifacts'.\nResearch to investigate why these artifacts emerge and how they can be detected\nand removed has yet to be sufficiently carried out. To analyze this, we first\nhypothesize that rarely activated neurons and frequently activated neurons have\ndifferent purposes and responsibilities for the progress of generating images.\nIn this study, by analyzing the statistics and the roles for those neurons, we\nempirically show that rarely activated neurons are related to the failure\nresults of making diverse objects and inducing artifacts. In addition, we\nsuggest a correction method, called 'Sequential Ablation', to repair the\ndefective part of the generated images without high computational cost and\nmanual efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hwanil Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Wonjoon Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaesik Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Condensation with Contrastive Signals. (arXiv:2202.02916v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02916","description":"<p>Recent studies have demonstrated that gradient matching-based dataset\nsynthesis, or dataset condensation (DC), methods can achieve state-of-the-art\nperformance when applied to data-efficient learning tasks. However, in this\nstudy, we prove that the existing DC methods can perform worse than the random\nselection method when task-irrelevant information forms a significant part of\nthe training dataset. We attribute this to the lack of participation of the\ncontrastive signals between the classes resulting from the class-wise gradient\nmatching strategy. To address this problem, we propose Dataset Condensation\nwith Contrastive signals (DCC) by modifying the loss function to enable the DC\nmethods to effectively capture the differences between classes. In addition, we\nanalyze the new loss function in terms of training dynamics by tracking the\nkernel velocity. Furthermore, we introduce a bi-level warm-up strategy to\nstabilize the optimization. Our experimental results indicate that while the\nexisting methods are ineffective for fine-grained image classification tasks,\nthe proposed method can successfully generate informative synthetic datasets\nfor the same tasks. Moreover, we demonstrate that the proposed method\noutperforms the baselines even on benchmark datasets such as SVHN, CIFAR-10,\nand CIFAR-100. Finally, we demonstrate the high applicability of the proposed\nmethod by applying it to continual learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Saehyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sangwon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Cyclical Training of Neural Networks. (arXiv:2202.08835v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.08835","description":"<p>This paper describes the principle of \"General Cyclical Training\" in machine\nlearning, where training starts and ends with \"easy training\" and the \"hard\ntraining\" happens during the middle epochs. We propose several manifestations\nfor training neural networks, including algorithmic examples (via\nhyper-parameters and loss functions), data-based examples, and model-based\nexamples. Specifically, we introduce several novel techniques: cyclical weight\ndecay, cyclical batch size, cyclical focal loss, cyclical softmax temperature,\ncyclical data augmentation, cyclical gradient clipping, and cyclical\nsemi-supervised learning. In addition, we demonstrate that cyclical weight\ndecay, cyclical softmax temperature, and cyclical gradient clipping (as three\nexamples of this principle) are beneficial in the test accuracy performance of\na trained model. Furthermore, we discuss model-based examples (such as\npretraining and knowledge distillation) from the perspective of general\ncyclical training and recommend some changes to the typical training\nmethodology. In summary, this paper defines the general cyclical training\nconcept and discusses several specific ways in which this concept can be\napplied to training neural networks. In the spirit of reproducibility, the code\nused in our experiments is available at \\url{https://github.com/lnsmith54/CFL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Leslie N. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyclical Focal Loss. (arXiv:2202.08978v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08978","description":"<p>The cross-entropy softmax loss is the primary loss function used to train\ndeep neural networks. On the other hand, the focal loss function has been\ndemonstrated to provide improved performance when there is an imbalance in the\nnumber of training samples in each class, such as in long-tailed datasets. In\nthis paper, we introduce a novel cyclical focal loss and demonstrate that it is\na more universal loss function than cross-entropy softmax loss or focal loss.\nWe describe the intuition behind the cyclical focal loss and our experiments\nprovide evidence that cyclical focal loss provides superior performance for\nbalanced, imbalanced, or long-tailed datasets. We provide numerous experimental\nresults for CIFAR-10/CIFAR-100, ImageNet, balanced and imbalanced 4,000\ntraining sample versions of CIFAR-10/CIFAR-100, and ImageNet-LT and Places-LT\nfrom the Open Long-Tailed Recognition (OLTR) challenge. Implementing the\ncyclical focal loss function requires only a few lines of code and does not\nincrease training time. In the spirit of reproducibility, our code is available\nat \\url{https://github.com/lnsmith54/CFL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Leslie N. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation. (arXiv:2203.01324v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01324","description":"<p>Semi-supervised segmentation remains challenging in medical imaging since the\namount of annotated medical data is often limited and there are many blurred\npixels near the adhesive edges or low-contrast regions. To address the issues,\nwe advocate to firstly constrain the consistency of samples with and without\nstrong perturbations to apply sufficient smoothness regularization and further\nencourage the class-level separation to exploit the unlabeled ambiguous pixels\nfor the model training. Particularly, in this paper, we propose the SS-Net for\nsemi-supervised medical image segmentation tasks, via exploring the pixel-level\nSmoothness and inter-class Separation at the same time. The pixel-level\nsmoothness forces the model to generate invariant results under adversarial\nperturbations. Meanwhile, the inter-class separation constrains individual\nclass features should approach their corresponding high-quality prototypes, in\norder to make each class distribution compact and separate different classes.\nWe evaluated our SS-Net against five recent methods on the public LA and ACDC\ndatasets. The experimental results under two semi-supervised settings\ndemonstrate the superiority of our proposed SS-Net, achieving new\nstate-of-the-art (SOTA) performance on both datasets. The code is available at\nhttps://github.com/ycwu1997/SS-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qianyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging. (arXiv:2203.02149v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.02149","description":"<p>The rapid development of deep learning provides a better solution for the\nend-to-end reconstruction of hyperspectral image (HSI). However, existing\nlearning-based methods have two major defects. Firstly, networks with\nself-attention usually sacrifice internal resolution to balance model\nperformance against complexity, losing fine-grained high-resolution (HR)\nfeatures. Secondly, even if the optimization focusing on spatial-spectral\ndomain learning (SDL) converges to the ideal solution, there is still a\nsignificant visual difference between the reconstructed HSI and the truth.\nTherefore, we propose a high-resolution dual-domain learning network (HDNet)\nfor HSI reconstruction. On the one hand, the proposed HR spatial-spectral\nattention module with its efficient feature fusion provides continuous and fine\npixel-level features. On the other hand, frequency domain learning (FDL) is\nintroduced for HSI reconstruction to narrow the frequency domain discrepancy.\nDynamic FDL supervision forces the model to reconstruct fine-grained\nfrequencies and compensate for excessive smoothing and distortion caused by\npixel-level losses. The HR pixel-level attention and frequency-level refinement\nin our HDNet mutually promote HSI perceptual quality. Extensive quantitative\nand qualitative evaluation experiments show that our method achieves SOTA\nperformance on simulated and real HSI datasets. Code and models will be\nreleased at https://github.com/caiyuanhao1998/MST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-scale Comprehensive Dataset and Copy-overlap Aware Evaluation Protocol for Segment-level Video Copy Detection. (arXiv:2203.02654v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02654","description":"<p>In this paper, we introduce VCSL (Video Copy Segment Localization), a new\ncomprehensive segment-level annotated video copy dataset. Compared with\nexisting copy detection datasets restricted by either video-level annotation or\nsmall-scale, VCSL not only has two orders of magnitude more segment-level\nlabelled data, with 160k realistic video copy pairs containing more than 280k\nlocalized copied segment pairs, but also covers a variety of video categories\nand a wide range of video duration. All the copied segments inside each\ncollected video pair are manually extracted and accompanied by precisely\nannotated starting and ending timestamps. Alongside the dataset, we also\npropose a novel evaluation protocol that better measures the prediction\naccuracy of copy overlapping segments between a video pair and shows improved\nadaptability in different scenarios. By benchmarking several baseline and\nstate-of-the-art segment-level video copy detection methods with the proposed\ndataset and evaluation metric, we provide a comprehensive analysis that\nuncovers the strengths and weaknesses of current approaches, hoping to open up\npromising directions for future works. The VCSL dataset, metric and benchmark\ncodes are all publicly available at https://github.com/alipay/VCSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sifeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xudong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Furong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingxiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_F/0/1/0/all/0/1\">Feng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaobo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction. (arXiv:2203.04845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04845","description":"<p>Many algorithms have been developed to solve the inverse problem of coded\naperture snapshot spectral imaging (CASSI), i.e., recovering the 3D\nhyperspectral images (HSIs) from a 2D compressive measurement. In recent years,\nlearning-based methods have demonstrated promising performance and dominated\nthe mainstream research direction. However, existing CNN-based methods show\nlimitations in capturing long-range dependencies and non-local self-similarity.\nPrevious Transformer-based methods densely sample tokens, some of which are\nuninformative, and calculate the multi-head self-attention (MSA) between some\ntokens that are unrelated in content. This does not fit the spatially sparse\nnature of HSI signals and limits the model scalability. In this paper, we\npropose a novel Transformer-based method, coarse-to-fine sparse Transformer\n(CST), firstly embedding HSI sparsity into deep learning for HSI\nreconstruction. In particular, CST uses our proposed spectra-aware screening\nmechanism (SASM) for coarse patch selecting. Then the selected patches are fed\ninto our customized spectra-aggregation hashing multi-head self-attention\n(SAH-MSA) for fine pixel clustering and self-similarity capturing.\nComprehensive experiments show that our CST significantly outperforms\nstate-of-the-art methods while requiring cheaper computational costs. The code\nand models will be released at https://github.com/caiyuanhao1998/MST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking. (arXiv:2203.09948v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09948","description":"<p>Situation-aware technologies enabled by multiobject tracking (MOT) methods\nwill create new services and applications in fields such as autonomous\nnavigation and applied ocean sciences. Belief propagation (BP) is a\nstate-of-the-art method for Bayesian MOT but fully relies on a statistical\nmodel and preprocessed sensor measurements. In this paper, we establish a\nhybrid method for model-based and data-driven MOT. The proposed neural enhanced\nbelief propagation (NEBP) approach complements BP by information learned from\nraw sensor data with the goal to improve data association and to reject false\nalarm measurements. We evaluate the performance of our NEBP approach for MOT on\nthe nuScenes autonomous driving dataset and demonstrate that it can outperform\nstate-of-the-art reference methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingchao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_F/0/1/0/all/0/1\">Florian Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of April Tag and WhyCode Fiducial Systems for Autonomous Precision Drone Landing with a Gimbal-Mounted Camera. (arXiv:2203.10180v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10180","description":"<p>Fiducial markers provide a computationally cheap way for drones to determine\ntheir location with respect to a landing pad and execute precision landings.\nHowever, most existing work in this field uses a fixed, downward facing camera\nthat does not leverage the common gimbal-mounted camera setup found on many\ndrones. Such rigid systems cannot easily track detected markers, and may lose\nsight of the markers in non-ideal conditions (e.g. wind gusts). This paper\nevaluates April Tag and WhyCode fiducial systems for drone landing with a\ngimbal-mounted, monocular camera, with the advantage that the drone system can\ntrack the marker over time. However, since the orientation of the camera\nchanges, we must know the orientation of the marker, which is unreliable in\nmonocular fiducial systems. Additionally, the system must be fast. We propose 2\nmethods for mitigating the orientation ambiguity of WhyCode, and 1 method for\nincreasing the runtime detection rate of April Tag. We evaluate our 3 systems\nagainst 2 default systems in terms of marker orientation ambiguity, and\ndetection rate. We test rates of marker detection in a ROS framework on a\nRaspberry Pi 4, and we rank the systems in terms of their performance. Our\nfirst WhyCode variant significantly reduces orientation ambiguity with an\ninsignificant reduction in detection rate. Our second WhyCode variant does not\nshow significantly different orientation ambiguity from the default WhyCode\nsystem, but does provide additional functionality in terms of multi-marker\nWhyCode bundle arrangements. Our April Tag variant does not show performance\nimprovements on a Raspberry Pi 4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1\">Joshua Springer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyas_M/0/1/0/all/0/1\">Marcel Kyas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Document Recognition and Understanding with Dessurt. (arXiv:2203.16618v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16618","description":"<p>We introduce Dessurt, a relatively simple document understanding transformer\ncapable of being fine-tuned on a greater variety of document tasks than prior\nmethods. It receives a document image and task string as input and generates\narbitrary text autoregressively as output. Because Dessurt is an end-to-end\narchitecture that performs text recognition in addition to the document\nunderstanding, it does not require an external recognition model as prior\nmethods do. Dessurt is a more flexible model than prior methods and is able to\nhandle a variety of document domains and tasks. We show that this model is\neffective at 9 different dataset-task combinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_B/0/1/0/all/0/1\">Brian Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morse_B/0/1/0/all/0/1\">Bryan Morse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Bryan Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tensmeyer_C/0/1/0/all/0/1\">Chris Tensmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wigington_C/0/1/0/all/0/1\">Curtis Wigington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad Morariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection. (arXiv:2203.17054v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17054","description":"<p>Single frame data contains finite information which limits the performance of\nthe existing vision-based multi-camera 3D object detection paradigms. For\nfundamentally pushing the performance boundary in this area, a novel paradigm\ndubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the\nspatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive\nBEVDet framework with a few modifications just for fusing the feature from the\nprevious frame with the corresponding one in the current frame. In this way,\nwith negligible additional computing budget, we enable BEVDet4D to access the\ntemporal cues by querying and comparing the two candidate features. Beyond\nthis, we simplify the task of velocity prediction by removing the factors of\nego-motion and time in the learning target. As a result, BEVDet4D with robust\ngeneralization performance reduces the velocity error by up to -62.9%. This\nmakes the vision-based methods, for the first time, become comparable with\nthose relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes,\nwe report a new record of 54.5% NDS with the high-performance configuration\ndubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base\nby +7.3% NDS. The source code is publicly available for further research at\nhttps://github.com/HuangJunJie2017/BEVDet .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faculty Distillation with Optimal Transport. (arXiv:2204.11526v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.11526","description":"<p>The outpouring of various pre-trained models empowers knowledge\ndistillation~(KD) by providing abundant teacher resources. Meanwhile, exploring\nthe massive model repository to select a suitable teacher and further\nextracting its knowledge become daunting challenges. Standard KD fails to\nsurmount two obstacles when training a student with the availability of\nplentiful pre-trained teachers, i.e., the \"faculty\". First, we need to seek out\nthe most contributive teacher in the faculty efficiently rather than\nenumerating all of them for a student. Second, since the teacher may be\npre-trained on different tasks w.r.t. the student, we must distill the\nknowledge from a more general label space. This paper studies this ``faculty\ndistillation'' where a student performs teacher assessment and generalized\nknowledge reuse. We take advantage of optimal transport to construct a unifying\nobjective for both problems, which bridges the semantic gap and measures the\nrelatedness between a pair of models. This objective can select the most\nrelevant teacher, and we minimize the same objective over student parameters to\ntransfer the knowledge from the selected teacher subsequently. Experiments in\nvarious settings demonstrate the succinctness and versatility of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Su Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unlocking High-Accuracy Differentially Private Image Classification through Scale. (arXiv:2204.13650v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.13650","description":"<p>Differential Privacy (DP) provides a formal privacy guarantee preventing\nadversaries with access to a machine learning model from extracting information\nabout individual training points. Differentially Private Stochastic Gradient\nDescent (DP-SGD), the most popular DP training method for deep learning,\nrealizes this protection by injecting noise during training. However previous\nworks have found that DP-SGD often leads to a significant degradation in\nperformance on standard image classification benchmarks. Furthermore, some\nauthors have postulated that DP-SGD inherently performs poorly on large models,\nsince the norm of the noise required to preserve privacy is proportional to the\nmodel dimension. In contrast, we demonstrate that DP-SGD on over-parameterized\nmodels can perform significantly better than previously thought. Combining\ncareful hyper-parameter tuning with simple techniques to ensure signal\npropagation and improve the convergence rate, we obtain a new SOTA without\nextra data on CIFAR-10 of 81.4% under (8, 10^{-5})-DP using a 40-layer\nWide-ResNet, improving over the previous SOTA of 71.7%. When fine-tuning a\npre-trained NFNet-F3, we achieve a remarkable 83.8% top-1 accuracy on ImageNet\nunder (0.5, 8*10^{-7})-DP. Additionally, we also achieve 86.7% top-1 accuracy\nunder (8, 8 \\cdot 10^{-7})-DP, which is just 4.3% below the current non-private\nSOTA for this task. We believe our results are a significant step towards\nclosing the accuracy gap between private and non-private image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1\">Soham De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrada_L/0/1/0/all/0/1\">Leonard Berrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_J/0/1/0/all/0/1\">Jamie Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Samuel L. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balle_B/0/1/0/all/0/1\">Borja Balle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement. (arXiv:2205.03569v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03569","description":"<p>Compressed video action recognition has recently drawn growing attention,\nsince it remarkably reduces the storage and computational cost via replacing\nraw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,\nmotion vectors and residuals). However, this task severely suffers from the\ncoarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB\nand motion modalities. To address the two issues above, this paper proposes a\nnovel framework, namely Attentive Cross-modal Interaction Network with Motion\nEnhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for\nthe RGB modality and the other for the motion modality. Particularly, the\nmotion stream employs a multi-scale block embedded with a denoising module to\nenhance representation learning. The interaction between the two streams is\nthen strengthened by introducing the Selective Motion Complement (SMC) and\nCross-Modality Augment (CMA) modules, where SMC complements the RGB modality\nwith spatio-temporally attentive local motion features and CMA further combines\nthe two modalities with selective feature augmentation. Extensive experiments\non the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the\neffectiveness and efficiency of MEACI-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xiuguo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Consistency Learning for Semi-supervised Domain Adaptation. (arXiv:2205.04066v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04066","description":"<p>Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from\na fully labeled source domain to a scarcely labeled target domain. In this\npaper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.\nSpecifically, our MCL regularizes the consistency of different views of target\ndomain samples at three levels: (i) at inter-domain level, we robustly and\naccurately align the source and target domains using a prototype-based optimal\ntransport method that utilizes the pros and cons of different views of target\nsamples; (ii) at intra-domain level, we facilitate the learning of both\ndiscriminative and compact target feature representations by proposing a novel\nclass-wise contrastive clustering loss; (iii) at sample level, we follow\nstandard practice and improve the prediction accuracy by conducting a\nconsistency-based self-training. Empirically, we verified the effectiveness of\nour MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,\nand Office-Home datasets, and the experimental results demonstrate that our MCL\nframework achieves the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zizheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yushuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Convolutional Dictionary Network for CT Metal Artifact Reduction. (arXiv:2205.07471v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.07471","description":"<p>Inspired by the great success of deep neural networks, learning-based methods\nhave gained promising performances for metal artifact reduction (MAR) in\ncomputed tomography (CT) images. However, most of the existing approaches put\nless emphasis on modelling and embedding the intrinsic prior knowledge\nunderlying this specific MAR task into their network designs. Against this\nissue, we propose an adaptive convolutional dictionary network (ACDNet), which\nleverages both model-based and learning-based methods. Specifically, we explore\nthe prior structures of metal artifacts, e.g., non-local repetitive streaking\npatterns, and encode them as an explicit weighted convolutional dictionary\nmodel. Then, a simple-yet-effective algorithm is carefully designed to solve\nthe model. By unfolding every iterative substep of the proposed algorithm into\na network module, we explicitly embed the prior structure into a deep network,\n\\emph{i.e.,} a clear interpretability for the MAR task. Furthermore, our ACDNet\ncan automatically learn the prior for artifact-free CT images via training data\nand adaptively adjust the representation kernels for each input CT image based\non its content. Hence, our method inherits the clear interpretability of\nmodel-based methods and maintains the powerful representation ability of\nlearning-based methods. Comprehensive experiments executed on synthetic and\nclinical datasets show the superiority of our ACDNet in terms of effectiveness\nand model generalization. {\\color{blue}{{\\textit{Code is available at\n{\\url{https://github.com/hongwang01/ACDNet}}.}}}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging. (arXiv:2205.10102v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10102","description":"<p>In coded aperture snapshot spectral compressive imaging (CASSI) systems,\nhyperspectral image (HSI) reconstruction methods are employed to recover the\nspatial-spectral signal from a compressed measurement. Among these algorithms,\ndeep unfolding methods demonstrate promising performance but suffer from two\nissues. Firstly, they do not estimate the degradation patterns and\nill-posedness degree from the highly related CASSI to guide the iterative\nlearning. Secondly, they are mainly CNN-based, showing limitations in capturing\nlong-range dependencies. In this paper, we propose a principled\nDegradation-Aware Unfolding Framework (DAUF) that estimates parameters from the\ncompressed image and physical mask, and then uses these parameters to control\neach iteration. Moreover, we customize a novel Half-Shuffle Transformer (HST)\nthat simultaneously captures local contents and non-local dependencies. By\nplugging HST into DAUF, we establish the first Transformer-based deep unfolding\nmethod, Degradation-Aware Unfolding Half-Shuffle Transformer (DAUHST), for HSI\nreconstruction. Experiments show that DAUHST significantly surpasses\nstate-of-the-art methods while requiring cheaper computational and memory\ncosts. Code and models will be released at\nhttps://github.com/caiyuanhao1998/MST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration. (arXiv:2205.10195v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10195","description":"<p>How to properly model the inter-frame relation within the video sequence is\nan important but unsolved challenge for video restoration (VR). In this work,\nwe propose an unsupervised flow-aligned sequence-to-sequence model (S2SVR) to\naddress this problem. On the one hand, the sequence-to-sequence model, which\nhas proven capable of sequence modeling in the field of natural language\nprocessing, is explored for the first time in VR. Optimized serialization\nmodeling shows potential in capturing long-range dependencies among frames. On\nthe other hand, we equip the sequence-to-sequence model with an unsupervised\noptical flow estimator to maximize its potential. The flow estimator is trained\nwith our proposed unsupervised distillation loss, which can alleviate the data\ndiscrepancy and inaccurate degraded optical flow issues of previous flow-based\nmethods. With reliable optical flow, we can establish accurate correspondence\namong multiple frames, narrowing the domain difference between 1D language and\n2D misaligned frames and improving the potential of the sequence-to-sequence\nmodel. S2SVR shows superior performance in multiple VR tasks, including video\ndeblurring, video super-resolution, and compressed video quality enhancement.\nCode and models are publicly available at\nhttps://github.com/linjing7/VR-Baseline\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xueyi Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation. (arXiv:2205.13542v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13542","description":"<p>Multi-sensor fusion is essential for an accurate and reliable autonomous\ndriving system. Recent approaches are based on point-level fusion: augmenting\nthe LiDAR point cloud with camera features. However, the camera-to-LiDAR\nprojection throws away the semantic density of camera features, hindering the\neffectiveness of such methods, especially for semantic-oriented tasks (such as\n3D scene segmentation). In this paper, we break this deeply-rooted convention\nwith BEVFusion, an efficient and generic multi-task multi-sensor fusion\nframework. It unifies multi-modal features in the shared bird's-eye view (BEV)\nrepresentation space, which nicely preserves both geometric and semantic\ninformation. To achieve this, we diagnose and lift key efficiency bottlenecks\nin the view transformation with optimized BEV pooling, reducing latency by more\nthan 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports\ndifferent 3D perception tasks with almost no architectural changes. It\nestablishes the new state of the art on nuScenes, achieving 1.3% higher mAP and\nNDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with\n1.9x lower computation cost. Code to reproduce our results is available at\nhttps://github.com/mit-han-lab/bevfusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Huizi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual embeddings and self-consistency for self-supervised learning. (arXiv:2206.06023v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06023","description":"<p>Self-supervised Learning (SSL) has recently gained much attention due to the\nhigh cost and data limitation in the training of supervised learning models.\nThe current paradigm in the SSL is to utilize data augmentation at the input\nspace to create different views of the same images and train a model to\nmaximize the representations between similar images and minimize them for\ndifferent ones. While this approach achieves state-of-the-art (SOTA) results in\nvarious downstream tasks, it still lakes the opportunity to investigate the\nlatent space augmentation. This paper proposes TriMix, a novel concept for SSL\nthat generates virtual embeddings through linear interpolation of the data,\nthus providing the model with novel representations. Our strategy focuses on\ntraining the model to extract the original embeddings from virtual ones, hence,\nbetter representation learning. Additionally, we propose a self-consistency\nterm that improves the consistency between the virtual and actual embeddings.\nWe validate TriMix on eight benchmark datasets consisting of natural and\nmedical images with an improvement of 2.71% and 0.41% better than the\nsecond-best models for both data types. Further, our approach outperformed the\ncurrent methods in semi-supervised learning, particularly in low data regimes.\nBesides, our pre-trained models showed better transfer to other datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bdair_T/0/1/0/all/0/1\">Tariq Bdair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelhamid_H/0/1/0/all/0/1\">Hossam Abdelhamid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albarqouni_S/0/1/0/all/0/1\">Shadi Albarqouni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06829","description":"<p>Vision transformers (ViTs) are changing the landscape of object detection\napproaches. A natural usage of ViTs in detection is to replace the CNN-based\nbackbone with a transformer-based backbone, which is straightforward and\neffective, with the price of bringing considerable computation burden for\ninference. More subtle usage is the DETR family, which eliminates the need for\nmany hand-designed components in object detection but introduces a decoder\ndemanding an extra-long time to converge. As a result, transformer-based object\ndetection can not prevail in large-scale applications. To overcome these\nissues, we propose a novel decoder-free fully transformer-based (DFFT) object\ndetector, achieving high efficiency in both training and inference stages, for\nthe first time. We simplify objection detection into an encoder-only\nsingle-level anchor-based dense prediction problem by centering around two\nentry points: 1) Eliminate the training-inefficient decoder and leverage two\nstrong encoders to preserve the accuracy of single-level feature map\nprediction; 2) Explore low-level semantic features for the detection task with\nlimited computational resources. In particular, we design a novel lightweight\ndetection-oriented transformer backbone that efficiently captures low-level\nfeatures with rich semantics based on a well-conceived ablation study.\nExtensive experiments on the MS COCO benchmark demonstrate that DFFT_SMALL\noutperforms DETR by 2.5% AP with 28% computation cost reduction and more than\n$10$x fewer training epochs. Compared with the cutting-edge anchor-based\ndetector RetinaNet, DFFT_SMALL obtains over 5.5% AP gain while cutting down 70%\ncomputation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peixian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S$^2$-FPN: Scale-ware Strip Attention Guided Feature Pyramid Network for Real-time Semantic Segmentation. (arXiv:2206.07298v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07298","description":"<p>Modern high-performance semantic segmentation methods employ a heavy backbone\nand dilated convolution to extract the relevant feature. Although extracting\nfeatures with both contextual and semantic information is critical for the\nsegmentation tasks, it brings a memory footprint and high computation cost for\nreal-time applications. This paper presents a new model to achieve a trade-off\nbetween accuracy/speed for real-time road scene semantic segmentation.\nSpecifically, we proposed a lightweight model named Scale-aware Strip Attention\nGuided Feature Pyramid Network (S$^2$-FPN). Our network consists of three main\nmodules: Attention Pyramid Fusion (APF) module, Scale-aware Strip Attention\nModule (SSAM), and Global Feature Upsample (GFU) module. APF adopts an\nattention mechanisms to learn discriminative multi-scale features and help\nclose the semantic gap between different levels. APF uses the scale-aware\nattention to encode global context with vertical stripping operation and models\nthe long-range dependencies, which helps relate pixels with similar semantic\nlabel. In addition, APF employs channel-wise reweighting block (CRB) to\nemphasize the channel features. Finally, the decoder of S$^2$-FPN then adopts\nGFU, which is used to fuse features from APF and the encoder. Extensive\nexperiments have been conducted on two challenging semantic segmentation\nbenchmarks, which demonstrate that our approach achieves better accuracy/speed\ntrade-off with different model settings. The proposed models have achieved a\nresults of 76.2\\%mIoU/87.3FPS, 77.4\\%mIoU/67FPS, and 77.8\\%mIoU/30.5FPS on\nCityscapes dataset, and 69.6\\%mIoU,71.0\\% mIoU, and 74.2\\% mIoU on Camvid\ndataset. The code for this work will be made available at\n\\url{https://github.com/mohamedac29/S2-FPN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elhassan_M/0/1/0/all/0/1\">Mohammed A. M. Elhassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenxi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munea_T/0/1/0/all/0/1\">Tewodros Legesse Munea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xin Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Common Ground While Reserving Differences: Multiple Anatomy Collaborative Framework for Undersampled MRI Reconstruction. (arXiv:2206.07364v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.07364","description":"<p>Recently, deep neural networks have greatly advanced undersampled Magnetic\nResonance Image (MRI) reconstruction, wherein most studies follow the\none-anatomy-one-network fashion, i.e., each expert network is trained and\nevaluated for a specific anatomy. Apart from inefficiency in training multiple\nindependent models, such convention ignores the shared de-aliasing knowledge\nacross various anatomies which can benefit each other. To explore the shared\nknowledge, one naive way is to combine all the data from various anatomies to\ntrain an all-round network. Unfortunately, despite the existence of the shared\nde-aliasing knowledge, we reveal that the exclusive knowledge across different\nanatomies can deteriorate specific reconstruction targets, yielding overall\nperformance degradation. Observing this, in this study, we present a novel deep\nMRI reconstruction framework with both anatomy-shared and anatomy-specific\nparameterized learners, aiming to \"seek common ground while reserving\ndifferences\" across different anatomies.Particularly, the primary\nanatomy-shared learners are exposed to different anatomies to model flourishing\nshared knowledge, while the efficient anatomy-specific learners are trained\nwith their target anatomy for exclusive knowledge. Four different\nimplementations of anatomy-specific learners are presented and explored on the\ntop of our framework in two MRI reconstruction networks. Comprehensive\nexperiments on brain, knee and cardiac MRI datasets demonstrate that three of\nthese learners are able to enhance reconstruction performance via multiple\nanatomy collaborative learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Chenghui Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hanbo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1\">Jianhua Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Detection Methods for Die Attachment and Wire Bonding Defects in Integrated Circuit Manufacturing. (arXiv:2206.07481v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2206.07481","description":"<p>Defect detection plays a vital role in the manufacturing process of\nintegrated circuits (ICs). Die attachment and wire bonding are two steps of the\nmanufacturing process that determine the power and signal transmission quality\nand dependability in an IC. This paper presents a survey or literature review\nof the methods used for detecting these defects based on different sensing\nmodalities used including optical, radiological, acoustical, and infrared\nthermography. A discussion of the detection methods used is provided in this\nsurvey. Both conventional and deep learning approaches for detecting die\nattachment and wire bonding defects are considered along with challenges and\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alam_L/0/1/0/all/0/1\">Lamia Alam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kehtarnavaz_N/0/1/0/all/0/1\">Nasser Kehtarnavaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}