{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation. (arXiv:2207.03509v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03509","description":"<p>Large pretrained language models (PLMs) are often domain- or task-adapted via\nfine-tuning or prompting. Finetuning requires modifying all of the parameters\nand having enough data to avoid overfitting while prompting requires no\ntraining and few examples but limits performance. Instead, we prepare PLMs for\ndata- and parameter-efficient adaptation by learning to learn the difference\nbetween general and adapted PLMs. This difference is expressed in terms of\nmodel weights and sublayer structure through our proposed dynamic low-rank\nreparameterization and learned architecture controller. Experiments on few-shot\ndialogue completion, low-resource abstractive summarization, and multi-domain\nlanguage modeling show improvements in adaptation time and performance over\ndirect finetuning or preparation via domain-adaptive pretraining. Ablations\nshow our task-adaptive reparameterization (TARP) and model search (TAMS)\ncomponents individually improve on other parameter-efficient transfer like\nadapters and structure-learning methods like learned sparsification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_J/0/1/0/all/0/1\">Julian Salazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polovets_G/0/1/0/all/0/1\">George Polovets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus. (arXiv:2207.03546v1 [eess.AS])","link":"http://arxiv.org/abs/2207.03546","description":"<p>BibleTTS is a large, high-quality, open speech dataset for ten languages\nspoken in Sub-Saharan Africa. The corpus contains up to 86 hours of aligned,\nstudio quality 48kHz single speaker recordings per language, enabling the\ndevelopment of high-quality text-to-speech models. The ten languages\nrepresented are: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu,\nLingala, Luganda, Luo, and Yoruba. This corpus is a derivative work of Bible\nrecordings made and released by the Open.Bible project from Biblica. We have\naligned, cleaned, and filtered the original recordings, and additionally\nhand-checked a subset of the alignments for each language. We present results\nfor text-to-speech models with Coqui TTS. The data is released under a\ncommercial-friendly CC-BY-SA license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meyer_J/0/1/0/all/0/1\">Josh Meyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Casanova_E/0/1/0/all/0/1\">Edresson Casanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oktem_A/0/1/0/all/0/1\">Alp &#xd6;ktem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weber_D/0/1/0/all/0/1\">Daniel Whitenack Julian Weber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kabongo_S/0/1/0/all/0/1\">Salomon Kabongo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orife_I/0/1/0/all/0/1\">Iroro Orife</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leong_C/0/1/0/all/0/1\">Colin Leong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogayo_P/0/1/0/all/0/1\">Perez Ogayo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Emezue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukiibi_J/0/1/0/all/0/1\">Jonathan Mukiibi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agbolo_A/0/1/0/all/0/1\">Apelete Agbolo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akinode_V/0/1/0/all/0/1\">Victor Akinode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Opoku_B/0/1/0/all/0/1\">Bernard Opoku</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Olanrewaju_S/0/1/0/all/0/1\">Samuel Olanrewaju</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Muhammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Translation with Compiler Representations. (arXiv:2207.03578v1 [cs.PL])","link":"http://arxiv.org/abs/2207.03578","description":"<p>In this paper, we leverage low-level compiler intermediate representations\n(IR) to improve code translation. Traditional transpilers rely on syntactic\ninformation and handcrafted rules, which limits their applicability and\nproduces unnatural-looking code. Applying neural machine translation (NMT)\napproaches to code has successfully broadened the set of programs on which one\ncan get a natural-looking translation. However, they treat the code as\nsequences of text tokens, and still do not differentiate well enough between\nsimilar pieces of code which have different semantics in different languages.\nThe consequence is low quality translation, reducing the practicality of NMT,\nand stressing the need for an approach significantly increasing its accuracy.\nHere we propose to augment code translation with IRs, specifically LLVM IR,\nwith results on the C++, Java, Rust, and Go languages. Our method improves upon\nthe state of the art for unsupervised code translation, increasing the number\nof correct translations by 11% on average, and up to 79% for the Java - Rust\npair. We extend previous test sets for code translation, by adding hundreds of\nGo and Rust functions. Additionally, we train models with high performance on\nthe problem of IR decompilation, generating programming source code from IR,\nand study using IRs as intermediary pivot for translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szafraniec_M/0/1/0/all/0/1\">Marc Szafraniec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Roziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_H/0/1/0/all/0/1\">Hugh Leather Francois Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quote Erat Demonstrandum: A Web Interface for Exploring the Quotebank Corpus. (arXiv:2207.03592v1 [cs.IR])","link":"http://arxiv.org/abs/2207.03592","description":"<p>The use of attributed quotes is the most direct and least filtered pathway of\ninformation propagation in news. Consequently, quotes play a central role in\nthe conception, reception, and analysis of news stories. Since quotes provide a\nmore direct window into a speaker's mind than regular reporting, they are a\nvaluable resource for journalists and researchers alike. While substantial\nresearch efforts have been devoted to methods for the automated extraction of\nquotes from news and their attribution to speakers, few comprehensive corpora\nof attributed quotes from contemporary sources are available to the public.\nHere, we present an adaptive web interface for searching Quotebank, a massive\ncollection of quotes from the news, which we make available at\nhttps://quotebank.dlab.tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vukovic_V/0/1/0/all/0/1\">Vuk Vukovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akhil Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huan-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spitz_A/0/1/0/all/0/1\">Andreas Spitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering. (arXiv:2207.03637v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03637","description":"<p>The information in tables can be an important complement to text, making\ntable-based question answering (QA) systems of great value. The intrinsic\ncomplexity of handling tables often adds an extra burden to both model design\nand data annotation. In this paper, we aim to develop a simple table-based QA\nmodel with minimal annotation effort. Motivated by the fact that table-based QA\nrequires both alignment between questions and tables and the ability to perform\ncomplicated reasoning over multiple table elements, we propose an omnivorous\npretraining approach that consumes both natural and synthetic data to endow\nmodels with these respective abilities. Specifically, given freely available\ntables, we leverage retrieval to pair them with relevant natural sentences for\nmask-based pretraining, and synthesize NL questions by converting SQL sampled\nfrom tables for pretraining with a QA loss. We perform extensive experiments in\nboth few-shot and full settings, and the results clearly demonstrate the\nsuperiority of our model OmniTab, with the best multitasking approach achieving\nan absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively,\nalso establishing a new state-of-the-art on WikiTableQuestions. Detailed\nablations and analyses reveal different characteristics of natural and\nsynthetic data, shedding light on future directions in omnivorous pretraining.\nCode, pretraining data, and pretrained models are available at\nhttps://github.com/jzbjyb/OmniTab.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SETSum: Summarization and Visualization of Student Evaluations of Teaching. (arXiv:2207.03640v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03640","description":"<p>Student Evaluations of Teaching (SETs) are widely used in colleges and\nuniversities. Typically SET results are summarized for instructors in a static\nPDF report. The report often includes summary statistics for quantitative\nratings and an unsorted list of open-ended student comments. The lack of\norganization and summarization of the raw comments hinders those interpreting\nthe reports from fully utilizing informative feedback, making accurate\ninferences, and designing appropriate instructional improvements. In this work,\nwe introduce a novel system, SETSum, that leverages sentiment analysis, aspect\nextraction, summarization, and visualization techniques to provide organized\nillustrations of SET findings to instructors and other reviewers. Ten\nuniversity professors from diverse departments serve as evaluators of the\nsystem and all agree that SETSum helps them interpret SET results more\nefficiently; and 6 out of 10 instructors prefer our system over the standard\nstatic PDF report (while the remaining 4 would like to have both). This\ndemonstrates that our work holds the potential to reform the SET reporting\nconventions in the future. Our code is available at\nhttps://github.com/evahuyn/SETSum\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinuo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathy_V/0/1/0/all/0/1\">Viji Sathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panter_A/0/1/0/all/0/1\">A. T. Panter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Getting BART to Ride the Idiomatic Train: Learning to Represent Idiomatic Expressions. (arXiv:2207.03679v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03679","description":"<p>Idiomatic expressions (IEs), characterized by their non-compositionality, are\nan important part of natural language. They have been a classical challenge to\nNLP, including pre-trained language models that drive today's state-of-the-art.\nPrior work has identified deficiencies in their contextualized representation\nstemming from the underlying compositional paradigm of representation. In this\nwork, we take a first-principles approach to build idiomaticity into BART using\nan adapter as a lightweight non-compositional language expert trained on\nidiomatic sentences. The improved capability over baselines (e.g., BART) is\nseen via intrinsic and extrinsic methods, where idiom embeddings score 0.19\npoints higher in homogeneity score for embedding clustering, and up to 25%\nhigher sequence accuracy on the idiom processing tasks of IE sense\ndisambiguation and span detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1\">Suma Bhat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crake: Causal-Enhanced Table-Filler for Question Answering over Large Scale Knowledge Base. (arXiv:2207.03680v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03680","description":"<p>Semantic parsing solves knowledge base (KB) question answering (KBQA) by\ncomposing a KB query, which generally involves node extraction (NE) and graph\ncomposition (GC) to detect and connect related nodes in a query. Despite the\nstrong causal effects between NE and GC, previous works fail to directly model\nsuch causalities in their pipeline, hindering the learning of subtask\ncorrelations. Also, the sequence-generation process for GC in previous works\ninduces ambiguity and exposure bias, which further harms accuracy. In this\nwork, we formalize semantic parsing into two stages. In the first stage (graph\nstructure generation), we propose a causal-enhanced table-filler to overcome\nthe issues in sequence-modelling and to learn the internal causalities. In the\nsecond stage (relation extraction), an efficient beam-search algorithm is\npresented to scale complex queries on large-scale KBs. Experiments on LC-QuAD\n1.0 indicate that our method surpasses previous state-of-the-arts by a large\nmargin (17%) while remaining time and space efficiency. The code and models are\navailable at https://github.com/AOZMH/Crake.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanzeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1\">Lei Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Schema Networks. (arXiv:2207.03777v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03777","description":"<p>Most modern language models infer representations that, albeit powerful, lack\nboth compositionality and semantic interpretability. Starting from the\nassumption that a large proportion of semantic content is necessarily\nrelational, we introduce a neural language model that discovers networks of\nsymbols (schemata) from text datasets. Using a variational autoencoder (VAE)\nframework, our model encodes sentences into sequences of symbols (composed\nrepresentation), which correspond to the nodes visited by biased random walkers\non a global latent graph. Sentences are then generated back, conditioned on the\nselected symbol sequences. We first demonstrate that the model is able to\nuncover ground-truth graphs from artificially generated datasets of random\ntoken sequences. Next we leverage pretrained BERT and GPT-2 language models as\nencoder and decoder, respectively, to train our model on language modelling\ntasks. Qualitatively, our results show that the model is able to infer schema\nnetworks encoding different aspects of natural language. Quantitatively, the\nmodel achieves state-of-the-art scores on VAE language modeling benchmarks.\nSource code to reproduce our experiments is available at\nhttps://github.com/ramsesjsf/HiddenSchemaNetworks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1\">Rams&#xe9;s J. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conrads_L/0/1/0/all/0/1\">Lukas Conrads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welke_P/0/1/0/all/0/1\">Pascal Welke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cvejoski_K/0/1/0/all/0/1\">Kostadin Cvejoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojeda_C/0/1/0/all/0/1\">C&#xe9;sar Ojeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v1 [cs.SD])","link":"http://arxiv.org/abs/2207.03800","description":"<p>Unconstrained lip-to-speech synthesis aims to generate corresponding speeches\nfrom silent videos of talking faces with no restriction on head poses or\nvocabulary. Current works mainly use sequence-to-sequence models to solve this\nproblem, either in an autoregressive architecture or a flow-based\nnon-autoregressive architecture. However, these models suffer from several\ndrawbacks: 1) Instead of directly generating audios, they use a two-stage\npipeline that first generates mel-spectrograms and then reconstructs audios\nfrom the spectrograms. This causes cumbersome deployment and degradation of\nspeech quality due to error propagation; 2) The audio reconstruction algorithm\nused by these models limits the inference speed and audio quality, while neural\nvocoders are not available for these models since their output spectrograms are\nnot accurate enough; 3) The autoregressive model suffers from high inference\nlatency, while the flow-based model has high memory occupancy: neither of them\nis efficient enough in both time and memory usage. To tackle these problems, we\npropose FastLTS, a non-autoregressive end-to-end model which can directly\nsynthesize high-quality speech audios from unconstrained talking videos with\nlow latency, and has a relatively small model size. Besides, different from the\nwidely used 3D-CNN visual frontend for lip movement encoding, we for the first\ntime propose a transformer-based visual frontend for this task. Experiments\nshow that our model achieves $19.76\\times$ speedup for audio waveform\ngeneration compared with the current autoregressive model on input sequences of\n3 seconds, and obtains superior audio quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficiency Study for SPLADE Models. (arXiv:2207.03834v1 [cs.IR])","link":"http://arxiv.org/abs/2207.03834","description":"<p>Latency and efficiency issues are often overlooked when evaluating IR models\nbased on Pretrained Language Models (PLMs) in reason of multiple hardware and\nsoftware testing scenarios. Nevertheless, efficiency is an important part of\nsuch systems and should not be overlooked.\n</p>\n<p>In this paper, we focus on improving the efficiency of the SPLADE model since\nit has achieved state-of-the-art zero-shot performance and competitive results\non TREC collections. SPLADE efficiency can be controlled via a regularization\nfactor, but solely controlling this regularization has been shown to not be\nefficient enough. In order to reduce the latency gap between SPLADE and\ntraditional retrieval systems, we propose several techniques including L1\nregularization for queries, a separation of document/query encoders, a\nFLOPS-regularized middle-training, and the use of faster query encoders. Our\nbenchmark demonstrates that we can drastically improve the efficiency of these\nmodels while increasing the performance metrics on in-domain data. To our\nknowledge, {we propose the first neural models that, under the same computing\nconstraints, \\textit{achieve similar latency (less than 4ms difference) as\ntraditional BM25}, while having \\textit{similar performance (less than 10\\%\nMRR@10 reduction)} as the state-of-the-art single-stage neural rankers on\nin-domain data}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSTEA: Dialogue State Tracking with Entity Adaptive Pre-training. (arXiv:2207.03858v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03858","description":"<p>Dialogue state tracking (DST) is a core sub-module of a dialogue system,\nwhich aims to extract the appropriate belief state (domain-slot-value) from a\nsystem and user utterances. Most previous studies have attempted to improve\nperformance by increasing the size of the pre-trained model or using additional\nfeatures such as graph relations. In this study, we propose dialogue state\ntracking with entity adaptive pre-training (DSTEA), a system in which key\nentities in a sentence are more intensively trained by the encoder of the DST\nmodel. DSTEA extracts important entities from input dialogues in four ways, and\nthen applies selective knowledge masking to train the model effectively.\nAlthough DSTEA conducts only pre-training without directly infusing additional\nknowledge to the DST model, it achieved better performance than the best-known\nbenchmark models on MultiWOZ 2.0, 2.1, and 2.2. The effectiveness of DSTEA was\nverified through various comparative experiments with regard to the entity type\nand different adaptive settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yukyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Takyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hoonsang Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1\">Junseong Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Misuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Medical Information Extraction Workbench to Process German Clinical Text. (arXiv:2207.03885v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03885","description":"<p>Background: In the information extraction and natural language processing\ndomain, accessible datasets are crucial to reproduce and compare results.\nPublicly available implementations and tools can serve as benchmark and\nfacilitate the development of more complex applications. However, in the\ncontext of clinical text processing the number of accessible datasets is scarce\n-- and so is the number of existing tools. One of the main reasons is the\nsensitivity of the data. This problem is even more evident for non-English\nlanguages.\n</p>\n<p>Approach: In order to address this situation, we introduce a workbench: a\ncollection of German clinical text processing models. The models are trained on\na de-identified corpus of German nephrology reports.\n</p>\n<p>Result: The presented models provide promising results on in-domain data.\nMoreover, we show that our models can be also successfully applied to other\nbiomedical text in German. Our workbench is made publicly available so it can\nbe used out of the box, as a benchmark or transferred to related problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roller_R/0/1/0/all/0/1\">Roland Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seiffe_L/0/1/0/all/0/1\">Laura Seiffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayach_A/0/1/0/all/0/1\">Ammer Ayach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marten_O/0/1/0/all/0/1\">Oliver Marten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_M/0/1/0/all/0/1\">Michael Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alt_C/0/1/0/all/0/1\">Christoph Alt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Danilo Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halleck_F/0/1/0/all/0/1\">Fabian Halleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_M/0/1/0/all/0/1\">Marcel Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duettmann_W/0/1/0/all/0/1\">Wiebke Duettmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budde_K/0/1/0/all/0/1\">Klemens Budde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Approach to Ensure Fairness in News Articles. (arXiv:2207.03938v1 [cs.IR])","link":"http://arxiv.org/abs/2207.03938","description":"<p>Recommender systems, information retrieval, and other information access\nsystems present unique challenges for examining and applying concepts of\nfairness and bias mitigation in unstructured text. This paper introduces Dbias,\nwhich is a Python package to ensure fairness in news articles. Dbias is a\ntrained Machine Learning (ML) pipeline that can take a text (e.g., a paragraph\nor news story) and detects if the text is biased or not. Then, it detects the\nbiased words in the text, masks them, and recommends a set of sentences with\nnew words that are bias-free or at least less biased. We incorporate the\nelements of data science best practices to ensure that this pipeline is\nreproducible and usable. We show in experiments that this pipeline can be\neffective for mitigating biases and outperforms the common neural network\narchitectures in ensuring fairness in the news articles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shaina Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reji_D/0/1/0/all/0/1\">Deepak John Reji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dora D. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bashir_S/0/1/0/all/0/1\">Syed Raza Bashir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_U/0/1/0/all/0/1\">Usman Naseem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination. (arXiv:2207.03961v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03961","description":"<p>As humans, we can modify our assumptions about a scene by imagining\nalternative objects or concepts in our minds. For example, we can easily\nanticipate the implications of the sun being overcast by rain clouds (e.g., the\nstreet will get wet) and accordingly prepare for that. In this paper, we\nintroduce a new task/dataset called Commonsense Reasoning for Counterfactual\nScene Imagination (CoSIm) which is designed to evaluate the ability of AI\nsystems to reason about scene change imagination. In this task/dataset, models\nare given an image and an initial question-response pair about the image. Next,\na counterfactual imagined scene change (in textual form) is applied, and the\nmodel has to predict the new response to the initial question based on this\nscene change. We collect 3.5K high-quality and challenging data instances, with\neach instance consisting of an image, a commonsense question with a response, a\ndescription of a counterfactual change, a new response to the question, and\nthree distractor responses. Our dataset contains various complex scene change\ntypes (such as object addition/removal/state change, event description,\nenvironment change, etc.) that require models to imagine many different\nscenarios and reason about the changed scenes. We present a baseline model\nbased on a vision-language Transformer (i.e., LXMERT) and ablation studies.\nThrough human evaluation, we demonstrate a large human-model performance gap,\nsuggesting room for promising future work on this challenging counterfactual,\nscene imagination task. Our code and dataset are publicly available at:\nhttps://github.com/hyounghk/CoSIm\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyounghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1\">Abhay Zala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Time Like the Present: Effects of Language Change on Automated Comment Moderation. (arXiv:2207.04003v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04003","description":"<p>The spread of online hate has become a significant problem for newspapers\nthat host comment sections. As a result, there is growing interest in using\nmachine learning and natural language processing for (semi-) automated abusive\nlanguage detection to avoid manual comment moderation costs or having to shut\ndown comment sections altogether. However, much of the past work on abusive\nlanguage detection assumes that classifiers operate in a static language\nenvironment, despite language and news being in a state of constant flux. In\nthis paper, we show using a new German newspaper comments dataset that the\nclassifiers trained with naive ML techniques like a random-test train split\nwill underperform on future data, and that a time stratified evaluation split\nis more appropriate. We also show that classifier performance rapidly degrades\nwhen evaluated on data from a different period than the training data. Our\nfindings suggest that it is necessary to consider the temporal dynamics of\nlanguage when developing an abusive language detection system or risk deploying\na model that will quickly become defunct.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Justen_L/0/1/0/all/0/1\">Lennart Justen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Kilian M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niemann_M/0/1/0/all/0/1\">Marco Niemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becker_J/0/1/0/all/0/1\">J&#xf6;rg Becker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABB-BERT: A BERT model for disambiguating abbreviations and contractions. (arXiv:2207.04008v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04008","description":"<p>Abbreviations and contractions are commonly found in text across different\ndomains. For example, doctors' notes contain many contractions that can be\npersonalized based on their choices. Existing spelling correction models are\nnot suitable to handle expansions because of many reductions of characters in\nwords. In this work, we propose ABB-BERT, a BERT-based model, which deals with\nan ambiguous language containing abbreviations and contractions. ABB-BERT can\nrank them from thousands of options and is designed for scale. It is trained on\nWikipedia text, and the algorithm allows it to be fine-tuned with little\ncompute to get better performance for a domain or person. We are publicly\nreleasing the training dataset for abbreviations and contractions derived from\nWikipedia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kacker_P/0/1/0/all/0/1\">Prateek Kacker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cupallari_A/0/1/0/all/0/1\">Andi Cupallari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Aswin Gridhar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nimit Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASL-Homework-RGBD Dataset: An annotated dataset of 45 fluent and non-fluent signers performing American Sign Language homeworks. (arXiv:2207.04021v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04021","description":"<p>We are releasing a dataset containing videos of both fluent and non-fluent\nsigners using American Sign Language (ASL), which were collected using a Kinect\nv2 sensor. This dataset was collected as a part of a project to develop and\nevaluate computer vision algorithms to support new technologies for automatic\ndetection of ASL fluency attributes. A total of 45 fluent and non-fluent\nparticipants were asked to perform signing homework assignments that are\nsimilar to the assignments used in introductory or intermediate level ASL\ncourses. The data is annotated to identify several aspects of signing including\ngrammatical features and non-manual markers. Sign language recognition is\ncurrently very data-driven and this dataset can support the design of\nrecognition technologies, especially technologies that can benefit ASL\nlearners. This dataset might also be interesting to ASL education researchers\nwho want to contrast fluent and non-fluent signing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Saad Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seita_M/0/1/0/all/0/1\">Matthew Seita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berke_L/0/1/0/all/0/1\">Larwan Berke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gale_E/0/1/0/all/0/1\">Elaine Gale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sooyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huenerfauth_M/0/1/0/all/0/1\">Matt Huenerfauth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications. (arXiv:2207.04043v1 [cs.CL])","link":"http://arxiv.org/abs/2207.04043","description":"<p>Innovation is a major driver of economic and social development, and\ninformation about many kinds of innovation is embedded in semi-structured data\nfrom patents and patent applications. Although the impact and novelty of\ninnovations expressed in patent data are difficult to measure through\ntraditional means, ML offers a promising set of techniques for evaluating\nnovelty, summarizing contributions, and embedding semantics. In this paper, we\nintroduce the Harvard USPTO Patent Dataset (HUPD), a large-scale,\nwell-structured, and multi-purpose corpus of English-language patent\napplications filed to the United States Patent and Trademark Office (USPTO)\nbetween 2004 and 2018. With more than 4.5 million patent documents, HUPD is two\nto three times larger than comparable corpora. Unlike previously proposed\npatent datasets in NLP, HUPD contains the inventor-submitted versions of patent\napplications--not the final versions of granted patents--thereby allowing us to\nstudy patentability at the time of filing using NLP methods for the first time.\nIt is also novel in its inclusion of rich structured metadata alongside the\ntext of patent filings: By providing each application's metadata along with all\nof its text fields, the dataset enables researchers to perform new sets of NLP\ntasks that leverage variation in structured covariates. As a case study on the\ntypes of research HUPD makes possible, we introduce a new task to the NLP\ncommunity--namely, binary classification of patent decisions. We additionally\nshow the structured metadata provided in the dataset enables us to conduct\nexplicit studies of concept shifts for this task. Finally, we demonstrate how\nHUPD can be used for three additional tasks: multi-class classification of\npatent subject areas, language modeling, and summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzgun_M/0/1/0/all/0/1\">Mirac Suzgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melas_Kyriazi_L/0/1/0/all/0/1\">Luke Melas-Kyriazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Suproteem K. Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kominers_S/0/1/0/all/0/1\">Scott Duke Kominers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shieber_S/0/1/0/all/0/1\">Stuart M. Shieber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sampling of Dependency Structures. (arXiv:2109.06521v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06521","description":"<p>Probabilistic distributions over spanning trees in directed graphs are a\nfundamental model of dependency structure in natural language processing,\nsyntactic dependency trees. In NLP, dependency trees often have an additional\nroot constraint: only one edge may emanate from the root. However, no sampling\nalgorithm has been presented in the literature to account for this additional\nconstraint. In this paper, we adapt two spanning tree sampling algorithms to\nfaithfully sample dependency trees from a graph subject to the root constraint.\nWilson (1996)'s sampling algorithm has a running time of $\\mathcal{O}(H)$ where\n$H$ is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm\nhas a running time of $\\mathcal{O}(N^3)$, which is often greater than the mean\nhitting time of a directed graph. Additionally, we build upon Colbourn's\nalgorithm and present a novel extension that can sample $K$ trees without\nreplacement in $\\mathcal{O}(K N^3 + K^2 N)$ time. To the best of our knowledge,\nno algorithm has been given for sampling spanning trees without replacement\nfrom a directed graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1\">Ran Zmigrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There is no rose without a thorn: Finding weaknesses on BlenderBot 2.0 in terms of Model, Data and User-Centric Approach. (arXiv:2201.03239v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03239","description":"<p>BlenderBot 2.0 is a dialogue model that represents open-domain chatbots by\nreflecting real-time information and remembering user information for an\nextended period using an internet search module and multi-session. Nonetheless,\nthe model still has room for improvement. To this end, we examine BlenderBot\n2.0 limitations and errors from three perspectives: model, data, and user. From\nthe data point of view, we highlight the unclear guidelines provided to workers\nduring the crowdsourcing process, as well as a lack of a process for refining\nhate speech in the collected data and verifying the accuracy of internet-based\ninformation. From a user perspective, we identify nine types of limitations of\nBlenderBot 2.0, and their causes are thoroughly investigated. Furthermore, for\neach point of view, we propose practical improvement methods and discuss\nseveral potential future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungseob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_M/0/1/0/all/0/1\">Midan Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Suhyune Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yujin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logiformer: A Two-Branch Graph Transformer Network for Interpretable Logical Reasoning. (arXiv:2205.00731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00731","description":"<p>Machine reading comprehension has aroused wide concerns, since it explores\nthe potential of model for text understanding. To further equip the machine\nwith the reasoning capability, the challenging task of logical reasoning is\nproposed. Previous works on logical reasoning have proposed some strategies to\nextract the logical units from different aspects. However, there still remains\na challenge to model the long distance dependency among the logical units.\nAlso, it is demanding to uncover the logical structures of the text and further\nfuse the discrete logic to the continuous text embedding. To tackle the above\nissues, we propose an end-to-end model Logiformer which utilizes a two-branch\ngraph transformer network for logical reasoning of text. Firstly, we introduce\ndifferent extraction strategies to split the text into two sets of logical\nunits, and construct the logical graph and the syntax graph respectively. The\nlogical graph models the causal relations for the logical branch while the\nsyntax graph captures the co-occurrence relations for the syntax branch.\nSecondly, to model the long distance dependency, the node sequence from each\ngraph is fed into the fully connected graph transformer structures. The two\nadjacent matrices are viewed as the attention biases for the graph transformer\nlayers, which map the discrete logical structures to the continuous text\nembedding space. Thirdly, a dynamic gate mechanism and a question-aware\nself-attention module are introduced before the answer prediction to update the\nfeatures. The reasoning process provides the interpretability by employing the\nlogical units, which are consistent with human cognition. The experimental\nresults show the superiority of our model, which outperforms the\nstate-of-the-art single model on two logical reasoning benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangzhi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qika Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yudai Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingling Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Structured Span Selector. (arXiv:2205.03977v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03977","description":"<p>Many natural language processing tasks, e.g., coreference resolution and\nsemantic role labeling, require selecting text spans and making decisions about\nthem.\n</p>\n<p>A typical approach to such tasks is to score all possible spans and greedily\nselect spans for task-specific downstream processing.\n</p>\n<p>This approach, however, does not incorporate any inductive bias about what\nsort of spans ought to be selected, e.g., that selected spans tend to be\nsyntactic constituents.\n</p>\n<p>In this paper, we propose a novel grammar-based structured span selection\nmodel which learns to make use of the partial span-level annotation provided\nfor such problems.\n</p>\n<p>Compared to previous approaches, our approach gets rid of the heuristic\ngreedy span selection scheme, allowing us to model the downstream task on an\noptimal set of spans. We evaluate our model on two popular span prediction\ntasks: coreference resolution and semantic role labeling.\n</p>\n<p>We show empirical improvements on both.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuchen Eleanor Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models. (arXiv:2206.11484v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.11484","description":"<p>This paper presents exploratory work on whether and to what extent biases\nagainst queer and trans people are encoded in large language models (LLMs) such\nas BERT. We also propose a method for reducing these biases in downstream\ntasks: finetuning the models on data written by and/or about queer people. To\nmeasure anti-queer bias, we introduce a new benchmark dataset, WinoQueer,\nmodeled after other bias-detection benchmarks but addressing homophobic and\ntransphobic biases. We found that BERT shows significant homophobic bias, but\nthis bias can be mostly mitigated by finetuning BERT on a natural language\ncorpus written by members of the LGBTQ+ community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Felkner_V/0/1/0/all/0/1\">Virginia K. Felkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Ho-Chun Herbert Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">Eugene Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Convolution Neural Network based Mode Decomposition for Degenerated Modes via Multiple Images from Polarizers. (arXiv:2207.03489v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03489","description":"<p>In this paper, a mode decomposition (MD) method for degenerated modes has\nbeen studied. Convolution neural network (CNN) has been applied for image\ntraining and predicting the mode coefficients. Four-fold degenerated $LP_{11}$\nseries has been the target to be decomposed. Multiple images are regarded as an\ninput to decompose the degenerate modes. Total of seven different images,\nincluding the full original near-field image, and images after linear\npolarizers of four directions (0$^\\circ$, 45$^\\circ$, 90$^\\circ$, and\n135$^\\circ$), and images after two circular polarizers (right-handed and\nleft-handed) has been considered for training, validation, and test. The output\nlabel of the model has been chosen as the real and imaginary components of the\nmode coefficient, and the loss function has been selected to be the\nroot-mean-square (RMS) of the labels. The RMS and mean-absolute-error (MAE) of\nthe label, intensity, phase, and field correlation between the actual and\npredicted values have been selected to be the metrics to evaluate the CNN\nmodel. The CNN model has been trained with 100,000 three-dimensional images\nwith depths of three, four, and seven. The performance of the trained model was\nevaluated via 10,000 test samples with four sets of images - images after three\nlinear polarizers (0$^\\circ$, 45$^\\circ$, 90$^\\circ$) and image after\nright-handed circular polarizer - showed 0.0634 of label RMS, 0.0292 of\nintensity RMS, 0.1867 rad of phase MAE, and 0.9978 of average field\ncorrelation. The performance of 4 image sets showed at least 50.68\\% of\nperformance enhancement compared to models considering only images after linear\npolarizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyuntai Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"False Negative Reduction in Semantic Segmentation under Domain Shift using Depth Estimation. (arXiv:2207.03513v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03513","description":"<p>State-of-the-art deep neural networks demonstrate outstanding performance in\nsemantic segmentation. However, their performance is tied to the domain\nrepresented by the training data. Open world scenarios cause inaccurate\npredictions which is hazardous in safety relevant applications like automated\ndriving. In this work, we enhance semantic segmentation predictions using\nmonocular depth estimation to improve segmentation by reducing the occurrence\nof non-detected objects in presence of domain shift. To this end, we infer a\ndepth heatmap via a modified segmentation network which generates\nforeground-background masks, operating in parallel to a given semantic\nsegmentation network. Both segmentation masks are aggregated with a focus on\nforeground classes (here road users) to reduce false negatives. To also reduce\nthe occurrence of false positives, we apply a pruning based on uncertainty\nestimates. Our approach is modular in a sense that it post-processes the output\nof any semantic segmentation network. In our experiments, we observe less\nnon-detected objects of most important classes and an enhanced generalization\nto other domains compared to the basic semantic segmentation prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maag_K/0/1/0/all/0/1\">Kira Maag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should All Proposals be Treated Equally in Object Detection?. (arXiv:2207.03520v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03520","description":"<p>The complexity-precision trade-off of an object detector is a critical\nproblem for resource constrained vision tasks. Previous works have emphasized\ndetectors implemented with efficient backbones. The impact on this trade-off of\nproposal processing by the detection head is investigated in this work. It is\nhypothesized that improved detection efficiency requires a paradigm shift,\ntowards the unequal processing of proposals, assigning more computation to good\nproposals than poor ones. This results in better utilization of available\ncomputational budget, enabling higher accuracy for the same FLOPS. We formulate\nthis as a learning problem where the goal is to assign operators to proposals,\nin the detection head, so that the total computational cost is constrained and\nthe precision is maximized. The key finding is that such matching can be\nlearned as a function that maps each proposal embedding into a one-hot code\nover operators. While this function induces a complex dynamic network routing\nmechanism, it can be implemented by a simple MLP and learned end-to-end with\noff-the-shelf object detectors. This 'dynamic proposal processing' (DPP) is\nshown to outperform state-of-the-art end-to-end object detectors (DETR, Sparse\nR-CNN) by a clear margin for a given computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1\">Nuno Vasconcelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RWT-SLAM: Robust Visual SLAM for Highly Weak-textured Environments. (arXiv:2207.03539v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03539","description":"<p>As a fundamental task for intelligent robots, visual SLAM has made great\nprogress over the past decades. However, robust SLAM under highly weak-textured\nenvironments still remains very challenging. In this paper, we propose a novel\nvisual SLAM system named RWT-SLAM to tackle this problem. We modify LoFTR\nnetwork which is able to produce dense point matching under low-textured scenes\nto generate feature descriptors. To integrate the new features into the popular\nORB-SLAM framework, we develop feature masks to filter out the unreliable\nfeatures and employ KNN strategy to strengthen the matching robustness. We also\nretrained visual vocabulary upon new descriptors for efficient loop closing.\nThe resulting RWT-SLAM is tested in various public datasets such as TUM and\nOpenLORIS, as well as our own data. The results shows very promising\nperformance under highly weak-textured environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qihao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Zhiyu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">YuanGang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xijun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highlight Specular Reflection Separation based on Tensor Low-rank and Sparse Decomposition Using Polarimetric Cues. (arXiv:2207.03543v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03543","description":"<p>This paper is concerned with specular reflection removal based on tensor\nlow-rank decomposition framework with the help of polarization information. Our\nmethod is motivated by the observation that the specular highlight of an image\nis sparsely distributed while the remaining diffuse reflection can be well\napproximated by a linear combination of several distinct colors using a\nlow-rank and sparse decomposition framework. Unlike current solutions, our\ntensor low-rank decomposition keeps the spatial structure of specular and\ndiffuse information which enables us to recover the diffuse image under strong\nspecular reflection or in saturated regions. We further define and impose a new\npolarization regularization term as constraint on color channels. This\nregularization boosts the performance of the method to recover an accurate\ndiffuse image by handling the color distortion, a common problem of\nchromaticity-based methods, especially in case of strong specular reflection.\nThrough comprehensive experiments on both synthetic and real polarization\nimages, we demonstrate that our method is able to significantly improve the\naccuracy of highlight specular removal, and outperform the competitive methods\nto recover the diffuse image, especially in regions of strong specular\nreflection or in saturated areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_M/0/1/0/all/0/1\">Moein Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Embedding-Dynamic Approach to Self-supervised Learning. (arXiv:2207.03552v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03552","description":"<p>A number of recent self-supervised learning methods have shown impressive\nperformance on image classification and other tasks. A somewhat bewildering\nvariety of techniques have been used, not always with a clear understanding of\nthe reasons for their benefits, especially when used in combination. Here we\ntreat the embeddings of images as point particles and consider model\noptimization as a dynamic process on this system of particles. Our dynamic\nmodel combines an attractive force for similar images, a locally dispersive\nforce to avoid local collapse, and a global dispersive force to achieve a\nglobally-homogeneous distribution of particles. The dynamic perspective\nhighlights the advantage of using a delayed-parameter image embedding (a la\nBYOL) together with multiple views of the same image. It also uses a\npurely-dynamic local dispersive force (Brownian motion) that shows improved\nperformance over other methods and does not require knowledge of other particle\ncoordinates. The method is called MSBReg which stands for (i) a Multiview\ncentroid loss, which applies an attractive force to pull different image view\nembeddings toward their centroid, (ii) a Singular value loss, which pushes the\nparticle system toward spatially homogeneous density, (iii) a Brownian\ndiffusive loss. We evaluate downstream classification performance of MSBReg on\nImageNet as well as transfer learning tasks including fine-grained\nclassification, multi-class object classification, object detection, and\ninstance segmentation. In addition, we also show that applying our\nregularization term to other methods further improves their performance and\nstabilize the training by preventing a mode collapse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Suhong Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buracas_D/0/1/0/all/0/1\">Domas Buracas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canny_J/0/1/0/all/0/1\">John Canny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mirror Complementary Transformer Network for RGB-thermal Salient Object Detection. (arXiv:2207.03558v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03558","description":"<p>RGB-thermal salient object detection (RGB-T SOD) aims to locate the common\nprominent objects of an aligned visible and thermal infrared image pair and\naccurately segment all the pixels belonging to those objects. It is promising\nin challenging scenes such as nighttime and complex backgrounds due to the\ninsensitivity to lighting conditions of thermal images. Thus, the key problem\nof RGB-T SOD is to make the features from the two modalities complement and\nadjust each other flexibly, since it is inevitable that any modalities of RGB-T\nimage pairs failure due to challenging scenes such as extreme light conditions\nand thermal crossover. In this paper, we propose a novel mirror complementary\nTransformer network (MCNet) for RGB-T SOD. Specifically, we introduce a\nTransformer-based feature extraction module to effective extract hierarchical\nfeatures of RGB and thermal images. Then, through the attention-based feature\ninteraction and serial multiscale dilated convolution (SDC) based feature\nfusion modules, the proposed model achieves the complementary interaction of\nlow-level features and the semantic fusion of deep features. Finally, based on\nthe mirror complementary structure, the salient regions of the two modalities\ncan be accurately extracted even one modality is invalid. To demonstrate the\nrobustness of the proposed model under challenging scenes in real world, we\nbuild a novel RGB-T SOD dataset VT723 based on a large public semantic\nsegmentation RGB-T dataset used in the autonomous driving domain. Expensive\nexperiments on benchmark and VT723 datasets show that the proposed method\noutperforms state-of-the-art approaches, including CNN-based and\nTransformer-based methods. The code and dataset will be released later at\nhttps://github.com/jxr326/SwinMCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiurong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yifan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hui Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The use of deep learning enables high diagnostic accuracy in detecting syndesmotic instability on weight-bearing CT scanning. (arXiv:2207.03568v1 [eess.IV])","link":"http://arxiv.org/abs/2207.03568","description":"<p>Delayed diagnosis of syndesmosis instability can lead to significant\nmorbidity and accelerated arthritic change in the ankle joint. Weight-bearing\ncomputed tomography (WBCT) has shown promising potential for early and reliable\ndetection of isolated syndesmotic instability using 3D volumetric measurements.\nWhile these measurements have been reported to be highly accurate, they are\nalso experience-dependent, time-consuming, and need a particular 3D measurement\nsoftware tool that leads the clinicians to still show more interest in the\nconventional diagnostic methods for syndesmotic instability. The purpose of\nthis study was to increase accuracy, accelerate analysis time, and reduce\ninter-observer bias by automating 3D volume assessment of syndesmosis anatomy\nusing WBCT scans. We conducted a retrospective study using previously collected\nWBCT scans of patients with unilateral syndesmotic instability. 144 bilateral\nankle WBCT scans were evaluated (48 unstable, 96 control). We developed three\ndeep learning (DL) models for analyzing WBCT scans to recognize syndesmosis\ninstability. These three models included two state-of-the-art models (Model 1 -\n3D convolutional neural network [CNN], and Model 2 - CNN with long short-term\nmemory [LSTM]), and a new model (Model 3 - differential CNN LSTM) that we\nintroduced in this study. Model 1 failed to analyze the WBCT scans (F1-score =\n0). Model 2 only misclassified two cases (F1-score = 0.80). Model 3\noutperformed Model 2 and achieved a nearly perfect performance, misclassifying\nonly one case (F1-score = 0.91) in the control group as unstable while being\nfaster than Model 2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Borjali_A/0/1/0/all/0/1\">Alireza Borjali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ashkani_Esfahani_S/0/1/0/all/0/1\">Soheil Ashkani-Esfahani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhimani_R/0/1/0/all/0/1\">Rohan Bhimani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guss_D/0/1/0/all/0/1\">Daniel Guss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muratoglu_O/0/1/0/all/0/1\">Orhun K. Muratoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+DiGiovanni_C/0/1/0/all/0/1\">Christopher W. DiGiovanni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varadarajan_K/0/1/0/all/0/1\">Kartik Mangudi Varadarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lubberts_B/0/1/0/all/0/1\">Bart Lubberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v1 [cs.CR])","link":"http://arxiv.org/abs/2207.03574","description":"<p>Neural networks' lack of robustness against attacks raises concerns in\nsecurity-sensitive settings such as autonomous vehicles. While many\ncountermeasures may look promising, only a few withstand rigorous evaluation.\nDefenses using random transformations (RT) have shown impressive results,\nparticularly BaRT (Raff et al., 2019) on ImageNet. However, this type of\ndefense has not been rigorously evaluated, leaving its robustness properties\npoorly understood. Their stochastic properties make evaluation more challenging\nand render many proposed attacks on deterministic models inapplicable. First,\nwe show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation\nis ineffective and likely overestimates its robustness. We then attempt to\nconstruct the strongest possible RT defense through the informed selection of\ntransformations and Bayesian optimization for tuning their parameters.\nFurthermore, we create the strongest possible attack to evaluate our RT\ndefense. Our new attack vastly outperforms the baseline, reducing the accuracy\nby 83% compared to the 19% reduction by the commonly used EoT attack\n($4.3\\times$ improvement). Our result indicates that the RT defense on the\nImagenette dataset (a ten-class subset of ImageNet) is not robust against\nadversarial examples. Extending the study further, we use our new attack to\nadversarially train RT defense (called AdvRT), resulting in a large robustness\ngain. Code is available at\nhttps://github.com/wagnergroup/demystify-random-transform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1\">Chawin Sitawarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golan_Strieb_Z/0/1/0/all/0/1\">Zachary Golan-Strieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1\">David Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaitTAKE: Gait Recognition by Temporal Attention \\\\and Keypoint-guided Embedding. (arXiv:2207.03608v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03608","description":"<p>Gait recognition, which refers to the recognition or identification of a\nperson based on their body shape and walking styles, derived from video data\ncaptured from a distance, is widely used in crime prevention, forensic\nidentification, and social security. However, to the best of our knowledge,\nmost of the existing methods use appearance, posture and temporal feautures\nwithout considering a learned temporal attention mechanism for global and local\ninformation fusion. In this paper, we propose a novel gait recognition\nframework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),\nwhich effectively fuses temporal-attention-based global and local appearance\nfeature and temporal aggregated human pose feature. Experimental results show\nthat our proposed method achieves a new SOTA in gait recognition with rank-1\naccuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait\ndataset; 90.4% accuracy on the OU-MVLP gait dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thuc_H/0/1/0/all/0/1\">Hoang Le Uyen Thuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang-Ju Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoseGU: 3D Human Pose Estimation with Novel Human Pose Generator and Unbiased Learning. (arXiv:2207.03618v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03618","description":"<p>3D pose estimation has recently gained substantial interests in computer\nvision domain. Existing 3D pose estimation methods have a strong reliance on\nlarge size well-annotated 3D pose datasets, and they suffer poor model\ngeneralization on unseen poses due to limited diversity of 3D poses in training\nsets. In this work, we propose PoseGU, a novel human pose generator that\ngenerates diverse poses with access only to a small size of seed samples, while\nequipping the Counterfactual Risk Minimization to pursue an unbiased evaluation\nobjective. Extensive experiments demonstrate PoseGU outforms almost all the\nstate-of-the-art 3D human pose methods under consideration over three popular\nbenchmark datasets. Empirical analysis also proves PoseGU generates 3D poses\nwith improved data diversity and better generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shannan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1\">Gengfa Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity. (arXiv:2207.03620v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03620","description":"<p>Transformers have quickly shined in the computer vision world since the\nemergence of Vision Transformers (ViTs). The dominant role of convolutional\nneural networks (CNNs) seems to be challenged by increasingly effective\ntransformer-based models. Very recently, a couple of advanced convolutional\nmodels strike back with large kernels motivated by the local but large\nattention mechanism, showing appealing performance and efficiency. While one of\nthem, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31\nwith improved performance, the performance starts to saturate as the kernel\nsize continues growing, compared to the scaling trend of advanced ViTs such as\nSwin Transformer. In this paper, we explore the possibility of training extreme\nconvolutions larger than 31x31 and test whether the performance gap can be\neliminated by strategically enlarging convolutions. This study ends up with a\nrecipe for applying extremely large kernels from the perspective of sparsity,\nwhich can smoothly scale up kernels to 61x61 with better performance. Built on\nthis recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN\narchitecture equipped with 51x51 kernels that can perform on par with or better\nthan state-of-the-art hierarchical Transformers and modern ConvNet\narchitectures like ConvNeXt and RepLKNet, on ImageNet classification as well as\ntypical downstream tasks. Our code is available here\nhttps://github.com/VITA-Group/SLaK.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1\">Qiao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Boqian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Support Vector Model of Pruning Trees Evaluation Based on OTSU Algorithm. (arXiv:2207.03638v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03638","description":"<p>The tree pruning process is the key to promoting fruits' growth and improving\ntheir productions due to effects on the photosynthesis efficiency of fruits and\nnutrition transportation in branches. Currently, pruning is still highly\ndependent on human labor. The workers' experience will strongly affect the\nrobustness of the performance of the tree pruning. Thus, it is a challenge for\nworkers and farmers to evaluate the pruning performance. Intended for a better\nsolution to the problem, this paper presents a novel pruning classification\nstrategy model called \"OTSU-SVM\" to evaluate the pruning performance based on\nthe shadows of branches and leaves. This model considers not only the available\nilluminated area of the tree but also the uniformity of the illuminated area of\nthe tree. More importantly, our group implements OTSU algorithm into the model,\nwhich highly reinforces robustness of the evaluation of this model. In\naddition, the data from the pear trees in the Yuhang District, Hangzhou is also\nused in the experiment. In this experiment, we prove that the OTSU-SVM has good\naccuracy with 80% and high performance in the evaluation of the pruning for the\npear trees. It can provide more successful pruning if applied into the orchard.\nA successful pruning can broaden the illuminated area of individual fruit, and\nincrease nutrition transportation from the target branch, dramatically\nelevating the weights and production of the fruits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xinli Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_C/0/1/0/all/0/1\">Chunhua Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1\">Fuguang Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning Early Exit Networks. (arXiv:2207.03644v1 [cs.LG])","link":"http://arxiv.org/abs/2207.03644","description":"<p>Deep learning models that perform well often have high computational costs.\nIn this paper, we combine two approaches that try to reduce the computational\ncost while keeping the model performance high: pruning and early exit networks.\nWe evaluate two approaches of pruning early exit networks: (1) pruning the\nentire network at once, (2) pruning the base network and additional linear\nclassifiers in an ordered fashion. Experimental results show that pruning the\nentire network at once is a better strategy in general. However, at high\naccuracy rates, the two approaches have a similar performance, which implies\nthat the processes of pruning and early exit can be separated without loss of\noptimality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gormez_A/0/1/0/all/0/1\">Alperen G&#xf6;rmez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyuncu_E/0/1/0/all/0/1\">Erdem Koyuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abs-CAM: A Gradient Optimization Interpretable Approach for Explanation of Convolutional Neural Networks. (arXiv:2207.03648v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03648","description":"<p>The black-box nature of Deep Neural Networks (DNNs) severely hinders its\nperformance improvement and application in specific scenes. In recent years,\nclass activation mapping-based method has been widely used to interpret the\ninternal decisions of models in computer vision tasks. However, when this\nmethod uses backpropagation to obtain gradients, it will cause noise in the\nsaliency map, and even locate features that are irrelevant to decisions. In\nthis paper, we propose an Absolute value Class Activation Mapping-based\n(Abs-CAM) method, which optimizes the gradients derived from the\nbackpropagation and turns all of them into positive gradients to enhance the\nvisual features of output neurons' activation, and improve the localization\nability of the saliency map. The framework of Abs-CAM is divided into two\nphases: generating initial saliency map and generating final saliency map. The\nfirst phase improves the localization ability of the saliency map by optimizing\nthe gradient, and the second phase linearly combines the initial saliency map\nwith the original image to enhance the semantic information of the saliency\nmap. We conduct qualitative and quantitative evaluation of the proposed method,\nincluding Deletion, Insertion, and Pointing Game. The experimental results show\nthat the Abs-CAM can obviously eliminate the noise in the saliency map, and can\nbetter locate the features related to decisions, and is superior to the\nprevious methods in recognition and localization tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chunyan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Kang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shiyan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Nan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Dialog as Conversation about Objects Living in Space-Time. (arXiv:2207.03656v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03656","description":"<p>It would be a technological feat to be able to create a system that can hold\na meaningful conversation with humans about what they watch. A setup toward\nthat goal is presented as a video dialog task, where the system is asked to\ngenerate natural utterances in response to a question in an ongoing dialog. The\ntask poses great visual, linguistic, and reasoning challenges that cannot be\neasily overcome without an appropriate representation scheme over video and\ndialog that supports high-level reasoning. To tackle these challenges we\npresent a new object-centric framework for video dialog that supports neural\nreasoning dubbed COST - which stands for Conversation about Objects in\nSpace-Time. Here dynamic space-time visual content in videos is first parsed\ninto object trajectories. Given this video abstraction, COST maintains and\ntracks object-associated dialog states, which are updated upon receiving new\nquestions. Object interactions are dynamically and conditionally inferred for\neach question, and these serve as the basis for relational reasoning among\nthem. COST also maintains a history of previous answers, and this allows\nretrieval of relevant object-centric information to enrich the answer forming\nprocess. Language production then proceeds in a step-wise manner, taking into\nthe context of the current utterance, the existing dialog, the current\nquestion. We evaluate COST on the DSTC7 and DSTC8 benchmarks, demonstrating its\ncompetitiveness against state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hoang-Anh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thao Minh Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1\">Vuong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phuong_T/0/1/0/all/0/1\">Tu Minh Phuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Face Traceability with Disentangling Reversing Network. (arXiv:2207.03666v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03666","description":"<p>Deepfake face not only violates the privacy of personal identity, but also\nconfuses the public and causes huge social harm. The current deepfake detection\nonly stays at the level of distinguishing true and false, and cannot trace the\noriginal genuine face corresponding to the fake face, that is, it does not have\nthe ability to trace the source of evidence. The deepfake countermeasure\ntechnology for judicial forensics urgently calls for deepfake traceability.\nThis paper pioneers an interesting question about face deepfake, active\nforensics that \"know it and how it happened\". Given that deepfake faces do not\ncompletely discard the features of original faces, especially facial\nexpressions and poses, we argue that original faces can be approximately\nspeculated from their deepfake counterparts. Correspondingly, we design a\ndisentangling reversing network that decouples latent space features of\ndeepfake faces under the supervision of fake-original face pair samples to\ninfer original faces in reverse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ai_J/0/1/0/all/0/1\">Jiaxin Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baojin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning High-quality Proposals for Acne Detection. (arXiv:2207.03674v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03674","description":"<p>Acne detection is crucial for interpretative diagnosis and precise treatment\nof skin disease. The arbitrary boundary and small size of acne lesions lead to\na significant number of poor-quality proposals in two-stage detection. In this\npaper, we propose a novel head structure for Region Proposal Network to improve\nthe proposals' quality in two ways. At first, a Spatial Aware Double Head(SADH)\nstructure is proposed to disentangle the representation learning for\nclassification and localization from two different spatial perspectives. The\nproposed SADH ensures a steeper classification confidence gradient and\nsuppresses the proposals having low intersection-over-union(IoU) with the\nmatched ground truth. Then, we propose a Normalized Wasserstein Distance\nprediction branch to improve the correlation between the proposals'\nclassification scores and IoUs. In addition, to facilitate further research on\nacne detection, we construct a new dataset named AcneSCU, with high-resolution\nimageries, precise annotations, and fine-grained lesion categories. Extensive\nexperiments are conducted on both AcneSCU and the public dataset ACNE04, and\nthe results demonstrate the proposed method could improve the proposals'\nquality, consistently outperforming state-of-the-art approaches. Code and the\ncollected dataset are available in\nhttps://github.com/pingguokiller/acnedetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning. (arXiv:2207.03677v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03677","description":"<p>Neural architecture search (NAS) has demonstrated amazing success in\nsearching for efficient deep neural networks (DNNs) from a given supernet. In\nparallel, the lottery ticket hypothesis has shown that DNNs contain small\nsubnetworks that can be trained from scratch to achieve a comparable or higher\naccuracy than original DNNs. As such, it is currently a common practice to\ndevelop efficient DNNs via a pipeline of first search and then prune.\nNevertheless, doing so often requires a search-train-prune-retrain process and\nthus prohibitive computational cost. In this paper, we discover for the first\ntime that both efficient DNNs and their lottery subnetworks (i.e., lottery\ntickets) can be directly identified from a supernet, which we term as\nSuperTickets, via a two-in-one training scheme with jointly architecture\nsearching and parameter pruning. Moreover, we develop a progressive and unified\nSuperTickets identification strategy that allows the connectivity of\nsubnetworks to change during supernet training, achieving better accuracy and\nefficiency trade-offs than conventional sparse training. Finally, we evaluate\nwhether such identified SuperTickets drawn from one task can transfer well to\nother tasks, validating their potential of handling multiple tasks\nsimultaneously. Extensive experiments and ablation studies on three tasks and\nfour benchmark datasets validate that our proposed SuperTickets achieve boosted\naccuracy and efficiency trade-offs than both typical NAS and pruning pipelines,\nregardless of having retraining or not. Codes and pretrained models are\navailable at https://github.com/RICE-EIC/SuperTickets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoran You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhanyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xu Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Music-driven Dance Regeneration with Controllable Key Pose Constraints. (arXiv:2207.03682v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03682","description":"<p>In this paper, we propose a novel framework for music-driven dance motion\nsynthesis with controllable key pose constraint. In contrast to methods that\ngenerate dance motion sequences only based on music without any other\ncontrollable conditions, this work targets on synthesizing high-quality dance\nmotion driven by music as well as customized poses performed by users. Our\nmodel involves two single-modal transformer encoders for music and motion\nrepresentations and a cross-modal transformer decoder for dance motions\ngeneration. The cross-modal transformer decoder achieves the capability of\nsynthesizing smooth dance motion sequences, which keeps a consistency with key\nposes at corresponding positions, by introducing the local neighbor position\nembedding. Such mechanism makes the decoder more sensitive to key poses and the\ncorresponding positions. Our dance synthesis model achieves satisfactory\nperformance both on quantitative and qualitative evaluations with extensive\nexperiments, which demonstrates the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Junfu Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization. (arXiv:2207.03684v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03684","description":"<p>Existing unsupervised domain adaptation methods based on adversarial learning\nhave achieved good performance in several medical imaging tasks. However, these\nmethods focus only on global distribution adaptation and ignore distribution\nconstraints at the category level, which would lead to sub-optimal adaptation\nperformance. This paper presents an unsupervised domain adaptation framework\nbased on category-level regularization that regularizes the category\ndistribution from three perspectives. Specifically, for inter-domain category\nregularization, an adaptive prototype alignment module is proposed to align\nfeature prototypes of the same category in the source and target domains. In\naddition, for intra-domain category regularization, we tailored a\nregularization technique for the source and target domains, respectively. In\nthe source domain, a prototype-guided discriminative loss is proposed to learn\nmore discriminative feature representations by enforcing intra-class\ncompactness and inter-class separability, and as a complement to traditional\nsupervised loss. In the target domain, an augmented consistency category\nregularization loss is proposed to force the model to produce consistent\npredictions for augmented/unaugmented target images, which encourages\nsemantically similar regions to be given the same label. Extensive experiments\non two publicly fundus datasets show that the proposed approach significantly\noutperforms other state-of-the-art comparison algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1\">Lie Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Implicit Dictionary via Mixture-of-Expert Training. (arXiv:2207.03691v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03691","description":"<p>Representing visual signals by coordinate-based deep fully-connected networks\nhas been shown advantageous in fitting complex details and solving inverse\nproblems than discrete grid-based representation. However, acquiring such a\ncontinuous Implicit Neural Representation (INR) requires tedious per-scene\ntraining on tons of signal measurements, which limits its practicality. In this\npaper, we present a generic INR framework that achieves both data and training\nefficiency by learning a Neural Implicit Dictionary (NID) from a data\ncollection and representing INR as a functional combination of basis sampled\nfrom the dictionary. Our NID assembles a group of coordinate-based subnetworks\nwhich are tuned to span the desired function space. After training, one can\ninstantly and robustly acquire an unseen scene representation by solving the\ncoding coefficients. To parallelly optimize a large group of networks, we\nborrow the idea from Mixture-of-Expert (MoE) to design and train our network\nwith a sparse gating mechanism. Our experiments show that, NID can improve\nreconstruction of 2D images or 3D scenes by 2 orders of magnitude faster with\nup to 98% less input data. We further demonstrate various applications of NID\nin image inpainting and occlusion removal, which are considered to be\nchallenging with vanilla INR. Our codes are available in\nhttps://github.com/VITA-Group/Neural-Implicit-Dict.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhiwen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Discriminative Food Regions for Accurate Food Recognition. (arXiv:2207.03692v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03692","description":"<p>Automatic food recognition is the very first step towards passive dietary\nmonitoring. In this paper, we address the problem of food recognition by mining\ndiscriminative food regions. Taking inspiration from Adversarial Erasing, a\nstrategy that progressively discovers discriminative object regions for weakly\nsupervised semantic segmentation, we propose a novel network architecture in\nwhich a primary network maintains the base accuracy of classifying an input\nimage, an auxiliary network adversarially mines discriminative food regions,\nand a region network classifies the resulting mined regions. The global (the\noriginal input image) and the local (the mined regions) representations are\nthen integrated for the final prediction. The proposed architecture denoted as\nPAR-Net is end-to-end trainable, and highlights discriminative regions in an\nonline fashion. In addition, we introduce a new fine-grained food dataset named\nas Sushi-50, which consists of 50 different sushi categories. Extensive\nexperiments have been conducted to evaluate the proposed approach. On three\nfood datasets chosen (Food-101, Vireo-172, and Sushi-50), our approach performs\nconsistently and achieves state-of-the-art results (top-1 testing accuracy of\n$90.4\\%$, $90.2\\%$, $92.0\\%$, respectively) compared with other existing\napproaches. Dataset and code are available at\nhttps://github.com/Jianing-Qiu/PARNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingnan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SST-Calib: Simultaneous Spatial-Temporal Parameter Calibration between LIDAR and Camera. (arXiv:2207.03704v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03704","description":"<p>With information from multiple input modalities, sensor fusion-based\nalgorithms usually out-perform their single-modality counterparts in robotics.\nCamera and LIDAR, with complementary semantic and depth information, are the\ntypical choices for detection tasks in complicated driving environments. For\nmost camera-LIDAR fusion algorithms, however, the calibration of the sensor\nsuite will greatly impact the performance. More specifically, the detection\nalgorithm usually requires an accurate geometric relationship among multiple\nsensors as the input, and it is often assumed that the contents from these\nsensors are captured at the same time. Preparing such sensor suites involves\ncarefully designed calibration rigs and accurate synchronization mechanisms,\nand the preparation process is usually done offline. In this work, a\nsegmentation-based framework is proposed to jointly estimate the geometrical\nand temporal parameters in the calibration of a camera-LIDAR suite. A semantic\nsegmentation mask is first applied to both sensor modalities, and the\ncalibration parameters are optimized through pixel-wise bidirectional loss. We\nspecifically incorporated the velocity information from optical flow for\ntemporal parameters. Since supervision is only performed at the segmentation\nlevel, no calibration label is needed within the framework. The proposed\nalgorithm is tested on the KITTI dataset, and the result shows an accurate\nreal-time calibration of both geometric and temporal parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodaira_A/0/1/0/all/0/1\">Akio Kodaira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_P/0/1/0/all/0/1\">Pengwei Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based Smoky Vehicle Detection with A Coarse-to-Fine Framework. (arXiv:2207.03708v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03708","description":"<p>Automatic smoky vehicle detection in videos is a superior solution to the\ntraditional expensive remote sensing one with ultraviolet-infrared light\ndevices for environmental protection agencies. However, it is challenging to\ndistinguish vehicle smoke from shadow and wet regions coming from rear vehicle\nor clutter roads, and could be worse due to limited annotated data. In this\npaper, we first introduce a real-world large-scale smoky vehicle dataset with\n75,000 annotated smoky vehicle images, facilitating the effective training of\nadvanced deep learning models. To enable fair algorithm comparison, we also\nbuild a smoky vehicle video dataset including 163 long videos with\nsegment-level annotations. Moreover, we present a new Coarse-to-fine Deep Smoky\nvehicle detection (CoDeS) framework for efficient smoky vehicle detection. The\nCoDeS first leverages a light-weight YOLO detector for fast smoke detection\nwith high recall rate, and then applies a smoke-vehicle matching strategy to\neliminate non-vehicle smoke, and finally uses a elaborately-designed 3D model\nto further refine the results in spatial temporal space. Extensive experiments\nin four metrics demonstrate that our framework is significantly superior to\nthose hand-crafted feature based methods and recent advanced methods. The code\nand dataset will be released at https://github.com/pengxj/smokyvehicle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiaomao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pan Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation. (arXiv:2207.03714v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03714","description":"<p>Sign language is the window for people differently-abled to express their\nfeelings as well as emotions. However, it remains challenging for people to\nlearn sign language in a short time. To address this real-world challenge, in\nthis work, we study the motion transfer system, which can transfer the user\nphoto to the sign language video of specific words. In particular, the\nappearance content of the output video comes from the provided user image,\nwhile the motion of the video is extracted from the specified tutorial video.\nWe observe two primary limitations in adopting the state-of-the-art motion\ntransfer methods to sign language generation:(1) Existing motion transfer works\nignore the prior geometrical knowledge of the human body. (2) The previous\nimage animation methods only take image pairs as input in the training stage,\nwhich could not fully exploit the temporal information within videos. In an\nattempt to address the above-mentioned limitations, we propose Structure-aware\nTemporal Consistency Network (STCNet) to jointly optimize the prior structure\nof human with the temporal consistency for sign language video generation.\nThere are two main contributions in this paper. (1) We harness a fine-grained\nskeleton detector to provide prior knowledge of the body keypoints. In this\nway, we ensure the keypoint movement in a valid range and make the model become\nmore explainable and robust. (2) We introduce two cycle-consistency losses,\ni.e., short-term cycle loss and long-term cycle loss, which are conducted to\nassure the continuity of the generated video. We optimize the two losses and\nkeypoint detector network in an end-to-end manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suo_Y/0/1/0/all/0/1\">Yucheng Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bounding Box Disparity: 3D Metrics for Object Detection With Full Degree of Freedom. (arXiv:2207.03720v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03720","description":"<p>The most popular evaluation metric for object detection in 2D images is\nIntersection over Union (IoU). Existing implementations of the IoU metric for\n3D object detection usually neglect one or more degrees of freedom. In this\npaper, we first derive the analytic solution for three dimensional bounding\nboxes. As a second contribution, a closed-form solution of the volume-to-volume\ndistance is derived. Finally, the Bounding Box Disparity is proposed as a\ncombined positive continuous metric. We provide open source implementations of\nthe three metrics as standalone python functions, as well as extensions to the\nOpen3D library and as ROS nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adam_M/0/1/0/all/0/1\">Michael G. Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccolrovazzi_M/0/1/0/all/0/1\">Martin Piccolrovazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Sebastian Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinbach_E/0/1/0/all/0/1\">Eckehard Steinbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Effectiveness of Video Perceptual Representation in Blind Video Quality Assessment. (arXiv:2207.03723v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03723","description":"<p>With the rapid growth of in-the-wild videos taken by non-specialists, blind\nvideo quality assessment (VQA) has become a challenging and demanding problem.\nAlthough lots of efforts have been made to solve this problem, it remains\nunclear how the human visual system (HVS) relates to the temporal quality of\nvideos. Meanwhile, recent work has found that the frames of natural video\ntransformed into the perceptual domain of the HVS tend to form a straight\ntrajectory of the representations. With the obtained insight that distortion\nimpairs the perceived video quality and results in a curved trajectory of the\nperceptual representation, we propose a temporal perceptual quality index\n(TPQI) to measure the temporal distortion by describing the graphic morphology\nof the representation. Specifically, we first extract the video perceptual\nrepresentations from the lateral geniculate nucleus (LGN) and primary visual\narea (V1) of the HVS, and then measure the straightness and compactness of\ntheir trajectories to quantify the degradation in naturalness and content\ncontinuity of video. Experiments show that the perceptual representation in the\nHVS is an effective way of predicting subjective temporal quality, and thus\nTPQI can, for the first time, achieve comparable performance to the spatial\nquality metric and be even more effective in assessing videos with large\ntemporal variations. We further demonstrate that by combining with NIQE, a\nspatial quality metric, TPQI can achieve top performance over popular\nin-the-wild video datasets. More importantly, TPQI does not require any\nadditional information beyond the video being evaluated and thus can be applied\nto any datasets without parameter tuning. Source code is available at\nhttps://github.com/UoLMM/TPQI-VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Liang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kangmin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TGRMPT: A Head-Shoulder Aided Multi-Person Tracker and a New Large-Scale Dataset for Tour-Guide Robot. (arXiv:2207.03726v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03726","description":"<p>A service robot serving safely and politely needs to track the surrounding\npeople robustly, especially for Tour-Guide Robot (TGR). However, existing\nmulti-object tracking (MOT) or multi-person tracking (MPT) methods are not\napplicable to TGR for the following reasons: 1. lacking relevant large-scale\ndatasets; 2. lacking applicable metrics to evaluate trackers. In this work, we\ntarget the visual perceptual tasks for TGR and present the TGRDB dataset, a\nnovel large-scale multi-person tracking dataset containing roughly 5.6 hours of\nannotated videos and over 450 long-term trajectories. Besides, we propose a\nmore applicable metric to evaluate trackers using our dataset. As part of our\nwork, we present TGRMPT, a novel MPT system that incorporates information from\nhead shoulder and whole body, and achieves state-of-the-art performance. We\nhave released our codes and dataset in https://github.com/wenwenzju/TGRMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shunda Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shiqiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheyuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1\">Tianlei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Z/0/1/0/all/0/1\">Zonghao Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanhai Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEMS: Scene Expansion using Generative Models of Graphs. (arXiv:2207.03729v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03729","description":"<p>Applications based on image retrieval require editing and associating in\nintermediate spaces that are representative of the high-level concepts like\nobjects and their relationships rather than dense, pixel-level representations\nlike RGB images or semantic-label maps. We focus on one such representation,\nscene graphs, and propose a novel scene expansion task where we enrich an input\nseed graph by adding new nodes (objects) and the corresponding relationships.\nTo this end, we formulate scene graph expansion as a sequential prediction task\ninvolving multiple steps of first predicting a new node and then predicting the\nset of relationships between the newly predicted node and previous nodes in the\ngraph. We propose a sequencing strategy for observed graphs that retains the\nclustering patterns amongst nodes. In addition, we leverage external knowledge\nto train our graph generation model, enabling greater generalization of node\npredictions. Due to the inefficiency of existing maximum mean discrepancy (MMD)\nbased metrics for graph generation problems in evaluating predicted\nrelationships between nodes (objects), we design novel metrics that\ncomprehensively evaluate different aspects of predicted relations. We conduct\nextensive experiments on Visual Genome and VRD datasets to evaluate the\nexpanded scene graphs using the standard MMD-based metrics and our proposed\nmetrics. We observe that the graphs generated by our method, GEMS, better\nrepresent the real distribution of the scene graphs than the baseline methods\nlike GraphRNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rishi Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_T/0/1/0/all/0/1\">Tirupati Saketh Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1\">Vaidehi Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1\">Aniruddha Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1\">Kuldeep Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinay_V/0/1/0/all/0/1\">Vishwa Vinay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Deep Learning with Good Old-Fashioned Machine Learning. (arXiv:2207.03757v1 [cs.LG])","link":"http://arxiv.org/abs/2207.03757","description":"<p>We present a comprehensive, stacking-based framework for combining deep\nlearning with good old-fashioned machine learning, called Deep GOld. Our\nframework involves ensemble selection from 51 retrained pretrained deep\nnetworks as first-level models, and 10 machine-learning algorithms as\nsecond-level models. Enabled by today's state-of-the-art software tools and\nhardware platforms, Deep GOld delivers consistent improvement when tested on\nfour image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny\nImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original\nnetworks' performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1\">Moshe Sipper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Axle Detector based on Analysis of Bridge Acceleration Measurements by Fully Convolutional Network. (arXiv:2207.03758v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03758","description":"<p>In the practical application of the Bridge Weigh-In-Motion (BWIM) methods,\nthe position of the wheels or axles during the passage of a vehicle is in most\ncases a prerequisite. To avoid the use of conventional axle detectors and\nbridge type specific methods, we propose a novel method for axle detection\nthrough the placement of accelerometers at any point of a bridge. In order to\ndevelop a model that is as simple and comprehensible as possible, the axle\ndetection task is implemented as a binary classification problem instead of a\nregression problem. The model is implemented as a Fully Convolutional Network\nto process signals in the form of Continuous Wavelet Transforms. This allows\npassages of any length to be processed in a single step with maximum efficiency\nwhile utilising multiple scales in a single evaluation. This enables our method\nto use acceleration signals at any location of the bridge structure serving as\nVirtual Axle Detectors (VADs) without being limited to specific structural\ntypes of bridges. To test the proposed method, we analysed 3787 train passages\nrecorded on a steel trough railway bridge of a long-distance traffic line. Our\nresults on the measurement data show that our model detects 95% of the axes,\nthus, 128,599 of 134,800 previously unseen axles were correctly detected. In\ntotal, 90% of the axles can be detected with a maximum spatial error of 20cm,\nwith a maximum velocity of $v_{\\mathrm{max}}=56,3~\\mathrm{m/s}$. The analysis\nshows that our developed model can use accelerometers as VADs even under real\noperating conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenzen_S/0/1/0/all/0/1\">Steven Robert Lorenzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_H/0/1/0/all/0/1\">Henrik Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupp_M/0/1/0/all/0/1\">Maximilian Michael Rupp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeiser_L/0/1/0/all/0/1\">Leon Schmeiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berthold_H/0/1/0/all/0/1\">Hagen Berthold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firus_A/0/1/0/all/0/1\">Andrei Firus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jens Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Intrinsic Common Discriminative Features Learning for Face Forgery Detection using Adversarial Learning. (arXiv:2207.03776v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03776","description":"<p>Existing face forgery detection methods usually treat face forgery detection\nas a binary classification problem and adopt deep convolution neural networks\nto learn discriminative features. The ideal discriminative features should be\nonly related to the real/fake labels of facial images. However, we observe that\nthe features learned by vanilla classification networks are correlated to\nunnecessary properties, such as forgery methods and facial identities. Such\nphenomenon would limit forgery detection performance especially for the\ngeneralization ability. Motivated by this, we propose a novel method which\nutilizes adversarial learning to eliminate the negative effect of different\nforgery methods and facial identities, which helps classification network to\nlearn intrinsic common discriminative features for face forgery detection. To\nleverage data lacking ground truth label of facial identities, we design a\nspecial identity discriminator based on similarity information derived from\noff-the-shelf face recognition model. With the help of adversarial learning,\nour face forgery detection model learns to extract common discriminative\nfeatures through eliminating the effect of forgery methods and facial\nidentities. Extensive experiments demonstrate the effectiveness of the proposed\nmethod under both intra-dataset and cross-dataset evaluation settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Wanyi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Haojie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Changtao Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VidConv: A modernized 2D ConvNet for Efficient Video Recognition. (arXiv:2207.03782v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03782","description":"<p>Since being introduced in 2020, Vision Transformers (ViT) has been steadily\nbreaking the record for many vision tasks and are often described as\n``all-you-need\" to replace ConvNet. Despite that, ViTs are generally\ncomputational, memory-consuming, and unfriendly for embedded devices. In\naddition, recent research shows that standard ConvNet if redesigned and trained\nappropriately can compete favorably with ViT in terms of accuracy and\nscalability. In this paper, we adopt the modernized structure of ConvNet to\ndesign a new backbone for action recognition. Particularly, our main target is\nto serve for industrial product deployment, such as FPGA boards in which only\nstandard operations are supported. Therefore, our network simply consists of 2D\nconvolutions, without using any 3D convolution, long-range attention plugin, or\nTransformer blocks. While being trained with much fewer epochs (5x-10x), our\nbackbone surpasses the methods using (2+1)D and 3D convolution, and achieve\ncomparable results with ViT on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_S/0/1/0/all/0/1\">Su Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Target-free Extrinsic Calibration of a Multi-Sensor System from a Sequence of Static Viewpoints. (arXiv:2207.03785v1 [cs.RO])","link":"http://arxiv.org/abs/2207.03785","description":"<p>Mobile robotic applications need precise information about the geometric\nposition of the individual sensors on the platform. This information is given\nby the extrinsic calibration parameters which define how the sensor is rotated\nand translated with respect to a fixed reference coordinate system. Erroneous\ncalibration parameters have a negative impact on typical robotic estimation\ntasks, e.g. SLAM. In this work we propose a new method for a continuous\nestimation of the calibration parameters during operation of the robot. The\nparameter estimation is based on the matching of point clouds which are\nacquired by the sensors from multiple static viewpoints. Consequently, our\nmethod does not need any special calibration targets and is applicable to any\nsensor whose measurements can be converted to point clouds. We demonstrate the\nsuitability of our method by calibrating a multi-sensor system composed by 2\nlidar sensors, 3 cameras, and an imaging radar sensor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glira_P/0/1/0/all/0/1\">Philipp Glira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidinger_C/0/1/0/all/0/1\">Christoph Weidinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weichselbaum_J/0/1/0/all/0/1\">Johann Weichselbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementing Brightness Constancy with Deep Networks for Optical Flow Prediction. (arXiv:2207.03790v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03790","description":"<p>State-of-the-art methods for optical flow estimation rely on deep learning,\nwhich require complex sequential training schemes to reach optimal performances\non real-world data. In this work, we introduce the COMBO deep network that\nexplicitly exploits the brightness constancy (BC) model used in traditional\nmethods. Since BC is an approximate physical model violated in several\nsituations, we propose to train a physically-constrained network complemented\nwith a data-driven network. We introduce a unique and meaningful flow\ndecomposition between the physical prior and the data-driven complement,\nincluding an uncertainty quantification of the BC model. We derive a joint\ntraining scheme for learning the different components of the decomposition\nensuring an optimal cooperation, in a supervised but also in a semi-supervised\ncontext. Experiments show that COMBO can improve performances over\nstate-of-the-art supervised networks, e.g. RAFT, reaching state-of-the-art\nresults on several benchmarks. We highlight how COMBO can leverage the BC model\nand adapt to its limitations. Finally, we show that our semi-supervised method\ncan significantly simplify the training procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guen_V/0/1/0/all/0/1\">Vincent Le Guen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambour_C/0/1/0/all/0/1\">Cl&#xe9;ment Rambour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1\">Nicolas Thome</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis. (arXiv:2207.03800v1 [cs.SD])","link":"http://arxiv.org/abs/2207.03800","description":"<p>Unconstrained lip-to-speech synthesis aims to generate corresponding speeches\nfrom silent videos of talking faces with no restriction on head poses or\nvocabulary. Current works mainly use sequence-to-sequence models to solve this\nproblem, either in an autoregressive architecture or a flow-based\nnon-autoregressive architecture. However, these models suffer from several\ndrawbacks: 1) Instead of directly generating audios, they use a two-stage\npipeline that first generates mel-spectrograms and then reconstructs audios\nfrom the spectrograms. This causes cumbersome deployment and degradation of\nspeech quality due to error propagation; 2) The audio reconstruction algorithm\nused by these models limits the inference speed and audio quality, while neural\nvocoders are not available for these models since their output spectrograms are\nnot accurate enough; 3) The autoregressive model suffers from high inference\nlatency, while the flow-based model has high memory occupancy: neither of them\nis efficient enough in both time and memory usage. To tackle these problems, we\npropose FastLTS, a non-autoregressive end-to-end model which can directly\nsynthesize high-quality speech audios from unconstrained talking videos with\nlow latency, and has a relatively small model size. Besides, different from the\nwidely used 3D-CNN visual frontend for lip movement encoding, we for the first\ntime propose a transformer-based visual frontend for this task. Experiments\nshow that our model achieves $19.76\\times$ speedup for audio waveform\ngeneration compared with the current autoregressive model on input sequences of\n3 seconds, and obtains superior audio quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Transfer Learning: Co-finetuning for Action Localisation. (arXiv:2207.03807v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03807","description":"<p>Transfer learning is the predominant paradigm for training deep networks on\nsmall target datasets. Models are typically pretrained on large ``upstream''\ndatasets for classification, as such labels are easy to collect, and then\nfinetuned on ``downstream'' tasks such as action localisation, which are\nsmaller due to their finer-grained annotations. In this paper, we question this\napproach, and propose co-finetuning -- simultaneously training a single model\non multiple ``upstream'' and ``downstream'' tasks. We demonstrate that\nco-finetuning outperforms traditional transfer learning when using the same\ntotal amount of data, and also show how we can easily extend our approach to\nmultiple ``upstream'' datasets to further improve performance. In particular,\nco-finetuning significantly improves the performance on rare classes in our\ndownstream task, as it has a regularising effect, and enables the network to\nlearn feature representations that transfer between different datasets.\nFinally, we observe how co-finetuning with public, video classification\ndatasets, we are able to achieve state-of-the-art results for spatio-temporal\naction localisation on the challenging AVA and AVA-Kinetics datasets,\noutperforming recent works which develop intricate models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xuehan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1\">Rob Romijnders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1\">Josip Djolonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lu&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Zero-shot Learning via Contrastive Optimization of Attribute Representations. (arXiv:2207.03824v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03824","description":"<p>Zero-shot learning (ZSL) aims to recognize classes that do not have samples\nin the training set. One representative solution is to directly learn an\nembedding function associating visual features with corresponding class\nsemantics for recognizing new classes. Many methods extend upon this solution,\nand recent ones are especially keen on extracting rich features from images,\ne.g. attribute features. These attribute features are normally extracted within\neach individual image; however, the common traits for features across images\nyet belonging to the same attribute are not emphasized. In this paper, we\npropose a new framework to boost ZSL by explicitly learning attribute\nprototypes beyond images and contrastively optimizing them with attribute-level\nfeatures within images. Besides the novel architecture, two elements are\nhighlighted for attribute representations: a new prototype generation module is\ndesigned to generate attribute prototypes from attribute semantics; a hard\nexample-based contrastive optimization scheme is introduced to reinforce\nattribute-level features in the embedding space. We explore two alternative\nbackbones, CNN-based and transformer-based, to build our framework and conduct\nexperiments on three standard benchmarks, CUB, SUN, AwA2. Results on these\nbenchmarks demonstrate that our method improves the state of the art by a\nconsiderable margin. Our codes will be available at\nhttps://github.com/dyabel/CoAR-ZSL.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Methods : Hamiltonian Domain Translation. (arXiv:2207.03843v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03843","description":"<p>This paper proposes a novel approach to domain translation. Leveraging\nestablished parallels between generative models and dynamical systems, we\npropose a reformulation of the Cycle-GAN architecture. By embedding our model\nwith a Hamiltonian structure, we obtain a continuous, expressive and most\nimportantly invertible generative model for domain translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menier_E/0/1/0/all/0/1\">Emmanuel Menier</a> (LISN, Inria, IRT SystemX), <a href=\"http://arxiv.org/find/cs/1/au:+Bucci_M/0/1/0/all/0/1\">Michele Alessandro Bucci</a> (Inria), <a href=\"http://arxiv.org/find/cs/1/au:+Yagoubi_M/0/1/0/all/0/1\">Mouadh Yagoubi</a> (IRT SystemX), <a href=\"http://arxiv.org/find/cs/1/au:+Mathelin_L/0/1/0/all/0/1\">Lionel Mathelin</a> (LISN), <a href=\"http://arxiv.org/find/cs/1/au:+Schoenauer_M/0/1/0/all/0/1\">Marc Schoenauer</a> (Inria, LISN)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain. (arXiv:2207.03860v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03860","description":"<p>Currently, under supervised learning, a model pretrained by a large-scale\nnature scene dataset and then fine-tuned on a few specific task labeling data\nis the paradigm that has dominated the knowledge transfer learning. It has\nreached the status of consensus solution for task-aware model training in\nremote sensing domain (RSD). Unfortunately, due to different categories of\nimaging data and stiff challenges of data annotation, there is not a large\nenough and uniform remote sensing dataset to support large-scale pretraining in\nRSD. Moreover, pretraining models on large-scale nature scene datasets by\nsupervised learning and then directly fine-tuning on diverse downstream tasks\nseems to be a crude method, which is easily affected by inevitable labeling\nnoise, severe domain gaps and task-aware discrepancies. Thus, in this paper,\nconsidering the self-supervised pretraining and powerful vision transformer\n(ViT) architecture, a concise and effective knowledge transfer learning\nstrategy called ConSecutive PreTraining (CSPT) is proposed based on the idea of\nnot stopping pretraining in natural language processing (NLP), which can\ngradually bridge the domain gap and transfer knowledge from the nature scene\ndomain to the RSD. The proposed CSPT also can release the huge potential of\nunlabeled data for task-aware model training. Finally, extensive experiments\nare carried out on twelve datasets in RSD involving three types of downstream\ntasks (e.g., scene classification, object detection and land cover\nclassification) and two types of imaging data (e.g., optical and SAR). The\nresults show that by utilizing the proposed CSPT for task-aware model training,\nalmost all downstream tasks in RSD can outperform the previous method of\nsupervised pretraining-then-fine-tuning and even surpass the state-of-the-art\n(SOTA) performance without any expensive labeling consumption and careful model\ndesign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanqun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">He Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel-level Correspondence for Self-Supervised Learning from Video. (arXiv:2207.03866v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03866","description":"<p>While self-supervised learning has enabled effective representation learning\nin the absence of labels, for vision, video remains a relatively untapped\nsource of supervision. To address this, we propose Pixel-level Correspondence\n(PiCo), a method for dense contrastive learning from video. By tracking points\nwith optical flow, we obtain a correspondence map which can be used to match\nlocal features at different points in time. We validate PiCo on standard\nbenchmarks, outperforming self-supervised baselines on multiple dense\nprediction tasks, without compromising performance on image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_C/0/1/0/all/0/1\">Chris Russell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sequential Descriptors for Sequence-based Visual Place Recognition. (arXiv:2207.03868v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03868","description":"<p>In robotics, Visual Place Recognition is a continuous process that receives\nas input a video stream to produce a hypothesis of the robot's current position\nwithin a map of known places. This task requires robust, scalable, and\nefficient techniques for real applications. This work proposes a detailed\ntaxonomy of techniques using sequential descriptors, highlighting different\nmechanism to fuse the information from the individual images. This\ncategorization is supported by a complete benchmark of experimental results\nthat provides evidence on the strengths and weaknesses of these different\narchitectural choices. In comparison to existing sequential descriptors\nmethods, we further investigate the viability of Transformers instead of CNN\nbackbones, and we propose a new ad-hoc sequence-level aggregator called\nSeqVLAD, which outperforms prior state of the art on different datasets. The\ncode is available at https://github.com/vandal-vpr/vg-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mereu_R/0/1/0/all/0/1\">Riccardo Mereu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivigno_G/0/1/0/all/0/1\">Gabriele Trivigno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berton_G/0/1/0/all/0/1\">Gabriele Berton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BlindSpotNet: Seeing Where We Cannot See. (arXiv:2207.03870v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03870","description":"<p>We introduce 2D blind spot estimation as a critical visual task for road\nscene understanding. By automatically detecting road regions that are occluded\nfrom the vehicle's vantage point, we can proactively alert a manual driver or a\nself-driving system to potential causes of accidents (e.g., draw attention to a\nroad region from which a child may spring out). Detecting blind spots in full\n3D would be challenging, as 3D reasoning on the fly even if the car is equipped\nwith LiDAR would be prohibitively expensive and error prone. We instead propose\nto learn to estimate blind spots in 2D, just from a monocular camera. We\nachieve this in two steps. We first introduce an automatic method for\ngenerating ``ground-truth'' blind spot training data for arbitrary driving\nvideos by leveraging monocular depth estimation, semantic segmentation, and\nSLAM. The key idea is to reason in 3D but from 2D images by defining blind\nspots as those road regions that are currently invisible but become visible in\nthe near future. We construct a large-scale dataset with this automatic offline\nblind spot estimation, which we refer to as Road Blind Spot (RBS) dataset.\nNext, we introduce BlindSpotNet (BSN), a simple network that fully leverages\nthis dataset for fully automatic estimation of frame-wise blind spot\nprobability maps for arbitrary driving videos. Extensive experimental results\ndemonstrate the validity of our RBS Dataset and the effectiveness of our BSN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fukuda_T/0/1/0/all/0/1\">Taichi Fukuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_K/0/1/0/all/0/1\">Kotaro Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishizaki_S/0/1/0/all/0/1\">Shinya Ishizaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nobuhara_S/0/1/0/all/0/1\">Shohei Nobuhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishino_K/0/1/0/all/0/1\">Ko Nishino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Transfer Learning in Agricultural Applications: AgriNet. (arXiv:2207.03881v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03881","description":"<p>Advances in deep learning and transfer learning have paved the way for\nvarious automation classification tasks in agriculture, including plant\ndiseases, pests, weeds, and plant species detection. However, agriculture\nautomation still faces various challenges, such as the limited size of datasets\nand the absence of plant-domain-specific pretrained models. Domain specific\npretrained models have shown state of art performance in various computer\nvision tasks including face recognition and medical imaging diagnosis. In this\npaper, we propose AgriNet dataset, a collection of 160k agricultural images\nfrom more than 19 geographical locations, several images captioning devices,\nand more than 423 classes of plant species and diseases. We also introduce\nAgriNet models, a set of pretrained models on five ImageNet architectures:\nVGG16, VGG19, Inception-v3, InceptionResNet-v2, and Xception. AgriNet-VGG19\nachieved the highest classification accuracy of 94 % and the highest F1-score\nof 92%. Additionally, all proposed models were found to accurately classify the\n423 classes of plant species, diseases, pests, and weeds with a minimum\naccuracy of 87% for the Inception-v3 model.Finally, experiments to evaluate of\nsuperiority of AgriNet models compared to ImageNet models were conducted on two\nexternal datasets: pest and plant diseases dataset from Bangladesh and a plant\ndiseases dataset from Kashmir.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahili_Z/0/1/0/all/0/1\">Zahraa Al Sahili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1\">Mariette Awad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Networks and Other Generative Models. (arXiv:2207.03887v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03887","description":"<p>Generative networks are fundamentally different in their aim and methods\ncompared to CNNs for classification, segmentation, or object detection. They\nhave initially not been meant to be an image analysis tool, but to produce\nnaturally looking images. The adversarial training paradigm has been proposed\nto stabilize generative methods, and has proven to be highly successful --\nthough by no means from the first attempt.\n</p>\n<p>This chapter gives a basic introduction into the motivation for Generative\nAdversarial Networks (GANs) and traces the path of their success by abstracting\nthe basic task and working mechanism, and deriving the difficulty of early\npractical approaches. Methods for a more stable training will be shown, and\nalso typical signs for poor convergence and their reasons.\n</p>\n<p>Though this chapter focuses on GANs that are meant for image generation and\nimage analysis, the adversarial training paradigm itself is not specific to\nimages, and also generalizes to tasks in image analysis. Examples of\narchitectures for image semantic segmentation and abnormality detection will be\nacclaimed, before contrasting GANs with further generative modeling approaches\nlately entering the scene. This will allow a contextualized view on the limits\nbut also benefits of GANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wenzel_M/0/1/0/all/0/1\">Markus Wenzel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defense Against Multi-target Trojan Attacks. (arXiv:2207.03895v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03895","description":"<p>Adversarial attacks on deep learning-based models pose a significant threat\nto the current AI infrastructure. Among them, Trojan attacks are the hardest to\ndefend against. In this paper, we first introduce a variation of the Badnet\nkind of attacks that introduces Trojan backdoors to multiple target classes and\nallows triggers to be placed anywhere in the image. The former makes it more\npotent and the latter makes it extremely easy to carry out the attack in the\nphysical space. The state-of-the-art Trojan detection methods fail with this\nthreat model. To defend against this attack, we first introduce a trigger\nreverse-engineering mechanism that uses multiple images to recover a variety of\npotential triggers. We then propose a detection mechanism by measuring the\ntransferability of such recovered triggers. A Trojan trigger will have very\nhigh transferability i.e. they make other images also go to the same class. We\nstudy many practical advantages of our attack method and then demonstrate the\ndetection performance using a variety of image datasets. The experimental\nresults show the superior detection performance of our method over the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harikumar_H/0/1/0/all/0/1\">Haripriya Harikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1\">Kien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sunil Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_W/0/1/0/all/0/1\">Wei Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susilo_W/0/1/0/all/0/1\">Willy Susilo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkastesh_S/0/1/0/all/0/1\">Svetha Venkastesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Big Learning: A Universal Machine Learning Paradigm?. (arXiv:2207.03899v1 [cs.LG])","link":"http://arxiv.org/abs/2207.03899","description":"<p>Recent breakthroughs based on big/foundation models reveal a vague avenue for\nartificial intelligence, that is, bid data, big/foundation models, big\nlearning, $\\cdots$. Following that avenue, here we elaborate on the newly\nintroduced big learning. Specifically, big learning comprehensively exploits\nthe available information inherent in large-scale complete/incomplete data, by\nsimultaneously learning to model many-to-all joint/conditional/marginal data\ndistributions (thus named big learning) with one universal foundation model. We\nreveal that big learning is what existing foundation models are implicitly\ndoing; accordingly, our big learning provides high-level guidance for flexible\ndesign and improvements of foundation models, accelerating the true\nself-learning on the Internet. Besides, big learning ($i$) is equipped with\nmarvelous flexibility for both training data and training-task customization;\n($ii$) potentially delivers all joint/conditional/marginal data capabilities\nafter training; ($iii$) significantly reduces the training-test gap with\nimproved model generalization; and ($iv$) unifies conventional machine learning\nparadigms e.g. supervised learning, unsupervised learning, generative learning,\netc. and enables their flexible cooperation, manifesting a universal learning\nparadigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yulai Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Miaoyun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducing sensory induced hallucinations via neural fields. (arXiv:2207.03901v1 [q-bio.NC])","link":"http://arxiv.org/abs/2207.03901","description":"<p>Understanding sensory-induced cortical patterns in the primary visual cortex\nV1 is an important challenge both for physiological motivations and for\nimproving our understanding of human perception and visual organisation. In\nthis work, we focus on pattern formation in the visual cortex when the cortical\nactivity is driven by a geometric visual hallucination-like stimulus. In\nparticular, we present a theoretical framework for sensory-induced\nhallucinations which allows one to reproduce novel psychophysical results such\nas the MacKay effect (Nature, 1957) and the Billock and Tsou experiences (PNAS,\n2007).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Tamekue_C/0/1/0/all/0/1\">Cyprien Tamekue</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Prandi_D/0/1/0/all/0/1\">Dario Prandi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chitour_Y/0/1/0/all/0/1\">Yacine Chitour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Mask Attention Interaction and Scale Enhancement Network for SAR Ship Instance Segmentation. (arXiv:2207.03912v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03912","description":"<p>Most of existing synthetic aperture radar (SAR) ship in-stance segmentation\nmodels do not achieve mask interac-tion or offer limited interaction\nperformance. Besides, their multi-scale ship instance segmentation performance\nis moderate especially for small ships. To solve these problems, we propose a\nmask attention interaction and scale enhancement network (MAI-SE-Net) for SAR\nship instance segmentation. MAI uses an atrous spatial pyra-mid pooling (ASPP)\nto gain multi-resolution feature re-sponses, a non-local block (NLB) to model\nlong-range spa-tial dependencies, and a concatenation shuffle attention block\n(CSAB) to improve interaction benefits. SE uses a content-aware reassembly of\nfeatures block (CARAFEB) to generate an extra pyramid bottom-level to boost\nsmall ship performance, a feature balance operation (FBO) to improve scale\nfeature description, and a global context block (GCB) to refine features.\nExperimental results on two public SSDD and HRSID datasets reveal that\nMAI-SE-Net outperforms the other nine competitive models, better than the\nsuboptimal model by 4.7% detec-tion AP and 3.4% segmentation AP on SSDD and by\n3.0% detection AP and 2.4% segmentation AP on HRSID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoling Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection. (arXiv:2207.03917v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03917","description":"<p>This paper presents a Refinement Pyramid Transformer (RePFormer) for robust\nfacial landmark detection. Most facial landmark detectors focus on learning\nrepresentative image features. However, these CNN-based feature representations\nare not robust enough to handle complex real-world scenarios due to ignoring\nthe internal structure of landmarks, as well as the relations between landmarks\nand context. In this work, we formulate the facial landmark detection task as\nrefining landmark queries along pyramid memories. Specifically, a pyramid\ntransformer head (PTH) is introduced to build both homologous relations among\nlandmarks and heterologous relations between landmarks and cross-scale\ncontexts. Besides, a dynamic landmark refinement (DLR) module is designed to\ndecompose the landmark regression into an end-to-end refinement procedure,\nwhere the dynamically aggregated queries are transformed to residual\ncoordinates predictions. Extensive experimental results on four facial landmark\ndetection benchmarks and their various subsets demonstrate the superior\nperformance and high robustness of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Furigana Text in Images. (arXiv:2207.03960v1 [cs.CV])","link":"http://arxiv.org/abs/2207.03960","description":"<p>Furigana are pronunciation notes used in Japanese writing. Being able to\ndetect these can help improve optical character recognition (OCR) performance\nor make more accurate digital copies of Japanese written media by correctly\ndisplaying furigana. This project focuses on detecting furigana in Japanese\nbooks and comics. While there has been research into the detection of Japanese\ntext in general, there are currently no proposed methods for detecting\nfurigana.\n</p>\n<p>We construct a new dataset containing Japanese written media and annotations\nof furigana. We propose an evaluation metric for such data which is similar to\nthe evaluation protocols used in object detection except that it allows groups\nof objects to be labeled by one annotation. We propose a method for detection\nof furigana that is based on mathematical morphology and connected component\nanalysis. We evaluate the detections of the dataset and compare different\nmethods for text extraction. We also evaluate different types of images such as\nbooks and comics individually and discuss the challenges of each type of image.\n</p>\n<p>The proposed method reaches an F1-score of 76\\% on the dataset. The method\nperforms well on regular books, but less so on comics, and books of irregular\nformat. Finally, we show that the proposed method can improve the performance\nof OCR by 5\\% on the manga109 dataset.\n</p>\n<p>Source code is available via\n\\texttt{\\url{https://github.com/nikolajkb/FuriganaDetection}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bjerregaard_N/0/1/0/all/0/1\">Nikolaj Kj&#xf8;ller Bjerregaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_S/0/1/0/all/0/1\">Stefan Heinrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination. (arXiv:2207.03961v1 [cs.CL])","link":"http://arxiv.org/abs/2207.03961","description":"<p>As humans, we can modify our assumptions about a scene by imagining\nalternative objects or concepts in our minds. For example, we can easily\nanticipate the implications of the sun being overcast by rain clouds (e.g., the\nstreet will get wet) and accordingly prepare for that. In this paper, we\nintroduce a new task/dataset called Commonsense Reasoning for Counterfactual\nScene Imagination (CoSIm) which is designed to evaluate the ability of AI\nsystems to reason about scene change imagination. In this task/dataset, models\nare given an image and an initial question-response pair about the image. Next,\na counterfactual imagined scene change (in textual form) is applied, and the\nmodel has to predict the new response to the initial question based on this\nscene change. We collect 3.5K high-quality and challenging data instances, with\neach instance consisting of an image, a commonsense question with a response, a\ndescription of a counterfactual change, a new response to the question, and\nthree distractor responses. Our dataset contains various complex scene change\ntypes (such as object addition/removal/state change, event description,\nenvironment change, etc.) that require models to imagine many different\nscenarios and reason about the changed scenes. We present a baseline model\nbased on a vision-language Transformer (i.e., LXMERT) and ablation studies.\nThrough human evaluation, we demonstrate a large human-model performance gap,\nsuggesting room for promising future work on this challenging counterfactual,\nscene imagination task. Our code and dataset are publicly available at:\nhttps://github.com/hyounghk/CoSIm\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyounghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zala_A/0/1/0/all/0/1\">Abhay Zala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Collapse in Contrast Maximization Frameworks. (arXiv:2207.04007v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04007","description":"<p>Context maximization (CMax) is a framework that provides state-of-the-art\nresults on several event-based computer vision tasks, such as ego-motion or\noptical flow estimation. However, it may suffer from a problem called event\ncollapse, which is an undesired solution where events are warped into too few\npixels. As prior works have largely ignored the issue or proposed workarounds,\nit is imperative to analyze this phenomenon in detail. Our work demonstrates\nevent collapse in its simplest form and proposes collapse metrics by using\nfirst principles of space-time deformation based on differential geometry and\nphysics. We experimentally show on publicly available datasets that the\nproposed metrics mitigate event collapse and do not harm well-posed warps. To\nthe best of our knowledge, regularizers based on the proposed metrics are the\nonly effective solution against event collapse in the experimental settings\nconsidered, compared with other methods. We hope that this work inspires\nfurther research to tackle more complex warp models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiba_S/0/1/0/all/0/1\">Shintaro Shiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_Y/0/1/0/all/0/1\">Yoshimitsu Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCAtt: A Cognitive-Conditioned Driver Attention Dataset (Supplementary Material). (arXiv:2207.04028v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04028","description":"<p>The task of driver attention prediction has drawn considerable interest among\nresearchers in robotics and the autonomous vehicle industry. Driver attention\nprediction can play an instrumental role in mitigating and preventing high-risk\nevents, like collisions and casualties. However, existing driver attention\nprediction models neglect the distraction state and intention of the driver,\nwhich can significantly influence how they observe their surroundings. To\naddress these issues, we present a new driver attention dataset, CoCAtt\n(Cognitive-Conditioned Attention). Unlike previous driver attention datasets,\nCoCAtt includes per-frame annotations that describe the distraction state and\nintention of the driver. In addition, the attention data in our dataset is\ncaptured in both manual and autopilot modes using eye-tracking devices of\ndifferent resolutions. Our results demonstrate that incorporating the above two\ndriver states into attention modeling can improve the performance of driver\nattention prediction. To the best of our knowledge, this work is the first to\nprovide autopilot attention data. Furthermore, CoCAtt is currently the largest\nand the most diverse driver attention dataset in terms of autonomy levels, eye\ntracker resolutions, and driving scenarios. CoCAtt is available for download at\nhttps://cocatt-dataset.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijayaratne_N/0/1/0/all/0/1\">Niviru Wijayaratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_P/0/1/0/all/0/1\">Pranav Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1\">Aamir Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Peter Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"k-means Mask Transformer. (arXiv:2207.04044v1 [cs.CV])","link":"http://arxiv.org/abs/2207.04044","description":"<p>The rise of transformers in vision tasks not only advances network backbone\ndesigns, but also starts a brand-new page to achieve end-to-end image\nrecognition (e.g., object detection and panoptic segmentation). Originated from\nNatural Language Processing (NLP), transformer architectures, consisting of\nself-attention and cross-attention, effectively learn long-range interactions\nbetween elements in a sequence. However, we observe that most existing\ntransformer-based vision models simply borrow the idea from NLP, neglecting the\ncrucial difference between languages and images, particularly the extremely\nlarge sequence length of spatially flattened pixel features. This subsequently\nimpedes the learning in cross-attention between pixel features and object\nqueries. In this paper, we rethink the relationship between pixels and object\nqueries and propose to reformulate the cross-attention learning as a clustering\nprocess. Inspired by the traditional k-means clustering algorithm, we develop a\nk-means Mask Xformer (kMaX-DeepLab) for segmentation tasks, which not only\nimproves the state-of-the-art, but also enjoys a simple and elegant design. As\na result, our kMaX-DeepLab achieves a new state-of-the-art performance on COCO\nval set with 58.0% PQ, and Cityscapes val set with 68.4% PQ, 44.0% AP, and\n83.5% mIoU without test-time augmentation or external dataset. We hope our work\ncan shed some light on designing transformers tailored for vision tasks. Code\nand models are available at https://github.com/google-research/deeplab2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qihang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1\">Siyuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Maxwell Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hatwig Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification. (arXiv:2101.10667v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.10667","description":"<p>The COVID-19 pandemic has threatened global health. Many studies have applied\ndeep convolutional neural networks (CNN) to recognize COVID-19 based on chest\n3D computed tomography (CT). Recent works show that no model generalizes well\nacross CT datasets from different countries, and manually designing models for\nspecific datasets requires expertise; thus, neural architecture search (NAS)\nthat aims to search models automatically has become an attractive solution. To\nreduce the search cost on large 3D CT datasets, most NAS-based works use the\nweight-sharing (WS) strategy to make all models share weights within a\nsupernet; however, WS inevitably incurs search instability, leading to\ninaccurate model estimation. In this work, we propose an efficient Evolutionary\nMulti-objective ARchitecture Search (EMARS) framework. We propose a new\nobjective, namely potential, which can help exploit promising models to\nindirectly reduce the number of models involved in weights training, thus\nalleviating search instability. We demonstrate that under objectives of\naccuracy and potential, EMARS can balance exploitation and exploration, i.e.,\nreducing search time and finding better models. Our searched models are small\nand perform better than prior works on three public COVID-19 3D CT datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xin He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_G/0/1/0/all/0/1\">Guohao Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiyong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chu_X/0/1/0/all/0/1\">Xiaowen Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Topography Characterization Using a Simple Optical Device and Artificial Neural Networks. (arXiv:2103.08482v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08482","description":"<p>State-of-the-art methods for quantifying wear in cylinder liners of large\ninternal combustion engines require disassembly and cutting of the liner. This\nis followed by laboratory-based high-resolution microscopic surface depth\nmeasurement that quantitatively evaluates wear based on bearing load curves\n(Abbott-Firestone curves). Such methods are destructive, time-consuming and\ncostly. The goal of the research presented is to develop nondestructive yet\nreliable methods for quantifying the surface topography. A novel machine\nlearning framework is proposed that allows prediction of the bearing load\ncurves from RGB images of the liner surface that can be collected with a\nhandheld microscope. A joint deep learning approach involving two neural\nnetwork modules optimizes the prediction quality of surface roughness\nparameters as well and is trained using a custom-built database containing 422\naligned depth profile and reflection image pairs of liner surfaces. The\nobserved success suggests its great potential for on-site wear assessment of\nengines during service.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_M/0/1/0/all/0/1\">Matthias Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravova_A/0/1/0/all/0/1\">Ad&#xe9;la Moravov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiesling_C/0/1/0/all/0/1\">Constantin Kiesling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_M/0/1/0/all/0/1\">Martin Kober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fimml_W/0/1/0/all/0/1\">Wolfgang Fimml</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Single-Image Depth Synthesis with cycle-consistent Wasserstein GANs. (arXiv:2103.16938v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16938","description":"<p>Real-time estimation of actual environment depth is an essential module for\nvarious autonomous system tasks such as localization, obstacle detection and\npose estimation. During the last decade of machine learning, extensive\ndeployment of deep learning methods to computer vision tasks yielded successful\napproaches for realistic depth synthesis out of a simple RGB modality. While\nmost of these models rest on paired depth data or availability of video\nsequences and stereo images, there is a lack of methods facing single-image\ndepth synthesis in an unsupervised manner. Therefore, in this study, latest\nadvancements in the field of generative neural networks are leveraged to fully\nunsupervised single-image depth synthesis. To be more exact, two\ncycle-consistent generators for RGB-to-depth and depth-to-RGB transfer are\nimplemented and simultaneously optimized using the Wasserstein-1 distance. To\nensure plausibility of the proposed method, we apply the models to a self\nacquised industrial data set as well as to the renown NYU Depth v2 data set,\nwhich allows comparison with existing approaches. The observed success in this\nstudy suggests high potential for unpaired single-image depth estimation in\nreal world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravova_A/0/1/0/all/0/1\">Ad&#xe9;la Moravov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Bayesian Posterior Approximation with Deep Ensembles. (arXiv:2105.14275v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14275","description":"<p>Ensembles of independently trained neural networks are a state-of-the-art\napproach to estimate predictive uncertainty in Deep Learning, and can be\ninterpreted as an approximation of the posterior distribution via a mixture of\ndelta functions. The training of ensembles relies on non-convexity of the loss\nlandscape and random initialization of their individual members, making the\nresulting posterior approximation uncontrolled. This paper proposes a novel and\nprincipled method to tackle this limitation, minimizing an $f$-divergence\nbetween the true posterior and a kernel density estimator (KDE) in a function\nspace. We analyze this objective from a combinatorial point of view, and show\nthat it is submodular with respect to mixture components for any $f$.\nSubsequently, we consider the problem of greedy ensemble construction. From the\nmarginal gain on the negative $f$-divergence, which quantifies an improvement\nin posterior approximation yielded by adding a new component into the KDE, we\nderive a novel diversity term for ensemble methods. The performance of our\napproach is demonstrated on computer vision out-of-distribution detection\nbenchmarks in a range of architectures trained on multiple datasets. The source\ncode of our method is made publicly available at\nhttps://github.com/Oulu-IMEDS/greedy_ensembles_training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiulpin_A/0/1/0/all/0/1\">Aleksei Tiulpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew B. Blaschko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction. (arXiv:2106.12102v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12102","description":"<p>Most modern deep learning-based multi-view 3D reconstruction techniques use\nRNNs or fusion modules to combine information from multiple images after\nindependently encoding them. These two separate steps have loose connections\nand do not allow easy information sharing among views. We propose LegoFormer, a\ntransformer model for voxel-based 3D reconstruction that uses the attention\nlayers to share information among views during all computational stages.\nMoreover, instead of predicting each voxel independently, we propose to\nparametrize the output with a series of low-rank decomposition factors. This\nreformulation allows the prediction of an object as a set of independent\nregular structures then aggregated to obtain the final reconstruction.\nExperiments conducted on ShapeNet demonstrate the competitive performance of\nour model with respect to the state of the art while having increased\ninterpretability thanks to the self-attention layers. We also show promising\ngeneralization results to real data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yagubbayli_F/0/1/0/all/0/1\">Farid Yagubbayli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1\">Alessio Tonioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image scaling by de la Vall\\'ee-Poussin filtered interpolation. (arXiv:2109.13897v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13897","description":"<p>We present a new image scaling method both for downscaling and upscaling,\nrunning with any scale factor or desired size. The resized image is achieved by\nsampling a bivariate polynomial which globally interpolates the data at the new\nscale. The method's particularities lay in both the sampling model and the\ninterpolation polynomial we use. Rather than classical uniform grids, we\nconsider an unusual sampling system based on Chebyshev zeros of the first kind.\nSuch optimal distribution of nodes permits to consider near--best interpolation\npolynomials defined by a filter of de la Vall\\'ee Poussin type. The action ray\nof this filter provides an additional parameter that can be suitably regulated\nto improve the approximation. The method has been tested on a significant\nnumber of different image datasets. The results are evaluated in qualitative\nand quantitative terms and compared with other available competitive methods.\nThe perceived quality of the resulting scaled images is such that important\ndetails are preserved, and the appearance of artifacts is low. Competitive\nquality measurement values, good visual quality, limited computational effort,\nand moderate memory demand make the method suitable for real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Occorsio_D/0/1/0/all/0/1\">Donatella Occorsio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramella_G/0/1/0/all/0/1\">Giuliana Ramella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Themistoclakis_W/0/1/0/all/0/1\">Woula Themistoclakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.08851","description":"<p>In recent times, deep neural networks achieved outstanding predictive\nperformance on various classification and pattern recognition tasks. However,\nmany real-world prediction problems have ordinal response variables, and this\nordering information is ignored by conventional classification losses such as\nthe multi-category cross-entropy. Ordinal regression methods for deep neural\nnetworks address this. One such method is the CORAL method, which is based on\nan earlier binary label extension framework and achieves rank consistency among\nits output layer tasks by imposing a weight-sharing constraint. However, while\nearlier experiments showed that CORAL's rank consistency is beneficial for\nperformance, {it is limited by a weight-sharing constraint in a neural\nnetwork's fully connected output layer. We propose a new method for\nrank-consistent ordinal regression without this limitation. Our rank-consistent\nordinal regression framework (CORN) achieves rank consistency by a novel\ntraining scheme. This training scheme uses} conditional training sets to obtain\nthe unconditional rank probabilities through applying the chain rule for\nconditional probability distributions. Experiments on various datasets\ndemonstrate the efficacy of the proposed method to utilize the ordinal target\ninformation, and the absence of the weight-sharing restriction improves the\nperformance substantially compared to the CORAL reference approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xintong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1\">Wenzhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raschka_S/0/1/0/all/0/1\">Sebastian Raschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions. (arXiv:2111.10337v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10337","description":"<p>We study joint video and language (VL) pre-training to enable cross-modality\nlearning and benefit plentiful downstream VL tasks. Existing works either\nextract low-quality video features or learn limited text embedding, while\nneglecting that high-resolution videos and diversified semantics can\nsignificantly improve cross-modality learning. In this paper, we propose a\nnovel High-resolution and Diversified VIdeo-LAnguage pre-training model\n(HD-VILA) for many visual tasks. In particular, we collect a large dataset with\ntwo distinct properties: 1) the first high-resolution dataset including 371.5k\nhours of 720p videos, and 2) the most diversified dataset covering 15 popular\nYouTube categories. To enable VL pre-training, we jointly optimize the HD-VILA\nmodel by a hybrid Transformer that learns rich spatiotemporal features, and a\nmultimodal Transformer that enforces interactions of the learned video features\nwith diversified texts. Our pre-training model achieves new state-of-the-art\nresults in 10 VL understanding tasks and 2 more novel text-to-visual generation\ntasks. For example, we outperform SOTA models with relative increases of 40.4%\nR@1 in zero-shot MSR-VTT text-to-video retrieval task and 55.4% in\nhigh-resolution dataset LSMDC. The learned VL embedding is also effective in\ngenerating visually pleasing and semantically relevant results in\ntext-to-visual editing and super-resolution tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongwei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_T/0/1/0/all/0/1\">Tiankai Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yanhong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"diffConv: Analyzing Irregular Point Clouds with an Irregular View. (arXiv:2111.14658v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14658","description":"<p>Standard spatial convolutions assume input data with a regular neighborhood\nstructure. Existing methods typically generalize convolution to the irregular\npoint cloud domain by fixing a regular \"view\" through e.g. a fixed neighborhood\nsize, where the convolution kernel size remains the same for each point.\nHowever, since point clouds are not as structured as images, the fixed neighbor\nnumber gives an unfortunate inductive bias. We present a novel graph\nconvolution named Difference Graph Convolution (diffConv), which does not rely\non a regular view. diffConv operates on spatially-varying and density-dilated\nneighborhoods, which are further adapted by a learned masked attention\nmechanism. Experiments show that our model is very robust to the noise,\nobtaining state-of-the-art performance in 3D shape classification and scene\nunderstanding tasks, along with a faster inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Manxi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CloudWalker: Random walks for 3D point cloud shape analysis. (arXiv:2112.01050v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01050","description":"<p>Point clouds are gaining prominence as a method for representing 3D shapes,\nbut their irregular structure poses a challenge for deep learning methods. In\nthis paper we propose CloudWalker, a novel method for learning 3D shapes using\nrandom walks. Previous works attempt to adapt Convolutional Neural Networks\n(CNNs) or impose a grid or mesh structure to 3D point clouds. This work\npresents a different approach for representing and learning the shape from a\ngiven point set. The key idea is to impose structure on the point set by\nmultiple random walks through the cloud for exploring different regions of the\n3D object. Then we learn a per-point and per-walk representation and aggregate\nmultiple walk predictions at inference. Our approach achieves state-of-the-art\nresults for two 3D shape analysis tasks: classification and retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mesika_A/0/1/0/all/0/1\">Adi Mesika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Shabat_Y/0/1/0/all/0/1\">Yizhak Ben-Shabat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tal_A/0/1/0/all/0/1\">Ayellet Tal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Attentional Network for Semantic Segmentation. (arXiv:2112.04108v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04108","description":"<p>Recent non-local self-attention methods have proven to be effective in\ncapturing long-range dependencies for semantic segmentation. These methods\nusually form a similarity map of RC*C (by compressing spatial dimensions) or\nRHW*HW (by compressing channels) to describe the feature relations along either\nchannel or spatial dimensions, where C is the number of channels, H and W are\nthe spatial dimensions of the input feature map. However, such practices tend\nto condense feature dependencies along the other dimensions,hence causing\nattention missing, which might lead to inferior results for small/thin\ncategories or inconsistent segmentation inside large objects. To address this\nproblem, we propose anew approach, namely Fully Attentional Network (FLANet),to\nencode both spatial and channel attentions in a single similarity map while\nmaintaining high computational efficiency. Specifically, for each channel map,\nour FLANet can harvest feature responses from all other channel maps, and the\nassociated spatial positions as well, through a novel fully attentional module.\nOur new method has achieved state-of-the-art performance on three challenging\nsemantic segmentation datasets,i.e., 83.6%, 46.99%, and 88.5% on the Cityscapes\ntest set,the ADE20K validation set, and the PASCAL VOC test set,respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Low-Light Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.03145","description":"<p>Images captured in the low-light condition suffer from low visibility and\nvarious imaging artifacts, e.g., real noise. Existing supervised enlightening\nalgorithms require a large set of pixel-aligned training image pairs, which are\nhard to prepare in practice. Though weakly-supervised or unsupervised methods\ncan alleviate such challenges without using paired training images, some\nreal-world artifacts inevitably get falsely amplified because of the lack of\ncorresponded supervision. In this paper, instead of using perfectly aligned\nimages for training, we creatively employ the misaligned real-world images as\nthe guidance, which are considerably easier to collect. Specifically, we\npropose a Cross-Image Disentanglement Network (CIDN) to separately extract\ncross-image brightness and image-specific content features from\nlow/normal-light images. Based on that, CIDN can simultaneously correct the\nbrightness and suppress image artifacts in the feature domain, which largely\nincreases the robustness to the pixel shifts. Furthermore, we collect a new\nlow-light image enhancement dataset consisting of misaligned training images\nwith real-world corruptions. Experimental results show that our model achieves\nstate-of-the-art performances on both the newly proposed dataset and other\npopular low-light datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Lanqing Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_R/0/1/0/all/0/1\">Renjie Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kot_A/0/1/0/all/0/1\">Alex Kot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Network. (arXiv:2202.09741v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09741","description":"<p>While originally designed for natural language processing tasks, the\nself-attention mechanism has recently taken various computer vision areas by\nstorm. However, the 2D nature of images brings three challenges for applying\nself-attention in computer vision. (1) Treating images as 1D sequences neglects\ntheir 2D structures. (2) The quadratic complexity is too expensive for\nhigh-resolution images. (3) It only captures spatial adaptability but ignores\nchannel adaptability. In this paper, we propose a novel linear attention named\nlarge kernel attention (LKA) to enable self-adaptive and long-range\ncorrelations in self-attention while avoiding its shortcomings. Furthermore, we\npresent a neural network based on LKA, namely Visual Attention Network (VAN).\nWhile extremely simple, VAN surpasses similar size vision transformers(ViTs)\nand convolutional neural networks(CNNs) in various tasks, including image\nclassification, object detection, semantic segmentation, panoptic segmentation,\npose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet\nbenchmark and set new state-of-the-art performance (58.2 PQ) for panoptic\nsegmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for\nsemantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object\ndetection on COCO dataset. It provides a novel method and a simple yet strong\nbaseline for the community. Code is available at\nhttps://github.com/Visual-Attention-Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-Ze Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng-Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v4 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2202.12154","description":"<p>Trojan attacks on deep neural networks are both dangerous and surreptitious.\nOver the past few years, Trojan attacks have advanced from using only a single\ninput-agnostic trigger and targeting only one class to using multiple,\ninput-specific triggers and targeting multiple classes. However, Trojan\ndefenses have not caught up with this development. Most defense methods still\nmake inadequate assumptions about Trojan triggers and target classes, thus, can\nbe easily circumvented by modern Trojan attacks. To deal with this problem, we\npropose two novel \"filtering\" defenses called Variational Input Filtering (VIF)\nand Adversarial Input Filtering (AIF) which leverage lossy data compression and\nadversarial learning respectively to effectively purify potential Trojan\ntriggers in the input at run time without making assumptions about the number\nof triggers/target classes or the input dependence property of triggers. In\naddition, we introduce a new defense mechanism called\n\"Filtering-then-Contrasting\" (FtC) which helps avoid the drop in classification\naccuracy on clean data caused by \"filtering\", and combine it with VIF/AIF to\nderive new defenses of this kind. Extensive experimental results and ablation\nstudies show that our proposed defenses significantly outperform well-known\nbaseline defenses in mitigating five advanced Trojan attacks including two\nrecent state-of-the-art while being quite robust to small amounts of training\ndata and large-norm triggers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_K/0/1/0/all/0/1\">Kien Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harikumar_H/0/1/0/all/0/1\">Haripriya Harikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Truyen Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_S/0/1/0/all/0/1\">Santu Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susilo_W/0/1/0/all/0/1\">Willy Susilo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Svetha Venkatesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised classification of medical ultrasound images based on generative adversarial network. (arXiv:2203.06184v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.06184","description":"<p>Medical ultrasound (US) is one of the most widely used imaging modalities in\nclinical practice. However, its use presents unique challenges such as variable\nimaging quality. Deep learning (DL) can be used as an advanced medical US image\nanalysis tool, while the performance of the DL model is greatly limited by the\nscarcity of big datasets. Here, we develop semi-supervised classification\nenhancement (SSCE) structures by combining convolutional neural network (CNN)\nand generative adversarial network (GAN) to address the data shortage. A breast\ncancer dataset with 780 images is used as our base dataset. The results show\nthat our SSCE structures obtain an accuracy of up to 97.9%, showing a maximum\n21.6% improvement compared with utilizing CNN models alone and outperforming\nthe previous methods using the same dataset by up to 23.9%. We believe our\nproposed state-of-the-art method can be regarded as a potential auxiliary tool\nfor the diagnoses of medical US images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoshan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chau Hung Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsity and Heterogeneous Dropout for Continual Learning in the Null Space of Neural Activations. (arXiv:2203.06514v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.06514","description":"<p>Continual/lifelong learning from a non-stationary input data stream is a\ncornerstone of intelligence. Despite their phenomenal performance in a wide\nvariety of applications, deep neural networks are prone to forgetting their\npreviously learned information upon learning new ones. This phenomenon is\ncalled \"catastrophic forgetting\" and is deeply rooted in the\nstability-plasticity dilemma. Overcoming catastrophic forgetting in deep neural\nnetworks has become an active field of research in recent years. In particular,\ngradient projection-based methods have recently shown exceptional performance\nat overcoming catastrophic forgetting. This paper proposes two\nbiologically-inspired mechanisms based on sparsity and heterogeneous dropout\nthat significantly increase a continual learner's performance over a long\nsequence of tasks. Our proposed approach builds on the Gradient Projection\nMemory (GPM) framework. We leverage k-winner activations in each layer of a\nneural network to enforce layer-wise sparse activations for each task, together\nwith a between-task heterogeneous dropout that encourages the network to use\nnon-overlapping activation patterns between different tasks. In addition, we\nintroduce two new benchmarks for continual learning under distributional shift,\nnamely Continual Swiss Roll and ImageNet SuperDog-40. Lastly, we provide an\nin-depth analysis of our proposed method and demonstrate a significant\nperformance boost on various benchmark continual learning problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1\">Ali Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nooralinejad_P/0/1/0/all/0/1\">Parsa Nooralinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1\">Soheil Kolouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased Directed Object Attention Graph for Object Navigation. (arXiv:2204.04421v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04421","description":"<p>Object navigation tasks require agents to locate specific objects in unknown\nenvironments based on visual information. Previously, graph convolutions were\nused to implicitly explore the relationships between objects. However, due to\ndifferences in visibility among objects, it is easy to generate biases in\nobject attention. Thus, in this paper, we propose a directed object attention\n(DOA) graph to guide the agent in explicitly learning the attention\nrelationships between objects, thereby reducing the object attention bias. In\nparticular, we use the DOA graph to perform unbiased adaptive object attention\n(UAOA) on the object features and unbiased adaptive image attention (UAIA) on\nthe raw images, respectively. To distinguish features in different branches, a\nconcise adaptive branch energy distribution (ABED) method is proposed. We\nassess our methods on the AI2-Thor dataset. Compared with the state-of-the-art\n(SOTA) method, our method reports 7.4%, 8.1% and 17.6% increase in success rate\n(SR), success weighted by path length (SPL) and success weighted by action\nefficiency (SAE), respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_R/0/1/0/all/0/1\">Ronghao Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhuofan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liuyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zongtao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengju Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satellite-based high-resolution maps of cocoa for C\\^ote d'Ivoire and Ghana. (arXiv:2206.06119v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06119","description":"<p>C\\^ote d'Ivoire and Ghana, the world's largest producers of cocoa, account\nfor two thirds of the global cocoa production. In both countries, cocoa is the\nprimary perennial crop, providing income to almost two million farmers. Yet\nprecise maps of cocoa planted area are missing, hindering accurate\nquantification of expansion in protected areas, production and yields, and\nlimiting information available for improved sustainability governance. Here, we\ncombine cocoa plantation data with publicly available satellite imagery in a\ndeep learning framework and create high-resolution maps of cocoa plantations\nfor both countries, validated in situ. Our results suggest that cocoa\ncultivation is an underlying driver of over 37% and 13% of forest loss in\nprotected areas in C\\^ote d'Ivoire and Ghana, respectively, and that official\nreports substantially underestimate the planted area, up to 40% in Ghana. These\nmaps serve as a crucial building block to advance understanding of conservation\nand economic development in cocoa producing regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalischek_N/0/1/0/all/0/1\">Nikolai Kalischek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Nico Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renier_C/0/1/0/all/0/1\">C&#xe9;cile Renier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1\">Rodrigo Caye Daudt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Addoah_T/0/1/0/all/0/1\">Thomas Addoah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_W/0/1/0/all/0/1\">William Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaser_Hart_W/0/1/0/all/0/1\">Wilma J. Blaser-Hart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_R/0/1/0/all/0/1\">Rachael Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1\">Jan D. Wegner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many Events do You Need? Event-based Visual Place Recognition Using Sparse But Varying Pixels. (arXiv:2206.13673v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13673","description":"<p>Event cameras continue to attract interest due to desirable characteristics\nsuch as high dynamic range, low latency, virtually no motion blur, and high\nenergy efficiency. One of the potential applications that would benefit from\nthese characteristics lies in visual place recognition for robot localization,\ni.e. matching a query observation to the corresponding reference place in the\ndatabase. In this letter, we explore the distinctiveness of event streams from\na small subset of pixels (in the tens or hundreds). We demonstrate that the\nabsolute difference in the number of events at those pixel locations\naccumulated into event frames can be sufficient for the place recognition task,\nwhen pixels that display large variations in the reference set are used. Using\nsuch sparse (over image coordinates) but varying (variance over the number of\nevents per pixel location) pixels enables frequent and computationally cheap\nupdates of the location estimates. Furthermore, when event frames contain a\nconstant number of events, our method takes full advantage of the event-driven\nnature of the sensory stream and displays promising robustness to changes in\nvelocity. We evaluate our proposed approach on the Brisbane-Event-VPR dataset\nin an outdoor driving scenario, as well as the newly contributed indoor\nQCR-Event-VPR dataset that was captured with a DAVIS346 camera mounted on a\nmobile robotic platform. Our results show that our approach achieves\ncompetitive performance when compared to several baseline methods on those\ndatasets, and is particularly well suited for compute- and energy-constrained\nplatforms such as interplanetary rovers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis. (arXiv:2206.15328v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15328","description":"<p>The human annotations are imperfect, especially when produced by junior\npractitioners. Multi-expert consensus is usually regarded as golden standard,\nwhile this annotation protocol is too expensive to implement in many real-world\nprojects. In this study, we propose a method to refine human annotation, named\nNeural Annotation Refinement (NeAR). It is based on a learnable implicit\nfunction, which decodes a latent vector into represented shape. By integrating\nthe appearance as an input of implicit functions, the appearance-aware NeAR\nfixes the annotation artefacts. Our method is demonstrated on the application\nof adrenal gland analysis. We first show that the NeAR can repair distorted\ngolden standards on a public adrenal gland segmentation dataset. Besides, we\ndevelop a new Adrenal gLand ANalysis (ALAN) dataset with the proposed NeAR,\nwhere each case consists of a 3D shape of adrenal gland and its diagnosis label\n(normal vs. abnormal) assigned by experts. We show that models trained on the\nshapes repaired by the NeAR can diagnose adrenal glands better than the\noriginal ones. The ALAN dataset will be open-source, with 1,584 shapes for\nadrenal gland diagnosis, which serves as a new benchmark for medical shape\nanalysis. Code and dataset are available at https://github.com/M3DV/NeAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Rui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1\">Udaranga Wickramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qikui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Detection Using Transfer Learning Approach from Computed Tomography Images. (arXiv:2207.00259v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.00259","description":"<p>Our main goal in this study is to propose a transfer learning based method\nfor COVID-19 detection from Computed Tomography (CT) images. The transfer\nlearning model used for the task is a pretrained Xception model. Both model\narchitecture and pre-trained weights on ImageNet were used. The resulting\nmodified model was trained with 128 batch size and 224x224, 3 channeled input\nimages, converted from original 512x512, grayscale images. The dataset used is\na the COV19-CT-DB. Labels in the dataset include COVID-19 cases and\nNon-COVID-19 cases for COVID-1919 detection. Firstly, a accuracy and loss on\nthe validation partition of the dataset as well as precision recall and macro\nF1 score were used to measure the performance of the proposed method. The\nresulting Macro F1 score on the validation set exceeded the baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1\">Kenan Morani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balikci_M/0/1/0/all/0/1\">Muhammet Fatih Balikci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Altuntas_T/0/1/0/all/0/1\">Tayfun Yigit Altuntas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unay_D/0/1/0/all/0/1\">Devrim Unay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval. (arXiv:2207.00733v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00733","description":"<p>Recently, the cross-modal pre-training task has been a hotspot because of its\nwide application in various down-streaming researches including retrieval,\ncaptioning, question answering and so on. However, exiting methods adopt a\none-stream pre-training model to explore the united vision-language\nrepresentation for conducting cross-modal retrieval, which easily suffer from\nthe calculation explosion. Moreover, although the conventional double-stream\nstructures are quite efficient, they still lack the vital cross-modal\ninteractions, resulting in low performances. Motivated by these challenges, we\nput forward a Contrastive Cross-Modal Knowledge Sharing Pre-training (COOKIE)\nto grasp the joint text-image representations. Structurally, COOKIE adopts the\ntraditional double-stream structure because of the acceptable time consumption.\nTo overcome the inherent defects of double-stream structure as mentioned above,\nwe elaborately design two effective modules. Concretely, the first module is a\nweight-sharing transformer that builds on the head of the visual and textual\nencoders, aiming to semantically align text and image. This design enables\nvisual and textual paths focus on the same semantics. The other one is three\nspecially designed contrastive learning, aiming to share knowledge between\ndifferent models. The shared cross-modal knowledge develops the study of\nunimodal representation greatly, promoting the single-modal retrieval tasks.\nExtensive experimental results on multi-modal matching researches that includes\ncross-modal retrieval, text matching, and image retrieval reveal the superiors\nin calculation efficiency and statistical indicators of our pre-training model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_K/0/1/0/all/0/1\">Keyu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhenshan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qingrong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaodong Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Game State Learning via Game Scene Augmentation. (arXiv:2207.01289v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01289","description":"<p>Having access to accurate game state information is of utmost importance for\nany artificial intelligence task including game-playing, testing, player\nmodeling, and procedural content generation. Self-Supervised Learning (SSL)\ntechniques have shown to be capable of inferring accurate game state\ninformation from the high-dimensional pixel input of game footage into\ncompressed latent representations. Contrastive Learning is a popular SSL\nparadigm where the visual understanding of the game's images comes from\ncontrasting dissimilar and similar game states defined by simple image\naugmentation methods. In this study, we introduce a new game scene augmentation\ntechnique -- named GameCLR -- that takes advantage of the game-engine to define\nand synthesize specific, highly-controlled renderings of different game states,\nthereby, boosting contrastive learning performance. We test our GameCLR\ntechnique on images of the CARLA driving simulator environment and compare it\nagainst the popular SimCLR baseline SSL method. Our results suggest that\nGameCLR can infer the game's state information from game footage more\naccurately compared to the baseline. Our proposed approach allows us to conduct\ngame artificial intelligence research by directly utilizing screen pixels as\ninput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_C/0/1/0/all/0/1\">Chintan Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makantasis_K/0/1/0/all/0/1\">Konstantinos Makantasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liapis_A/0/1/0/all/0/1\">Antonios Liapis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yannakakis_G/0/1/0/all/0/1\">Georgios N. Yannakakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans. (arXiv:2207.01579v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.01579","description":"<p>Computed tomography (CT) imaging could be very practical for diagnosing\nvarious diseases. However, the nature of the CT images is even more diverse\nsince the resolution and number of the slices of a CT scan are determined by\nthe machine and its settings. Conventional deep learning models are hard to\ntickle such diverse data since the essential requirement of the deep neural\nnetwork is the consistent shape of the input data. In this paper, we propose a\nnovel, effective, two-step-wise approach to tickle this issue for COVID-19\nsymptom classification thoroughly. First, the semantic feature embedding of\neach slice for a CT scan is extracted by conventional backbone networks. Then,\nwe proposed a long short-term memory (LSTM) and Transformer-based sub-network\nto deal with temporal feature learning, leading to spatiotemporal feature\nrepresentation learning. In this fashion, the proposed two-step LSTM model\ncould prevent overfitting, as well as increase performance. Comprehensive\nexperiments reveal that the proposed two-step method not only shows excellent\nperformance but also could be compensated for each other. More specifically,\nthe two-step LSTM model has a lower false-negative rate, while the 2-step Swin\nmodel has a lower false-positive rate. In summary, it is suggested that the\nmodel ensemble could be adopted for more stable and promising performance in\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hsu_C/0/1/0/all/0/1\">Chih-Chung Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsai_C/0/1/0/all/0/1\">Chi-Han Tsai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Guan-Lin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Sin-Di Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tai_S/0/1/0/all/0/1\">Shen-Chieh Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning. (arXiv:2207.01798v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01798","description":"<p>Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the\nseen and unseen classes by transferring semantic knowledge from seen to unseen\nclasses. It is a promising solution to take the advantage of generative models\nto hallucinate realistic unseen samples based on the knowledge learned from the\nseen classes. However, due to the generation shifts, the synthesized samples by\nmost existing methods may drift from the real distribution of the unseen data.\nTo address this issue, we propose a novel flow-based generative framework that\nconsists of multiple conditional affine coupling layers for learning unseen\ndata generation. Specifically, we discover and address three potential problems\nthat trigger the generation shifts, i.e., semantic inconsistency, variance\ncollapse, and structure disorder. First, to enhance the reflection of the\nsemantic information in the generated samples, we explicitly embed the semantic\ninformation into the transformation in each conditional affine coupling layer.\nSecond, to recover the intrinsic variance of the real unseen features, we\nintroduce a boundary sample mining strategy with entropy maximization to\ndiscover more difficult visual variants of semantic prototypes and hereby\nadjust the decision boundary of the classifiers. Third, a relative positioning\nstrategy is proposed to revise the attribute embeddings, guiding them to fully\npreserve the inter-class geometric structure and further avoid structure\ndisorder in the semantic space. Extensive experimental results on four GZSL\nbenchmark datasets demonstrate that GSMFlow achieves the state-of-the-art\nperformance on GZSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers. (arXiv:2207.02255v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02255","description":"<p>We present OSFormer, the first one-stage transformer framework for\ncamouflaged instance segmentation (CIS). OSFormer is based on two key designs.\nFirst, we design a location-sensing transformer (LST) to obtain the location\nlabel and instance-aware parameters by introducing the location-guided queries\nand the blend-convolution feedforward network. Second, we develop a\ncoarse-to-fine fusion (CFF) to merge diverse context information from the LST\nencoder and CNN backbone. Coupling these two components enables OSFormer to\nefficiently blend local features and long-range context dependencies for\npredicting camouflaged instances. Compared with two-stage frameworks, our\nOSFormer reaches 41% AP and achieves good convergence efficiency without\nrequiring enormous training data, i.e., only 3,040 samples under 60 epochs.\nCode link: https://github.com/PJLallen/OSFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jialun Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">He Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuanbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.02327","description":"<p>Diffusion MRI tractography is an advanced imaging technique for quantitative\nmapping of the brain's structural connectivity. Whole brain tractography (WBT)\ndata contains over hundreds of thousands of individual fiber streamlines\n(estimated brain connections), and this data is usually parcellated to create\ncompact representations for data analysis applications such as disease\nclassification. In this paper, we propose a novel parcellation-free WBT\nanalysis framework, TractoFormer, that leverages tractography information at\nthe level of individual fiber streamlines and provides a natural mechanism for\ninterpretation of results using the attention mechanism of transformers.\nTractoFormer includes two main contributions. First, we propose a novel and\nsimple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber\nspatial relationships and any feature of interest that can be computed from\nindividual fibers (such as FA or MD). Second, we design a network based on\nvision transformers (ViTs) that includes: 1) data augmentation to overcome\nmodel overfitting on small datasets, 2) identification of discriminative fibers\nfor interpretation of results, and 3) ensemble learning to leverage fiber\ninformation from different brain regions. In a synthetic data experiment,\nTractoFormer successfully identifies discriminative fibers with simulated group\ndifferences. In a disease classification experiment comparing several methods,\nTractoFormer achieves the highest accuracy in classifying schizophrenia vs\ncontrol. Discriminative fibers are identified in left hemispheric frontal and\nparietal superficial white matter regions, which have previously been shown to\nbe affected in schizophrenia patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_T/0/1/0/all/0/1\">Tengfei Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Westin_C/0/1/0/all/0/1\">Carl-Fredrik Westin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty of Atmospheric Motion Vectors by Sampling Tempered Posterior Distributions. (arXiv:2207.03182v2 [stat.ME] UPDATED)","link":"http://arxiv.org/abs/2207.03182","description":"<p>Atmospheric motion vectors (AMVs) extracted from satellite imagery are the\nonly wind observations with good global coverage. They are important features\nfor feeding numerical weather prediction (NWP) models. Several Bayesian models\nhave been proposed to estimate AMVs. Although critical for correct assimilation\ninto NWP models, very few methods provide a thorough characterization of the\nestimation errors. The difficulty of estimating errors stems from the\nspecificity of the posterior distribution, which is both very high dimensional,\nand highly ill-conditioned due to a singular likelihood, which becomes critical\nin particular in the case of missing data (unobserved pixels). This work\nstudies the evaluation of the expected error of AMVs using gradient-based\nMarkov Chain Monte Carlo (MCMC) algorithms. Our main contribution is to propose\na tempering strategy, which amounts to sampling a local approximation of the\njoint posterior distribution of AMVs and image variables in the neighborhood of\na point estimate. In addition, we provide efficient preconditioning with the\ncovariance related to the prior family itself (fractional Brownian motion),\nwith possibly different hyper-parameters. From a theoretical point of view, we\nshow that under regularity assumptions, the family of tempered posterior\ndistributions converges in distribution as temperature decreases to an\n{optimal} Gaussian approximation at a point estimate given by the Maximum A\nPosteriori (MAP) log-density. From an empirical perspective, we evaluate the\nproposed approach based on some quantitative Bayesian evaluation criteria. Our\nnumerical simulations performed on synthetic and real meteorological data\nreveal a significant gain in terms of accuracy of the AMV point estimates and\nof their associated expected error estimates, but also a substantial\nacceleration in the convergence speed of the MCMC algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Heas_P/0/1/0/all/0/1\">Patrick H&#xe9;as</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cerou_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric C&#xe9;rou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rousset_M/0/1/0/all/0/1\">Mathias Rousset</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralGrasps: Learning Implicit Representations for Grasps of Multiple Robotic Hands. (arXiv:2207.02959v1 [cs.RO] CROSS LISTED)","link":"http://arxiv.org/abs/2207.02959","description":"<p>We introduce a neural implicit representation for grasps of objects from\nmultiple robotic hands. Different grasps across multiple robotic hands are\nencoded into a shared latent space. Each latent vector is learned to decode to\nthe 3D shape of an object and the 3D shape of a robotic hand in a grasping pose\nin terms of the signed distance functions of the two 3D shapes. In addition,\nthe distance metric in the latent space is learned to preserve the similarity\nbetween grasps across different robotic hands, where the similarity of grasps\nis defined according to contact regions of the robotic hands. This property\nenables our method to transfer grasps between different grippers including a\nhuman hand, and grasp transfer has the potential to share grasping skills\nbetween robots and enable robots to learn grasping skills from humans.\nFurthermore, the encoded signed distance functions of objects and grasps in our\nimplicit representation can be used for 6D object pose estimation with grasping\ncontact optimization from partial point clouds, which enables robotic grasping\nin the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khargonkar_N/0/1/0/all/0/1\">Ninad Khargonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_N/0/1/0/all/0/1\">Neil Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zesheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_B/0/1/0/all/0/1\">Balakrishnan Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}