{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding. (arXiv:2207.09514v1 [eess.AS])","link":"http://arxiv.org/abs/2207.09514","description":"<p>This paper presents recent progress on integrating speech separation and\nenhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE\nwork, numerous features have been added, including recent state-of-the-art\nspeech enhancement models with their respective training and evaluation\nrecipes. Importantly, a new interface has been designed to flexibly combine\nspeech enhancement front-ends with other tasks, including automatic speech\nrecognition (ASR), speech translation (ST), and spoken language understanding\n(SLU). To showcase such integration, we performed experiments on carefully\ndesigned synthetic datasets for noisy-reverberant multi-channel ST and SLU\ntasks, which can be used as benchmark corpora for future research. In addition\nto these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and\nsingle-channel SE approaches. Results show that the integration of SE\nfront-ends with back-end tasks is a promising research direction even for tasks\nbesides ASR, especially in the multi-channel scenario. The code is available\nonline at https://github.com/ESPnet/ESPnet. The multi-channel ST and SLU\ndatasets, which are another contribution of this work, are released on\nHuggingFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yen-Ju Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chenda Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wangyou Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cornell_S/0/1/0/all/0/1\">Samuele Cornell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_Z/0/1/0/all/0/1\">Zhaoheng Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masuyama_Y/0/1/0/all/0/1\">Yoshiki Masuyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scheibler_R/0/1/0/all/0/1\">Robin Scheibler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhong-Qiu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yanmin Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification. (arXiv:2207.09519v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09519","description":"<p>Contrastive Vision-Language Pre-training, known as CLIP, has provided a new\nparadigm for learning visual representations using large-scale image-text\npairs. It shows impressive performance on downstream tasks by zero-shot\nknowledge transfer. To further enhance CLIP's adaption capability, existing\nmethods proposed to fine-tune additional learnable modules, which significantly\nimproves the few-shot performance but introduces extra training time and\ncomputational resources. In this paper, we propose a training-free adaption\nmethod for CLIP to conduct few-shot classification, termed as Tip-Adapter,\nwhich not only inherits the training-free advantage of zero-shot CLIP but also\nperforms comparably to those training-required approaches. Tip-Adapter\nconstructs the adapter via a key-value cache model from the few-shot training\nset, and updates the prior knowledge encoded in CLIP by feature retrieval. On\ntop of that, the performance of Tip-Adapter can be further boosted to be\nstate-of-the-art on ImageNet by fine-tuning the cache model for 10$\\times$\nfewer epochs than existing methods, which is both effective and efficient. We\nconduct extensive experiments of few-shot classification on 11 datasets to\ndemonstrate the superiority of our proposed methods. Code is released at\nhttps://github.com/gaopengcuhk/Tip-Adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rongyao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuoteKG: A Multilingual Knowledge Graph of Quotes. (arXiv:2207.09562v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09562","description":"<p>Quotes of public figures can mark turning points in history. A quote can\nexplain its originator's actions, foreshadowing political or personal decisions\nand revealing character traits. Impactful quotes cross language barriers and\ninfluence the general population's reaction to specific stances, always facing\nthe risk of being misattributed or taken out of context. The provision of a\ncross-lingual knowledge graph of quotes that establishes the authenticity of\nquotes and their contexts is of great importance to allow the exploration of\nthe lives of important people as well as topics from the perspective of what\nwas actually said. In this paper, we present QuoteKG, the first multilingual\nknowledge graph of quotes. We propose the QuoteKG creation pipeline that\nextracts quotes from Wikiquote, a free and collaboratively created collection\nof quotes in many languages, and aligns different mentions of the same quote.\nQuoteKG includes nearly one million quotes in $55$ languages, said by more than\n$69,000$ people of public interest across a wide range of topics. QuoteKG is\npublicly available and can be accessed via a SPARQL endpoint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuculo_T/0/1/0/all/0/1\">Tin Kuculo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_S/0/1/0/all/0/1\">Simon Gottschalk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidova_E/0/1/0/all/0/1\">Elena Demidova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-guided Collaborative Problem Solving: A Natural Language based Framework. (arXiv:2207.09566v1 [cs.HC])","link":"http://arxiv.org/abs/2207.09566","description":"<p>We consider the problem of human-machine collaborative problem solving as a\nplanning task coupled with natural language communication. Our framework\nconsists of three components -- a natural language engine that parses the\nlanguage utterances to a formal representation and vice-versa, a concept\nlearner that induces generalized concepts for plans based on limited\ninteractions with the user, and an HTN planner that solves the task based on\nhuman interaction. We illustrate the ability of this framework to address the\nkey challenges of collaborative problem solving by demonstrating it on a\ncollaborative building task in a Minecraft-based blocksworld domain. The\naccompanied demo video is available at https://youtu.be/q1pWe4aahF0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kokel_H/0/1/0/all/0/1\">Harsha Kokel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mayukh Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1\">Rakibul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonn_J/0/1/0/all/0/1\">Julia Bonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jon Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Soham Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayannavar_P/0/1/0/all/0/1\">Prashant Jayannavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doppa_J/0/1/0/all/0/1\">Janardhan Rao Doppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1\">Julia Hockenmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1\">Sriraam Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Martha Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets. (arXiv:2207.09638v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09638","description":"<p>Over-parameterized models, typically pre-trained language models (LMs), have\nshown an appealing expressive power due to their small learning bias. However,\nthe huge learning capacity of LMs can also lead to large learning variance. In\na pilot study, we find that, when faced with multiple domains, a critical\nportion of parameters behave unexpectedly in a domain-specific manner while\nothers behave in a domain-general one. Motivated by this phenomenon, we for the\nfirst time posit that domain-general parameters can underpin a domain-general\nLM that can be derived from the original LM. To uncover the domain-general LM,\nwe propose to identify domain-general parameters by playing lottery tickets\n(dubbed doge tickets). In order to intervene the lottery, we propose a\ndomain-general score, which depicts how domain-invariant a parameter is by\nassociating it with the variance. Comprehensive experiments are conducted on\nthe Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets\nobtains an improved out-of-domain generalization in comparison with a range of\ncompetitive baselines. Analysis results further hint the existence of\ndomain-general parameters and the performance consistency of doge tickets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Linguistic Theory and Neural Language Models. (arXiv:2207.09643v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09643","description":"<p>Transformer-based language models have recently achieved remarkable results\nin many natural language tasks. However, performance on leaderboards is\ngenerally achieved by leveraging massive amounts of training data, and rarely\nby encoding explicit linguistic knowledge into neural models. This has led many\nto question the relevance of linguistics for modern natural language\nprocessing. In this dissertation, I present several case studies to illustrate\nhow theoretical linguistics and neural language models are still relevant to\neach other. First, language models are useful to linguists by providing an\nobjective tool to measure semantic distance, which is difficult to do using\ntraditional methods. On the other hand, linguistic theory contributes to\nlanguage modelling research by providing frameworks and sources of data to\nprobe our language models for specific aspects of language understanding.\n</p>\n<p>This thesis contributes three studies that explore different aspects of the\nsyntax-semantics interface in language models. In the first part of my thesis,\nI apply language models to the problem of word class flexibility. Using mBERT\nas a source of semantic distance measurements, I present evidence in favour of\nanalyzing word class flexibility as a directional process. In the second part\nof my thesis, I propose a method to measure surprisal at intermediate layers of\nlanguage models. My experiments show that sentences containing morphosyntactic\nanomalies trigger surprisals earlier in language models than semantic and\ncommonsense anomalies. Finally, in the third part of my thesis, I adapt several\npsycholinguistic studies to show that language models contain knowledge of\nargument structure constructions. In summary, my thesis develops new\nconnections between natural language processing, linguistic theory, and\npsycholinguistics to provide fresh perspectives for the interpretation of\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bai Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features. (arXiv:2207.09666v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09666","description":"<p>Current state-of-the-art methods for image captioning employ region-based\nfeatures, as they provide object-level information that is essential to\ndescribe the content of images; they are usually extracted by an object\ndetector such as Faster R-CNN. However, they have several issues, such as lack\nof contextual information, the risk of inaccurate detection, and the high\ncomputational cost. The first two could be resolved by additionally using\ngrid-based features. However, how to extract and fuse these two types of\nfeatures is uncharted. This paper proposes a Transformer-only neural\narchitecture, dubbed GRIT (Grid- and Region-based Image captioning\nTransformer), that effectively utilizes the two visual features to generate\nbetter captions. GRIT replaces the CNN-based detector employed in previous\nmethods with a DETR-based one, making it computationally faster. Moreover, its\nmonolithic design consisting only of Transformers enables end-to-end training\nof the model. This innovative design and the integration of the dual visual\nfeatures bring about significant performance improvement. The experimental\nresults on several image captioning benchmarks show that GRIT outperforms\nprevious methods in inference accuracy and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van-Quang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Data Driven Inverse Text Normalization using Data Augmentation. (arXiv:2207.09674v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09674","description":"<p>Inverse text normalization (ITN) is used to convert the spoken form output of\nan automatic speech recognition (ASR) system to a written form. Traditional\nhandcrafted ITN rules can be complex to transcribe and maintain. Meanwhile\nneural modeling approaches require quality large-scale spoken-written pair\nexamples in the same or similar domain as the ASR system (in-domain data), to\ntrain. Both these approaches require costly and complex annotations. In this\npaper, we present a data augmentation technique that effectively generates rich\nspoken-written numeric pairs from out-of-domain textual data with minimal human\nannotation. We empirically demonstrate that ITN model trained using our data\naugmentation technique consistently outperform ITN model trained using only\nin-domain data across all numeric surfaces like cardinal, currency, and\nfraction, by an overall accuracy of 14.44%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_L/0/1/0/all/0/1\">Laxmi Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1\">Debjyoti Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yutong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuedong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_K/0/1/0/all/0/1\">Kjell Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_M/0/1/0/all/0/1\">Mark Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1\">Yatharth Saraf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval. (arXiv:2207.09732v1 [eess.AS])","link":"http://arxiv.org/abs/2207.09732","description":"<p>The amount of audio data available on public websites is growing rapidly, and\nan efficient mechanism for accessing the desired data is necessary. We propose\na content-based audio retrieval method that can retrieve a target audio that is\nsimilar to but slightly different from the query audio by introducing auxiliary\ntextual information which describes the difference between the query and target\naudio. While the range of conventional content-based audio retrieval is limited\nto audio that is similar to the query audio, the proposed method can adjust the\nretrieval range by adding an embedding of the auxiliary text query-modifier to\nthe embedding of the query sample audio in a shared latent space. To evaluate\nour method, we built a dataset comprising two different audio clips and the\ntext that describes the difference. The experimental results show that the\nproposed method retrieves the paired audio more accurately than the baseline.\nWe also confirmed based on visualization that the proposed method obtains the\nshared latent space in which the audio difference and the corresponding text\nare represented as similar embedding vectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Takeuchi_D/0/1/0/all/0/1\">Daiki Takeuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohishi_Y/0/1/0/all/0/1\">Yasunori Ohishi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niizumi_D/0/1/0/all/0/1\">Daisuke Niizumi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harada_N/0/1/0/all/0/1\">Noboru Harada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kashino_K/0/1/0/all/0/1\">Kunio Kashino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09847","description":"<p>For human children as well as machine learning systems, a key challenge in\nlearning a word is linking the word to the visual phenomena it describes. We\nexplore this aspect of word learning by using the performance of computer\nvision systems as a proxy for the difficulty of learning a word from visual\ncues. We show that the age at which children acquire different categories of\nwords is predicted by the performance of visual classification and captioning\nsystems, over and above the expected effects of word frequency. The performance\nof the computer vision systems is related to human judgments of the\nconcreteness of words, supporting the idea that we are capturing the\nrelationship between words and visual phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1\">Sunayana Rane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nencheva_M/0/1/0/all/0/1\">Mira L. Nencheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_Williams_C/0/1/0/all/0/1\">Casey Lew-Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Is TTS Augmentation Through a Pivot Language Useful?. (arXiv:2207.09889v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09889","description":"<p>Developing Automatic Speech Recognition (ASR) for low-resource languages is a\nchallenge due to the small amount of transcribed audio data. For many such\nlanguages, audio and text are available separately, but not audio with\ntranscriptions. Using text, speech can be synthetically produced via\ntext-to-speech (TTS) systems. However, many low-resource languages do not have\nquality TTS systems either. We propose an alternative: produce synthetic audio\nby running text from the target language through a trained TTS system for a\nhigher-resource pivot language. We investigate when and how this technique is\nmost effective in low-resource settings. In our experiments, using several\nthousand synthetic TTS text-speech pairs and duplicating authentic data to\nbalance yields optimal results. Our findings suggest that searching over a set\nof candidate pivot languages can lead to marginal improvements and that,\nsurprisingly, ASR performance can by harmed by increases in measured TTS\nquality. Application of these findings improves ASR by 64.5\\% and 45.0\\%\ncharacter error reduction rate (CERR) respectively for two low-resource\nlanguages: Guaran\\'i and Suba.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_N/0/1/0/all/0/1\">Nathaniel Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogayo_P/0/1/0/all/0/1\">Perez Ogayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangu_S/0/1/0/all/0/1\">Swetha Gangu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFACTOR GNNS: Revisiting Factorisation-based Models from a Message-Passing Perspective. (arXiv:2207.09980v1 [cs.LG])","link":"http://arxiv.org/abs/2207.09980","description":"<p>Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring\nsuccess for Knowledge Graph Completion (KGC) tasks, often outperforming Graph\nNeural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node\nfeatures and to generalise to unseen nodes in inductive settings. Our work\nbridges the gap between FMs and GNNs by proposing REFACTOR GNNS. This new\narchitecture draws upon both modelling paradigms, which previously were largely\nthought of as disjoint. Concretely, using a message-passing formalism, we show\nhow FMs can be cast as GNNs by reformulating the gradient descent procedure as\nmessage-passing operations, which forms the basis of our REFACTOR GNNS. Across\na multitude of well-established KGC benchmarks, our REFACTOR GNNS achieve\ncomparable transductive performance to FMs, and state-of-the-art inductive\nperformance while using an order of magnitude fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pushkar Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franceschi_L/0/1/0/all/0/1\">Luca Franceschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Harmful Online Conversational Content towards LGBTQIA+ Individuals. (arXiv:2207.10032v1 [cs.CL])","link":"http://arxiv.org/abs/2207.10032","description":"<p>Online discussions, panels, talk page edits, etc., often contain harmful\nconversational content i.e., hate speech, death threats and offensive language,\nespecially towards certain demographic groups. For example, individuals who\nidentify as members of the LGBTQIA+ community and/or BIPOC (Black, Indigenous,\nPeople of Color) are at higher risk for abuse and harassment online. In this\nwork, we first introduce a real-world dataset that will enable us to study and\nunderstand harmful online conversational content. Then, we conduct several\nexploratory data analysis experiments to gain deeper insights from the dataset.\nWe later describe our approach for detecting harmful online Anti-LGBTQIA+\nconversational content, and finally, we implement two baseline machine learning\nmodels (i.e., Support Vector Machine and Logistic Regression), and fine-tune 3\npre-trained large language models (BERT, RoBERTa, and HateBERT). Our findings\nverify that large language models can achieve very promising performance on\ndetecting online Anti-LGBTQIA+ conversational content detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dacon_J/0/1/0/all/0/1\">Jamell Dacon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shomer_H/0/1/0/all/0/1\">Harry Shomer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crum_Dacon_S/0/1/0/all/0/1\">Shaylynn Crum-Dacon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transgender Community Sentiment Analysis from Social Media Data: A Natural Language Processing Approach. (arXiv:2010.13062v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.13062","description":"<p>Transgender community is experiencing a huge disparity in mental health\nconditions compared with the general population. Interpreting the social medial\ndata posted by transgender people may help us understand the sentiments of\nthese sexual minority groups better and apply early interventions. In this\nstudy, we manually categorize 300 social media comments posted by transgender\npeople to the sentiment of negative, positive, and neutral. 5 machine learning\nalgorithms and 2 deep neural networks are adopted to build sentiment analysis\nclassifiers based on the annotated data. Results show that our annotations are\nreliable with a high Cohen's Kappa score over 0.8 across all three classes.\nLSTM model yields an optimal performance of accuracy over 0.85 and AUC of\n0.876. Our next step will focus on using advanced natural language processing\nalgorithms on a larger annotated dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yudan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Ying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Textual Adversarial Examples through Randomized Substitution and Vote. (arXiv:2109.05698v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05698","description":"<p>A line of work has shown that natural text processing models are vulnerable\nto adversarial examples. Correspondingly, various defense methods are proposed\nto mitigate the threat of textual adversarial examples, eg, adversarial\ntraining, input transformations, detection, etc. In this work, we treat the\noptimization process for synonym substitution based textual adversarial attacks\nas a specific sequence of word replacement, in which each word mutually\ninfluences other words. We identify that we could destroy such mutual\ninteraction and eliminate the adversarial perturbation by randomly substituting\na word with its synonyms. Based on this observation, we propose a novel textual\nadversarial example detection method, termed Randomized Substitution and Vote\n(RS&amp;V), which votes the prediction label by accumulating the logits of k\nsamples generated by randomly substituting the words in the input text with\nsynonyms. The proposed RS&amp;V is generally applicable to any existing neural\nnetworks without modification on the architecture or extra training, and it is\northogonal to prior work on making the classification network itself more\nrobust. Empirical evaluations on three benchmark datasets demonstrate that our\nRS&amp;V could detect the textual adversarial examples more successfully than the\nexisting detection methods while maintaining the high classification accuracy\non benign samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yifeng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet. (arXiv:2110.08352v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.08352","description":"<p>From wearables to powerful smart devices, modern automatic speech recognition\n(ASR) models run on a variety of edge devices with different computational\nbudgets. To navigate the Pareto front of model accuracy vs model size,\nresearchers are trapped in a dilemma of optimizing model accuracy by training\nand fine-tuning models for each individual edge device while keeping the\ntraining GPU-hours tractable. In this paper, we propose Omni-sparsity DNN,\nwhere a single neural network can be pruned to generate optimized model for a\nlarge range of model sizes. We develop training strategies for Omni-sparsity\nDNN that allows it to find models along the Pareto front of word-error-rate\n(WER) vs model size while keeping the training GPU-hours to no more than that\nof training one singular model. We demonstrate the Omni-sparsity DNN with\nstreaming E2E ASR models. Our results show great saving on training time and\nresources with similar or better accuracy on LibriSpeech compared to\nindividually pruned sparse models: 2%-6.6% better WER on Test-other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_G/0/1/0/all/0/1\">Ganesh Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Topic Transition in Dialogue. (arXiv:2111.14188v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.14188","description":"<p>Transitioning between topics is a natural component of human-human dialog.\nAlthough topic transition has been studied in dialogue for decades, only a\nhandful of corpora based studies have been performed to investigate the\nsubtleties of topic transitions. Thus, this study annotates 215 conversations\nfrom the switchboard corpus and investigates how variables such as length,\nnumber of topic transitions, topic transitions share by participants and\nturns/topic are related. This work presents an empirical study on topic\ntransition in switchboard corpus followed by modelling topic transition with a\nprecision of 83% for in-domain(id) test set and 82% on 10 out-of-domain}(ood)\ntest set. It is envisioned that this work will help in emulating human-human\nlike topic transition in open-domain dialog systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_M/0/1/0/all/0/1\">Mayank Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spillane_B/0/1/0/all/0/1\">Brendan Spillane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilmartin_E/0/1/0/all/0/1\">Emer Gilmartin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saam_C/0/1/0/all/0/1\">Christian Saam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowan_B/0/1/0/all/0/1\">Benjamin R. Cowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_V/0/1/0/all/0/1\">Vincent Wade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A pragmatic account of the weak evidence effect. (arXiv:2112.03799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03799","description":"<p>Language is not only used to inform. We often seek to persuade by arguing in\nfavor of a particular view. Persuasion raises a number of challenges for\nclassical accounts of belief updating, as information cannot be taken at face\nvalue. How should listeners account for a speaker's \"hidden agenda\" when\nincorporating new information? Here, we extend recent probabilistic models of\nrecursive social reasoning to allow for persuasive goals and show that our\nmodel provides a new pragmatic explanation for why weakly favorable arguments\nmay backfire, a phenomenon known as the weak evidence effect. Critically, our\nmodel predicts a relationship between belief updating and speaker expectations:\nweak evidence should only backfire when speakers are expected to act under\npersuasive goals, implying the absence of stronger evidence. We introduce a\nsimple experimental paradigm called the Stick Contest to measure the extent to\nwhich the weak evidence effect depends on speaker expectations, and show that a\npragmatic listener model accounts for the empirical data better than\nalternative models. Our findings suggest potential avenues for rational models\nof social reasoning to further illuminate decision-making phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barnett_S/0/1/0/all/0/1\">Samuel A. Barnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05203","description":"<p>3D dense captioning is a recently-proposed novel task, where point clouds\ncontain more geometric information than the 2D counterpart. However, it is also\nmore challenging due to the higher complexity and wider variety of inter-object\nrelations contained in point clouds. Existing methods only treat such relations\nas by-products of object feature learning in graphs without specifically\nencoding them, which leads to sub-optimal results. In this paper, aiming at\nimproving 3D dense captioning via capturing and utilizing the complex relations\nin the 3D scene, we propose MORE, a Multi-Order RElation mining model, to\nsupport generating more descriptive and comprehensive captions. Technically,\nour MORE encodes object relations in a progressive manner since complex\nrelations can be deduced from a limited number of basic ones. We first devise a\nnovel Spatial Layout Graph Convolution (SLGC), which semantically encodes\nseveral first-order relations as edges of a graph constructed over 3D object\nproposals. Next, from the resulting graph, we further extract multiple triplets\nwhich encapsulate basic first-order relations as the basic unit, and construct\nseveral Object-centric Triplet Attention Graphs (OTAG) to infer multi-order\nrelations for every target object. The updated node features from OTAG are\naggregated and fed into the caption decoder to provide abundant relational\ncues, so that captions including diverse relations with context objects can be\ngenerated. Extensive experiments on the Scan2Cap dataset prove the\neffectiveness of our proposed MORE and its components, and we also outperform\nthe current state-of-the-art method. Our code is available at\nhttps://github.com/SxJyJay/MORE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12886","description":"<p>Preschool evaluation is crucial because it gives teachers and parents\ninfluential knowledge about children's growth and development. The COVID-19\npandemic has highlighted the necessity of online assessment for preschool\nchildren. One of the areas that should be tested is their ability to speak.\nEmploying an Automatic Speech Recognition(ASR) system is useless since they are\npre-trained on voices that are different from children's voices in terms of\nfrequency and amplitude. We constructed an ASR for our cognitive test system to\nsolve this issue using the Wav2Vec 2.0 model with a new pre-training objective\ncalled Random Frequency Pitch(RFP). In addition, we used our new dataset to\nfine-tune our model for Meaningless Words(MW) and Rapid Automatic Naming(RAN)\ntests. Our new approach reaches a Word Error Rate(WER) of 6.45 on the Persian\nsection of the CommonVoice dataset. Furthermore, our novel methodology produces\npositive outcomes in zero- and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech. (arXiv:2203.17190v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.17190","description":"<p>Recently, leveraging BERT pre-training to improve the phoneme encoder in text\nto speech (TTS) has drawn increasing attention. However, the works apply\npre-training with character-based units to enhance the TTS phoneme encoder,\nwhich is inconsistent with the TTS fine-tuning that takes phonemes as input.\nPre-training only with phonemes as input can alleviate the input mismatch but\nlack the ability to model rich representations and semantic information due to\nlimited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a\nnovel variant of the BERT model that uses mixed phoneme and sup-phoneme\nrepresentations to enhance the learning capability. Specifically, we merge the\nadjacent phonemes into sup-phonemes and combine the phoneme sequence and the\nmerged sup-phoneme sequence as the model input, which can enhance the model\ncapacity to learn rich contextual representations. Experiment results\ndemonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS\nperformance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The\nMixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to\nthe previous TTS pre-trained model PnG BERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_D/0/1/0/all/0/1\">Daxin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yuzi Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Tan Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner. (arXiv:2205.09224v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09224","description":"<p>Large language models have achieved high performance on various question\nanswering (QA) benchmarks, but the explainability of their output remains\nelusive. Structured explanations, called entailment trees, were recently\nsuggested as a way to explain and inspect a QA system's answer. In order to\nbetter generate such entailment trees, we propose an architecture called\nIterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a\ngiven hypothesis by systematically generating a step-by-step explanation from\ntextual premises. The IRGR model iteratively searches for suitable premises,\nconstructing a single entailment step at a time. Contrary to previous\napproaches, our method combines generation steps and retrieval of premises,\nallowing the model to leverage intermediate conclusions, and mitigating the\ninput size limit of baseline encoder-decoder models. We conduct experiments\nusing the EntailmentBank dataset, where we outperform existing benchmarks on\npremise retrieval and entailment tree generation, with around 300% gain in\noverall correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_D/0/1/0/all/0/1\">Danilo Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Rui Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaokai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henry Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinchi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELECTRA is a Zero-Shot Learner, Too. (arXiv:2207.08141v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.08141","description":"<p>Recently, for few-shot or even zero-shot learning, the new paradigm\n\"pre-train, prompt, and predict\" has achieved remarkable achievements compared\nwith the \"pre-train, fine-tune\" paradigm. After the success of prompt-based\nGPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)\nprompt learning methods became popular and widely used. However, another\nefficient pre-trained discriminative model, ELECTRA, has probably been\nneglected. In this paper, we attempt to accomplish several NLP tasks in the\nzero-shot scenario using a novel our proposed replaced token detection\n(RTD)-based prompt learning method. Experimental results show that ELECTRA\nmodel based on RTD-prompt learning achieves surprisingly state-of-the-art\nzero-shot performance. Numerically, compared to MLM-RoBERTa-large and\nMLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%\nimprovement on all 15 tasks. Especially on the SST-2 task, our\nRTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training\ndata. Overall, compared to the pre-trained masked language models, the\npre-trained replaced token detection model performs better in zero-shot\nlearning. The source code is available at:\nhttps://github.com/nishiwen1214/RTD-ELECTRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shiwen Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_H/0/1/0/all/0/1\">Hung-Yu Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search. (arXiv:2207.09068v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.09068","description":"<p>Since BERT (Devlin et al., 2018), learning contextualized word embeddings has\nbeen a de-facto standard in NLP. However, the progress of learning\ncontextualized phrase embeddings is hindered by the lack of a human-annotated,\nphrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of\n~28K of noun phrases accompanied by their contextual Wikipedia pages and a\nsuite of three tasks of increasing difficulty for evaluating the quality of\nphrase embeddings. We find that training on our dataset improves ranking\nmodels' accuracy and remarkably pushes Question Answering (QA) models to\nnear-human accuracy which is 95% Exact Match (EM) on semantic search given a\nquery phrase and a passage. Interestingly, we find evidence that such\nimpressive performance is because the QA models learn to better capture the\ncommon meaning of a phrase regardless of its actual context. That is, on our\nPhrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially\n(60% EM), failing to differentiate between two different senses of the same\nphrase under two different contexts. Further results on our 3-task PiC\nbenchmark reveal that learning contextualized phrase embeddings remains an\ninteresting, open challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thang M. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Comparison of automatic prostate zones segmentation models in MRI images using U-net-like architectures. (arXiv:2207.09483v1 [eess.IV])","link":"http://arxiv.org/abs/2207.09483","description":"<p>Prostate cancer is the second-most frequently diagnosed cancer and the sixth\nleading cause of cancer death in males worldwide. The main problem that\nspecialists face during the diagnosis of prostate cancer is the localization of\nRegions of Interest (ROI) containing a tumor tissue. Currently, the\nsegmentation of this ROI in most cases is carried out manually by expert\ndoctors, but the procedure is plagued with low detection rates (of about\n27-44%) or overdiagnosis in some patients. Therefore, several research works\nhave tackled the challenge of automatically segmenting and extracting features\nof the ROI from magnetic resonance images, as this process can greatly\nfacilitate many diagnostic and therapeutic applications. However, the lack of\nclear prostate boundaries, the heterogeneity inherent to the prostate tissue,\nand the variety of prostate shapes makes this process very difficult to\nautomate.In this work, six deep learning models were trained and analyzed with\na dataset of MRI images obtained from the Centre Hospitalaire de Dijon and\nUniversitat Politecnica de Catalunya. We carried out a comparison of multiple\ndeep learning models (i.e. U-Net, Attention U-Net, Dense-UNet, Attention\nDense-UNet, R2U-Net, and Attention R2U-Net) using categorical cross-entropy\nloss function. The analysis was performed using three metrics commonly used for\nimage segmentation: Dice score, Jaccard index, and mean squared error. The\nmodel that give us the best result segmenting all the zones was R2U-Net, which\nachieved 0.869, 0.782, and 0.00013 for Dice, Jaccard and mean squared error,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Quihui_Rubio_P/0/1/0/all/0/1\">Pablo Cesar Quihui-Rubio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1\">Miguel Gonzalez-Mendoza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_Hernandez_G/0/1/0/all/0/1\">Gerardo Rodriguez-Hernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mata_C/0/1/0/all/0/1\">Christian Mata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Analysis of Visual Product Reviews. (arXiv:2207.09499v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09499","description":"<p>With the proliferation of the e-commerce industry, analyzing customer\nfeedback is becoming indispensable to a service provider. In recent days, it\ncan be noticed that customers upload the purchased product images with their\nreview scores. In this paper, we undertake the task of analyzing such visual\nreviews, which is very new of its kind. In the past, the researchers worked on\nanalyzing language feedback, but here we do not take any assistance from\nlinguistic reviews that may be absent, since a recent trend can be observed\nwhere customers prefer to quickly upload the visual feedback instead of typing\nlanguage feedback. We propose a hierarchical architecture, where the\nhigher-level model engages in product categorization, and the lower-level model\npays attention to predicting the review score from a customer-provided product\nimage. We generated a database by procuring real visual product reviews, which\nwas quite challenging. Our architecture obtained some promising results by\nperforming extensive experiments on the employed database. The proposed\nhierarchical architecture attained a 57.48% performance improvement over the\nsingle-level best comparable architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adak_C/0/1/0/all/0/1\">Chandranath Adak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1\">Soumi Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saqib_M/0/1/0/all/0/1\">Muhammad Saqib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariant Feature Learning for Generalized Long-Tailed Classification. (arXiv:2207.09504v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09504","description":"<p>Existing long-tailed classification (LT) methods only focus on tackling the\nclass-wise imbalance that head classes have more samples than tail classes, but\noverlook the attribute-wise imbalance. In fact, even if the class is balanced,\nsamples within each class may still be long-tailed due to the varying\nattributes. Note that the latter is fundamentally more ubiquitous and\nchallenging than the former because attributes are not just implicit for most\ndatasets, but also combinatorially complex, thus prohibitively expensive to be\nbalanced. Therefore, we introduce a novel research problem: Generalized\nLong-Tailed classification (GLT), to jointly consider both kinds of imbalances.\nBy \"generalized\", we mean that a GLT method should naturally solve the\ntraditional LT, but not vice versa. Not surprisingly, we find that most\nclass-wise LT methods degenerate in our proposed two benchmarks: ImageNet-GLT\nand MSCOCO-GLT. We argue that it is because they over-emphasize the adjustment\nof class distribution while neglecting to learn attribute-invariant features.\nTo this end, we propose an Invariant Feature Learning (IFL) method as the first\nstrong baseline for GLT. IFL first discovers environments with divergent\nintra-class distributions from the imperfect predictions and then learns\ninvariant features across them. Promisingly, as an improved feature backbone,\nIFL boosts all the LT line-up: one/two-stage re-balance, augmentation, and\nensemble. Codes and benchmarks are available on Github:\nhttps://github.com/KaihuaTang/Generalized-Long-Tailed-Benchmarks.pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kaihua Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Mingyuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiaxin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Method for Face Quality Assessment on the Edge. (arXiv:2207.09505v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09505","description":"<p>Face recognition applications in practice are composed of two main steps:\nface detection and feature extraction. In a sole vision-based solution, the\nfirst step generates multiple detection for a single identity by ingesting a\ncamera stream. A practical approach on edge devices should prioritize these\ndetection of identities according to their conformity to recognition. In this\nperspective, we propose a face quality score regression by just appending a\nsingle layer to a face landmark detection network. With almost no additional\ncost, face quality scores are obtained by training this single layer to regress\nrecognition scores with surveillance like augmentations. We implemented the\nproposed approach on edge GPUs with all face detection pipeline steps,\nincluding detection, tracking, and alignment. Comprehensive experiments show\nthe proposed approach's efficiency through comparison with SOTA face quality\nregression models on different data sets and real-life scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okcu_S/0/1/0/all/0/1\">Sefa Burak Okcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozkalayci_B/0/1/0/all/0/1\">Burak O&#x11f;uz &#xd6;zkalayc&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cigla_C/0/1/0/all/0/1\">Cevahir &#xc7;&#x131;&#x11f;la</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeasoNet: A Seasonal Scene Classification, segmentation and Retrieval dataset for satellite Imagery over Germany. (arXiv:2207.09507v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09507","description":"<p>This work presents SeasoNet, a new large-scale multi-label land cover and\nland use scene understanding dataset. It includes $1\\,759\\,830$ images from\nSentinel-2 tiles, with 12 spectral bands and patch sizes of up to $ 120 \\\n\\mathrm{px} \\times 120 \\ \\mathrm{px}$. Each image is annotated with large scale\npixel level labels from the German land cover model LBM-DE2018 with land cover\nclasses based on the CORINE Land Cover database (CLC) 2018 and a five times\nsmaller minimum mapping unit (MMU) than the original CLC maps. We provide pixel\nsynchronous examples from all four seasons, plus an additional snowy set. These\nproperties make SeasoNet the currently most versatile and biggest remote\nsensing scene understanding dataset with possible applications ranging from\nscene classification over land cover mapping to content-based cross season\nimage retrieval and self-supervised feature learning. We provide baseline\nresults by evaluating state-of-the-art deep networks on the new dataset in\nscene classification and semantic segmentation scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kossmann_D/0/1/0/all/0/1\">Dominik Ko&#xdf;mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brack_V/0/1/0/all/0/1\">Viktor Brack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilhelm_T/0/1/0/all/0/1\">Thorsten Wilhelm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images. (arXiv:2207.09508v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09508","description":"<p>In this paper, we present the results of the HSE-NN team in the 4th\ncompetition on Affective Behavior Analysis in-the-wild (ABAW). The novel\nmulti-task EfficientNet model is trained for simultaneous recognition of facial\nexpressions and prediction of valence and arousal on static photos. The\nresulting MT-EmotiEffNet extracts visual features that are fed into simple\nfeed-forward neural networks in the multi-task learning challenge. We obtain\nperformance measure 1.3 on the validation set, which is significantly greater\nwhen compared to either performance of baseline (0.3) or existing models that\nare trained only on the s-Aff-Wild2 database. In the learning from synthetic\ndata challenge, the quality of the original synthetic training set is increased\nby using the super-resolution techniques, such as Real-ESRGAN. Next, the\nMT-EmotiEffNet is fine-tuned on the new training set. The final prediction is a\nsimple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our\naverage validation F1 score is 18% greater than the baseline convolutional\nneural network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savchenko_A/0/1/0/all/0/1\">Andrey V. Savchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contributions of Shape, Texture, and Color in Visual Recognition. (arXiv:2207.09510v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09510","description":"<p>We investigate the contributions of three important features of the human\nvisual system (HVS)~ -- ~shape, texture, and color ~ -- ~to object\nclassification. We build a humanoid vision engine (HVE) that explicitly and\nseparately computes shape, texture, and color features from images. The\nresulting feature vectors are then concatenated to support the final\nclassification. We show that HVE can summarize and rank-order the contributions\nof the three features to object recognition. We use human experiments to\nconfirm that both HVE and humans predominantly use some specific features to\nsupport the classification of specific classes (e.g., texture is the dominant\nfeature to distinguish a zebra from other quadrupeds, both for humans and HVE).\nWith the help of HVE, given any environment (dataset), we can summarize the\nmost important features for the whole task (task-specific; e.g., color is the\nmost important feature overall for classification with the CUB dataset), and\nfor each class (class-specific; e.g., shape is the most important feature to\nrecognize boats in the iLab-20M dataset). To demonstrate more usefulness of\nHVE, we use it to simulate the open-world zero-shot learning ability of humans\nwith no attribute labeling. Finally, we show that HVE can also simulate human\nimagination ability with the combination of different features. We will\nopen-source the HVE engine and corresponding datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yunhao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1\">Laurent Itti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification. (arXiv:2207.09519v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09519","description":"<p>Contrastive Vision-Language Pre-training, known as CLIP, has provided a new\nparadigm for learning visual representations using large-scale image-text\npairs. It shows impressive performance on downstream tasks by zero-shot\nknowledge transfer. To further enhance CLIP's adaption capability, existing\nmethods proposed to fine-tune additional learnable modules, which significantly\nimproves the few-shot performance but introduces extra training time and\ncomputational resources. In this paper, we propose a training-free adaption\nmethod for CLIP to conduct few-shot classification, termed as Tip-Adapter,\nwhich not only inherits the training-free advantage of zero-shot CLIP but also\nperforms comparably to those training-required approaches. Tip-Adapter\nconstructs the adapter via a key-value cache model from the few-shot training\nset, and updates the prior knowledge encoded in CLIP by feature retrieval. On\ntop of that, the performance of Tip-Adapter can be further boosted to be\nstate-of-the-art on ImageNet by fine-tuning the cache model for 10$\\times$\nfewer epochs than existing methods, which is both effective and efficient. We\nconduct extensive experiments of few-shot classification on 11 datasets to\ndemonstrate the superiority of our proposed methods. Code is released at\nhttps://github.com/gaopengcuhk/Tip-Adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rongyao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dice loss in the context of missing or empty labels: Introducing $\\Phi$ and $\\epsilon$. (arXiv:2207.09521v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09521","description":"<p>Albeit the Dice loss is one of the dominant loss functions in medical image\nsegmentation, most research omits a closer look at its derivative, i.e. the\nreal motor of the optimization when using gradient descent. In this paper, we\nhighlight the peculiar action of the Dice loss in the presence of missing or\nempty labels. First, we formulate a theoretical basis that gives a general\ndescription of the Dice loss and its derivative. It turns out that the choice\nof the reduction dimensions $\\Phi$ and the smoothing term $\\epsilon$ is\nnon-trivial and greatly influences its behavior. We find and propose heuristic\ncombinations of $\\Phi$ and $\\epsilon$ that work in a segmentation setting with\neither missing or empty labels. Second, we empirically validate these findings\nin a binary and multiclass segmentation setting using two publicly available\ndatasets. We confirm that the choice of $\\Phi$ and $\\epsilon$ is indeed\npivotal. With $\\Phi$ chosen such that the reductions happen over a single batch\n(and class) element and with a negligible $\\epsilon$, the Dice loss deals with\nmissing labels naturally and performs similarly compared to recent adaptations\nspecific for missing labels. With $\\Phi$ chosen such that the reductions happen\nover multiple batch elements or with a heuristic value for $\\epsilon$, the Dice\nloss handles empty labels correctly. We believe that this work highlights some\nessential perspectives and hope that it encourages researchers to better\ndescribe their exact implementation of the Dice loss in future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tilborghs_S/0/1/0/all/0/1\">Sofie Tilborghs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertels_J/0/1/0/all/0/1\">Jeroen Bertels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robben_D/0/1/0/all/0/1\">David Robben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandermeulen_D/0/1/0/all/0/1\">Dirk Vandermeulen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maes_F/0/1/0/all/0/1\">Frederik Maes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge distillation with a class-aware loss for endoscopic disease detection. (arXiv:2207.09530v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09530","description":"<p>Prevalence of gastrointestinal (GI) cancer is growing alarmingly every year\nleading to a substantial increase in the mortality rate. Endoscopic detection\nis providing crucial diagnostic support, however, subtle lesions in upper and\nlower GI are quite hard to detect and cause considerable missed detection. In\nthis work, we leverage deep learning to develop a framework to improve the\nlocalization of difficult to detect lesions and minimize the missed detection\nrate. We propose an end to end student-teacher learning setup where class\nprobabilities of a trained teacher model on one class with larger dataset are\nused to penalize multi-class student network. Our model achieves higher\nperformance in terms of mean average precision (mAP) on both endoscopic disease\ndetection (EDD2020) challenge and Kvasir-SEG datasets. Additionally, we show\nthat using such learning paradigm, our model is generalizable to unseen test\nset giving higher APs for clinically crucial neoplastic and polyp categories\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavarrias_Solanon_P/0/1/0/all/0/1\">Pedro E. Chavarrias-Solanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_Teevno_M/0/1/0/all/0/1\">Mansoor Ali-Teevno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sharib Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Block-based Convolutional Neural Network for Low-Resolution Image Classification. (arXiv:2207.09531v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09531","description":"<p>The success of CNN-based architecture on image classification in learning and\nextracting features made them so popular these days, but the task of image\nclassification becomes more challenging when we use state of art models to\nclassify noisy and low-quality images. To solve this problem, we proposed a\nnovel image classification architecture that learns subtle details in\nlow-resolution images that are blurred and noisy. In order to build our new\nblocks, we used the idea of Res Connections and the Inception module ideas.\nUsing the MNIST datasets, we have conducted extensive experiments that show\nthat the introduced architecture is more accurate and faster than other\nstate-of-the-art Convolutional neural networks. As a result of the special\ncharacteristics of our model, it can achieve a better result with fewer\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganj_A/0/1/0/all/0/1\">Ashkan Ganj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebadpour_M/0/1/0/all/0/1\">Mohsen Ebadpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darvish_M/0/1/0/all/0/1\">Mahdi Darvish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahador_H/0/1/0/all/0/1\">Hamid Bahador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of 3D Dental Images Using Deep Learning. (arXiv:2207.09582v1 [eess.IV])","link":"http://arxiv.org/abs/2207.09582","description":"<p>3D image segmentation is a recent and crucial step in many medical analysis\nand recognition schemes. In fact, it represents a relevant research subject and\na fundamental challenge due to its importance and influence. This paper\nprovides a multi-phase Deep Learning-based system that hybridizes various\nefficient methods in order to get the best 3D segmentation output. First, to\nreduce the amount of data and accelerate the processing time, the application\nof Decimate compression technique is suggested and justified. We then use a CNN\nmodel to segment dental images into fifteen separated classes. In the end, a\nspecial KNN-based transformation is applied for the purpose of removing\nisolated meshes and of correcting dental forms. Experimentations demonstrate\nthe precision and the robustness of the selected framework applied to 3D dental\nimages within a private clinical benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boudraa_O/0/1/0/all/0/1\">Omar Boudraa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICRICS: Iterative Compensation Recovery for Image Compressive Sensing. (arXiv:2207.09594v1 [cs.LG])","link":"http://arxiv.org/abs/2207.09594","description":"<p>Closed-loop architecture is widely utilized in automatic control systems and\nattain distinguished performance. However, classical compressive sensing\nsystems employ open-loop architecture with separated sampling and\nreconstruction units. Therefore, a method of iterative compensation recovery\nfor image compressive sensing (ICRICS) is proposed by introducing closed-loop\nframework into traditional compresses sensing systems. The proposed method\ndepends on any existing approaches and upgrades their reconstruction\nperformance by adding negative feedback structure. Theory analysis on negative\nfeedback of compressive sensing systems is performed. An approximate\nmathematical proof of the effectiveness of the proposed method is also\nprovided. Simulation experiments on more than 3 image datasets show that the\nproposed method is superior to 10 competition approaches in reconstruction\nperformance. The maximum increment of average peak signal-to-noise ratio is\n4.36 dB and the maximum increment of average structural similarity is 0.034 on\none dataset. The proposed method based on negative feedback mechanism can\nefficiently correct the recovery error in the existing systems of image\ncompressive sensing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Honggui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trocan_M/0/1/0/all/0/1\">Maria Trocan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galayko_D/0/1/0/all/0/1\">Dimitri Galayko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawan_M/0/1/0/all/0/1\">Mohamad Sawan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AiATrack: Attention in Attention for Transformer Visual Tracking. (arXiv:2207.09603v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09603","description":"<p>Transformer trackers have achieved impressive advancements recently, where\nthe attention mechanism plays an important role. However, the independent\ncorrelation computation in the attention mechanism could result in noisy and\nambiguous attention weights, which inhibits further performance improvement. To\naddress this issue, we propose an attention in attention (AiA) module, which\nenhances appropriate correlations and suppresses erroneous ones by seeking\nconsensus among all correlation vectors. Our AiA module can be readily applied\nto both self-attention blocks and cross-attention blocks to facilitate feature\naggregation and information propagation for visual tracking. Moreover, we\npropose a streamlined Transformer tracking framework, dubbed AiATrack, by\nintroducing efficient feature reuse and target-background embeddings to make\nfull use of temporal references. Experiments show that our tracker achieves\nstate-of-the-art performance on six tracking benchmarks while running at a\nreal-time speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenyuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunluan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate and Robust Classification in Continuously Transitioning Industrial Sprays with Mixup. (arXiv:2207.09609v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09609","description":"<p>Image classification with deep neural networks has seen a surge of\ntechnological breakthroughs with promising applications in areas such as face\nrecognition, medical imaging, and autonomous driving. In engineering problems,\nhowever, such as high-speed imaging of engine fuel injector sprays or body\npaint sprays, deep neural networks face a fundamental challenge related to the\navailability of adequate and diverse data. Typically, only thousands or\nsometimes even hundreds of samples are available for training. In addition, the\ntransition between different spray classes is a continuum and requires a high\nlevel of domain expertise to label the images accurately. In this work, we used\nMixup as an approach to systematically deal with the data scarcity and\nambiguous class boundaries found in industrial spray applications. We show that\ndata augmentation can mitigate the over-fitting problem of large neural\nnetworks on small data sets, to a certain level, but cannot fundamentally\nresolve the issue. We discuss how a convex linear interpolation of different\nclasses naturally aligns with the continuous transition between different\nclasses in our application. Our experiments demonstrate Mixup as a simple yet\neffective method to train an accurate and robust deep neural network classifier\nwith only a few hundred samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongjiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shui_H/0/1/0/all/0/1\">Huanyi Shui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Admasu_A/0/1/0/all/0/1\">Alemayehu Admasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1\">Praveen Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_D/0/1/0/all/0/1\">Devesh Upadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Deep Multi-Shape Matching. (arXiv:2207.09610v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09610","description":"<p>3D shape matching is a long-standing problem in computer vision and computer\ngraphics. While deep neural networks were shown to lead to state-of-the-art\nresults in shape matching, existing learning-based approaches are limited in\nthe context of multi-shape matching: (i) either they focus on matching pairs of\nshapes only and thus suffer from cycle-inconsistent multi-matchings, or (ii)\nthey require an explicit template shape to address the matching of a collection\nof shapes. In this paper, we present a novel approach for deep multi-shape\nmatching that ensures cycle-consistent multi-matchings while not depending on\nan explicit template shape. To this end, we utilise a shape-to-universe\nmulti-matching representation that we combine with powerful functional map\nregularisation, so that our multi-shape matching neural network can be trained\nin a fully unsupervised manner. While the functional map regularisation is only\nconsidered during training time, functional maps are not computed for\npredicting correspondences, thereby allowing for fast inference. We demonstrate\nthat our method achieves state-of-the-art results on several challenging\nbenchmark datasets, and, most remarkably, that our unsupervised method even\noutperforms recent supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongliang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Domain Transferability for Collaborative Inter-level Domain Adaptive Object Detection. (arXiv:2207.09613v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09613","description":"<p>Domain adaptation for object detection (DAOD) has recently drawn much\nattention owing to its capability of detecting target objects without any\nannotations. To tackle the problem, previous works focus on aligning features\nextracted from partial levels (e.g., image-level, instance-level, RPN-level) in\na two-stage detector via adversarial training. However, individual levels in\nthe object detection pipeline are closely related to each other and this\ninter-level relation is unconsidered yet. To this end, we introduce a novel\nframework for DAOD with three proposed components: Multi-scale-aware\nUncertainty Attention (MUA), Transferable Region Proposal Network (TRPN), and\nDynamic Instance Sampling (DIS). With these modules, we seek to reduce the\nnegative transfer effect during training while maximizing transferability as\nwell as discriminability in both domains. Finally, our framework implicitly\nlearns domain invariant regions for object detection via exploiting the\ntransferable information and enhances the complementarity between different\ndetection levels by collaboratively utilizing their domain information. Through\nablation studies and experiments, we show that the proposed modules contribute\nto the performance improvement in a synergic way, demonstrating the\neffectiveness of our method. Moreover, our model achieves a new\nstate-of-the-art performance on various benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1\">Mirae Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1\">Pilhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kibeom Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yu-seung Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overlooked factors in concept-based explanations: Dataset choice, concept salience, and human capability. (arXiv:2207.09615v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09615","description":"<p>Concept-based interpretability methods aim to explain deep neural network\nmodel predictions using a predefined set of semantic concepts. These methods\nevaluate a trained model on a new, \"probe\" dataset and correlate model\npredictions with the visual concepts labeled in that dataset. Despite their\npopularity, they suffer from limitations that are not well-understood and\narticulated by the literature. In this work, we analyze three commonly\noverlooked factors in concept-based explanations. First, the choice of the\nprobe dataset has a profound impact on the generated explanations. Our analysis\nreveals that different probe datasets may lead to very different explanations,\nand suggests that the explanations are not generalizable outside the probe\ndataset. Second, we find that concepts in the probe dataset are often less\nsalient and harder to learn than the classes they claim to explain, calling\ninto question the correctness of the explanations. We argue that only visually\nsalient concepts should be used in concept-based explanations. Finally, while\nexisting methods use hundreds or even thousands of concepts, our human studies\nreveal a much stricter upper bound of 32 concepts or less, beyond which the\nexplanations are much less practically useful. We make suggestions for future\ndevelopment and analysis of concept-based interpretability methods. Code for\nour analysis and user interface can be found at\n\\url{https://github.com/princetonvisualai/OverlookedFactors}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramaswamy_V/0/1/0/all/0/1\">Vikram V. Ramaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunnie S. Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_R/0/1/0/all/0/1\">Ruth Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from few examples: Classifying sex from retinal images via deep learning. (arXiv:2207.09624v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09624","description":"<p>Deep learning has seen tremendous interest in medical imaging, particularly\nin the use of convolutional neural networks (CNNs) for developing automated\ndiagnostic tools. The facility of its non-invasive acquisition makes retinal\nfundus imaging amenable to such automated approaches. Recent work in analyzing\nfundus images using CNNs relies on access to massive data for training and\nvalidation - hundreds of thousands of images. However, data residency and data\nprivacy restrictions stymie the applicability of this approach in medical\nsettings where patient confidentiality is a mandate. Here, we showcase results\nfor the performance of DL on small datasets to classify patient sex from fundus\nimages - a trait thought not to be present or quantifiable in fundus images\nuntil recently. We fine-tune a Resnet-152 model whose last layer has been\nmodified for binary classification. In several experiments, we assess\nperformance in the small dataset context using one private (DOVS) and one\npublic (ODIR) data source. Our models, developed using approximately 2500\nfundus images, achieved test AUC scores of up to 0.72 (95% CI: [0.67, 0.77]).\nThis corresponds to a mere 25% decrease in performance despite a nearly\n1000-fold decrease in the dataset size compared to prior work in the\nliterature. Even with a hard task like sex categorization from retinal images,\nwe find that classification is possible with very small datasets. Additionally,\nwe perform domain adaptation experiments between DOVS and ODIR; explore the\neffect of data curation on training and generalizability; and investigate model\nensembling to maximize CNN classifier performance in the context of small\ndevelopment datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berk_A/0/1/0/all/0/1\">Aaron Berk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturan_G/0/1/0/all/0/1\">Gulcenur Ozturan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delavari_P/0/1/0/all/0/1\">Parsa Delavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maberley_D/0/1/0/all/0/1\">David Maberley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;r Y&#x131;lmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oruc_I/0/1/0/all/0/1\">Ipek Oruc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explicit Image Caption Editing. (arXiv:2207.09625v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09625","description":"<p>Given an image and a reference caption, the image caption editing task aims\nto correct the misalignment errors and generate a refined caption. However, all\nexisting caption editing works are implicit models, ie, they directly produce\nthe refined captions without explicit connections to the reference captions. In\nthis paper, we introduce a new task: Explicit Caption Editing (ECE). ECE models\nexplicitly generate a sequence of edit operations, and this edit operation\nsequence can translate the reference caption into a refined one. Compared to\nthe implicit editing, ECE has multiple advantages: 1) Explainable: it can trace\nthe whole editing path. 2) Editing Efficient: it only needs to modify a few\nwords. 3) Human-like: it resembles the way that humans perform caption editing,\nand tries to keep original sentence structures. To solve this new task, we\npropose the first ECE model: TIger. TIger is a non-autoregressive\ntransformer-based model, consisting of three modules: Tagger_del, Tagger_add,\nand Inserter. Specifically, Tagger_del decides whether each word should be\npreserved or not, Tagger_add decides where to add new words, and Inserter\npredicts the specific word for adding. To further facilitate ECE research, we\npropose two new ECE benchmarks by re-organizing two existing datasets, dubbed\nCOCO-EE and Flickr30K-EE, respectively. Extensive ablations on both two\nbenchmarks have demonstrated the effectiveness of TIger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jian Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVHA: Explainable Vision System for Hardware Testing and Assurance -- An Overview. (arXiv:2207.09627v1 [cs.CR])","link":"http://arxiv.org/abs/2207.09627","description":"<p>Due to the ever-growing demands for electronic chips in different sectors the\nsemiconductor companies have been mandated to offshore their manufacturing\nprocesses. This unwanted matter has made security and trustworthiness of their\nfabricated chips concerning and caused creation of hardware attacks. In this\ncondition, different entities in the semiconductor supply chain can act\nmaliciously and execute an attack on the design computing layers, from devices\nto systems. Our attack is a hardware Trojan that is inserted during mask\ngeneration/fabrication in an untrusted foundry. The Trojan leaves a footprint\nin the fabricated through addition, deletion, or change of design cells. In\norder to tackle this problem, we propose Explainable Vision System for Hardware\nTesting and Assurance (EVHA) in this work that can detect the smallest possible\nchange to a design in a low-cost, accurate, and fast manner. The inputs to this\nsystem are Scanning Electron Microscopy (SEM) images acquired from the\nIntegrated Circuits (ICs) under examination. The system output is determination\nof IC status in terms of having any defect and/or hardware Trojan through\naddition, deletion, or change in the design cells at the cell-level. This\narticle provides an overview on the design, development, implementation, and\nanalysis of our defense system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md Mahfuz Al Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostafiz_M/0/1/0/all/0/1\">Mohammad Tahsin Mostafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thomas An Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julia_J/0/1/0/all/0/1\">Jake Julia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vashistha_N/0/1/0/all/0/1\">Nidish Vashistha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taheri_S/0/1/0/all/0/1\">Shayan Taheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadizanjani_N/0/1/0/all/0/1\">Navid Asadizanjani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective Phase Angle Model for Polarimetric 3D Reconstruction. (arXiv:2207.09629v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09629","description":"<p>Current polarimetric 3D reconstruction methods, including those in the\nwell-established shape from polarization literature, are all developed under\nthe orthographic projection assumption. In the case of a large field of view,\nhowever, this assumption does not hold and may result in significant\nreconstruction errors in methods that make this assumption. To address this\nproblem, we present the perspective phase angle (PPA) model that is applicable\nto perspective cameras. Compared with the orthographic model, the proposed PPA\nmodel accurately describes the relationship between polarization phase angle\nand surface normal under perspective projection. In addition, the PPA model\nmakes it possible to estimate surface normals from only one single-view phase\nangle map and does not suffer from the so-called {\\pi}-ambiguity problem.\nExperiments on real data show that the PPA model is more accurate for surface\nnormal estimation with a perspective camera than the orthographic model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangcheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Li He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yisheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperNet: Self-Supervised Hyperspectral Spatial-Spectral Feature Understanding Network for Hyperspectral Change Detection. (arXiv:2207.09634v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09634","description":"<p>The fast development of self-supervised learning lowers the bar learning\nfeature representation from massive unlabeled data and has triggered a series\nof research on change detection of remote sensing images. Challenges in\nadapting self-supervised learning from natural images classification to remote\nsensing images change detection arise from difference between the two tasks.\nThe learned patch-level feature representations are not satisfying for the\npixel-level precise change detection. In this paper, we proposed a novel\npixel-level self-supervised hyperspectral spatial-spectral understanding\nnetwork (HyperNet) to accomplish pixel-wise feature representation for\neffective hyperspectral change detection. Concretely, not patches but the whole\nimages are fed into the network and the multi-temporal spatial-spectral\nfeatures are compared pixel by pixel. Instead of processing the two-dimensional\nimaging space and spectral response dimension in hybrid style, a powerful\nspatial-spectral attention module is put forward to explore the spatial\ncorrelation and discriminative spectral features of multi-temporal\nhyperspectral images (HSIs), separately. Only the positive samples at the same\nlocation of bi-temporal HSIs are created and forced to be aligned, aiming at\nlearning the spectral difference-invariant features. Moreover, a new similarity\nloss function named focal cosine is proposed to solve the problem of imbalanced\neasy and hard positive samples comparison, where the weights of those hard\nsamples are enlarged and highlighted to promote the network training. Six\nhyperspectral datasets have been adopted to test the validity and\ngeneralization of proposed HyperNet. The extensive experiments demonstrate the\nsuperiority of HyperNet over the state-of-the-art algorithms on downstream\nhyperspectral change detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Meiqi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DC-BENCH: Dataset Condensation Benchmark. (arXiv:2207.09639v1 [cs.LG])","link":"http://arxiv.org/abs/2207.09639","description":"<p>Dataset Condensation is a newly emerging technique aiming at learning a tiny\ndataset that captures the rich information encoded in the original dataset. As\nthe size of datasets contemporary machine learning models rely on becomes\nincreasingly large, condensation methods become a prominent direction for\naccelerating network training and reducing data storage. Despite numerous\nmethods have been proposed in this rapidly growing field, evaluating and\ncomparing different condensation methods is non-trivial and still remains an\nopen issue. The quality of condensed dataset are often shadowed by many\ncritical contributing factors to the end performance, such as data augmentation\nand model architectures. The lack of a systematic way to evaluate and compare\ncondensation methods not only hinders our understanding of existing techniques,\nbut also discourages practical usage of the synthesized datasets. This work\nprovides the first large-scale standardized benchmark on Dataset Condensation.\nIt consists of a suite of evaluations to comprehensively reflect the\ngenerability and effectiveness of condensation methods through the lens of\ntheir generated dataset. Leveraging this benchmark, we conduct a large-scale\nstudy of current condensation methods, and report many insightful findings that\nopen up new possibilities for future development. The benchmark library,\nincluding evaluators, baseline methods, and generated datasets, is open-sourced\nto facilitate future research and application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Justin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1\">Si Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning. (arXiv:2207.09644v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09644","description":"<p>Despite the success of fully-supervised human skeleton sequence modeling,\nutilizing self-supervised pre-training for skeleton sequence representation\nlearning has been an active field because acquiring task-specific skeleton\nannotations at large scales is difficult. Recent studies focus on learning\nvideo-level temporal and discriminative information using contrastive learning,\nbut overlook the hierarchical spatial-temporal nature of human skeletons.\nDifferent from such superficial supervision at the video level, we propose a\nself-supervised hierarchical pre-training scheme incorporated into a\nhierarchical Transformer-based skeleton sequence encoder (Hi-TRS), to\nexplicitly capture spatial, short-term, and long-term temporal dependencies at\nframe, clip, and video levels, respectively. To evaluate the proposed\nself-supervised pre-training scheme with Hi-TRS, we conduct extensive\nexperiments covering three skeleton-based downstream tasks including action\nrecognition, action detection, and motion prediction. Under both supervised and\nsemi-supervised evaluation protocols, our method achieves the state-of-the-art\nperformance. Additionally, we demonstrate that the prior knowledge learned by\nour model in the pre-training stage has strong transfer capability for\ndifferent downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhaoyang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Ligong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aware of the History: Trajectory Forecasting with the Local Behavior Data. (arXiv:2207.09646v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09646","description":"<p>The historical trajectories previously passing through a location may help\ninfer the future trajectory of an agent currently at this location. Despite\ngreat improvements in trajectory forecasting with the guidance of\nhigh-definition maps, only a few works have explored such local historical\ninformation. In this work, we re-introduce this information as a new type of\ninput data for trajectory forecasting systems: the local behavior data, which\nwe conceptualize as a collection of location-specific historical trajectories.\nLocal behavior data helps the systems emphasize the prediction locality and\nbetter understand the impact of static map objects on moving agents. We propose\na novel local-behavior-aware (LBA) prediction framework that improves\nforecasting accuracy by fusing information from observed trajectories, HD maps,\nand local behavior data. Also, where such historical data is insufficient or\nunavailable, we employ a local-behavior-free (LBF) prediction framework, which\nadopts a knowledge-distillation-based architecture to infer the impact of\nmissing data. Extensive experiments demonstrate that upgrading existing methods\nwith these two frameworks significantly improves their performances.\nEspecially, the LBA framework boosts the SOTA methods' performance on the\nnuScenes dataset by at least 14% for the K=1 metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zhenyang Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenText: Unsupervised Artistic Text Generation via Decoupled Font and Texture Manipulation. (arXiv:2207.09649v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09649","description":"<p>Automatic artistic text generation is an emerging topic which receives\nincreasing attention due to its wide applications. The artistic text can be\ndivided into three components, content, font, and texture, respectively.\nExisting artistic text generation models usually focus on manipulating one\naspect of the above components, which is a sub-optimal solution for\ncontrollable general artistic text generation. To remedy this issue, we propose\na novel approach, namely GenText, to achieve general artistic text style\ntransfer by separably migrating the font and texture styles from the different\nsource images to the target images in an unsupervised manner. Specifically, our\ncurrent work incorporates three different stages, stylization, destylization,\nand font transfer, respectively, into a unified platform with a single powerful\nencoder network and two separate style generator networks, one for font\ntransfer, the other for stylization and destylization. The destylization stage\nfirst extracts the font style of the font reference image, then the font\ntransfer stage generates the target content with the desired font style.\nFinally, the stylization stage renders the resulted font image with respect to\nthe texture style in the reference image. Moreover, considering the difficult\ndata acquisition of paired artistic text images, our model is designed under\nthe unsupervised setting, where all stages can be effectively optimized from\nunpaired data. Qualitative and quantitative results are performed on artistic\ntext benchmarks, which demonstrate the superior performance of our proposed\nmodel. The code with models will become publicly available in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qirui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Bin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zhang_A/0/1/0/all/0/1\">Aozhong zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Topological Interactions for Multi-Class Medical Image Segmentation. (arXiv:2207.09654v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09654","description":"<p>Deep learning methods have achieved impressive performance for multi-class\nmedical image segmentation. However, they are limited in their ability to\nencode topological interactions among different classes (e.g., containment and\nexclusion). These constraints naturally arise in biomedical images and can be\ncrucial in improving segmentation quality. In this paper, we introduce a novel\ntopological interaction module to encode the topological interactions into a\ndeep neural network. The implementation is completely convolution-based and\nthus can be very efficient. This empowers us to incorporate the constraints\ninto end-to-end training and enrich the feature representation of neural\nnetworks. The efficacy of the proposed method is validated on different types\nof interactions. We also demonstrate the generalizability of the method on both\nproprietary and public challenge datasets, in both 2D and 3D settings, as well\nas across different modalities such as CT and Ultrasound. Code is available at:\nhttps://github.com/TopoXLab/TopoInteraction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Saumya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoling Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaan_J/0/1/0/all/0/1\">James Kaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Michael Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mpoy_M/0/1/0/all/0/1\">Mutshipay Mpoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_K/0/1/0/all/0/1\">Katherine Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saltz_M/0/1/0/all/0/1\">Mary Saltz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saltz_J/0/1/0/all/0/1\">Joel Saltz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassiopoulos_A/0/1/0/all/0/1\">Apostolos Tassiopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for One-stage Object Detector using Offsets to Bounding Box. (arXiv:2207.09656v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09656","description":"<p>Most existing domain adaptive object detection methods exploit adversarial\nfeature alignment to adapt the model to a new domain. Recent advances in\nadversarial feature alignment strives to reduce the negative effect of\nalignment, or negative transfer, that occurs because the distribution of\nfeatures varies depending on the category of objects. However, by analyzing the\nfeatures of the anchor-free one-stage detector, in this paper, we find that\nnegative transfer may occur because the feature distribution varies depending\non the regression value for the offset to the bounding box as well as the\ncategory. To obtain domain invariance by addressing this issue, we align the\nfeature conditioned on the offset value, considering the modality of the\nfeature distribution. With a very simple and effective conditioning method, we\npropose OADA (Offset-Aware Domain Adaptive object detector) that achieves\nstate-of-the-art performances in various experimental settings. In addition, by\nanalyzing through singular value decomposition, we find that our model enhances\nboth discriminability and transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jayeon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_I/0/1/0/all/0/1\">Inseop Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Depth from Focus in the Wild. (arXiv:2207.09658v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09658","description":"<p>For better photography, most recent commercial cameras including smartphones\nhave either adopted large-aperture lens to collect more light or used a burst\nmode to take multiple images within short times. These interesting features\nlead us to examine depth from focus/defocus.\n</p>\n<p>In this work, we present a convolutional neural network-based depth\nestimation from single focal stacks. Our method differs from relevant\nstate-of-the-art works with three unique features. First, our method allows\ndepth maps to be inferred in an end-to-end manner even with image alignment.\nSecond, we propose a sharp region detection module to reduce blur ambiguities\nin subtle focus changes and weakly texture-less regions. Third, we design an\neffective downsampling module to ease flows of focal information in feature\nextractions. In addition, for the generalization of the proposed network, we\ndevelop a simulator to realistically reproduce the features of commercial\ncameras, such as changes in field of view, focal length and principal points.\n</p>\n<p>By effectively incorporating these three unique features, our network\nachieves the top rank in the DDFF 12-Scene benchmark on most metrics. We also\ndemonstrate the effectiveness of the proposed method on various quantitative\nevaluations and real-world images taken from various off-the-shelf cameras\ncompared with state-of-the-art methods. Our source code is publicly available\nat https://github.com/wcy199705/DfFintheWild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Won_C/0/1/0/all/0/1\">Changyeon Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hae-Gon Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand-Assisted Expression Recognition Method from Synthetic Images at the Fourth ABAW Challenge. (arXiv:2207.09661v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09661","description":"<p>Learning from synthetic images plays an important role in facial expression\nrecognition task due to the difficulties of labeling the real images, and it is\nchallenging because of the gap between the synthetic images and real images.\nThe fourth Affective Behavior Analysis in-the-wild Competition raises the\nchallenge and provides the synthetic images generated from Aff-Wild2 dataset.\nIn this paper, we propose a hand-assisted expression recognition method to\nreduce the gap between the synthetic data and real data. Our method consists of\ntwo parts: expression recognition module and hand prediction module. Expression\nrecognition module extracts expression information and hand prediction module\npredicts whether the image contains hands. Decision mode is used to combine the\nresults of two modules, and post-pruning is used to improve the result. F1\nscore is used to verify the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xiangyu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yanan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shangfei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers. (arXiv:2207.09662v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09662","description":"<p>Temporal action localization (TAL) is a task of identifying a set of actions\nin a video, which involves localizing the start and end frames and classifying\neach action instance. Existing methods have addressed this task by using\npredefined anchor windows or heuristic bottom-up boundary-matching strategies,\nwhich are major bottlenecks in inference time. Additionally, the main challenge\nis the inability to capture long-range actions due to a lack of global\ncontextual information. In this paper, we present a novel anchor-free\nframework, referred to as HTNet, which predicts a set of &lt;start time, end time,\nclass&gt; triplets from a video based on a Transformer architecture. After the\nprediction of coarse boundaries, we refine it through a background feature\nsampling (BFS) module and hierarchical Transformers, which enables our model to\naggregate global contextual information and effectively exploit the inherent\nsemantic relationships in a video. We demonstrate how our method localizes\naccurate action instances and achieves state-of-the-art performance on two TAL\nbenchmark datasets: THUMOS14 and ActivityNet 1.3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_T/0/1/0/all/0/1\">Tae-Kyung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streamable Neural Fields. (arXiv:2207.09663v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09663","description":"<p>Neural fields have emerged as a new data representation paradigm and have\nshown remarkable success in various signal representations. Since they preserve\nsignals in their network parameters, the data transfer by sending and receiving\nthe entire model parameters prevents this emerging technology from being used\nin many practical scenarios. We propose streamable neural fields, a single\nmodel that consists of executable sub-networks of various widths. The proposed\narchitectural and training techniques enable a single network to be streamable\nover time and reconstruct different qualities and parts of signals. For\nexample, a smaller sub-network produces smooth and low-frequency signals, while\na larger sub-network can represent fine details. Experimental results have\nshown the effectiveness of our method in various domains, such as 2D images,\nvideos, and 3D signed distance functions. Finally, we demonstrate that our\nproposed method improves training stability, by exploiting parameter sharing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Seungtae Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rho_D/0/1/0/all/0/1\">Daniel Rho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jong Hwan Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunbyung Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-label Guided Cross-video Pixel Contrast for Robotic Surgical Scene Segmentation with Limited Annotations. (arXiv:2207.09664v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09664","description":"<p>Surgical scene segmentation is fundamentally crucial for prompting cognitive\nassistance in robotic surgery. However, pixel-wise annotating surgical video in\na frame-by-frame manner is expensive and time consuming. To greatly reduce the\nlabeling burden, in this work, we study semi-supervised scene segmentation from\nrobotic surgical video, which is practically essential yet rarely explored\nbefore. We consider a clinically suitable annotation situation under the\nequidistant sampling. We then propose PGV-CL, a novel pseudo-label guided\ncross-video contrast learning method to boost scene segmentation. It\neffectively leverages unlabeled data for a trusty and global model\nregularization that produces more discriminative feature representation.\nConcretely, for trusty representation learning, we propose to incorporate\npseudo labels to instruct the pair selection, obtaining more reliable\nrepresentation pairs for pixel contrast. Moreover, we expand the representation\nlearning space from previous image-level to cross-video, which can capture the\nglobal semantics to benefit the learning process. We extensively evaluate our\nmethod on a public robotic surgery dataset EndoVis18 and a public cataract\ndataset CaDIS. Experimental results demonstrate the effectiveness of our\nmethod, consistently outperforming the state-of-the-art semi-supervised methods\nunder different labeling ratios, and even surpassing fully supervised training\non EndoVis18 with 10.1% labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features. (arXiv:2207.09666v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09666","description":"<p>Current state-of-the-art methods for image captioning employ region-based\nfeatures, as they provide object-level information that is essential to\ndescribe the content of images; they are usually extracted by an object\ndetector such as Faster R-CNN. However, they have several issues, such as lack\nof contextual information, the risk of inaccurate detection, and the high\ncomputational cost. The first two could be resolved by additionally using\ngrid-based features. However, how to extract and fuse these two types of\nfeatures is uncharted. This paper proposes a Transformer-only neural\narchitecture, dubbed GRIT (Grid- and Region-based Image captioning\nTransformer), that effectively utilizes the two visual features to generate\nbetter captions. GRIT replaces the CNN-based detector employed in previous\nmethods with a DETR-based one, making it computationally faster. Moreover, its\nmonolithic design consisting only of Transformers enables end-to-end training\nof the model. This innovative design and the integration of the dual visual\nfeatures bring about significant performance improvement. The experimental\nresults on several image captioning benchmarks show that GRIT outperforms\nprevious methods in inference accuracy and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Van-Quang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERA: Expert Retrieval and Assembly for Early Action Prediction. (arXiv:2207.09675v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09675","description":"<p>Early action prediction aims to successfully predict the class label of an\naction before it is completely performed. This is a challenging task because\nthe beginning stages of different actions can be very similar, with only minor\nsubtle differences for discrimination. In this paper, we propose a novel Expert\nRetrieval and Assembly (ERA) module that retrieves and assembles a set of\nexperts most specialized at using discriminative subtle differences, to\ndistinguish an input sample from other highly similar samples. To encourage our\nmodel to effectively use subtle differences for early action prediction, we\npush experts to discriminate exclusively between samples that are highly\nsimilar, forcing these experts to learn to use subtle differences that exist\nbetween those samples. Additionally, we design an effective Expert Learning\nRate Optimization method that balances the experts' optimization and leads to\nbetter performance. We evaluate our ERA module on four public action datasets\nand achieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foo_L/0/1/0/all/0/1\">Lin Geng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianjiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1\">Qiuhong Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Deepfake Detection by Analysing Image Matching. (arXiv:2207.09679v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09679","description":"<p>This paper aims to interpret how deepfake detection models learn artifact\nfeatures of images when just supervised by binary labels. To this end, three\nhypotheses from the perspective of image matching are proposed as follows. 1.\nDeepfake detection models indicate real/fake images based on visual concepts\nthat are neither source-relevant nor target-relevant, that is, considering such\nvisual concepts as artifact-relevant. 2. Besides the supervision of binary\nlabels, deepfake detection models implicitly learn artifact-relevant visual\nconcepts through the FST-Matching (i.e. the matching fake, source, target\nimages) in the training set. 3. Implicitly learned artifact visual concepts\nthrough the FST-Matching in the raw training set are vulnerable to video\ncompression. In experiments, the above hypotheses are verified among various\nDNNs. Furthermore, based on this understanding, we propose the FST-Matching\nDeepfake Detection Model to boost the performance of forgery detection on\ncompressed videos. Experiment results show that our method achieves great\nperformance, especially on highly-compressed (e.g. c40) videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Shichao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiajun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Renhe Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Versatile Uses of Partial Distance Correlation in Deep Learning. (arXiv:2207.09684v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09684","description":"<p>Comparing the functional behavior of neural network models, whether it is a\nsingle network over time or two (or more networks) during or post-training, is\nan essential step in understanding what they are learning (and what they are\nnot), and for identifying strategies for regularization or efficiency\nimprovements. Despite recent progress, e.g., comparing vision transformers to\nCNNs, systematic comparison of function, especially across different networks,\nremains difficult and is often carried out layer by layer. Approaches such as\ncanonical correlation analysis (CCA) are applicable in principle, but have been\nsparingly used so far. In this paper, we revisit a (less widely known) from\nstatistics, called distance correlation (and its partial variant), designed to\nevaluate correlation between feature spaces of different dimensions. We\ndescribe the steps necessary to carry out its deployment for large scale models\n-- this opens the door to a surprising array of applications ranging from\nconditioning one deep model w.r.t. another, learning disentangled\nrepresentations as well as optimizing diverse models that would directly be\nmore robust to adversarial attacks. Our experiments suggest a versatile\nregularizer (or constraint) with many advantages, which avoids some of the\ncommon difficulties one faces in such analyses. Code is at\nhttps://github.com/zhenxingjian/Partial_Distance_Correlation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xingjian Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zihang Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Rudrasis Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikas Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigColor: Colorization using a Generative Color Prior for Natural Images. (arXiv:2207.09685v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09685","description":"<p>For realistic and vivid colorization, generative priors have recently been\nexploited. However, such generative priors often fail for in-the-wild complex\nimages due to their limited representation space. In this paper, we propose\nBigColor, a novel colorization approach that provides vivid colorization for\ndiverse in-the-wild images with complex structures. While previous generative\npriors are trained to synthesize both image structures and colors, we learn a\ngenerative color prior to focus on color synthesis given the spatial structure\nof an image. In this way, we reduce the burden of synthesizing image structures\nfrom the generative prior and expand its representation space to cover diverse\nimages. To this end, we propose a BigGAN-inspired encoder-generator network\nthat uses a spatial feature map instead of a spatially-flattened BigGAN latent\ncode, resulting in an enlarged representation space. Our method enables robust\ncolorization for diverse inputs in a single forward pass, supports arbitrary\ninput resolutions, and provides multi-modal colorization results. We\ndemonstrate that BigColor significantly outperforms existing methods especially\non in-the-wild images with complex structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geonung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1\">Kyoungkook Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seongtae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwayoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jonghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Compositional Neural Implicit Surfaces. (arXiv:2207.09686v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09686","description":"<p>The neural implicit representation has shown its effectiveness in novel view\nsynthesis and high-quality 3D reconstruction from multi-view images. However,\nmost approaches focus on holistic scene representation yet ignore individual\nobjects inside it, thus limiting potential downstream applications. In order to\nlearn object-compositional representation, a few works incorporate the 2D\nsemantic map as a cue in training to grasp the difference between objects. But\nthey neglect the strong connections between object geometry and instance\nsemantic information, which leads to inaccurate modeling of individual\ninstance. This paper proposes a novel framework, ObjectSDF, to build an\nobject-compositional neural implicit representation with high fidelity in 3D\nreconstruction and object representation. Observing the ambiguity of\nconventional volume rendering pipelines, we model the scene by combining the\nSigned Distance Functions (SDF) of individual object to exert explicit surface\nconstraint. The key in distinguishing different instances is to revisit the\nstrong association between an individual object's SDF and semantic label.\nParticularly, we convert the semantic information to a function of object SDF\nand develop a unified and compact representation for scene and objects.\nExperimental results show the superiority of ObjectSDF framework in\nrepresenting both the holistic object-compositional scene and the individual\ninstances. Code can be found at https://qianyiwu.github.io/objectsdf/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuedong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kejie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanxia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianmin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Inspired Underwater Image Enhancement. (arXiv:2207.09689v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09689","description":"<p>A main challenge faced in the deep learning-based Underwater Image\nEnhancement (UIE) is that the ground truth high-quality image is unavailable.\nMost of the existing methods first generate approximate reference maps and then\ntrain an enhancement network with certainty. This kind of method fails to\nhandle the ambiguity of the reference map. In this paper, we resolve UIE into\ndistribution estimation and consensus process. We present a novel probabilistic\nnetwork to learn the enhancement distribution of degraded underwater images.\nSpecifically, we combine conditional variational autoencoder with adaptive\ninstance normalization to construct the enhancement distribution. After that,\nwe adopt a consensus process to predict a deterministic result based on a set\nof samples from the distribution. By learning the enhancement distribution, our\nmethod can cope with the bias introduced in the reference map labeling to some\nextent. Additionally, the consensus process is useful to capture a robust and\nstable result. We examined the proposed method on two widely used real-world\nunderwater image enhancement datasets. Experimental results demonstrate that\nour approach enables sampling possible enhancement predictions. Meanwhile, the\nconsensus estimate yields competitive performance compared with\nstate-of-the-art UIE methods. Code available at\nhttps://github.com/zhenqifu/PUIE-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhenqi Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai-Kuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Meta-Tuning for Content-aware Neural Video Delivery. (arXiv:2207.09691v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09691","description":"<p>Recently, Deep Neural Networks (DNNs) are utilized to reduce the bandwidth\nand improve the quality of Internet video delivery. Existing methods train\ncorresponding content-aware super-resolution (SR) model for each video chunk on\nthe server, and stream low-resolution (LR) video chunks along with SR models to\nthe client. Although they achieve promising results, the huge computational\ncost of network training limits their practical applications. In this paper, we\npresent a method named Efficient Meta-Tuning (EMT) to reduce the computational\ncost. Instead of training from scratch, EMT adapts a meta-learned model to the\nfirst chunk of the input video. As for the following chunks, it fine-tunes the\npartial parameters selected by gradient masking of previous adapted model. In\norder to achieve further speedup for EMT, we propose a novel sampling strategy\nto extract the most challenging patches from video frames. The proposed\nstrategy is highly efficient and brings negligible additional cost. Our method\nsignificantly reduces the computational cost and achieves even better\nperformance, paving the way for applying neural video delivery techniques to\npractical applications. We conduct extensive experiments based on various\nefficient SR architectures, including ESPCN, SRCNN, FSRCNN and EDSR-1,\ndemonstrating the generalization ability of our work. The code is released at\n\\url{https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shizun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Cheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Anbang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Object Detection With Inaccurate Bounding Boxes. (arXiv:2207.09697v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09697","description":"<p>Learning accurate object detectors often requires large-scale training data\nwith precise object bounding boxes. However, labeling such data is expensive\nand time-consuming. As the crowd-sourcing labeling process and the ambiguities\nof the objects may raise noisy bounding box annotations, the object detectors\nwill suffer from the degenerated training data. In this work, we aim to address\nthe challenge of learning robust object detectors with inaccurate bounding\nboxes. Inspired by the fact that localization precision suffers significantly\nfrom inaccurate bounding boxes while classification accuracy is less affected,\nwe propose leveraging classification as a guidance signal for refining\nlocalization results. Specifically, by treating an object as a bag of\ninstances, we introduce an Object-Aware Multiple Instance Learning approach\n(OA-MIL), featured with object-aware instance selection and object-aware\ninstance extension. The former aims to select accurate instances for training,\ninstead of directly using inaccurate box annotations. The latter focuses on\ngenerating high-quality instances for selection. Extensive experiments on\nsynthetic noisy datasets (i.e., noisy PASCAL VOC and MS-COCO) and a real noisy\nwheat head dataset demonstrate the effectiveness of our OA-MIL. Code is\navailable at https://github.com/cxliu0/OA-MIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kewei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiguo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziming Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving Copycat Problems in Visual Imitation Learning via Residual Action Prediction. (arXiv:2207.09705v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09705","description":"<p>Imitation learning is a widely used policy learning method that enables\nintelligent agents to acquire complex skills from expert demonstrations. The\ninput to the imitation learning algorithm is usually composed of both the\ncurrent observation and historical observations since the most recent\nobservation might not contain enough information. This is especially the case\nwith image observations, where a single image only includes one view of the\nscene, and it suffers from a lack of motion information and object occlusions.\nIn theory, providing multiple observations to the imitation learning agent will\nlead to better performance. However, surprisingly people find that sometimes\nimitation from observation histories performs worse than imitation from the\nmost recent observation. In this paper, we explain this phenomenon from the\ninformation flow within the neural network perspective. We also propose a novel\nimitation learning neural network architecture that does not suffer from this\nissue by design. Furthermore, our method scales to high-dimensional image\nobservations. Finally, we benchmark our approach on two widely used simulators,\nCARLA and MuJoCo, and it successfully alleviates the copycat problem and\nsurpasses the existing solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_C/0/1/0/all/0/1\">Chia-Chi Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Donglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1\">Chuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sequence Representations by Non-local Recurrent Neural Memory. (arXiv:2207.09710v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09710","description":"<p>The key challenge of sequence representation learning is to capture the\nlong-range temporal dependencies. Typical methods for supervised sequence\nrepresentation learning are built upon recurrent neural networks to capture\ntemporal dependencies. One potential limitation of these methods is that they\nonly model one-order information interactions explicitly between adjacent time\nsteps in a sequence, hence the high-order interactions between nonadjacent time\nsteps are not fully exploited. It greatly limits the capability of modeling the\nlong-range temporal dependencies since the temporal features learned by\none-order interactions cannot be maintained for a long term due to temporal\ninformation dilution and gradient vanishing. To tackle this limitation, we\npropose the Non-local Recurrent Neural Memory (NRNM) for supervised sequence\nrepresentation learning, which performs non-local operations \\MR{by means of\nself-attention mechanism} to learn full-order interactions within a sliding\ntemporal memory block and models global interactions between memory blocks in a\ngated recurrent manner. Consequently, our model is able to capture long-range\ndependencies. Besides, the latent high-level features contained in high-order\ninteractions can be distilled by our model. We validate the effectiveness and\ngeneralization of our NRNM on three types of sequence applications across\ndifferent modalities, including sequence classification, step-wise sequential\nprediction and sequence similarity learning. Our model compares favorably\nagainst other state-of-the-art methods specifically designed for each of these\nsequence applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Canmiao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qiong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning for Emotion Descriptors Estimation at the fourth ABAW Challenge. (arXiv:2207.09716v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09716","description":"<p>Facial valence/arousal, expression and action unit are related tasks in\nfacial affective analysis. However, the tasks only have limited performance in\nthe wild due to the various collected conditions. The 4th competition on\naffective behavior analysis in the wild (ABAW) provided images with\nvalence/arousal, expression and action unit labels. In this paper, we introduce\nmulti-task learning framework to enhance the performance of three related tasks\nin the wild. Feature sharing and label fusion are used to utilize their\nrelations. We conduct experiments on the provided training and validating data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yanan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xiangyu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shangfei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Representation Learning for Unsupervised Cross-domain Image Retrieval. (arXiv:2207.09721v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09721","description":"<p>Current supervised cross-domain image retrieval methods can achieve excellent\nperformance. However, the cost of data collection and labeling imposes an\nintractable barrier to practical deployment in real applications. In this\npaper, we investigate the unsupervised cross-domain image retrieval task, where\nclass labels and pairing annotations are no longer a prerequisite for training.\nThis is an extremely challenging task because there is no supervision for both\nin-domain feature representation learning and cross-domain alignment. We\naddress both challenges by introducing: 1) a new cluster-wise contrastive\nlearning mechanism to help extract class semantic-aware features, and 2) a\nnovel distance-of-distance loss to effectively measure and minimize the domain\ndiscrepancy without any external supervision. Experiments on the Office-Home\nand DomainNet datasets consistently show the superior image retrieval\naccuracies of our framework over state-of-the-art approaches. Our source code\ncan be found at https://github.com/conghuihu/UCDIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Conghui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos. (arXiv:2207.09725v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09725","description":"<p>Although many approaches for multi-human pose estimation in videos have shown\nprofound results, they require densely annotated data which entails excessive\nman labor. Furthermore, there exists occlusion and motion blur that inevitably\nlead to poor estimation performance. To address these problems, we propose a\nmethod that leverages an attention mask for occluded joints and encodes\ntemporal dependency between frames using transformers. First, our framework\ncomposes different combinations of sparsely annotated frames that denote the\ntrack of the overall joint movement. We propose an occlusion attention mask\nfrom these combinations that enable encoding occlusion-aware heatmaps as a\nsemi-supervised task. Second, the proposed temporal encoder employs transformer\narchitecture to effectively aggregate the temporal relationship and\nkeypoint-wise attention from each time step and accurately refines the target\nframe's final pose estimation. We achieve state-of-the-art pose estimation\nresults for PoseTrack2017 and PoseTrack2018 datasets and demonstrate the\nrobustness of our approach to occlusion and motion blur in sparsely annotated\nvideo data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kyung-Min Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting data augmentation for subspace clustering. (arXiv:2207.09728v1 [cs.LG])","link":"http://arxiv.org/abs/2207.09728","description":"<p>Subspace clustering is the classical problem of clustering a collection of\ndata samples that approximately lie around several low-dimensional subspaces.\nThe current state-of-the-art approaches for this problem are based on the\nself-expressive model which represents the samples as linear combination of\nother samples. However, these approaches require sufficiently well-spread\nsamples for accurate representation which might not be necessarily accessible\nin many applications. In this paper, we shed light on this commonly neglected\nissue and argue that data distribution within each subspace plays a critical\nrole in the success of self-expressive models. Our proposed solution to tackle\nthis issue is motivated by the central role of data augmentation in the\ngeneralization power of deep neural networks. We propose two subspace\nclustering frameworks for both unsupervised and semi-supervised settings that\nuse augmented samples as an enlarged dictionary to improve the quality of the\nself-expressive representation. We present an automatic augmentation strategy\nusing a few labeled samples for the semi-supervised problem relying on the fact\nthat the data samples lie in the union of multiple linear subspaces.\nExperimental results confirm the effectiveness of data augmentation, as it\nsignificantly improves the performance of general self-expressive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdolali_M/0/1/0/all/0/1\">Maryam Abdolali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillis_N/0/1/0/all/0/1\">Nicolas Gillis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossHuman: Learning Cross-Guidance from Multi-Frame Images for Human Reconstruction. (arXiv:2207.09735v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09735","description":"<p>We propose CrossHuman, a novel method that learns cross-guidance from\nparametric human model and multi-frame RGB images to achieve high-quality 3D\nhuman reconstruction. To recover geometry details and texture even in invisible\nregions, we design a reconstruction pipeline combined with tracking-based\nmethods and tracking-free methods. Given a monocular RGB sequence, we track the\nparametric human model in the whole sequence, the points (voxels) corresponding\nto the target frame are warped to reference frames by the parametric body\nmotion. Guided by the geometry priors of the parametric body and spatially\naligned features from RGB sequence, the robust implicit surface is fused.\nMoreover, a multi-frame transformer (MFT) and a self-supervised warp refinement\nmodule are integrated to the framework to relax the requirements of parametric\nbody and help to deal with very loose cloth. Compared with previous works, our\nCrossHuman enables high-fidelity geometry details and texture in both visible\nand invisible regions and improves the accuracy of the human reconstruction\neven under estimated inaccurate parametric human models. The experiments\ndemonstrate that our method achieves state-of-the-art (SOTA) performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Latent Spaces of Generative Models for Medical Images using Unsupervised Methods. (arXiv:2207.09740v1 [eess.IV])","link":"http://arxiv.org/abs/2207.09740","description":"<p>Generative models such as Generative Adversarial Networks (GANs) and\nVariational Autoencoders (VAEs) play an increasingly important role in medical\nimage analysis. The latent spaces of these models often show semantically\nmeaningful directions corresponding to human-interpretable image\ntransformations. However, until now, their exploration for medical images has\nbeen limited due to the requirement of supervised data. Several methods for\nunsupervised discovery of interpretable directions in GAN latent spaces have\nshown interesting results on natural images. This work explores the potential\nof applying these techniques on medical images by training a GAN and a VAE on\nthoracic CT scans and using an unsupervised method to discover interpretable\ndirections in the resulting latent space. We find several directions\ncorresponding to non-trivial image transformations, such as rotation or breast\nsize. Furthermore, the directions show that the generative models capture 3D\nstructure despite being presented only with 2D data. The results show that\nunsupervised methods to discover interpretable directions in GANs generalize to\nVAEs and can be applied to medical images. This opens a wide array of future\nwork using these methods in medical image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schon_J/0/1/0/all/0/1\">Julian Sch&#xf6;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Affect Analysis: Learning from Synthetic Data & Multi-Task Learning Challenges. (arXiv:2207.09748v1 [cs.LG])","link":"http://arxiv.org/abs/2207.09748","description":"<p>Facial affect analysis remains a challenging task with its setting\ntransitioned from lab-controlled to in-the-wild situations. In this paper, we\npresent novel frameworks to handle the two challenges in the 4th Affective\nBehavior Analysis In-The-Wild (ABAW) competition: i) Multi-Task-Learning (MTL)\nChallenge and ii) Learning from Synthetic Data (LSD) Challenge. For MTL\nchallenge, we adopt the SMM-EmotionNet with a better ensemble strategy of\nfeature vectors. For LSD challenge, we propose respective methods to combat the\nproblems of single labels, imbalanced distribution, fine-tuning limitations,\nand choice of model architectures. Experimental results on the official\nvalidation sets from the competition demonstrated that our proposed approaches\noutperformed baselines by a large margin. The code is available at\nhttps://github.com/sylyoung/ABAW4-HUST-ANT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huanyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yingjie Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiajiong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jingting Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition. (arXiv:2207.09759v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09759","description":"<p>A primary challenge faced in few-shot action recognition is inadequate video\ndata for training. To address this issue, current methods in this field mainly\nfocus on devising algorithms at the feature level while little attention is\npaid to processing input video data. Moreover, existing frame sampling\nstrategies may omit critical action information in temporal and spatial\ndimensions, which further impacts video utilization efficiency. In this paper,\nwe propose a novel video frame sampler for few-shot action recognition to\naddress this issue, where task-specific spatial-temporal frame sampling is\nachieved via a temporal selector (TS) and a spatial amplifier (SA).\nSpecifically, our sampler first scans the whole video at a small computational\ncost to obtain a global perception of video frames. The TS plays its role in\nselecting top-T frames that contribute most significantly and subsequently. The\nSA emphasizes the discriminative information of each frame by amplifying\ncritical regions with the guidance of saliency maps. We further adopt\ntask-adaptive learning to dynamically adjust the sampling strategy according to\nthe episode task at hand. Both the implementations of TS and SA are\ndifferentiable for end-to-end optimization, facilitating seamless integration\nof our proposed sampler with most few-shot action recognition methods.\nExtensive experiments show a significant boost in the performances on various\nbenchmarks including long-term videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huabin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1\">Weixian Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D LiDAR Segmentation. (arXiv:2207.09763v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09763","description":"<p>3D point cloud semantic segmentation is fundamental for autonomous driving.\nMost approaches in the literature neglect an important aspect, i.e., how to\ndeal with domain shift when handling dynamic scenes. This can significantly\nhinder the navigation capabilities of self-driving vehicles. This paper\nadvances the state of the art in this research field. Our first contribution\nconsists in analysing a new unexplored scenario in point cloud segmentation,\nnamely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We\nexperimentally show that state-of-the-art methods have a rather limited ability\nto adapt pre-trained deep network models to unseen domains in an online manner.\nOur second contribution is an approach that relies on adaptive self-training\nand geometric-feature propagation to adapt a pre-trained source model online\nwithout requiring either source data or target labels. Our third contribution\nis to study SF-OUDA in a challenging setup where source data is synthetic and\ntarget data is point clouds captured in the real world. We use the recent\nSynLiDAR dataset as a synthetic source and introduce two new synthetic (source)\ndatasets, which can stimulate future synthetic-to-real autonomous driving\nresearch. Our experiments show the effectiveness of our segmentation approach\non thousands of real-world point clouds. Code and synthetic datasets are\navailable at https://github.com/saltoricristiano/gipso-sfouda.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saltori_C/0/1/0/all/0/1\">Cristiano Saltori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivosheev_E/0/1/0/all/0/1\">Evgeny Krivosheev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galasso_F/0/1/0/all/0/1\">Fabio Galasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1\">Fabio Poiesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborating Domain-shared and Target-specific Feature Clustering for Cross-domain 3D Action Recognition. (arXiv:2207.09767v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09767","description":"<p>In this work, we consider the problem of cross-domain 3D action recognition\nin the open-set setting, which has been rarely explored before. Specifically,\nthere is a source domain and a target domain that contain the skeleton\nsequences with different styles and categories, and our purpose is to cluster\nthe target data by utilizing the labeled source data and unlabeled target data.\nFor such a challenging task, this paper presents a novel approach dubbed CoDT\nto collaboratively cluster the domain-shared features and target-specific\nfeatures. CoDT consists of two parallel branches. One branch aims to learn\ndomain-shared features with supervised learning in the source domain, while the\nother is to learn target-specific features using contrastive learning in the\ntarget domain. To cluster the features, we propose an online clustering\nalgorithm that enables simultaneous promotion of robust pseudo label generation\nand feature clustering. Furthermore, to leverage the complementarity of\ndomain-shared features and target-specific features, we propose a novel\ncollaborative clustering strategy to enforce pair-wise relationship consistency\nbetween the two branches. We conduct extensive experiments on multiple\ncross-domain 3D action recognition datasets, and the results demonstrate the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qinying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Convolutional Neural Network with Meta Feature Learning for Abnormality Detection in Wireless Capsule Endoscopy Images. (arXiv:2207.09769v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09769","description":"<p>Wireless Capsule Endoscopy is one of the most advanced non-invasive methods\nfor the examination of gastrointestinal tracts. An intelligent computer-aided\ndiagnostic system for detecting gastrointestinal abnormalities like polyp,\nbleeding, inflammation, etc. is highly exigent in wireless capsule endoscopy\nimage analysis. Abnormalities greatly differ in their shape, size, color, and\ntexture, and some appear to be visually similar to normal regions. This poses a\nchallenge in designing a binary classifier due to intra-class variations. In\nthis study, a hybrid convolutional neural network is proposed for abnormality\ndetection that extracts a rich pool of meaningful features from wireless\ncapsule endoscopy images using a variety of convolution operations. It consists\nof three parallel convolutional neural networks, each with a distinctive\nfeature learning capability. The first network utilizes depthwise separable\nconvolution, while the second employs cosine normalized convolution operation.\nA novel meta-feature extraction mechanism is introduced in the third network,\nto extract patterns from the statistical information drawn over the features\ngenerated from the first and second networks and its own previous layer. The\nnetwork trio effectively handles intra-class variance and efficiently detects\ngastrointestinal abnormalities. The proposed hybrid convolutional neural\nnetwork model is trained and tested on two widely used publicly available\ndatasets. The test results demonstrate that the proposed model outperforms six\nstate-of-the-art methods with 97\\% and 98\\% classification accuracy on KID and\nKvasir-Capsule datasets respectively. Cross dataset evaluation results also\ndemonstrate the generalization performance of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Samir Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seal_A/0/1/0/all/0/1\">Ayan Seal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Aparajita Ojha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localization supervision of chest x-ray classifiers using label-specific eye-tracking annotation. (arXiv:2207.09771v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09771","description":"<p>Convolutional neural networks (CNNs) have been successfully applied to chest\nx-ray (CXR) images. Moreover, annotated bounding boxes have been shown to\nimprove the interpretability of a CNN in terms of localizing abnormalities.\nHowever, only a few relatively small CXR datasets containing bounding boxes are\navailable, and collecting them is very costly. Opportunely, eye-tracking (ET)\ndata can be collected in a non-intrusive way during the clinical workflow of a\nradiologist. We use ET data recorded from radiologists while dictating CXR\nreports to train CNNs. We extract snippets from the ET data by associating them\nwith the dictation of keywords and use them to supervise the localization of\nabnormalities. We show that this method improves a model's interpretability\nwithout impacting its image-level classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lanfredi_R/0/1/0/all/0/1\">Ricardo Bigolin Lanfredi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drivable Volumetric Avatars using Texel-Aligned Features. (arXiv:2207.09774v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09774","description":"<p>Photorealistic telepresence requires both high-fidelity body modeling and\nfaithful driving to enable dynamically synthesized appearance that is\nindistinguishable from reality. In this work, we propose an end-to-end\nframework that addresses two core challenges in modeling and driving full-body\navatars of real people. One challenge is driving an avatar while staying\nfaithful to details and dynamics that cannot be captured by a global\nlow-dimensional parameterization such as body pose. Our approach supports\ndriving of clothed avatars with wrinkles and motion that a real driving\nperformer exhibits beyond the training corpus. Unlike existing global state\nrepresentations or non-parametric screen-space approaches, we introduce\ntexel-aligned features -- a localised representation which can leverage both\nthe structural prior of a skeleton-based parametric model and observed sparse\nimage signals at the same time. Another challenge is modeling a temporally\ncoherent clothed avatar, which typically requires precise surface tracking. To\ncircumvent this, we propose a novel volumetric avatar representation by\nextending mixtures of volumetric primitives to articulated objects. By\nexplicitly incorporating articulation, our approach naturally generalizes to\nunseen poses. We also introduce a localized viewpoint conditioning, which leads\nto a large improvement in generalization of view-dependent appearance. The\nproposed volumetric representation does not require high-quality mesh tracking\nas a prerequisite and brings significant quality improvements compared to\nmesh-based counterparts. In our experiments, we carefully examine our design\nchoices and demonstrate the efficacy of our approach, outperforming the\nstate-of-the-art methods on challenging driving scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remelli_E/0/1/0/all/0/1\">Edoardo Remelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1\">Tomas Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shih-En Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kaiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhe Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Jason Saragih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Practical Scenario of Open-set Object Detection: Open at Category Level and Closed at Super-category Level. (arXiv:2207.09775v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09775","description":"<p>Open-set object detection (OSOD) has recently attracted considerable\nattention. It is to detect unknown objects while correctly\ndetecting/classifying known objects. We first point out that the scenario of\nOSOD considered in recent studies, which considers an unlimited variety of\nunknown objects similar to open-set recognition (OSR), has a fundamental issue.\nThat is, we cannot determine what to detect and what not for such unlimited\nunknown objects, which is necessary for detection tasks. This issue leads to\ndifficulty with the evaluation of methods' performance on unknown object\ndetection. We then introduce a novel scenario of OSOD, which deals with only\nunknown objects that share the super-category with known objects. It has many\nreal-world applications, e.g., detecting an increasing number of fine-grained\nobjects. This new setting is free from the above issue and evaluation\ndifficulty. Moreover, it makes detecting unknown objects more realistic owing\nto the visual similarity between known and unknown objects. We show through\nexperimental results that a simple method based on the uncertainty of class\nprediction from standard detectors outperforms the current state-of-the-art\nOSOD methods tested in the previous setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosoya_Y/0/1/0/all/0/1\">Yusuke Hosoya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AU-Supervised Convolutional Vision Transformers for Synthetic Facial Expression Recognition. (arXiv:2207.09777v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09777","description":"<p>The paper describes our proposed methodology for the six basic expression\nclassification track of Affective Behavior Analysis in-the-wild (ABAW)\nCompetition 2022. In Learing from Synthetic Data(LSD) task, facial expression\nrecognition (FER) methods aim to learn the representation of expression from\nthe artificially generated data and generalise to real data. Because of the\nambiguous of the synthetic data and the objectivity of the facial Action Unit\n(AU), we resort to the AU information for performance boosting, and make\ncontributions as follows. First, to adapt the model to synthetic scenarios, we\nuse the knowledge from pre-trained large-scale face recognition data. Second,\nwe propose a conceptually-new framework, termed as AU-Supervised Convolutional\nVision Transformers (AU-CVT), which clearly improves the performance of FER by\njointly training auxiliary datasets with AU or pseudo AU labels. Our AU-CVT\nachieved F1 score as $0.6863$, accuracy as $0.7433$ on the validation set. The\nsource code of our work is publicly available online:\nhttps://github.com/msy1412/ABAW4\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shuyi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR Segmentation. (arXiv:2207.09778v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09778","description":"<p>3D LiDAR semantic segmentation is fundamental for autonomous driving. Several\nUnsupervised Domain Adaptation (UDA) methods for point cloud data have been\nrecently proposed to improve model generalization for different sensors and\nenvironments. Researchers working on UDA problems in the image domain have\nshown that sample mixing can mitigate domain shift. We propose a new approach\nof sample mixing for point cloud UDA, namely Compositional Semantic Mix\n(CoSMix), the first UDA approach for point cloud segmentation based on sample\nmixing. CoSMix consists of a two-branch symmetric network that can process\nlabelled synthetic data (source) and real-world unlabelled point clouds\n(target) concurrently. Each branch operates on one domain by mixing selected\npieces of data from the other one, and by using the semantic information\nderived from source labels and target pseudo-labels. We evaluate CoSMix on two\nlarge-scale datasets, showing that it outperforms state-of-the-art methods by a\nlarge margin. Our code is available at\nhttps://github.com/saltoricristiano/cosmix-uda.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saltori_C/0/1/0/all/0/1\">Cristiano Saltori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galasso_F/0/1/0/all/0/1\">Fabio Galasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1\">Fabio Poiesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceFormer: Scale-aware Blind Face Restoration with Transformers. (arXiv:2207.09790v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09790","description":"<p>Blind face restoration usually encounters with diverse scale face inputs,\nespecially in the real world. However, most of the current works support\nspecific scale faces, which limits its application ability in real-world\nscenarios. In this work, we propose a novel scale-aware blind face restoration\nframework, named FaceFormer, which formulates facial feature restoration as\nscale-aware transformation. The proposed Facial Feature Up-sampling (FFUP)\nmodule dynamically generates upsampling filters based on the original\nscale-factor priors, which facilitate our network to adapt to arbitrary face\nscales. Moreover, we further propose the facial feature embedding (FFE) module\nwhich leverages transformer to hierarchically extract diversity and robustness\nof facial latent. Thus, our FaceFormer achieves fidelity and robustness\nrestored faces, which possess realistic and symmetrical details of facial\ncomponents. Extensive experiments demonstrate that our proposed method trained\nwith synthetic dataset generalizes better to a natural low quality images than\ncurrent state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aijin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Industrial Anomaly Detection via Pattern Generative and Contrastive Networks. (arXiv:2207.09792v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09792","description":"<p>It is hard to collect enough flaw images for training deep learning network\nin industrial production. Therefore, existing industrial anomaly detection\nmethods prefer to use CNN-based unsupervised detection and localization network\nto achieve this task. However, these methods always fail when there are\nvarieties happened in new signals since traditional end-to-end networks suffer\nbarriers of fitting nonlinear model in high-dimensional space. Moreover, they\nhave a memory library by clustering the feature of normal images essentially,\nwhich cause it is not robust to texture change. To this end, we propose the\nVision Transformer based (VIT-based) unsupervised anomaly detection network. It\nutilizes a hierarchical task learning and human experience to enhance its\ninterpretability. Our network consists of pattern generation and comparison\nnetworks. Pattern generation network uses two VIT-based encoder modules to\nextract the feature of two consecutive image patches, then uses VIT-based\ndecoder module to learn the human designed style of these features and predict\nthe third image patch. After this, we use the Siamese-based network to compute\nthe similarity of the generation image patch and original image patch. Finally,\nwe refine the anomaly localization by the bi-directional inference strategy.\nComparison experiments on public dataset MVTec dataset show our method achieves\n99.8% AUC, which surpasses previous state-of-the-art methods. In addition, we\ngive a qualitative illustration on our own leather and cloth datasets. The\naccurate segment results strongly prove the accuracy of our method in anomaly\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yimin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Shiguo Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EASNet: Searching Elastic and Accurate Network Architecture for Stereo Matching. (arXiv:2207.09796v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09796","description":"<p>Recent advanced studies have spent considerable human efforts on optimizing\nnetwork architectures for stereo matching but hardly achieved both high\naccuracy and fast inference speed. To ease the workload in network design,\nneural architecture search (NAS) has been applied with great success to various\nsparse prediction tasks, such as image classification and object detection.\nHowever, existing NAS studies on the dense prediction task, especially stereo\nmatching, still cannot be efficiently and effectively deployed on devices of\ndifferent computing capabilities. To this end, we propose to train an elastic\nand accurate network for stereo matching (EASNet) that supports various 3D\narchitectural settings on devices with different computing capabilities. Given\nthe deployment latency constraint on the target device, we can quickly extract\na sub-network from the full EASNet without additional training while the\naccuracy of the sub-network can still be maintained. Extensive experiments show\nthat our EASNet outperforms both state-of-the-art human-designed and NAS-based\narchitectures on Scene Flow and MPI Sintel datasets in terms of model accuracy\nand inference speed. Particularly, deployed on an inference GPU, EASNet\nachieves a new SOTA 0.73 EPE on the Scene Flow dataset with 100 ms, which is\n4.5$\\times$ faster than LEAStereo with a better quality model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaohuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaiyong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaowen Chu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Transformer for Automatic 3D Annotation and Object Detection. (arXiv:2207.09805v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09805","description":"<p>Despite a growing number of datasets being collected for training 3D object\ndetection models, significant human effort is still required to annotate 3D\nboxes on LiDAR scans. To automate the annotation and facilitate the production\nof various customized datasets, we propose an end-to-end multimodal transformer\n(MTrans) autolabeler, which leverages both LiDAR scans and images to generate\nprecise 3D box annotations from weak 2D bounding boxes. To alleviate the\npervasive sparsity problem that hinders existing autolabelers, MTrans densifies\nthe sparse point clouds by generating new 3D points based on 2D image\ninformation. With a multi-task design, MTrans segments the\nforeground/background, densifies LiDAR point clouds, and regresses 3D boxes\nsimultaneously. Experimental results verify the effectiveness of the MTrans for\nimproving the quality of the generated labels. By enriching the sparse point\nclouds, our method achieves 4.48\\% and 4.03\\% better 3D AP on KITTI moderate\nand hard samples, respectively, versus the state-of-the-art autolabeler. MTrans\ncan also be extended to improve the accuracy for 3D object detection, resulting\nin a remarkable 89.45\\% AP on KITTI hard samples. Codes are at\n\\url{https://github.com/Cliu2/MTrans}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xiaoyan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Binxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_E/0/1/0/all/0/1\">Edmund Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Siew-Chong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing. (arXiv:2207.09812v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09812","description":"<p>Machine learning is transforming the video editing industry. Recent advances\nin computer vision have leveled-up video editing tasks such as intelligent\nreframing, rotoscoping, color grading, or applying digital makeups. However,\nmost of the solutions have focused on video manipulation and VFX. This work\nintroduces the Anatomy of Video Editing, a dataset, and benchmark, to foster\nresearch in AI-assisted video editing. Our benchmark suite focuses on video\nediting tasks, beyond visual effects, such as automatic footage organization\nand assisted video assembling. To enable research on these fronts, we annotate\nmore than 1.5M tags, with relevant concepts to cinematography, from 196176\nshots sampled from movie scenes. We establish competitive baseline methods and\ndetailed analyses for each of the tasks. We hope our work sparks innovative\nresearch towards underexplored areas of AI-assisted video editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Argaw_D/0/1/0/all/0/1\">Dawit Mureja Argaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodson_M/0/1/0/all/0/1\">Markus Woodson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis. (arXiv:2207.09814v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09814","description":"<p>In this paper, we present NUWA-Infinity, a generative model for infinite\nvisual synthesis, which is defined as the task of generating arbitrarily-sized\nhigh-resolution images or long-duration videos. An autoregressive over\nautoregressive generation mechanism is proposed to deal with this variable-size\ngeneration task, where a global patch-level autoregressive model considers the\ndependencies between patches, and a local token-level autoregressive model\nconsiders dependencies between visual tokens within each patch. A Nearby\nContext Pool (NCP) is introduced to cache-related patches already generated as\nthe context for the current patch being generated, which can significantly save\ncomputation costs without sacrificing patch-level dependency modeling. An\nArbitrary Direction Controller (ADC) is used to decide suitable generation\norders for different visual synthesis tasks and learn order-aware positional\nembeddings. Compared to DALL-E, Imagen and Parti, NUWA-Infinity can generate\nhigh-resolution images with arbitrary sizes and support long-duration video\ngeneration additionally. Compared to NUWA, which also covers images and videos,\nNUWA-Infinity has superior visual synthesis capabilities in terms of resolution\nand variable-size generation. The GitHub link is\nhttps://github.com/microsoft/NUWA. The homepage link is\nhttps://nuwa-infinity.microsoft.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuejian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIF: United Neural Implicit Functions for Clothed Human Reconstruction and Animation. (arXiv:2207.09835v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09835","description":"<p>We propose united implicit functions (UNIF), a part-based method for clothed\nhuman reconstruction and animation with raw scans and skeletons as the input.\nPrevious part-based methods for human reconstruction rely on ground-truth part\nlabels from SMPL and thus are limited to minimal-clothed humans. In contrast,\nour method learns to separate parts from body motions instead of part\nsupervision, thus can be extended to clothed humans and other articulated\nobjects. Our Partition-from-Motion is achieved by a bone-centered\ninitialization, a bone limit loss, and a section normal loss that ensure stable\npart division even when the training poses are limited. We also present a\nminimal perimeter loss for SDF to suppress extra surfaces and part overlapping.\nAnother core of our method is an adjacent part seaming algorithm that produces\nnon-rigid deformations to maintain the connection between parts which\nsignificantly relieves the part-based artifacts. Under this algorithm, we\nfurther propose \"Competing Parts\", a method that defines blending weights by\nthe relative position of a point to bones instead of the absolute position,\navoiding the generalization problem of neural implicit functions with inverse\nLBS (linear blend skinning). We demonstrate the effectiveness of our method by\nclothed human body reconstruction and animation on the CAPE and the ClothSeq\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shenhan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiale Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer. (arXiv:2207.09840v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09840","description":"<p>Most existing methods view makeup transfer as transferring color\ndistributions of different facial regions and ignore details such as eye\nshadows and blushes. Besides, they only achieve controllable transfer within\npredefined fixed regions. This paper emphasizes the transfer of makeup details\nand steps towards more flexible controls. To this end, we propose Exquisite and\nlocally editable GAN for makeup transfer (EleGANt). It encodes facial\nattributes into pyramidal feature maps to preserves high-frequency information.\nIt uses attention to extract makeup features from the reference and adapt them\nto the source face, and we introduce a novel Sow-Attention Module that applies\nattention within shifted overlapped windows to reduce the computational cost.\nMoreover, EleGANt is the first to achieve customized local editing within\narbitrary areas by corresponding editing on the feature maps. Extensive\nexperiments demonstrate that EleGANt generates realistic makeup faces with\nexquisite details and achieves state-of-the-art performance. The code is\navailable at https://github.com/Chenyu-Yang-2000/EleGANt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanrong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yingqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v1 [cs.CL])","link":"http://arxiv.org/abs/2207.09847","description":"<p>For human children as well as machine learning systems, a key challenge in\nlearning a word is linking the word to the visual phenomena it describes. We\nexplore this aspect of word learning by using the performance of computer\nvision systems as a proxy for the difficulty of learning a word from visual\ncues. We show that the age at which children acquire different categories of\nwords is predicted by the performance of visual classification and captioning\nsystems, over and above the expected effects of word frequency. The performance\nof the computer vision systems is related to human judgments of the\nconcreteness of words, supporting the idea that we are capturing the\nrelationship between words and visual phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1\">Sunayana Rane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nencheva_M/0/1/0/all/0/1\">Mira L. Nencheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_Williams_C/0/1/0/all/0/1\">Casey Lew-Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Embedded Monocular Vision Approach for Ground-Aware Objects Detection and Position Estimation. (arXiv:2207.09851v1 [cs.RO])","link":"http://arxiv.org/abs/2207.09851","description":"<p>In the RoboCup Small Size League (SSL), teams are encouraged to propose\nsolutions for executing basic soccer tasks inside the SSL field using only\nembedded sensing information. Thus, this work proposes an embedded monocular\nvision approach for detecting objects and estimating relative positions inside\nthe soccer field. Prior knowledge from the environment is exploited by assuming\nobjects lay on the ground, and the onboard camera has its position fixed on the\nrobot. We implemented the proposed method on an NVIDIA Jetson Nano and employed\nSSD MobileNet v2 for 2D Object Detection with TensorRT optimization, detecting\nballs, robots, and goals with distances up to 3.5 meters. Ball localization\nevaluation shows that the proposed solution overcomes the currently used SSL\nvision system for positions closer than 1 meter to the onboard camera with a\nRoot Mean Square Error of 14.37 millimeters. In addition, the proposed method\nachieves real-time performance with an average processing speed of 30 frames\nper second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melo_J/0/1/0/all/0/1\">Jo&#xe3;o G. Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barros_E/0/1/0/all/0/1\">Edna Barros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Everything is There in Latent Space: Attribute Editing and Attribute Style Manipulation by StyleGAN Latent Space Exploration. (arXiv:2207.09855v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09855","description":"<p>Unconstrained Image generation with high realism is now possible using recent\nGenerative Adversarial Networks (GANs). However, it is quite challenging to\ngenerate images with a given set of attributes. Recent methods use style-based\nGAN models to perform image editing by leveraging the semantic hierarchy\npresent in the layers of the generator. We present Few-shot Latent-based\nAttribute Manipulation and Editing (FLAME), a simple yet effective framework to\nperform highly controlled image editing by latent space manipulation.\nSpecifically, we estimate linear directions in the latent space (of a\npre-trained StyleGAN) that controls semantic attributes in the generated image.\nIn contrast to previous methods that either rely on large-scale attribute\nlabeled datasets or attribute classifiers, FLAME uses minimal supervision of a\nfew curated image pairs to estimate disentangled edit directions. FLAME can\nperform both individual and sequential edits with high precision on a diverse\nset of images while preserving identity. Further, we propose a novel task of\nAttribute Style Manipulation to generate diverse styles for attributes such as\neyeglass and hair. We first encode a set of synthetic images of the same\nidentity but having different attribute styles in the latent space to estimate\nan attribute style manifold. Sampling a new latent from this manifold will\nresult in a new attribute style in the generated image. We propose a novel\nsampling method to sample latent from the manifold, enabling us to generate a\ndiverse set of attribute styles beyond the styles present in the training set.\nFLAME can generate diverse attribute styles in a disentangled manner. We\nillustrate the superior performance of FLAME against previous image editing\nmethods by extensive qualitative and quantitative comparisons. FLAME also\ngeneralizes well on multiple datasets such as cars and churches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parihar_R/0/1/0/all/0/1\">Rishubh Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhiman_A/0/1/0/all/0/1\">Ankit Dhiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmali_T/0/1/0/all/0/1\">Tejan Karmali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Stability of Deep Image Quality Assessment With Respect to Image Scaling. (arXiv:2207.09856v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09856","description":"<p>Image quality assessment (IQA) is a fundamental metric for image processing\ntasks (e.g., compression). With full-reference IQAs, traditional IQAs, such as\nPSNR and SSIM, have been used. Recently, IQAs based on deep neural networks\n(deep IQAs), such as LPIPS and DISTS, have also been used. It is known that\nimage scaling is inconsistent among deep IQAs, as some perform down-scaling as\npre-processing, whereas others instead use the original image size. In this\npaper, we show that the image scale is an influential factor that affects deep\nIQA performance. We comprehensively evaluate four deep IQAs on the same five\ndatasets, and the experimental results show that image scale significantly\ninfluences IQA performance. We found that the most appropriate image scale is\noften neither the default nor the original size, and the choice differs\ndepending on the methods and datasets used. We visualized the stability and\nfound that PieAPP is the most stable among the four deep IQAs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsubota_K/0/1/0/all/0/1\">Koki Tsubota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akutsu_H/0/1/0/all/0/1\">Hiroaki Akutsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1\">Kiyoharu Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete-Constrained Regression for Local Counting Models. (arXiv:2207.09865v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09865","description":"<p>Local counts, or the number of objects in a local area, is a continuous value\nby nature. Yet recent state-of-the-art methods show that formulating counting\nas a classification task performs better than regression. Through a series of\nexperiments on carefully controlled synthetic data, we show that this\ncounter-intuitive result is caused by imprecise ground truth local counts.\nFactors such as biased dot annotations and incorrectly matched Gaussian kernels\nused to generate ground truth counts introduce deviations from the true local\ncounts. Standard continuous regression is highly sensitive to these errors,\nexplaining the performance gap between classification and regression. To\nmitigate the sensitivity, we loosen the regression formulation from a\ncontinuous scale to a discrete ordering and propose a novel\ndiscrete-constrained (DC) regression. Applied to crowd counting, DC-regression\nis more accurate than both classification and standard regression on three\npublic benchmarks. A similar advantage also holds for the age estimation task,\nverifying the overall effectiveness of DC-regression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haipeng Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Angela Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Mixture of Experts Learning for Generalizable Face Anti-Spoofing. (arXiv:2207.09868v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09868","description":"<p>With various face presentation attacks emerging continually, face\nanti-spoofing (FAS) approaches based on domain generalization (DG) have drawn\ngrowing attention. Existing DG-based FAS approaches always capture the\ndomain-invariant features for generalizing on the various unseen domains.\nHowever, they neglect individual source domains' discriminative characteristics\nand diverse domain-specific information of the unseen domains, and the trained\nmodel is not sufficient to be adapted to various unseen domains. To address\nthis issue, we propose an Adaptive Mixture of Experts Learning (AMEL)\nframework, which exploits the domain-specific information to adaptively\nestablish the link among the seen source domains and unseen target domains to\nfurther improve the generalization. Concretely, Domain-Specific Experts (DSE)\nare designed to investigate discriminative and unique domain-specific features\nas a complement to common domain-invariant features. Moreover, Dynamic Expert\nAggregation (DEA) is proposed to adaptively aggregate the complementary\ninformation of each source expert based on the domain relevance to the unseen\ntarget domain. And combined with meta-learning, these modules work\ncollaboratively to adaptively aggregate meaningful domain-specific information\nfor the various unseen target domains. Extensive experiments and visualizations\ndemonstrate the effectiveness of our method against the state-of-the-art\ncompetitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Neural Network Training Method for Autonomous Driving Using Semi-Pseudo-Labels and 3D Data Augmentations. (arXiv:2207.09869v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09869","description":"<p>Training neural networks to perform 3D object detection for autonomous\ndriving requires a large amount of diverse annotated data. However, obtaining\ntraining data with sufficient quality and quantity is expensive and sometimes\nimpossible due to human and sensor constraints. Therefore, a novel solution is\nneeded for extending current training methods to overcome this limitation and\nenable accurate 3D object detection. Our solution for the above-mentioned\nproblem combines semi-pseudo-labeling and novel 3D augmentations. For\ndemonstrating the applicability of the proposed method, we have designed a\nconvolutional neural network for 3D object detection which can significantly\nincrease the detection range in comparison with the training data distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matuszka_T/0/1/0/all/0/1\">Tamas Matuszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozma_D/0/1/0/all/0/1\">Daniel Kozma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Samples are at Large: Leveraging Hard-distance Elastic Loss for Re-identification. (arXiv:2207.09884v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09884","description":"<p>We present a Momentum Re-identification (MoReID) framework that can leverage\na very large number of negative samples in training for general\nre-identification task. The design of this framework is inspired by Momentum\nContrast (MoCo), which uses a dictionary to store current and past batches to\nbuild a large set of encoded samples. As we find it less effective to use past\npositive samples which may be highly inconsistent to the encoded feature\nproperty formed with the current positive samples, MoReID is designed to use\nonly a large number of negative samples stored in the dictionary. However, if\nwe train the model using the widely used Triplet loss that uses only one sample\nto represent a set of positive/negative samples, it is hard to effectively\nleverage the enlarged set of negative samples acquired by the MoReID framework.\nTo maximize the advantage of using the scaled-up negative sample set, we newly\nintroduce Hard-distance Elastic loss (HE loss), which is capable of using more\nthan one hard sample to represent a large number of samples. Our experiments\ndemonstrate that a large number of negative samples provided by MoReID\nframework can be utilized at full capacity only with the HE loss, achieving the\nstate-of-the-art accuracy on three re-ID benchmarks, VeRi-776, Market-1501, and\nVeRi-Wild.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungtae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eum_S/0/1/0/all/0/1\">Sungmin Eum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heesung Kwon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Labeling instructions matter in biomedical image analysis. (arXiv:2207.09899v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09899","description":"<p>Biomedical image analysis algorithm validation depends on high-quality\nannotation of reference datasets, for which labeling instructions are key.\nDespite their importance, their optimization remains largely unexplored. Here,\nwe present the first systematic study of labeling instructions and their impact\non annotation quality in the field. Through comprehensive examination of\nprofessional practice and international competitions registered at the MICCAI\nSociety, we uncovered a discrepancy between annotators' needs for labeling\ninstructions and their current quality and availability. Based on an analysis\nof 14,040 images annotated by 156 annotators from four professional companies\nand 708 Amazon Mechanical Turk (MTurk) crowdworkers using instructions with\ndifferent information density levels, we further found that including exemplary\nimages significantly boosts annotation performance compared to text-only\ndescriptions, while solely extending text descriptions does not. Finally,\nprofessional annotators constantly outperform MTurk crowdworkers. Our study\nraises awareness for the need of quality standards in biomedical image analysis\nlabeling instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radsch_T/0/1/0/all/0/1\">Tim R&#xe4;dsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weru_V/0/1/0/all/0/1\">Vivienn Weru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreck_N/0/1/0/all/0/1\">Nicholas Schreck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavur_A/0/1/0/all/0/1\">A. Emre Kavur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pekdemir_B/0/1/0/all/0/1\">B&#xfc;nyamin Pekdemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_T/0/1/0/all/0/1\">Tobias Ro&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_Schneider_A/0/1/0/all/0/1\">Annette Kopp-Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1\">Lena Maier-Hein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A note on the variation of geometric functionals. (arXiv:2207.09915v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09915","description":"<p>Calculus of Variation combined with Differential Geometry as tools of\nmodelling and solving problems in image processing and computer vision were\nintroduced in the late 80's and the 90s of the 20th century. The beginning of\nan extensive work in these directions was marked by works such as Geodesic\nActive Contours (GAC), the Beltrami framework, level set method of Osher and\nSethian the works of Charpiat et al. and the works by Chan and Vese to name\njust a few. In many cases the optimization of these functional are done by the\ngradient descent method via the calculation of the Euler-Lagrange equations.\nStraightforward use of the resulted EL equations in the gradient descent scheme\nleads to non-geometric and in some cases non sensical equations. It is\ncostumary to modify these EL equations or even the functional itself in order\nto obtain geometric and/or sensical equations. The aim of this note is to point\nto the correct way to derive the EL and the gradient descent equations such\nthat the resulted gradient descent equation is geometric and makes sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sochen_N/0/1/0/all/0/1\">Nir Sochen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Framework for Few-shot Skeleton-based Temporal Action Segmentation. (arXiv:2207.09925v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09925","description":"<p>Temporal action segmentation (TAS) aims to classify and locate actions in the\nlong untrimmed action sequence. With the success of deep learning, many deep\nmodels for action segmentation have emerged. However, few-shot TAS is still a\nchallenging problem. This study proposes an efficient framework for the\nfew-shot skeleton-based TAS, including a data augmentation method and an\nimproved model. The data augmentation approach based on motion interpolation is\npresented here to solve the problem of insufficient data, and can increase the\nnumber of samples significantly by synthesizing action sequences. Besides, we\nconcatenate a Connectionist Temporal Classification (CTC) layer with a network\ndesigned for skeleton-based TAS to obtain an optimized model. Leveraging CTC\ncan enhance the temporal alignment between prediction and ground truth and\nfurther improve the segment-wise metrics of segmentation results. Extensive\nexperiments on both public and self-constructed datasets, including two\nsmall-scale datasets and one large-scale dataset, show the effectiveness of two\nproposed methods in improving the performance of the few-shot skeleton-based\nTAS task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Leiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViGAT: Bottom-up event recognition and explanation in video using factorized graph attention network. (arXiv:2207.09927v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09927","description":"<p>In this paper a pure-attention bottom-up approach, called ViGAT, that\nutilizes an object detector together with a Vision Transformer (ViT) backbone\nnetwork to derive object and frame features, and a head network to process\nthese features for the task of event recognition and explanation in video, is\nproposed. The ViGAT head consists of graph attention network (GAT) blocks\nfactorized along the spatial and temporal dimensions in order to capture\neffectively both local and long-term dependencies between objects or frames.\nMoreover, using the weighted in-degrees (WiDs) derived from the adjacency\nmatrices at the various GAT blocks, we show that the proposed architecture can\nidentify the most salient objects and frames that explain the decision of the\nnetwork. A comprehensive evaluation study is performed, demonstrating that the\nproposed approach provides state-of-the-art results on three large, publicly\navailable video datasets (FCVID, Mini-Kinetics, ActivityNet).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gkalelis_N/0/1/0/all/0/1\">Nikolaos Gkalelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daskalakis_D/0/1/0/all/0/1\">Dimitrios Daskalakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezaris_V/0/1/0/all/0/1\">Vasileios Mezaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Landmark-based Stent Tracking in X-ray Fluoroscopy. (arXiv:2207.09933v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09933","description":"<p>In clinical procedures of angioplasty (i.e., open clogged coronary arteries),\ndevices such as balloons and stents need to be placed and expanded in arteries\nunder the guidance of X-ray fluoroscopy. Due to the limitation of X-ray dose,\nthe resulting images are often noisy. To check the correct placement of these\ndevices, typically multiple motion-compensated frames are averaged to enhance\nthe view. Therefore, device tracking is a necessary procedure for this purpose.\nEven though angioplasty devices are designed to have radiopaque markers for the\nease of tracking, current methods struggle to deliver satisfactory results due\nto the small marker size and complex scenes in angioplasty. In this paper, we\npropose an end-to-end deep learning framework for single stent tracking, which\nconsists of three hierarchical modules: U-Net based landmark detection, ResNet\nbased stent proposal and feature extraction, and graph convolutional neural\nnetwork (GCN) based stent tracking that temporally aggregates both spatial\ninformation and appearance features. The experiments show that our method\nperforms significantly better in detection compared with the state-of-the-art\npoint-based tracking models. In addition, its fast inference speed satisfies\nclinical requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Luojie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yikang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Eric Z Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shanhui Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepIPC: Deeply Integrated Perception and Control for Mobile Robot in Real Environments. (arXiv:2207.09934v1 [cs.RO])","link":"http://arxiv.org/abs/2207.09934","description":"<p>We propose DeepIPC, an end-to-end multi-task model that handles both\nperception and control tasks in driving a mobile robot autonomously. The model\nconsists of two main parts, perception and controller modules. The perception\nmodule takes RGB image and depth map to perform semantic segmentation and\nbird's eye view (BEV) semantic mapping along with providing their encoded\nfeatures. Meanwhile, the controller module processes these features with the\nmeasurement of GNSS locations and angular speed to estimate waypoints that come\nwith latent features. Then, two different agents are used to translate\nwaypoints and latent features into a set of navigational controls to drive the\nrobot. The model is evaluated by predicting driving records and performing\nautomated driving under various conditions in the real environment. Based on\nthe experimental results, DeepIPC achieves the best drivability and multi-task\nperformance even with fewer parameters compared to the other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Natan_O/0/1/0/all/0/1\">Oskar Natan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miura_J/0/1/0/all/0/1\">Jun Miura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient and Scale-Robust Ultra-High-Definition Image Demoireing. (arXiv:2207.09935v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09935","description":"<p>With the rapid development of mobile devices, modern widely-used mobile\nphones typically allow users to capture 4K resolution (i.e.,\nultra-high-definition) images. However, for image demoireing, a challenging\ntask in low-level vision, existing works are generally carried out on\nlow-resolution or synthetic images. Hence, the effectiveness of these methods\non 4K resolution images is still unknown. In this paper, we explore moire\npattern removal for ultra-high-definition images. To this end, we propose the\nfirst ultra-high-definition demoireing dataset (UHDM), which contains 5,000\nreal-world 4K resolution image pairs, and conduct a benchmark study on current\nstate-of-the-art methods. Further, we present an efficient baseline model\nESDNet for tackling 4K moire images, wherein we build a semantic-aligned\nscale-aware module to address the scale variation of moire patterns. Extensive\nexperiments manifest the effectiveness of our approach, which outperforms\nstate-of-the-art methods by a large margin while being much more lightweight.\nCode and dataset are available at https://xinyu-andy.github.io/uhdm-page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probable Domain Generalization via Quantile Risk Minimization. (arXiv:2207.09944v1 [stat.ML])","link":"http://arxiv.org/abs/2207.09944","description":"<p>Domain generalization (DG) seeks predictors which perform well on unseen test\ndistributions by leveraging labeled training data from multiple related\ndistributions or domains. To achieve this, the standard formulation optimizes\nfor worst-case performance over the set of all possible domains. However, with\nworst-case shifts very unlikely in practice, this generally leads to\noverly-conservative solutions. In fact, a recent study found that no DG\nalgorithm outperformed empirical risk minimization in terms of average\nperformance. In this work, we argue that DG is neither a worst-case problem nor\nan average-case problem, but rather a probabilistic one. To this end, we\npropose a probabilistic framework for DG, which we call Probable Domain\nGeneralization, wherein our key idea is that distribution shifts seen during\ntraining should inform us of probable shifts at test time. To realize this, we\nexplicitly relate training and test domains as draws from the same underlying\nmeta-distribution, and propose a new optimization problem -- Quantile Risk\nMinimization (QRM) -- which requires that predictors generalize with high\nprobability. We then prove that QRM: (i) produces predictors that generalize to\nnew domains with a desired probability, given sufficiently many domains and\nsamples; and (ii) recovers the causal predictor as the desired probability of\ngeneralization approaches one. In our experiments, we introduce a more holistic\nquantile-focused evaluation protocol for DG, and show that our algorithms\noutperform state-of-the-art baselines on real and synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Eastwood_C/0/1/0/all/0/1\">Cian Eastwood</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Singh_S/0/1/0/all/0/1\">Shashank Singh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VirtualPose: Learning Generalizable 3D Human Pose Models from Virtual Data. (arXiv:2207.09949v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09949","description":"<p>While monocular 3D pose estimation seems to have achieved very accurate\nresults on the public datasets, their generalization ability is largely\noverlooked. In this work, we perform a systematic evaluation of the existing\nmethods and find that they get notably larger errors when tested on different\ncameras, human poses and appearance. To address the problem, we introduce\nVirtualPose, a two-stage learning framework to exploit the hidden \"free lunch\"\nspecific to this task, i.e. generating infinite number of poses and cameras for\ntraining models at no cost. To that end, the first stage transforms images to\nabstract geometry representations (AGR), and then the second maps them to 3D\nposes. It addresses the generalization issue from two aspects: (1) the first\nstage can be trained on diverse 2D datasets to reduce the risk of over-fitting\nto limited appearance; (2) the second stage can be trained on diverse AGR\nsynthesized from a large number of virtual cameras and poses. It outperforms\nthe SOTA methods without using any paired images and 3D poses from the\nbenchmarks, which paves the way for practical applications. Code is available\nat https://github.com/wkom/VirtualPose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jiajun Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoxuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Pedestrian Group Representations for Multi-modal Trajectory Prediction. (arXiv:2207.09953v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09953","description":"<p>Modeling the dynamics of people walking is a problem of long-standing\ninterest in computer vision. Many previous works involving pedestrian\ntrajectory prediction define a particular set of individual actions to\nimplicitly model group actions. In this paper, we present a novel architecture\nnamed GP-Graph which has collective group representations for effective\npedestrian trajectory prediction in crowded environments, and is compatible\nwith all types of existing approaches. A key idea of GP-Graph is to model both\nindividual-wise and group-wise relations as graph representations. To do this,\nGP-Graph first learns to assign each pedestrian into the most likely behavior\ngroup. Using this assignment information, GP-Graph then forms both intra- and\ninter-group interactions as graphs, accounting for human-human relations within\na group and group-group relations, respectively. To be specific, for the\nintra-group interaction, we mask pedestrian graph edges out of an associated\ngroup. We also propose group pooling&amp;unpooling operations to represent a group\nwith multiple pedestrians as one graph node. Lastly, GP-Graph infers a\nprobability map for socially-acceptable future trajectories from the integrated\nfeatures of both group interactions. Moreover, we introduce a group-level\nlatent vector sampling to ensure collective inferences over a set of possible\nfuture trajectories. Extensive experiments are conducted to validate the\neffectiveness of our architecture, which demonstrates consistent performance\nimprovements with publicly available benchmarks. Code is publicly available at\nhttps://github.com/inhwanbae/GPGraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_I/0/1/0/all/0/1\">Inhwan Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jin-Hwi Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hae-Gon Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Telepresence Video Quality Assessment. (arXiv:2207.09956v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09956","description":"<p>Video conferencing, which includes both video and audio content, has\ncontributed to dramatic increases in Internet traffic, as the COVID-19 pandemic\nforced millions of people to work and learn from home. Global Internet traffic\nof video conferencing has dramatically increased Because of this, efficient and\naccurate video quality tools are needed to monitor and perceptually optimize\ntelepresence traffic streamed via Zoom, Webex, Meet, etc. However, existing\nmodels are limited in their prediction capabilities on multi-modal, live\nstreaming telepresence content. Here we address the significant challenges of\nTelepresence Video Quality Assessment (TVQA) in several ways. First, we\nmitigated the dearth of subjectively labeled data by collecting ~2k\ntelepresence videos from different countries, on which we crowdsourced ~80k\nsubjective quality labels. Using this new resource, we created a\nfirst-of-a-kind online video quality prediction framework for live streaming,\nusing a multi-modal learning framework with separate pathways to compute visual\nand audio quality predictions. Our all-in-one model is able to provide accurate\nquality predictions at the patch, frame, clip, and audiovisual levels. Our\nmodel achieves state-of-the-art performance on both existing quality databases\nand our new TVQA database, at a considerably lower computational expense,\nmaking it an attractive solution for mobile and embedded systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zhenqiang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1\">Deepti Ghadiyaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores. (arXiv:2207.09957v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09957","description":"<p>Machine learning models are typically deployed in a test setting that differs\nfrom the training setting, potentially leading to decreased model performance\nbecause of domain shift. If we could estimate the performance that a\npre-trained model would achieve on data from a specific deployment setting, for\nexample a certain clinic, we could judge whether the model could safely be\ndeployed or if its performance degrades unacceptably on the specific data.\nExisting approaches estimate this based on the confidence of predictions made\non unlabeled test data from the deployment's domain. We find existing methods\nstruggle with data that present class imbalance, because the methods used to\ncalibrate confidence do not account for bias induced by class imbalance,\nconsequently failing to estimate class-wise accuracy. Here, we introduce\nclass-wise calibration within the framework of performance estimation for\nimbalanced datasets. Specifically, we derive class-specific modifications of\nstate-of-the-art confidence-based model evaluation methods including\ntemperature scaling (TS), difference of confidences (DoC), and average\nthresholded confidence (ATC). We also extend the methods to estimate Dice\nsimilarity coefficient (DSC) in image segmentation. We conduct experiments on\nfour tasks and find the proposed modifications consistently improve the\nestimation accuracy for imbalanced datasets. Our methods improve accuracy\nestimation by 18\\% in classification under natural domain shifts, and double\nthe estimation accuracy on segmentation tasks, when compared with prior\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeju Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamnitsas_K/0/1/0/all/0/1\">Konstantinos Kamnitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Few-Shot Class-Incremental Learning with Open-Set Hypothesis in Hyperbolic Geometry. (arXiv:2207.09963v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09963","description":"<p>Few-Shot Class-Incremental Learning (FSCIL) aims at incrementally learning\nnovel classes from a few labeled samples by avoiding the overfitting and\ncatastrophic forgetting simultaneously. The current protocol of FSCIL is built\nby mimicking the general class-incremental learning setting, while it is not\ntotally appropriate due to the different data configuration, i.e., novel\nclasses are all in the limited data regime. In this paper, we rethink the\nconfiguration of FSCIL with the open-set hypothesis by reserving the\npossibility in the first session for incoming categories. To assign better\nperformances on both close-set and open-set recognition to the model,\nHyperbolic Reciprocal Point Learning module (Hyper-RPL) is built on Reciprocal\nPoint Learning (RPL) with hyperbolic neural networks. Besides, for learning\nnovel categories from limited labeled data, we incorporate a hyperbolic metric\nlearning (Hyper-Metric) module into the distillation-based framework to\nalleviate the overfitting issue and better handle the trade-off issue between\nthe preservation of old knowledge and the acquisition of new knowledge. The\ncomprehensive assessments of the proposed configuration and modules on three\nbenchmark datasets are executed to validate the effectiveness concerning three\nevaluation indicators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yawen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2-Net: Multi-stages Specular Highlight Detection and Removal in Multi-scenes. (arXiv:2207.09965v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09965","description":"<p>In this paper, we propose a novel uniformity framework for highlight\ndetection and removal in multi-scenes, including synthetic images, face images,\nnatural images, and text images. The framework consists of three main\ncomponents, highlight feature extractor module, highlight coarse removal\nmodule, and highlight refine removal module. Firstly, the highlight feature\nextractor module can directly separate the highlight feature and non-highlight\nfeature from the original highlight image. Then highlight removal image is\nobtained using a coarse highlight removal network. To further improve the\nhighlight removal effect, the refined highlight removal image is finally\nobtained using refine highlight removal module based on contextual highlight\nattention mechanisms. Extensive experimental results in multiple scenes\nindicate that the proposed framework can obtain excellent visual effects of\nhighlight removal and achieve state-of-the-art results in several quantitative\nevaluation metrics. Our algorithm is applied for the first time in video\nhighlight removal with promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaoyangfan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingjun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal and cross-modal attention for audio-visual zero-shot learning. (arXiv:2207.09966v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09966","description":"<p>Audio-visual generalised zero-shot learning for video classification requires\nunderstanding the relations between the audio and visual information in order\nto be able to recognise samples from novel, previously unseen classes at test\ntime. The natural semantic and temporal alignment between audio and visual data\nin video data can be exploited to learn powerful representations that\ngeneralise to unseen classes at test time. We propose a multi-modal and\nTemporal Cross-attention Framework (\\modelName) for audio-visual generalised\nzero-shot learning. Its inputs are temporally aligned audio and visual features\nthat are obtained from pre-trained networks. Encouraging the framework to focus\non cross-modal correspondence across time instead of self-attention within the\nmodalities boosts the performance significantly. We show that our proposed\nframework that ingests temporal features yields state-of-the-art performance on\nthe \\ucf, \\vgg, and \\activity benchmarks for (generalised) zero-shot learning.\nCode for reproducing all results is available at\n\\url{https://github.com/ExplainableML/TCAF-GZSL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mercea_O/0/1/0/all/0/1\">Otniel-Bogdan Mercea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hummel_T/0/1/0/all/0/1\">Thomas Hummel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1\">A. Sophia Koepke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralBF: Neural Bilateral Filtering for Top-down Instance Segmentation on Point Clouds. (arXiv:2207.09978v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09978","description":"<p>We introduce a method for instance proposal generation for 3D point clouds.\nExisting techniques typically directly regress proposals in a single\nfeed-forward step, leading to inaccurate estimation. We show that this serves\nas a critical bottleneck, and propose a method based on iterative bilateral\nfiltering with learned kernels. Following the spirit of bilateral filtering, we\nconsider both the deep feature embeddings of each point, as well as their\nlocations in the 3D space. We show via synthetic experiments that our method\nbrings drastic improvements when generating instance proposals for a given\npoint of interest. We further validate our method on the challenging ScanNet\nbenchmark, achieving the best instance segmentation performance amongst the\nsub-category of top-down methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebain_D/0/1/0/all/0/1\">Daniel Rebain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Renjie Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tankovich_V/0/1/0/all/0/1\">Vladimir Tankovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_S/0/1/0/all/0/1\">Soroosh Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation. (arXiv:2207.09988v1 [cs.CV])","link":"http://arxiv.org/abs/2207.09988","description":"<p>Unsupervised domain adaptation in semantic segmentation has been raised to\nalleviate the reliance on expensive pixel-wise annotations. It leverages a\nlabeled source domain dataset as well as unlabeled target domain images to\nlearn a segmentation network. In this paper, we observe two main issues of the\nexisting domain-invariant learning framework. (1) Being distracted by the\nfeature distribution alignment, the network cannot focus on the segmentation\ntask. (2) Fitting source domain data well would compromise the target domain\nperformance. To address these issues, we propose DecoupleNet that alleviates\nsource domain overfitting and enables the final model to focus more on the\nsegmentation task. Furthermore, we put forward Self-Discrimination (SD) and\nintroduce an auxiliary classifier to learn more discriminative target domain\nfeatures with pseudo labels. Finally, we propose Online Enhanced Self-Training\n(OEST) to contextually enhance the quality of pseudo labels in an online\nmanner. Experiments show our method outperforms existing state-of-the-art\nmethods, and extensive ablation studies verify the effectiveness of each\ncomponent. Code is available at https://github.com/dvlab-research/DecoupleNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaogang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingcong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Shortcut Learning in a Target Domain by Generalizing Basic Visual Factors from a Source Domain. (arXiv:2207.10002v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10002","description":"<p>Shortcut learning occurs when a deep neural network overly relies on spurious\ncorrelations in the training dataset in order to solve downstream tasks. Prior\nworks have shown how this impairs the compositional generalization capability\nof deep learning models. To address this problem, we propose a novel approach\nto mitigate shortcut learning in uncontrolled target domains. Our approach\nextends the training set with an additional dataset (the source domain), which\nis specifically designed to facilitate learning independent representations of\nbasic visual factors. We benchmark our idea on synthetic target domains where\nwe explicitly control shortcut opportunities as well as real-world target\ndomains. Furthermore, we analyze the effect of different specifications of the\nsource domain and the network architecture on compositional generalization. Our\nmain finding is that leveraging data from a source domain is an effective way\nto mitigate shortcut learning. By promoting independence across different\nfactors of variation in the learned representations, networks can learn to\nconsider only predictive factors and ignore potential shortcut factors during\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saranrittichai_P/0/1/0/all/0/1\">Piyapat Saranrittichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1\">Chaithanya Kumar Mummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blaiotta_C/0/1/0/all/0/1\">Claudia Blaiotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munoz_M/0/1/0/all/0/1\">Mauricio Munoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_V/0/1/0/all/0/1\">Volker Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BYEL : Bootstrap on Your Emotion Latent. (arXiv:2207.10003v1 [cs.LG])","link":"http://arxiv.org/abs/2207.10003","description":"<p>According to the problem of dataset construction cost for training in deep\nlearning and the development of generative models, more and more researches are\nbeing conducted to train with synthetic data and to inference using real data.\nWe propose emotion aware Self-Supervised Learning using ABAW's Learning\nSynthetic Data (LSD) dataset. We pre-train our method to LSD dataset as a\nself-supervised learning and then use the same LSD dataset to do downstream\ntraining on the emotion classification task as a supervised learning. As a\nresult, a higher result(0.63) than baseline(0.5) was obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hwangyu Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sejoon Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-Graph: Minimal Solution for Rigid Rotation with Extensibility Graphs. (arXiv:2207.10008v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10008","description":"<p>Minimal solutions for relative rotation and translation estimation tasks have\nbeen explored in different scenarios, typically relying on the so-called\nco-visibility graph. However, how to build direct rotation relationships\nbetween two frames without overlap is still an open topic, which, if solved,\ncould greatly improve the accuracy of visual odometry.\n</p>\n<p>In this paper, a new minimal solution is proposed to solve relative rotation\nestimation between two images without overlapping areas by exploiting a new\ngraph structure, which we call Extensibility Graph (E-Graph). Differently from\na co-visibility graph, high-level landmarks, including vanishing directions and\nplane normals, are stored in our E-Graph, which are geometrically extensible.\nBased on E-Graph, the rotation estimation problem becomes simpler and more\nelegant, as it can deal with pure rotational motion and requires fewer\nassumptions, e.g. Manhattan/Atlanta World, planar/vertical motion. Finally, we\nembed our rotation estimation strategy into a complete camera tracking and\nmapping system which obtains 6-DoF camera poses and a dense 3D mesh model.\n</p>\n<p>Extensive experiments on public benchmarks demonstrate that the proposed\nmethod achieves state-of-the-art tracking performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Domain Adaptation for Face Anti-Spoofing. (arXiv:2207.10015v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10015","description":"<p>Face anti-spoofing (FAS) approaches based on unsupervised domain adaption\n(UDA) have drawn growing attention due to promising performances for target\nscenarios. Most existing UDA FAS methods typically fit the trained models to\nthe target domain via aligning the distribution of semantic high-level\nfeatures. However, insufficient supervision of unlabeled target domains and\nneglect of low-level feature alignment degrade the performances of existing\nmethods. To address these issues, we propose a novel perspective of UDA FAS\nthat directly fits the target data to the models, i.e., stylizes the target\ndata to the source-domain style via image translation, and further feeds the\nstylized data into the well-trained source model for classification. The\nproposed Generative Domain Adaptation (GDA) framework combines two carefully\ndesigned consistency constraints: 1) Inter-domain neural statistic consistency\nguides the generator in narrowing the inter-domain gap. 2) Dual-level semantic\nconsistency ensures the semantic quality of stylized images. Besides, we\npropose intra-domain spectrum mixup to further expand target data distributions\nto ensure generalization and reduce the intra-domain gap. Extensive experiments\nand visualizations demonstrate the effectiveness of our method against the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Secrets of Event-Based Optical Flow. (arXiv:2207.10022v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10022","description":"<p>Event cameras respond to scene dynamics and offer advantages to estimate\nmotion. Following recent image-based deep-learning achievements, optical flow\nestimation methods for event cameras have rushed to combine those image-based\nmethods with event data. However, it requires several adaptations (data\nconversion, loss function, etc.) as they have very different properties. We\ndevelop a principled method to extend the Contrast Maximization framework to\nestimate optical flow from events alone. We investigate key elements: how to\ndesign the objective function to prevent overfitting, how to warp events to\ndeal better with occlusions, and how to improve convergence with multi-scale\nraw events. With these key elements, our method ranks first among unsupervised\nmethods on the MVSEC benchmark, and is competitive on the DSEC benchmark.\nMoreover, our method allows us to expose the issues of the ground truth flow in\nthose benchmarks, and produces remarkable results when it is transferred to\nunsupervised learning settings. Our code is available at\nhttps://github.com/tub-rip/event_based_optical_flow\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiba_S/0/1/0/all/0/1\">Shintaro Shiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_Y/0/1/0/all/0/1\">Yoshimitsu Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Guillermo Gallego</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tailoring Self-Supervision for Supervised Learning. (arXiv:2207.10023v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10023","description":"<p>Recently, it is shown that deploying a proper self-supervision is a\nprospective way to enhance the performance of supervised learning. Yet, the\nbenefits of self-supervision are not fully exploited as previous pretext tasks\nare specialized for unsupervised representation learning. To this end, we begin\nby presenting three desirable properties for such auxiliary tasks to assist the\nsupervised objective. First, the tasks need to guide the model to learn rich\nfeatures. Second, the transformations involved in the self-supervision should\nnot significantly alter the training distribution. Third, the tasks are\npreferred to be light and generic for high applicability to prior arts.\nSubsequently, to show how existing pretext tasks can fulfill these and be\ntailored for supervised learning, we propose a simple auxiliary\nself-supervision task, predicting localizable rotation (LoRot). Our exhaustive\nexperiments validate the merits of LoRot as a pretext task tailored for\nsupervised learning in terms of robustness and generalization capability. Our\ncode is available at https://github.com/wjun0830/Localizable-Rotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_W/0/1/0/all/0/1\">WonJun Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Ji-Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Jae-Pil Heo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Difficulty-Aware Simulator for Open Set Recognition. (arXiv:2207.10024v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10024","description":"<p>Open set recognition (OSR) assumes unknown instances appear out of the blue\nat the inference time. The main challenge of OSR is that the response of models\nfor unknowns is totally unpredictable. Furthermore, the diversity of open set\nmakes it harder since instances have different difficulty levels. Therefore, we\npresent a novel framework, DIfficulty-Aware Simulator (DIAS), that generates\nfakes with diverse difficulty levels to simulate the real world. We first\ninvestigate fakes from generative adversarial network (GAN) in the classifier's\nviewpoint and observe that these are not severely challenging. This leads us to\ndefine the criteria for difficulty by regarding samples generated with GANs\nhaving moderate-difficulty. To produce hard-difficulty examples, we introduce\nCopycat, imitating the behavior of the classifier. Furthermore, moderate- and\neasy-difficulty samples are also yielded by our modified GAN and Copycat,\nrespectively. As a result, DIAS outperforms state-of-the-art methods with both\nmetrics of AUROC and F-score. Our code is available at\nhttps://github.com/wjun0830/Difficulty-Aware-Simulator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_W/0/1/0/all/0/1\">WonJun Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seong_H/0/1/0/all/0/1\">Hyun Seok Seong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_C/0/1/0/all/0/1\">Cheol-Ho Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Jae-Pil Heo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks. (arXiv:2207.10025v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10025","description":"<p>Facial expression in-the-wild is essential for various interactive computing\ndomains. Especially, \"Learning from Synthetic Data\" (LSD) is an important topic\nin the facial expression recognition task. In this paper, we propose a\nmulti-task learning-based facial expression recognition approach which consists\nof emotion and appearance learning branches that can share all face\ninformation, and present preliminary results for the LSD challenge introduced\nin the 4th affective behavior analysis in-the-wild (ABAW) competition. Our\nmethod achieved the mean F1 score of 0.71.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jae-Yeop Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yeong-Gi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">JiYeon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sumin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jin-Woo Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yuchul Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locality Guidance for Improving Vision Transformers on Tiny Datasets. (arXiv:2207.10026v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10026","description":"<p>While the Vision Transformer (VT) architecture is becoming trendy in computer\nvision, pure VT models perform poorly on tiny datasets. To address this issue,\nthis paper proposes the locality guidance for improving the performance of VTs\non tiny datasets. We first analyze that the local information, which is of\ngreat importance for understanding images, is hard to be learned with limited\ndata due to the high flexibility and intrinsic globality of the self-attention\nmechanism in VTs. To facilitate local information, we realize the locality\nguidance for VTs by imitating the features of an already trained convolutional\nneural network (CNN), inspired by the built-in local-to-global hierarchy of\nCNN. Under our dual-task learning paradigm, the locality guidance provided by a\nlightweight CNN trained on low-resolution images is adequate to accelerate the\nconvergence and improve the performance of VTs to a large extent. Therefore,\nour locality guidance approach is very simple and efficient, and can serve as a\nbasic performance enhancement method for VTs on tiny datasets. Extensive\nexperiments demonstrate that our method can significantly improve VTs when\ntraining from scratch on tiny datasets and is compatible with different kinds\nof VTs and datasets. For example, our proposed method can boost the performance\nof various VTs on tiny datasets (e.g., 13.07% for DeiT, 8.98% for T2T and 7.85%\nfor PVT), and enhance even stronger baseline PVTv2 by 1.86% to 79.30%, showing\nthe potential of VTs on tiny datasets. The code is available at\nhttps://github.com/lkhl/tiny-transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kehan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Runyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhennan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guoli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOTCOM: The Multi-Object Tracking Dataset Complexity Metric. (arXiv:2207.10031v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10031","description":"<p>There exists no comprehensive metric for describing the complexity of\nMulti-Object Tracking (MOT) sequences. This lack of metrics decreases\nexplainability, complicates comparison of datasets, and reduces the\nconversation on tracker performance to a matter of leader board position. As a\nremedy, we present the novel MOT dataset complexity metric (MOTCOM), which is a\ncombination of three sub-metrics inspired by key problems in MOT: occlusion,\nerratic motion, and visual similarity. The insights of MOTCOM can open nuanced\ndiscussions on tracker performance and may lead to a wider acknowledgement of\nnovel contributions developed for either less known datasets or those aimed at\nsolving sub-problems. We evaluate MOTCOM on the comprehensive MOT17, MOT20, and\nMOTSynth datasets and show that MOTCOM is far better at describing the\ncomplexity of MOT sequences compared to the conventional density and number of\ntracks. Project page at https://vap.aau.dk/motcom\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1\">Malte Pedersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1\">Joakim Bruslund Haurum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dendorfer_P/0/1/0/all/0/1\">Patrick Dendorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Sparse 3D Object Detection. (arXiv:2207.10035v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10035","description":"<p>As the perception range of LiDAR increases, LiDAR-based 3D object detection\nbecomes a dominant task in the long-range perception task of autonomous\ndriving. The mainstream 3D object detectors usually build dense feature maps in\nthe network backbone and prediction head. However, the computational and\nspatial costs on the dense feature map are quadratic to the perception range,\nwhich makes them hardly scale up to the long-range setting. To enable efficient\nlong-range LiDAR-based object detection, we build a fully sparse 3D object\ndetector (FSD). The computational and spatial cost of FSD is roughly linear to\nthe number of points and independent of the perception range. FSD is built upon\nthe general sparse voxel encoder and a novel sparse instance recognition (SIR)\nmodule. SIR first groups the points into instances and then applies\ninstance-wise feature extraction and prediction. In this way, SIR resolves the\nissue of center feature missing, which hinders the design of the fully sparse\narchitecture for all center-based or anchor-based detectors. Moreover, SIR\navoids the time-consuming neighbor queries in previous point-based methods by\ngrouping points into instances. We conduct extensive experiments on the\nlarge-scale Waymo Open Dataset to reveal the working mechanism of FSD, and\nstate-of-the-art performance is reported. To demonstrate the superiority of FSD\nin long-range detection, we also conduct experiments on Argoverse 2 Dataset,\nwhich has a much larger perception range ($200m$) than Waymo Open Dataset\n($75m$). On such a large perception range, FSD achieves state-of-the-art\nperformance and is 2.4$\\times$ faster than the dense counterpart.Codes will be\nreleased at https://github.com/TuSimple/SST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model. (arXiv:2207.10040v1 [eess.IV])","link":"http://arxiv.org/abs/2207.10040","description":"<p>Image restoration algorithms for atmospheric turbulence are known to be much\nmore challenging to design than traditional ones such as blur or noise because\nthe distortion caused by the turbulence is an entanglement of spatially varying\nblur, geometric distortion, and sensor noise. Existing CNN-based restoration\nmethods built upon convolutional kernels with static weights are insufficient\nto handle the spatially dynamical atmospheric turbulence effect. To address\nthis problem, in this paper, we propose a physics-inspired transformer model\nfor imaging through atmospheric turbulence. The proposed network utilizes the\npower of transformer blocks to jointly extract a dynamical turbulence\ndistortion map and restore a turbulence-free image. In addition, recognizing\nthe lack of a comprehensive dataset, we collect and present two new real-world\nturbulence datasets that allow for evaluation with both classical objective\nmetrics (e.g., PSNR and SSIM) and a new task-driven metric using text\nrecognition accuracy. Both real testing sets and all related code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiyuan Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Densely Constrained Depth Estimator for Monocular 3D Object Detection. (arXiv:2207.10047v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10047","description":"<p>Estimating accurate 3D locations of objects from monocular images is a\nchallenging problem because of lacking depth. Previous work shows that\nutilizing the object's keypoint projection constraints to estimate multiple\ndepth candidates boosts the detection performance. However, the existing\nmethods can only utilize vertical edges as projection constraints for depth\nestimation. So these methods only use a small number of projection constraints\nand produce insufficient depth candidates, leading to inaccurate depth\nestimation. In this paper, we propose a method that utilizes dense projection\nconstraints from edges of any direction. In this way, we employ much more\nprojection constraints and produce considerable depth candidates. Besides, we\npresent a graph matching weighting module to merge the depth candidates. The\nproposed method DCD (Densely Constrained Detector) achieves state-of-the-art\nperformance on the KITTI and WOD benchmarks. Code is released at\nhttps://github.com/BraveGroup/DCD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunchao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiawei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining a Neural Network before Knowing Its Architecture. (arXiv:2207.10049v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10049","description":"<p>Training large neural networks is possible by training a smaller hypernetwork\nthat predicts parameters for the large ones. A recently released Graph\nHyperNetwork (GHN) trained this way on one million smaller ImageNet\narchitectures is able to predict parameters for large unseen networks such as\nResNet-50. While networks with predicted parameters lose performance on the\nsource task, the predicted parameters have been found useful for fine-tuning on\nother tasks. We study if fine-tuning based on the same GHN is still useful on\nnovel strong architectures that were published after the GHN had been trained.\nWe found that for recent architectures such as ConvNeXt, GHN initialization\nbecomes less useful than for ResNet-50. One potential reason is the increased\ndistribution shift of novel architectures from those used to train the GHN. We\nalso found that the predicted parameters lack the diversity necessary to\nsuccessfully fine-tune parameters with gradient descent. We alleviate this\nlimitation by applying simple post-processing techniques to predicted\nparameters before fine-tuning them on a target task and improve fine-tuning of\nResNet-50 and ConvNeXt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knyazev_B/0/1/0/all/0/1\">Boris Knyazev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Clothed Human Reconstruction in the Wild. (arXiv:2207.10053v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10053","description":"<p>Although much progress has been made in 3D clothed human reconstruction, most\nof the existing methods fail to produce robust results from in-the-wild images,\nwhich contain diverse human poses and appearances. This is mainly due to the\nlarge domain gap between training datasets and in-the-wild datasets. The\ntraining datasets are usually synthetic ones, which contain rendered images\nfrom GT 3D scans. However, such datasets contain simple human poses and less\nnatural image appearances compared to those of real in-the-wild datasets, which\nmakes generalization of it to in-the-wild images extremely challenging. To\nresolve this issue, in this work, we propose ClothWild, a 3D clothed human\nreconstruction framework that firstly addresses the robustness on in-thewild\nimages. First, for the robustness to the domain gap, we propose a weakly\nsupervised pipeline that is trainable with 2D supervision targets of\nin-the-wild datasets. Second, we design a DensePose-based loss function to\nreduce ambiguities of the weak supervision. Extensive empirical tests on\nseveral public in-the-wild datasets demonstrate that our proposed ClothWild\nproduces much more accurate and robust results than the state-of-the-art\nmethods. The codes are available in here:\nhttps://github.com/hygenie1228/ClothWild_RELEASE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_G/0/1/0/all/0/1\">Gyeongsik Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1\">Hyeongjin Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular 3D Object Reconstruction with GAN Inversion. (arXiv:2207.10061v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10061","description":"<p>Recovering a textured 3D mesh from a monocular image is highly challenging,\nparticularly for in-the-wild objects that lack 3D ground truths. In this work,\nwe present MeshInversion, a novel framework to improve the reconstruction by\nexploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh\nsynthesis. Reconstruction is achieved by searching for a latent space in the 3D\nGAN that best resembles the target mesh in accordance with the single view\nobservation. Since the pre-trained GAN encapsulates rich 3D semantics in terms\nof mesh geometry and texture, searching within the GAN manifold thus naturally\nregularizes the realness and fidelity of the reconstruction. Importantly, such\nregularization is directly applied in the 3D space, providing crucial guidance\nof mesh parts that are unobserved in the 2D space. Experiments on standard\nbenchmarks show that our framework obtains faithful 3D reconstructions with\nconsistent geometry and texture across both observed and unobserved parts.\nMoreover, it generalizes well to meshes that are less commonly seen, such as\nthe extended articulation of deformable objects. Code is released at\nhttps://github.com/junzhezhang/mesh-inversion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Daxuan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1\">Chai Kiat Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic uncertainty intervals for disentangled latent spaces. (arXiv:2207.10074v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10074","description":"<p>Meaningful uncertainty quantification in computer vision requires reasoning\nabout semantic information -- say, the hair color of the person in a photo or\nthe location of a car on the street. To this end, recent breakthroughs in\ngenerative modeling allow us to represent semantic information in disentangled\nlatent spaces, but providing uncertainties on the semantic latent variables has\nremained challenging. In this work, we provide principled uncertainty intervals\nthat are guaranteed to contain the true semantic factors for any underlying\ngenerative model. The method does the following: (1) it uses quantile\nregression to output a heuristic uncertainty interval for each element in the\nlatent space (2) calibrates these uncertainties such that they contain the true\nvalue of the latent for a new, unseen input. The endpoints of these calibrated\nintervals can then be propagated through the generator to produce interpretable\nuncertainty visualizations for each semantic factor. This technique reliably\ncommunicates semantically meaningful, principled, and instance-adaptive\nuncertainty in inverse problems like image super-resolution and image\ncompletion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_S/0/1/0/all/0/1\">Swami Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios N. Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romano_Y/0/1/0/all/0/1\">Yaniv Romano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is an Object-Centric Video Representation Beneficial for Transfer?. (arXiv:2207.10075v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10075","description":"<p>The objective of this work is to learn an object-centric video\nrepresentation, with the aim of improving transferability to novel tasks, i.e.,\ntasks different from the pre-training task of action classification. To this\nend, we introduce a new object-centric video recognition model based on a\ntransformer architecture. The model learns a set of object-centric summary\nvectors for the video, and uses these vectors to fuse the visual and\nspatio-temporal trajectory `modalities' of the video clip. We also introduce a\nnovel trajectory contrast loss to further enhance objectness in these summary\nvectors. With experiments on four datasets -- SomethingSomething-V2,\nSomethingElse, Action Genome and EpicKitchens -- we show that the\nobject-centric model outperforms prior video representations (both\nobject-agnostic and object-aware), when: (1) classifying actions on unseen\nobjects and unseen environments; (2) low-shot learning to novel classes; (3)\nlinear probe to other downstream tasks; as well as (4) for standard action\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankush Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discover and Mitigate Unknown Biases with Debiasing Alternate Networks. (arXiv:2207.10077v1 [cs.CV])","link":"http://arxiv.org/abs/2207.10077","description":"<p>Deep image classifiers have been found to learn biases from datasets. To\nmitigate the biases, most previous methods require labels of protected\nattributes (e.g., age, skin tone) as full-supervision, which has two\nlimitations: 1) it is infeasible when the labels are unavailable; 2) they are\nincapable of mitigating unknown biases -- biases that humans do not\npreconceive. To resolve those problems, we propose Debiasing Alternate Networks\n(DebiAN), which comprises two networks -- a Discoverer and a Classifier. By\ntraining in an alternate manner, the discoverer tries to find multiple unknown\nbiases of the classifier without any annotations of biases, and the classifier\naims at unlearning the biases identified by the discoverer. While previous\nworks evaluate debiasing results in terms of a single bias, we create\nMulti-Color MNIST dataset to better benchmark mitigation of multiple biases in\na multi-bias setting, which not only reveals the problems in previous methods\nbut also demonstrates the advantage of DebiAN in identifying and mitigating\nmultiple biases simultaneously. We further conduct extensive experiments on\nreal-world datasets, showing that the discoverer in DebiAN can identify unknown\nbiases that may be hard to be found by humans. Regarding debiasing, DebiAN\nachieves strong bias mitigation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogs_A/0/1/0/all/0/1\">Anthony Hoogs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of the hands in egocentric vision: A survey. (arXiv:1912.10867v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.10867","description":"<p>Egocentric vision (a.k.a. first-person vision - FPV) applications have\nthrived over the past few years, thanks to the availability of affordable\nwearable cameras and large annotated datasets. The position of the wearable\ncamera (usually mounted on the head) allows recording exactly what the camera\nwearers have in front of them, in particular hands and manipulated objects.\nThis intrinsic advantage enables the study of the hands from multiple\nperspectives: localizing hands and their parts within the images; understanding\nwhat actions and activities the hands are involved in; and developing\nhuman-computer interfaces that rely on hand gestures. In this survey, we review\nthe literature that focuses on the hands using egocentric vision, categorizing\nthe existing approaches into: localization (where are the hands or parts of\nthem?); interpretation (what are the hands doing?); and application (e.g.,\nsystems that used egocentric hand cues for solving a specific problem).\nMoreover, a list of the most prominent datasets with hand-based annotations is\nprovided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandini_A/0/1/0/all/0/1\">Andrea Bandini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zariffa_J/0/1/0/all/0/1\">Jos&#xe9; Zariffa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks on the DNN Interpretation System. (arXiv:2011.10698v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2011.10698","description":"<p>Interpretability is crucial to understand the inner workings of deep neural\nnetworks (DNNs) and many interpretation methods generate saliency maps that\nhighlight parts of the input image that contribute the most to the prediction\nmade by the DNN. In this paper we design a backdoor attack that alters the\nsaliency map produced by the network for an input image only with injected\ntrigger that is invisible to the naked eye while maintaining the prediction\naccuracy. The attack relies on injecting poisoned data with a trigger into the\ntraining data set. The saliency maps are incorporated in the penalty term of\nthe objective function that is used to train a deep model and its influence on\nmodel training is conditioned upon the presence of a trigger. We design two\ntypes of attacks: targeted attack that enforces a specific modification of the\nsaliency map and untargeted attack when the importance scores of the top pixels\nfrom the original saliency map are significantly reduced. We perform empirical\nevaluation of the proposed backdoor attacks on gradient-based and gradient-free\ninterpretation methods for a variety of deep learning architectures. We show\nthat our attacks constitute a serious security threat when deploying deep\nlearning models developed by untrusty sources. Finally, in the Supplement we\ndemonstrate that the proposed methodology can be used in an inverted setting,\nwhere the correct saliency map can be obtained only in the presence of a\ntrigger (key), effectively making the interpretation system available only to\nselected users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shihong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choromanska_A/0/1/0/all/0/1\">Anna Choromanska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate Active Camera Localization. (arXiv:2012.04263v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04263","description":"<p>In this work, we tackle the problem of active camera localization, which\ncontrols the camera movements actively to achieve an accurate camera pose. The\npast solutions are mostly based on Markov Localization, which reduces the\nposition-wise camera uncertainty for localization. These approaches localize\nthe camera in the discrete pose space and are agnostic to the\nlocalization-driven scene property, which restricts the camera pose accuracy in\nthe coarse scale. We propose to overcome these limitations via a novel active\ncamera localization algorithm, composed of a passive and an active localization\nmodule. The former optimizes the camera pose in the continuous pose space by\nestablishing point-wise camera-world correspondences. The latter explicitly\nmodels the scene and camera uncertainty components to plan the right path for\naccurate camera pose estimation. We validate our algorithm on the challenging\nlocalization scenarios from both synthetic and scanned real-world indoor\nscenes. Experimental results demonstrate that our algorithm outperforms both\nthe state-of-the-art Markov Localization based approach and other compared\napproaches on the fine-scale camera pose accuracy. Code and data are released\nat https://github.com/qhFang/AccurateACL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qihang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yingda Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D2C-SR: A Divergence to Convergence Approach for Real-World Image Super-Resolution. (arXiv:2103.14373v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14373","description":"<p>In this paper, we present D2C-SR, a novel framework for the task of\nreal-world image super-resolution. As an ill-posed problem, the key challenge\nin super-resolution related tasks is there can be multiple predictions for a\ngiven low-resolution input. Most classical deep learning based approaches\nignored the fundamental fact and lack explicit modeling of the underlying\nhigh-frequency distribution which leads to blurred results. Recently, some\nmethods of GAN-based or learning super-resolution space can generate simulated\ntextures but do not promise the accuracy of the textures which have low\nquantitative performance. Rethinking both, we learn the distribution of\nunderlying high-frequency details in a discrete form and propose a two-stage\npipeline: divergence stage to convergence stage. At divergence stage, we\npropose a tree-based structure deep network as our divergence backbone.\nDivergence loss is proposed to encourage the generated results from the\ntree-based network to diverge into possible high-frequency representations,\nwhich is our way of discretely modeling the underlying high-frequency\ndistribution. At convergence stage, we assign spatial weights to fuse these\ndivergent predictions to obtain the final output with more accurate details.\nOur approach provides a convenient end-to-end manner to inference. We conduct\nevaluations on several real-world benchmarks, including a new proposed\nD2CRealSR dataset with x8 scaling factor. Our experiments demonstrate that\nD2C-SR achieves better accuracy and visual improvements against\nstate-of-the-art methods, with a significantly less parameters number and our\nD2C structure can also be applied as a generalized structure to some other\nmethods to obtain improvement. Our codes and dataset are available at\nhttps://github.com/megvii-research/D2C-SR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_L/0/1/0/all/0/1\">Lanpeng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training strategies and datasets for facial representation learning. (arXiv:2103.16554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16554","description":"<p>What is the best way to learn a universal face representation? Recent work on\nDeep Learning in the area of face analysis has focused on supervised learning\nfor specific tasks of interest (e.g. face recognition, facial landmark\nlocalization etc.) but has overlooked the overarching question of how to find a\nfacial representation that can be readily adapted to several facial analysis\ntasks and datasets. To this end, we make the following 4 contributions: (a) we\nintroduce, for the first time, a comprehensive evaluation benchmark for facial\nrepresentation learning consisting of 5 important face analysis tasks. (b) We\nsystematically investigate two ways of large-scale representation learning\napplied to faces: supervised and unsupervised pre-training. Importantly, we\nfocus our evaluations on the case of few-shot facial learning. (c) We\ninvestigate important properties of the training datasets including their size\nand quality (labelled, unlabelled or even uncurated). (d) To draw our\nconclusions, we conducted a very large number of experiments. Our main two\nfindings are: (1) Unsupervised pre-training on completely in-the-wild,\nuncurated data provides consistent and, in some cases, significant accuracy\nimprovements for all facial tasks considered. (2) Many existing facial video\ndatasets seem to have a large amount of redundancy. We will release code, and\npre-trained models to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bulat_A/0/1/0/all/0/1\">Adrian Bulat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shiyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garbett_A/0/1/0/all/0/1\">Andrew Garbett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_E/0/1/0/all/0/1\">Enrique Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TREND: Truncated Generalized Normal Density Estimation of Inception Embeddings for GAN Evaluation. (arXiv:2104.14767v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14767","description":"<p>Evaluating image generation models such as generative adversarial networks\n(GANs) is a challenging problem. A common approach is to compare the\ndistributions of the set of ground truth images and the set of generated test\nimages. The Frech\\'et Inception distance is one of the most widely used metrics\nfor evaluation of GANs, which assumes that the features from a trained\nInception model for a set of images follow a normal distribution. In this\npaper, we argue that this is an over-simplified assumption, which may lead to\nunreliable evaluation results, and more accurate density estimation can be\nachieved using a truncated generalized normal distribution. Based on this, we\npropose a novel metric for accurate evaluation of GANs, named TREND (TRuncated\ngEneralized Normal Density estimation of inception embeddings). We demonstrate\nthat our approach significantly reduces errors of density estimation, which\nconsequently eliminates the risk of faulty evaluation results. Furthermore, we\nshow that the proposed metric significantly improves robustness of evaluation\nresults against variation of the number of image samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junghyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Seok Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaCLR: Motion-aware Contrastive Learning of Representations for Videos. (arXiv:2106.09703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09703","description":"<p>We present MaCLR, a novel method to explicitly perform cross-modal\nself-supervised video representations learning from visual and motion\nmodalities. Compared to previous video representation learning methods that\nmostly focus on learning motion cues implicitly from RGB inputs, MaCLR enriches\nstandard contrastive learning objectives for RGB video clips with a cross-modal\nlearning objective between a Motion pathway and a Visual pathway. We show that\nthe representation learned with our MaCLR method focuses more on foreground\nmotion regions and thus generalizes better to downstream tasks. To demonstrate\nthis, we evaluate MaCLR on five datasets for both action recognition and action\ndetection, and demonstrate state-of-the-art self-supervised performance on all\ndatasets. Furthermore, we show that MaCLR representation can be as effective as\nrepresentations learned with full supervision on UCF101 and HMDB51 action\nrecognition, and even outperform the supervised representation for action\nrecognition on VidSitu and SSv2, and action detection on AVA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fanyi Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Unbiased Visual Emotion Recognition via Causal Intervention. (arXiv:2107.12096v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12096","description":"<p>Although much progress has been made in visual emotion recognition,\nresearchers have realized that modern deep networks tend to exploit dataset\ncharacteristics to learn spurious statistical associations between the input\nand the target. Such dataset characteristics are usually treated as dataset\nbias, which damages the robustness and generalization performance of these\nrecognition systems. In this work, we scrutinize this problem from the\nperspective of causal inference, where such dataset characteristic is termed as\na confounder which misleads the system to learn the spurious correlation. To\nalleviate the negative effects brought by the dataset bias, we propose a novel\nInterventional Emotion Recognition Network (IERN) to achieve the backdoor\nadjustment, which is one fundamental deconfounding technique in causal\ninference. Specifically, IERN starts by disentangling the dataset-related\ncontext feature from the actual emotion feature, where the former forms the\nconfounder. The emotion feature will then be forced to see each confounder\nstratum equally before being fed into the classifier. A series of designed\ntests validate the efficacy of IERN, and experiments on three emotion\nbenchmarks demonstrate that IERN outperforms state-of-the-art approaches for\nunbiased visual emotion recognition. Code is available at\nhttps://github.com/donydchen/causal_emotion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuedong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1\">Tat-Jen Cham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised learning methods and applications in medical imaging analysis: A survey. (arXiv:2109.08685v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08685","description":"<p>The scarcity of high-quality annotated medical imaging datasets is a major\nproblem that collides with machine learning applications in the field of\nmedical imaging analysis and impedes its advancement. Self-supervised learning\nis a recent training paradigm that enables learning robust representations\nwithout the need for human annotation which can be considered an effective\nsolution for the scarcity of annotated medical data. This article reviews the\nstate-of-the-art research directions in self-supervised learning approaches for\nimage data with a concentration on their applications in the field of medical\nimaging analysis. The article covers a set of the most recent self-supervised\nlearning methods from the computer vision field as they are applicable to the\nmedical imaging analysis and categorize them as predictive, generative, and\ncontrastive approaches. Moreover, the article covers 40 of the most recent\nresearch papers in the field of self-supervised learning in medical imaging\nanalysis aiming at shedding the light on the recent innovation in the field.\nFinally, the article concludes with possible future research directions in the\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shurrab_S/0/1/0/all/0/1\">Saeed Shurrab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duwairi_R/0/1/0/all/0/1\">Rehab Duwairi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04079","description":"<p>Accurate and reliable lane detection is vital for the safe performance of\nlane-keeping assistance and lane departure warning systems. However, under\ncertain challenging circumstances, it is difficult to get satisfactory\nperformance in accurately detecting the lanes from one single image as mostly\ndone in current literature. Since lane markings are continuous lines, the lanes\nthat are difficult to be accurately detected in the current single image can\npotentially be better deduced if information from previous frames is\nincorporated. This study proposes a novel hybrid spatial-temporal (ST)\nsequence-to-one deep learning architecture. This architecture makes full use of\nthe ST information in multiple continuous image frames to detect the lane\nmarkings in the very last frame. Specifically, the hybrid model integrates the\nfollowing aspects: (a) the single image feature extraction module equipped with\nthe spatial convolutional neural network; (b) the ST feature integration module\nconstructed by ST recurrent neural network; (c) the encoder-decoder structure,\nwhich makes this image segmentation problem work in an end-to-end supervised\nlearning format. Extensive experiments reveal that the proposed model\narchitecture can effectively handle challenging driving scenes and outperforms\navailable state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yongqi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Sandeep Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arem_B/0/1/0/all/0/1\">Bart van Arem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farah_H/0/1/0/all/0/1\">Haneen Farah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointMixer: MLP-Mixer for Point Cloud Understanding. (arXiv:2111.11187v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11187","description":"<p>MLP-Mixer has newly appeared as a new challenger against the realm of CNNs\nand transformer. Despite its simplicity compared to transformer, the concept of\nchannel-mixing MLPs and token-mixing MLPs achieves noticeable performance in\nvisual recognition tasks. Unlike images, point clouds are inherently sparse,\nunordered and irregular, which limits the direct use of MLP-Mixer for point\ncloud understanding. In this paper, we propose PointMixer, a universal point\nset operator that facilitates information sharing among unstructured 3D points.\nBy simply replacing token-mixing MLPs with a softmax function, PointMixer can\n\"mix\" features within/between point sets. By doing so, PointMixer can be\nbroadly used in the network as inter-set mixing, intra-set mixing, and pyramid\nmixing. Extensive experiments show the competitive or superior performance of\nPointMixer in semantic segmentation, classification, and point reconstruction\nagainst transformer-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1\">Jaesung Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Space-Partitioning RANSAC. (arXiv:2111.12385v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12385","description":"<p>A new algorithm is proposed to accelerate RANSAC model quality calculations.\nThe method is based on partitioning the joint correspondence space, e.g., 2D-2D\npoint correspondences, into a pair of regular grids. The grid cells are mapped\nby minimal sample models, estimated within RANSAC, to reject correspondences\nthat are inconsistent with the model parameters early. The proposed technique\nis general. It works with arbitrary transformations even if a point is mapped\nto a point set, e.g., as a fundamental matrix maps to epipolar lines. The\nmethod is tested on thousands of image pairs from publicly available datasets\non fundamental and essential matrix, homography and radially distorted\nhomography estimation. On average, it reduces the RANSAC run-time by 41% with\nprovably no deterioration in the accuracy. It can be straightforwardly plugged\ninto state-of-the-art RANSAC frameworks, e.g. VSAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valasek_G/0/1/0/all/0/1\">Gabor Valasek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning. (arXiv:2111.12990v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2111.12990","description":"<p>Is intelligence realized by connectionist or classicist? While connectionist\napproaches have achieved superhuman performance, there has been growing\nevidence that such task-specific superiority is particularly fragile in\nsystematic generalization. This observation lies in the central debate between\nconnectionist and classicist, wherein the latter continually advocates an\nalgebraic treatment in cognitive architectures. In this work, we follow the\nclassicist's call and propose a hybrid approach to improve systematic\ngeneralization in reasoning. Specifically, we showcase a prototype with\nalgebraic representation for the abstract spatial-temporal reasoning task of\nRaven's Progressive Matrices (RPM) and present the ALgebra-Aware\nNeuro-Semi-Symbolic (ALANS) learner. The ALANS learner is motivated by abstract\nalgebra and the representation theory. It consists of a neural visual\nperception frontend and an algebraic abstract reasoning backend: the frontend\nsummarizes the visual information from object-based representation, while the\nbackend transforms it into an algebraic structure and induces the hidden\noperator on the fly. The induced operator is later executed to predict the\nanswer's representation, and the choice most similar to the prediction is\nselected as the solution. Extensive experiments show that by incorporating an\nalgebraic treatment, the ALANS learner outperforms various pure connectionist\nmodels in domains requiring systematic generalization. We further show the\ngenerative nature of the learned algebraic representation; it can be decoded by\nisomorphism to generate an answer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sirui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1\">Baoxiong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManiFest: Manifold Deformation for Few-shot Image Translation. (arXiv:2111.13681v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13681","description":"<p>Most image-to-image translation methods require a large number of training\nimages, which restricts their applicability. We instead propose ManiFest: a\nframework for few-shot image translation that learns a context-aware\nrepresentation of a target domain from a few images only. To enforce feature\nconsistency, our framework learns a style manifold between source and proxy\nanchor domains (assumed to be composed of large numbers of images). The learned\nmanifold is interpolated and deformed towards the few-shot target domain via\npatch-based adversarial and feature statistics alignment losses. All of these\ncomponents are trained simultaneously during a single end-to-end loop. In\naddition to the general few-shot translation task, our approach can\nalternatively be conditioned on a single exemplar image to reproduce its\nspecific style. Extensive experiments demonstrate the efficacy of ManiFest on\nmultiple tasks, outperforming the state-of-the-art on all metrics and in both\nthe general- and exemplar-based scenarios. Our code is available at\nhttps://github.com/cv-rits/Manifest .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Image Transformations for Transfer-based Adversarial Attack. (arXiv:2111.13844v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13844","description":"<p>Adversarial attacks provide a good way to study the robustness of deep\nlearning models. One category of methods in transfer-based black-box attack\nutilizes several image transformation operations to improve the transferability\nof adversarial examples, which is effective, but fails to take the specific\ncharacteristic of the input image into consideration. In this work, we propose\na novel architecture, called Adaptive Image Transformation Learner (AITL),\nwhich incorporates different image transformation operations into a unified\nframework to further improve the transferability of adversarial examples.\nUnlike the fixed combinational transformations used in existing works, our\nelaborately designed transformation learner adaptively selects the most\neffective combination of image transformations specific to the input image.\nExtensive experiments on ImageNet demonstrate that our method significantly\nimproves the attack success rates on both normally trained models and defense\nmodels under various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Discriminative Shrinkage Deep Networks for Image Deconvolution. (arXiv:2111.13876v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13876","description":"<p>Most existing methods usually formulate the non-blind deconvolution problem\ninto a maximum-a-posteriori framework and address it by manually designing\nkinds of regularization terms and data terms of the latent clear images.\nHowever, explicitly designing these two terms is quite challenging and usually\nleads to complex optimization problems which are difficult to solve. In this\npaper, we propose an effective non-blind deconvolution approach by learning\ndiscriminative shrinkage functions to implicitly model these terms. In contrast\nto most existing methods that use deep convolutional neural networks (CNNs) or\nradial basis functions to simply learn the regularization term, we formulate\nboth the data term and regularization term and split the deconvolution model\ninto data-related and regularization-related sub-problems according to the\nalternating direction method of multipliers. We explore the properties of the\nMaxout function and develop a deep CNN model with a Maxout layer to learn\ndiscriminative shrinkage functions to directly approximate the solutions of\nthese two sub-problems. Moreover, given the fast-Fourier-transform-based image\nrestoration usually leads to ringing artifacts while conjugate-gradient-based\napproach is time-consuming, we develop the Conjugate Gradient Network to\nrestore the latent clear images effectively and efficiently. Experimental\nresults show that the proposed method performs favorably against the\nstate-of-the-art ones in terms of efficiency and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_P/0/1/0/all/0/1\">Pin-Hung Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"META: Mimicking Embedding via oThers' Aggregation for Generalizable Person Re-identification. (arXiv:2112.08684v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08684","description":"<p>Domain generalizable (DG) person re-identification (ReID) aims to test across\nunseen domains without access to the target domain data at training time, which\nis a realistic but challenging problem. In contrast to methods assuming an\nidentical model for different domains, Mixture of Experts (MoE) exploits\nmultiple domain-specific networks for leveraging complementary information\nbetween domains, obtaining impressive results. However, prior MoE-based DG ReID\nmethods suffer from a large model size with the increase of the number of\nsource domains, and most of them overlook the exploitation of domain-invariant\ncharacteristics. To handle the two issues above, this paper presents a new\napproach called Mimicking Embedding via oThers' Aggregation (META) for DG ReID.\nTo avoid the large model size, experts in META do not add a branch network for\neach source domain but share all the parameters except for the batch\nnormalization layers. Besides multiple experts, META leverages Instance\nNormalization (IN) and introduces it into a global branch to pursue invariant\nfeatures across domains. Meanwhile, META considers the relevance of an unseen\ntarget sample and source domains via normalization statistics and develops an\naggregation network to adaptively integrate multiple experts for mimicking\nunseen target domain. Benefiting from a proposed consistency loss and an\nepisodic training algorithm, we can expect META to mimic embedding for a truly\nunseen target domain. Extensive experiments verify that META surpasses\nstate-of-the-art DG ReID methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Boqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Uncertain Single-View Depths in Colonoscopies. (arXiv:2112.08906v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08906","description":"<p>Estimating depth information from endoscopic images is a prerequisite for a\nwide set of AI-assisted technologies, such as accurate localization and\nmeasurement of tumors, or identification of non-inspected areas. As the domain\nspecificity of colonoscopies -- deformable low-texture environments with\nfluids, poor lighting conditions and abrupt sensor motions -- pose challenges\nto multi-view 3D reconstructions, single-view depth learning stands out as a\npromising line of research. Depth learning can be extended in a Bayesian\nsetting, which enables continual learning, improves decision making and can be\nused to compute confidence intervals or quantify uncertainty for in-body\nmeasurements. In this paper, we explore for the first time Bayesian deep\nnetworks for single-view depth estimation in colonoscopies. Our specific\ncontribution is two-fold: 1) an exhaustive analysis of scalable Bayesian\nnetworks for depth learning in different datasets, highlighting challenges and\nconclusions regarding synthetic-to-real domain changes and supervised vs.\nself-supervised methods; and 2) a novel teacher-student approach to deep depth\nlearning that takes into account the teacher uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Puigvert_J/0/1/0/all/0/1\">Javier Rodr&#xed;guez-Puigvert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recasens_D/0/1/0/all/0/1\">David Recasens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Cantin_R/0/1/0/all/0/1\">Rub&#xe9;n Mart&#xed;nez-Cant&#xed;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Responsive Listening Head Generation: A Benchmark Dataset and Baseline. (arXiv:2112.13548v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13548","description":"<p>We present a new listening head generation benchmark, for synthesizing\nresponsive feedbacks of a listener (e.g., nod, smile) during a face-to-face\nconversation. As the indispensable complement to talking heads generation,\nlistening head generation has seldomly been studied in literature.\nAutomatically synthesizing listening behavior that actively responds to a\ntalking head, is critical to applications such as digital human, virtual agents\nand social robots. In this work, we propose a novel dataset \"ViCo\",\nhighlighting the listening head generation during a face-to-face conversation.\nA total number of 92 identities (67 speakers and 76 listeners) are involved in\nViCo, featuring 483 clips in a paired \"speaking-listening\" pattern, where\nlisteners show three listening styles based on their attitudes: positive,\nneutral, negative. Different from traditional speech-to-gesture or talking-head\ngeneration, listening head generation takes as input both the audio and visual\nsignals from the speaker, and gives non-verbal feedbacks (e.g., head motions,\nfacial expressions) in a real-time manner. Our dataset supports a wide range of\napplications such as human-to-human interaction, video-to-video translation,\ncross-modal understanding and generation. To encourage further research, we\nalso release a listening head generation baseline, conditioning on different\nlistening attitudes. Code &amp; ViCo dataset: https://project.mhzhou.com/vico.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mohan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yalong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Visual-Auditory Human-eye Fixation Prediction with Multigranularity Perception. (arXiv:2112.13697v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13697","description":"<p>Thanks to the rapid advances in deep learning techniques and the wide\navailability of large-scale training sets, the performance of video saliency\ndetection models has been improving steadily and significantly. However, deep\nlearning-based visualaudio fixation prediction is still in its infancy. At\npresent, only a few visual-audio sequences have been furnished, with real\nfixations being recorded in real visual-audio environments. Hence, it would be\nneither efficient nor necessary to recollect real fixations under the same\nvisual-audio circumstances. To address this problem, this paper promotes a\nnovel approach in a weakly supervised manner to alleviate the demand of\nlarge-scale training sets for visual-audio model training. By using only the\nvideo category tags, we propose the selective class activation mapping (SCAM)\nand its upgrade (SCAM+). In the spatial-temporal-audio circumstance, the former\nfollows a coarse-to-fine strategy to select the most discriminative regions,\nand these regions are usually capable of exhibiting high consistency with the\nreal human-eye fixations. The latter equips the SCAM with an additional\nmulti-granularity perception mechanism, making the whole process more\nconsistent with that of the real human visual system. Moreover, we distill\nknowledge from these regions to obtain complete new spatial-temporal-audio\n(STA) fixation prediction (FP) networks, enabling broad applications in cases\nwhere video tags are not available. Without resorting to any real human-eye\nfixation, the performances of these STA FP networks are comparable to those of\nfully supervised networks. The code and results are publicly available at\nhttps://github.com/guotaowang/STANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guotao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chenglizhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_A/0/1/0/all/0/1\">Aimin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hong Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poseur: Direct Human Pose Regression with Transformers. (arXiv:2201.07412v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07412","description":"<p>We propose a direct, regression-based approach to 2D human pose estimation\nfrom single images. We formulate the problem as a sequence prediction task,\nwhich we solve using a Transformer network. This network directly learns a\nregression mapping from images to the keypoint coordinates, without resorting\nto intermediate representations such as heatmaps. This approach avoids much of\nthe complexity associated with heatmap-based approaches. To overcome the\nfeature misalignment issues of previous regression-based methods, we propose an\nattention mechanism that adaptively attends to the features that are most\nrelevant to the target keypoints, considerably improving the accuracy.\nImportantly, our framework is end-to-end differentiable, and naturally learns\nto exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,\ntwo predominant pose-estimation datasets, demonstrate that our method\nsignificantly improves upon the state-of-the-art in regression-based pose\nestimation. More notably, ours is the first regression-based approach to\nperform favorably compared to the best heatmap-based pose estimation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weian Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yongtao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhibin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing the Cost of Model Extraction with Calibrated Proof of Work. (arXiv:2201.09243v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2201.09243","description":"<p>In model extraction attacks, adversaries can steal a machine learning model\nexposed via a public API by repeatedly querying it and adjusting their own\nmodel based on obtained predictions. To prevent model stealing, existing\ndefenses focus on detecting malicious queries, truncating, or distorting\noutputs, thus necessarily introducing a tradeoff between robustness and model\nutility for legitimate users. Instead, we propose to impede model extraction by\nrequiring users to complete a proof-of-work before they can read the model's\npredictions. This deters attackers by greatly increasing (even up to 100x) the\ncomputational effort needed to leverage query access for model extraction.\nSince we calibrate the effort required to complete the proof-of-work to each\nquery, this only introduces a slight overhead for regular users (up to 2x). To\nachieve this, our calibration applies tools from differential privacy to\nmeasure the information revealed by a query. Our method requires no\nmodification of the victim model and can be applied by machine learning\npractitioners to guard their publicly exposed models against being easily\nstolen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1\">Adam Dziedzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaleem_M/0/1/0/all/0/1\">Muhammad Ahmad Kaleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Shen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1\">Nicolas Papernot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FORML: Learning to Reweight Data for Fairness. (arXiv:2202.01719v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01719","description":"<p>Machine learning models are trained to minimize the mean loss for a single\nmetric, and thus typically do not consider fairness and robustness. Neglecting\nsuch metrics in training can make these models prone to fairness violations\nwhen training data are imbalanced or test distributions differ. This work\nintroduces Fairness Optimized Reweighting via Meta-Learning (FORML), a training\nalgorithm that balances fairness and robustness with accuracy by jointly\nlearning training sample weights and neural network parameters. The approach\nincreases model fairness by learning to balance the contributions from both\nover- and under-represented sub-groups through dynamic reweighting of the data\nlearned from a user-specified held-out set representative of the distribution\nunder which fairness is desired. FORML improves equality of opportunity\nfairness criteria on image classification tasks, reduces bias of corrupted\nlabels, and facilitates building more fair datasets via data condensation.\nThese improvements are achieved without pre-processing data or post-processing\nmodel outputs, without learning an additional weighting function, without\nchanging model architecture, and while maintaining accuracy on the original\npredictive metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bobby Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seto_S/0/1/0/all/0/1\">Skyler Seto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks. (arXiv:2202.07261v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07261","description":"<p>3D dynamic point clouds provide a discrete representation of real-world\nobjects or scenes in motion, which have been widely applied in immersive\ntelepresence, autonomous driving, surveillance, \\textit{etc}. However, point\nclouds acquired from sensors are usually perturbed by noise, which affects\ndownstream tasks such as surface reconstruction and analysis. Although many\nefforts have been made for static point cloud denoising, few works address\ndynamic point cloud denoising. In this paper, we propose a novel gradient-based\ndynamic point cloud denoising method, exploiting the temporal correspondence\nfor the estimation of gradient fields -- also a fundamental problem in dynamic\npoint cloud processing and analysis. The gradient field is the gradient of the\nlog-probability function of the noisy point cloud, based on which we perform\ngradient ascent so as to converge each point to the underlying clean surface.\nWe estimate the gradient of each surface patch by exploiting the temporal\ncorrespondence, where the temporally corresponding patches are searched\nleveraging on rigid motion in classical mechanics. In particular, we treat each\npatch as a rigid object, which moves in the gradient field of an adjacent frame\nvia force until reaching a balanced state, i.e., when the sum of gradients over\nthe patch reaches 0. Since the gradient would be smaller when the point is\ncloser to the underlying surface, the balanced patch would fit the underlying\nsurface well, thus leading to the temporal correspondence. Finally, the\nposition of each point in the patch is updated along the direction of the\ngradient averaged from corresponding patches in adjacent frames. Experimental\nresults demonstrate that the proposed model outperforms state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qianjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provable Stochastic Optimization for Global Contrastive Learning: Small Batch Does Not Harm Performance. (arXiv:2202.12387v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.12387","description":"<p>In this paper, we study contrastive learning from an optimization\nperspective, aiming to analyze and address a fundamental issue of existing\ncontrastive learning methods that either rely on a large batch size or a large\ndictionary of feature vectors. We consider a global objective for contrastive\nlearning, which contrasts each positive pair with all negative pairs for an\nanchor point. From the optimization perspective, we explain why existing\nmethods such as SimCLR require a large batch size in order to achieve a\nsatisfactory result. In order to remove such requirement, we propose a\nmemory-efficient Stochastic Optimization algorithm for solving the Global\nobjective of Contrastive Learning of Representations, named SogCLR. We show\nthat its optimization error is negligible under a reasonable condition after a\nsufficient number of iterations or is diminishing for a slightly different\nglobal contrastive objective. Empirically, we demonstrate that SogCLR with\nsmall batch size (e.g., 256) can achieve similar performance as SimCLR with\nlarge batch size (e.g., 8192) on self-supervised learning task on ImageNet-1K.\nWe also attempt to show that the proposed optimization technique is generic and\ncan be applied to solving other contrastive losses, e.g., two-way contrastive\nlosses for bimodal contrastive learning. The proposed method is implemented in\nour open-sourced library LibAUC (www.libauc.org).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuoning Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zi-Hao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations. (arXiv:2203.01325v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01325","description":"<p>In this paper, we consider two challenging issues in reference-based\nsuper-resolution (RefSR), (i) how to choose a proper reference image, and (ii)\nhow to learn real-world RefSR in a self-supervised manner. Particularly, we\npresent a novel self-supervised learning approach for real-world image SR from\nobservations at dual camera zooms (SelfDZSR). Considering the popularity of\nmultiple cameras in modern smartphones, the more zoomed (telephoto) image can\nbe naturally leveraged as the reference to guide the SR of the lesser zoomed\n(short-focus) image. Furthermore, SelfDZSR learns a deep network to obtain the\nSR result of short-focus image to have the same resolution as the telephoto\nimage. For this purpose, we take the telephoto image instead of an additional\nhigh-resolution image as the supervision information and select a center patch\nfrom it as the reference to super-resolve the corresponding short-focus image\npatch. To mitigate the effect of the misalignment between short-focus\nlow-resolution (LR) image and telephoto ground-truth (GT) image, we design an\nauxiliary-LR generator and map the GT to an auxiliary-LR while keeping the\nspatial position unchanged. Then the auxiliary-LR can be utilized to deform the\nLR features by the proposed adaptive spatial transformer networks (AdaSTN), and\nmatch the Ref features to GT. During testing, SelfDZSR can be directly deployed\nto super-solve the whole short-focus image with the reference of telephoto\nimage. Experiments show that our method achieves better quantitative and\nqualitative performance against state-of-the-arts. Codes are available at\nhttps://github.com/cszhilu1998/SelfDZSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhilu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Ruohao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hongzhi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yunjin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextformer: A Transformer with Spatio-Channel Attention for Context Modeling in Learned Image Compression. (arXiv:2203.02452v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.02452","description":"<p>Entropy modeling is a key component for high-performance image compression\nalgorithms. Recent developments in autoregressive context modeling helped\nlearning-based methods to surpass their classical counterparts. However, the\nperformance of those models can be further improved due to the underexploited\nspatio-channel dependencies in latent space, and the suboptimal implementation\nof context adaptivity. Inspired by the adaptive characteristics of the\ntransformers, we propose a transformer-based context model, named\nContextformer, which generalizes the de facto standard attention mechanism to\nspatio-channel attention. We replace the context model of a modern compression\nframework with the Contextformer and test it on the widely used Kodak,\nCLIC2020, and Tecnick image datasets. Our experimental results show that the\nproposed model provides up to 11% rate savings compared to the standard\nVersatile Video Coding (VVC) Test Model (VTM) 16.2, and outperforms various\nlearning-based models in terms of PSNR and MS-SSIM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Koyuncu_A/0/1/0/all/0/1\">A. Burakhan Koyuncu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_H/0/1/0/all/0/1\">Han Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boev_A/0/1/0/all/0/1\">Atanas Boev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaikov_G/0/1/0/all/0/1\">Georgii Gaikov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alshina_E/0/1/0/all/0/1\">Elena Alshina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Steinbach_E/0/1/0/all/0/1\">Eckehard Steinbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PASS: Part-Aware Self-Supervised Pre-Training for Person Re-Identification. (arXiv:2203.03931v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03931","description":"<p>In person re-identification (ReID), very recent researches have validated\npre-training the models on unlabelled person images is much better than on\nImageNet. However, these researches directly apply the existing self-supervised\nlearning (SSL) methods designed for image classification to ReID without any\nadaption in the framework. These SSL methods match the outputs of local views\n(e.g., red T-shirt, blue shorts) to those of the global views at the same time,\nlosing lots of details. In this paper, we propose a ReID-specific pre-training\nmethod, Part-Aware Self-Supervised pre-training (PASS), which can generate\npart-level features to offer fine-grained information and is more suitable for\nReID. PASS divides the images into several local areas, and the local views\nrandomly cropped from each area are assigned with a specific learnable [PART]\ntoken. On the other hand, the [PART]s of all local areas are also appended to\nthe global views. PASS learns to match the output of the local views and global\nviews on the same [PART]. That is, the learned [PART] of the local views from a\nlocal area is only matched with the corresponding [PART] learned from the\nglobal views. As a result, each [PART] can focus on a specific local area of\nthe image and extracts fine-grained information of this area. Experiments show\nPASS sets the new state-of-the-art performances on Market1501 and MSMT17 on\nvarious ReID tasks, e.g., vanilla ViT-S/16 pre-trained by PASS achieves\n92.2\\%/90.2\\%/88.5\\% mAP accuracy on Market1501 for supervised/UDA/USL ReID.\nOur codes are available at https://github.com/CASIA-IVA-Lab/PASS-reID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haiyun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tianyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankSeg: Adaptive Pixel Classification with Image Category Ranking for Segmentation. (arXiv:2203.04187v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04187","description":"<p>The segmentation task has traditionally been formulated as a complete-label\npixel classification task to predict a class for each pixel from a fixed number\nof predefined semantic categories shared by all images or videos. Yet,\nfollowing this formulation, standard architectures will inevitably encounter\nvarious challenges under more realistic settings where the scope of categories\nscales up (e.g., beyond the level of 1k). On the other hand, in a typical image\nor video, only a few categories, i.e., a small subset of the complete label are\npresent. Motivated by this intuition, in this paper, we propose to decompose\nsegmentation into two sub-problems: (i) image-level or video-level multi-label\nclassification and (ii) pixel-level rank-adaptive selected-label\nclassification. Given an input image or video, our framework first conducts\nmulti-label classification over the complete label, then sorts the complete\nlabel and selects a small subset according to their class confidence scores. We\nthen use a rank-adaptive pixel classifier to perform the pixel-wise\nclassification over only the selected labels, which uses a set of rank-oriented\nlearnable temperature parameters to adjust the pixel classifications scores.\nOur approach is conceptually general and can be used to improve various\nexisting segmentation frameworks by simply using a lightweight multi-label\nclassification head and rank-adaptive pixel classifier. We demonstrate the\neffectiveness of our framework with competitive experimental results across\nfour tasks, including image semantic segmentation, image panoptic segmentation,\nvideo instance segmentation, and video semantic segmentation. Especially, with\nour RankSeg, Mask2Former gains +0.8%/+0.7%/+0.7% on ADE20K panoptic\nsegmentation/YouTubeVIS 2019 video instance segmentation/VSPW video semantic\nsegmentation benchmarks respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haodi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiangyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04203","description":"<p>A long-standing goal of intelligent assistants such as AR glasses/robots has\nbeen to assist users in affordance-centric real-world scenarios, such as \"how\ncan I run the microwave for 1 minute?\". However, there is still no clear task\ndefinition and suitable benchmarks. In this paper, we define a new task called\nAffordance-centric Question-driven Task Completion, where the AI assistant\nshould learn from instructional videos to provide step-by-step help in the\nuser's view. To support the task, we constructed AssistQ, a new dataset\ncomprising 531 question-answer samples from 100 newly filmed instructional\nvideos. We also developed a novel Question-to-Actions (Q2A) model to address\nthe AQTC task and validate it on the AssistQ dataset. The results show that our\nmodel significantly outperforms several VQA-related baselines while still\nhaving large room for improvement. We expect our task and dataset to advance\nEgocentric AI Assistant's development. Our project page is available at:\nhttps://showlab.github.io/assistq/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_B/0/1/0/all/0/1\">Benita Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Joya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1\">Dongxing Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align-Deform-Subtract: An Interventional Framework for Explaining Object Differences. (arXiv:2203.04694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04694","description":"<p>Given two object images, how can we explain their differences in terms of the\nunderlying object properties? To address this question, we propose\nAlign-Deform-Subtract (ADS) -- an interventional framework for explaining\nobject differences. By leveraging semantic alignments in image-space as\ncounterfactual interventions on the underlying object properties, ADS\niteratively quantifies and removes differences in object properties. The result\nis a set of \"disentangled\" error measures which explain object differences in\nterms of the underlying properties. Experiments on real and synthetic data\nillustrate the efficacy of the framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1\">Cian Eastwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanbo_L/0/1/0/all/0/1\">Li Nanbo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher K. I. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05203","description":"<p>3D dense captioning is a recently-proposed novel task, where point clouds\ncontain more geometric information than the 2D counterpart. However, it is also\nmore challenging due to the higher complexity and wider variety of inter-object\nrelations contained in point clouds. Existing methods only treat such relations\nas by-products of object feature learning in graphs without specifically\nencoding them, which leads to sub-optimal results. In this paper, aiming at\nimproving 3D dense captioning via capturing and utilizing the complex relations\nin the 3D scene, we propose MORE, a Multi-Order RElation mining model, to\nsupport generating more descriptive and comprehensive captions. Technically,\nour MORE encodes object relations in a progressive manner since complex\nrelations can be deduced from a limited number of basic ones. We first devise a\nnovel Spatial Layout Graph Convolution (SLGC), which semantically encodes\nseveral first-order relations as edges of a graph constructed over 3D object\nproposals. Next, from the resulting graph, we further extract multiple triplets\nwhich encapsulate basic first-order relations as the basic unit, and construct\nseveral Object-centric Triplet Attention Graphs (OTAG) to infer multi-order\nrelations for every target object. The updated node features from OTAG are\naggregated and fed into the caption decoder to provide abundant relational\ncues, so that captions including diverse relations with context objects can be\ngenerated. Extensive experiments on the Scan2Cap dataset prove the\neffectiveness of our proposed MORE and its components, and we also outperform\nthe current state-of-the-art method. Our code is available at\nhttps://github.com/SxJyJay/MORE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuromorphic Data Augmentation for Training Spiking Neural Networks. (arXiv:2203.06145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06145","description":"<p>Developing neuromorphic intelligence on event-based datasets with Spiking\nNeural Networks (SNNs) has recently attracted much research attention. However,\nthe limited size of event-based datasets makes SNNs prone to overfitting and\nunstable convergence. This issue remains unexplored by previous academic works.\nIn an effort to minimize this generalization gap, we propose Neuromorphic Data\nAugmentation (NDA), a family of geometric augmentations specifically designed\nfor event-based datasets with the goal of significantly stabilizing the SNN\ntraining and reducing the generalization gap between training and test\nperformance. The proposed method is simple and compatible with existing SNN\ntraining pipelines. Using the proposed augmentation, for the first time, we\ndemonstrate the feasibility of unsupervised contrastive learning for SNNs. We\nconduct comprehensive experiments on prevailing neuromorphic vision benchmarks\nand show that NDA yields substantial improvements over previous\nstate-of-the-art results. For example, the NDA-based SNN achieves accuracy gain\non CIFAR10-DVS and N-Caltech 101 by 10.1% and 13.7%, respectively. Code is\navailable on GitHub https://github.com/Intelligent-Computing-Lab-Yale/NDA_SNN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyoungseob Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geller_T/0/1/0/all/0/1\">Tamar Geller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bringing Rolling Shutter Images Alive with Dual Reversed Distortion. (arXiv:2203.06451v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06451","description":"<p>Rolling shutter (RS) distortion can be interpreted as the result of picking a\nrow of pixels from instant global shutter (GS) frames over time during the\nexposure of the RS camera. This means that the information of each instant GS\nframe is partially, yet sequentially, embedded into the row-dependent\ndistortion. Inspired by this fact, we address the challenging task of reversing\nthis process, i.e., extracting undistorted GS frames from images suffering from\nRS distortion. However, since RS distortion is coupled with other factors such\nas readout settings and the relative velocity of scene elements to the camera,\nmodels that only exploit the geometric correlation between temporally adjacent\nimages suffer from poor generality in processing data with different readout\nsettings and dynamic scenes with both camera motion and object motion. In this\npaper, instead of two consecutive frames, we propose to exploit a pair of\nimages captured by dual RS cameras with reversed RS directions for this highly\nchallenging task. Grounded on the symmetric and complementary nature of dual\nreversed distortion, we develop a novel end-to-end model, IFED, to generate\ndual optical flow sequence through iterative learning of the velocity field\nduring the RS time. Extensive experimental results demonstrate that IFED is\nsuperior to naive cascade schemes, as well as the state-of-the-art which\nutilizes adjacent RS images. Most importantly, although it is trained on a\nsynthetic dataset, IFED is shown to be effective at retrieving GS frame\nsequences from real-world RS distorted images of dynamic scenes. Code is\navailable at https://github.com/zzh-tech/Dual-Reversed-RS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhongyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Imari Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes. (arXiv:2203.09440v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09440","description":"<p>Many basic indoor activities such as eating or writing are always conducted\nupon different tabletops (e.g., coffee tables, writing desks). It is\nindispensable to understanding tabletop scenes in 3D indoor scene parsing\napplications. Unfortunately, it is hard to meet this demand by directly\ndeploying data-driven algorithms, since 3D tabletop scenes are rarely available\nin current datasets. To remedy this defect, we introduce TO-Scene, a\nlarge-scale dataset focusing on tabletop scenes, which contains 20,740 scenes\nwith three variants. To acquire the data, we design an efficient and scalable\nframework, where a crowdsourcing UI is developed to transfer CAD objects from\nModelNet and ShapeNet onto tables from ScanNet, then the output tabletop scenes\nare simulated into real scans and annotated automatically.\n</p>\n<p>Further, a tabletop-aware learning strategy is proposed for better perceiving\nthe small-sized tabletop instances. Notably, we also provide a real scanned\ntest set TO-Real to verify the practical value of TO-Scene. Experiments show\nthat the algorithms trained on TO-Scene indeed work on the realistic test data,\nand our proposed tabletop-aware learning strategy greatly improves the\nstate-of-the-art results on both 3D semantic segmentation and object detection\ntasks. Dataset and code are available at\nhttps://github.com/GAP-LAB-CUHK-SZ/TO-Scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mutian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haolin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoGS: Controllable Generation and Search from Sketch and Style. (arXiv:2203.09554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09554","description":"<p>We present CoGS, a novel method for the style-conditioned, sketch-driven\nsynthesis of images. CoGS enables exploration of diverse appearance\npossibilities for a given sketched object, enabling decoupled control over the\nstructure and the appearance of the output. Coarse-grained control over object\nstructure and appearance are enabled via an input sketch and an exemplar\n\"style\" conditioning image to a transformer-based sketch and style encoder to\ngenerate a discrete codebook representation. We map the codebook representation\ninto a metric space, enabling fine-grained control over selection and\ninterpolation between multiple synthesis options before generating the image\nvia a vector quantized GAN (VQGAN) decoder. Our framework thereby unifies\nsearch and synthesis tasks, in that a sketch and style pair may be used to run\nan initial synthesis which may be refined via combination with similar results\nin a search corpus to produce an image more closely matching the user's intent.\nWe show that our model, trained on the 125 object classes of our newly created\nPseudosketches dataset, is capable of producing a diverse gamut of semantic\ncontent and appearance styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ham_C/0/1/0/all/0/1\">Cusuh Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarres_G/0/1/0/all/0/1\">Gemma Canet Tarres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1\">James Hays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Visual Tracking by Segmentation. (arXiv:2203.11191v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11191","description":"<p>Estimating the target extent poses a fundamental challenge in visual object\ntracking. Typically, trackers are box-centric and fully rely on a bounding box\nto define the target in the scene. In practice, objects often have complex\nshapes and are not aligned with the image axis. In these cases, bounding boxes\ndo not provide an accurate description of the target and often contain a\nmajority of background pixels. We propose a segmentation-centric tracking\npipeline that not only produces a highly accurate segmentation mask, but also\ninternally works with segmentation masks instead of bounding boxes. Thus, our\ntracker is able to better learn a target representation that clearly\ndifferentiates the target in the scene from background content. In order to\nachieve the necessary robustness for the challenging tracking scenario, we\npropose a separate instance localization component that is used to condition\nthe segmentation decoder when producing the output mask. We infer a bounding\nbox from the segmentation mask, validate our tracker on challenging tracking\ndatasets and achieve the new state of the art on LaSOT with a success AUC score\nof 69.7%. Since most tracking datasets do not contain mask annotations, we\ncannot use them to evaluate predicted segmentation masks. Instead, we validate\nour segmentation quality on two popular video object segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_M/0/1/0/all/0/1\">Matthieu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_C/0/1/0/all/0/1\">Christoph Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Scene Graph Generation with Data Transfer. (arXiv:2203.11654v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11654","description":"<p>Scene graph generation (SGG) is designed to extract (subject, predicate,\nobject) triplets in images. Recent works have made a steady progress on SGG,\nand provide useful tools for high-level vision and language understanding.\nHowever, due to the data distribution problems including long-tail distribution\nand semantic ambiguity, the predictions of current SGG models tend to collapse\nto several frequent but uninformative predicates (e.g., on, at), which limits\npractical application of these models in downstream tasks. To deal with the\nproblems above, we propose a novel Internal and External Data Transfer\n(IETrans) method, which can be applied in a plug-and-play fashion and expanded\nto large SGG with 1,807 predicate classes. Our IETrans tries to relieve the\ndata distribution problem by automatically creating an enhanced dataset that\nprovides more sufficient and coherent annotations for all predicates. By\ntraining on the enhanced dataset, a Neural Motif model doubles the macro\nperformance while maintaining competitive micro performance. The code and data\nare publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Prompt Tuning. (arXiv:2203.12119v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12119","description":"<p>The current modus operandi in adapting pre-trained models involves updating\nall the backbone parameters, ie, full fine-tuning. This paper introduces Visual\nPrompt Tuning (VPT) as an efficient and effective alternative to full\nfine-tuning for large-scale Transformer models in vision. Taking inspiration\nfrom recent advances in efficiently tuning large language models, VPT\nintroduces only a small amount (less than 1% of model parameters) of trainable\nparameters in the input space while keeping the model backbone frozen. Via\nextensive experiments on a wide variety of downstream recognition tasks, we\nshow that VPT achieves significant performance gains compared to other\nparameter efficient tuning protocols. Most importantly, VPT even outperforms\nfull fine-tuning in many cases across model capacities and training data\nscales, while reducing per-task storage cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Luming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bor-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale and Cross-scale Contrastive Learning for Semantic Segmentation. (arXiv:2203.13409v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13409","description":"<p>This work considers supervised contrastive learning for semantic\nsegmentation. We apply contrastive learning to enhance the discriminative power\nof the multi-scale features extracted by semantic segmentation networks. Our\nkey methodological insight is to leverage samples from the feature spaces\nemanating from multiple stages of a model's encoder itself requiring neither\ndata augmentation nor online memory banks to obtain a diverse set of samples.\nTo allow for such an extension we introduce an efficient and effective sampling\nprocess, that enables applying contrastive losses over the encoder's features\nat multiple scales. Furthermore, by first mapping the encoder's multi-scale\nrepresentations to a common feature space, we instantiate a novel form of\nsupervised local-global constraint by introducing cross-scale contrastive\nlearning linking high-resolution local features to low-resolution global\nfeatures. Combined, our multi-scale and cross-scale contrastive losses boost\nperformance of various models (DeepLabV3, HRNet, OCRNet, UPerNet) with both CNN\nand Transformer backbones, when evaluated on 4 diverse datasets from natural\n(Cityscapes, PascalContext, ADE20K) but also surgical (CaDIS) domains. Our code\nis available at https://github.com/RViMLab/MS_CS_ContrSeg. datasets from\nnatural (Cityscapes, PascalContext, ADE20K) but also surgical (CaDIS) domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pissas_T/0/1/0/all/0/1\">Theodoros Pissas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravasio_C/0/1/0/all/0/1\">Claudio S. Ravasio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1\">Lyndon Da Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergeles_C/0/1/0/all/0/1\">Christos Bergeles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Learning Neural Representations from Shadows. (arXiv:2203.15946v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15946","description":"<p>We present a method that learns neural shadow fields which are neural scene\nrepresentations that are only learnt from the shadows present in the scene.\nWhile traditional shape-from-shadow (SfS) algorithms reconstruct geometry from\nshadows, they assume a fixed scanning setup and fail to generalize to complex\nscenes. Neural rendering algorithms, on the other hand, rely on photometric\nconsistency between RGB images, but largely ignore physical cues such as\nshadows, which have been shown to provide valuable information about the scene.\nWe observe that shadows are a powerful cue that can constrain neural scene\nrepresentations to learn SfS, and even outperform NeRF to reconstruct otherwise\nhidden geometry. We propose a graphics-inspired differentiable approach to\nrender accurate shadows with volumetric rendering, predicting a shadow map that\ncan be compared to the ground truth shadow. Even with just binary shadow maps,\nwe show that neural rendering can localize the object and estimate coarse\ngeometry. Our approach reveals that sparse cues in images can be used to\nestimate geometry using differentiable volumetric rendering. Moreover, our\nframework is highly generalizable and can work alongside existing 3D\nreconstruction techniques that otherwise only use photometric consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_K/0/1/0/all/0/1\">Kushagra Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinghoffer_T/0/1/0/all/0/1\">Tzofi Klinghoffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PseCo: Pseudo Labeling and Consistency Training for Semi-Supervised Object Detection. (arXiv:2203.16317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16317","description":"<p>In this paper, we delve into two key techniques in Semi-Supervised Object\nDetection (SSOD), namely pseudo labeling and consistency training. We observe\nthat these two techniques currently neglect some important properties of object\ndetection, hindering efficient learning on unlabeled data. Specifically, for\npseudo labeling, existing works only focus on the classification score yet fail\nto guarantee the localization precision of pseudo boxes; For consistency\ntraining, the widely adopted random-resize training only considers the\nlabel-level consistency but misses the feature-level one, which also plays an\nimportant role in ensuring the scale invariance. To address the problems\nincurred by noisy pseudo boxes, we design Noisy Pseudo box Learning (NPL) that\nincludes Prediction-guided Label Assignment (PLA) and Positive-proposal\nConsistency Voting (PCV). PLA relies on model predictions to assign labels and\nmakes it robust to even coarse pseudo boxes; while PCV leverages the regression\nconsistency of positive proposals to reflect the localization quality of pseudo\nboxes. Furthermore, in consistency training, we propose Multi-view\nScale-invariant Learning (MSL) that includes mechanisms of both label- and\nfeature-level consistency, where feature consistency is achieved by aligning\nshifted feature pyramids between two images with identical content but varied\nscales. On COCO benchmark, our method, termed PSEudo labeling and COnsistency\ntraining (PseCo), outperforms the SOTA (Soft Teacher) by 2.0, 1.8, 2.0 points\nunder 1%, 5%, and 10% labelling ratios, respectively. It also significantly\nimproves the learning efficiency for SSOD, e.g., PseCo halves the training time\nof the SOTA approach but achieves even better performance. Code is available at\nhttps://github.com/ligang-cs/PseCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yichao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanshan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RFNet-4D: Joint Object Reconstruction and Flow Estimation from 4D Point Clouds. (arXiv:2203.16482v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16482","description":"<p>Object reconstruction from 3D point clouds has achieved impressive progress\nin the computer vision and computer graphics research field. However,\nreconstruction from time-varying point clouds (a.k.a. 4D point clouds) is\ngenerally overlooked. In this paper, we propose a new network architecture,\nnamely RFNet-4D, that jointly reconstruct objects and their motion flows from\n4D point clouds. The key insight is that simultaneously performing both tasks\nvia learning spatial and temporal features from a sequence of point clouds can\nleverage individual tasks, leading to improved overall performance. To prove\nthis ability, we design a temporal vector field learning module using\nunsupervised learning approach for flow estimation, leveraged by supervised\nlearning of spatial structures for object reconstruction. Extensive experiments\nand analyses on benchmark dataset validated the effectiveness and efficiency of\nour method. As shown in experimental results, our method achieves\nstate-of-the-art performance on both flow estimation and object reconstruction\nwhile performing much faster than existing methods in both training and\ninference. Our code and data are available at\nhttps://github.com/hkust-vgd/RFNet-4D\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tuan-Anh Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quang-Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai-Kit Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring hand use in the home after cervical spinal cord injury using egocentric video. (arXiv:2203.16996v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.16996","description":"<p>Background: Egocentric video has recently emerged as a potential solution for\nmonitoring hand function in individuals living with tetraplegia in the\ncommunity, especially for its ability to detect functional use in the home\nenvironment. Objective: To develop and validate a wearable vision-based system\nfor measuring hand use in the home among individuals living with tetraplegia.\nMethods: Several deep learning algorithms for detecting functional hand-object\ninteractions were developed and compared. The most accurate algorithm was used\nto extract measures of hand function from 65 hours of unscripted video recorded\nat home by 20 participants with tetraplegia. These measures were: the\npercentage of interaction time over total recording time (Perc); the average\nduration of individual interactions (Dur); the number of interactions per hour\n(Num). To demonstrate the clinical validity of the technology, egocentric\nmeasures were correlated with validated clinical assessments of hand function\nand independence (Graded Redefined Assessment of Strength, Sensibility and\nPrehension - GRASSP, Upper Extremity Motor Score - UEMS, and Spinal Cord\nIndependent Measure - SCIM). Results: Hand-object interactions were\nautomatically detected with a median F1-score of 0.80 (0.67-0.87). Our results\ndemonstrated that higher UEMS and better prehension were related to greater\ntime spent interacting, whereas higher SCIM and better hand sensation resulted\nin a higher number of interactions performed during the egocentric video\nrecordings. Conclusions: For the first time, measures of hand function\nautomatically estimated in an unconstrained environment in individuals with\ntetraplegia have been validated against internationally accepted measures of\nhand function. Future work will necessitate a formal evaluation of the\nreliability and responsiveness of the egocentric-based performance measures for\nhand use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bandini_A/0/1/0/all/0/1\">Andrea Bandini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dousty_M/0/1/0/all/0/1\">Mehdy Dousty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hitzig_S/0/1/0/all/0/1\">Sander L. Hitzig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Craven_B/0/1/0/all/0/1\">B. Catharine Craven</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalsi_Ryan_S/0/1/0/all/0/1\">Sukhvinder Kalsi-Ryan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zariffa_J/0/1/0/all/0/1\">Jos&#xe9; Zariffa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00298","description":"<p>To make full use of computer vision technology in stores, it is required to\nconsider the actual needs that fit the characteristics of the retail scene.\nPursuing this goal, we introduce the United Retail Datasets (Unitail), a\nlarge-scale benchmark of basic visual tasks on products that challenges\nalgorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped\ninstances annotated, the Unitail offers a detection dataset to align product\nappearance better. Furthermore, it provides a gallery-style OCR dataset\ncontaining 1454 product categories, 30k text regions, and 21k transcriptions to\nenable robust reading on products and motivate enhanced product matching.\nBesides benchmarking the datasets using various state-of-the-arts, we customize\na new detector for product detection and provide a simple OCR-based matching\nsolution that verifies its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaiwang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_J/0/1/0/all/0/1\">Jiachen Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_U/0/1/0/all/0/1\">Uzair Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval. (arXiv:2204.00486v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00486","description":"<p>Cognitive science has shown that humans perceive videos in terms of events\nseparated by the state changes of dominant subjects. State changes trigger new\nevents and are one of the most useful among the large amount of redundant\ninformation perceived. However, previous research focuses on the overall\nunderstanding of segments without evaluating the fine-grained status changes\ninside. In this paper, we introduce a new dataset called Kinetic-GEB+. The\ndataset consists of over 170k boundaries associated with captions describing\nstatus changes in the generic events in 12K videos. Upon this new dataset, we\npropose three tasks supporting the development of a more fine-grained, robust,\nand human-like understanding of videos through status changes. We evaluate many\nrepresentative baselines in our dataset, where we also design a new TPD\n(Temporal-based Pairwise Difference) Modeling method for visual difference and\nachieve significant performance improvements. Besides, the results show there\nare still formidable challenges for current methods in the utilization of\ndifferent granularities, representation of visual difference, and the accurate\nlocalization of status changes. Further analysis shows that our dataset can\ndrive developing more powerful methods to understand status changes and thus\nimprove video level comprehension. The dataset is available at\nhttps://github.com/Yuxuan-W/GEB-Plus\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1\">Matt Feiszli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00559","description":"<p>We introduce a camera relocalization pipeline that combines absolute pose\nregression (APR) and direct feature matching. By incorporating\nexposure-adaptive novel view synthesis, our method successfully addresses\nphotometric distortions in outdoor environments that existing photometric-based\nmethods fail to handle. With domain-invariant feature matching, our solution\nimproves pose regression accuracy using semi-supervised learning on unlabeled\ndata. In particular, the pipeline consists of two components: Novel View\nSynthesizer and DFNet. The former synthesizes novel views compensating for\nchanges in exposure and the latter regresses camera poses and extracts robust\nfeatures that close the domain gap between real images and synthetic ones.\nFurthermore, we introduce an online synthetic data generation scheme. We show\nthat these approaches effectively enhance camera pose estimation both in indoor\nand outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by\noutperforming existing single-image APR methods by as much as 56%, comparable\nto 3D structure-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Adrian Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2204.00570v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.00570","description":"<p>We consider unsupervised domain adaptation (UDA), where labeled data from a\nsource domain (e.g., photographs) and unlabeled data from a target domain\n(e.g., sketches) are used to learn a classifier for the target domain.\nConventional UDA methods (e.g., domain adversarial training) learn\ndomain-invariant features to improve generalization to the target domain. In\nthis paper, we show that contrastive pre-training, which learns features on\nunlabeled source and target data and then fine-tunes on labeled source data, is\ncompetitive with strong UDA methods. However, we find that contrastive\npre-training does not learn domain-invariant features, diverging from\nconventional UDA intuitions. We show theoretically that contrastive\npre-training can learn features that vary subtantially across domains but still\ngeneralize to the target domain, by disentangling domain and class information.\nOur results suggest that domain invariance is not necessary for UDA. We\nempirically validate our theory on benchmark vision datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">Kendrick Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Robbie Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ananya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An application of Pixel Interval Down-sampling (PID) for dense tiny microorganism counting on environmental microorganism images. (arXiv:2204.01341v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01341","description":"<p>This paper proposes a novel pixel interval down-sampling network (PID-Net)\nfor dense tiny object (yeast cells) counting tasks with higher accuracy. The\nPID-Net is an end-to-end convolutional neural network (CNN) model with an\nencoder--decoder architecture. The pixel interval down-sampling operations are\nconcatenated with max-pooling operations to combine the sparse and dense\nfeatures. This addresses the limitation of contour conglutination of dense\nobjects while counting. The evaluation was conducted using classical\nsegmentation metrics (the Dice, Jaccard and Hausdorff distance) as well as\ncounting metrics. The experimental results show that the proposed PID-Net had\nthe best performance and potential for dense tiny object counting tasks, which\nachieved 96.97\\% counting accuracy on the dataset with 2448 yeast cell images.\nBy comparing with the state-of-the-art approaches, such as Attention U-Net,\nSwin U-Net and Trans U-Net, the proposed PID-Net can segment dense tiny objects\nwith clearer boundaries and fewer incorrect debris, which shows the great\npotential of PID-Net in the task of accurate counting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yu-Dong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Hao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1\">Wenjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOSTER: Feature Boosting and Compression for Class-Incremental Learning. (arXiv:2204.04662v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04662","description":"<p>The ability to learn new concepts continually is necessary in this\never-changing world. However, deep neural networks suffer from catastrophic\nforgetting when learning new categories. Many works have been proposed to\nalleviate this phenomenon, whereas most of them either fall into the\nstability-plasticity dilemma or take too much computation or storage overhead.\nInspired by the gradient boosting algorithm to gradually fit the residuals\nbetween the target model and the previous ensemble model, we propose a novel\ntwo-stage learning paradigm FOSTER, empowering the model to learn new\ncategories adaptively. Specifically, we first dynamically expand new modules to\nfit the residuals between the target and the output of the original model.\nNext, we remove redundant parameters and feature dimensions through an\neffective distillation strategy to maintain the single backbone model. We\nvalidate our method FOSTER on CIFAR-100 and ImageNet-100/1000 under different\nsettings. Experimental results show that our method achieves state-of-the-art\nperformance. Code is available at: https://github.com/G-U-N/ECCV22-FOSTER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fu-Yun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Baselines for Image Restoration. (arXiv:2204.04676v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04676","description":"<p>Although there have been significant advances in the field of image\nrestoration recently, the system complexity of the state-of-the-art (SOTA)\nmethods is increasing as well, which may hinder the convenient analysis and\ncomparison of methods. In this paper, we propose a simple baseline that exceeds\nthe SOTA methods and is computationally efficient. To further simplify the\nbaseline, we reveal that the nonlinear activation functions, e.g. Sigmoid,\nReLU, GELU, Softmax, etc. are not necessary: they could be replaced by\nmultiplication or removed. Thus, we derive a Nonlinear Activation Free Network,\nnamely NAFNet, from the baseline. SOTA results are achieved on various\nchallenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring),\nexceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs;\n40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28\ndB with less than half of its computational costs. The code and the pre-trained\nmodels are released at https://github.com/megvii-research/NAFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v10 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. The image convolution operation\nhelps CNNs to get good performance on image-related tasks. However, the image\nconvolution has high computation complexity and hard to be implemented. This\npaper proposes the CEMNet, which can be trained in the frequency domain. The\nmost important motivation of this research is that we can use the\nstraightforward element-wise multiplication operation to replace the image\nconvolution in the frequency domain based on the Cross-Correlation Theorem,\nwhich obviously reduces the computation complexity. We further introduce a\nWeight Fixation mechanism to alleviate the problem of over-fitting, and analyze\nthe working behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.13170","description":"<p>In Federated Learning (FL), a number of clients or devices collaborate to\ntrain a model without sharing their data. Models are optimized locally at each\nclient and further communicated to a central hub for aggregation. While FL is\nan appealing decentralized training paradigm, heterogeneity among data from\ndifferent clients can cause the local optimization to drift away from the\nglobal objective. In order to estimate and therefore remove this drift,\nvariance reduction techniques have been incorporated into FL optimization\nrecently. However, these approaches inaccurately estimate the clients' drift\nand ultimately fail to remove it properly. In this work, we propose an adaptive\nalgorithm that accurately estimates drift across clients. In comparison to\nprevious works, our approach necessitates less storage and communication\nbandwidth, as well as lower compute costs. Additionally, our proposed\nmethodology induces stability by constraining the norm of estimates for client\ndrift, making it more practical for large scale FL. Experimental findings\ndemonstrate that the proposed algorithm converges significantly faster and\nachieves higher accuracy than the baselines across various FL benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varno_F/0/1/0/all/0/1\">Farshid Varno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saghayi_M/0/1/0/all/0/1\">Marzie Saghayi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevyeri_L/0/1/0/all/0/1\">Laya Rafiee Sevyeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sharut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1\">Stan Matwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havaei_M/0/1/0/all/0/1\">Mohammad Havaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2205.06230v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06230","description":"<p>Combining simple architectures with large-scale pre-training has led to\nmassive improvements in image classification. For object detection,\npre-training and scaling approaches are less well established, especially in\nthe long-tailed and open-vocabulary setting, where training data is relatively\nscarce. In this paper, we propose a strong recipe for transferring image-text\nmodels to open-vocabulary object detection. We use a standard Vision\nTransformer architecture with minimal modifications, contrastive image-text\npre-training, and end-to-end detection fine-tuning. Our analysis of the scaling\nproperties of this setup shows that increasing image-level pre-training and\nmodel size yield consistent improvements on the downstream detection task. We\nprovide the adaptation strategies and regularizations needed to attain very\nstrong performance on zero-shot text-conditioned and one-shot image-conditioned\nobject detection. Code and models are available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1\">Matthias Minderer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1\">Alexey Gritsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1\">Austin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_M/0/1/0/all/0/1\">Maxim Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weissenborn_D/0/1/0/all/0/1\">Dirk Weissenborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendran_A/0/1/0/all/0/1\">Aravindh Mahendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhuoran Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kipf_T/0/1/0/all/0/1\">Thomas Kipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Vertex Descent: A New Direction for 3D Human Model Fitting. (arXiv:2205.06254v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06254","description":"<p>We propose a novel optimization-based paradigm for 3D human model fitting on\nimages and scans. In contrast to existing approaches that directly regress the\nparameters of a low-dimensional statistical body model (e.g. SMPL) from input\nimages, we train an ensemble of per-vertex neural fields network. The network\npredicts, in a distributed manner, the vertex descent direction towards the\nground truth, based on neural features extracted at the current vertex\nprojection. At inference, we employ this network, dubbed LVD, within a\ngradient-descent optimization pipeline until its convergence, which typically\noccurs in a fraction of a second even when initializing all vertices into a\nsingle point. An exhaustive evaluation demonstrates that our approach is able\nto capture the underlying body of clothed people with very different body\nshapes, achieving a significant improvement compared to state-of-the-art. LVD\nis also applicable to 3D model fitting of humans and hands, for which we show a\nsignificant improvement to the SOTA with a much simpler and faster method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corona_E/0/1/0/all/0/1\">Enric Corona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alenya_G/0/1/0/all/0/1\">Guillem Aleny&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-rigid Point Cloud Registration with Neural Deformation Pyramid. (arXiv:2205.12796v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12796","description":"<p>Non-rigid point cloud registration is a key component in many computer vision\nand computer graphics applications. The high complexity of the unknown\nnon-rigid motion make this task a challenging problem. In this paper, we break\ndown this problem via hierarchical motion decomposition. Our method called\nNeural Deformation Pyramid (NDP) represents non-rigid motion using a pyramid\narchitecture. Each pyramid level, denoted by a Multi-Layer Perception (MLP),\ntakes as input a sinusoidally encoded 3D point and outputs its motion\nincrements from the previous level. The sinusoidal function starts with a low\ninput frequency and gradually increases when the pyramid level goes down. This\nallows a multi-level rigid to nonrigid motion decomposition and also speeds up\nthe solving by 50 times compared to the existing MLP-based approach. Our method\nachieves advanced partialto-partial non-rigid point cloud registration results\non the 4DMatch/4DLoMatch benchmark under both no-learned and supervised\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation Models. (arXiv:2205.15781v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15781","description":"<p>Semantic image segmentation is a central and challenging task in autonomous\ndriving, addressed by training deep models. Since this training draws to a\ncurse of human-based image labeling, using synthetic images with automatically\ngenerated labels together with unlabeled real-world images is a promising\nalternative. This implies to address an unsupervised domain adaptation (UDA)\nproblem. In this paper, we propose a new co-training procedure for\nsynth-to-real UDA of semantic segmentation models. It consists of a\nself-training stage, which provides two domain-adapted models, and a model\ncollaboration loop for the mutual improvement of these two models. These models\nare then used to provide the final semantic segmentation labels (pseudo-labels)\nfor the real-world images. The overall procedure treats the deep models as\nblack boxes and drives their collaboration at the level of pseudo-labeled\ntarget images, i.e., neither modifying loss functions is required, nor explicit\nfeature alignment. We test our proposal on standard synthetic and real-world\ndatasets for on-board semantic segmentation. Our procedure shows improvements\nranging from ~13 to ~26 mIoU points over baselines, so establishing new\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_J/0/1/0/all/0/1\">Jose L. G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villalonga_G/0/1/0/all/0/1\">Gabriel Villalonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Antonio M. L&#xf3;pez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection. (arXiv:2206.07458v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07458","description":"<p>The goal of this work is to reconstruct speech from a silent talking face\nvideo. Recent studies have shown impressive performance on synthesizing speech\nfrom silent talking face videos. However, they have not explicitly considered\non varying identity characteristics of different speakers, which place a\nchallenge in the video-to-speech synthesis, and this becomes more critical in\nunseen-speaker settings. Our approach is to separate the speech content and the\nvisage-style from a given silent talking face video. By guiding the model to\nindependently focus on modeling the two representations, we can obtain the\nspeech of high intelligibility from the model even when the input video of an\nunseen subject is given. To this end, we introduce speech-visage selection that\nseparates the speech content and the speaker identity from the visual features\nof the input video. The disentangled representations are jointly incorporated\nto synthesize speech through visage-style based synthesizer which generates\nspeech by coating the visage-styles while maintaining the speech content. Thus,\nthe proposed framework brings the advantage of synthesizing the speech\ncontaining the right content even with the silent talking face video of an\nunseen subject. We validate the effectiveness of the proposed framework on the\nGRID, TCD-TIMIT volunteer, and LRW datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joanna Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversified Adversarial Attacks based on Conjugate Gradient Method. (arXiv:2206.09628v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.09628","description":"<p>Deep learning models are vulnerable to adversarial examples, and adversarial\nattacks used to generate such examples have attracted considerable research\ninterest. Although existing methods based on the steepest descent have achieved\nhigh attack success rates, ill-conditioned problems occasionally reduce their\nperformance. To address this limitation, we utilize the conjugate gradient (CG)\nmethod, which is effective for this type of problem, and propose a novel attack\nalgorithm inspired by the CG method, named the Auto Conjugate Gradient (ACG)\nattack. The results of large-scale evaluation experiments conducted on the\nlatest robust models show that, for most models, ACG was able to find more\nadversarial examples with fewer iterations than the existing SOTA algorithm\nAuto-PGD (APGD). We investigated the difference in search performance between\nACG and APGD in terms of diversification and intensification, and define a\nmeasure called Diversity Index (DI) to quantify the degree of diversity. From\nthe analysis of the diversity using this index, we show that the more diverse\nsearch of the proposed method remarkably improves its attack success rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamamura_K/0/1/0/all/0/1\">Keiichiro Yamamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_H/0/1/0/all/0/1\">Haruki Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tateiwa_N/0/1/0/all/0/1\">Nariaki Tateiwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hata_N/0/1/0/all/0/1\">Nozomi Hata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitsutake_T/0/1/0/all/0/1\">Toru Mitsutake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oe_I/0/1/0/all/0/1\">Issa Oe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikura_H/0/1/0/all/0/1\">Hiroki Ishikura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujisawa_K/0/1/0/all/0/1\">Katsuki Fujisawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions. (arXiv:2206.14180v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.14180","description":"<p>Image-based virtual try-on aims to synthesize an image of a person wearing a\ngiven clothing item. To solve the task, the existing methods warp the clothing\nitem to fit the person's body and generate the segmentation map of the person\nwearing the item before fusing the item with the person. However, when the\nwarping and the segmentation generation stages operate individually without\ninformation exchange, the misalignment between the warped clothes and the\nsegmentation map occurs, which leads to the artifacts in the final image. The\ninformation disconnection also causes excessive warping near the clothing\nregions occluded by the body parts, so-called pixel-squeezing artifacts. To\nsettle the issues, we propose a novel try-on condition generator as a unified\nmodule of the two stages (i.e., warping and segmentation generation stages). A\nnewly proposed feature fusion block in the condition generator implements the\ninformation exchange, and the condition generator does not create any\nmisalignment or pixel-squeezing artifacts. We also introduce discriminator\nrejection that filters out the incorrect segmentation map predictions and\nassures the performance of virtual try-on frameworks. Experiments on a\nhigh-resolution dataset demonstrate that our model successfully handles the\nmisalignment and occlusion, and significantly outperforms the baselines. Code\nis available at https://github.com/sangyun884/HR-VITON.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Gyojung Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seunghwan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NARRATE: A Normal Assisted Free-View Portrait Stylizer. (arXiv:2207.00974v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00974","description":"<p>In this work, we propose NARRATE, a novel pipeline that enables\nsimultaneously editing portrait lighting and perspective in a photorealistic\nmanner. As a hybrid neural-physical face model, NARRATE leverages complementary\nbenefits of geometry-aware generative approaches and normal-assisted physical\nface models. In a nutshell, NARRATE first inverts the input portrait to a\ncoarse geometry and employs neural rendering to generate images resembling the\ninput, as well as producing convincing pose changes. However, inversion step\nintroduces mismatch, bringing low-quality images with less facial details. As\nsuch, we further estimate portrait normal to enhance the coarse geometry,\ncreating a high-fidelity physical face model. In particular, we fuse the neural\nand physical renderings to compensate for the imperfect inversion, resulting in\nboth realistic and view-consistent novel perspective images. In relighting\nstage, previous works focus on single view portrait relighting but ignoring\nconsistency between different perspectives as well, leading unstable and\ninconsistent lighting effects for view changes. We extend Total Relighting to\nfix this problem by unifying its multi-view input normal maps with the physical\nface model. NARRATE conducts relighting with consistent normal maps, imposing\ncross-view constraints and exhibiting stable and coherent illumination effects.\nWe experimentally demonstrate that NARRATE achieves more photorealistic,\nreliable results over prior works. We further bridge NARRATE with animation and\nstyle transfer tools, supporting pose change, light change, facial animation,\nand style transfer, either separately or in combination, all at a photographic\nquality. We showcase vivid free-view facial animations as well as 3D-aware\nrelightable stylization, which help facilitate various AR/VR applications like\nvirtual cinematography, 3D video conferencing, and post-production.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Youjia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiwen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harmonizer: Learning to Perform White-Box Image and Video Harmonization. (arXiv:2207.01322v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01322","description":"<p>Recent works on image harmonization solve the problem as a pixel-wise image\ntranslation task via large autoencoders. They have unsatisfactory performances\nand slow inference speeds when dealing with high-resolution images. In this\nwork, we observe that adjusting the input arguments of basic image filters,\ne.g., brightness and contrast, is sufficient for humans to produce realistic\nimages from the composite ones. Hence, we frame image harmonization as an\nimage-level regression problem to learn the arguments of the filters that\nhumans use for the task. We present a Harmonizer framework for image\nharmonization. Unlike prior methods that are based on black-box autoencoders,\nHarmonizer contains a neural network for filter argument prediction and several\nwhite-box filters (based on the predicted arguments) for image harmonization.\nWe also introduce a cascade regressor and a dynamic loss strategy for\nHarmonizer to learn filter arguments more stably and precisely. Since our\nnetwork only outputs image-level arguments and the filters we used are\nefficient, Harmonizer is much lighter and faster than existing methods.\nComprehensive experiments demonstrate that Harmonizer surpasses existing\nmethods notably, especially with high-resolution inputs. Finally, we apply\nHarmonizer to video harmonization, which achieves consistent results across\nframes and 56 fps at 1080P resolution. Code and models are available at:\nhttps://github.com/ZHKKKe/Harmonizer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zhanghan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chunyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphVid: It Only Takes a Few Nodes to Understand a Video. (arXiv:2207.01375v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01375","description":"<p>We propose a concise representation of videos that encode perceptually\nmeaningful features into graphs. With this representation, we aim to leverage\nthe large amount of redundancies in videos and save computations. First, we\nconstruct superpixel-based graph representations of videos by considering\nsuperpixels as graph nodes and create spatial and temporal connections between\nadjacent superpixels. Then, we leverage Graph Convolutional Networks to process\nthis representation and predict the desired output. As a result, we are able to\ntrain models with much fewer parameters, which translates into short training\nperiods and a reduction in computation resource requirements. A comprehensive\nexperimental study on the publicly available datasets Kinetics-400 and Charades\nshows that the proposed method is highly cost-effective and uses limited\ncommodity hardware during training and inference. It reduces the computational\nrequirements 10-fold while achieving results that are comparable to\nstate-of-the-art methods. We believe that the proposed approach is a promising\ndirection that could open the door to solving video understanding more\nefficiently and enable more resource limited users to thrive in this research\nfield.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kosman_E/0/1/0/all/0/1\">Eitan Kosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Self-supervised Learning for Video Understanding. (arXiv:2207.01975v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01975","description":"<p>The ubiquity of camera-enabled mobile devices has lead to large amounts of\nunlabelled video data being produced at the edge. Although various\nself-supervised learning (SSL) methods have been proposed to harvest their\nlatent spatio-temporal representations for task-specific training, practical\nchallenges including privacy concerns and communication costs prevent SSL from\nbeing deployed at large scales. To mitigate these issues, we propose the use of\nFederated Learning (FL) to the task of video SSL. In this work, we evaluate the\nperformance of current state-of-the-art (SOTA) video-SSL techniques and\nidentify their shortcomings when integrated into the large-scale FL setting\nsimulated with kinetics-400 dataset. We follow by proposing a novel federated\nSSL framework for video, dubbed FedVSSL, that integrates different aggregation\nstrategies and partial weight updating. Extensive experiments demonstrate the\neffectiveness and significance of FedVSSL as it outperforms the centralized\nSOTA for the downstream retrieval task by 6.66% on UCF-101 and 5.13% on\nHMDB-51.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rehman_Y/0/1/0/all/0/1\">Yasar Abbas Ur Rehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1\">Pedro Porto Buarque de Gusmao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas Lane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DG-STFM: 3D Geometric Guided Student-Teacher Feature Matching. (arXiv:2207.02375v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02375","description":"<p>We tackle the essential task of finding dense visual correspondences between\na pair of images. This is a challenging problem due to various factors such as\npoor texture, repetitive patterns, illumination variation, and motion blur in\npractical scenarios. In contrast to methods that use dense correspondence\nground-truths as direct supervision for local feature matching training, we\ntrain 3DG-STFM: a multi-modal matching model (Teacher) to enforce the depth\nconsistency under 3D dense correspondence supervision and transfer the\nknowledge to 2D unimodal matching model (Student). Both teacher and student\nmodels consist of two transformer-based matching modules that obtain dense\ncorrespondences in a coarse-to-fine manner. The teacher model guides the\nstudent model to learn RGB-induced depth information for the matching purpose\non both coarse and fine branches. We also evaluate 3DG-STFM on a model\ncompression task. To the best of our knowledge, 3DG-STFM is the first\nstudent-teacher learning method for the local feature matching task. The\nexperiments show that our method outperforms state-of-the-art methods on indoor\nand outdoor camera pose estimations, and homography estimation problems. Code\nis available at: https://github.com/Ryan-prime/3DG-STFM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Runyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yatong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation. (arXiv:2207.02466v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02466","description":"<p>The inherent ambiguity in ground-truth annotations of 3D bounding boxes\ncaused by occlusions, signal missing, or manual annotation errors can confuse\ndeep 3D object detectors during training, thus deteriorating the detection\naccuracy. However, existing methods overlook such issues to some extent and\ntreat the labels as deterministic. In this paper, we formulate the label\nuncertainty problem as the diversity of potentially plausible bounding boxes of\nobjects, then propose GLENet, a generative framework adapted from conditional\nvariational autoencoders, to model the one-to-many relationship between a\ntypical 3D object and its potential ground-truth bounding boxes with latent\nvariables. The label uncertainty generated by GLENet is a plug-and-play module\nand can be conveniently integrated into existing deep 3D detectors to build\nprobabilistic detectors and supervise the learning of the localization\nuncertainty. Besides, we propose an uncertainty-aware quality estimator\narchitecture in probabilistic detectors to guide the training of IoU-branch\nwith predicted localization uncertainty. We incorporate the proposed methods\ninto various popular base 3D detectors and demonstrate significant and\nconsistent performance gains on both KITTI and Waymo benchmark datasets.\nEspecially, the proposed GLENet-VR outperforms all published LiDAR-based\napproaches by a large margin and ranks $1^{st}$ among single-modal methods on\nthe challenging KITTI test set. We will make the source code and pre-trained\nmodels publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Scale-Aware, Robust, and Generalizable Unsupervised Monocular Depth Estimation by Integrating IMU Motion Dynamics. (arXiv:2207.04680v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04680","description":"<p>Unsupervised monocular depth and ego-motion estimation has drawn extensive\nresearch attention in recent years. Although current methods have reached a\nhigh up-to-scale accuracy, they usually fail to learn the true scale metric due\nto the inherent scale ambiguity from training with monocular sequences. In this\nwork, we tackle this problem and propose DynaDepth, a novel scale-aware\nframework that integrates information from vision and IMU motion dynamics.\nSpecifically, we first propose an IMU photometric loss and a cross-sensor\nphotometric consistency loss to provide dense supervision and absolute scales.\nTo fully exploit the complementary information from both sensors, we further\ndrive a differentiable camera-centric extended Kalman filter (EKF) to update\nthe IMU preintegrated motions when observing visual measurements. In addition,\nthe EKF formulation enables learning an ego-motion uncertainty measure, which\nis non-trivial for unsupervised methods. By leveraging IMU during training,\nDynaDepth not only learns an absolute scale, but also provides a better\ngeneralization ability and robustness against vision degradation such as\nillumination change and moving objects. We validate the effectiveness of\nDynaDepth by conducting extensive experiments and simulations on the KITTI and\nMake3D datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCCF: Deep Comprehensible Color Filter Learning Framework for High-Resolution Image Harmonization. (arXiv:2207.04788v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.04788","description":"<p>Image color harmonization algorithm aims to automatically match the color\ndistribution of foreground and background images captured in different\nconditions. Previous deep learning based models neglect two issues that are\ncritical for practical applications, namely high resolution (HR) image\nprocessing and model comprehensibility. In this paper, we propose a novel Deep\nComprehensible Color Filter (DCCF) learning framework for high-resolution image\nharmonization. Specifically, DCCF first downsamples the original input image to\nits low-resolution (LR) counter-part, then learns four human comprehensible\nneural filters (i.e. hue, saturation, value and attentive rendering filters) in\nan end-to-end manner, finally applies these filters to the original input image\nto get the harmonized result. Benefiting from the comprehensible neural\nfilters, we could provide a simple yet efficient handler for users to cooperate\nwith deep model to get the desired results with very little effort when\nnecessary. Extensive experiments demonstrate the effectiveness of DCCF learning\nframework and it outperforms state-of-the-art post-processing method on\niHarmony4 dataset on images' full-resolutions by achieving 7.63% and 1.69%\nrelative improvements on MSE and PSNR respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1\">Ben Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_S/0/1/0/all/0/1\">Shenghui Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Quan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Rongfei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Binqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xing Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-Level 6D Object Pose and Size Estimation using Self-Supervised Deep Prior Deformation Networks. (arXiv:2207.05444v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05444","description":"<p>It is difficult to precisely annotate object instances and their semantics in\n3D space, and as such, synthetic data are extensively used for these tasks,\ne.g., category-level 6D object pose and size estimation. However, the easy\nannotations in synthetic domains bring the downside effect of synthetic-to-real\n(Sim2Real) domain gap. In this work, we aim to address this issue in the task\nsetting of Sim2Real, unsupervised domain adaptation for category-level 6D\nobject pose and size estimation. We propose a method that is built upon a novel\nDeep Prior Deformation Network, shortened as DPDN. DPDN learns to deform\nfeatures of categorical shape priors to match those of object observations, and\nis thus able to establish deep correspondence in the feature space for direct\nregression of object poses and sizes. To reduce the Sim2Real domain gap, we\nformulate a novel self-supervised objective upon DPDN via consistency learning;\nmore specifically, we apply two rigid transformations to each object\nobservation in parallel, and feed them into DPDN respectively to yield dual\nsets of predictions; on top of the parallel learning, an inter-consistency term\nis employed to keep cross consistency between dual predictions for improving\nthe sensitivity of DPDN to pose changes, while individual intra-consistency\nones are used to enforce self-adaptation within each learning itself. We train\nDPDN on both training sets of the synthetic CAMERA25 and real-world REAL275\ndatasets; our results outperform the existing methods on REAL275 test set under\nboth the unsupervised and supervised settings. Ablation studies also verify the\nefficacy of our designs. Our code is released publicly at\nhttps://github.com/JiehongLin/Self-DPDN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiehong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zewei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliminating Gradient Conflict in Reference-based Line-Art Colorization. (arXiv:2207.06095v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.06095","description":"<p>Reference-based line-art colorization is a challenging task in computer\nvision. The color, texture, and shading are rendered based on an abstract\nsketch, which heavily relies on the precise long-range dependency modeling\nbetween the sketch and reference. Popular techniques to bridge the cross-modal\ninformation and model the long-range dependency employ the attention mechanism.\nHowever, in the context of reference-based line-art colorization, several\ntechniques would intensify the existing training difficulty of attention, for\ninstance, self-supervised training protocol and GAN-based losses. To understand\nthe instability in training, we detect the gradient flow of attention and\nobserve gradient conflict among attention branches. This phenomenon motivates\nus to alleviate the gradient issue by preserving the dominant gradient branch\nwhile removing the conflict ones. We propose a novel attention mechanism using\nthis training strategy, Stop-Gradient Attention (SGA), outperforming the\nattention baseline by a large margin with better training stability. Compared\nwith state-of-the-art modules in line-art colorization, our approach\ndemonstrates significant improvements in Fr\\'echet Inception Distance (FID, up\nto 27.21%) and structural similarity index measure (SSIM, up to 25.67%) on\nseveral benchmarks. The code of SGA is available at\nhttps://github.com/kunkun0w0/SGA .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransGrasp: Grasp Pose Estimation of a Category of Objects by Transferring Grasps from Only One Labeled Instance. (arXiv:2207.07861v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2207.07861","description":"<p>Grasp pose estimation is an important issue for robots to interact with the\nreal world. However, most of existing methods require exact 3D object models\navailable beforehand or a large amount of grasp annotations for training. To\navoid these problems, we propose TransGrasp, a category-level grasp pose\nestimation method that predicts grasp poses of a category of objects by\nlabeling only one object instance. Specifically, we perform grasp pose transfer\nacross a category of objects based on their shape correspondences and propose a\ngrasp pose refinement module to further fine-tune grasp pose of grippers so as\nto ensure successful grasps. Experiments demonstrate the effectiveness of our\nmethod on achieving high-quality grasps with the transferred grasp poses. Our\ncode is available at https://github.com/yanjh97/TransGrasp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Hongtao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wanli Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras. (arXiv:2207.08000v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08000","description":"<p>We propose DiffuStereo, a novel system using only sparse cameras (8 in this\nwork) for high-quality 3D human reconstruction. At its core is a novel\ndiffusion-based stereo module, which introduces diffusion models, a type of\npowerful generative models, into the iterative stereo matching network. To this\nend, we design a new diffusion kernel and additional stereo constraints to\nfacilitate stereo matching and depth estimation in the network. We further\npresent a multi-level stereo network architecture to handle high-resolution (up\nto 4k) inputs without requiring unaffordable memory footprint. Given a set of\nsparse-view color images of a human, the proposed multi-level diffusion-based\nstereo network can produce highly accurate depth maps, which are then converted\ninto a high-quality 3D human model through an efficient multi-view fusion\nstrategy. Overall, our method enables automatic reconstruction of human models\nwith quality on par to high-end dense-view camera rigs, and this is achieved\nusing a much more light-weight hardware setup. Experiments show that our method\noutperforms state-of-the-art methods by a large margin both qualitatively and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruizhi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zerong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jingxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExAgt: Expert-guided Augmentation for Representation Learning of Traffic Scenarios. (arXiv:2207.08609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08609","description":"<p>Representation learning in recent years has been addressed with\nself-supervised learning methods. The input data is augmented into two\ndistorted views and an encoder learns the representations that are invariant to\ndistortions -- cross-view prediction. Augmentation is one of the key components\nin cross-view self-supervised learning frameworks to learn visual\nrepresentations. This paper presents ExAgt, a novel method to include expert\nknowledge for augmenting traffic scenarios, to improve the learnt\nrepresentations without any human annotation. The expert-guided augmentations\nare generated in an automated fashion based on the infrastructure, the\ninteractions between the EGO and the traffic participants and an ideal sensor\nmodel. The ExAgt method is applied in two state-of-the-art cross-view\nprediction methods and the representations learnt are tested in downstream\ntasks like classification and clustering. Results show that the ExAgt method\nimproves representation learning compared to using only standard augmentations\nand it provides a better representation space stability. The code is available\nat https://github.com/lab176344/ExAgt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_L/0/1/0/all/0/1\">Lakshman Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wurst_J/0/1/0/all/0/1\">Jonas Wurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egolf_R/0/1/0/all/0/1\">Robin Egolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1\">Michael Botsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utschick_W/0/1/0/all/0/1\">Wolfgang Utschick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Ke Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-Aware Observer Network for Out-of-Distribution Object Segmentation. (arXiv:2207.08782v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08782","description":"<p>Recent work on Observer Network has shown promising results on\nOut-Of-Distribution (OOD) detection for semantic segmentation. These methods\nhave difficulty in precisely locating the point of interest in the image, i.e,\nthe anomaly. This limitation is due to the difficulty of fine-grained\nprediction at the pixel level. To address this issue, we provide instance\nknowledge to the observer. We extend the approach of ObsNet by harnessing an\ninstance-wise mask prediction. We use an additional, class agnostic, object\ndetector to filter and aggregate observer predictions. Finally, we predict an\nunique anomaly score for each instance in the image. We show that our proposed\nmethod accurately disentangle in-distribution objects from Out-Of-Distribution\nobjects on three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Besnier_V/0/1/0/all/0/1\">Victor Besnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briot_A/0/1/0/all/0/1\">Alexandre Briot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"eCDT: Event Clustering for Simultaneous Feature Detection and Tracking-. (arXiv:2207.09108v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09108","description":"<p>Contrary to other standard cameras, event cameras interpret the world in an\nentirely different manner; as a collection of asynchronous events. Despite\nevent camera's unique data output, many event feature detection and tracking\nalgorithms have shown significant progress by making detours to frame-based\ndata representations. This paper questions the need to do so and proposes a\nnovel event data-friendly method that achieve simultaneous feature detection\nand tracking, called event Clustering-based Detection and Tracking (eCDT). Our\nmethod employs a novel clustering method, named as k-NN Classifier-based\nSpatial Clustering and Applications with Noise (KCSCAN), to cluster adjacent\npolarity events to retrieve event trajectories.With the aid of a Head and Tail\nDescriptor Matching process, event clusters that reappear in a different\npolarity are continually tracked, elongating the feature tracks. Thanks to our\nclustering approach in spatio-temporal space, our method automatically solves\nfeature detection and feature tracking simultaneously. Also, eCDT can extract\nfeature tracks at any frequency with an adjustable time window, which does not\ncorrupt the high temporal resolution of the original event data. Our method\nachieves 30% better feature tracking ages compared with the state-of-the-art\napproach while also having a low error approximately equal to it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sumin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Alex Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios. (arXiv:2207.09120v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09120","description":"<p>Clustering traffic scenarios and detecting novel scenario types are required\nfor scenario-based testing of autonomous vehicles. These tasks benefit from\neither good similarity measures or good representations for the traffic\nscenarios. In this work, an expert-knowledge aided representation learning for\ntraffic scenarios is presented. The latent space so formed is used for\nsuccessful clustering and novel scenario type detection. Expert-knowledge is\nused to define objectives that the latent representations of traffic scenarios\nshall fulfill. It is presented, how the network architecture and loss is\ndesigned from these objectives, thereby incorporating expert-knowledge. An\nautomatic mining strategy for traffic scenarios is presented, such that no\nmanual labeling is required. Results show the performance advantage compared to\nbaseline methods. Additionally, extensive analysis of the latent space is\nperformed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wurst_J/0/1/0/all/0/1\">Jonas Wurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_L/0/1/0/all/0/1\">Lakshman Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botsch_M/0/1/0/all/0/1\">Michael Botsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utschick_W/0/1/0/all/0/1\">Wolfgang Utschick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Stop Learning: Towards Continual Learning for the CLIP Model. (arXiv:2207.09248v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09248","description":"<p>The Contrastive Language-Image Pre-training (CLIP) Model is a recently\nproposed large-scale pre-train model which attracts increasing attention in the\ncomputer vision community. Benefiting from its gigantic image-text training\nset, the CLIP model has learned outstanding capabilities in zero-shot learning\nand image-text matching. To boost the recognition performance of CLIP on some\ntarget visual concepts, it is often desirable to further update the CLIP model\nby fine-tuning some classes-of-interest on extra training data. This operation,\nhowever, raises an important concern: will the update hurt the zero-shot\nlearning or image-text matching capability of the CLIP, i.e., the catastrophic\nforgetting issue? If yes, could existing continual learning algorithms be\nadapted to alleviate the risk of catastrophic forgetting? To answer these\nquestions, this work conducts a systemic study on the continual learning issue\nof the CLIP model. We construct evaluation protocols to measure the impact of\nfine-tuning updates and explore different ways to upgrade existing continual\nlearning methods to mitigate the forgetting issue of the CLIP model. Our study\nreveals the particular challenges of CLIP continual learning problem and lays a\nfoundation for further researches. Moreover, we propose a new algorithm, dubbed\nLearning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact\neffectiveness for alleviating the forgetting issue of the CLIP model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chunna Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Haoxuan Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking IoU-based Optimization for Single-stage 3D Object Detection. (arXiv:2207.09332v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09332","description":"<p>Since Intersection-over-Union (IoU) based optimization maintains the\nconsistency of the final IoU prediction metric and losses, it has been widely\nused in both regression and classification branches of single-stage 2D object\ndetectors. Recently, several 3D object detection methods adopt IoU-based\noptimization and directly replace the 2D IoU with 3D IoU. However, such a\ndirect computation in 3D is very costly due to the complex implementation and\ninefficient backward operations. Moreover, 3D IoU-based optimization is\nsub-optimal as it is sensitive to rotation and thus can cause training\ninstability and detection performance deterioration. In this paper, we propose\na novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the\nrotation-sensitivity issue, and produce more efficient optimization objectives\ncompared with 3D IoU during the training stage. Specifically, our RDIoU\nsimplifies the complex interactions of regression parameters by decoupling the\nrotation variable as an independent term, yet preserving the geometry of 3D\nIoU. By incorporating RDIoU into both the regression and classification\nbranches, the network is encouraged to learn more precise bounding boxes and\nconcurrently overcome the misalignment issue between classification and\nregression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset\nvalidate that our RDIoU method can bring substantial improvement for the\nsingle-stage 3D object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1\">Hualian Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Sijia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Na Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Min-Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space. (arXiv:2206.11895v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2206.11895","description":"<p>Humans are remarkably flexible in understanding viewpoint changes due to\nvisual cortex supporting the perception of 3D structure. In contrast, most of\nthe computer vision models that learn visual representation from a pool of 2D\nimages often fail to generalize over novel camera viewpoints. Recently, the\nvision architectures have shifted towards convolution-free architectures,\nvisual Transformers, which operate on tokens derived from image patches.\nHowever, neither these Transformers nor 2D convolutional networks perform\nexplicit operations to learn viewpoint-agnostic representation for visual\nunderstanding. To this end, we propose a 3D Token Representation Layer (3DTRL)\nthat estimates the 3D positional information of the visual tokens and leverages\nit for learning viewpoint-agnostic representations. The key elements of 3DTRL\ninclude a pseudo-depth estimator and a learned camera matrix to impose\ngeometric transformations on the tokens. These enable 3DTRL to recover the 3D\npositional information of the tokens from 2D patches. In practice, 3DTRL is\neasily plugged-in into a Transformer. Our experiments demonstrate the\neffectiveness of 3DTRL in many vision tasks including image classification,\nmulti-view video alignment, and action recognition. The models with 3DTRL\noutperform their backbone Transformers in all the tasks with minimal added\ncomputation. Our project page is at\nhttps://www3.cs.stonybrook.edu/~jishang/3dtrl/3dtrl.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jinghuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}