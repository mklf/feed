{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Variance of Twitter Embeddings and Temporal Trends of COVID-19 cases. (arXiv:2110.00031v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00031","description":"<p>The severity of the coronavirus pandemic necessitates the need of effective\nadministrative decisions. Over 4 lakh people in India succumbed to COVID-19,\nwith over 3 crore confirmed cases, and still counting. The threat of a\nplausible third wave continues to haunt millions. In this ever changing dynamic\nof the virus, predictive modeling methods can serve as an integral tool. The\npandemic has further triggered an unprecedented usage of social media. This\npaper aims to propose a method for harnessing social media, specifically\nTwitter, to predict the upcoming scenarios related to COVID-19 cases. In this\nstudy, we seek to understand how the surges in COVID-19 related tweets can\nindicate rise in the cases. This prospective analysis can be utilised to aid\nadministrators about timely resource allocation to lessen the severity of the\ndamage. Using word embeddings to capture the semantic meaning of tweets, we\nidentify Significant Dimensions (SDs).Our methodology predicts the rise in\ncases with a lead time of 15 days and 30 days with R2 scores of 0.80 and 0.62\nrespectively. Finally, we explain the thematic utility of the SDs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_K/0/1/0/all/0/1\">Khushbu Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadhu_A/0/1/0/all/0/1\">Ambika Sadhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_M/0/1/0/all/0/1\">Mayank Sethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagpal_S/0/1/0/all/0/1\">Sargun Nagpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#ContextMatters: Advantages and Limitations of Using Machine Learning to Support Women in Politics. (arXiv:2110.00116v1 [cs.SI])","link":"http://arxiv.org/abs/2110.00116","description":"<p>The United Nations identified gender equality as a Sustainable Development\nGoal in 2015, recognizing the underrepresentation of women in politics as a\nspecific barrier to achieving gender equality. Political systems around the\nworld experience gender inequality across all levels of elected government as\nfewer women run for office than men. This is due in part to online abuse,\nparticularly on social media platforms like Twitter, where women seeking or in\npower tend to be targeted with more toxic maltreatment than their male\ncounterparts. In this paper, we present reflections on ParityBOT - the first\nnatural language processing-based intervention designed to affect online\ndiscourse for women in politics for the better, at scale. Deployed across\nelections in Canada, the United States and New Zealand, ParityBOT was used to\nanalyse and classify more than 12 million tweets directed at women candidates\nand counter toxic tweets with supportive ones. From these elections we present\nthree case studies highlighting the current limitations of, and future research\nand application opportunities for, using a natural language processing-based\nsystem to detect online toxicity, specifically with regards to contextually\nimportant microaggressions. We examine the rate of false negatives, where\nParityBOT failed to pick up on insults directed at specific high profile women,\nwhich would be obvious to human users. We examine the unaddressed harms of\nmicroaggressions and the potential of yet unseen damage they cause for women in\nthese communities, and for progress towards gender equality overall, in light\nof these technological blindspots. This work concludes with a discussion on the\nbenefits of partnerships between nonprofit social groups and technology experts\nto develop responsible, socially impactful approaches to addressing online\nhate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Comer_J/0/1/0/all/0/1\">Jacqueline Comer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Work_S/0/1/0/all/0/1\">Sam Work</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1\">Kory W Mathewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuthbertson_L/0/1/0/all/0/1\">Lana Cuthbertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machin_K/0/1/0/all/0/1\">Kasey Machin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-Constrained Graph Neural Networks For Argument Mining. (arXiv:2110.00124v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00124","description":"<p>We propose a novel architecture for Graph Neural Networks that is inspired by\nthe idea behind Tree Kernels of measuring similarity between trees by taking\ninto account their common substructures, named fragments. By imposing a series\nof regularization constraints to the learning problem, we exploit a pooling\nmechanism that incorporates such notion of fragments within the node soft\nassignment function that produces the embeddings. We present an extensive\nexperimental evaluation on a collection of sentence classification tasks\nconducted on several argument mining corpora, showing that the proposed\napproach performs well with respect to state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruggeri_F/0/1/0/all/0/1\">Federico Ruggeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemBERT: Injecting Unstructured Knowledge into BERT. (arXiv:2110.00125v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00125","description":"<p>Transformers changed modern NLP in many ways. However, they can hardly\nexploit domain knowledge, and like other blackbox models, they lack\ninterpretability. Unfortunately, structured knowledge injection, in the long\nrun, risks to suffer from a knowledge acquisition bottleneck. We thus propose a\nmemory enhancement of transformer models that makes use of unstructured domain\nknowledge expressed in plain natural language. An experimental evaluation\nconducted on two challenging NLP tasks demonstrates that our approach yields\nbetter performance and model interpretability than baseline transformer-based\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruggeri_F/0/1/0/all/0/1\">Federico Ruggeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis. (arXiv:2110.00135v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00135","description":"<p>Global models are trained to be as generalizable as possible, with user\ninvariance considered desirable since the models are shared across multitudes\nof users. As such, these models are often unable to produce personalized\nresponses for individual users, based on their data. Contrary to widely-used\npersonalization techniques based on few-shot learning, we propose\nUserIdentifier, a novel scheme for training a single shared model for all\nusers. Our approach produces personalized responses by adding fixed,\nnon-trainable user identifiers to the input data. We empirically demonstrate\nthat this proposed method outperforms the prefix-tuning based state-of-the-art\napproach by up to 13%, on a suite of sentiment analysis datasets. We also show\nthat, unlike prior work, this method needs neither any additional model\nparameters nor any extra rounds of few-shot fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1\">Vaishnavi Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1\">Robert Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1\">Dimitrios Dimitriadis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Labeling Approach for Vietnamese and Chinese Word Segmentation. (arXiv:2110.00156v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00156","description":"<p>In this paper, we propose a span labeling approach to model n-gram\ninformation for Vietnamese word segmentation, namely SPAN SEG. We compare the\nspan labeling approach with the conditional random field by using encoders with\nthe same architecture. Since Vietnamese and Chinese have similar linguistic\nphenomena, we evaluated the proposed method on the Vietnamese treebank\nbenchmark dataset and five Chinese benchmark datasets. Through our experimental\nresults, the proposed approach SpanSeg achieves higher performance than the\nsequence tagging approach with the state-of-the-art F-score of 98.31% on the\nVietnamese treebank benchmark, when they both apply the contextual pre-trained\nlanguage model XLM-RoBERTa and the predicted word boundary information.\nBesides, we do fine-tuning experiments for the span labeling approach on BERT\nand ZEN pre-trained language model for Chinese with fewer parameters, faster\ninference time, and competitive or higher F-scores than the previous\nstate-of-the-art approach, word segmentation with word-hood memory networks, on\nfive Chinese benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_L/0/1/0/all/0/1\">Linh-Bao Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thin_D/0/1/0/all/0/1\">Dang Van Thin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Under the Microscope: Interpreting Readability Assessment Models for Filipino. (arXiv:2110.00157v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00157","description":"<p>Readability assessment is the process of identifying the level of ease or\ndifficulty of a certain piece of text for its intended audience. Approaches\nhave evolved from the use of arithmetic formulas to more complex\npattern-recognizing models trained using machine learning algorithms. While\nusing these approaches provide competitive results, limited work is done on\nanalyzing how linguistic variables affect model inference quantitatively. In\nthis work, we dissect machine learning-based readability assessment models in\nFilipino by performing global and local model interpretation to understand the\ncontributions of varying linguistic features and discuss its implications in\nthe context of the Filipino language. Results show that using a model trained\nwith top features from global interpretation obtained higher performance than\nthe ones using features selected by Spearman correlation. Likewise, we also\nempirically observed local feature weight boundaries for discriminating reading\ndifficulty at an extremely fine-grained level and their corresponding effects\nif values are perturbed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Ethel Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building an Efficient and Effective Retrieval-based Dialogue System via Mutual Learning. (arXiv:2110.00159v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00159","description":"<p>Establishing retrieval-based dialogue systems that can select appropriate\nresponses from the pre-built index has gained increasing attention from\nresearchers. For this task, the adoption of pre-trained language models (such\nas BERT) has led to remarkable progress in a number of benchmarks. There exist\ntwo common approaches, including cross-encoders which perform full attention\nover the inputs, and bi-encoders that encode the context and response\nseparately. The former gives considerable improvements in accuracy but is often\ninapplicable in practice for large-scale retrieval given the cost of the full\nattention required for each sample at test time. The latter is efficient for\nbillions of indexes but suffers from sub-optimal performance. In this work, we\npropose to combine the best of both worlds to build a retrieval system.\nSpecifically, we employ a fast bi-encoder to replace the traditional\nfeature-based pre-retrieval model (such as BM25) and set the response\nre-ranking model as a more complicated architecture (such as cross-encoder). To\nfurther improve the effectiveness of our framework, we train the pre-retrieval\nmodel and the re-ranking model at the same time via mutual learning, which\nenables two models to learn from each other throughout the training process. We\nconduct experiments on two benchmarks and evaluation results demonstrate the\nefficiency and effectiveness of our proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiazhan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation by Self- and Semi-supervised Learning. (arXiv:2110.00165v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and Semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT4GCN: Using BERT Intermediate Layers to Augment GCN for Aspect-based Sentiment Classification. (arXiv:2110.00171v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00171","description":"<p>Graph-based Aspect-based Sentiment Classification (ABSC) approaches have\nyielded state-of-the-art results, expecially when equipped with contextual word\nembedding from pre-training language models (PLMs). However, they ignore\nsequential features of the context and have not yet made the best of PLMs. In\nthis paper, we propose a novel model, BERT4GCN, which integrates the\ngrammatical sequential features from the PLM of BERT, and the syntactic\nknowledge from dependency graphs. BERT4GCN utilizes outputs from intermediate\nlayers of BERT and positional information between words to augment GCN (Graph\nConvolutional Network) to better encode the dependency graphs for the\ndownstream classification. Experimental results demonstrate that the proposed\nBERT4GCN outperforms all state-of-the-art baselines, justifying that augmenting\nGCN with the grammatical features from intermediate layers of BERT can\nsignificantly empower ABSC models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zeguan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiarun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Congjian Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00269","description":"<p>Pre-trained models learn contextualized word representations on large-scale\ntext corpus through a self-supervised learning method, which has achieved\npromising performance after fine-tuning. These models, however, suffer from\npoor robustness and lack of interpretability. Pre-trained models with knowledge\ninjection, which we call knowledge enhanced pre-trained models (KEPTMs),\npossess deep understanding and logical reasoning and introduce interpretability\nto some extent. In this survey, we provide a comprehensive overview of KEPTMs\nfor natural language processing. We first introduce the progress of pre-trained\nmodels and knowledge representation learning. Then we systematically categorize\nexisting KEPTMs from three different perspectives. Finally, we outline some\npotential directions of KEPTMs for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Gang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinghui Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Harmful Memes and Their Targets. (arXiv:2110.00413v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00413","description":"<p>Among the various modes of communication in social media, the use of Internet\nmemes has emerged as a powerful means to convey political, psychological, and\nsocio-cultural opinions. Although memes are typically humorous in nature,\nrecent days have witnessed a proliferation of harmful memes targeted to abuse\nvarious social entities. As most harmful memes are highly satirical and\nabstruse without appropriate contexts, off-the-shelf multimodal models may not\nbe adequate to understand their underlying semantics. In this work, we propose\ntwo novel problem formulations: detecting harmful memes and the social entities\nthat these harmful memes target. To this end, we present HarMeme, the first\nbenchmark dataset, containing 3,544 memes related to COVID-19. Each meme went\nthrough a rigorous two-stage annotation process. In the first stage, we labeled\na meme as very harmful, partially harmful, or harmless; in the second stage, we\nfurther annotated the type of target(s) that each harmful meme points to:\nindividual, organization, community, or society/general public/other. The\nevaluation results using ten unimodal and multimodal models highlight the\nimportance of using multimodal signals for both tasks. We further discuss the\nlimitations of these models and we argue that more research is needed to\naddress these problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1\">Rituparna Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiLMing Multimodal Sarcasm Detection with Attention. (arXiv:2110.00416v1 [cs.MM])","link":"http://arxiv.org/abs/2110.00416","description":"<p>Sarcasm detection identifies natural language expressions whose intended\nmeaning is different from what is implied by its surface meaning. It finds\napplications in many NLP tasks such as opinion mining, sentiment analysis, etc.\nToday, social media has given rise to an abundant amount of multimodal data\nwhere users express their opinions through text and images. Our paper aims to\nleverage multimodal data to improve the performance of the existing systems for\nsarcasm detection. So far, various approaches have been proposed that uses text\nand image modality and a fusion of both. We propose a novel architecture that\nuses the RoBERTa model with a co-attention layer on top to incorporate context\nincongruity between input text and image attributes. Further, we integrate\nfeature-wise affine transformation by conditioning the input image through\nFiLMed ResNet blocks with the textual features using the GRU network to capture\nthe multimodal information. The output from both the models and the CLS token\nfrom RoBERTa is concatenated and used for the final prediction. Our results\ndemonstrate that our proposed model outperforms the existing state-of-the-art\nmethod by 6.14% F1 score on the public Twitter multimodal sarcasm detection\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sundesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Aditya Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Miten Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syiemlieh_L/0/1/0/all/0/1\">Laribok Syiemlieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurya_C/0/1/0/all/0/1\">Chandresh Maurya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Non-Negative Matrix Factorization and n-stage Latent Dirichlet Allocation for Emotion Analysis in Turkish Tweets. (arXiv:2110.00418v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00418","description":"<p>With the development of technology, the use of social media has become quite\ncommon. Analyzing comments on social media in areas such as media and\nadvertising plays an important role today. For this reason, new and traditional\nnatural language processing methods are used to detect the emotion of these\nshares. In this paper, the Latent Dirichlet Allocation, namely LDA, and\nNon-Negative Matrix Factorization methods in topic modeling were used to\ndetermine which emotion the Turkish tweets posted via Twitter. In addition, the\naccuracy of a proposed n-level method based on LDA was analyzed. Dataset\nconsists of 5 emotions, namely angry, fear, happy, sad and confused. NMF was\nthe most successful method among all topic modeling methods in this study.\nThen, the F1-measure of Random Forest, Naive Bayes and Support Vector Machine\nmethods was analyzed by obtaining a file suitable for Weka by using the word\nweights and class labels of the topics. Among the Weka results, the most\nsuccessful method was n-stage LDA, and the most successful algorithm was Random\nForest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guven_Z/0/1/0/all/0/1\">Zekeriya Anil Guven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diri_B/0/1/0/all/0/1\">Banu Diri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakaloglu_T/0/1/0/all/0/1\">Tolgahan Cakaloglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Web Scale Entity Extraction System. (arXiv:2110.00423v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00423","description":"<p>Understanding the semantic meaning of content on the web through the lens of\nentities and concepts has many practical advantages. However, when building\nlarge-scale entity extraction systems, practitioners are facing unique\nchallenges involving finding the best ways to leverage the scale and variety of\ndata available on internet platforms. We present learnings from our efforts in\nbuilding an entity extraction system for multiple document types at large scale\nusing multi-modal Transformers. We empirically demonstrate the effectiveness of\nmulti-lingual, multi-task and cross-document type learning. We also discuss the\nlabel collection schemes that help to minimize the amount of noise in the\ncollected data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xuanting Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Quanbin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengkan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_P/0/1/0/all/0/1\">Pushkar Tripathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rumor Detection on Social Media with Hierarchical Adversarial Training. (arXiv:2110.00425v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00425","description":"<p>The proliferation of rumors on social media has a huge impact on society.\nHowever, natural language text is high-dimensional and sparse, and the same\nrumor may be expressed in hundreds of ways on social media. As such, the\nrobustness and generalization of the current rumor detection model are put into\nquestion. We propose a new hierarchical model called HAT-RD, which is divided\ninto two categories: post-level modules and event-level modules. HAT-RD adopts\na novel hierarchical adversarial training method based on gradient ascent by\nadding adversarial perturbations to the embedding layers both of post-level\nmodules and event-level modules to deceive the detector. At the same time, the\ndetector uses stochastic gradient descent to minimize the adversarial risk to\nlearn a more robust model. In this way, the post-level and event-level sample\nspaces are enhanced, and experiments indicate that the model drift into an area\nwith a flat loss landscape that leads to better generalization. Experiments on\ntwo real-world datasets demonstrate that our model achieves better results than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shiwen Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_H/0/1/0/all/0/1\">Hung-Yu Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Natural Language Video Localization. (arXiv:2110.00428v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00428","description":"<p>Understanding videos to localize moments with natural language often requires\nlarge expensive annotated video regions paired with language queries. To\neliminate the annotation costs, we make a first attempt to train a natural\nlanguage video localization model in zero-shot manner. Inspired by unsupervised\nimage captioning setup, we merely require random text corpora, unlabeled video\ncollections, and an off-the-shelf object detector to train a model. With the\nunpaired data, we propose to generate pseudo-supervision of candidate temporal\nregions and corresponding query sentences, and develop a simple NLVL model to\ntrain with the pseudo-supervision. Our empirical validations show that the\nproposed pseudo-supervised method outperforms several baseline approaches and a\nnumber of methods using stronger supervision on Charades-STA and\nActivityNet-Captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Jinwoo Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1\">Daechul Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Seong Jong Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External knowledge transfer deployment inside a simple double agent Viterbi algorithm. (arXiv:2110.00433v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00433","description":"<p>We consider in this paper deploying external knowledge transfer inside a\nsimple double agent Viterbi algorithm which is an algorithm firstly introduced\nby the author in his preprint \"Hidden Markov Based Mathematical Model dedicated\nto Extract Ingredients from Recipe Text\". The key challenge of this work lies\nin discovering the reason why our old model does have bad performances when it\nis confronted with estimating ingredient state for unknown words and see if\ndeploying external knowledge transfer directly on calculating state matrix\ncould be the solution instead of deploying it only on back propagating step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baklouti_Z/0/1/0/all/0/1\">Zied Baklouti</a> (ENIT, UP)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention based Sequence to Sequence Learning for Machine Translation of Low Resourced Indic Languages -- A case of Sanskrit to Hindi. (arXiv:2110.00435v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00435","description":"<p>Deep Learning techniques are powerful in mimicking humans in a particular set\nof problems. They have achieved a remarkable performance in complex learning\ntasks. Deep learning inspired Neural Machine Translation (NMT) is a proficient\ntechnique that outperforms traditional machine translation. Performing\nmachine-aided translation on Indic languages has always been a challenging task\nconsidering their rich and diverse grammar. The neural machine translation has\nshown quality results compared to the traditional machine translation\napproaches. The fully automatic machine translation becomes problematic when it\ncomes to low-resourced languages, especially with Sanskrit. This paper presents\nattention mechanism based neural machine translation by selectively focusing on\na particular part of language sentences during translation. The work shows the\nconstruction of Sanskrit to Hindi bilingual parallel corpus with nearly 10K\nsamples and having 178,000 tokens. The neural translation model equipped with\nan attention mechanism has been trained on Sanskrit to Hindi parallel corpus.\nThe approach has shown the significance of attention mechanisms to overcome\nlong-term dependencies, primarily associated with low resources Indic\nlanguages. The paper shows the attention plots on testing data to demonstrate\nthe alignment between source and translated words. For the evaluation of the\ntranslated sentences, manual score based human evaluation and automatic\nevaluation metric based techniques have been adopted. The attention mechanism\nbased neural translation has achieved 88% accuracy in human evaluation and a\nBLEU score of 0.92 on Sanskrit to Hindi translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakarola_V/0/1/0/all/0/1\">Vishvajit Bakarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasriwala_J/0/1/0/all/0/1\">Jitendra Nasriwala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phonology Recognition in American Sign Language. (arXiv:2110.00453v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00453","description":"<p>Inspired by recent developments in natural language processing, we propose a\nnovel approach to sign language processing based on phonological properties\nvalidated by American Sign Language users. By taking advantage of datasets\ncomposed of phonological data and people speaking sign language, we use a\npretrained deep model based on mesh reconstruction to extract the 3D\ncoordinates of the signers keypoints. Then, we train standard statistical and\ndeep machine learning models in order to assign phonological classes to each\ntemporal sequence of coordinates.\n</p>\n<p>Our paper introduces the idea of exploiting the phonological properties\nmanually assigned by sign language users to classify videos of people\nperforming signs by regressing a 3D mesh. We establish a new baseline for this\nproblem based on the statistical distribution of 725 different signs. Our\nbest-performing models achieve a micro-averaged F1-score of 58% for the major\nlocation class and 70% for the sign type using statistical and deep learning\nalgorithms, compared to their corresponding baselines of 35% and 39%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavella_F/0/1/0/all/0/1\">Federico Tavella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galata_A/0/1/0/all/0/1\">Aphrodite Galata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1\">Angelo Cangelosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ask for Data-Efficient Event Argument Extraction. (arXiv:2110.00479v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00479","description":"<p>Event argument extraction (EAE) is an important task for information\nextraction to discover specific argument roles. In this study, we cast EAE as a\nquestion-based cloze task and empirically analyze fixed discrete token template\nperformance. As generating human-annotated question templates is often\ntime-consuming and labor-intensive, we further propose a novel approach called\n\"Learning to Ask,\" which can learn optimized question templates for EAE without\nhuman annotations. Experiments using the ACE-2005 dataset demonstrate that our\nmethod based on optimized questions achieves state-of-the-art performance in\nboth the few-shot and supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEMON: Explainable Entity Matching. (arXiv:2110.00516v1 [cs.DB])","link":"http://arxiv.org/abs/2110.00516","description":"<p>State-of-the-art entity matching (EM) methods are hard to interpret, and\nthere is significant value in bringing explainable AI to EM. Unfortunately,\nmost popular explainability methods do not work well out of the box for EM and\nneed adaptation. In this paper, we identify three challenges of applying local\npost hoc feature attribution methods to entity matching: cross-record\ninteraction effects, non-match explanations, and variation in sensitivity. We\npropose our novel model-agnostic and schema-flexible method LEMON that\naddresses all three challenges by (i) producing dual explanations to avoid\ncross-record interaction effects, (ii) introducing the novel concept of\nattribution potential to explain how two records could have matched, and (iii)\nautomatically choosing explanation granularity to match the sensitivity of the\nmatcher and record pair in question. Experiments on public datasets demonstrate\nthat the proposed method is more faithful to the matcher and does a better job\nof helping users understand the decision boundary of the matcher than previous\nwork. Furthermore, user studies show that the rate at which human subjects can\nconstruct counterfactual examples after seeing an explanation from our proposed\nmethod increases from 54% to 64% for matches and from 15% to 49% for\nnon-matches compared to explanations from a standard adaptation of LIME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barlaug_N/0/1/0/all/0/1\">Nils Barlaug</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images. (arXiv:2110.00519v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00519","description":"<p>While neural symbolic methods demonstrate impressive performance in visual\nquestion answering on synthetic images, their performance suffers on real\nimages. We identify that the long-tail distribution of visual concepts and\nunequal importance of reasoning steps in real data are the two key obstacles\nthat limit the models' real-world potentials. To address these challenges, we\npropose a new paradigm, Calibrating Concepts and Operations (CCO), which\nenables neural symbolic models to capture underlying data characteristics and\nto reason with hierarchical importance. Specifically, we introduce an executor\nwith learnable concept embedding magnitudes for handling distribution\nimbalance, and an operation calibrator for highlighting important operations\nand suppressing redundant ones. Our experiments show CCO substantially boosts\nthe performance of neural symbolic methods on real images. By evaluating models\non the real world dataset GQA, CCO helps the neural symbolic method NSCL\noutperforms its vanilla counterpart by 9.1% (from 47.0% to 56.1%); this result\nalso largely reduces the performance gap between symbolic and non-symbolic\nmethods. Additionally, we create a perturbed test set for better understanding\nand analyzing model performance on real images. Code is available at\nhttps://github.com/Lizw14/CaliCO.git .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens. (arXiv:2110.00521v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00521","description":"<p>Much of the world's population experiences some form of disability during\ntheir lifetime. Caution must be exercised while designing natural language\nprocessing (NLP) systems to prevent systems from inadvertently perpetuating\nableist bias against people with disabilities, i.e., prejudice that favors\nthose with typical abilities. We report on various analyses based on word\npredictions of a large-scale BERT language model. Statistically significant\nresults demonstrate that people with disabilities can be disadvantaged.\nFindings also explore overlapping forms of discrimination related to\ninterconnected gender and race identities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Saad Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huenerfauth_M/0/1/0/all/0/1\">Matt Huenerfauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alm_C/0/1/0/all/0/1\">Cecilia Ovesdotter Alm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00534","description":"<p>Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human--human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\",\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models' abilities in dialogue understanding, language\ngrounding, and task execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramithu_R/0/1/0/all/0/1\">Robinson Piramithu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural language understanding for logical games. (arXiv:2110.00558v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00558","description":"<p>We developed a system able to automatically solve logical puzzles in natural\nlanguage. Our solution is composed by a parser and an inference module. The\nparser translates the text into first order logic (FOL), while the MACE4 model\nfinder is used to compute the models of the given FOL theory. We also empower\nour software agent with the capability to provide Yes/No answers to natural\nlanguage questions related to each puzzle. Moreover, in line with Explainalbe\nArtificial Intelligence (XAI), the agent can back its answer, providing a\ngraphical representation of the proof. The advantage of using reasoning for\nNatural Language Understanding (NLU) instead of Machine learning is that the\nuser can obtain an explanation of the reasoning chain. We illustrate how the\nsystem performs on various types of natural language puzzles, including 382\nknights and knaves puzzles. These features together with the overall\nperformance rate of 80.89\\% makes the proposed solution an improvement upon\nsimilar solvers for natural language understanding in the puzzles domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Groza_A/0/1/0/all/0/1\">Adrian Groza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitu_C/0/1/0/all/0/1\">Cristian Nitu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Punctuation Restoration for Speech Transcripts via External Data. (arXiv:2110.00560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00560","description":"<p>Automatic Speech Recognition (ASR) systems generally do not produce\npunctuated transcripts. To make transcripts more readable and follow the\nexpected input format for downstream language models, it is necessary to add\npunctuation marks. In this paper, we tackle the punctuation restoration problem\nspecifically for the noisy text (e.g., phone conversation scenarios). To\nleverage the available written text datasets, we introduce a data sampling\ntechnique based on an n-gram language model to sample more training data that\nare similar to our in-domain data. Moreover, we propose a two-stage fine-tuning\napproach that utilizes the sampled external data as well as our in-domain\ndataset for models based on BERT. Extensive experiments show that the proposed\napproach outperforms the baseline with an improvement of 1:12% F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xue-Yong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TN_S/0/1/0/all/0/1\">Shashi Bhushan TN</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corston_Oliver_S/0/1/0/all/0/1\">Simon Corston-Oliver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrases as Foreign Languages in Multilingual Neural Machine Translation. (arXiv:1808.08438v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1808.08438","description":"<p>Paraphrases, the rewordings of the same semantic meaning, are useful for\nimproving generalization and translation. However, prior works only explore\nparaphrases at the word or phrase level, not at the sentence or corpus level.\nUnlike previous works that only explore paraphrases at the word or phrase\nlevel, we use different translations of the whole training data that are\nconsistent in structure as paraphrases at the corpus level. We train on\nparallel paraphrases in multiple languages from various sources. We treat\nparaphrases as foreign languages, tag source sentences with paraphrase labels,\nand train on parallel paraphrases in the style of multilingual Neural Machine\nTranslation (NMT). Our multi-paraphrase NMT that trains only on two languages\noutperforms the multilingual baselines. Adding paraphrases improves the rare\nword translation and increases entropy and diversity in lexical choice. Adding\nthe source paraphrases boosts performance better than adding the target ones.\nCombining both the source and the target paraphrases lifts performance further;\ncombining paraphrases with multilingual data helps but has mixed performance.\nWe achieve a BLEU score of 57.2 for French-to-English translation using 24\ncorpus-level paraphrases of the Bible, which outperforms the multilingual\nbaselines and is +34.7 above the single-source single-target NMT baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperber_M/0/1/0/all/0/1\">Matthias Sperber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergence of Pragmatics from Referential Game between Theory of Mind Agents. (arXiv:2001.07752v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2001.07752","description":"<p>Pragmatics studies how context can contribute to language meanings. In human\ncommunication, language is never interpreted out of context, and sentences can\nusually convey more information than their literal meanings. However, this\nmechanism is missing in most multi-agent systems, restricting the communication\nefficiency and the capability of human-agent interaction. In this paper, we\npropose an algorithm, using which agents can spontaneously learn the ability to\n\"read between lines\" without any explicit hand-designed rules. We integrate the\ntheory of mind (ToM) in a cooperative multi-agent pedagogical situation and\npropose an adaptive reinforcement learning (RL) algorithm to develop a\ncommunication protocol. ToM is a profound cognitive science concept, claiming\nthat people regularly reason about other's mental states, including beliefs,\ngoals, and intentions, to obtain performance advantage in competition,\ncooperation or coalition. With this ability, agents consider language as not\nonly messages but also rational acts reflecting others' hidden states. Our\nexperiments demonstrate the advantage of pragmatic protocols over non-pragmatic\nprotocols. We also show the teaching complexity following the pragmatic\nprotocol empirically approximates to recursive teaching dimension (RTD).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Luyao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zipeng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jingyue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Junhong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Self-Training for Sentiment Analysis of Code-Switched Data. (arXiv:2103.14797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14797","description":"<p>Sentiment analysis is an important task in understanding social media content\nlike customer reviews, Twitter and Facebook feeds etc. In multilingual\ncommunities around the world, a large amount of social media text is\ncharacterized by the presence of Code-Switching. Thus, it has become important\nto build models that can handle code-switched data. However, annotated\ncode-switched data is scarce and there is a need for unsupervised models and\nalgorithms. We propose a general framework called Unsupervised Self-Training\nand show its applications for the specific use case of sentiment analysis of\ncode-switched data. We use the power of pre-trained BERT models for\ninitialization and fine-tune them in an unsupervised manner, only using pseudo\nlabels produced by zero-shot transfer. We test our algorithm on multiple\ncode-switched languages and provide a detailed analysis of the learning\ndynamics of the algorithm with the aim of answering the question - `Does our\nunsupervised model understand the Code-Switched languages or does it just learn\nits representations?'. Our unsupervised models compete well with their\nsupervised counterparts, with their performance reaching within 1-7\\% (weighted\nF1 scores) when compared to supervised models trained for a two class problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menghani_S/0/1/0/all/0/1\">Sargam Menghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sai Krishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers. (arXiv:2104.01604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01604","description":"<p>This paper introduces Timers and Such, a new open source dataset of spoken\nEnglish commands for common voice control use cases involving numbers. We\ndescribe the gap in existing spoken language understanding datasets that Timers\nand Such fills, the design and creation of the dataset, and experiments with a\nnumber of ASR-based and end-to-end baseline models, the code for which has been\nmade available as part of the SpeechBrain toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugosch_L/0/1/0/all/0/1\">Loren Lugosch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papreja_P/0/1/0/all/0/1\">Piyush Papreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanelli_M/0/1/0/all/0/1\">Mirco Ravanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heba_A/0/1/0/all/0/1\">Abdelwahab Heba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRAKE: Fusional Real-time Automatic Keyword Extraction. (arXiv:2104.04830v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04830","description":"<p>Keyword extraction is the process of identifying the words or phrases that\nexpress the main concepts of text to the best of one's ability. Electronic\ninfrastructure creates a considerable amount of text every day and at all\ntimes. This massive volume of documents makes it practically impossible for\nhuman resources to study and manage them. Nevertheless, the need for these\ndocuments to be accessed efficiently and effectively is evident in numerous\npurposes. A blog, news article, or technical note is considered a relatively\nlong text since the reader aims to learn the subject based on keywords or\ntopics. Our approach consists of a combination of two models: graph centrality\nfeatures and textural features. The proposed method has been used to extract\nthe best keyword among the candidate keywords with an optimal combination of\ngraph centralities, such as degree, betweenness, eigenvector, closeness\ncentrality and etc, and textural, such as Casing, Term position, Term frequency\nnormalization, Term different sentence, Part Of Speech tagging. There have also\nbeen attempts to distinguish keywords from candidate phrases and consider them\non separate keywords. For evaluating the proposed method, seven datasets were\nused: Semeval2010, SemEval2017, Inspec, fao30, Thesis100, pak2018, and\nWikinews, with results reported as Precision, Recall, and F- measure. Our\nproposed method performed much better in terms of evaluation metrics in all\nreviewed datasets compared with available methods in literature. An approximate\n16.9% increase was witnessed in F-score metric and this was much more for the\nInspec in English datasets and WikiNews in forgone languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zehtab_Salmasi_A/0/1/0/all/0/1\">Aidin Zehtab-Salmasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohamad-Ali Balafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation. (arXiv:2104.05848v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05848","description":"<p>We translate a closed text that is known in advance into a severely low\nresource language by leveraging massive source parallelism. In other words,\ngiven a text in 124 source languages, we translate it into a severely low\nresource language using only ~1,000 lines of low resource data without any\nexternal help. Firstly, we propose a systematic method to rank and choose\nsource languages that are close to the low resource language. We call the\nlinguistic definition of language family Family of Origin (FAMO), and we call\nthe empirical definition of higher-ranked languages using our metrics Family of\nChoice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual\nOrder-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines\n(~3.5%) of low resource data. To translate named entities correctly, we build a\nmassive lexicon table for 2,939 Bible named entities in 124 source languages,\nand include many that occur once and covers more than 66 severely low resource\nlanguages. Moreover, we also build a novel method of combining translations\nfrom different source languages into one. Using English as a hypothetical low\nresource language, we get a +23.9 BLEU increase over a multilingual baseline,\nand a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We\nget a 42.8 BLEU score for Portuguese-English translation on the medical EMEA\ndataset. We also have good results for a real severely low resource Mayan\nlanguage, Eastern Pokomchi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving on from OntoNotes: Coreference Resolution Model Transfer. (arXiv:2104.08457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08457","description":"<p>Academic neural models for coreference resolution (coref) are typically\ntrained on a single dataset, OntoNotes, and model improvements are benchmarked\non that same dataset. However, real-world applications of coref depend on the\nannotation guidelines and the domain of the target dataset, which often differ\nfrom those of OntoNotes. We aim to quantify transferability of coref models\nbased on the number of annotated documents available in the target dataset. We\nexamine eleven target datasets and find that continued training is consistently\neffective and especially beneficial when there are few target documents. We\nestablish new benchmarks across several datasets, including state-of-the-art\nresults on PreCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Patrick Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP. (arXiv:2104.08835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08835","description":"<p>Humans can learn a new language task efficiently with only few examples, by\nleveraging their knowledge obtained when learning prior tasks. In this paper,\nwe explore whether and how such cross-task generalization ability can be\nacquired, and further applied to build better few-shot learners across diverse\nNLP tasks. We introduce CrossFit, a problem setup for studying cross-task\ngeneralization ability, which standardizes seen/unseen task partitions, data\naccess during different learning stages, and the evaluation protocols. To\ninstantiate different seen/unseen task partitions in CrossFit and facilitate\nin-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse\nfew-shot NLP tasks created from open-access NLP datasets and converted to a\nunified text-to-text format. Our analysis reveals that the few-shot learning\nability on unseen tasks can be improved via an upstream learning stage using a\nset of seen tasks. We also observe that the selection of upstream learning\ntasks can significantly influence few-shot performance on unseen tasks, asking\nfurther analysis on task similarity and transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Influence of Masking Policies in Intermediate Pre-training. (arXiv:2104.08840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08840","description":"<p>Current NLP models are predominantly trained through a two-stage \"pre-train\nthen fine-tune\" pipeline. Prior work has shown that inserting an intermediate\npre-training stage, using heuristic masking policies for masked language\nmodeling (MLM), can significantly improve final performance. However, it is\nstill unclear (1) in what cases such intermediate pre-training is helpful, (2)\nwhether hand-crafted heuristic objectives are optimal for a given task, and (3)\nwhether a masking policy designed for one task is generalizable beyond that\ntask. In this paper, we perform a large-scale empirical study to investigate\nthe effect of various masking policies in intermediate pre-training with nine\nselected tasks across three categories. Crucially, we introduce methods to\nautomate the discovery of optimal masking policies via direct supervision or\nmeta-learning. We conclude that the success of intermediate pre-training is\ndependent on appropriate pre-train corpus, selection of output format (i.e.,\nmasked spans or full sentence), and clear understanding of the role that MLM\nplays for the downstream task. In addition, we find our learned masking\npolicies outperform the heuristic of masking named entities on TriviaQA, and\npolicies learned from one task can positively transfer to other tasks in\ncertain cases, inviting future research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Belinda Z. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_B/0/1/0/all/0/1\">Benjamin Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Clinical Note Summarization. (arXiv:2104.08942v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08942","description":"<p>The trend of deploying digital systems in numerous industries has induced a\nhike in recording digital information. The health sector has observed an\nextensive adoption of digital devices and systems that generate large volumes\nof personal medical records. Electronic health records contain valuable\ninformation for retrospective and prospective analysis that is often not\nentirely exploited because of the dense information storage. The crude purpose\nof condensing health records is to select the information that holds most\ncharacteristics of the original documents based on reported disease. These\nsummaries may boost diagnosis and extend a doctor's time with the patient\nduring a high workload situation like the COVID-19 pandemic. In this paper, we\npropose applying a multi-head attention-based mechanism to perform extractive\nsummarization of meaningful phrases in clinical notes. This method finds major\nsentences for a summary by correlating tokens, segments, and positional\nembeddings. The model outputs attention scores that are statistically\ntransformed to extract key phrases and can be used to projection on the\nheat-mapping tool for visual and human use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanwal_N/0/1/0/all/0/1\">Neel Kanwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizzo_G/0/1/0/all/0/1\">Giuseppe Rizzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03842","description":"<p>Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09996","description":"<p>We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training. Code is\nmade available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1\">Prahal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1\">Masoumeh Aminzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Inconsistency and Entity Bias in MultiWOZ. (arXiv:2105.14150v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14150","description":"<p>MultiWOZ is one of the most popular multi-domain task-oriented dialog\ndatasets, containing 10K+ annotated dialogs covering eight domains. It has been\nwidely accepted as a benchmark for various dialog tasks, e.g., dialog state\ntracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog\nmodeling. In this work, we identify an overlooked issue with dialog state\nannotation inconsistencies in the dataset, where a slot type is tagged\ninconsistently across similar dialogs leading to confusion for DST modeling. We\npropose an automated correction for this issue, which is present in a whopping\n70% of the dialogs. Additionally, we notice that there is significant entity\nbias in the dataset (e.g., \"cambridge\" appears in 50% of the destination cities\nin the train domain). The entity bias can potentially lead to named entity\nmemorization in generative models, which may go unnoticed as the test set\nsuffers from a similar entity bias as well. We release a new test set with all\nentities replaced with unseen entities. Finally, we benchmark joint goal\naccuracy (JGA) of the state-of-the-art DST baselines on these modified versions\nof the data. Our experiments show that the annotation inconsistency corrections\nlead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in\nJGA when models are evaluated on the new test set with unseen entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Ankita De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-utterance Reranking Models with BERT and Graph Convolutional Networks for Conversational Speech Recognition. (arXiv:2106.06922v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06922","description":"<p>How to effectively incorporate cross-utterance information cues into a neural\nlanguage model (LM) has emerged as one of the intriguing issues for automatic\nspeech recognition (ASR). Existing research efforts on improving\ncontextualization of an LM typically regard previous utterances as a sequence\nof additional input and may fail to capture complex global structural\ndependencies among these utterances. In view of this, we in this paper seek to\nrepresent the historical context information of an utterance as\ngraph-structured data so as to distill cross-utterances, global word\ninteraction relationships. To this end, we apply a graph convolutional network\n(GCN) on the resulting graph to obtain the corresponding GCN embeddings of\nhistorical words. GCN has recently found its versatile applications on\nsocial-network analysis, text summarization, and among others due mainly to its\nability of effectively capturing rich relational information among elements.\nHowever, GCN remains largely underexplored in the context of ASR, especially\nfor dealing with conversational speech. In addition, we frame ASR N-best\nreranking as a prediction problem, leveraging bidirectional encoder\nrepresentations from transformers (BERT) as the vehicle to not only seize the\nlocal intrinsic word regularity patterns inherent in a candidate hypothesis but\nalso incorporate the cross-utterance, historical word interaction cues\ndistilled by GCN for promoting performance. Extensive experiments conducted on\nthe AMI benchmark dataset seem to confirm the pragmatic utility of our methods,\nin relation to some current top-of-the-line methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1\">Tien-Hong Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fu-An Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation of Low-Resource Indo-European Languages. (arXiv:2108.03739v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03739","description":"<p>In this work, we investigate methods for the challenging task of translating\nbetween low-resource language pairs that exhibit some level of similarity. In\nparticular, we consider the utility of transfer learning for translating\nbetween several Indo-European low-resource languages from the Germanic and\nRomance language families. In particular, we build two main classes of\ntransfer-based systems to study how relatedness can benefit the translation\nperformance. The primary system fine-tunes a model pre-trained on a related\nlanguage pair and the contrastive system fine-tunes one pre-trained on an\nunrelated language pair. Our experiments show that although relatedness is not\nnecessary for transfer learning to work, it does benefit model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition. (arXiv:2108.07789v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07789","description":"<p>Language models (LMs) pre-trained on massive amounts of text, in particular\nbidirectional encoder representations from Transformers (BERT), generative\npre-training (GPT), and GPT-2, have become a key technology for many natural\nlanguage processing tasks. In this paper, we present results using fine-tuned\nGPT, GPT-2, and their combination for automatic speech recognition (ASR).\nUnlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct\nproduct of the output probabilities is no longer a valid language prior\nprobability. A conversion method is proposed to compute the correct language\nprior probability based on bidirectional LM outputs in a mathematically exact\nway. Experimental results on the widely used AMI and Switchboard ASR tasks\nshowed that the combination of the fine-tuned GPT and GPT-2 outperformed the\ncombination of three neural LMs with different architectures trained from\nscratch on the in-domain text by up to a 12% relative word error rate reduction\n(WERR). Furthermore, on the AMI corpus, the proposed conversion for language\nprior probabilities enables BERT to obtain an extra 3% relative WERR, and the\ncombination of BERT, GPT and GPT-2 results in further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xianrui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02401","description":"<p>Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2109.08186","description":"<p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.\nFaST-VGS is a Transformer-based model for learning the associations between raw\nspeech waveforms and visual images. The model unifies dual-encoder and\ncross-attention architectures into a single model, reaping the superior\nretrieval speed of the former along with the accuracy of the latter. FaST-VGS\nachieves state-of-the-art speech-image retrieval accuracy on benchmark\ndatasets, and its learned representations exhibit strong performance on the\nZeroSpeech 2021 phonetic and semantic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Invariant Properties in Natural Language Processing. (arXiv:2109.13037v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13037","description":"<p>Meaning is context-dependent, but many properties of language (should) remain\nthe same even if we transform the context. For example, sentiment, entailment,\nor speaker properties should be the same in a translation and original of a\ntext. We introduce language invariant properties: i.e., properties that should\nnot change when we transform text, and how they can be used to quantitatively\nevaluate the robustness of transformation algorithms. We use translation and\nparaphrasing as transformation examples, but our findings apply more broadly to\nany transformation. Our results indicate that many NLP transformations change\nproperties like author characteristics, i.e., make them sound more male. We\nbelieve that studying these properties will allow NLP to address both social\nfactors and pragmatic aspects of language. We also release an application suite\nthat can be used to evaluate the invariance of transformation applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition. (arXiv:2109.13226v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2109.13226","description":"<p>We summarize the results of a host of efforts using giant automatic speech\nrecognition (ASR) models pre-trained using large, diverse unlabeled datasets\ncontaining approximately a million hours of audio. We find that the combination\nof pre-training, self-training and scaling up model size greatly increases data\nefficiency, even for extremely large tasks with tens of thousands of hours of\nlabeled data. In particular, on an ASR task with 34k hours of labeled data, by\nfine-tuning an 8 billion parameter pre-trained Conformer model we can match\nstate-of-the-art (SoTA) performance with only 3% of the training data and\nsignificantly improve SoTA with the full training set. We also report on the\nuniversal benefits gained from using big pre-trained and self-trained models\nfor a large set of downstream tasks that cover a wide range of speech domains\nand span multiple orders of magnitudes of dataset sizes, including obtaining\nSoTA performance on many public benchmarks. In addition, we utilize the learned\nrepresentation of pre-trained networks to achieve SoTA results on non-ASR\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_D/0/1/0/all/0/1\">Daniel S. Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulati_A/0/1/0/all/0/1\">Anmol Gulati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jansen_A/0/1/0/all/0/1\">Aren Jansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shibo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zongwei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation. (arXiv:2109.13318v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13318","description":"<p>Automating sign language translation (SLT) is a challenging real world\napplication. Despite its societal importance, though, research progress in the\nfield remains rather poor. Crucially, existing methods that yield viable\nperformance necessitate the availability of laborious to obtain gloss sequence\ngroundtruth. In this paper, we attenuate this need, by introducing an\nend-to-end SLT model that does not entail explicit use of glosses; the model\nonly needs text groundtruth. This is in stark contrast to existing end-to-end\nmodels that use gloss sequence groundtruth, either in the form of a modality\nthat is recognized at an intermediate model stage, or in the form of a parallel\noutput process, jointly trained with the SLT model. Our approach constitutes a\nTransformer network with a novel type of layers that combines: (i) local\nwinner-takes-all (LWTA) layers with stochastic winner sampling, instead of\nconventional ReLU layers, (ii) stochastic weights with posterior distributions\nestimated via variational inference, and (iii) a weight compression technique\nat inference time that exploits estimated posterior variance to perform\nmassive, almost lossless compression. We demonstrate that our approach can\nreach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark,\nbut without making use of glosses for model training, and with a memory\nfootprint reduced by more than 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voskou_A/0/1/0/all/0/1\">Andreas Voskou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosmopoulos_D/0/1/0/all/0/1\">Dimitrios Kosmopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14084","description":"<p>We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/tree/main/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Fact Linking. (arXiv:2109.14364v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14364","description":"<p>Knowledge-intensive NLP tasks can benefit from linking natural language text\nwith facts from a Knowledge Graph (KG). Although facts themselves are\nlanguage-agnostic, the fact labels (i.e., language-specific representation of\nthe fact) in the KG are often present only in a few languages. This makes it\nchallenging to link KG facts to sentences in languages other than the limited\nset of languages. To address this problem, we introduce the task of\nMultilingual Fact Linking (MFL) where the goal is to link fact expressed in a\nsentence to corresponding fact in the KG, even when the fact label in the KG is\nnot available in the language of the sentence. To facilitate research in this\narea, we present a new evaluation dataset, IndicLink. This dataset contains\n11,293 linked WikiData facts and 6,429 sentences spanning English and six\nIndian languages. We propose a Retrieval+Generation model, ReFCoG, that can\nscale to millions of KG facts by combining Dual Encoder based retrieval with a\nSeq2Seq based generation model which is constrained to output only valid KG\nfacts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in\nPrecision@1. In spite of this gain, the model achieves an overall score of\n52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink\ndata are available at https://github.com/SaiKeshav/mfl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolluru_K/0/1/0/all/0/1\">Keshav Kolluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezk_M/0/1/0/all/0/1\">Martin Rezk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDGAR-CORPUS: Billions of Tokens Make The World Go Round. (arXiv:2109.14394v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14394","description":"<p>We release EDGAR-CORPUS, a novel corpus comprising annual reports from all\nthe publicly traded companies in the US spanning a period of more than 25\nyears. To the best of our knowledge, EDGAR-CORPUS is the largest financial NLP\ncorpus available to date. All the reports are downloaded, split into their\ncorresponding items (sections), and provided in a clean, easy-to-use JSON\nformat. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC\nembeddings for the financial domain. We employ these embeddings in a battery of\nfinancial NLP tasks and showcase their superiority over generic GloVe\nembeddings and other existing financial word embeddings. We also open-source\nEDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future\nannual reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1\">Lefteris Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1\">Prodromos Malakasiotis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14420","description":"<p>Error correction is widely used in automatic speech recognition (ASR) to\npost-process the generated sentence, and can further reduce the word error rate\n(WER). Although multiple candidates are generated by an ASR system through beam\nsearch, current error correction approaches can only correct one sentence at a\ntime, failing to leverage the voting effect from multiple candidates to better\ndetect and correct error tokens. In this work, we propose FastCorrect 2, an\nerror correction model that takes multiple ASR candidates as input for better\ncorrection accuracy. FastCorrect 2 adopts non-autoregressive generation for\nfast inference, which consists of an encoder that processes multiple source\nsentences and a decoder that generates the target sentence in parallel from the\nadjusted source sentence, where the adjustment is based on the predicted\nduration of each source token. However, there are some issues when handling\nmultiple source sentences. First, it is non-trivial to leverage the voting\neffect from multiple source sentences since they usually vary in length. Thus,\nwe propose a novel alignment algorithm to maximize the degree of token\nalignment among multiple sentences in terms of token and pronunciation\nsimilarity. Second, the decoder can only take one adjusted source sentence as\ninput, while there are multiple source sentences. Thus, we develop a candidate\npredictor to detect the most suitable candidate for the decoder. Experiments on\nour inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce\nthe WER over the previous correction model with single candidate by 3.2% and\n2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR\nerror correction. FastCorrect 2 achieves better performance than the cascaded\nre-scoring and correction pipeline and can serve as a unified post-processing\nmodule for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Fake News Detection Using Bidirectional Encoder Representations from Transformers Based Models. (arXiv:2109.14816v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14816","description":"<p>Nowadays, the development of social media allows people to access the latest\nnews easily. During the COVID-19 pandemic, it is important for people to access\nthe news so that they can take corresponding protective measures. However, the\nfake news is flooding and is a serious issue especially under the global\npandemic. The misleading fake news can cause significant loss in terms of the\nindividuals and the society. COVID-19 fake news detection has become a novel\nand important task in the NLP field. However, fake news always contain the\ncorrect portion and the incorrect portion. This fact increases the difficulty\nof the classification task. In this paper, we fine tune the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model as our\nbase model. We add BiLSTM layers and CNN layers on the top of the finetuned\nBERT model with frozen parameters or not frozen parameters methods\nrespectively. The model performance evaluation results showcase that our best\nmodel (BERT finetuned model with frozen parameters plus BiLSTM layers) achieves\nstate-of-the-art results towards COVID-19 fake news detection task. We also\nexplore keywords evaluation methods using our best model and evaluate the model\nperformance after removing keywords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuebo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prose2Poem: The Blessing of Transformers in Translating Prose to Persian Poetry. (arXiv:2109.14934v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14934","description":"<p>Persian Poetry has consistently expressed its philosophy, wisdom, speech, and\nrationale on the basis of its couplets, making it an enigmatic language on its\nown to both native and non-native speakers. Nevertheless, the notice able gap\nbetween Persian prose and poem has left the two pieces of literature\nmedium-less. Having curated a parallel corpus of prose and their equivalent\npoems, we introduce a novel Neural Machine Translation (NMT) approach to\ntranslate prose to ancient Persian poetry using transformer-based Language\nModels in an extremely low-resource setting. More specifically, we trained a\nTransformer model from scratch to obtain initial translations and pretrained\ndifferent variations of BERT to obtain final translations. To address the\nchallenge of using masked language modelling under poeticness criteria, we\nheuristically joined the two models and generated valid poems in terms of\nautomatic and human assessments. Final results demonstrate the eligibility and\ncreativity of our novel heuristically aided approach among Literature\nprofessionals and non-professionals in generating novel Persian poems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirshafiee_M/0/1/0/all/0/1\">Mitra Sadat Mirshafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouryabi_Y/0/1/0/all/0/1\">Yazdan Rezaee Jouryabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Mining for strong gravitational lenses with self-supervised learning. (arXiv:2110.00023v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2110.00023","description":"<p>We employ self-supervised representation learning to distill information from\n76 million galaxy images from the Dark Energy Spectroscopic Instrument (DESI)\nLegacy Imaging Surveys' Data Release 9. Targeting the identification of new\nstrong gravitational lens candidates, we first create a rapid similarity search\ntool to discover new strong lenses given only a single labelled example. We\nthen show how training a simple linear classifier on the self-supervised\nrepresentations, requiring only a few minutes on a CPU, can automatically\nclassify strong lenses with great efficiency. We present 1192 new strong lens\ncandidates that we identified through a brief visual identification campaign,\nand release an interactive web-based similarity search tool and the top network\npredictions to facilitate crowd-sourcing rapid discovery of additional strong\ngravitational lenses and other rare objects:\ngithub.com/georgestein/ssl-legacysurvey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Stein_G/0/1/0/all/0/1\">George Stein</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Blaum_J/0/1/0/all/0/1\">Jacqueline Blaum</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Harrington_P/0/1/0/all/0/1\">Peter Harrington</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Medan_T/0/1/0/all/0/1\">Tomislav Medan</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Lukic_Z/0/1/0/all/0/1\">Zarija Lukic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-Site Harmonization of Magnetic Resonance Images Without Traveling Human Phantoms. (arXiv:2110.00041v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00041","description":"<p>Harmonization improves data consistency and is central to effective\nintegration of diverse imaging data acquired across multiple sites. Recent deep\nlearning techniques for harmonization are predominantly supervised in nature\nand hence require imaging data of the same human subjects to be acquired at\nmultiple sites. Data collection as such requires the human subjects to travel\nacross sites and is hence challenging, costly, and impractical, more so when\nsufficient sample size is needed for reliable network training. Here we show\nhow harmonization can be achieved with a deep neural network that does not rely\non traveling human phantom data. Our method disentangles site-specific\nappearance information and site-invariant anatomical information from images\nacquired at multiple sites and then employs the disentangled information to\ngenerate the image of each subject for any target site. We demonstrate with\nmore than 6,000 multi-site T1- and T2-weighted images that our method is\nremarkably effective in generating images with realistic site-specific\nappearances without altering anatomical details. Our method allows\nretrospective harmonization of data in a wide range of existing modern\nlarge-scale imaging studies, conducted via different scanners and protocols,\nwithout additional data collection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Siyuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Quadratic Optimisation over the Stiefel Manifold with Application to Permutation Synchronisation. (arXiv:2110.00053v1 [math.OC])","link":"http://arxiv.org/abs/2110.00053","description":"<p>We address the non-convex optimisation problem of finding a sparse matrix on\nthe Stiefel manifold (matrices with mutually orthogonal columns of unit length)\nthat maximises (or minimises) a quadratic objective function. Optimisation\nproblems on the Stiefel manifold occur for example in spectral relaxations of\nvarious combinatorial problems, such as graph matching, clustering, or\npermutation synchronisation. Although sparsity is a desirable property in such\nsettings, it is mostly neglected in spectral formulations since existing\nsolvers, e.g. based on eigenvalue decomposition, are unable to account for\nsparsity while at the same time maintaining global optimality guarantees. We\nfill this gap and propose a simple yet effective sparsity-promoting\nmodification of the Orthogonal Iteration algorithm for finding the dominant\neigenspace of a matrix. By doing so, we can guarantee that our method finds a\nStiefel matrix that is globally optimal with respect to the quadratic objective\nfunction, while in addition being sparse. As a motivating application we\nconsider the task of permutation synchronisation, which can be understood as a\nconstrained clustering problem that has particular relevance for matching\nmultiple images or 3D shapes in computer vision, computer graphics, and beyond.\nWe demonstrate that the proposed approach outperforms previous methods in this\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/math/1/au:+Thunberg_J/0/1/0/all/0/1\">Johan Thunberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Predict Trustworthiness with Steep Slope Loss. (arXiv:2110.00054v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00054","description":"<p>Understanding the trustworthiness of a prediction yielded by a classifier is\ncritical for the safe and effective use of AI models. Prior efforts have been\nproven to be reliable on small-scale datasets. In this work, we study the\nproblem of predicting trustworthiness on real-world large-scale datasets, where\nthe task is more challenging due to high-dimensional features, diverse visual\nconcepts, and large-scale samples. In such a setting, we observe that the\ntrustworthiness predictors trained with prior-art loss functions, i.e., the\ncross entropy loss, focal loss, and true class probability confidence loss, are\nprone to view both correct predictions and incorrect predictions to be\ntrustworthy. The reasons are two-fold. Firstly, correct predictions are\ngenerally dominant over incorrect predictions. Secondly, due to the data\ncomplexity, it is challenging to differentiate the incorrect predictions from\nthe correct ones on real-world large-scale datasets. To improve the\ngeneralizability of trustworthiness predictors, we propose a novel steep slope\nloss to separate the features w.r.t. correct predictions from the ones w.r.t.\nincorrect predictions by two slide-like curves that oppose each other. The\nproposed loss is evaluated with two representative deep learning models, i.e.,\nVision Transformer and ResNet, as trustworthiness predictors. We conduct\ncomprehensive experiments and analyses on ImageNet, which show that the\nproposed loss effectively improves the generalizability of trustworthiness\npredictors. The code and pre-trained trustworthiness predictors for\nreproducibility are available at\nhttps://github.com/luoyan407/predict_trustworthiness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan S. Kankanhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific evidence extraction. (arXiv:2110.00061v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00061","description":"<p>Recently, interest has grown in applying machine learning to the problem of\ntable structure inference and extraction from unstructured documents. However,\nprogress in this area has been challenging both to make and to measure, due to\nseveral issues that arise in training and evaluating models from labeled data.\nThis includes challenges as fundamental as the lack of a single definitive\nground truth output for each input sample and the lack of an ideal metric for\nmeasuring partial correctness for this task. To address these we propose a new\ndataset, PubMed Tables One Million (PubTables-1M), and a new class of metric,\ngrid table similarity (GriTS). PubTables-1M is nearly twice as large as the\nprevious largest comparable dataset, can be used for models across multiple\narchitectures and modalities, and addresses issues such as ambiguity and lack\nof consistency in the annotations. We apply DETR to table extraction for the\nfirst time and show that object detection models trained on PubTables-1M\nproduce excellent results out-of-the-box for all three tasks of detection,\nstructure recognition, and functional analysis. We describe the dataset in\ndetail to enable others to build on our work and combine this data with other\ndatasets for these and related tasks. It is our hope that PubTables-1M and the\nproposed metrics can further progress in this area by creating a benchmark\nsuitable for training and evaluating a wide variety of models for table\nextraction. Data and code will be released at\nhttps://github.com/microsoft/table-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smock_B/0/1/0/all/0/1\">Brandon Smock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesala_R/0/1/0/all/0/1\">Rohith Pesala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_R/0/1/0/all/0/1\">Robin Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise2Recon: A Semi-Supervised Framework for Joint MRI Reconstruction and Denoising. (arXiv:2110.00075v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00075","description":"<p>Deep learning (DL) has shown promise for faster, high quality accelerated MRI\nreconstruction. However, standard supervised DL methods depend on extensive\namounts of fully-sampled ground-truth data and are sensitive to\nout-of-distribution (OOD) shifts, in particular for low signal-to-noise ratio\n(SNR) acquisitions. To alleviate this challenge, we propose a semi-supervised,\nconsistency-based framework (termed Noise2Recon) for joint MR reconstruction\nand denoising. Our method enables the usage of a limited number of\nfully-sampled and a large number of undersampled-only scans. We compare our\nmethod to augmentation-based supervised techniques and fine-tuned denoisers.\nResults demonstrate that even with minimal ground-truth data, Noise2Recon (1)\nachieves high performance on in-distribution (low-noise) scans and (2) improves\ngeneralizability to OOD, noisy scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Desai_A/0/1/0/all/0/1\">Arjun D Desai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu M Ozturkler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandino_C/0/1/0/all/0/1\">Christopher M Sandino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasanawala_S/0/1/0/all/0/1\">Shreyas Vasanawala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hargreaves_B/0/1/0/all/0/1\">Brian A Hargreaves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Re_C/0/1/0/all/0/1\">Christopher M Re</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pauly_J/0/1/0/all/0/1\">John M Pauly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay S Chaudhari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Inverse Rendering By Using a GPU and Reuse of Light Paths. (arXiv:2110.00085v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00085","description":"<p>Inverse rendering seeks to estimate scene characteristics from a set of data\nimages. The dominant approach is based on differential rendering using\nMonte-Carlo. Algorithms as such usually rely on a forward model and use an\niterative gradient method that requires sampling millions of light paths per\niteration. This paper presents an efficient framework that speeds up existing\ninverse rendering algorithms. This is achieved by tailoring the iterative\nprocess of inverse rendering specifically to a GPU architecture. For this\ncause, we introduce two interleaved steps - Path Sorting and Path Recycling.\nPath Sorting allows the GPU to deal with light paths of the same size. Path\nRecycling allows the algorithm to use light paths from previous iterations to\nbetter utilize the information they encode. Together, these steps significantly\nspeed up gradient optimization. In this paper, we give the theoretical\nbackground for Path Recycling. We demonstrate its efficiency for volumetric\nscattering tomography and reflectometry (surface reflections).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Czerninski_I/0/1/0/all/0/1\">Ido Czerninski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schechner_Y/0/1/0/all/0/1\">Yoav Y. Schechner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing Glass: Joint Point Cloud and Depth Completion for Transparent Objects. (arXiv:2110.00087v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00087","description":"<p>The basis of many object manipulation algorithms is RGB-D input. Yet,\ncommodity RGB-D sensors can only provide distorted depth maps for a wide range\nof transparent objects due light refraction and absorption. To tackle the\nperception challenges posed by transparent objects, we propose TranspareNet, a\njoint point cloud and depth completion method, with the ability to complete the\ndepth of transparent objects in cluttered and complex scenes, even with\npartially filled fluid contents within the vessels. To address the shortcomings\nof existing transparent object data collection schemes in literature, we also\npropose an automated dataset creation workflow that consists of\nrobot-controlled image collection and vision-based automatic annotation.\nThrough this automated workflow, we created Toronto Transparent Objects Depth\nDataset (TODD), which consists of nearly 15000 RGB-D images. Our experimental\nevaluation demonstrates that TranspareNet outperforms existing state-of-the-art\ndepth completion methods on multiple datasets, including ClearGrasp, and that\nit also handles cluttered scenes when trained on TODD. Code and dataset will be\nreleased at https://www.pair.toronto.edu/TranspareNet/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Ru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eppel_S/0/1/0/all/0/1\">Sagi Eppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aspuru_Guzik_A/0/1/0/all/0/1\">Al&#xe0;n Aspuru-Guzik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shkurti_F/0/1/0/all/0/1\">Florian Shkurti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMCAT: Large-Scale Deep Clustering for Medical Image Categorization. (arXiv:2110.00109v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00109","description":"<p>In recent years, the research landscape of machine learning in medical\nimaging has changed drastically from supervised to semi-, weakly- or\nunsupervised methods. This is mainly due to the fact that ground-truth labels\nare time-consuming and expensive to obtain manually. Generating labels from\npatient metadata might be feasible but it suffers from user-originated errors\nwhich introduce biases. In this work, we propose an unsupervised approach for\nautomatically clustering and categorizing large-scale medical image datasets,\nwith a focus on cardiac MR images, and without using any labels. We\ninvestigated the end-to-end training using both class-balanced and imbalanced\nlarge-scale datasets. Our method was able to create clusters with high purity\nand achieved over 0.99 cluster purity on these datasets. The results\ndemonstrate the potential of the proposed method for categorizing unstructured\nlarge medical databases, such as organizing clinical PACS systems in hospitals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kart_T/0/1/0/all/0/1\">Turkay Kart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Action Detection in Untrimmed Videos: A Survey. (arXiv:2110.00111v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00111","description":"<p>Understanding human behavior and activity facilitates advancement of numerous\nreal-world applications, and is critical for video analysis. Despite the\nprogress of action recognition algorithms in trimmed videos, the majority of\nreal-world videos are lengthy and untrimmed with sparse segments of interest.\nThe task of temporal activity detection in untrimmed videos aims to localize\nthe temporal boundary of actions and classify the action categories. Temporal\nactivity detection task has been investigated in full and limited supervision\nsettings depending on the availability of action annotations. This paper\nprovides an extensive overview of deep learning-based algorithms to tackle\ntemporal action detection in untrimmed videos with different supervision levels\nincluding fully-supervised, weakly-supervised, unsupervised, self-supervised,\nand semi-supervised. In addition, this paper also reviews advances in\nspatio-temporal action detection where actions are localized in both temporal\nand spatial dimensions. Moreover, the commonly used action detection benchmark\ndatasets and evaluation metrics are described, and the performance of the\nstate-of-the-art methods are compared. Finally, real-world applications of\ntemporal action detection in untrimmed videos and a set of future directions\nare discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vahdani_E/0/1/0/all/0/1\">Elahe Vahdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUMBI: A Large Multiview Dataset of Human Body Expressions and Benchmark Challenge. (arXiv:2110.00119v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00119","description":"<p>This paper presents a new large multiview dataset called HUMBI for human body\nexpressions with natural clothing. The goal of HUMBI is to facilitate modeling\nview-specific appearance and geometry of five primary body signals including\ngaze, face, hand, body, and garment from assorted people. 107 synchronized HD\ncameras are used to capture 772 distinctive subjects across gender, ethnicity,\nage, and style. With the multiview image streams, we reconstruct high fidelity\nbody expressions using 3D mesh models, which allows representing view-specific\nappearance. We demonstrate that HUMBI is highly effective in learning and\nreconstructing a complete human model and is complementary to the existing\ndatasets of human body expressions with limited views and subjects such as\nMPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets. Based on HUMBI,\nwe formulate a new benchmark challenge of a pose-guided appearance rendering\ntask that aims to substantially extend photorealism in modeling diverse human\nexpressions in 3D, which is the key enabling factor of authentic social\ntele-presence. HUMBI is publicly available at <a href=\"http://humbi-data.net\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jae Shin Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhixuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyun Soo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of the algorithm for differentiating bone metastases and trauma of the ribs in bone scintigraphy and demonstration of visual evidence of the algorithm -- Using only anterior bone scan view of thorax. (arXiv:2110.00130v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00130","description":"<p>Background: Although there are many studies on the application of artificial\nintelligence (AI) models to medical imaging, there is no report of an AI model\nthat determines the accumulation of ribs in bone metastases and trauma only\nusing the anterior image of thorax of bone scintigraphy. In recent years, a\nmethod for visualizing diagnostic grounds called Gradient-weighted Class\nActivation Mapping (Grad-CAM) has been proposed in the area of diagnostic\nimages using Deep Convolutional Neural Network (DCNN). As far as we have\ninvestigated, there are no reports of visualization of the diagnostic basis in\nbone scintigraphy. Our aim is to visualize the area of interest of DCNN, in\naddition to developing an algorithm to classify and diagnose whether RI\naccumulation on the ribs is bone metastasis or trauma using only anterior bone\nscan view of thorax. Material and Methods: For this retrospective study, we\nused 838 patients who underwent bone scintigraphy to search for bone metastases\nat our institution. A frontal chest image of bone scintigraphy was used to\ncreate the algorithm. We used 437 cases with bone metastases on the ribs and\n401 cases with abnormal RI accumulation due to trauma. Result: AI model was\nable to detect bone metastasis lesion with a sensitivity of 90.00% and accuracy\nof 86.5%. And it was possible to visualize the part that the AI model focused\non with Grad-CAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higashiyama_S/0/1/0/all/0/1\">Shigeaki Higashiyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohta_Y/0/1/0/all/0/1\">Yukino Ohta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Katayama_Y/0/1/0/all/0/1\">Yutaka Katayama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshida_A/0/1/0/all/0/1\">Atsushi Yoshida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawabe_J/0/1/0/all/0/1\">Joji Kawabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Neighbourhood-Preserving Transformations for Quantization-Based Unsupervised Hashing. (arXiv:2110.00216v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00216","description":"<p>An effective unsupervised hashing algorithm leads to compact binary codes\npreserving the neighborhood structure of data as much as possible. One of the\nmost established schemes for unsupervised hashing is to reduce the\ndimensionality of data and then find a rigid (neighbourhood-preserving)\ntransformation that reduces the quantization error. Although employing rigid\ntransformations is effective, we may not reduce quantization loss to the\nultimate limits. As well, reducing dimensionality and quantization loss in two\nseparate steps seems to be sub-optimal. Motivated by these shortcomings, we\npropose to employ both rigid and non-rigid transformations to reduce\nquantization error and dimensionality simultaneously. We relax the\northogonality constraint on the projection in a PCA-formulation and regularize\nthis by a quantization term. We show that both the non-rigid projection matrix\nand rotation matrix contribute towards minimizing quantization loss but in\ndifferent ways. A scalable nested coordinate descent approach is proposed to\noptimize this mixed-integer optimization problem. We evaluate the proposed\nmethod on five public benchmark datasets providing almost half a million\nimages. Comparative results indicate that the proposed method mostly\noutperforms state-of-art linear methods and competes with end-to-end deep\nsolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hemati_S/0/1/0/all/0/1\">Sobhan Hemati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1\">H.R. Tizhoosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Object Permanence using Agent Actions and Reasoning. (arXiv:2110.00238v1 [cs.RO])","link":"http://arxiv.org/abs/2110.00238","description":"<p>Object permanence in psychology means knowing that objects still exist even\nif they are no longer visible. It is a crucial concept for robots to operate\nautonomously in uncontrolled environments. Existing approaches learn object\npermanence from low-level perception, but perform poorly on more complex\nscenarios, like when objects are contained and carried by others. Knowledge\nabout manipulation actions performed on an object prior to its disappearance\nallows us to reason about its location, e.g., that the object has been placed\nin a carrier. In this paper we argue that object permanence can be improved\nwhen the robot uses knowledge about executed actions and describe an approach\nto infer hidden object states from agent actions. We show that considering\nagent actions not only improves rule-based reasoning models but also purely\nneural approaches, showing its general applicability. Then, we conduct\nquantitative experiments on a snitch localization task using a dataset of 1,371\nsynthesized videos, where we compare the performance of different object\npermanence models with and without action annotations. We demonstrate that\nmodels with action annotations can significantly increase performance of both\nneural and rule-based approaches. Finally, we evaluate the usability of our\napproach in real-world applications by conducting qualitative experiments with\ntwo Universal Robots (UR5 and UR16e) in both lab and industrial settings. The\nrobots complete benchmark tasks for a gearbox assembly and demonstrate the\nobject permanence capabilities with real sensor data in an industrial\nenvironment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Ying Siu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dongkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_K/0/1/0/all/0/1\">Kenneth Kwok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Instance Segmentation with a Single GPU. (arXiv:2110.00242v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00242","description":"<p>Not everyone is wealthy enough to have hundreds of GPUs or TPUs. Therefore,\nwe've got to find a way out. In this paper, we introduce a data-efficient\ninstance segmentation method we used in the 2021 VIPriors Instance Segmentation\nChallenge. Our solution is a modified version of Swin Transformer, based on the\nmmdetection which is a powerful toolbox. To solve the problem of lack of data,\nwe utilize data augmentation including random flip and multiscale training to\ntrain our model. During inference, multiscale fusion is used to boost the\nperformance. We only use a single GPU during the whole training and testing\nstages. In the end, our team named THU_IVG_2018 achieved the result of 0.366\nfor AP@0.50:0.95 on the test set, which is competitive with other top-ranking\nmethods while only one GPU is used. Besides, our method achieved the\nAP@0.50:0.95 (medium) of 0.592, which ranks second among all contestants\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Transformer in Federated Setting for Human Activity Recognition. (arXiv:2110.00244v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00244","description":"<p>Human Activity Recognition (HAR) has been a challenging problem yet it needs\nto be solved. It will mainly be used for eldercare and healthcare as an\nassistive technology when ensemble with other technologies like Internet of\nThings(IoT). HAR can be achieved with the help of sensors, smartphones or\nimages. Deep neural network techniques like artificial neural networks,\nconvolutional neural networks and recurrent neural networks have been used in\nHAR, both in centralized and federated setting. However, these techniques have\ncertain limitations. RNNs have limitation of parallelization, CNNS have the\nlimitation of sequence length and they are computationally expensive. In this\npaper, to address the state of art challenges, we present a inertial\nsensors-based novel one patch transformer which gives the best of both RNNs and\nCNNs for Human activity recognition. We also design a testbed to collect\nreal-time human activity data. The data collected is further used to train and\ntest the proposed transformer. With the help of experiments, we show that the\nproposed transformer outperforms the state of art CNN and RNN based\nclassifiers, both in federated and centralized setting. Moreover, the proposed\ntransformer is computationally inexpensive as it uses very few parameter\ncompared to the existing state of art CNN and RNN based classifier. Thus its\nmore suitable for federated learning as it provides less communication and\ncomputational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1\">Ali Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Kim Phuc Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehl_L/0/1/0/all/0/1\">Ludovic Koehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benzaidi_K/0/1/0/all/0/1\">Khaled Benzaidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synergizing between Self-Training and Adversarial Learning for Domain Adaptive Object Detection. (arXiv:2110.00249v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00249","description":"<p>We study adapting trained object detectors to unseen domains manifesting\nsignificant variations of object appearance, viewpoints and backgrounds. Most\ncurrent methods align domains by either using image or instance-level feature\nalignment in an adversarial fashion. This often suffers due to the presence of\nunwanted background and as such lacks class-specific alignment. A common remedy\nto promote class-level alignment is to use high confidence predictions on the\nunlabelled domain as pseudo labels. These high confidence predictions are often\nfallacious since the model is poorly calibrated under domain shift. In this\npaper, we propose to leverage model predictive uncertainty to strike the right\nbalance between adversarial feature alignment and class-level alignment.\nSpecifically, we measure predictive uncertainty on class assignments and the\nbounding box predictions. Model predictions with low uncertainty are used to\ngenerate pseudo-labels for self-supervision, whereas the ones with higher\nuncertainty are used to generate tiles for an adversarial feature alignment\nstage. This synergy between tiling around the uncertain object regions and\ngenerating pseudo-labels from highly certain object regions allows us to\ncapture both the image and instance level context during the model adaptation\nstage. We perform extensive experiments covering various domain shift\nscenarios. Our approach improves upon existing state-of-the-art methods with\nvisible margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munir_M/0/1/0/all/0/1\">Muhammad Akhtar Munir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Haris Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfraz_M/0/1/0/all/0/1\">M. Saquib Sarfraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Memory-Guided Semantic Reasoning Model for Image Inpainting. (arXiv:2110.00261v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00261","description":"<p>Most existing methods for image inpainting focus on learning the intra-image\npriors from the known regions of the current input image to infer the content\nof the corrupted regions in the same image. While such methods perform well on\nimages with small corrupted regions, it is challenging for these methods to\ndeal with images with large corrupted area due to two potential limitations: 1)\nsuch methods tend to overfit each single training pair of images relying solely\non the intra-image prior knowledge learned from the limited known area; 2) the\ninter-image prior knowledge about the general distribution patterns of visual\nsemantics, which can be transferred across images sharing similar semantics, is\nnot exploited. In this paper, we propose the Generative Memory-Guided Semantic\nReasoning Model (GM-SRM), which not only learns the intra-image priors from the\nknown regions, but also distills the inter-image reasoning priors to infer the\ncontent of the corrupted regions. In particular, the proposed GM-SRM first\npre-learns a generative memory from the whole training data to capture the\nsemantic distribution patterns in a global view. Then the learned memory are\nleveraged to retrieve the matching inter-image priors for the current corrupted\nimage to perform semantic reasoning during image inpainting. While the\nintra-image priors are used for guaranteeing the pixel-level content\nconsistency, the inter-image priors are favorable for performing high-level\nsemantic reasoning, which is particularly effective for inferring semantic\ncontent for large corrupted area. Extensive experiments on Paris Street View,\nCelebA-HQ, and Places2 benchmarks demonstrate that our GM-SRM outperforms the\nstate-of-the-art methods for image inpainting in terms of both the visual\nquality and quantitative metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From SLAM to Situational Awareness: Challenges and Survey. (arXiv:2110.00273v1 [cs.RO])","link":"http://arxiv.org/abs/2110.00273","description":"<p>The knowledge that an intelligent and autonomous mobile robot has and is able\nto acquire of itself and the environment, namely the situation, limits its\nreasoning, decision-making, and execution skills to efficiently and safely\nperform complex missions. Situational awareness is a basic capability of humans\nthat has been deeply studied in fields like Psychology, Military, Aerospace,\nEducation, etc., but it has barely been considered in robotics, which has\nfocused on ideas such as sensing, perception, sensor fusion, state estimation,\nlocalization and mapping, spatial AI, etc. In our research, we connected the\nbroad multidisciplinary existing knowledge on situational awareness with its\ncounterpart in mobile robotics. In this paper, we survey the state-of-the-art\nrobotics algorithms, we analyze the situational awareness aspects that have\nbeen covered by them, and we discuss their missing points. We found out that\nthe existing robotics algorithms are still missing manifold important aspects\nof situational awareness. As a consequence, we conclude that these missing\nfeatures are limiting the performance of robotic situational awareness, and\nfurther research is needed to overcome this challenge. We see this as an\nopportunity, and provide our vision for future research on robotic situational\nawareness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bavle_H/0/1/0/all/0/1\">Hriday Bavle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Lopez_J/0/1/0/all/0/1\">Jose Luis Sanchez-Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_E/0/1/0/all/0/1\">Eduardo F. Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voos_H/0/1/0/all/0/1\">Holger Voos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Modeling for Learnable Human Pose Triangulation. (arXiv:2110.00280v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00280","description":"<p>We propose a stochastic modeling framework for 3D human pose triangulation\nand evaluate its performance across different datasets and spatial camera\narrangements. The common approach to 3D pose estimation is to first detect 2D\nkeypoints in images and then apply the triangulation from multiple views.\nHowever, the majority of existing triangulation models are limited to a single\ndataset, i.e. camera arrangement and their number. Moreover, they require known\ncamera parameters. The proposed stochastic pose triangulation model\nsuccessfully generalizes to different camera arrangements and between two\npublic datasets. In each step, we generate a set of 3D pose hypotheses obtained\nby triangulation from a random subset of views. The hypotheses are evaluated by\na neural network and the expectation of the triangulation error is minimized.\nThe key novelty is that the network learns to evaluate the poses without taking\ninto account the spatial camera arrangement, thus improving generalization.\nAdditionally, we demonstrate that the proposed stochastic framework can also be\nused for fundamental matrix estimation, showing promising results towards\nrelative camera pose estimation from noisy keypoint correspondences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartol_K/0/1/0/all/0/1\">Kristijan Bartol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanic_D/0/1/0/all/0/1\">David Bojani&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_T/0/1/0/all/0/1\">Tomislav Petkovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pribanic_T/0/1/0/all/0/1\">Tomislav Pribani&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCT based Fusion of Variable Exposure Images for HDRI. (arXiv:2110.00312v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00312","description":"<p>Combining images with different exposure settings are of prime importance in\nthe field of computational photography. Both transform domain approach and\nfiltering based approaches are possible for fusing multiple exposure images, to\nobtain the well-exposed image. We propose a Discrete Cosine Transform\n(DCT-based) approach for fusing multiple exposure images. The input image stack\nis processed in the transform domain by an averaging operation and the inverse\ntransform is performed on the averaged image obtained to generate the fusion of\nmultiple exposure image. The experimental observation leads us to the\nconjecture that the obtained DCT coefficients are indicators of parameters to\nmeasure well-exposedness, contrast and saturation as specified in the\ntraditional exposure fusion based approach and the averaging performed\nindicates equal weights assigned to the DCT coefficients in this non-parametric\nand non pyramidal approach to fuse the multiple exposure stack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ramakarishnan_V/0/1/0/all/0/1\">Vivek Ramakarishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pete_D/0/1/0/all/0/1\">Dnyaneshwar Jageshwar Pete</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Cluster Separation Using High-Dimensional Sharpened Dimensionality Reduction. (arXiv:2110.00317v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00317","description":"<p>Applying dimensionality reduction (DR) to large, high-dimensional data sets\ncan be challenging when distinguishing the underlying high-dimensional data\nclusters in a 2D projection for exploratory analysis. We address this problem\nby first sharpening the clusters in the original high-dimensional data prior to\nthe DR step using Local Gradient Clustering (LGC). We then project the\nsharpened data from the high-dimensional space to 2D by a user-selected DR\nmethod. The sharpening step aids this method to preserve cluster separation in\nthe resulting 2D projection. With our method, end-users can label each distinct\ncluster to further analyze an otherwise unlabeled data set. Our\n`High-Dimensional Sharpened DR' (HD-SDR) method, tested on both synthetic and\nreal-world data sets, is favorable to DR methods with poor cluster separation\nand yields a better visual cluster separation than these DR methods with no\nsharpening. Our method achieves good quality (measured by quality metrics) and\nscales computationally well with large high-dimensional data. To illustrate its\nconcrete applications, we further apply HD-SDR on a recent astronomical\ncatalog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1\">Alexandru C. Telea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trager_S/0/1/0/all/0/1\">Scott C. Trager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roerdink_J/0/1/0/all/0/1\">Jos B. T. M. Roerdink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Student Helping Teacher: Teacher Evolution via Self-Knowledge Distillation. (arXiv:2110.00329v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00329","description":"<p>Knowledge distillation usually transfers the knowledge from a pre-trained\ncumbersome teacher network to a compact student network, which follows the\nclassical teacher-teaching-student paradigm. Based on this paradigm, previous\nmethods mostly focus on how to efficiently train a better student network for\ndeployment. Different from the existing practices, in this paper, we propose a\nnovel student-helping-teacher formula, Teacher Evolution via Self-Knowledge\nDistillation (TESKD), where the target teacher (for deployment) is learned with\nthe help of multiple hierarchical students by sharing the structural backbone.\nThe diverse feedback from multiple students allows the teacher to improve\nitself through the shared feature representations. The effectiveness of our\nproposed framework is demonstrated by extensive experiments with various\nnetwork settings on two standard benchmarks including CIFAR-100 and ImageNet.\nNotably, when trained together with our proposed method, ResNet-18 achieves\n79.15% and 71.14% accuracy on CIFAR-100 and ImageNet, outperforming the\nbaseline results by 4.74% and 1.43%, respectively. The code is available at:\nhttps://github.com/zhengli427/TESKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhigeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry Attention Transformer with Position-aware LSTMs for Image Captioning. (arXiv:2110.00335v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00335","description":"<p>In recent years, transformer structures have been widely applied in image\ncaptioning with impressive performance. For good captioning results, the\ngeometry and position relations of different visual objects are often thought\nof as crucial information. Aiming to further promote image captioning by\ntransformers, this paper proposes an improved Geometry Attention Transformer\n(GAT) model. In order to further leverage geometric information, two novel\ngeometry-aware architectures are designed respectively for the encoder and\ndecoder in our GAT. Besides, this model includes the two work modules: 1) a\ngeometry gate-controlled self-attention refiner, for explicitly incorporating\nrelative spatial information into image region representations in encoding\nsteps, and 2) a group of position-LSTMs, for precisely informing the decoder of\nrelative word position in generating caption texts. The experiment comparisons\non the datasets MS COCO and Flickr30K show that our GAT is efficient, and it\ncould often outperform current state-of-the-art image captioning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Luping Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhiNets: a scalable backbone for low-power AI at the edge. (arXiv:2110.00337v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00337","description":"<p>In the Internet of Things era, where we see many interconnected and\nheterogeneous mobile and fixed smart devices, distributing the intelligence\nfrom the cloud to the edge has become a necessity. Due to limited computational\nand communication capabilities, low memory and limited energy budget, bringing\nartificial intelligence algorithms to peripheral devices, such as the end-nodes\nof a sensor network, is a challenging task and requires the design of\ninnovative methods. In this work, we present PhiNets, a new scalable backbone\noptimized for deep-learning-based image processing on resource-constrained\nplatforms. PhiNets are based on inverted residual blocks specifically designed\nto decouple the computational cost, working memory, and parameter memory, thus\nexploiting all the available resources. With a YoloV2 detection head and Simple\nOnline and Realtime Tracking, the proposed architecture has achieved the\nstate-of-the-art results in (i) detection on the COCO and VOC2012 benchmarks,\nand (ii) tracking on the MOT15 benchmark. PhiNets reduce the parameter count of\n87% to 93% with respect to previous state-of-the-art models (EfficientNetv1,\nMobileNetv2) and achieve better performance with lower computational cost.\nMoreover, we demonstrate our approach on a prototype node based on a STM32H743\nmicrocontroller (MCU) with 2MB of internal Flash and 1MB of RAM and achieve\npower requirements in the order of 10 mW. The code for the PhiNets is publicly\navailable on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paissan_F/0/1/0/all/0/1\">Francesco Paissan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ancilotto_A/0/1/0/all/0/1\">Alberto Ancilotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farella_E/0/1/0/all/0/1\">Elisabetta Farella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarize and Search: Learning Consensus-aware Dynamic Convolution for Co-Saliency Detection. (arXiv:2110.00338v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00338","description":"<p>Humans perform co-saliency detection by first summarizing the consensus\nknowledge in the whole group and then searching corresponding objects in each\nimage. Previous methods usually lack robustness, scalability, or stability for\nthe first process and simply fuse consensus features with image features for\nthe second process. In this paper, we propose a novel consensus-aware dynamic\nconvolution model to explicitly and effectively perform the \"summarize and\nsearch\" process. To summarize consensus image features, we first summarize\nrobust features for every single image using an effective pooling method and\nthen aggregate cross-image consensus cues via the self-attention mechanism. By\ndoing this, our model meets the scalability and stability requirements. Next,\nwe generate dynamic kernels from consensus features to encode the summarized\nconsensus knowledge. Two kinds of kernels are generated in a supplementary way\nto summarize fine-grained image-specific consensus object cues and the coarse\ngroup-wise common knowledge, respectively. Then, we can effectively perform\nobject searching by employing dynamic convolution at multiple scales. Besides,\na novel and effective data synthesis method is also proposed to train our\nnetwork. Experimental results on four benchmark datasets verify the\neffectiveness of our proposed method. Our code and saliency maps are available\nat \\url{https://github.com/nnizhang/CADC}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ni Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Spiking Variational Autoencoder. (arXiv:2110.00375v1 [cs.NE])","link":"http://arxiv.org/abs/2110.00375","description":"<p>Spiking neural networks (SNNs) can be run on neuromorphic devices with\nultra-high speed and ultra-low energy consumption because of their binary and\nevent-driven nature. Therefore, SNNs are expected to have various applications,\nincluding as generative models being running on edge devices to create\nhigh-quality images. In this study, we build a variational autoencoder (VAE)\nwith SNN to enable image generation. VAE is known for its stability among\ngenerative models; recently, its quality advanced. In vanilla VAE, the latent\nspace is represented as a normal distribution, and floating-point calculations\nare required in sampling. However, this is not possible in SNNs because all\nfeatures must be binary time series data. Therefore, we constructed the latent\nspace with an autoregressive SNN model, and randomly selected samples from its\noutput to sample the latent variables. This allows the latent variables to\nfollow the Bernoulli process and allows variational learning. Thus, we build\nthe Fully Spiking Variational Autoencoder where all modules are constructed\nwith SNN. To the best of our knowledge, we are the first to build a VAE only\nwith SNN layers. We experimented with several datasets, and confirmed that it\ncan generate images with the same or better quality compared to conventional\nANNs. The code will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamata_H/0/1/0/all/0/1\">Hiromichi Kamata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1\">Yusuke Mukuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN-based Reactive Motion Synthesis with Class-aware Discriminators for Human-human Interaction. (arXiv:2110.00380v1 [cs.GR])","link":"http://arxiv.org/abs/2110.00380","description":"<p>Creating realistic characters that can react to the users' or another\ncharacter's movement can benefit computer graphics, games and virtual reality\nhugely. However, synthesizing such reactive motions in human-human interactions\nis a challenging task due to the many different ways two humans can interact.\nWhile there are a number of successful researches in adapting the generative\nadversarial network (GAN) in synthesizing single human actions, there are very\nfew on modelling human-human interactions. In this paper, we propose a\nsemi-supervised GAN system that synthesizes the reactive motion of a character\ngiven the active motion from another character. Our key insights are two-fold.\nFirst, to effectively encode the complicated spatial-temporal information of a\nhuman motion, we empower the generator with a part-based long short-term memory\n(LSTM) module, such that the temporal movement of different limbs can be\neffectively modelled. We further include an attention module such that the\ntemporal significance of the interaction can be learned, which enhances the\ntemporal alignment of the active-reactive motion pair. Second, as the reactive\nmotion of different types of interactions can be significantly different, we\nintroduce a discriminator that not only tells if the generated movement is\nrealistic or not, but also tells the class label of the interaction. This\nallows the use of such labels in supervising the training of the generator. We\nexperiment with the SBU and the HHOI datasets. The high quality of the\nsynthetic motion demonstrates the effective design of our generator, and the\ndiscriminability of the synthesis also demonstrates the strength of our\ndiscriminator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1\">Qianhui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_E/0/1/0/all/0/1\">Edmond S. L. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leung_H/0/1/0/all/0/1\">Howard Leung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Retrogress-Resilient Framework for Real-World Medical Federated Learning. (arXiv:2110.00394v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00394","description":"<p>Nowadays, deep learning methods with large-scale datasets can produce\nclinically useful models for computer-aided diagnosis. However, the privacy and\nethical concerns are increasingly critical, which make it difficult to collect\nlarge quantities of data from multiple institutions. Federated Learning (FL)\nprovides a promising decentralized solution to train model collaboratively by\nexchanging client models instead of private data. However, the server\naggregation of existing FL methods is observed to degrade the model performance\nin real-world medical FL setting, which is termed as retrogress. To address\nthis problem, we propose a personalized retrogress-resilient framework to\nproduce a superior personalized model for each client. Specifically, we devise\na Progressive Fourier Aggregation (PFA) at the server to achieve more stable\nand effective global knowledge gathering by integrating client models from\nlow-frequency to high-frequency gradually. Moreover, with an introduced deputy\nmodel to receive the aggregated server model, we design a Deputy-Enhanced\nTransfer (DET) strategy at the client and conduct three steps of\nRecover-Exchange-Sublimate to ameliorate the personalized local model by\ntransferring the global knowledge smoothly. Extensive experiments on real-world\ndermoscopic FL dataset prove that our personalized retrogress-resilient\nframework outperforms state-of-the-art FL methods, as well as the\ngeneralization on an out-of-distribution cohort. The code and dataset are\navailable at https://github.com/CityU-AIM-Group/PRR-FL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Meilu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning of Inter-Label Geometric Relationships Using Self-Supervised Learning: Application To Gleason Grade Segmentation. (arXiv:2110.00404v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00404","description":"<p>Segmentation of Prostate Cancer (PCa) tissues from Gleason graded\nhistopathology images is vital for accurate diagnosis. Although deep learning\n(DL) based segmentation methods achieve state-of-the-art accuracy, they rely on\nlarge datasets with manual annotations. We propose a method to synthesize for\nPCa histopathology images by learning the geometrical relationship between\ndifferent disease labels using self-supervised learning. We use a weakly\nsupervised segmentation approach that uses Gleason score to segment the\ndiseased regions and the resulting segmentation map is used to train a Shape\nRestoration Network (ShaRe-Net) to predict missing mask segments in a\nself-supervised manner. Using DenseUNet as the backbone generator architecture\nwe incorporate latent variable sampling to inject diversity in the image\ngeneration process and thus improve robustness. Experiments on multiple\nhistopathology datasets demonstrate the superiority of our method over\ncompeting image synthesis methods for segmentation tasks. Ablation studies show\nthe benefits of integrating geometry and diversity in generating high-quality\nimages, and our self-supervised approach with limited class-labeled data\nachieves similar performance as fully supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Natural Language Video Localization. (arXiv:2110.00428v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00428","description":"<p>Understanding videos to localize moments with natural language often requires\nlarge expensive annotated video regions paired with language queries. To\neliminate the annotation costs, we make a first attempt to train a natural\nlanguage video localization model in zero-shot manner. Inspired by unsupervised\nimage captioning setup, we merely require random text corpora, unlabeled video\ncollections, and an off-the-shelf object detector to train a model. With the\nunpaired data, we propose to generate pseudo-supervision of candidate temporal\nregions and corresponding query sentences, and develop a simple NLVL model to\ntrain with the pseudo-supervision. Our empirical validations show that the\nproposed pseudo-supervised method outperforms several baseline approaches and a\nnumber of methods using stronger supervision on Charades-STA and\nActivityNet-Captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Jinwoo Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1\">Daechul Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Seong Jong Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Protecting Face Embeddings in Mobile Face Verification Scenarios. (arXiv:2110.00434v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00434","description":"<p>This paper proposes PolyProtect, a method for protecting the sensitive face\nembeddings that are used to represent people's faces in neural-network-based\nface verification systems. PolyProtect transforms a face embedding to a more\nsecure template, using a mapping based on multivariate polynomials\nparameterised by user-specific coefficients and exponents. In this work,\nPolyProtect is evaluated on two open-source face verification systems in a\nmobile application context, under the toughest threat model that assumes a\nfully-informed attacker with complete knowledge of the system and all its\nparameters. Results indicate that PolyProtect can be tuned to achieve a\nsatisfactory trade-off between the recognition accuracy of the PolyProtected\nface verification system and the irreversibility of the PolyProtected\ntemplates. Furthermore, PolyProtected templates are shown to be effectively\nunlinkable, especially if the user-specific parameters employed in the\nPolyProtect mapping are selected in a non-naive manner. The evaluation is\nconducted using practical methodologies with tangible results, to present\nrealistic insight into the method's robustness as a face embedding protection\nscheme in practice. The code to fully reproduce this work is available at:\nhttps://gitlab.idiap.ch/bob/bob.paper.polyprotect_2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_V/0/1/0/all/0/1\">Vedrana Krivoku&#x107;a Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1\">S&#xe9;bastien Marcel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoCInIS: Camera Independent Monocular 3D Object Detection using Instance Segmentation. (arXiv:2110.00464v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00464","description":"<p>Monocular 3D object detection has recently shown promising results, however\nthere remain challenging problems. One of those is the lack of invariance to\ndifferent camera intrinsic parameters, which can be observed across different\n3D object datasets. Little effort has been made to exploit the combination of\nheterogeneous 3D object datasets. In contrast to general intuition, we show\nthat more data does not automatically guarantee a better performance, but\nrather, methods need to have a degree of 'camera independence' in order to\nbenefit from large and heterogeneous training data. In this paper we propose a\ncategory-level pose estimation method based on instance segmentation, using\ncamera independent geometric reasoning to cope with the varying camera\nviewpoints and intrinsics of different datasets. Every pixel of an instance\npredicts the object dimensions, the 3D object reference points projected in 2D\nimage space and, optionally, the local viewing angle. Camera intrinsics are\nonly used outside of the learned network to lift the predicted 2D reference\npoints to 3D. We surpass camera independent methods on the challenging KITTI3D\nbenchmark and show the key benefits compared to camera dependent methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heylen_J/0/1/0/all/0/1\">Jonas Heylen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_M/0/1/0/all/0/1\">Mark De Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawagne_B/0/1/0/all/0/1\">Bruno Dawagne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proesmans_M/0/1/0/all/0/1\">Marc Proesmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeloos_W/0/1/0/all/0/1\">Wim Abbeloos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelkawy_H/0/1/0/all/0/1\">Hazem Abdelkawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reino_D/0/1/0/all/0/1\">Daniel Olmeda Reino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph-theoretic Algorithm for Small Bowel Path Tracking in CT Scans. (arXiv:2110.00466v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00466","description":"<p>We present a novel graph-theoretic method for small bowel path tracking. It\nis formulated as finding the minimum cost path between given start and end\nnodes on a graph that is constructed based on the bowel wall detection. We\nobserved that a trivial solution with many short-cuts is easily made even with\nthe wall detection, where the tracked path penetrates indistinct walls around\nthe contact between different parts of the small bowel. Thus, we propose to\ninclude must-pass nodes in finding the path to better cover the entire course\nof the small bowel. The proposed method does not entail training with\nground-truth paths while the previous methods do. We acquired ground-truth\npaths that are all connected from start to end of the small bowel for 10\nabdominal CT scans, which enables the evaluation of the path tracking for the\nentire course of the small bowel. The proposed method showed clear improvements\nin terms of several metrics compared to the baseline method. The maximum length\nof the path that is tracked without an error per scan, by the proposed method,\nis above 800mm on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shin_S/0/1/0/all/0/1\">Seung Yeon Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sungwon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Summers_R/0/1/0/all/0/1\">Ronald M. Summers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance Segmentation Challenge Track Technical Report, VIPriors Workshop at ICCV 2021: Task-Specific Copy-Paste Data Augmentation Method for Instance Segmentation. (arXiv:2110.00470v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00470","description":"<p>Copy-Paste has proven to be a very effective data augmentation for instance\nsegmentation which can improve the generalization of the model. We used a\ntask-specific Copy-Paste data augmentation method to achieve good performance\non the instance segmentation track of the 2nd VIPriors workshop challenge. We\nalso applied additional data augmentation techniques including RandAugment and\nGridMask. Our segmentation model is the HTC detector on the CBSwin-B with CBFPN\nwith some tweaks. This model was trained at the multi-scale mode by a random\nsampler on the 6x schedule and tested at the single-scale mode. By combining\nthese techniques, we achieved 0.398 AP@0.50:0.95 with the validation set and\n0.433 AP@0.50:0.95 with the test set. Finally, we reached 0.477 AP@0.50:0.95\nwith the test set by adding the validation set to the training data. Source\ncode is available at https://github.com/jahongir7174/VIP2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yunusov_J/0/1/0/all/0/1\">Jahongir Yunusov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhmatov_S/0/1/0/all/0/1\">Shohruh Rakhmatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namozov_A/0/1/0/all/0/1\">Abdulaziz Namozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaybulayev_A/0/1/0/all/0/1\">Abdulaziz Gaybulayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae-Hyong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey and synthesis of state of the art in driver monitoring. (arXiv:2110.00472v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00472","description":"<p>Road-vehicle accidents are mostly due to human errors, and many such\naccidents could be avoided by continuously monitoring the driver. Driver\nmonitoring (DM) is a topic of growing interest in the automotive industry, and\nit will remain relevant for all vehicles that are not fully autonomous, and\nthus for decades for the average vehicle owner. The present paper focuses on\nthe first step of DM, which consists in characterizing the state of the driver.\nSince DM will be increasingly linked to driving automation (DA), this paper\npresents a clear view of the role of DM at each of the six SAE levels of DA.\nThis paper surveys the state of the art of DM, and then synthesizes it,\nproviding a unique, structured, polychotomous view of the many characterization\ntechniques of DM. Informed by the survey, the paper characterizes the driver\nstate along the five main dimensions--called here \"(sub)states\"--of drowsiness,\nmental workload, distraction, emotions, and under the influence. The\npolychotomous view of DM is presented through a pair of interlocked tables that\nrelate these states to their indicators (e.g., the eye-blink rate) and the\nsensors that can access each of these indicators (e.g., a camera). The tables\nfactor in not only the effects linked directly to the driver, but also those\nlinked to the (driven) vehicle and the (driving) environment. They show, at a\nglance, to concerned researchers, equipment providers, and vehicle\nmanufacturers (1) most of the options they have to implement various forms of\nadvanced DM systems, and (2) fruitful areas for further research and\ninnovation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Halin_A/0/1/0/all/0/1\">Ana&#xef;s Halin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verly_J/0/1/0/all/0/1\">Jacques G. Verly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Score-Based Generative Classifiers. (arXiv:2110.00473v1 [stat.ML])","link":"http://arxiv.org/abs/2110.00473","description":"<p>The tremendous success of generative models in recent years raises the\nquestion whether they can also be used to perform classification. Generative\nmodels have been used as adversarially robust classifiers on simple datasets\nsuch as MNIST, but this robustness has not been observed on more complex\ndatasets like CIFAR-10. Additionally, on natural image datasets, previous\nresults have suggested a trade-off between the likelihood of the data and\nclassification accuracy. In this work, we investigate score-based generative\nmodels as classifiers for natural images. We show that these models not only\nobtain competitive likelihood values but simultaneously achieve\nstate-of-the-art classification accuracy for generative classifiers on\nCIFAR-10. Nevertheless, we find that these models are only slightly, if at all,\nmore robust than discriminative baseline models on out-of-distribution tasks\nbased on common image corruptions. Similarly and contrary to prior results, we\nfind that score-based are prone to worst-case distribution shifts in the form\nof adversarial perturbations. Our work highlights that score-based generative\nmodels are closing the gap in classification accuracy compared to standard\ndiscriminative models. While they do not yet deliver on the promise of\nadversarial and out-of-domain robustness, they provide a different approach to\nclassification that warrants further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roland S. Zimmermann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schott_L/0/1/0/all/0/1\">Lukas Schott</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dunn_B/0/1/0/all/0/1\">Benjamin A. Dunn</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Klindt_D/0/1/0/all/0/1\">David A. Klindt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResNet strikes back: An improved training procedure in timm. (arXiv:2110.00476v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00476","description":"<p>The influential Residual Networks designed by He et al. remain the\ngold-standard architecture in numerous scientific publications. They typically\nserve as the default architecture in studies, or as baselines when new\narchitectures are proposed. Yet there has been significant progress on best\npractices for training neural networks since the inception of the ResNet\narchitecture in 2015. Novel optimization &amp; data-augmentation have increased the\neffectiveness of the training recipes. In this paper, we re-evaluate the\nperformance of the vanilla ResNet-50 when trained with a procedure that\nintegrates such advances. We share competitive training settings and\npre-trained models in the timm open-source library, with the hope that they\nwill serve as better baselines for future work. For instance, with our more\ndemanding training setting, a vanilla ResNet-50 reaches 80.4% top-1 accuracy at\nresolution 224x224 on ImageNet-val without extra data or distillation. We also\nreport the performance achieved with popular models with our training\nprocedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wightman_R/0/1/0/all/0/1\">Ross Wightman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustly Removing Deep Sea Lighting Effects for Visual Mapping of Abyssal Plains. (arXiv:2110.00480v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00480","description":"<p>The majority of Earth's surface lies deep in the oceans, where no surface\nlight reaches. Robots diving down to great depths must bring light sources that\ncreate moving illumination patterns in the darkness, such that the same 3D\npoint appears with different color in each image. On top, scattering and\nattenuation of light in the water makes images appear foggy and typically\nblueish, the degradation depending on each pixel's distance to its observed\nseafloor patch, on the local composition of the water and the relative poses\nand cones of the light sources. Consequently, visual mapping, including image\nmatching and surface albedo estimation, severely suffers from the effects that\nco-moving light sources produce, and larger mosaic maps from photos are often\ndominated by lighting effects that obscure the actual seafloor structure. In\nthis contribution a practical approach to estimating and compensating these\nlighting effects on predominantly homogeneous, flat seafloor regions, as can be\nfound in the Abyssal plains of our oceans, is presented. The method is\nessentially parameter-free and intended as a preprocessing step to facilitate\nvisual mapping, but already produces convincing lighting artefact compensation\nup to a global white balance factor. It does not require to be trained\nbeforehand on huge sets of annotated images, which are not available for the\ndeep sea. Rather, we motivate our work by physical models of light propagation,\nperform robust statistics-based estimates of additive and multiplicative\nnuisances that avoid explicit parameters for light, camera, water or scene,\ndiscuss the breakdown point of the algorithms and show results on imagery\ncaptured by robots in several kilometer water depth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koser_K/0/1/0/all/0/1\">Kevin K&#xf6;ser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_L/0/1/0/all/0/1\">Lasse Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenzlaff_E/0/1/0/all/0/1\">Emanuel Wenzlaff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woelk_F/0/1/0/all/0/1\">Felix Woelk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preconditioned Plug-and-Play ADMM with Locally Adjustable Denoiser for Image Restoration. (arXiv:2110.00493v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00493","description":"<p>Plug-and-Play optimization recently emerged as a powerful technique for\nsolving inverse problems by plugging a denoiser into a classical optimization\nalgorithm. The denoiser accounts for the regularization and therefore\nimplicitly determines the prior knowledge on the data, hence replacing typical\nhandcrafted priors. In this paper, we extend the concept of plug-and-play\noptimization to use denoisers that can be parameterized for non-constant noise\nvariance. In that aim, we introduce a preconditioning of the ADMM algorithm,\nwhich mathematically justifies the use of such an adjustable denoiser. We\nadditionally propose a procedure for training a convolutional neural network\nfor high quality non-blind image denoising that also allows for pixel-wise\ncontrol of the noise standard deviation. We show that our pixel-wise adjustable\ndenoiser, along with a suitable preconditioning strategy, can further improve\nthe plug-and-play ADMM approach for several applications, including image\ncompletion, interpolation, demosaicing and Poisson denoising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pendu_M/0/1/0/all/0/1\">Mikael Le Pendu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guillemot_C/0/1/0/all/0/1\">Christine Guillemot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASH: A Modern Framework for Parallel Spatial Hashing in 3D Perception. (arXiv:2110.00511v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00511","description":"<p>We present ASH, a modern and high-performance framework for parallel spatial\nhashing on GPU. Compared to existing GPU hash map implementations, ASH achieves\nhigher performance, supports richer functionality, and requires fewer lines of\ncode (LoC) when used for implementing spatially varying operations from\nvolumetric geometry reconstruction to differentiable appearance reconstruction.\nUnlike existing GPU hash maps, the ASH framework provides a versatile tensor\ninterface, hiding low-level details from the users. In addition, by decoupling\nthe internal hashing data structures and key-value data in buffers, we offer\ndirect access to spatially varying data via indices, enabling seamless\nintegration to modern libraries such as PyTorch. To achieve this, we 1) detach\nstored key-value data from the low-level hash map implementation; 2) bridge the\npointer-first low level data structures to index-first high-level tensor\ninterfaces via an index heap; 3) adapt both generic and non-generic\ninteger-only hash map implementations as backends to operate on\nmulti-dimensional keys. We first profile our hash map against state-of-the-art\nhash maps on synthetic data to show the performance gain from this\narchitecture. We then show that ASH can consistently achieve higher performance\non various large-scale 3D perception tasks with fewer LoC by showcasing several\napplications, including 1) point cloud voxelization, 2) dense volumetric SLAM,\n3) non-rigid point cloud registration and volumetric deformation, and 4)\nspatially varying geometry and appearance refinement. ASH and its example\napplications are open sourced in Open3D (<a href=\"http://www.open3d.org\">this http URL</a>).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lao_Y/0/1/0/all/0/1\">Yixing Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaess_M/0/1/0/all/0/1\">Michael Kaess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optic Disc Segmentation using Disk-Centered Patch Augmentation. (arXiv:2110.00512v1 [eess.IV])","link":"http://arxiv.org/abs/2110.00512","description":"<p>The optic disc is a crucial diagnostic feature in the eye since changes to\nits physiognomy is correlated with the severity of various ocular and\ncardiovascular diseases. While identifying the bulk of the optic disc in a\ncolor fundus image is straightforward, accurately segmenting its boundary at\nthe pixel level is very challenging. In this work, we propose disc-centered\npatch augmentation (DCPA) -- a simple, yet novel training scheme for deep\nneural networks -- to address this problem. DCPA achieves state-of-the-art\nresults on full-size images even when using small neural networks, specifically\na U-Net with only 7 million parameters as opposed to the original 31 million.\nIn DCPA, we restrict the training data to patches that fully contain the optic\nnerve. In addition, we also train the network using dynamic cost functions to\nincrease its robustness. We tested DCPA-trained networks on five retinal\ndatasets: DRISTI, DRIONS-DB, DRIVE, AV-WIDE, and CHASE-DB. The first two had\navailable optic disc ground truth, and we manually estimated the ground truth\nfor the latter three. Our approach achieved state-of-the-art F1 and IOU results\non four datasets (95 % F1, 91 % IOU on DRISTI; 92 % F1, 84 % IOU on DRIVE; 83 %\nF1, 71 % IOU on AV-WIDE; 83 % F1, 71 % IOU on CHASEDB) and competitive results\non the fifth (95 % F1, 91 % IOU on DRIONS-DB), confirming its generality. Our\nopen-source code and ground-truth annotations are available at:\nhttps://github.com/saeidmotevali/fundusdisk\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Motevali_S/0/1/0/all/0/1\">Saeid Motevali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khanal_A/0/1/0/all/0/1\">Aashis Khanal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Estrada_R/0/1/0/all/0/1\">Rolando Estrada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images. (arXiv:2110.00519v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00519","description":"<p>While neural symbolic methods demonstrate impressive performance in visual\nquestion answering on synthetic images, their performance suffers on real\nimages. We identify that the long-tail distribution of visual concepts and\nunequal importance of reasoning steps in real data are the two key obstacles\nthat limit the models' real-world potentials. To address these challenges, we\npropose a new paradigm, Calibrating Concepts and Operations (CCO), which\nenables neural symbolic models to capture underlying data characteristics and\nto reason with hierarchical importance. Specifically, we introduce an executor\nwith learnable concept embedding magnitudes for handling distribution\nimbalance, and an operation calibrator for highlighting important operations\nand suppressing redundant ones. Our experiments show CCO substantially boosts\nthe performance of neural symbolic methods on real images. By evaluating models\non the real world dataset GQA, CCO helps the neural symbolic method NSCL\noutperforms its vanilla counterpart by 9.1% (from 47.0% to 56.1%); this result\nalso largely reduces the performance gap between symbolic and non-symbolic\nmethods. Additionally, we create a perturbed test set for better understanding\nand analyzing model performance on real images. Code is available at\nhttps://github.com/Lizw14/CaliCO.git .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask or Non-Mask? Robust Face Mask Detector via Triplet-Consistency Representation Learning. (arXiv:2110.00523v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00523","description":"<p>In the absence of vaccines or medicines to stop COVID-19, one of the\neffective methods to slow the spread of the coronavirus and reduce the\noverloading of healthcare is to wear a face mask. Nevertheless, to mandate the\nuse of face masks or coverings in public areas, additional human resources are\nrequired, which is tedious and attention-intensive. To automate the monitoring\nprocess, one of the promising solutions is to leverage existing object\ndetection models to detect the faces with or without masks. As such, security\nofficers do not have to stare at the monitoring devices or crowds, and only\nhave to deal with the alerts triggered by the detection of faces without masks.\nExisting object detection models usually focus on designing the CNN-based\nnetwork architectures for extracting discriminative features. However, the size\nof training datasets of face mask detection is small, while the difference\nbetween faces with and without masks is subtle. Therefore, in this paper, we\npropose a face mask detection framework that uses the context attention module\nto enable the effective attention of the feed-forward convolution neural\nnetwork by adapting their attention maps feature refinement. Moreover, we\nfurther propose an anchor-free detector with Triplet-Consistency Representation\nLearning by integrating the consistency loss and the triplet loss to deal with\nthe small-scale training data and the similarity between masks and occlusions.\nExtensive experimental results show that our method outperforms the other\nstate-of-the-art methods. The source code is released as a public download to\nimprove public health at https://github.com/wei-1006/MaskFaceDetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun-Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_T/0/1/0/all/0/1\">Thanh-Hai Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen-Huang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Explanations by Contrastive Learning. (arXiv:2110.00527v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00527","description":"<p>Understanding and explaining the decisions of neural networks are critical to\nbuilding trust, rather than relying on them as black box algorithms. Post-hoc\nevaluation techniques, such as Grad-CAM, enable humans to inspect the spatial\nregions responsible for a particular network decision. However, it is shown\nthat such explanations are not always consistent with human priors, such as\nconsistency across image transformations. Given an interpretation algorithm,\ne.g., Grad-CAM, we introduce a novel training method to train the model to\nproduce more consistent explanations. Since obtaining the ground truth for a\ndesired model interpretation is not a well-defined task, we adopt ideas from\ncontrastive self-supervised learning and apply them to the interpretations of\nthe model rather than its embeddings. Explicitly training the network to\nproduce more reasonable interpretations and subsequently evaluating those\ninterpretations will enhance our ability to trust the network. We show that our\nmethod, Contrastive Grad-CAM Consistency (CGC), results in Grad-CAM\ninterpretation heatmaps that are consistent with human annotations while still\nachieving comparable classification accuracy. Moreover, since our method can be\nseen as a form of regularizer, on limited-data fine-grained classification\nsettings, our method outperforms the baseline classification accuracy on\nCaltech-Birds, Stanford Cars, VGG Flowers, and FGVC-Aircraft datasets. In\naddition, because our method does not rely on annotations, it allows for the\nincorporation of unlabeled data into training, which enables better\ngeneralization of the model. Our code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pillai_V/0/1/0/all/0/1\">Vipin Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouligian_A/0/1/0/all/0/1\">Ashley Ouligian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_D/0/1/0/all/0/1\">Dennis Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Self-Supervised and Supervised Methods Learn Similar Visual Representations?. (arXiv:2110.00528v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00528","description":"<p>Despite the success of a number of recent techniques for visual\nself-supervised deep learning, there remains limited investigation into the\nrepresentations that are ultimately learned. By using recent advances in\ncomparing neural representations, we explore in this direction by comparing a\nconstrastive self-supervised algorithm (SimCLR) to supervision for simple image\ndata in a common architecture. We find that the methods learn similar\nintermediate representations through dissimilar means, and that the\nrepresentations diverge rapidly in the final few layers. We investigate this\ndivergence, finding that it is caused by these layers strongly fitting to the\ndistinct learning objectives. We also find that SimCLR's objective implicitly\nfits the supervised objective in intermediate layers, but that the reverse is\nnot true. Our work particularly highlights the importance of the learned\nintermediate representations, and raises important questions for auxiliary task\ndesign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grigg_T/0/1/0/all/0/1\">Tom George Grigg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busbridge_D/0/1/0/all/0/1\">Dan Busbridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramapuram_J/0/1/0/all/0/1\">Jason Ramapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webb_R/0/1/0/all/0/1\">Russ Webb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Motion Representation Learning with Capsule Autoencoders. (arXiv:2110.00529v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00529","description":"<p>We propose the Motion Capsule Autoencoder (MCAE), which addresses a key\nchallenge in the unsupervised learning of motion representations:\ntransformation invariance. MCAE models motion in a two-level hierarchy. In the\nlower level, a spatio-temporal motion signal is divided into short, local, and\nsemantic-agnostic snippets. In the higher level, the snippets are aggregated to\nform full-length semantic-aware segments. For both levels, we represent motion\nwith a set of learned transformation invariant templates and the corresponding\ngeometric transformations by using capsule autoencoders of a novel design. This\nleads to a robust and efficient encoding of viewpoint changes. MCAE is\nevaluated on a novel Trajectory20 motion dataset and various real-world\nskeleton-based human action datasets. Notably, it achieves better results than\nbaselines on Trajectory20 with considerably fewer parameters and\nstate-of-the-art performance on the unsupervised skeleton-based action\nrecognition task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xudong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan S Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00534","description":"<p>Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human--human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\",\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models' abilities in dialogue understanding, language\ngrounding, and task execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramithu_R/0/1/0/all/0/1\">Robinson Piramithu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Secondary Landmark Detection via 3D Representation Learning. (arXiv:2110.00543v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00543","description":"<p>Recent technological developments have spurred great advances in the\ncomputerized tracking of joints and other landmarks in moving animals,\nincluding humans. Such tracking promises important advances in biology and\nbiomedicine. Modern tracking models depend critically on labor-intensive\nannotated datasets of primary landmarks by non-expert humans. However, such\nannotation approaches can be costly and impractical for secondary landmarks,\nthat is, ones that reflect fine-grained geometry of animals, and that are often\nspecific to customized behavioral tasks. Due to visual and geometric ambiguity,\nnonexperts are often not qualified for secondary landmark annotation, which can\nrequire anatomical and zoological knowledge. These barriers significantly\nimpede downstream behavioral studies because the learned tracking models\nexhibit limited generalizability. We hypothesize that there exists a shared\nrepresentation between the primary and secondary landmarks because the range of\nmotion of the secondary landmarks can be approximately spanned by that of the\nprimary landmarks. We present a method to learn this spatial relationship of\nthe primary and secondary landmarks in three dimensional space, which can, in\nturn, self-supervise the secondary landmark detector. This 3D representation\nlearning is generic, and can therefore be applied to various multiview settings\nacross diverse organisms, including macaques, flies, and humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bala_P/0/1/0/all/0/1\">Praneet C. Bala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_J/0/1/0/all/0/1\">Jan Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyun Soo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayden_B/0/1/0/all/0/1\">Benjamin Y. Hayden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Decomposition, Disentanglement and Prediction of Video Sequences while Interpreting Dynamics: A Koopman Perspective. (arXiv:2110.00547v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00547","description":"<p>Human interpretation of the world encompasses the use of symbols to\ncategorize sensory inputs and compose them in a hierarchical manner. One of the\nlong-term objectives of Computer Vision and Artificial Intelligence is to endow\nmachines with the capacity of structuring and interpreting the world as we do.\nTowards this goal, recent methods have successfully been able to decompose and\ndisentangle video sequences into their composing objects and dynamics, in a\nself-supervised fashion. However, there has been a scarce effort in giving\ninterpretation to the dynamics of the scene. We propose a method to decompose a\nvideo into moving objects and their attributes, and model each object's\ndynamics with linear system identification tools, by means of a Koopman\nembedding. This allows interpretation, manipulation and extrapolation of the\ndynamics of the different objects by employing the Koopman operator K. We test\nour method in various synthetic datasets and successfully forecast challenging\ntrajectories while interpreting them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Comas_A/0/1/0/all/0/1\">Armand Comas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghimire_S/0/1/0/all/0/1\">Sandesh Ghimire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznaier_M/0/1/0/all/0/1\">Mario Sznaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camps_O/0/1/0/all/0/1\">Octavia Camps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Temporal Relationship Mining for Data-Efficient Person Re-identification. (arXiv:2110.00549v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00549","description":"<p>This paper is a technical report to our submission to the ICCV 2021 VIPriors\nRe-identification Challenge. In order to make full use of the visual inductive\npriors of the data, we treat the query and gallery images of the same identity\nas continuous frames in a video sequence. And we propose one novel\npost-processing strategy for video temporal relationship mining, which not only\ncalculates the distance matrix between query and gallery images, but also the\nmatrix between gallery images. The initial query image is used to retrieve the\nmost similar image from the gallery, then the retrieved image is treated as a\nnew query to retrieve its most similar image from the gallery. By iteratively\nsearching for the closest image, we can achieve accurate image retrieval and\nfinally obtain a robust retrieval sequence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dengjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lishuai Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Fan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neurally-Inspired Hierarchical Prediction Network for Spatiotemporal Sequence Learning and Prediction. (arXiv:1901.09002v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/1901.09002","description":"<p>In this paper we developed a hierarchical network model, called Hierarchical\nPrediction Network (HPNet), to understand how spatiotemporal memories might be\nlearned and encoded in the recurrent circuits in the visual cortical hierarchy\nfor predicting future video frames. This neurally inspired model operates in\nthe analysis-by-synthesis framework. It contains a feed-forward path that\ncomputes and encodes spatiotemporal features of successive complexity and a\nfeedback path for the successive levels to project their interpretations to the\nlevel below. Within each level, the feed-forward path and the feedback path\nintersect in a recurrent gated circuit, instantiated in a LSTM module, to\ngenerate a prediction or explanation of the incoming signals. The network\nlearns its internal model of the world by minimizing the errors of its\nprediction of the incoming signals at each level of the hierarchy. We found\nthat hierarchical interaction in the network increases semantic clustering of\nglobal movement patterns in the population codes of the units along the\nhierarchy, even in the earliest module. This facilitates the learning of\nrelationships among movement patterns, yielding state-of-the-art performance in\nlong range video sequence predictions in the benchmark datasets. The network\nmodel automatically reproduces a variety of prediction suppression and\nfamiliarity suppression neurophysiological phenomena observed in the visual\ncortex, suggesting that hierarchical prediction might indeed be an important\nprinciple for representational learning in the visual cortex.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jielin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Ge Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tai Sing Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distance Metric Learned Collaborative Representation Classifier. (arXiv:1905.01168v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1905.01168","description":"<p>Any generic deep machine learning algorithm is essentially a function fitting\nexercise, where the network tunes its weights and parameters to learn\ndiscriminatory features by minimizing some cost function. Though the network\ntries to learn the optimal feature space, it seldom tries to learn an optimal\ndistance metric in the cost function, and hence misses out on an additional\nlayer of abstraction. We present a simple effective way of achieving this by\nlearning a generic Mahalanabis distance in a collaborative loss function in an\nend-to-end fashion with any standard convolutional network as the feature\nlearner. The proposed method DML-CRC gives state-of-the-art performance on\nbenchmark fine-grained classification datasets CUB Birds, Oxford Flowers and\nOxford-IIIT Pets using the VGG-19 deep network. The method is network agnostic\nand can be used for any similar classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1\">Tapabrata Chakraborti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCane_B/0/1/0/all/0/1\">Brendan McCane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_S/0/1/0/all/0/1\">Steven Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Compositional Augmentations for Scene Graph Prediction. (arXiv:2007.05756v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.05756","description":"<p>Inferring objects and their relationships from an image in the form of a\nscene graph is useful in many applications at the intersection of vision and\nlanguage. We consider a challenging problem of compositional generalization\nthat emerges in this task due to a long tail data distribution. Current scene\ngraph generation models are trained on a tiny fraction of the distribution\ncorresponding to the most frequent compositions, e.g. &lt;cup, on, table&gt;.\nHowever, test images might contain zero- and few-shot compositions of objects\nand relationships, e.g. &lt;cup, on, surfboard&gt;. Despite each of the object\ncategories and the predicate (e.g. 'on') being frequent in the training data,\nthe models often fail to properly understand such unseen or rare compositions.\nTo improve generalization, it is natural to attempt increasing the diversity of\nthe training distribution. However, in the graph domain this is non-trivial. To\nthat end, we propose a method to synthesize rare yet plausible scene graphs by\nperturbing real ones. We then propose and empirically study a model based on\nconditional generative adversarial networks (GANs) that allows us to generate\nvisual features of perturbed scene graphs and learn from them in a joint\nfashion. When evaluated on the Visual Genome dataset, our approach yields\nmarginal, but consistent improvements in zero- and few-shot metrics. We analyze\nthe limitations of our approach indicating promising directions for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knyazev_B/0/1/0/all/0/1\">Boris Knyazev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangea_C/0/1/0/all/0/1\">C&#x103;t&#x103;lina Cangea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1\">Graham W. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layered Neural Rendering for Retiming People in Video. (arXiv:2009.07833v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.07833","description":"<p>We present a method for retiming people in an ordinary, natural video --\nmanipulating and editing the time in which different motions of individuals in\nthe video occur. We can temporally align different motions, change the speed of\ncertain actions (speeding up/slowing down, or entirely \"freezing\" people), or\n\"erase\" selected people from the video altogether. We achieve these effects\ncomputationally via a dedicated learning-based layered video representation,\nwhere each frame in the video is decomposed into separate RGBA layers,\nrepresenting the appearance of different people in the video. A key property of\nour model is that it not only disentangles the direct motions of each person in\nthe input video, but also correlates each person automatically with the scene\nchanges they generate -- e.g., shadows, reflections, and motion of loose\nclothing. The layers can be individually retimed and recombined into a new\nvideo, allowing us to achieve realistic, high-quality renderings of retiming\neffects for real-world videos depicting complex actions and involving multiple\nindividuals, including dancing, trampoline jumping, or group running.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_E/0/1/0/all/0/1\">Erika Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1\">Forrester Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesin_D/0/1/0/all/0/1\">David Salesin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1\">Michael Rubinstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building 3D Morphable Models from a Single Scan. (arXiv:2011.12440v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12440","description":"<p>We propose a method for constructing generative models of 3D objects from a\nsingle 3D mesh. Our method produces a 3D morphable model that represents shape\nand albedo in terms of Gaussian processes. We define the shape deformations in\nphysical (3D) space and the albedo deformations as a combination of\nphysical-space and color-space deformations. Whereas previous approaches have\ntypically built 3D morphable models from multiple high-quality 3D scans through\nprincipal component analysis, we build 3D morphable models from a single scan\nor template. As we demonstrate in the face domain, these models can be used to\ninfer 3D reconstructions from 2D data (inverse graphics) or 3D data\n(registration). Specifically, we show that our approach can be used to perform\nface recognition using only a single 3D scan (one scan total, not one per\nperson), and further demonstrate how multiple scans can be incorporated to\nimprove performance without requiring dense correspondence. Our approach\nenables the synthesis of 3D morphable models for 3D object categories where\ndense correspondence between multiple scans is unavailable. We demonstrate this\nby constructing additional 3D morphable models for fish and birds and use them\nto perform simple inverse rendering tasks. We share the code used to generate\nthese models and to perform our inverse rendering and registration experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sutherland_S/0/1/0/all/0/1\">Skylar Sutherland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images. (arXiv:2103.05745v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.05745","description":"<p>Interactive simulation of ultrasound imaging greatly facilitates sonography\ntraining. Although ray-tracing based methods have shown promising results,\nobtaining realistic images requires substantial modeling effort and manual\nparameter tuning. In addition, current techniques still result in a significant\nappearance gap between simulated images and real clinical scans. Herein we\nintroduce a novel content-preserving image translation framework (ConPres) to\nbridge this appearance gap, while maintaining the simulated anatomical layout.\nWe achieve this goal by leveraging both simulated images with semantic\nsegmentations and unpaired in-vivo ultrasound scans. Our framework is based on\nrecent contrastive unpaired translation techniques and we propose a\nregularization approach by learning an auxiliary segmentation-to-real image\ntranslation task, which encourages the disentanglement of content and style. In\naddition, we extend the generator to be class-conditional, which enables the\nincorporation of additional losses, in particular a cyclic consistency loss, to\nfurther improve the translation quality. Qualitative and quantitative\ncomparisons against state-of-the-art unpaired translation methods demonstrate\nthe superiority of our proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tomar_D/0/1/0/all/0/1\">Devavrat Tomar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Portenier_T/0/1/0/all/0/1\">Tiziano Portenier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collapsible Linear Blocks for Super-Efficient Super Resolution. (arXiv:2103.09404v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.09404","description":"<p>With the advent of smart devices that support 4K and 8K resolution, Single\nImage Super Resolution (SISR) has become an important computer vision problem.\nHowever, most super resolution deep networks are computationally very\nexpensive. In this paper, we propose SESR, a new class of Super-Efficient Super\nResolution networks that significantly improve image quality and reduce\ncomputational complexity. Detailed experiments across six benchmark datasets\ndemonstrate that SESR achieves similar or better image quality than\nstate-of-the-art models while requiring 2x to 330x fewer Multiply-Accumulate\n(MAC) operations. As a result, SESR can be used on constrained hardware to\nperform x2 (1080p to 4K) and x4 SISR (1080p to 8K). Towards this, we simulate\nhardware performance numbers for a commercial mobile Neural Processing Unit\n(NPU) for 1080p to 4K (x2) and 1080p to 8K (x4) SISR. Our results highlight the\nchallenges faced by super resolution on AI accelerators and demonstrate that\nSESR is significantly faster than existing models. Overall, SESR establishes a\nnew Pareto frontier on the quality (PSNR)-computation relationship for the\nsuper resolution task. The code for this work is available at\nhttps://github.com/ARM-software/sesr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhardwaj_K/0/1/0/all/0/1\">Kartikeya Bhardwaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milosavljevic_M/0/1/0/all/0/1\">Milos Milosavljevic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chalfin_A/0/1/0/all/0/1\">Alex Chalfin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suda_N/0/1/0/all/0/1\">Naveen Suda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeil_L/0/1/0/all/0/1\">Liam O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gope_D/0/1/0/all/0/1\">Dibakar Gope</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_L/0/1/0/all/0/1\">Lingchuan Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matas_R/0/1/0/all/0/1\">Ramon Matas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loh_D/0/1/0/all/0/1\">Danny Loh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Track with Object Permanence. (arXiv:2103.14258v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14258","description":"<p>Tracking by detection, the dominant approach for online multi-object\ntracking, alternates between localization and association steps. As a result,\nit strongly depends on the quality of instantaneous observations, often failing\nwhen objects are not fully visible. In contrast, tracking in humans is\nunderlined by the notion of object permanence: once an object is recognized, we\nare aware of its physical existence and can approximately localize it even\nunder full occlusions. In this work, we introduce an end-to-end trainable\napproach for joint object detection and tracking that is capable of such\nreasoning. We build on top of the recent CenterTrack architecture, which takes\npairs of frames as input, and extend it to videos of arbitrary length. To this\nend, we augment the model with a spatio-temporal, recurrent memory module,\nallowing it to reason about object locations and identities in the current\nframe using all the previous history. It is, however, not obvious how to train\nsuch an approach. We study this question on a new, large-scale, synthetic\ndataset for multi-object tracking, which provides ground truth annotations for\ninvisible objects, and propose several approaches for supervising tracking\nbehind occlusions. Our model, trained jointly on synthetic and real data,\noutperforms the state of the art on KITTI and MOT17 datasets thanks to its\nrobustness to occlusions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1\">Pavel Tokmakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Vibration Tomography: Estimating Interior Material Properties from Monocular Video. (arXiv:2104.02735v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02735","description":"<p>An object's interior material properties, while invisible to the human eye,\ndetermine motion observed on its surface. We propose an approach that estimates\nheterogeneous material properties of an object directly from a monocular video\nof its surface vibrations. Specifically, we estimate Young's modulus and\ndensity throughout a 3D object with known geometry. Knowledge of how these\nvalues change across the object is useful for characterizing defects and\nsimulating how the object will interact with different environments.\nTraditional non-destructive testing approaches, which generally estimate\nhomogenized material properties or the presence of defects, are expensive and\nuse specialized instruments. We propose an approach that leverages monocular\nvideo to (1) measure and object's sub-pixel motion and decompose this motion\ninto image-space modes, and (2) directly infer spatially-varying Young's\nmodulus and density values from the observed image-space modes. On both\nsimulated and real videos, we demonstrate that our approach is able to image\nmaterial properties simply by analyzing surface motion. In particular, our\nmethod allows us to identify unseen defects on a 2D drum head from real,\nhigh-speed video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Berthy T. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogren_A/0/1/0/all/0/1\">Alexander C. Ogren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daraio_C/0/1/0/all/0/1\">Chiara Daraio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouman_K/0/1/0/all/0/1\">Katherine L. Bouman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X2CT-FLOW: Maximum a posteriori reconstruction using a progressive flow-based deep generative model for ultra sparse-view computed tomography in ultra low-dose protocols. (arXiv:2104.04179v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.04179","description":"<p>Ultra sparse-view computed tomography (CT) algorithms can reduce radiation\nexposure of patients, but those algorithms lack an explicit cycle consistency\nloss minimization and an explicit log-likelihood maximization in testing. Here,\nwe propose X2CT-FLOW for the maximum a posteriori (MAP) reconstruction of a\nthree-dimensional (3D) chest CT image from a single or a few two-dimensional\n(2D) projection images using a progressive flow-based deep generative model,\nespecially for ultra low-dose protocols. The MAP reconstruction can\nsimultaneously optimize the cycle consistency loss and the log-likelihood. The\nproposed algorithm is built upon a newly developed progressive flow-based deep\ngenerative model, which is featured with exact log-likelihood estimation,\nefficient sampling, and progressive learning. We applied X2CT-FLOW to\nreconstruction of 3D chest CT images from biplanar projection images without\nnoise contamination (assuming a standard-dose protocol) and with strong noise\ncontamination (assuming an ultra low-dose protocol). With the standard-dose\nprotocol, our images reconstructed from 2D projected images and 3D ground-truth\nCT images showed good agreement in terms of structural similarity (SSIM, 0.7675\non average), peak signal-to-noise ratio (PSNR, 25.89 dB on average), mean\nabsolute error (MAE, 0.02364 on average), and normalized root mean square error\n(NRMSE, 0.05731 on average). Moreover, with the ultra low-dose protocol, our\nimages reconstructed from 2D projected images and the 3D ground-truth CT images\nalso showed good agreement in terms of SSIM (0.7008 on average), PSNR (23.58 dB\non average), MAE (0.02991 on average), and NRMSE (0.07349 on average).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shibata_H/0/1/0/all/0/1\">Hisaichi Shibata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hanaoka_S/0/1/0/all/0/1\">Shouhei Hanaoka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nomura_Y/0/1/0/all/0/1\">Yukihiro Nomura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nakao_T/0/1/0/all/0/1\">Takahiro Nakao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takenaga_T/0/1/0/all/0/1\">Tomomi Takenaga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_N/0/1/0/all/0/1\">Naoto Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abe_O/0/1/0/all/0/1\">Osamu Abe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Human and Automated Identification of Wildlife Images. (arXiv:2105.02320v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02320","description":"<p>Camera trapping is increasingly used to monitor wildlife, but this technology\ntypically requires extensive data annotation. Recently, deep learning has\nsignificantly advanced automatic wildlife recognition. However, current methods\nare hampered by a dependence on large static data sets when wildlife data is\nintrinsically dynamic and involves long-tailed distributions. These two\ndrawbacks can be overcome through a hybrid combination of machine learning and\nhumans in the loop. Our proposed iterative human and automated identification\napproach is capable of learning from wildlife imagery data with a long-tailed\ndistribution. Additionally, it includes self-updating learning that facilitates\ncapturing the community dynamics of rapidly changing natural systems. Extensive\nexperiments show that our approach can achieve a ~90% accuracy employing only\n~20% of the human annotations of existing approaches. Our synergistic\ncollaboration of humans and machines transforms deep learning from a relatively\ninefficient post-annotation tool to a collaborative on-going annotation tool\nthat vastly relieves the burden of human annotation and enables efficient and\nconstant model updates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhongqi Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaynor_K/0/1/0/all/0/1\">Kaitlyn M. Gaynor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1\">Meredith S. Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getz_W/0/1/0/all/0/1\">Wayne M. Getz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of Anatomical Layers and Artifacts in Intravascular Polarization Sensitive Optical Coherence Tomography Using Attending Physician and Boundary Cardinality Losses. (arXiv:2105.05137v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.05137","description":"<p>Intravascular ultrasound and optical coherence tomography are widely\navailable for characterizing coronary stenoses and provide critical vessel\nparameters to optimize percutaneous intervention. Intravascular\npolarization-sensitive optical coherence tomography (PS-OCT) simultaneously\nprovides high-resolution cross-sectional images of vascular structures while\nalso revealing preponderant tissue components such as collagen and smooth\nmuscle and thereby enhances plaque characterization. Automated interpretation\nof these features promises to facilitate the objective clinical investigation\nof the natural history and significance of coronary atheromas. Here, we propose\na convolutional neural network model, optimized using a new multi-term loss\nfunction, to classify the lumen, intima, and media layers in addition to the\nguidewire and plaque shadows. We demonstrate that our multi-class\nclassification model outperforms state-of-the-art methods in detecting the\ncoronary anatomical layers. Furthermore, the proposed model segments two\nclasses of common imaging artifacts and detects the anatomical layers within\nthe thickened vessel wall regions that were excluded from analysis by other\nstudies. The source code and the trained model are publicly available at\nhttps://github.com/mhaft/OCTseg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Haft_Javaherian_M/0/1/0/all/0/1\">Mohammad Haft-Javaherian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Villiger_M/0/1/0/all/0/1\">Martin Villiger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otsuka_K/0/1/0/all/0/1\">Kenichiro Otsuka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daemen_J/0/1/0/all/0/1\">Joost Daemen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Libby_P/0/1/0/all/0/1\">Peter Libby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bouma_B/0/1/0/all/0/1\">Brett E. Bouma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omnimatte: Associating Objects and Their Effects in Video. (arXiv:2105.06993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06993","description":"<p>Computer vision is increasingly effective at segmenting objects in images and\nvideos; however, scene effects related to the objects -- shadows, reflections,\ngenerated smoke, etc -- are typically overlooked. Identifying such scene\neffects and associating them with the objects producing them is important for\nimproving our fundamental understanding of visual scenes, and can also assist a\nvariety of applications such as removing, duplicating, or enhancing objects in\nvideo. In this work, we take a step towards solving this novel problem of\nautomatically associating objects with their effects in video. Given an\nordinary video and a rough segmentation mask over time of one or more subjects\nof interest, we estimate an omnimatte for each subject -- an alpha matte and\ncolor image that includes the subject along with all its related time-varying\nscene elements. Our model is trained only on the input video in a\nself-supervised manner, without any manual labels, and is generic -- it\nproduces omnimattes automatically for arbitrary objects and a variety of\neffects. We show results on real-world videos containing interactions between\ndifferent types of subjects (cars, animals, people) and complex effects,\nranging from semi-transparent elements such as smoke and reflections, to fully\nopaque effects such as objects attached to the subject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_E/0/1/0/all/0/1\">Erika Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1\">Forrester Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1\">Michael Rubinstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09996","description":"<p>We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training. Code is\nmade available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1\">Prahal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1\">Masoumeh Aminzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Deep Neural Network Calibration by Regularization and its Impact on Refinement. (arXiv:2106.09385v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09385","description":"<p>Deep neural networks have been shown to be highly miscalibrated. often they\ntend to be overconfident in their predictions. It poses a significant challenge\nfor safety-critical systems to utilise deep neural networks (DNNs), reliably.\nMany recently proposed approaches to mitigate this have demonstrated\nsubstantial progress in improving DNN calibration. However, they hardly touch\nupon refinement, which historically has been an essential aspect of\ncalibration. Refinement indicates separability of a network's correct and\nincorrect predictions. This paper presents a theoretically and empirically\nsupported exposition reviewing refinement of a calibrated model. Firstly, we\nshow the breakdown of expected calibration error (ECE), into predicted\nconfidence and refinement under the assumption of over-confident predictions.\nSecondly, linking with this result, we highlight that regularization based\ncalibration only focuses on naively reducing a model's confidence. This\nlogically has a severe downside to a model's refinement as correct and\nincorrect predictions become tightly coupled. Lastly, connecting refinement\nwith ECE also provides support to existing refinement based approaches which\nimprove calibration but do not explain the reasoning behind it. We support our\nclaims through rigorous empirical evaluations of many state of the art\ncalibration approaches on widely used datasets and neural networks. We find\nthat many calibration approaches with the likes of label smoothing, mixup etc.\nlower the usefulness of a DNN by degrading its refinement. Even under natural\ndata shift, this calibration-refinement trade-off holds for the majority of\ncalibration methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Aditya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bay_A/0/1/0/all/0/1\">Alessandro Bay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_B/0/1/0/all/0/1\">Biswa Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirabile_A/0/1/0/all/0/1\">Andrea Mirabile</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Stream Reciprocal Disentanglement Learning for Domain Adaptation Person Re-Identification. (arXiv:2106.13929v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13929","description":"<p>Since human-labeled samples are free for the target set, unsupervised person\nre-identification (Re-ID) has attracted much attention in recent years, by\nadditionally exploiting the source set. However, due to the differences on\ncamera styles, illumination and backgrounds, there exists a large gap between\nsource domain and target domain, introducing a great challenge on cross-domain\nmatching. To tackle this problem, in this paper we propose a novel method named\nDual-stream Reciprocal Disentanglement Learning (DRDL), which is quite\nefficient in learning domain-invariant features. In DRDL, two encoders are\nfirst constructed for id-related and id-unrelated feature extractions, which\nare respectively measured by their associated classifiers. Furthermore,\nfollowed by an adversarial learning strategy, both streams reciprocally and\npositively effect each other, so that the id-related features and id-unrelated\nfeatures are completely disentangled from a given image, allowing the encoder\nto be powerful enough to obtain the discriminative but domain-invariant\nfeatures. In contrast to existing approaches, our proposed method is free from\nimage generation, which not only reduces the computational complexity\nremarkably, but also removes redundant information from id-related features.\nExtensive experiments substantiate the superiority of our proposed method\ncompared with the state-of-the-arts. The source code has been released in\nhttps://github.com/lhf12278/DRDL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huafeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaixiong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengtao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02239","description":"<p>Although transformers have become the neural architectures of choice for\nnatural language processing, they require orders of magnitude more training\ndata, GPU memory, and computations in order to compete with convolutional\nneural networks for computer vision. The attention mechanism of transformers\nscales quadratically with the length of the input sequence, and unrolled images\nhave long sequence lengths. Plus, transformers lack an inductive bias that is\nappropriate for images. We tested three modifications to vision transformer\n(ViT) architectures that address these shortcomings. Firstly, we alleviate the\nquadratic bottleneck by using linear attention mechanisms, called X-formers\n(such that, X in {Performer, Linformer, Nystr\\\"omformer}), thereby creating\nVision X-formers (ViXs). This resulted in up to a seven times reduction in the\nGPU memory requirement. We also compared their performance with FNet and\nmulti-layer perceptron mixers, which further reduced the GPU memory\nrequirement. Secondly, we introduced an inductive bias for images by replacing\nthe initial linear embedding layer by convolutional layers in ViX, which\nsignificantly increased classification accuracy without increasing the model\nsize. Thirdly, we replaced the learnable 1D position embeddings in ViT with\nRotary Position Embedding (RoPE), which increases the classification accuracy\nfor the same model size. We believe that incorporating such changes can\ndemocratize transformers by making them accessible to those with limited data\nand computing resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1\">Pranav Jeevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1\">Amit Sethi</a> (Indian Institute of Technology Bombay)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Image Synthesis from Intuitive User Input: A Review and Perspectives. (arXiv:2107.04240v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04240","description":"<p>In many applications of computer graphics, art and design, it is desirable\nfor a user to provide intuitive non-image input, such as text, sketch, stroke,\ngraph or layout, and have a computer system automatically generate\nphoto-realistic images that adhere to the input content. While classic works\nthat allow such automatic image content generation have followed a framework of\nimage retrieval and composition, recent advances in deep generative models such\nas generative adversarial networks (GANs), variational autoencoders (VAEs), and\nflow-based methods have enabled more powerful and versatile image generation\ntasks. This paper reviews recent works for image synthesis given intuitive user\ninput, covering advances in input versatility, image generation methodology,\nbenchmark datasets, and evaluation metrics. This motivates new perspectives on\ninput representation and interactivity, cross pollination between major image\ngeneration paradigms, and evaluation and comparison of generation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuan-Chen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable, Axiomatic Explanations of Deep Alzheimer's Diagnosis from Heterogeneous Data. (arXiv:2107.05997v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05997","description":"<p>Deep Neural Networks (DNNs) have an enormous potential to learn from complex\nbiomedical data. In particular, DNNs have been used to seamlessly fuse\nheterogeneous information from neuroanatomy, genetics, biomarkers, and\nneuropsychological tests for highly accurate Alzheimer's disease diagnosis. On\nthe other hand, their black-box nature is still a barrier for the adoption of\nsuch a system in the clinic, where interpretability is absolutely essential. We\npropose Shapley Value Explanation of Heterogeneous Neural Networks (SVEHNN) for\nexplaining the Alzheimer's diagnosis made by a DNN from the 3D point cloud of\nthe neuroanatomy and tabular biomarkers. Our explanations are based on the\nShapley value, which is the unique method that satisfies all fundamental axioms\nfor local explanations previously established in the literature. Thus, SVEHNN\nhas many desirable characteristics that previous work on interpretability for\nmedical decision making is lacking. To avoid the exponential time complexity of\nthe Shapley value, we propose to transform a given DNN into a Lightweight\nProbabilistic Deep Network without re-training, thus achieving a complexity\nonly quadratic in the number of features. In our experiments on synthetic and\nreal data, we show that we can closely approximate the exact Shapley value with\na dramatically reduced runtime and can reveal the hidden knowledge the network\nhas learned from the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigner_C/0/1/0/all/0/1\">Christina Aigner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Neural Human Performance Rendering from Sparse RGBD Videos. (arXiv:2107.06505v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06505","description":"<p>Recent neural rendering approaches for human activities achieve remarkable\nview synthesis results, but still rely on dense input views or dense training\nwith all the capture frames, leading to deployment difficulty and inefficient\ntraining overload. However, existing advances will be ill-posed if the input is\nboth spatially and temporally sparse. To fill this gap, in this paper we\npropose a few-shot neural human rendering approach (FNHR) from only sparse RGBD\ninputs, which exploits the temporal and spatial redundancy to generate\nphoto-realistic free-view output of human activities. Our FNHR is trained only\non the key-frames which expand the motion manifold in the input sequences. We\nintroduce a two-branch neural blending to combine the neural point render and\nclassical graphics texturing pipeline, which integrates reliable observations\nover sparse key-frames. Furthermore, we adopt a patch-based adversarial\ntraining process to make use of the local redundancy and avoids over-fitting to\nthe key-frames, which generates fine-detailed rendering results. Extensive\nexperiments demonstrate the effectiveness of our approach to generate\nhigh-quality free view-point results for challenging human performances under\nthe sparse setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haimin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00394","description":"<p>Graph matching is an important problem that has received widespread\nattention, especially in the field of computer vision. Recently,\nstate-of-the-art methods seek to incorporate graph matching with deep learning.\nHowever, there is no research to explain what role the graph matching algorithm\nplays in the model. Therefore, we propose an approach integrating a MILP\nformulation of the graph matching problem. This formulation is solved to\noptimal and it provides inherent baseline. Meanwhile, similar approaches are\nderived by releasing the optimal guarantee of the graph matching solver and by\nintroducing a quality level. This quality level controls the quality of the\nsolutions provided by the graph matching solver. In addition, several\nrelaxations of the graph matching problem are put to the test. Our experimental\nevaluation gives several theoretical insights and guides the direction of deep\ngraph matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Puqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1\">Romain Raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02093","description":"<p>We unveil a long-standing problem in the prevailing co-saliency detection\nsystems: there is indeed inconsistency between training and testing.\nConstructing a high-quality co-saliency detection dataset involves\ntime-consuming and labor-intensive pixel-level labeling, which has forced most\nrecent works to rely instead on semantic segmentation or saliency detection\ndatasets for training. However, the lack of proper co-saliency and the absence\nof multiple foreground objects in these datasets can lead to spurious\nvariations and inherent biases learned by models. To tackle this, we introduce\nthe idea of counterfactual training through context adjustment and propose a\n\"cost-free\" group-cut-paste (GCP) procedure to leverage off-the-shelf images\nand synthesize new samples. Following GCP, we collect a novel dataset called\nContext Adjustment Training (CAT). CAT consists of 33,500 images, which is four\ntimes larger than the current co-saliency detection datasets. All samples are\nautomatically annotated with high-quality mask annotations, object categories,\nand edge maps. Extensive experiments on recent benchmarks are conducted, show\nthat CAT can improve various state-of-the-art models by a large margin (5% ~\n25%). We hope that the scale, diversity, and quality of our dataset can benefit\nresearchers in this area and beyond. Our dataset will be publicly accessible\nthrough our project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingdong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1\">Prakhar Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEDIC: A Multi-Task Learning Dataset for Disaster Image Classification. (arXiv:2108.12828v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12828","description":"<p>Recent research in disaster informatics demonstrates a practical and\nimportant use case of artificial intelligence to save human lives and\nsufferings during post-natural disasters based on social media contents (text\nand images). While notable progress has been made using texts, research on\nexploiting the images remains relatively under-explored. To advance the\nimage-based approach, we propose MEDIC (available at:\nhttps://crisisnlp.qcri.org/medic/index.html), which is the largest social media\nimage classification dataset for humanitarian response consisting of 71,198\nimages to address four different tasks in a multi-task learning setup. This is\nthe first dataset of its kind: social media image, disaster response, and\nmulti-task learning research. An important property of this dataset is its high\npotential to contribute research on multi-task learning, which recently\nreceives much interest from the machine learning community and has shown\nremarkable results in terms of memory, inference speed, performance, and\ngeneralization capability. Therefore, the proposed dataset is an important\nresource for advancing image-based disaster management and multi-task machine\nlearning research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasnat_A/0/1/0/all/0/1\">Abul Hasnat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Human Deformation Transfer. (arXiv:2109.01588v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01588","description":"<p>We consider the problem of human deformation transfer, where the goal is to\nretarget poses between different characters. Traditional methods that tackle\nthis problem require a clear definition of the pose, and use this definition to\ntransfer poses between characters. In this work, we take a different approach\nand transform the identity of a character into a new identity without modifying\nthe character's pose. This offers the advantage of not having to define\nequivalences between 3D human poses, which is not straightforward as poses tend\nto change depending on the identity of the character performing them, and as\ntheir meaning is highly contextual. To achieve the deformation transfer, we\npropose a neural encoder-decoder architecture where only identity information\nis encoded and where the decoder is conditioned on the pose. We use pose\nindependent representations, such as isometry-invariant shape characteristics,\nto represent identity features. Our model uses these features to supervise the\nprediction of offsets from the deformed pose to the result of the transfer. We\nshow experimentally that our method outperforms state-of-the-art methods both\nquantitatively and qualitatively, and generalises better to poses not seen\nduring training. We also introduce a fine-tuning step that allows to obtain\ncompetitive results for extreme identities, and allows to transfer simple\nclothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basset_J/0/1/0/all/0/1\">Jean Basset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1\">Adnane Boukhayma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuhrer_S/0/1/0/all/0/1\">Stefanie Wuhrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Multon_F/0/1/0/all/0/1\">Franck Multon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving Object Detection for Event-based Vision using k-means Clustering. (arXiv:2109.01879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01879","description":"<p>Moving object detection is important in computer vision. Event-based cameras\nare bio-inspired cameras that work by mimicking the working of the human eye.\nThese cameras have multiple advantages over conventional frame-based cameras,\nlike reduced latency, HDR, reduced motion blur during high motion, low power\nconsumption, etc. In spite of these advantages, event-based cameras are\nnoise-sensitive and have low resolution. Moreover, the task of moving object\ndetection in these cameras is difficult, as event-based sensors lack useful\nvisual features like texture and color. In this paper, we investigate the\napplication of the k-means clustering technique in detecting moving objects in\nevent-based data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Anindya Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mayukhmali Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Event Detection based on Spatio-Temporal Latent Action Unit using Skeletal Information. (arXiv:2109.02376v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02376","description":"<p>This paper propose a novel dictionary learning approach to detect event\naction using skeletal information extracted from RGBD video. The event action\nis represented as several latent atoms and composed of latent spatial and\ntemporal attributes. We perform the method at the example of fall event\ndetection. The skeleton frames are clustered by an initial K-means method. Each\nskeleton frame is assigned with a varying weight parameter and fed into our\nGradual Online Dictionary Learning (GODL) algorithm. During the training\nprocess, outlier frames will be gradually filtered by reducing the weight that\nis inversely proportional to a cost. In order to strictly distinguish the event\naction from similar actions and robustly acquire its action unit, we build a\nlatent unit temporal structure for each sub-action. We evaluate the proposed\nmethod on parts of the NTURGB+D dataset, which includes 209 fall videos, 405\nground-lift videos, 420 sit-down videos, and 280 videos of 46 otheractions. We\npresent the experimental validation of the achieved accuracy, recall and\nprecision. Our approach achieves the bestperformance on precision and accuracy\nof human fall event detection, compared with other existing dictionary learning\nmethods. With increasing noise ratio, our method remains the highest accuracy\nand the lowest variance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_H/0/1/0/all/0/1\">Hao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuxuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingchuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burschka_D/0/1/0/all/0/1\">Darius Burschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03201","description":"<p>Transformers, the default model of choices in natural language processing,\nhave drawn scant attention from the medical imaging community. Given the\nability to exploit long-term dependencies, transformers are promising to help\natypical convolutional neural networks (convnets) to overcome its inherent\nshortcomings of spatial inductive bias. However, most of recently proposed\ntransformer-based segmentation approaches simply treated transformers as\nassisted modules to help encode global context into convolutional\nrepresentations without investigating how to optimally combine self-attention\n(i.e., the core of transformers) with convolution. To address this issue, in\nthis paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful\nsegmentation model with an interleaved architecture based on empirical\ncombination of self-attention and convolution. In practice, nnFormer learns\nvolumetric representations from 3D local volumes. Compared to the naive\nvoxel-level self-attention implementation, such volume-based operations help to\nreduce the computational complexity by approximate 98% and 99.5% on Synapse and\nACDC datasets, respectively. In comparison to prior-art network configurations,\nnnFormer achieves tremendous improvements over previous transformer-based\nmethods on two commonly used datasets Synapse and ACDC. For instance, nnFormer\noutperforms Swin-UNet by over 7 percents on Synapse. Even when compared to\nnnUNet, currently the best performing fully-convolutional medical segmentation\nnetwork, nnFormer still provides slightly better performance on Synapse and\nACDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiansen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Rotation Invariance in Object Detection. (arXiv:2109.13488v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13488","description":"<p>Rotation augmentations generally improve a model's invariance/equivariance to\nrotation - except in object detection. In object detection the shape is not\nknown, therefore rotation creates a label ambiguity. We show that the de-facto\nmethod for bounding box label rotation, the Largest Box Method, creates very\nlarge labels, leading to poor performance and in many cases worse performance\nthan using no rotation at all. We propose a new method of rotation augmentation\nthat can be implemented in a few lines of code. First, we create a\ndifferentiable approximation of label accuracy and show that axis-aligning the\nbounding box around an ellipse is optimal. We then introduce Rotation\nUncertainty (RU) Loss, allowing the model to adapt to the uncertainty of the\nlabels. On five different datasets (including COCO, PascalVOC, and Transparent\nObject Bin Picking), this approach improves the rotational invariance of both\none-stage and two-stage architectures when measured with AP, AP50, and AP75.\nThe code is available at https://github.com/akasha-imaging/ICCV2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalra_A/0/1/0/all/0/1\">Agastya Kalra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoppi_G/0/1/0/all/0/1\">Guy Stoppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_B/0/1/0/all/0/1\">Bradley Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_R/0/1/0/all/0/1\">Rishav Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1\">Achuta Kadambi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14084","description":"<p>We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/tree/main/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for 3D Medical Image Analysis using 3D SimCLR and Monte Carlo Dropout. (arXiv:2109.14288v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.14288","description":"<p>Self-supervised learning methods can be used to learn meaningful\nrepresentations from unlabeled data that can be transferred to supervised\ndownstream tasks to reduce the need for labeled data. In this paper, we propose\na 3D self-supervised method that is based on the contrastive (SimCLR) method.\nAdditionally, we show that employing Bayesian neural networks (with Monte-Carlo\nDropout) during the inference phase can further enhance the results on the\ndownstream tasks. We showcase our models on two medical imaging segmentation\ntasks: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation\nfrom 3D CT. Our experimental results demonstrate the benefits of our proposed\nmethods in both downstream data-efficiency and performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_Y/0/1/0/all/0/1\">Yamen Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taleb_A/0/1/0/all/0/1\">Aiham Taleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1\">Marina M. -C. H&#xf6;hne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippert_C/0/1/0/all/0/1\">Christoph Lippert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Modal Focal Loss for RGBD Face Anti-Spoofing. (arXiv:2103.00948v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2103.00948","description":"<p>Automatic methods for detecting presentation attacks are essential to ensure\nthe reliable use of facial recognition technology. Most of the methods\navailable in the literature for presentation attack detection (PAD) fails in\ngeneralizing to unseen attacks. In recent years, multi-channel methods have\nbeen proposed to improve the robustness of PAD systems. Often, only a limited\namount of data is available for additional channels, which limits the\neffectiveness of these methods. In this work, we present a new framework for\nPAD that uses RGB and depth channels together with a novel loss function. The\nnew architecture uses complementary information from the two modalities while\nreducing the impact of overfitting. Essentially, a cross-modal focal loss\nfunction is proposed to modulate the loss contribution of each channel as a\nfunction of the confidence of individual channels. Extensive evaluations in two\npublicly available datasets demonstrate the effectiveness of the proposed\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+George_A/0/1/0/all/0/1\">Anjith George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1\">Sebastien Marcel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}