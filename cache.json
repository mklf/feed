{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction. (arXiv:2206.14264v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14264","description":"<p>A key challenge in attribute value extraction (AVE) from e-commerce sites is\nhow to handle a large number of attributes for diverse products. Although this\nchallenge is partially addressed by a question answering (QA) approach which\nfinds a value in product data for a given query (attribute), it does not work\neffectively for rare and ambiguous queries. We thus propose simple\nknowledge-driven query expansion based on possible answers (values) of a query\n(attribute) for QA-based AVE. We retrieve values of a query (attribute) from\nthe training data to expand the query. We train a model with two tricks,\nknowledge dropout and knowledge token mixing, which mimic the imperfection of\nthe value knowledge in testing. Experimental results on our cleaned version of\nAliExpress dataset show that our method improves the performance of AVE (+6.08\nmacro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro\nF1, respectively).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinzato_K/0/1/0/all/0/1\">Keiji Shinzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshinaga_N/0/1/0/all/0/1\">Naoki Yoshinaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yandi Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Te Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BertNet: Harvesting Knowledge Graphs from Pretrained Language Models. (arXiv:2206.14268v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14268","description":"<p>Symbolic knowledge graphs (KGs) have been constructed either by expensive\nhuman crowdsourcing or with domain-specific complex information extraction\npipelines. The emerging large pretrained language models (LMs), such as Bert,\nhave shown to implicitly encode massive knowledge which can be queried with\nproperly designed prompts. However, compared to the explicit KGs, the implict\nknowledge in the black-box LMs is often difficult to access or edit and lacks\nexplainability. In this work, we aim at harvesting symbolic KGs from the LMs, a\nnew framework for automatic KG construction empowered by the neural LMs'\nflexibility and scalability. Compared to prior works that often rely on large\nhuman annotated data or existing massive KGs, our approach requires only the\nminimal definition of relations as inputs, and hence is suitable for extracting\nknowledge of rich new relations not available before.The approach automatically\ngenerates diverse prompts, and performs efficient knowledge search within a\ngiven LM for consistent and extensive outputs. The harvested knowledge with our\napproach is substantially more accurate than with previous methods, as shown in\nboth automatic and human evaluation. As a result, we derive from diverse LMs a\nfamily of new KGs (e.g., BertNet and RoBERTaNet) that contain a richer set of\ncommonsense relations, including complex ones (e.g., \"A is capable of but not\ngood at B\"), than the human-annotated KGs (e.g., ConceptNet). Besides, the\nresulting KGs also serve as a vehicle to interpret the respective source LMs,\nleading to new insights into the varying knowledge capability of different LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shibo Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kaiwen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collecting high-quality adversarial data for machine reading comprehension tasks with humans and models in the loop. (arXiv:2206.14272v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14272","description":"<p>We present our experience as annotators in the creation of high-quality,\nadversarial machine-reading-comprehension data for extractive QA for Task 1 of\nthe First Workshop on Dynamic Adversarial Data Collection (DADC). DADC is an\nemergent data collection paradigm with both models and humans in the loop. We\nset up a quasi-experimental annotation design and perform quantitative analyses\nacross groups with different numbers of annotators focusing on successful\nadversarial attacks, cost analysis, and annotator confidence correlation. We\nfurther perform a qualitative analysis of our perceived difficulty of the task\ngiven the different topics of the passages in our dataset and conclude with\nrecommendations and suggestions that might be of value to people working on\nfuture DADC tasks and related annotation interfaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diaz_D/0/1/0/all/0/1\">Damian Y. Romero Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aniol_M/0/1/0/all/0/1\">Magdalena Anio&#x142;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culnan_J/0/1/0/all/0/1\">John Culnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding. (arXiv:2206.14318v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14318","description":"<p>End-to-end spoken language understanding (SLU) systems benefit from\npretraining on large corpora, followed by fine-tuning on application-specific\ndata. The resulting models are too large for on-edge applications. For\ninstance, BERT-based systems contain over 110M parameters. Observing the model\nis overparameterized, we propose lean transformer structure where the dimension\nof the attention mechanism is automatically reduced using group sparsity. We\npropose a variant where the learned attention subspace is transferred to an\nattention bottleneck layer. In a low-resource setting and without pre-training,\nthe resulting compact SLU model achieves accuracies competitive with\npre-trained large models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Can Secondary Predictions Tell Us? An Exploration on Question-Answering with SQuAD-v2.0. (arXiv:2206.14348v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14348","description":"<p>Performance in natural language processing, and specifically for the\nquestion-answer task, is typically measured by comparing a model\\'s most\nconfident (primary) prediction to golden answers (the ground truth). We are\nmaking the case that it is also useful to quantify how close a model came to\npredicting a correct answer even for examples that failed. We define the Golden\nRank (GR) of an example as the rank of its most confident prediction that\nexactly matches a ground truth, and show why such a match always exists. For\nthe 16 transformer models we analyzed, the majority of exactly matched golden\nanswers in secondary prediction space hover very close to the top rank. We\nrefer to secondary predictions as those ranking above 0 in descending\nconfidence probability order. We demonstrate how the GR can be used to classify\nquestions and visualize their spectrum of difficulty, from persistent near\nsuccesses to persistent extreme failures. We derive a new aggregate statistic\nover entire test sets, named the Golden Rank Interpolated Median (GRIM) that\nquantifies the proximity of failed predictions to the top choice made by the\nmodel. To develop some intuition and explore the applicability of these metrics\nwe use the Stanford Question Answering Dataset (SQuAD-2) and a few popular\ntransformer models from the Hugging Face hub. We first demonstrate that the\nGRIM is not directly correlated with the F1 and exact match (EM) scores. We\nthen calculate and visualize these scores for various transformer\narchitectures, probe their applicability in error analysis by clustering failed\npredictions, and compare how they relate to other training diagnostics such as\nthe EM and F1 scores. We finally suggest various research goals, such as\nbroadening data collection for these metrics and their possible use in\nadversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamfonas_M/0/1/0/all/0/1\">Michael Kamfonas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_G/0/1/0/all/0/1\">Gabriel Alon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual Question Answering. (arXiv:2206.14355v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14355","description":"<p>The availability of clean and diverse labeled data is a major roadblock for\ntraining models on complex tasks such as visual question answering (VQA). The\nextensive work on large vision-and-language models has shown that\nself-supervised learning is effective for pretraining multimodal interactions.\nIn this technical report, we focus on visual representations. We review and\nevaluate self-supervised methods to leverage unlabeled images and pretrain a\nmodel, which we then fine-tune on a custom VQA task that allows controlled\nevaluation and diagnosis. We compare energy-based models (EBMs) with\ncontrastive learning (CL). While EBMs are growing in popularity, they lack an\nevaluation on downstream tasks. We find that both EBMs and CL can learn\nrepresentations from unlabeled images that enable training a VQA model on very\nlittle annotated data. In a simple setting similar to CLEVR, we find that CL\nrepresentations also improve systematic generalization, and even match the\nperformance of representations from a larger, supervised, ImageNet-pretrained\nmodel. However, we find EBMs to be difficult to train because of instabilities\nand high variability in their results. Although EBMs prove useful for OOD\ndetection, other results on supervised energy-based training and uncertainty\ncalibration are largely negative. Overall, CL currently seems a preferable\noption over EBMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shevchenko_V/0/1/0/all/0/1\">Violetta Shevchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1\">Ehsan Abbasnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_A/0/1/0/all/0/1\">Anthony Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Twitter Data to Understand Public Perceptions of Approved versus Off-label Use for COVID-19-related Medications. (arXiv:2206.14358v1 [cs.CY])","link":"http://arxiv.org/abs/2206.14358","description":"<p>Understanding public discourse on emergency use of unproven therapeutics is\nessential to monitor safe use and combat misinformation. We developed a natural\nlanguage processing (NLP)-based pipeline to understand public perceptions of\nand stances on COVID-19-related drugs on Twitter across time. This\nretrospective study included 609,189 US-based tweets between January 29th, 2020\nand November 30th, 2021 on four drugs that gained wide public attention during\nthe COVID-19 pandemic: 1) Hydroxychloroquine and Ivermectin, drug therapies\nwith anecdotal evidence; and 2) Molnupiravir and Remdesivir, FDA-approved\ntreatment options for eligible patients. Time-trend analysis was used to\nunderstand the popularity and related events. Content and demographic analyses\nwere conducted to explore potential rationales of people's stances on each\ndrug. Time-trend analysis revealed that Hydroxychloroquine and Ivermectin\nreceived much more discussion than Molnupiravir and Remdesivir, particularly\nduring COVID-19 surges. Hydroxychloroquine and Ivermectin were highly\npoliticized, related to conspiracy theories, hearsay, celebrity effects, etc.\nThe distribution of stance between the two major US political parties was\nsignificantly different (p&lt;0.001); Republicans were much more likely to support\nHydroxychloroquine (+55%) and Ivermectin (+30%) than Democrats. People with\nhealthcare backgrounds tended to oppose Hydroxychloroquine (+7%) more than the\ngeneral population; in contrast, the general population was more likely to\nsupport Ivermectin (+14%). We make all the data, code, and models available at\nhttps://github.com/ningkko/COVID-drug.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yining Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shixu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plasek_J/0/1/0/all/0/1\">Joseph M. Plasek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_D/0/1/0/all/0/1\">David W. Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation of Transformer-based Language Models Revisited. (arXiv:2206.14366v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14366","description":"<p>In the past few years, transformer-based pre-trained language models have\nachieved astounding success in both industry and academia. However, the large\nmodel size and high run-time latency are serious impediments to applying them\nin practice, especially on mobile phones and Internet of Things (IoT) devices.\nTo compress the model, considerable literature has grown up around the theme of\nknowledge distillation (KD) recently. Nevertheless, how KD works in\ntransformer-based models is still unclear. We tease apart the components of KD\nand propose a unified KD framework. Through the framework, systematic and\nextensive experiments that spent over 23,000 GPU hours render a comprehensive\nanalysis from the perspectives of knowledge types, matching strategies,\nwidth-depth trade-off, initialization, model size, etc. Our empirical results\nshed light on the distillation in the pre-train language model and with\nrelative significant improvement over previous state-of-the-arts(SOTA).\nFinally, we provide a best-practice guideline for the KD in transformer-based\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chengqiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yunfei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Word Sense Embedding with SememeWSD and Synonym Set. (arXiv:2206.14388v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14388","description":"<p>Word embedding is a fundamental natural language processing task which can\nlearn feature of words. However, most word embedding methods assign only one\nvector to a word, even if polysemous words have multi-senses. To address this\nlimitation, we propose SememeWSD Synonym (SWSDS) model to assign a different\nvector to every sense of polysemous words with the help of word sense\ndisambiguation (WSD) and synonym set in OpenHowNet. We use the SememeWSD model,\nan unsupervised word sense disambiguation model based on OpenHowNet, to do word\nsense disambiguation and annotate the polysemous word with sense id. Then, we\nobtain top 10 synonyms of the word sense from OpenHowNet and calculate the\naverage vector of synonyms as the vector of the word sense. In experiments, We\nevaluate the SWSDS model on semantic similarity calculation with Gensim's\nwmdistance method. It achieves improvement of accuracy. We also examine the\nSememeWSD model on different BERT models to find the more effective model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangxi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Junping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhe Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zeli Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GERNERMED++: Transfer Learning in German Medical NLP. (arXiv:2206.14504v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14504","description":"<p>We present a statistical model for German medical natural language processing\ntrained for named entity recognition (NER) as an open, publicly available\nmodel. The work serves as a refined successor to our first GERNERMED model\nwhich is substantially outperformed by our work. We demonstrate the\neffectiveness of combining multiple techniques in order to achieve strong\nresults in entity recognition performance by the means of transfer-learning on\npretrained deep language models (LM), word-alignment and neural machine\ntranslation. Due to the sparse situation on open, public medical entity\nrecognition models for German texts, this work offers benefits to the German\nresearch community on medical NLP as a baseline model. Since our model is based\non public English data, its weights are provided without legal restrictions on\nusage and distribution. The sample code and the statistical model is available\nat: https://github.com/frankkramer-lab/GERNERMED-pp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frei_J/0/1/0/all/0/1\">Johann Frei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frei_Stuber_L/0/1/0/all/0/1\">Ludwig Frei-Stuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kramer_F/0/1/0/all/0/1\">Frank Kramer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Fusion for Language Model Fine-tuning. (arXiv:2206.14574v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14574","description":"<p>Language Models such as BERT have grown in popularity due to their ability to\nbe pre-trained and perform robustly on a wide range of Natural Language\nProcessing tasks. Often seen as an evolution over traditional word embedding\ntechniques, they can produce semantic representations of text, useful for tasks\nsuch as semantic similarity. However, state-of-the-art models often have high\ncomputational requirements and lack global context or domain knowledge which is\nrequired for complete language understanding. To address these limitations, we\ninvestigate the benefits of knowledge incorporation into the fine-tuning stages\nof BERT. An existing K-BERT model, which enriches sentences with triplets from\na Knowledge Graph, is adapted for the English language and extended to inject\ncontextually relevant information into sentences. As a side-effect, changes\nmade to K-BERT for accommodating the English language also extend to other\nword-based languages. Experiments conducted indicate that injected knowledge\nintroduces noise. We see statistically significant improvements for\nknowledge-driven tasks when this noise is minimised. We show evidence that,\ngiven the appropriate task, modest injection with relevant, high-quality\nknowledge is most performant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhana_N/0/1/0/all/0/1\">Nimesh Bhana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1\">Terence L. van Zyl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why Robust Natural Language Understanding is a Challenge. (arXiv:2206.14575v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14575","description":"<p>With the proliferation of Deep Machine Learning into real-life applications,\na particular property of this technology has been brought to attention: Neural\nNetworks notoriously present low robustness and can be highly sensitive to\nsmall input perturbations. Recently, many methods for verifying networks'\ngeneral properties of robustness have been proposed, but they are mostly\napplied in Computer Vision. In this paper we propose a Verification method for\nNatural Language Understanding classification based on larger regions of\ninterest, and we discuss the challenges of such task. We observe that, although\nthe data is almost linearly separable, the verifier does not output positive\nresults and we explain the problems and implications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casadio_M/0/1/0/all/0/1\">Marco Casadio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komendantskaya_E/0/1/0/all/0/1\">Ekaterina Komendantskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daggitt_M/0/1/0/all/0/1\">Matthew L. Daggitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kienitz_D/0/1/0/all/0/1\">Daniel Kienitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnaboldi_L/0/1/0/all/0/1\">Luca Arnaboldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokke_W/0/1/0/all/0/1\">Wen Kokke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using cognitive psychology to understand GPT-3. (arXiv:2206.14576v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14576","description":"<p>We study GPT-3, a recent large language model, using tools from cognitive\npsychology. More specifically, we assess GPT-3's decision-making, information\nsearch, deliberation, and causal reasoning abilities on a battery of canonical\nexperiments from the literature. We find that much of GPT-3's behavior is\nimpressive: it solves vignette-based tasks similarly or better than human\nsubjects, is able to make decent decisions from descriptions, outperforms\nhumans in a multi-armed bandit task, and shows signatures of model-based\nreinforcement learning. Yet we also find that small perturbations to\nvignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures\nof directed exploration, and that it fails miserably in a causal reasoning\ntask. These results enrich our understanding of current large language models\nand pave the way for future investigations using tools from cognitive\npsychology to study increasingly capable and opaque artificial agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Binz_M/0/1/0/all/0/1\">Marcel Binz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1\">Eric Schulz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Generative Patent Language Models. (arXiv:2206.14578v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14578","description":"<p>This research aims to build generative language models in the patent domain\nand to evaluate the models from a human-centric perspective. The evaluation\nmetric is to calculate the ratio of keystrokes that can be saved for a user in\nan autocomplete context based on the prediction of the generative models. The\nperformance of models in different sizes can also be evaluated in such a metric\nby measuring a number of newly granted patents. On the basis of the metric, it\nis found that the largest model is not necessarily the best. Several models are\npre-trained from scratch with patent corpus and are released. The experiments\nin this manuscript focus on patent claims, but the ideas and implementation can\nbe applied to other parts of a patent document. Furthermore, this research is\nmotivated to measure how close the pre-trained language model can generate a\nnewly granted patent claim. Or, conversely, the task is to measure the\nprobabilities for the model to generate each token text given the newly granted\npatent claim. In addition, this manuscript raises several legal implications on\npatent law for potential interdisciplinary research in the future. In\nparticular, can the metric based on model prediction be a metric to measure the\nnonobviousness requirement in the patent law?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jieh-Sheng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14579","description":"<p>Medical report generation task, which targets to produce long and coherent\ndescriptions of medical images, has attracted growing research interests\nrecently. Different from the general image captioning tasks, medical report\ngeneration is more challenging for data-driven neural models. This is mainly\ndue to 1) the serious data bias and 2) the limited medical data. To alleviate\nthe data bias and make best use of available data, we propose a\nCompetence-based Multimodal Curriculum Learning framework (CMCL). Specifically,\nCMCL simulates the learning process of radiologists and optimizes the model in\na step by step manner. Firstly, CMCL estimates the difficulty of each training\ninstance and evaluates the competence of current model; Secondly, CMCL selects\nthe most suitable batch of training instances considering current model\ncompetence. By iterating above two steps, CMCL can gradually improve the\nmodel's performance. The experiments on the public IU-Xray and MIMIC-CXR\ndatasets show that CMCL can be incorporated into existing models to improve\ntheir performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-specific Characteristic Assistance for Code-switching Speech Recognition. (arXiv:2206.14580v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14580","description":"<p>Dual-encoder structure successfully utilizes two language-specific encoders\n(LSEs) for code-switching speech recognition. Because LSEs are initialized by\ntwo pre-trained language-specific models (LSMs), the dual-encoder structure can\nexploit sufficient monolingual data and capture the individual language\nattributes. However, existing methods have no language constraints on LSEs and\nunderutilize language-specific knowledge of LSMs. In this paper, we propose a\nlanguage-specific characteristic assistance (LSCA) method to mitigate the above\nproblems. Specifically, during training, we introduce two language-specific\nlosses as language constraints and generate corresponding language-specific\ntargets for them. During decoding, we take the decoding abilities of LSMs into\naccount by combining the output probabilities of two LSMs and the mixture model\nto obtain the final predictions. Experiments show that either the training or\ndecoding method of LSCA can improve the model's performance. Furthermore, the\nbest result can obtain up to 15.4% relative error reduction on the\ncode-switching test set by combining the training and decoding methods of LSCA.\nMoreover, the system can process code-switching speech recognition tasks well\nwithout extra shared parameters or even retraining based on two pre-trained\nLSMs by using our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1\">Tongtong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1\">Meng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yongjie Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_J/0/1/0/all/0/1\">Jianwu Dang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models. (arXiv:2206.14589v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14589","description":"<p>In Spoken Language Understanding (SLU) the task is to extract important\ninformation from audio commands, like the intent of what a user wants the\nsystem to do and special entities like locations or numbers. This paper\npresents a simple method for embedding intents and entities into Finite State\nTransducers, and, in combination with a pretrained general-purpose\nSpeech-to-Text model, allows building SLU-models without any additional\ntraining. Building those models is very fast and only takes a few seconds. It\nis also completely language independent. With a comparison on different\nbenchmarks it is shown that this method can outperform multiple other, more\nresource demanding SLU approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bermuth_D/0/1/0/all/0/1\">Daniel Bermuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poeppel_A/0/1/0/all/0/1\">Alexander Poeppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_W/0/1/0/all/0/1\">Wolfgang Reif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NERDA-Con: Extending NER models for Continual Learning -- Integrating Distinct Tasks and Updating Distribution Shifts. (arXiv:2206.14607v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14607","description":"<p>With increasing applications in areas such as biomedical information\nextraction pipelines and social media analytics, Named Entity Recognition (NER)\nhas become an indispensable tool for knowledge extraction. However, with the\ngradual shift in language structure and vocabulary, NERs are plagued with\ndistribution shifts, making them redundant or not as profitable without\nre-training. Re-training NERs based on Large Language Models (LLMs) from\nscratch over newly acquired data poses economic disadvantages. In contrast,\nre-training only with newly acquired data will result in Catastrophic\nForgetting of previously acquired knowledge. Therefore, we propose NERDA-Con, a\npipeline for training NERs with LLM bases by incorporating the concept of\nElastic Weight Consolidation (EWC) into the NER fine-tuning NERDA pipeline. As\nwe believe our work has implications to be utilized in the pipeline of\ncontinual learning and NER, we open-source our code as well as provide the\nfine-tuning library of the same name NERDA-Con at\nhttps://github.com/SupritiVijay/NERDA-Con and\nhttps://pypi.org/project/NERDA-Con/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multilingual Dataset of COVID-19 Vaccination Attitudes on Twitter. (arXiv:2206.14619v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14619","description":"<p>Vaccine hesitancy is considered as one main cause of the stagnant uptake\nratio of COVID-19 vaccines in Europe and the US where vaccines are sufficiently\nsupplied. Fast and accurate grasp of public attitudes toward vaccination is\ncritical to address vaccine hesitancy, and social media platforms have proved\nto be an effective source of public opinions. In this paper, we describe the\ncollection and release of a dataset of tweets related to COVID-19 vaccines.\nThis dataset consists of the IDs of 2,198,090 tweets collected from Western\nEurope, 17,934 of which are annotated with the originators' vaccination\nstances. Our annotation will facilitate using and developing data-driven models\nto extract vaccination attitudes from social media posts and thus further\nconfirm the power of social media in public health surveillance. To lay the\ngroundwork for future research, we not only perform statistical analysis and\nvisualisation of our dataset, but also evaluate and compare the performance of\nestablished text-based benchmarks in vaccination stance extraction. We\ndemonstrate one potential use of our data in practice in tracking the temporal\nchanges of public COVID-19 vaccination attitudes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Ninghan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xihui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jun Pang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Weighted Finite Automata from Recurrent Neural Networks for Natural Languages. (arXiv:2206.14621v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14621","description":"<p>Recurrent Neural Networks (RNNs) have achieved tremendous success in\nsequential data processing. However, it is quite challenging to interpret and\nverify RNNs' behaviors directly. To this end, many efforts have been made to\nextract finite automata from RNNs. Existing approaches such as exact learning\nare effective in extracting finite-state models to characterize the state\ndynamics of RNNs for formal languages, but are limited in the scalability to\nprocess natural languages. Compositional approaches that are scablable to\nnatural languages fall short in extraction precision. In this paper, we\nidentify the transition sparsity problem that heavily impacts the extraction\nprecision. To address this problem, we propose a transition rule extraction\napproach, which is scalable to natural language processing models and effective\nin improving extraction precision. Specifically, we propose an empirical method\nto complement the missing rules in the transition diagram. In addition, we\nfurther adjust the transition matrices to enhance the context-aware ability of\nthe extracted weighted finite automaton (WFA). Finally, we propose two data\naugmentation tactics to track more dynamic behaviors of the target RNN.\nExperiments on two popular natural language datasets show that our method can\nextract WFA from RNN for natural language processing with better precision than\nexisting approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zeming Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Meng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody. (arXiv:2206.14643v1 [eess.AS])","link":"http://arxiv.org/abs/2206.14643","description":"<p>Generating expressive and contextually appropriate prosody remains a\nchallenge for modern text-to-speech (TTS) systems. This is particularly evident\nfor long, multi-sentence inputs. In this paper, we examine simple extensions to\na Transformer-based FastSpeech-like system, with the goal of improving prosody\nfor multi-sentence TTS. We find that long context, powerful text features, and\ntraining on multi-speaker data all improve prosody. More interestingly, they\nresult in synergies. Long context disambiguates prosody, improves coherence,\nand plays to the strengths of Transformers. Fine-tuning word-level features\nfrom a powerful language model, such as BERT, appears to profit from more\ntraining data, readily available in a multi-speaker setting. We look into\nobjective metrics on pausing and pacing and perform thorough subjective\nevaluations for speech naturalness. Our main system, which incorporates all the\nextensions, achieves consistently strong results, including statistically\nsignificant improvements in speech naturalness over all its competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Makarov_P/0/1/0/all/0/1\">Peter Makarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1\">Ammar Abbas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lajszczak_M/0/1/0/all/0/1\">Mateusz &#x141;ajszczak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1\">Arnaud Joly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1\">Sri Karlapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1\">Alexis Moinet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1\">Penny Karanasou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Based Audio Retrieval with Converging Tied Layers and Contrastive Loss. (arXiv:2206.14659v1 [cs.SD])","link":"http://arxiv.org/abs/2206.14659","description":"<p>In this paper, we tackle the new Language-Based Audio Retrieval task proposed\nin DCASE 2022. Firstly, we introduce a simple, scalable architecture which ties\nboth the audio and text encoder together. Secondly, we show that using this\narchitecture along with contrastive loss allows the model to significantly beat\nthe performance of the baseline model. Finally, in addition to having an\nextremely low training memory requirement, we are able to use pretrained models\nas it is without needing to finetune them. We test our methods and show that\nusing a combination of our methods beats the baseline scores significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_A/0/1/0/all/0/1\">Andrew Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The THUEE System Description for the IARPA OpenASR21 Challenge. (arXiv:2206.14660v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14660","description":"<p>This paper describes the THUEE team's speech recognition system for the IARPA\nOpen Automatic Speech Recognition Challenge (OpenASR21), with further\nexperiment explorations. We achieve outstanding results under both the\nConstrained and Constrained-plus training conditions. For the Constrained\ntraining condition, we construct our basic ASR system based on the standard\nhybrid architecture. To alleviate the Out-Of-Vocabulary (OOV) problem, we\nextend the pronunciation lexicon using Grapheme-to-Phoneme (G2P) techniques for\nboth OOV and potential new words. Standard acoustic model structures such as\nCNN-TDNN-F and CNN-TDNN-F-A are adopted. In addition, multiple data\naugmentation techniques are applied. For the Constrained-plus training\ncondition, we use the self-supervised learning framework wav2vec2.0. We\nexperiment with various fine-tuning techniques with the Connectionist Temporal\nClassification (CTC) criterion on top of the publicly available pre-trained\nmodel XLSR-53. We find that the frontend feature extractor plays an important\nrole when applying the wav2vec2.0 pre-trained model to the encoder-decoder\nbased CTC/Attention ASR architecture. Extra improvements can be achieved by\nusing the CTC model finetuned in the target language as the frontend feature\nextractor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_S/0/1/0/all/0/1\">Shuzhou Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guan-Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guoguo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Qiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Data-Driven Requirements Engineering Approach: Automatic Analysis of User Reviews. (arXiv:2206.14669v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14669","description":"<p>We are concerned by Data Driven Requirements Engineering, and in particular\nthe consideration of user's reviews. These online reviews are a rich source of\ninformation for extracting new needs and improvement requests. In this work, we\nprovide an automated analysis using CamemBERT, which is a state-of-the-art\nlanguage model in French. We created a multi-label classification dataset of\n6000 user reviews from three applications in the Health &amp; Fitness field. The\nresults are encouraging and suggest that it's possible to identify\nautomatically the reviews concerning requests for new features.\n</p>\n<p>Dataset is available at:\nhttps://github.com/Jl-wei/APIA2022-French-user-reviews-classification-dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jialiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courbis_A/0/1/0/all/0/1\">Anne-Lise Courbis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambolais_T/0/1/0/all/0/1\">Thomas Lambolais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Binbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_P/0/1/0/all/0/1\">Pierre Louis Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dray_G/0/1/0/all/0/1\">G&#xe9;rard Dray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it possible not to cheat on the Turing Test: Exploring the potential and challenges for true natural language 'understanding' by computers. (arXiv:2206.14672v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14672","description":"<p>Recent hype surrounding the increasing sophistication of language processing\nmodels has renewed optimism regarding machines achieving a human-like command\nof natural language. The area of natural language understanding in artificial\nintelligence claims to have been making great strides in this area, however,\nthe lack of conceptual clarity in how 'understanding' is used in this and other\ndisciplines have made it difficult to discern how close we actually are. A\ncomprehensive, interdisciplinary overview of current approaches and remaining\nchallenges is yet to be carried out. Beyond linguistic knowledge, this requires\nconsidering our species-specific capabilities to categorize, memorize, label\nand communicate our (sufficiently similar) embodied and situated experiences.\nMoreover, gauging the practical constraints requires critically analyzing the\ntechnical capabilities of current models, as well as deeper philosophical\nreflection on theoretical possibilities and limitations. In this paper, I unite\nall of these perspectives -- the philosophical, cognitive-linguistic, and\ntechnical -- to unpack the challenges involved in reaching true (human-like)\nlanguage understanding. By unpacking the theoretical assumptions inherent in\ncurrent approaches, I hope to illustrate how far we actually are from achieving\nthis goal, if indeed it is the goal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberts_L/0/1/0/all/0/1\">Lize Alberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Deliberation by Text-Only and Semi-Supervised Training. (arXiv:2206.14716v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14716","description":"<p>Text-only and semi-supervised training based on audio-only data has gained\npopularity recently due to the wide availability of unlabeled text and speech\ndata. In this work, we propose incorporating text-only and semi-supervised\ntraining into an attention-based deliberation model. By incorporating text-only\ndata in training a bidirectional encoder representation from transformer (BERT)\nfor the deliberation text encoder, and large-scale text-to-speech and\naudio-only utterances using joint acoustic and text decoder (JATD) and\nsemi-supervised training, we achieved 4%-12% WER reduction for various tasks\ncompared to the baseline deliberation. Compared to a state-of-the-art language\nmodel (LM) rescoring method, the deliberation model reduces the Google Voice\nSearch WER by 11% relative. We show that the deliberation model also achieves a\npositive human side-by-side evaluation compared to the state-of-the-art LM\nrescorer with reasonable endpointer latencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavandadi_S/0/1/0/all/0/1\">Sepand Mavandadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision. (arXiv:2206.14719v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14719","description":"<p>Clinical trials are essential for drug development but are extremely\nexpensive and time-consuming to conduct. It is beneficial to study similar\nhistorical trials when designing a clinical trial. However, lengthy trial\ndocuments and lack of labeled data make trial similarity search difficult. We\npropose a zero-shot clinical trial retrieval method, Trial2Vec, which learns\nthrough self-supervision without annotating similar clinical trials.\nSpecifically, the meta-structure of trial documents (e.g., title, eligibility\ncriteria, target disease) along with clinical knowledge (e.g., UMLS knowledge\nbase https://www.nlm.nih.gov/research/umls/index.html) are leveraged to\nautomatically generate contrastive samples. Besides, Trial2Vec encodes trial\ndocuments considering meta-structure thus producing compact embeddings\naggregating multi-aspect information from the whole document. We show that our\nmethod yields medically interpretable embeddings by visualization and it gets a\n15% average improvement over the best baselines on precision/recall for trial\nretrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we\nprove the pre-trained embeddings benefit the downstream trial outcome\nprediction task over 240k trials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks. (arXiv:2206.14729v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14729","description":"<p>Developing methods to adversarially challenge NLP systems is a promising\navenue for improving both model performance and interpretability. Here, we\ndescribe the approach of the team \"longhorns\" on Task 1 of the The First\nWorkshop on Dynamic Adversarial Data Collection (DADC), which asked teams to\nmanually fool a model on an Extractive Question Answering task. Our team\nfinished first, with a model error rate of 62%. We advocate for a systematic,\nlinguistically informed approach to formulating adversarial questions, and we\ndescribe the results of our pilot experiments, as well as our official\nsubmission.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1\">Venelin Kovatchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_T/0/1/0/all/0/1\">Trina Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Govindarajan_V/0/1/0/all/0/1\">Venkata S Govindarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chronis_G/0/1/0/all/0/1\">Gabriella Chronis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Anubrata Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erk_K/0/1/0/all/0/1\">Katrin Erk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lease_M/0/1/0/all/0/1\">Matthew Lease</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yating Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TweetNLP: Cutting-Edge Natural Language Processing for Social Media. (arXiv:2206.14774v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14774","description":"<p>In this paper we present TweetNLP, an integrated platform for Natural\nLanguage Processing (NLP) in social media. TweetNLP supports a diverse set of\nNLP tasks, including generic focus areas such as sentiment analysis and named\nentity recognition, as well as social media-specific tasks such as emoji\nprediction and offensive language identification. Task-specific systems are\npowered by reasonably-sized Transformer-based language models specialized on\nsocial media text (in particular, Twitter) which can be run without the need\nfor dedicated hardware or cloud services. The main contributions of TweetNLP\nare: (1) an integrated Python library for a modern toolkit supporting social\nmedia analysis using our various task-specific models adapted to the social\ndomain; (2) an interactive online demo for codeless experimentation using our\nmodels; and (3) a tutorial covering a wide variety of typical social media\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezaee_K/0/1/0/all/0/1\">Kiamehr Rezaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riahi_T/0/1/0/all/0/1\">Talayeh Riahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loureiro_D/0/1/0/all/0/1\">Daniel Loureiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boisson_J/0/1/0/all/0/1\">Joanne Boisson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Camara_E/0/1/0/all/0/1\">Eugenio Mart&#xed;nez-C&#xe1;mara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medina_G/0/1/0/all/0/1\">Gonzalo Medina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhrmann_T/0/1/0/all/0/1\">Thomas Buhrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method. (arXiv:2206.14796v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14796","description":"<p>Most works on modeling the conversation history in Conversational Question\nAnswering (CQA) report a single main result on a common CQA benchmark. While\nexisting models show impressive results on CQA leaderboards, it remains unclear\nwhether they are robust to shifts in setting (sometimes to more realistic\nones), training data size (e.g. from large to small sets) and domain. In this\nwork, we design and conduct the first large-scale robustness study of history\nmodeling approaches for CQA. We find that high benchmark scores do not\nnecessarily translate to strong robustness, and that various methods can\nperform extremely differently under different settings. Equipped with the\ninsights from our study, we design a novel prompt-based history modeling\napproach, and demonstrate its strong robustness across various settings. Our\napproach is inspired by existing methods that highlight historic answers in the\npassage. However, instead of highlighting by modifying the passage token\nembeddings, we add textual prompts directly in the passage text. Our approach\nis simple, easy-to-plug into practically any model, and highly effective, thus\nwe recommend it as a starting point for future model developers. We also hope\nthat our study and insights will raise awareness to the importance of\nrobustness-focused evaluation, in addition to obtaining high leaderboard\nscores, leading to better CQA systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gekhman_Z/0/1/0/all/0/1\">Zorik Gekhman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oved_N/0/1/0/all/0/1\">Nadav Oved</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_O/0/1/0/all/0/1\">Orgad Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations. (arXiv:2109.14989v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14989","description":"<p>We investigate the extent to which modern, neural language models are\nsusceptible to structural priming, the phenomenon whereby the structure of a\nsentence makes the same structure more probable in a follow-up sentence. We\nexplore how priming can be used to study the potential of these models to learn\nabstract structural information, which is a prerequisite for good performance\non tasks that require natural language understanding skills. We introduce a\nnovel metric and release Prime-LM, a large corpus where we control for various\nlinguistic factors which interact with priming strength. We find that\nTransformer models indeed show evidence of structural priming, but also that\nthe generalisations they learned are to some extent modulated by semantic\ninformation. Our experiments also show that the representations acquired by the\nmodels may not only encode abstract sequential structure but involve certain\nlevel of hierarchical syntactic information. More generally, our study shows\nthat the priming paradigm is a useful, additional tool for gaining insights\ninto the capacities of language models and opens the door to future\npriming-based investigations that probe the model's internal states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinclair_A/0/1/0/all/0/1\">Arabella Sinclair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jumelet_J/0/1/0/all/0/1\">Jaap Jumelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1\">Willem Zuidema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCA-MDD: A Coupled Cross-Attention based Framework for Streaming Mispronunciation Detection and Diagnosis. (arXiv:2111.08191v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08191","description":"<p>Mispronunciation detection and diagnosis (MDD) is a popular research focus in\ncomputer-aided pronunciation training (CAPT) systems. End-to-end (e2e)\napproaches are becoming dominant in MDD. However an e2e MDD model usually\nrequires entire speech utterances as input context, which leads to significant\ntime latency especially for long paragraphs. We propose a streaming e2e MDD\nmodel called CoCA-MDD. We utilize conv-transformer structure to encode input\nspeech in a streaming manner. A coupled cross-attention (CoCA) mechanism is\nproposed to integrate frame-level acoustic features with encoded reference\nlinguistic features. CoCA also enables our model to perform mispronunciation\nclassification with whole utterances. The proposed model allows system fusion\nbetween the streaming output and mispronunciation classification output for\nfurther performance enhancement. We evaluate CoCA-MDD on publicly available\ncorpora. CoCA-MDD achieves F1 scores of 57.03% and 60.78% for streaming and\nfusion modes respectively on L2-ARCTIC. For phone-level pronunciation scoring,\nCoCA-MDD achieves 0.58 Pearson correlation coefficient (PCC) value on\nSpeechOcean762.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nianzu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baohua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing language context confusion for end-to-end code-switching automatic speech recognition. (arXiv:2201.12155v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12155","description":"<p>Code-switching deals with alternative languages in communication process.\nTraining end-to-end (E2E) automatic speech recognition (ASR) systems for\ncode-switching is especially challenging as code-switching training data are\nalways insufficient to combat the increased multilingual context confusion due\nto the presence of more than one language. We propose a language-related\nattention mechanism to reduce multilingual context confusion for the E2E\ncode-switching ASR model based on the Equivalence Constraint (EC) Theory. The\nlinguistic theory requires that any monolingual fragment that occurs in the\ncode-switching sentence must occur in one of the monolingual sentences. The\ntheory establishes a bridge between monolingual data and code-switching data.\nWe leverage this linguistics theory to design the code-switching E2E ASR model.\nThe proposed model efficiently transfers language knowledge from rich\nmonolingual data to improve the performance of the code-switching ASR model. We\nevaluate our model on ASRU 2019 Mandarin-English code-switching challenge\ndataset. Compared to the baseline model, our proposed model achieves a 17.12%\nrelative error reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhengkun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data. (arXiv:2203.11476v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11476","description":"<p>Human speakers encode information into raw speech which is then decoded by\nthe listeners. This complex relationship between encoding (production) and\ndecoding (perception) is often modeled separately. Here, we test how encoding\nand decoding of lexical semantic information can emerge automatically from raw\nspeech in unsupervised generative deep convolutional networks that combine the\nproduction and perception principles of speech. We introduce, to our knowledge,\nthe most challenging objective in unsupervised lexical learning: a network that\nmust learn unique representations for lexical items with no direct access to\ntraining data. We train several models (ciwGAN and fiwGAN <a href=\"/abs/2006.02951\">arXiv:2006.02951</a>) and\ntest how the networks classify acoustic lexical items in unobserved test data.\nStrong evidence in favor of lexical learning and a causal relationship between\nlatent codes and meaningful sublexical units emerge. The architecture that\ncombines the production and perception principles is thus able to learn to\ndecode unique information from raw acoustic data without accessing real\ntraining data directly. We propose a technique to explore lexical (holistic)\nand sublexical (featural) learned representations in the classifier network.\nThe results bear implications for unsupervised speech technology, as well as\nfor unsupervised semantic modeling as language models increasingly bypass text\nand operate from raw acoustics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GateNLP-UShef at SemEval-2022 Task 8: Entity-Enriched Siamese Transformer for Multilingual News Article Similarity. (arXiv:2205.15812v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15812","description":"<p>This paper describes the second-placed system on the leaderboard of\nSemEval-2022 Task 8: Multilingual News Article Similarity. We propose an\nentity-enriched Siamese Transformer which computes news article similarity\nbased on different sub-dimensions, such as the shared narrative, entities,\nlocation and time of the event discussed in the news article. Our system\nexploits a Siamese network architecture using a Transformer encoder to learn\ndocument-level representations for the purpose of capturing the narrative\ntogether with the auxiliary entity-based features extracted from the news\narticles. The intuition behind using all these features together is to capture\nthe similarity between news articles at different granularity levels and to\nassess the extent to which different news outlets write about \"the same\nevents\". Our experimental results and detailed ablation study demonstrate the\neffectiveness and the validity of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Iknoor Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thong_M/0/1/0/all/0/1\">Melissa Thong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery. (arXiv:2206.11706v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2206.11706","description":"<p>Latent Dirichlet allocation (LDA) is widely used for unsupervised topic\nmodelling on sets of documents. No temporal information is used in the model.\nHowever, there is often a relationship between the corresponding topics of\nconsecutive tokens. In this paper, we present an extension to LDA that uses a\nMarkov chain to model temporal information. We use this new model for acoustic\nunit discovery from speech. As input tokens, the model takes a discretised\nencoding of speech from a vector quantised (VQ) neural network with 512 codes.\nThe goal is then to map these 512 VQ codes to 50 phone-like units (topics) in\norder to more closely resemble true phones. In contrast to the base LDA, which\nonly considers how VQ codes co-occur within utterances (documents), the Markov\nchain LDA additionally captures how consecutive codes follow one another. This\nextension leads to an increase in cluster quality and phone segmentation\nresults compared to the base LDA. Compared to a recent vector quantised neural\nnetwork approach that also learns 50 units, the extended LDA model performs\nbetter in phone segmentation but worse in mutual information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Merwe_W/0/1/0/all/0/1\">Werner van der Merwe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1\">Herman Kamper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Preez_J/0/1/0/all/0/1\">Johan du Preez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective. (arXiv:2206.12759v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.12759","description":"<p>Accented speech recognition and accent classification are relatively\nunder-explored research areas in speech technology. Recently, deep\nlearning-based methods and Transformer-based pretrained models have achieved\nsuperb performances in both areas. However, most accent classification tasks\nfocused on classifying different kinds of English accents and little attention\nwas paid to geographically-proximate accent classification, especially under a\nlow-resource setting where forensic speech science tasks usually encounter. In\nthis paper, we explored three main accent modelling methods combined with two\ndifferent classifiers based on 105 speaker recordings retrieved from five urban\nvarieties in Northern England. Although speech representations generated from\npretrained models generally have better performances in downstream\nclassification, traditional methods like Mel Frequency Cepstral Coefficients\n(MFCCs) and formant measurements are equipped with specific strengths. These\nresults suggest that in forensic phonetics scenario where data are relatively\nscarce, a simple modelling method and classifier could be competitive with\nstate-of-the-art pretrained speech models as feature extractors, which could\nenhance a sooner estimation for the accent information in practices. Besides,\nour findings also cross-validated a new methodology in quantifying\nsociophonetic changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qingcheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1\">Dading Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bengali Common Voice Speech Dataset for Automatic Speech Recognition. (arXiv:2206.14053v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14053","description":"<p>Bengali is one of the most spoken languages in the world with over 300\nmillion speakers globally. Despite its popularity, research into the\ndevelopment of Bengali speech recognition systems is hindered due to the lack\nof diverse open-source datasets. As a way forward, we have crowdsourced the\nBengali Common Voice Speech Dataset, which is a sentence-level automatic speech\nrecognition corpus. Collected on the Mozilla Common Voice platform, the dataset\nis part of an ongoing campaign that has led to the collection of over 400 hours\nof data in 2 months and is growing rapidly. Our analysis shows that this\ndataset has more speaker, phoneme, and environmental diversity compared to the\nOpenSLR Bengali ASR dataset, the largest existing open-source Bengali speech\ndataset. We present insights obtained from the dataset and discuss key\nlinguistic challenges that need to be addressed in future versions.\nAdditionally, we report the current performance of a few Automatic Speech\nRecognition (ASR) algorithms and set a benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Samiul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sushmit_A/0/1/0/all/0/1\">Asif Sushmit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_Z/0/1/0/all/0/1\">Zaowad Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakkhatra_S/0/1/0/all/0/1\">Shahrin Nakkhatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansary_M/0/1/0/all/0/1\">MD. Nazmuddoha Ansary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossen_S/0/1/0/all/0/1\">Syed Mobassir Hossen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehnaz_S/0/1/0/all/0/1\">Sazia Morshed Mehnaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reasat_T/0/1/0/all/0/1\">Tahsin Reasat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Masked World Models for Visual Control. (arXiv:2206.14244v1 [cs.RO])","link":"http://arxiv.org/abs/2206.14244","description":"<p>Visual model-based reinforcement learning (RL) has the potential to enable\nsample-efficient robot learning from visual observations. Yet the current\napproaches typically train a single model end-to-end for learning both visual\nrepresentations and dynamics, making it difficult to accurately model the\ninteraction between robots and small objects. In this work, we introduce a\nvisual model-based RL framework that decouples visual representation learning\nand dynamics learning. Specifically, we train an autoencoder with convolutional\nlayers and vision transformers (ViT) to reconstruct pixels given masked\nconvolutional features, and learn a latent dynamics model that operates on the\nrepresentations from the autoencoder. Moreover, to encode task-relevant\ninformation, we introduce an auxiliary reward prediction objective for the\nautoencoder. We continually update both autoencoder and dynamics model using\nonline samples collected from environment interaction. We demonstrate that our\ndecoupling approach achieves state-of-the-art performance on a variety of\nvisual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7%\nsuccess rate on 50 visual robotic manipulation tasks from Meta-world, while the\nbaseline achieves 67.9%. Code is available on the project website:\nhttps://sites.google.com/view/mwm-rl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Younggyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafner_D/0/1/0/all/0/1\">Danijar Hafner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SImProv: Scalable Image Provenance Framework for Robust Content Attribution. (arXiv:2206.14245v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14245","description":"<p>We present SImProv - a scalable image provenance framework to match a query\nimage back to a trusted database of originals and identify possible\nmanipulations on the query. SImProv consists of three stages: a scalable search\nstage for retrieving top-k most similar images; a re-ranking and\nnear-duplicated detection stage for identifying the original among the\ncandidates; and finally a manipulation detection and visualization stage for\nlocalizing regions within the query that may have been manipulated to differ\nfrom the original. SImProv is robust to benign image transformations that\ncommonly occur during online redistribution, such as artifacts due to noise and\nrecompression degradation, as well as out-of-place transformations due to image\npadding, warping, and changes in size and shape. Robustness towards\nout-of-place transformations is achieved via the end-to-end training of a\ndifferentiable warping module within the comparator architecture. We\ndemonstrate effective retrieval and manipulation detection over a dataset of\n100 million images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alexander Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenni_S/0/1/0/all/0/1\">Simon Jenni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminanthan_V/0/1/0/all/0/1\">Viswanathan Swaminanthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN-based Intrinsic Exploration For Sample Efficient Reinforcement Learning. (arXiv:2206.14256v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14256","description":"<p>In this study, we address the problem of efficient exploration in\nreinforcement learning. Most common exploration approaches depend on random\naction selection, however these approaches do not work well in environments\nwith sparse or no rewards. We propose Generative Adversarial Network-based\nIntrinsic Reward Module that learns the distribution of the observed states and\nsends an intrinsic reward that is computed as high for states that are out of\ndistribution, in order to lead agent to unexplored states. We evaluate our\napproach in Super Mario Bros for a no reward setting and in Montezuma's Revenge\nfor a sparse reward setting and show that our approach is indeed capable of\nexploring efficiently. We discuss a few weaknesses and conclude by discussing\nfuture works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamar_D/0/1/0/all/0/1\">Do&#x11f;ay Kamar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ure_N/0/1/0/all/0/1\">Naz&#x131;m Kemal &#xdc;re</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">G&#xf6;zde &#xdc;nal</a> (1 and 2) ((1) Faculty of Computer and Informatics, Istanbul Technical University (2) Artificial Intelligence and Data Science Research Center, Istanbul Technical University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZoDIAC: Zoneout Dropout Injection Attention Calculation. (arXiv:2206.14263v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14263","description":"<p>Recently the use of self-attention has yielded to state-of-the-art results in\nvision-language tasks such as image captioning as well as natural language\nunderstanding and generation (NLU and NLG) tasks and computer vision tasks such\nas image classification. This is since self-attention maps the internal\ninteractions among the elements of input source and target sequences. Although\nself-attention successfully calculates the attention values and maps the\nrelationships among the elements of input source and target sequence, yet there\nis no mechanism to control the intensity of attention. In real world, when\ncommunicating with each other face to face or vocally, we tend to express\ndifferent visual and linguistic context with various amounts of intensity. Some\nwords might carry (be spoken with) more stress and weight indicating the\nimportance of that word in the context of the whole sentence. Based on this\nintuition, we propose Zoneout Dropout Injection Attention Calculation (ZoDIAC)\nin which the intensities of attention values in the elements of the input\nsequence are calculated with respect to the context of the elements of input\nsequence. The results of our experiments reveal that employing ZoDIAC leads to\nbetter performance in comparison with the self-attention module in the\nTransformer model. The ultimate goal is to find out if we could modify\nself-attention module in the Transformer model with a method that is\npotentially extensible to other models that leverage on self-attention at their\ncore. Our findings suggest that this particular goal deserves further attention\nand investigation by the research community.\n</p>\n<p>The code for ZoDIAC is available on www.github.com/zanyarz/zodiac .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zohourianshahzadi_Z/0/1/0/all/0/1\">Zanyar Zohourianshahzadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal Kalita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning in Medical Image Analysis: Concepts, Applications, Challenges, and Future Directions. (arXiv:2206.14302v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14302","description":"<p>Motivation: Medical image analysis involves tasks to assist physicians in\nqualitative and quantitative analysis of lesions or anatomical structures,\nsignificantly improving the accuracy and reliability of diagnosis and\nprognosis. Traditionally, these tasks are finished by physicians or medical\nphysicists and lead to two major problems: (i) low efficiency; (ii) biased by\npersonal experience. In the past decade, many machine learning methods have\nbeen applied to accelerate and automate the image analysis process. Compared to\nthe enormous deployments of supervised and unsupervised learning models,\nattempts to use reinforcement learning in medical image analysis are scarce.\nThis review article could serve as the stepping-stone for related research.\nSignificance: From our observation, though reinforcement learning has gradually\ngained momentum in recent years, many researchers in the medical analysis field\nfind it hard to understand and deploy in clinics. One cause is lacking\nwell-organized review articles targeting readers lacking professional computer\nscience backgrounds. Rather than providing a comprehensive list of all\nreinforcement learning models in medical image analysis, this paper may help\nthe readers to learn how to formulate and solve their medical image analysis\nresearch as reinforcement learning problems. Approach &amp; Results: We selected\npublished articles from Google Scholar and PubMed. Considering the scarcity of\nrelated articles, we also included some outstanding newest preprints. The\npapers are carefully reviewed and categorized according to the type of image\nanalysis task. We first review the basic concepts and popular models of\nreinforcement learning. Then we explore the applications of reinforcement\nlearning models in landmark detection. Finally, we conclude the article by\ndiscussing the reviewed reinforcement learning approaches' limitations and\npossible improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Mingzhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matkovic_L/0/1/0/all/0/1\">Luke Matkovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaofeng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multistep Automated Data Labelling Procedure (MADLaP) for Thyroid Nodules on Ultrasound: An Artificial Intelligence Approach for Automating Image Annotation. (arXiv:2206.14305v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14305","description":"<p>Machine learning (ML) for diagnosis of thyroid nodules on ultrasound is an\nactive area of research. However, ML tools require large, well-labelled\ndatasets, the curation of which is time-consuming and labor-intensive. The\npurpose of our study was to develop and test a deep-learning-based tool to\nfacilitate and automate the data annotation process for thyroid nodules; we\nnamed our tool Multistep Automated Data Labelling Procedure (MADLaP). MADLaP\nwas designed to take multiple inputs included pathology reports, ultrasound\nimages, and radiology reports. Using multiple step-wise modules including\nrule-based natural language processing, deep-learning-based imaging\nsegmentation, and optical character recognition, MADLaP automatically\nidentified images of a specific thyroid nodule and correctly assigned a\npathology label. The model was developed using a training set of 378 patients\nacross our health system and tested on a separate set of 93 patients. Ground\ntruths for both sets were selected by an experienced radiologist. Performance\nmetrics including yield (how many labeled images the model produced) and\naccuracy (percentage correct) were measured using the test set. MADLaP achieved\na yield of 63% and an accuracy of 83%. The yield progressively increased as the\ninput data moved through each module, while accuracy peaked part way through.\nError analysis showed that inputs from certain examination sites had lower\naccuracy (40%) than the other sites (90%, 100%). MADLaP successfully created\ncurated datasets of labeled ultrasound images of thyroid nodules. While\naccurate, the relatively suboptimal yield of MADLaP exposed some challenges\nwhen trying to automatically label radiology images from heterogeneous sources.\nThe complex task of image curation and annotation could be automated, allowing\nfor enrichment of larger datasets for use in machine learning development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jikai Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej M. Mazurowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Allen_B/0/1/0/all/0/1\">Brian C. Allen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wildman_Torbiner_B/0/1/0/all/0/1\">Benjamin Wildman-Torbiner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Neural Articulated Radiance Fields. (arXiv:2206.14314v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14314","description":"<p>Unsupervised learning of 3D-aware generative adversarial networks (GANs)\nusing only collections of single-view 2D photographs has very recently made\nmuch progress. These 3D GANs, however, have not been demonstrated for human\nbodies and the generated radiance fields of existing frameworks are not\ndirectly editable, limiting their applicability in downstream tasks. We propose\na solution to these challenges by developing a 3D GAN framework that learns to\ngenerate radiance fields of human bodies or faces in a canonical pose and warp\nthem using an explicit deformation field into a desired body pose or facial\nexpression. Using our framework, we demonstrate the first high-quality radiance\nfield generation results for human bodies. Moreover, we show that our\ndeformation-aware training procedure significantly improves the quality of\ngenerated bodies or faces when editing their poses or facial expressions\ncompared to a 3D GAN that is not trained with explicit deformations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">Alexander W. Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellnhofer_P/0/1/0/all/0/1\">Petr Kellnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_E/0/1/0/all/0/1\">Eric R. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1\">David B. Lindell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Adjacency Matrix Configuration in GCN-based Models for Skeleton-based Action Recognition. (arXiv:2206.14344v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14344","description":"<p>Human skeleton data has received increasing attention in action recognition\ndue to its background robustness and high efficiency. In skeleton-based action\nrecognition, graph convolutional network (GCN) has become the mainstream\nmethod. This paper analyzes the fundamental factor for GCN-based models -- the\nadjacency matrix. We notice that most GCN-based methods conduct their adjacency\nmatrix based on the human natural skeleton structure. Based on our former work\nand analysis, we propose that the human natural skeleton structure adjacency\nmatrix is not proper for skeleton-based action recognition. We propose a new\nadjacency matrix that abandons all rigid neighbor connections but lets the\nmodel adaptively learn the relationships of joints. We conduct extensive\nexperiments and analysis with a validation model on two skeleton-based action\nrecognition datasets (NTURGBD60 and FineGYM). Comprehensive experimental\nresults and analysis reveals that 1) the most widely used human natural\nskeleton structure adjacency matrix is unsuitable in skeleton-based action\nrecognition; 2) The proposed adjacency matrix is superior in model performance,\nnoise robustness and transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiongwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tieyong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yunfei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Meng Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network Based Partial Face Detection. (arXiv:2206.14350v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14350","description":"<p>Due to the massive explanation of artificial intelligence, machine learning\ntechnology is being used in various areas of our day-to-day life. In the world,\nthere are a lot of scenarios where a simple crime can be prevented before it\nmay even happen or find the person responsible for it. A face is one\ndistinctive feature that we have and can differentiate easily among many other\nspecies. But not just different species, it also plays a significant role in\ndetermining someone from the same species as us, humans. Regarding this\ncritical feature, a single problem occurs most often nowadays. When the camera\nis pointed, it cannot detect a person's face, and it becomes a poor image. On\nthe other hand, where there was a robbery and a security camera installed, the\nrobber's identity is almost indistinguishable due to the low-quality camera.\nBut just making an excellent algorithm to work and detecting a face reduces the\ncost of hardware, and it doesn't cost that much to focus on that area. Facial\nrecognition, widget control, and such can be done by detecting the face\ncorrectly. This study aims to create and enhance a machine learning model that\ncorrectly recognizes faces. Total 627 Data have been collected from different\nBangladeshi people's faces on four angels. In this work, CNN, Harr Cascade,\nCascaded CNN, Deep CNN &amp; MTCNN are these five machine learning approaches\nimplemented to get the best accuracy of our dataset. After creating and running\nthe model, Multi-Task Convolutional Neural Network (MTCNN) achieved 96.2% best\nmodel accuracy with training data rather than other machine learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Towfiqul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_T/0/1/0/all/0/1\">Tanzim Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">A.B.M. Raihanur Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Taminul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md. Sadekur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habib_M/0/1/0/all/0/1\">Md. Tarek Habib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual Question Answering. (arXiv:2206.14355v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14355","description":"<p>The availability of clean and diverse labeled data is a major roadblock for\ntraining models on complex tasks such as visual question answering (VQA). The\nextensive work on large vision-and-language models has shown that\nself-supervised learning is effective for pretraining multimodal interactions.\nIn this technical report, we focus on visual representations. We review and\nevaluate self-supervised methods to leverage unlabeled images and pretrain a\nmodel, which we then fine-tune on a custom VQA task that allows controlled\nevaluation and diagnosis. We compare energy-based models (EBMs) with\ncontrastive learning (CL). While EBMs are growing in popularity, they lack an\nevaluation on downstream tasks. We find that both EBMs and CL can learn\nrepresentations from unlabeled images that enable training a VQA model on very\nlittle annotated data. In a simple setting similar to CLEVR, we find that CL\nrepresentations also improve systematic generalization, and even match the\nperformance of representations from a larger, supervised, ImageNet-pretrained\nmodel. However, we find EBMs to be difficult to train because of instabilities\nand high variability in their results. Although EBMs prove useful for OOD\ndetection, other results on supervised energy-based training and uncertainty\ncalibration are largely negative. Overall, CL currently seems a preferable\noption over EBMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shevchenko_V/0/1/0/all/0/1\">Violetta Shevchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1\">Ehsan Abbasnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_A/0/1/0/all/0/1\">Anthony Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formalizing and Evaluating Requirements of Perception Systems for Automated Vehicles using Spatio-Temporal Perception Logic. (arXiv:2206.14372v1 [cs.RO])","link":"http://arxiv.org/abs/2206.14372","description":"<p>Automated vehicles (AV) heavily depend on robust perception systems. Current\nmethods for evaluating vision systems focus mainly on frame-by-frame\nperformance. Such evaluation methods appear to be inadequate in assessing the\nperformance of a perception subsystem when used within an AV. In this paper, we\npresent a logic -- referred to as Spatio-Temporal Perception Logic (STPL) --\nwhich utilizes both spatial and temporal modalities. STPL enables reasoning\nover perception data using spatial and temporal relations. One major advantage\nof STPL is that it facilitates basic sanity checks on the real-time performance\nof the perception system, even without ground-truth data in some cases. We\nidentify a fragment of STPL which is efficiently monitorable offline in\npolynomial time. Finally, we present a range of specifications for AV\nperception systems to highlight the types of requirements that can be expressed\nand analyzed through offline monitoring with STPL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hekmatnejad_M/0/1/0/all/0/1\">Mohammad Hekmatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoxha_B/0/1/0/all/0/1\">Bardh Hoxha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1\">Jyotirmoy V. Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fainekos_G/0/1/0/all/0/1\">Georgios Fainekos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022. (arXiv:2206.14381v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14381","description":"<p>In this report, we present our approach for EPIC-KITCHENS-100 Multi-Instance\nRetrieval Challenge 2022. We first parse sentences into semantic roles\ncorresponding to verbs and nouns; then utilize self-attentions to exploit\nsemantic role contextualized video features along with textual features via\ntriplet losses in multiple embedding spaces. Our method overpasses the strong\nbaseline in normalized Discounted Cumulative Gain (nDCG), which is more\nvaluable for semantic similarity. Our submission is ranked 3rd for nDCG and\nranked 4th for mAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Satar_B/0/1/0/all/0/1\">Burak Satar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joo Hwee Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C2FTrans: Coarse-to-Fine Transformers for Medical Image Segmentation. (arXiv:2206.14409v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14409","description":"<p>Convolutional neural networks (CNN), the most prevailing architecture for\ndeep-learning based medical image analysis, are still functionally limited by\ntheir intrinsic inductive biases and inadequate receptive fields. Transformer,\nborn to address this issue, has drawn explosive attention in natural language\nprocessing and computer vision due to its remarkable ability in capturing\nlong-range dependency. However, most recent transformer-based methods for\nmedical image segmentation directly apply vanilla transformers as an auxiliary\nmodule in CNN-based methods, resulting in severe detail loss due to the rigid\npatch partitioning scheme in transformers. To address this problem, we propose\nC2FTrans, a novel multi-scale architecture that formulates medical image\nsegmentation as a coarse-to-fine procedure. C2FTrans mainly consists of a\ncross-scale global transformer (CGT) which addresses local contextual\nsimilarity in CNN and a boundary-aware local transformer (BLT) which overcomes\nboundary uncertainty brought by rigid patch partitioning in transformers.\nSpecifically, CGT builds global dependency across three different small-scale\nfeature maps to obtain rich global semantic features with an acceptable\ncomputational cost, while BLT captures mid-range dependency by adaptively\ngenerating windows around boundaries under the guidance of entropy to reduce\ncomputational complexity and minimize detail loss based on large-scale feature\nmaps. Extensive experimental results on three public datasets demonstrate the\nsuperior performance of C2FTrans against state-of-the-art CNN-based and\ntransformer-based methods with fewer parameters and lower FLOPs. We believe the\ndesign of C2FTrans would further inspire future work on developing efficient\nand lightweight transformers for medical image segmentation. The source code of\nthis paper is publicly available at https://github.com/xianlin7/C2FTrans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Li Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning. (arXiv:2206.14413v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14413","description":"<p>Vision transformers have recently set off a new wave in the field of medical\nimage analysis due to their remarkable performance on various computer vision\ntasks. However, recent hybrid-/transformer-based approaches mainly focus on the\nbenefits of transformers in capturing long-range dependency while ignoring the\nissues of their daunting computational complexity, high training costs, and\nredundant dependency. In this paper, we propose to employ adaptive pruning to\ntransformers for medical image segmentation and propose a lightweight and\neffective hybrid network APFormer. To our best knowledge, this is the first\nwork on transformer pruning for medical image analysis tasks. The key features\nof APFormer mainly are self-supervised self-attention (SSA) to improve the\nconvergence of dependency establishment, Gaussian-prior relative position\nembedding (GRPE) to foster the learning of position information, and adaptive\npruning to eliminate redundant computations and perception information.\nSpecifically, SSA and GRPE consider the well-converged dependency distribution\nand the Gaussian heatmap distribution separately as the prior knowledge of\nself-attention and position embedding to ease the training of transformers and\nlay a solid foundation for the following pruning operation. Then, adaptive\ntransformer pruning, both query-wise and dependency-wise, is performed by\nadjusting the gate control parameters for both complexity reduction and\nperformance improvement. Extensive experiments on two widely-used datasets\ndemonstrate the prominent segmentation performance of APFormer against the\nstate-of-the-art methods with much fewer parameters and lower GFLOPs. More\nimportantly, we prove, through ablation studies, that adaptive pruning can work\nas a plug-n-play module for performance improvement on other\nhybrid-/transformer-based methods. Code is available at\nhttps://github.com/xianlin7/APFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Li Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zengqiang Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaNi: Maximizing Mutual Information for Nuclei Cross-Domain Unsupervised Segmentation. (arXiv:2206.14437v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14437","description":"<p>In this work, we propose a mutual information (MI) based unsupervised domain\nadaptation (UDA) method for the cross-domain nuclei segmentation. Nuclei vary\nsubstantially in structure and appearances across different cancer types,\nleading to a drop in performance of deep learning models when trained on one\ncancer type and tested on another. This domain shift becomes even more critical\nas accurate segmentation and quantification of nuclei is an essential\nhistopathology task for the diagnosis/ prognosis of patients and annotating\nnuclei at the pixel level for new cancer types demands extensive effort by\nmedical experts. To address this problem, we maximize the MI between labeled\nsource cancer type data and unlabeled target cancer type data for transferring\nnuclei segmentation knowledge across domains. We use the Jensen-Shanon\ndivergence bound, requiring only one negative pair per positive pair for MI\nmaximization. We evaluate our set-up for multiple modeling frameworks and on\ndifferent datasets comprising of over 20 cancer-type domain shifts and\ndemonstrate competitive performance. All the recently proposed approaches\nconsist of multiple components for improving the domain adaptation, whereas our\nproposed module is light and can be easily incorporated into other methods\n(Implementation: https://github.com/YashSharma/MaNi ).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Sana Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1\">Donald E. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SRCN3D: Sparse R-CNN 3D Surround-View Camera Object Detection and Tracking for Autonomous Driving. (arXiv:2206.14451v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14451","description":"<p>Detection And Tracking of Moving Objects (DATMO) is an essential component in\nenvironmental perception for autonomous driving. While 3D detectors using\nsurround-view cameras are just flourishing, there is a growing tendency of\nusing different transformer-based methods to learn queries in 3D space from 2D\nfeature maps of perspective view. This paper proposes Sparse R-CNN 3D (SRCN3D),\na novel two-stage fully-convolutional mapping pipeline for surround-view camera\ndetection and tracking. SRCN3D adopts a cascade structure with twin-track\nupdate of both fixed number of proposal boxes and proposal latent features.\nProposal boxes are projected to perspective view so as to aggregate Region of\nInterest (RoI) local features. Based on that, proposal features are refined via\na dynamic instance interactive head, which then generates classification and\nthe offsets applied to original bounding boxes. Compared to prior arts, our\nsparse feature sampling module only utilizes local 2D features for adjustment\nof each corresponding 3D proposal box, leading to a complete sparse paradigm.\nThe proposal features and appearance features are both taken in data\nassociation process in a multi-hypotheses 3D multi-object tracking approach.\nExtensive experiments on nuScenes dataset demonstrate the effectiveness of our\nproposed SRCN3D detector and tracker. Code is available at\nhttps://github.com/synsin0/SRCN3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yining Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jingyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diange Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-domain Generalization in Medical Image Segmentation via Test-time Adaptation from Shape Dictionary. (arXiv:2206.14467v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14467","description":"<p>Domain generalization typically requires data from multiple source domains\nfor model learning. However, such strong assumption may not always hold in\npractice, especially in medical field where the data sharing is highly\nconcerned and sometimes prohibitive due to privacy issue. This paper studies\nthe important yet challenging single domain generalization problem, in which a\nmodel is learned under the worst-case scenario with only one source domain to\ndirectly generalize to different unseen target domains. We present a novel\napproach to address this problem in medical image segmentation, which extracts\nand integrates the semantic shape prior information of segmentation that are\ninvariant across domains and can be well-captured even from single domain data\nto facilitate segmentation under distribution shifts. Besides, a test-time\nadaptation strategy with dual-consistency regularization is further devised to\npromote dynamic incorporation of these shape priors under each unseen domain to\nimprove model generalizability. Extensive experiments on two medical image\nsegmentation tasks demonstrate the consistent improvements of our method across\nvarious unseen domains, as well as its superiority over state-of-the-art\napproaches in addressing domain generalization under the worst-case scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quande Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Contrastive Embedding Network for Compositional Zero-Shot Learning. (arXiv:2206.14475v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14475","description":"<p>Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions\nformed from seen state and object during training. Since the same state may be\nvarious in the visual appearance while entangled with different objects, CZSL\nis still a challenging task. Some methods recognize state and object with two\ntrained classifiers, ignoring the impact of the interaction between object and\nstate; the other methods try to learn the joint representation of the\nstate-object compositions, leading to the domain gap between seen and unseen\ncomposition sets. In this paper, we propose a novel Siamese Contrastive\nEmbedding Network (SCEN) (Code: https://github.com/XDUxyLi/SCEN-master) for\nunseen composition recognition. Considering the entanglement between state and\nobject, we embed the visual feature into a Siamese Contrastive Space to capture\nprototypes of them separately, alleviating the interaction between state and\nobject. In addition, we design a State Transition Module (STM) to increase the\ndiversity of training compositions, improving the robustness of the recognition\nmodel. Extensive experiments indicate that our method significantly outperforms\nthe state-of-the-art approaches on three challenging benchmark datasets,\nincluding the recent proposed C-QGA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Cheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muli Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond neural scaling laws: beating power law scaling via data pruning. (arXiv:2206.14486v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14486","description":"<p>Widely observed neural scaling laws, in which error falls off as a power of\nthe training set size, model size, or both, have driven substantial performance\nimprovements in deep learning. However, these improvements through scaling\nalone require considerable costs in compute and energy. Here we focus on the\nscaling of error with dataset size and show how both in theory and practice we\ncan break beyond power law scaling and reduce it to exponential scaling instead\nif we have access to a high-quality data pruning metric that ranks the order in\nwhich training examples should be discarded to achieve any pruned dataset size.\nWe then test this new exponential scaling prediction with pruned dataset size\nempirically, and indeed observe better than power law scaling performance on\nResNets trained on CIFAR-10, SVHN, and ImageNet. Given the importance of\nfinding high-quality pruning metrics, we perform the first large-scale\nbenchmarking study of ten different data pruning metrics on ImageNet. We find\nmost existing high performing metrics scale poorly to ImageNet, while the best\nare computationally intensive and require labels for every image. We therefore\ndeveloped a new simple, cheap and scalable self-supervised pruning metric that\ndemonstrates comparable performance to the best supervised metrics. Overall,\nour work suggests that the discovery of good data-pruning metrics may provide a\nviable path forward to substantially improved neural scaling laws, thereby\nreducing the resource costs of modern deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sorscher_B/0/1/0/all/0/1\">Ben Sorscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shashank Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Surya Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy and Out Distribution Robustness. (arXiv:2206.14502v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14502","description":"<p>We show that the effectiveness of the well celebrated Mixup [Zhang et al.,\n2018] can be further improved if instead of using it as the sole learning\nobjective, it is utilized as an additional regularizer to the standard\ncross-entropy loss. This simple change not only provides much improved accuracy\nbut also significantly improves the quality of the predictive uncertainty\nestimation of Mixup in most cases under various forms of covariate shifts and\nout-of-distribution detection experiments. In fact, we observe that Mixup\nyields much degraded performance on detecting out-of-distribution samples\npossibly, as we show empirically, because of its tendency to learn models that\nexhibit high-entropy throughout; making it difficult to differentiate\nin-distribution samples from out-distribution ones. To show the efficacy of our\napproach (RegMixup), we provide thorough analyses and experiments on vision\ndatasets (ImageNet &amp; CIFAR-10/100) and compare it with a suite of recent\napproaches for reliable uncertainty estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinto_F/0/1/0/all/0/1\">Francesco Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Procrustes Analysis with Deformations: A Closed-Form Solution by Eigenvalue Decomposition. (arXiv:2206.14528v1 [cs.RO])","link":"http://arxiv.org/abs/2206.14528","description":"<p>Generalized Procrustes Analysis (GPA) is the problem of bringing multiple\nshapes into a common reference by estimating transformations. GPA has been\nextensively studied for the Euclidean and affine transformations. We introduce\nGPA with deformable transformations, which forms a much wider and difficult\nproblem. We specifically study a class of transformations called the Linear\nBasis Warps (LBWs), which contains the affine transformation and most of the\nusual deformation models, such as the Thin-Plate Spline (TPS). GPA with\ndeformations is a nonconvex underconstrained problem. We resolve the\nfundamental ambiguities of deformable GPA using two shape constraints requiring\nthe eigenvalues of the shape covariance. These eigenvalues can be computed\nindependently as a prior or posterior. We give a closed-form and optimal\nsolution to deformable GPA based on an eigenvalue decomposition. This solution\nhandles regularization, favoring smooth deformation fields. It requires the\ntransformation model to satisfy a fundamental property of free-translations,\nwhich asserts that the model can implement any translation. We show that this\nproperty fortunately holds true for most common transformation models,\nincluding the affine and TPS models. For the other models, we give another\nclosed-form solution to GPA, which agrees exactly with the first solution for\nmodels with free-translation. We give pseudo-code for computing our solution,\nleading to the proposed DefGPA method, which is fast, globally optimal and\nwidely applicable. We validate our method and compare it to previous work on\nsix diverse 2D and 3D datasets, with special care taken to choose the\nhyperparameters from cross-validation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartoli_A/0/1/0/all/0/1\">Adrien Bartoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"vMFNet: Compositionality Meets Domain-generalised Segmentation. (arXiv:2206.14538v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14538","description":"<p>Training medical image segmentation models usually requires a large amount of\nlabeled data. By contrast, humans can quickly learn to accurately recognise\nanatomy of interest from medical (e.g. MRI and CT) images with some limited\nguidance. Such recognition ability can easily generalise to new images from\ndifferent clinical centres. This rapid and generalisable learning ability is\nmostly due to the compositional structure of image patterns in the human brain,\nwhich is less incorporated in medical image segmentation. In this paper, we\nmodel the compositional components (i.e. patterns) of human anatomy as\nlearnable von-Mises-Fisher (vMF) kernels, which are robust to images collected\nfrom different domains (e.g. clinical centres). The image features can be\ndecomposed to (or composed by) the components with the composing operations,\ni.e. the vMF likelihoods. The vMF likelihoods tell how likely each anatomical\npart is at each position of the image. Hence, the segmentation mask can be\npredicted based on the vMF likelihoods. Moreover, with a reconstruction module,\nunlabeled data can also be used to learn the vMF kernels and likelihoods by\nrecombining them to reconstruct the input image. Extensive experiments show\nthat the proposed vMFNet achieves improved generalisation performance on two\nbenchmarks, especially when annotations are limited. Code is publicly available\nat: https://github.com/vios-s/vMFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pedro Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why patient data cannot be easily forgotten?. (arXiv:2206.14541v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14541","description":"<p>Rights provisioned within data protection regulations, permit patients to\nrequest that knowledge about their information be eliminated by data holders.\nWith the advent of AI learned on data, one can imagine that such rights can\nextent to requests for forgetting knowledge of patient's data within AI models.\nHowever, forgetting patients' imaging data from AI models, is still an\nunder-explored problem. In this paper, we study the influence of patient data\non model performance and formulate two hypotheses for a patient's data: either\nthey are common and similar to other patients or form edge cases, i.e. unique\nand rare cases. We show that it is not possible to easily forget patient data.\nWe propose a targeted forgetting approach to perform patient-wise forgetting.\nExtensive experiments on the benchmark Automated Cardiac Diagnosis Challenge\ndataset showcase the improved performance of the proposed targeted forgetting\napproach as opposed to a state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruolin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware Panoptic Segmentation. (arXiv:2206.14554v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14554","description":"<p>Reliable scene understanding is indispensable for modern autonomous systems.\nCurrent learning-based methods typically try to maximize their performance\nbased on segmentation metrics that only consider the quality of the\nsegmentation. However, for the safe operation of a system in the real world it\nis crucial to consider the uncertainty in the prediction as well. In this work,\nwe introduce the novel task of uncertainty-aware panoptic segmentation, which\naims to predict per-pixel semantic and instance segmentations, together with\nper-pixel uncertainty estimates. We define two novel metrics to facilitate its\nquantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the\npanoptic Expected Calibration Error (pECE). We further propose the novel\ntop-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.\nOur architecture employs a simple yet effective probabilistic fusion module\nthat leverages the predicted uncertainties. Additionally, we propose a new\nLov\\'asz evidential loss function to optimize the IoU for the segmentation\nutilizing the probabilities provided by deep evidential learning. Furthermore,\nwe provide several strong baselines combining state-of-the-art panoptic\nsegmentation networks with sampling-free uncertainty estimation techniques.\nExtensive evaluations show that our EvPSNet achieves the new state-of-the-art\nfor the standard Panoptic Quality (PQ), as well as for our uncertainty-aware\npanoptic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirohi_K/0/1/0/all/0/1\">Kshitij Sirohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marvi_S/0/1/0/all/0/1\">Sajad Marvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buscher_D/0/1/0/all/0/1\">Daniel B&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technical Report for CVPR 2022 LOVEU AQTC Challenge. (arXiv:2206.14555v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14555","description":"<p>This technical report presents the 2nd winning model for AQTC, a task newly\nintroduced in CVPR 2022 LOng-form VidEo Understanding (LOVEU) challenges. This\nchallenge faces difficulties with multi-step answers, multi-modal, and diverse\nand changing button representations in video. We address this problem by\nproposing a new context ground module attention mechanism for more effective\nfeature mapping. In addition, we also perform the analysis over the number of\nbuttons and ablation study of different step networks and video features. As a\nresult, we achieved the overall 2nd place in LOVEU competition track 3,\nspecifically the 1st place in two out of four evaluation metrics. Our code is\navailable at https://github.com/jaykim9870/ CVPR-22_LOVEU_unipyler.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeonyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jeonghun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sanguk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehwan Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-based Multimodal Curriculum Learning for Medical Report Generation. (arXiv:2206.14579v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14579","description":"<p>Medical report generation task, which targets to produce long and coherent\ndescriptions of medical images, has attracted growing research interests\nrecently. Different from the general image captioning tasks, medical report\ngeneration is more challenging for data-driven neural models. This is mainly\ndue to 1) the serious data bias and 2) the limited medical data. To alleviate\nthe data bias and make best use of available data, we propose a\nCompetence-based Multimodal Curriculum Learning framework (CMCL). Specifically,\nCMCL simulates the learning process of radiologists and optimizes the model in\na step by step manner. Firstly, CMCL estimates the difficulty of each training\ninstance and evaluates the competence of current model; Secondly, CMCL selects\nthe most suitable batch of training instances considering current model\ncompetence. By iterating above two steps, CMCL can gradually improve the\nmodel's performance. The experiments on the public IU-Xray and MIMIC-CXR\ndatasets show that CMCL can be incorporated into existing models to improve\ntheir performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-device Synaptic Memory Consolidation using Fowler-Nordheim Quantum-tunneling. (arXiv:2206.14581v1 [cs.ET])","link":"http://arxiv.org/abs/2206.14581","description":"<p>Synaptic memory consolidation has been heralded as one of the key mechanisms\nfor supporting continual learning in neuromorphic Artificial Intelligence (AI)\nsystems. Here we report that a Fowler-Nordheim (FN) quantum-tunneling device\ncan implement synaptic memory consolidation similar to what can be achieved by\nalgorithmic consolidation models like the cascade and the elastic weight\nconsolidation (EWC) models. The proposed FN-synapse not only stores the\nsynaptic weight but also stores the synapse's historical usage statistic on the\ndevice itself. We also show that the operation of the FN-synapse is\nnear-optimal in terms of the synaptic lifetime and we demonstrate that a\nnetwork comprising FN-synapses outperforms a comparable EWC network for a small\nbenchmark continual learning task. With an energy footprint of femtojoules per\nsynaptic update, we believe that the proposed FN-synapse provides an\nultra-energy-efficient approach for implementing both synaptic memory\nconsolidation and persistent learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mustafizur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bose_S/0/1/0/all/0/1\">Subhankar Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabartty_S/0/1/0/all/0/1\">Shantanu Chakrabartty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective (In)consistency of Paint by Text. (arXiv:2206.14617v1 [cs.GR])","link":"http://arxiv.org/abs/2206.14617","description":"<p>Type \"a sea otter with a pearl earring by Johannes Vermeer\" or \"a photo of a\nteddy bear on a skateboard in Times Square\" into OpenAI's DALL-E-2\npaint-by-text synthesis engine and you will not be disappointed by the\ndelightful and eerily pertinent results. The ability to synthesize highly\nrealistic images -- with seemingly no limitation other than our imagination --\nis sure to yield many exciting and creative applications. These images are also\nlikely to pose new challenges to the photo-forensic community. Motivated by the\nfact that paint by text is not based on explicit geometric modeling, and the\nhuman visual system's often obliviousness to even glaring geometric\ninconsistencies, we provide an initial exploration of the perspective\nconsistency of DALL-E-2 synthesized images to determine if geometric-based\nforensic analyses will prove fruitful in detecting this new breed of synthetic\nmedia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1\">Hany Farid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoT-SORT: Robust Associations Multi-Pedestrian Tracking. (arXiv:2206.14651v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14651","description":"<p>The goal of multi-object tracking (MOT) is detecting and tracking all the\nobjects in a scene, while keeping a unique identifier for each object. In this\npaper, we present a new robust state-of-the-art tracker, which can combine the\nadvantages of motion and appearance information, along with camera-motion\ncompensation, and a more accurate Kalman filter state vector. Our new trackers\nBoT-SORT, and BoT-SORT-ReID rank first in the datasets of MOTChallenge [29, 11]\non both MOT17 and MOT20 test sets, in terms of all the main MOT metrics: MOTA,\nIDF1, and HOTA. For MOT17: 80.5 MOTA, 80.2 IDF1, and 65.0 HOTA are achieved.\nThe source code and the pre-trained models are available at\nhttps://github.com/NirAharon/BOT-SORT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aharon_N/0/1/0/all/0/1\">Nir Aharon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orfaig_R/0/1/0/all/0/1\">Roy Orfaig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobrovsky_B/0/1/0/all/0/1\">Ben-Zion Bobrovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cut Inner Layers: A Structured Pruning Strategy for Efficient U-Net GANs. (arXiv:2206.14658v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14658","description":"<p>Pruning effectively compresses overparameterized models. Despite the success\nof pruning methods for discriminative models, applying them for generative\nmodels has been relatively rarely approached. This study conducts structured\npruning on U-Net generators of conditional GANs. A per-layer sensitivity\nanalysis confirms that many unnecessary filters exist in the innermost layers\nnear the bottleneck and can be substantially pruned. Based on this observation,\nwe prune these filters from multiple inner layers or suggest alternative\narchitectures by completely eliminating the layers. We evaluate our approach\nwith Pix2Pix for image-to-image translation and Wav2Lip for speech-driven\ntalking face generation. Our method outperforms global pruning baselines,\ndemonstrating the importance of properly considering where to prune for U-Net\ngenerators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Bo-Kyeong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Shinkook Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hancheol Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiometryNet: Landmark-based Fetal Biometry Estimation from Standard Ultrasound Planes. (arXiv:2206.14678v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14678","description":"<p>Fetal growth assessment from ultrasound is based on a few biometric\nmeasurements that are performed manually and assessed relative to the expected\ngestational age. Reliable biometry estimation depends on the precise detection\nof landmarks in standard ultrasound planes. Manual annotation can be\ntime-consuming and operator dependent task, and may results in high\nmeasurements variability. Existing methods for automatic fetal biometry rely on\ninitial automatic fetal structure segmentation followed by geometric landmark\ndetection. However, segmentation annotations are time-consuming and may be\ninaccurate, and landmark detection requires developing measurement-specific\ngeometric methods. This paper describes BiometryNet, an end-to-end landmark\nregression framework for fetal biometry estimation that overcomes these\nlimitations. It includes a novel Dynamic Orientation Determination (DOD) method\nfor enforcing measurement-specific orientation consistency during network\ntraining. DOD reduces variabilities in network training, increases landmark\nlocalization accuracy, thus yields accurate and robust biometric measurements.\nTo validate our method, we assembled a dataset of 3,398 ultrasound images from\n1,829 subjects acquired in three clinical sites with seven different ultrasound\ndevices. Comparison and cross-validation of three different biometric\nmeasurements on two independent datasets shows that BiometryNet is robust and\nyields accurate measurements whose errors are lower than the clinically\npermissible errors, outperforming other existing automated biometry estimation\nmethods. Code is available at\nhttps://github.com/netanellavisdris/fetalbiometry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Avisdris_N/0/1/0/all/0/1\">Netanell Avisdris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joskowicz_L/0/1/0/all/0/1\">Leo Joskowicz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dromey_B/0/1/0/all/0/1\">Brian Dromey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peebles_D/0/1/0/all/0/1\">Donald M. Peebles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bashat_D/0/1/0/all/0/1\">Dafna Ben Bashat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Physical Representations for Approximating PDE Solutions with Graph Neural Operators. (arXiv:2206.14687v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14687","description":"<p>Representing physical signals at different scales is among the most\nchallenging problems in engineering. Several multi-scale modeling tools have\nbeen developed to describe physical systems governed by \\emph{Partial\nDifferential Equations} (PDEs). These tools are at the crossroad of principled\nphysical models and numerical schema. Recently, data-driven models have been\nintroduced to speed-up the approximation of PDE solutions compared to numerical\nsolvers. Among these recent data-driven methods, neural integral operators are\na class that learn a mapping between function spaces. These functions are\ndiscretized on graphs (meshes) which are appropriate for modeling interactions\nin physical phenomena. In this work, we study three multi-resolution schema\nwith integral kernel operators that can be approximated with \\emph{Message\nPassing Graph Neural Networks} (MPGNNs). To validate our study, we make\nextensive MPGNNs experiments with well-chosen metrics considering steady and\nunsteady PDEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Migus_L/0/1/0/all/0/1\">L&#xe9;on Migus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazari_J/0/1/0/all/0/1\">Jocelyn Ahmed Mazari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interventional Contrastive Learning with Meta Semantic Regularizer. (arXiv:2206.14702v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14702","description":"<p>Contrastive learning (CL)-based self-supervised learning models learn visual\nrepresentations in a pairwise manner. Although the prevailing CL model has\nachieved great progress, in this paper, we uncover an ever-overlooked\nphenomenon: When the CL model is trained with full images, the performance\ntested in full images is better than that in foreground areas; when the CL\nmodel is trained with foreground areas, the performance tested in full images\nis worse than that in foreground areas. This observation reveals that\nbackgrounds in images may interfere with the model learning semantic\ninformation and their influence has not been fully eliminated. To tackle this\nissue, we build a Structural Causal Model (SCM) to model the background as a\nconfounder. We propose a backdoor adjustment-based regularization method,\nnamely Interventional Contrastive Learning with Meta Semantic Regularizer\n(ICL-MSR), to perform causal intervention towards the proposed SCM. ICL-MSR can\nbe incorporated into any existing CL methods to alleviate background\ndistractions from representation learning. Theoretically, we prove that ICL-MSR\nachieves a tighter error bound. Empirically, our experiments on multiple\nbenchmark datasets demonstrate that ICL-MSR is able to improve the performances\nof different state-of-the-art CL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An extensible Benchmarking Graph-Mesh dataset for studying Steady-State Incompressible Navier-Stokes Equations. (arXiv:2206.14709v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14709","description":"<p>Recent progress in \\emph{Geometric Deep Learning} (GDL) has shown its\npotential to provide powerful data-driven models. This gives momentum to\nexplore new methods for learning physical systems governed by \\emph{Partial\nDifferential Equations} (PDEs) from Graph-Mesh data. However, despite the\nefforts and recent achievements, several research directions remain unexplored\nand progress is still far from satisfying the physical requirements of\nreal-world phenomena. One of the major impediments is the absence of\nbenchmarking datasets and common physics evaluation protocols. In this paper,\nwe propose a 2-D graph-mesh dataset to study the airflow over airfoils at high\nReynolds regime (from $10^6$ and beyond). We also introduce metrics on the\nstress forces over the airfoil in order to evaluate GDL models on important\nphysical quantities. Moreover, we provide extensive GDL baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonnet_F/0/1/0/all/0/1\">Florent Bonnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazari_J/0/1/0/all/0/1\">Jocelyn Ahmed Mazari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Munzer_T/0/1/0/all/0/1\">Thibaut Munzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yser_P/0/1/0/all/0/1\">Pierre Yser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CONVIQT: Contrastive Video Quality Estimator. (arXiv:2206.14713v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14713","description":"<p>Perceptual video quality assessment (VQA) is an integral component of many\nstreaming and video sharing platforms. Here we consider the problem of learning\nperceptually relevant video quality representations in a self-supervised\nmanner. Distortion type identification and degradation level determination is\nemployed as an auxiliary task to train a deep learning model containing a deep\nConvolutional Neural Network (CNN) that extracts spatial features, as well as a\nrecurrent unit that captures temporal information. The model is trained using a\ncontrastive loss and we therefore refer to this training framework and\nresulting model as CONtrastive VIdeo Quality EstimaTor (CONVIQT). During\ntesting, the weights of the trained model are frozen, and a linear regressor\nmaps the learned features to quality scores in a no-reference (NR) setting. We\nconduct comprehensive evaluations of the proposed model on multiple VQA\ndatabases by analyzing the correlations between model predictions and\nground-truth quality ratings, and achieve competitive performance when compared\nto state-of-the-art NR-VQA models, even though it is not trained on those\ndatabases. Our ablation experiments demonstrate that the learned\nrepresentations are highly robust and generalize well across synthetic and\nrealistic distortions. Our results indicate that compelling representations\nwith perceptual bearing can be obtained using self-supervised learning. The\nimplementations used in this work have been made available at\nhttps://github.com/pavancm/CONVIQT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Madhusudana_P/0/1/0/all/0/1\">Pavan C. Madhusudana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LViT: Language meets Vision Transformer in Medical Image Segmentation. (arXiv:2206.14718v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14718","description":"<p>Deep learning has been widely used in medical image segmentation and other\naspects. However, the performance of existing medical image segmentation models\nhas been limited by the challenge of obtaining sufficient number of\nhigh-quality data with the high cost of data annotation. To overcome the\nlimitation, we propose a new vision-language medical image segmentation model\nLViT (Language meets Vision Transformer). In our model, medical text annotation\nis introduced to compensate for the quality deficiency in image data. In\naddition, the text information can guide the generation of pseudo labels to a\ncertain extent and further guarantee the quality of pseudo labels in\nsemi-supervised learning. We also propose the Exponential Pseudo label\nIteration mechanism (EPI) to help extend the semi-supervised version of LViT\nand the Pixel-Level Attention Module (PLAM) to preserve local features of\nimages. In our model, LV (Language-Vision) loss is designed to supervise the\ntraining of unlabeled images using text information directly. To validate the\nperformance of LViT, we construct multimodal medical segmentation datasets\n(image + text) containing pathological images, X-rays,etc. Experimental results\nshow that our proposed LViT has better segmentation performance in both fully\nand semi-supervised conditions. Code and datasets are available at\nhttps://github.com/HUANGLIZI/LViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingde Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">You Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Puyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Dazhou Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dakai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Q/0/1/0/all/0/1\">Qingqi Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GO-Surf: Neural Feature Grid Optimization for Fast, High-Fidelity RGB-D Surface Reconstruction. (arXiv:2206.14735v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14735","description":"<p>We present GO-Surf, a direct feature grid optimization method for accurate\nand fast surface reconstruction from RGB-D sequences. We model the underlying\nscene with a learned hierarchical feature voxel grid that encapsulates\nmulti-level geometric and appearance local information. Feature vectors are\ndirectly optimized such that after being tri-linearly interpolated, decoded by\ntwo shallow MLPs into signed distance and radiance values, and rendered via\nsurface volume rendering, the discrepancy between synthesized and observed\nRGB/depth values is minimized. Our supervision signals -- RGB, depth and\napproximate SDF -- can be obtained directly from input images without any need\nfor fusion or post-processing. We formulate a novel SDF gradient regularization\nterm that encourages surface smoothness and hole filling while maintaining high\nfrequency details. GO-Surf can optimize sequences of $1$-$2$K frames in\n$15$-$45$ minutes, a speedup of $\\times60$ over NeuralRGB-D, the most related\napproach based on an MLP representation, while maintaining on par performance\non standard benchmarks. Project page: https://jingwenwang95.github.io/go_surf/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleja_T/0/1/0/all/0/1\">Tymoteusz Bleja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1\">Lourdes Agapito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Placenta Segmentation in Ultrasound Imaging: Addressing Sources of Uncertainty and Limited Field-of-View. (arXiv:2206.14746v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14746","description":"<p>Automatic segmentation of the placenta in fetal ultrasound (US) is\nchallenging due to the (i) high diversity of placenta appearance, (ii) the\nrestricted quality in US resulting in highly variable reference annotations,\nand (iii) the limited field-of-view of US prohibiting whole placenta assessment\nat late gestation. In this work, we address these three challenges with a\nmulti-task learning approach that combines the classification of placental\nlocation (e.g., anterior, posterior) and semantic placenta segmentation in a\nsingle convolutional neural network. Through the classification task the model\ncan learn from larger and more diverse datasets while improving the accuracy of\nthe segmentation task in particular in limited training set conditions. With\nthis approach we investigate the variability in annotations from multiple\nraters and show that our automatic segmentations (Dice of 0.86 for anterior and\n0.83 for posterior placentas) achieve human-level performance as compared to\nintra- and inter-observer variability. Lastly, our approach can deliver whole\nplacenta segmentation using a multi-view US acquisition pipeline consisting of\nthree stages: multi-probe image acquisition, image fusion and image\nsegmentation. This results in high quality segmentation of larger structures\nsuch as the placenta in US with reduced image artifacts which are beyond the\nfield-of-view of single probes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gomez_A/0/1/0/all/0/1\">Alberto Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Skelton_E/0/1/0/all/0/1\">Emily Skelton</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wright_R/0/1/0/all/0/1\">Robert Wright</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wheeler_G/0/1/0/all/0/1\">Gavin Wheeler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_S/0/1/0/all/0/1\">Shujie Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghavami_N/0/1/0/all/0/1\">Nooshin Ghavami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lloyd_K/0/1/0/all/0/1\">Karen Lloyd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hajnal_J/0/1/0/all/0/1\">Joseph V. Hajnal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-Aware Video Generation. (arXiv:2206.14797v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14797","description":"<p>Generative models have emerged as an essential building block for many image\nsynthesis and editing tasks. Recent advances in this field have also enabled\nhigh-quality 3D or video content to be generated that exhibits either\nmulti-view or temporal consistency. With our work, we explore 4D generative\nadversarial networks (GANs) that learn unconditional generation of 3D-aware\nvideos. By combining neural implicit representations with time-aware\ndiscriminator, we develop a GAN framework that synthesizes 3D video supervised\nonly with monocular videos. We show that our method learns a rich embedding of\ndecomposable 3D structures and motions that enables new visual effects of\nspatio-temporal renderings while producing imagery with quality comparable to\nthat of existing 3D or video GANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1\">Sherwin Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeong Joon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paschalidou_D/0/1/0/all/0/1\">Despoina Paschalidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoloGAN: Multi-domain Multimodal Unpaired Image-to-Image Translation via a Single Generative Adversarial Network. (arXiv:2008.01681v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.01681","description":"<p>Despite significant advances in image-to-image (I2I) translation with\ngenerative adversarial networks (GANs), it remains challenging to effectively\ntranslate an image to a set of diverse images in multiple target domains using\na single pair of generator and discriminator. Existing I2I translation methods\nadopt multiple domain-specific content encoders for different domains, where\neach domain-specific content encoder is trained with images from the same\ndomain only. Nevertheless, we argue that the content (domain-invariance)\nfeatures should be learned from images among all of the domains. Consequently,\neach domain-specific content encoder of existing schemes fails to extract the\ndomain-invariant features efficiently. To address this issue, we present a\nflexible and general SoloGAN model for efficient multimodal I2I translation\namong multiple domains with unpaired data. In contrast to existing methods, the\nSoloGAN algorithm uses a single projection discriminator with an additional\nauxiliary classifier and shares the encoder and generator for all domains.\nConsequently, the SoloGAN can be trained effectively with images from all\ndomains such that the domain-invariance content representation can be\nefficiently extracted. Qualitative and quantitative results over a wide range\nof datasets against several counterparts and variants of the SoloGAN\ndemonstrate the merits of the method, especially for challenging I2I\ntranslation datasets, i.e., datasets involving extreme shape variations or need\nto keep the complex backgrounds unchanged after translations. Furthermore, we\ndemonstrate the contribution of each component in SoloGAN by ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shihua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Cheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zoom-to-Inpaint: Image Inpainting with High-Frequency Details. (arXiv:2012.09401v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.09401","description":"<p>Although deep learning has enabled a huge leap forward in image inpainting,\ncurrent methods are often unable to synthesize realistic high-frequency\ndetails. In this paper, we propose applying super-resolution to coarsely\nreconstructed outputs, refining them at high resolution, and then downscaling\nthe output to the original resolution. By introducing high-resolution images to\nthe refinement network, our framework is able to reconstruct finer details that\nare usually smoothed out due to spectral bias - the tendency of neural networks\nto reconstruct low frequencies better than high frequencies. To assist training\nthe refinement network on large upscaled holes, we propose a progressive\nlearning technique in which the size of the missing regions increases as\ntraining progresses. Our zoom-in, refine and zoom-out strategy, combined with\nhigh-resolution supervision and progressive learning, constitutes a\nframework-agnostic approach for enhancing high-frequency details that can be\napplied to any CNN-based inpainting method. We provide qualitative and\nquantitative evaluations along with an ablation analysis to show the\neffectiveness of our approach. This seemingly simple, yet powerful approach,\noutperforms state-of-the-art inpainting methods. Our code is available in\nhttps://github.com/google/zoom-to-inpaint\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo Ye Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_N/0/1/0/all/0/1\">Nori Kanazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1\">Rahul Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhwa_N/0/1/0/all/0/1\">Neal Wadhwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karnad_N/0/1/0/all/0/1\">Nikhil Karnad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Munchurl Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liba_O/0/1/0/all/0/1\">Orly Liba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Session Visual SLAM for Illumination Invariant Re-Localization in Indoor Environments. (arXiv:2103.03827v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.03827","description":"<p>For robots navigating using only a camera, illumination changes in indoor\nenvironments can cause re-localization failures during autonomous navigation.\nIn this paper, we present a multi-session visual SLAM approach to create a map\nmade of multiple variations of the same locations in different illumination\nconditions. The multi-session map can then be used at any hour of the day for\nimproved re-localization capability. The approach presented is independent of\nthe visual features used, and this is demonstrated by comparing re-localization\nperformance between multi-session maps created using the RTAB-Map library with\nSURF, SIFT, BRIEF, BRISK, KAZE, DAISY and SuperPoint visual features. The\napproach is tested on six mapping and six localization sessions recorded at 30\nminute intervals during sunset using a Google Tango phone in a real apartment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labbe_M/0/1/0/all/0/1\">Mathieu Labb&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michaud_F/0/1/0/all/0/1\">Fran&#xe7;ois Michaud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMoGAN: continuous model-guided image-to-image translation. (arXiv:2103.06879v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.06879","description":"<p>CoMoGAN is a continuous GAN relying on the unsupervised reorganization of the\ntarget data on a functional manifold. To that matter, we introduce a new\nFunctional Instance Normalization layer and residual mechanism, which together\ndisentangle image content from position on target manifold. We rely on naive\nphysics-inspired models to guide the training while allowing private\nmodel/translations features. CoMoGAN can be used with any GAN backbone and\nallows new types of image translation, such as cyclic image translation like\ntimelapse generation, or detached linear translation. On all datasets, it\noutperforms the literature. Our code is available at\n<a href=\"http://github.com/cv-rits/CoMoGAN\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-based Distance Decomposition for Monocular 3D Object Detection. (arXiv:2104.03775v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03775","description":"<p>Monocular 3D object detection is of great significance for autonomous driving\nbut remains challenging. The core challenge is to predict the distance of\nobjects in the absence of explicit depth information. Unlike regressing the\ndistance as a single variable in most existing methods, we propose a novel\ngeometry-based distance decomposition to recover the distance by its factors.\nThe decomposition factors the distance of objects into the most representative\nand stable variables, i.e. the physical height and the projected visual height\nin the image plane. Moreover, the decomposition maintains the self-consistency\nbetween the two heights, leading to robust distance prediction when both\npredicted heights are inaccurate. The decomposition also enables us to trace\nthe causes of the distance uncertainty for different scenarios. Such\ndecomposition makes the distance prediction interpretable, accurate, and\nrobust. Our method directly predicts 3D bounding boxes from RGB images with a\ncompact architecture, making the training and inference simple and efficient.\nThe experimental results show that our method achieves the state-of-the-art\nperformance on the monocular 3D Object Detection and Birds Eye View tasks of\nthe KITTI dataset, and can generalize to images with different camera\nintrinsics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xuepeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaozhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuangrong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae-Kyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead. (arXiv:2105.09121v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.09121","description":"<p>Deploying deep learning models in time-critical applications with limited\ncomputational resources, for instance in edge computing systems and IoT\nnetworks, is a challenging task that often relies on dynamic inference methods\nsuch as early exiting. In this paper, we introduce a novel architecture for\nearly exiting based on the vision transformer architecture, as well as a\nfine-tuning strategy that significantly increase the accuracy of early exit\nbranches compared to conventional approaches while introducing less overhead.\nThrough extensive experiments on image and audio classification as well as\naudiovisual crowd counting, we show that our method works for both\nclassification and regression problems, and in both single- and multi-modal\nsettings. Additionally, we introduce a novel method for integrating audio and\nvisual modalities within early exits in audiovisual data analysis, that can\nlead to a more fine-grained dynamic inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1\">Arian Bakhtiarnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Latent Space of Autoencoders with Interventional Assays. (arXiv:2106.16091v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.16091","description":"<p>Autoencoders exhibit impressive abilities to embed the data manifold into a\nlow-dimensional latent space, making them a staple of representation learning\nmethods. However, without explicit supervision, which is often unavailable, the\nrepresentation is usually uninterpretable, making analysis and principled\nprogress challenging. We propose a framework, called latent responses, which\nexploits the locally contractive behavior exhibited by variational autoencoders\nto explore the learned manifold. More specifically, we develop tools to probe\nthe representation using interventions in the latent space to quantify the\nrelationships between latent variables. We extend the notion of disentanglement\nto take the learned generative process into account and consequently avoid the\nlimitations of existing metrics that may rely on spurious correlations. Our\nanalyses underscore the importance of studying the causal structure of the\nrepresentation to improve performance on downstream tasks such as generation,\ninterpolation, and inference of the factors of variation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besserve_M/0/1/0/all/0/1\">Michel Besserve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOVE: Learning Deformable 3D Objects by Watching Videos. (arXiv:2107.10844v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10844","description":"<p>Learning deformable 3D objects from 2D images is often an ill-posed problem.\nExisting methods rely on explicit supervision to establish multi-view\ncorrespondences, such as template shape models and keypoint annotations, which\nrestricts their applicability on objects \"in the wild\". A more natural way of\nestablishing correspondences is by watching videos of objects moving around. In\nthis paper, we present DOVE, a method that learns textured 3D models of\ndeformable object categories from monocular videos available online, without\nkeypoint, viewpoint or template shape supervision. By resolving\nsymmetry-induced pose ambiguities and leveraging temporal correspondences in\nvideos, the model automatically learns to factor out 3D shape, articulated pose\nand texture from each individual RGB frame, and is ready for single-image\ninference at test time. In the experiments, we show that existing methods fail\nto learn sensible 3D shapes without additional keypoint or template\nsupervision, whereas our method produces temporally consistent 3D models, which\ncan be animated and rendered from arbitrary viewpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakab_T/0/1/0/all/0/1\">Tomas Jakab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1\">Christian Rupprecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-informed Guided Disentanglement in Generative Networks. (arXiv:2107.14229v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14229","description":"<p>Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), lowering altogether the translation quality, controllability and\nvariability. In this paper, we build upon collection of simple physics models\nand present a comprehensive method for disentangling visual traits in target\nimages, guiding the process with a physical model that renders some of the\ntarget traits, and learning the remaining ones. Because it allows explicit and\ninterpretable outputs, our physical models (optimally regressed on target)\nallows generating unseen scenarios in a controllable manner. We also extend our\nframework, showing versatility to neural-guided disentanglement. The results\nshow our disentanglement strategies dramatically increase performances\nqualitatively and quantitatively in several challenging scenarios for image\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-imaging real-time detection and tracking of fast-moving objects using a single-pixel detector. (arXiv:2108.06009v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.06009","description":"<p>Detection and tracking of fast-moving objects have widespread utility in many\nfields. However, fulfilling this demand for fast and efficient detecting and\ntracking using image-based techniques is problematic, owing to the complex\ncalculations and limited data processing capabilities. To tackle this problem,\nwe propose an image-free method to achieve real-time detection and tracking of\nfast-moving objects. It employs the Hadamard pattern to illuminate the\nfast-moving object by a spatial light modulator, in which the resulting light\nsignal is collected by a single-pixel detector. The single-pixel measurement\nvalues are directly used to reconstruct the position information without image\nreconstruction. Furthermore, a new sampling method is used to optimize the\npattern projection way for achieving an ultra-low sampling rate. Compared with\nthe state-of-the-art methods, our approach is not only capable of handling\nreal-time detection and tracking, but also it has a small amount of calculation\nand high efficiency. We experimentally demonstrate that the proposed method,\nusing a 22kHz digital micro-mirror device, can implement a 105fps frame rate at\na 1.28% sampling rate when tracked. Our method breaks through the traditional\ntracking ways, which can implement the object real-time tracking without image\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_F/0/1/0/all/0/1\">Fengming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Image Region Mining with Region Prototypical Network for Weakly Supervised Segmentation. (arXiv:2108.07413v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07413","description":"<p>Weakly supervised image segmentation trained with image-level labels usually\nsuffers from inaccurate coverage of object areas during the generation of the\npseudo groundtruth. This is because the object activation maps are trained with\nthe classification objective and lack the ability to generalize. To improve the\ngenerality of the objective activation maps, we propose a region prototypical\nnetwork RPNet to explore the cross-image object diversity of the training set.\nSimilar object parts across images are identified via region feature\ncomparison. Object confidence is propagated between regions to discover new\nobject areas while background regions are suppressed. Experiments show that the\nproposed method generates more complete and accurate pseudo object masks, while\nachieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. In\naddition, we investigate the robustness of the proposed method on reduced\ntraining sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangfei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Segmentation with Optimal Transport Matching and Message Flow. (arXiv:2108.08518v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08518","description":"<p>We tackle the challenging task of few-shot segmentation in this work. It is\nessential for few-shot semantic segmentation to fully utilize the support\ninformation. Previous methods typically adopt masked average pooling over the\nsupport feature to extract the support clues as a global vector, usually\ndominated by the salient part and lost certain essential clues. In this work,\nwe argue that every support pixel's information is desired to be transferred to\nall query pixels and propose a Correspondence Matching Network (CMNet) with an\nOptimal Transport Matching module to mine out the correspondence between the\nquery and support images. Besides, it is critical to fully utilize both local\nand global information from the annotated support images. To this end, we\npropose a Message Flow module to propagate the message along the inner-flow\ninside the same image and cross-flow between support and query images, which\ngreatly helps enhance the local feature representations. Experiments on PASCAL\nVOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new\nstate-of-the-art few-shot segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_T/0/1/0/all/0/1\">Tzu-Yi Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays. (arXiv:2109.14187v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.14187","description":"<p>Deep learning has shown recent success in classifying anomalies in chest\nx-rays, but datasets are still small compared to natural image datasets.\nSupervision of abnormality localization has been shown to improve trained\nmodels, partially compensating for dataset sizes. However, explicitly labeling\nthese anomalies requires an expert and is very time-consuming. We propose a\npotentially scalable method for collecting implicit localization data using an\neye tracker to capture gaze locations and a microphone to capture a dictation\nof a report, imitating the setup of a reading room. The resulting REFLACX\n(Reports and Eye-Tracking Data for Localization of Abnormalities in Chest\nX-rays) dataset was labeled across five radiologists and contains 3,032\nsynchronized sets of eye-tracking data and timestamped report transcriptions\nfor 2,616 chest x-rays from the MIMIC-CXR dataset. We also provide auxiliary\nannotations, including bounding boxes around lungs and heart and validation\nlabels consisting of ellipses localizing abnormalities and image-level labels.\nFurthermore, a small subset of the data contains readings from all\nradiologists, allowing for the calculation of inter-rater scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lanfredi_R/0/1/0/all/0/1\">Ricardo Bigolin Lanfredi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Auffermann_W/0/1/0/all/0/1\">William F. Auffermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_J/0/1/0/all/0/1\">Jessica Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_P/0/1/0/all/0/1\">Phuong-Anh T. Duong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drew_T/0/1/0/all/0/1\">Trafton Drew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tasdizen_T/0/1/0/all/0/1\">Tolga Tasdizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranSalNet: Towards perceptually relevant visual saliency prediction. (arXiv:2110.03593v3 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2110.03593","description":"<p>Visual saliency prediction using transformers - Convolutional neural networks\n(CNNs) have significantly advanced computational modelling for saliency\nprediction. However, accurately simulating the mechanisms of visual attention\nin the human cortex remains an academic challenge. It is critical to integrate\nproperties of human vision into the design of CNN architectures, leading to\nperceptually more relevant saliency prediction. Due to the inherent inductive\nbiases of CNN architectures, there is a lack of sufficient long-range\ncontextual encoding capacity. This hinders CNN-based saliency models from\ncapturing properties that emulate viewing behaviour of humans. Transformers\nhave shown great potential in encoding long-range information by leveraging the\nself-attention mechanism. In this paper, we propose a novel saliency model that\nintegrates transformer components to CNNs to capture the long-range contextual\nvisual information. Experimental results show that the transformers provide\nadded value to saliency prediction, enhancing its perceptual relevance in the\nperformance. Our proposed saliency model using transformers has achieved\nsuperior results on public benchmarks and competitions for saliency prediction\nmodels.\n</p>\n<p>The source code of our proposed saliency model TranSalNet is available at:\nhttps://github.com/LJOVO/TranSalNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jianxun Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hanhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_D/0/1/0/all/0/1\">David Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saupe_D/0/1/0/all/0/1\">Dietmar Saupe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hantao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fire Together Wire Together: A Dynamic Pruning Approach with Self-Supervised Mask Prediction. (arXiv:2110.08232v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08232","description":"<p>Dynamic model pruning is a recent direction that allows for the inference of\na different sub-network for each input sample during deployment. However,\ncurrent dynamic methods rely on learning a continuous channel gating through\nregularization by inducing sparsity loss. This formulation introduces\ncomplexity in balancing different losses (e.g task loss, regularization loss).\nIn addition, regularization based methods lack transparent tradeoff\nhyperparameter selection to realize a computational budget. Our contribution is\ntwo-fold: 1) decoupled task and pruning losses. 2) Simple hyperparameter\nselection that enables FLOPs reduction estimation before training. Inspired by\nthe Hebbian theory in Neuroscience: \"neurons that fire together wire together\",\nwe propose to predict a mask to process k filters in a layer based on the\nactivation of its previous layer. We pose the problem as a self-supervised\nbinary classification problem. Each mask predictor module is trained to predict\nif the log-likelihood for each filter in the current layer belongs to the top-k\nactivated filters. The value k is dynamically estimated for each input based on\na novel criterion using the mass of heatmaps. We show experiments on several\nneural architectures, such as VGG, ResNet and MobileNet on CIFAR and ImageNet\ndatasets. On CIFAR, we reach similar accuracy to SOTA methods with 15% and 24%\nhigher FLOPs reduction. Similarly in ImageNet, we achieve lower drop in\naccuracy with up to 13% improvement in FLOPs reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elkerdawy_S/0/1/0/all/0/1\">Sara Elkerdawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1\">Mostafa Elhoushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention W-Net: Improved Skip Connections for better Representations. (arXiv:2110.08811v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.08811","description":"<p>Segmentation of macro and microvascular structures in fundoscopic retinal\nimages plays a crucial role in the detection of multiple retinal and systemic\ndiseases, yet it is a difficult problem to solve. Most neural network\napproaches face several issues such as lack of enough parameters, overfitting\nand/or incompatibility between internal feature-spaces. We propose Attention\nW-Net, a new U-Net based architecture for retinal vessel segmentation to\naddress these problems. In this architecture, we have two main contributions:\nAttention Block and regularisation measures. Our Attention Block uses attention\nbetween encoder and decoder features, resulting in higher compatibility upon\naddition. Our regularisation measures include augmentation and modifications to\nthe ResNet Block used, which greatly prevent overfitting. We observe an F1 and\nAUC of 0.8407 and 0.9833 on the DRIVE and 0.8174 and 0.9865 respectively on the\nCHASE-DB1 datasets - a sizeable improvement over its backbone as well as\ncompetitive performance among contemporary state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mohan_S/0/1/0/all/0/1\">Shikhar Mohan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayantari Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Joint Modelling Based on Hierarchical Transformer for Co-summarization. (arXiv:2112.13478v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13478","description":"<p>Video summarization aims to automatically generate a summary (storyboard or\nvideo skim) of a video, which can facilitate large-scale video retrieval and\nbrowsing. Most of the existing methods perform video summarization on\nindividual videos, which neglects the correlations among similar videos. Such\ncorrelations, however, are also informative for video understanding and video\nsummarization. To address this limitation, we propose Video Joint Modelling\nbased on Hierarchical Transformer (VJMHT) for co-summarization, which takes\ninto consideration the semantic dependencies across videos. Specifically, VJMHT\nconsists of two layers of Transformer: the first layer extracts semantic\nrepresentation from individual shots of similar videos, while the second layer\nperforms shot-level video joint modelling to aggregate cross-video semantic\ninformation. By this means, complete cross-video high-level patterns are\nexplicitly modelled and learned for the summarization of individual videos.\nMoreover, Transformer-based video representation reconstruction is introduced\nto maximize the high-level similarity between the summary and the original\nvideo. Extensive experiments are conducted to verify the effectiveness of the\nproposed modules and the superiority of VJMHT in terms of F-measure and\nrank-based evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haopeng_L/0/1/0/all/0/1\">Li Haopeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiuhong_K/0/1/0/all/0/1\">Ke Qiuhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingming_G/0/1/0/all/0/1\">Gong Mingming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_Z/0/1/0/all/0/1\">Zhang Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Centroid-Encoder: A Nonlinear Model for Feature Selection. (arXiv:2201.12910v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12910","description":"<p>Autoencoders have been widely used as a nonlinear tool for data\ndimensionality reduction. While autoencoders don't utilize the label\ninformation, Centroid-Encoders (CE)\\cite{ghosh2022supervised} use the class\nlabel in their learning process. In this study, we propose a sparse\noptimization using the Centroid-Encoder architecture to determine a minimal set\nof features that discriminate between two or more classes. The resulting\nalgorithm, Sparse Centroid-Encoder (SCE), extracts discriminatory features in\ngroups using a sparsity inducing $\\ell_1$-norm while mapping a point to its\nclass centroid. One key attribute of SCE is that it can extract informative\nfeatures from a multi-modal data set, i.e., data sets whose classes appear to\nhave multiple clusters. The algorithm is applied to a wide variety of real\nworld data sets, including single-cell data, high dimensional biological data,\nimage data, speech data, and accelerometer sensor data. We compared our method\nto various state-of-the-art feature selection techniques, including supervised\nConcrete Autoencoders (SCAE), Feature Selection Network (FsNet), deep feature\nselection (DFS), Stochastic Gate (STG), and LassoNet. We empirically showed\nthat SCE features often produced better classification accuracy than other\nmethods on sequester test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_T/0/1/0/all/0/1\">Tomojit Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_M/0/1/0/all/0/1\">Michael Kirby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Monocular Depth Estimation and Uncertainty Quantification using Classification Approaches for Regression. (arXiv:2202.12369v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12369","description":"<p>Monocular depth is important in many tasks, such as 3D reconstruction and\nautonomous driving. Deep learning based models achieve state-of-the-art\nperformance in this field. A set of novel approaches for estimating monocular\ndepth consists of transforming the regression task into a classification one.\nHowever, there is a lack of detailed descriptions and comparisons for\nClassification Approaches for Regression (CAR) in the community and no in-depth\nexploration of their potential for uncertainty estimation. To this end, this\npaper will introduce a taxonomy and summary of CAR approaches, a new\nuncertainty estimation solution for CAR, and a set of experiments on depth\naccuracy and uncertainty quantification for CAR-based models on KITTI dataset.\nThe experiments reflect the differences in the portability of various CAR\nmethods on two backbones. Meanwhile, the newly proposed method for uncertainty\nestimation can outperform the ensembling method with only one forward\npropagation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldea_E/0/1/0/all/0/1\">Emanuel Aldea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation. (arXiv:2203.15865v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15865","description":"<p>Supervised approaches to 3D pose estimation from single images are remarkably\neffective when labeled data is abundant. However, as the acquisition of\nground-truth 3D labels is labor intensive and time consuming, recent attention\nhas shifted towards semi- and weakly-supervised learning. Generating an\neffective form of supervision with little annotations still poses major\nchallenge in crowded scenes. In this paper we propose to impose multi-view\ngeometrical constraints by means of a weighted differentiable triangulation and\nuse it as a form of self-supervision when no labels are available. We therefore\ntrain a 2D pose estimator in such a way that its predictions correspond to the\nre-projection of the triangulated 3D pose and train an auxiliary network on\nthem to produce the final 3D poses. We complement the triangulation with a\nweighting mechanism that alleviates the impact of noisy predictions caused by\nself-occlusion or occlusion from other subjects. We demonstrate the\neffectiveness of our semi-supervised approach on Human3.6M and MPI-INF-3DHP\ndatasets, as well as on a new multi-view multi-person dataset that features\nocclusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Soumava Kumar Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Citraro_L/0/1/0/all/0/1\">Leonardo Citraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honari_S/0/1/0/all/0/1\">Sina Honari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Generalizable Dexterous Manipulation from Human Grasp Affordance. (arXiv:2204.02320v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.02320","description":"<p>Dexterous manipulation with a multi-finger hand is one of the most\nchallenging problems in robotics. While recent progress in imitation learning\nhas largely improved the sample efficiency compared to Reinforcement Learning,\nthe learned policy can hardly generalize to manipulate novel objects, given\nlimited expert demonstrations. In this paper, we propose to learn dexterous\nmanipulation using large-scale demonstrations with diverse 3D objects in a\ncategory, which are generated from a human grasp affordance model. This\ngeneralizes the policy to novel object instances within the same category. To\ntrain the policy, we propose a novel imitation learning objective jointly with\na geometric representation learning objective using our demonstrations. By\nexperimenting with relocating diverse objects in simulation, we show that our\napproach outperforms baselines with a large margin when manipulating novel\nobjects. We also ablate the importance on 3D object representation learning for\nmanipulation. We include videos, code, and additional information on the\nproject website - https://kristery.github.io/ILAD/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueh-Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis. (arXiv:2204.03804v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.03804","description":"<p>Generating multi-contrasts/modal MRI of the same anatomy enriches diagnostic\ninformation but is limited in practice due to excessive data acquisition time.\nIn this paper, we propose a novel deep-learning model for joint reconstruction\nand synthesis of multi-modal MRI using incomplete k-space data of several\nsource modalities as inputs. The output of our model includes reconstructed\nimages of the source modalities and high-quality image synthesized in the\ntarget modality. Our proposed model is formulated as a variational problem that\nleverages several learnable modality-specific feature extractors and a\nmultimodal synthesis module. We propose a learnable optimization algorithm to\nsolve this model, which induces a multi-phase network whose parameters can be\ntrained using multi-modal MRI data. Moreover, a bilevel-optimization framework\nis employed for robust parameter training. We demonstrate the effectiveness of\nour approach using extensive numerical experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bian_W/0/1/0/all/0/1\">Wanyu Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingchao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_X/0/1/0/all/0/1\">Xiaojing Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning. (arXiv:2204.08499v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.08499","description":"<p>Coreset selection, which aims to select a subset of the most informative\ntraining samples, is a long-standing learning problem that can benefit many\ndownstream tasks such as data-efficient learning, continual learning, neural\narchitecture search, active learning, etc. However, many existing coreset\nselection methods are not designed for deep learning, which may have high\ncomplexity and poor generalization performance. In addition, the recently\nproposed methods are evaluated on models, datasets, and settings of different\ncomplexities. To advance the research of coreset selection in deep learning, we\ncontribute a comprehensive code library, namely DeepCore, and provide an\nempirical study on popular coreset selection methods on CIFAR10 and ImageNet\ndatasets. Extensive experiments on CIFAR10 and ImageNet datasets verify that,\nalthough various methods have advantages in certain experiment settings, random\nselection is still a strong baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yanbing Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Copy Motion From One to Another: Fake Motion Video Generation. (arXiv:2205.01373v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01373","description":"<p>One compelling application of artificial intelligence is to generate a video\nof a target person performing arbitrary desired motion (from a source person).\nWhile the state-of-the-art methods are able to synthesize a video demonstrating\nsimilar broad stroke motion details, they are generally lacking in texture\ndetails. A pertinent manifestation appears as distorted face, feet, and hands,\nand such flaws are very sensitively perceived by human observers. Furthermore,\ncurrent methods typically employ GANs with a L2 loss to assess the authenticity\nof the generated videos, inherently requiring a large amount of training\nsamples to learn the texture details for adequate video generation. In this\nwork, we tackle these challenges from three aspects: 1) We disentangle each\nvideo frame into foreground (the person) and background, focusing on generating\nthe foreground to reduce the underlying dimension of the network output. 2) We\npropose a theoretically motivated Gromov-Wasserstein loss that facilitates\nlearning the mapping from a pose to a foreground image. 3) To enhance texture\ndetails, we encode facial features with geometric guidance and employ local\nGANs to refine the face, feet, and hands. Extensive experiments show that our\nmethod is able to generate realistic target person videos, faithfully copying\ncomplex motions from a source person. Our code and datasets are released at\nhttps://github.com/Sifann/FakeMotion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chejian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided Diffusion Model for Adversarial Purification. (arXiv:2205.14969v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14969","description":"<p>With wider application of deep neural networks (DNNs) in various algorithms\nand frameworks, security threats have become one of the concerns. Adversarial\nattacks disturb DNN-based image classifiers, in which attackers can\nintentionally add imperceptible adversarial perturbations on input images to\nfool the classifiers. In this paper, we propose a novel purification approach,\nreferred to as guided diffusion model for purification (GDMP), to help protect\nclassifiers from adversarial attacks. The core of our approach is to embed\npurification into the diffusion denoising process of a Denoised Diffusion\nProbabilistic Model (DDPM), so that its diffusion process could submerge the\nadversarial perturbations with gradually added Gaussian noises, and both of\nthese noises can be simultaneously removed following a guided denoising\nprocess. On our comprehensive experiments across various datasets, the proposed\nGDMP is shown to reduce the perturbations raised by adversarial attacks to a\nshallow range, thereby significantly improving the correctness of\nclassification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under\nPGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on\nthe challenging ImageNet dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongfei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Building Damage Assessment from Large-scale xBD Satellite Imagery Benchmark Datasets. (arXiv:2205.15688v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15688","description":"<p>In the field of post-disaster assessment, for timely and accurate rescue and\nlocalization after a disaster, people need to know the location of damaged\nbuildings. In deep learning, some scholars have proposed methods to make\nautomatic and highly accurate building damage assessments by remote sensing\nimages, which are proved to be more efficient than assessment by domain\nexperts. However, due to the lack of a large amount of labeled data, these\nkinds of tasks can suffer from being able to do an accurate assessment, as the\nefficiency of deep learning models relies highly on labeled data. Although\nexisting semi-supervised and unsupervised studies have made breakthroughs in\nthis area, none of them has completely solved this problem. Therefore, we\npropose adopting a self-supervised comparative learning approach to address the\ntask without the requirement of labeled data. We constructed a novel asymmetric\ntwin network architecture and tested its performance on the xBD dataset.\nExperiment results of our model show the improvement compared to baseline and\ncommonly used methods. We also demonstrated the potential of self-supervised\nmethods for building damage recognition awareness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zaishuo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yanbing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jinze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adriano_B/0/1/0/all/0/1\">Bruno Adriano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays. (arXiv:2206.03935v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.03935","description":"<p>Chest X-ray (CXR) is the most typical radiological exam for diagnosis of\nvarious diseases. Due to the expensive and time-consuming annotations,\ndetecting anomalies in CXRs in an unsupervised fashion is very promising.\nHowever, almost all of the existing methods consider anomaly detection as a\none-class classification (OCC) problem. They model the distribution of only\nknown normal images during training and identify the samples not conforming to\nnormal profile as anomalies in the testing phase. A large number of unlabeled\nimages containing anomalies are thus ignored in the training phase, although\nthey are easy to obtain in clinical practice. In this paper, we propose a novel\nstrategy, Dual-distribution Discrepancy for Anomaly Detection (DDAD), utilizing\nboth known normal images and unlabeled images. The proposed method consists of\ntwo modules. During training, one module takes both known normal and unlabeled\nimages as inputs, capturing anomalous features from unlabeled images in some\nway, while the other one models the distribution of only known normal images.\nSubsequently, inter-discrepancy between the two modules, and intra-discrepancy\ninside the module that is trained on only normal images are designed as anomaly\nscores to indicate anomalies. Experiments on three CXR datasets demonstrate\nthat the proposed DDAD achieves consistent, significant gains and outperforms\nstate-of-the-art methods. Code is available at\nhttps://github.com/caiyu6666/DDAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yu Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing. (arXiv:2206.04310v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.04310","description":"<p>Certified defenses such as randomized smoothing have shown promise towards\nbuilding reliable machine learning systems against $\\ell_p$-norm bounded\nattacks. However, existing methods are insufficient or unable to provably\ndefend against semantic transformations, especially those without closed-form\nexpressions (such as defocus blur and pixelate), which are more common in\npractice and often unrestricted. To fill up this gap, we propose generalized\nrandomized smoothing (GSmooth), a unified theoretical framework for certifying\nrobustness against general semantic transformations via a novel dimension\naugmentation strategy. Under the GSmooth framework, we present a scalable\nalgorithm that uses a surrogate image-to-image network to approximate the\ncomplex transformation. The surrogate model provides a powerful tool for\nstudying the properties of semantic transformations and certifying robustness.\nExperimental results on several datasets demonstrate the effectiveness of our\napproach for robustness certification against multiple kinds of semantic\ntransformations and corruptions, which is not achievable by the alternative\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhongkai Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1\">Chengyang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpA-Former: Transformer image shadow detection and removal via spatial attention. (arXiv:2206.10910v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.10910","description":"<p>In this paper, we propose an end-to-end SpA-Former to recover a shadow-free\nimage from a single shaded image. Unlike traditional methods that require two\nsteps for shadow detection and then shadow removal, the SpA-Former unifies\nthese steps into one, which is a one-stage network capable of directly learning\nthe mapping function between shadows and no shadows, it does not require a\nseparate shadow detection. Thus, SpA-former is adaptable to real image\nde-shadowing for shadows projected on different semantic regions. SpA-Former\nconsists of transformer layer and a series of joint Fourier transform residual\nblocks and two-wheel joint spatial attention. The network in this paper is able\nto handle the task while achieving a very fast processing efficiency.\n</p>\n<p>Our code is relased on https://github.com/\nzhangbaijin/Spatial-Transformer-shadow-removal\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chao Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shan Ying Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated GI tract segmentation using deep learning. (arXiv:2206.11048v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.11048","description":"<p>The job of Radiation oncologists is to deliver x-ray beams pointed toward the\ntumor and at the same time avoid the stomach and intestines. With MR-Linacs\n(magnetic resonance imaging and linear accelerator systems), oncologists can\nvisualize the position of the tumor and allow for precise dose according to\ntumor cell presence which can vary from day to day. The current job of\noutlining the position of the stomach and intestines to adjust the X-ray beams\ndirection for the dose delivery to the tumor while avoiding the organs. This is\na time-consuming and labor-intensive process that can easily prolong treatments\nfrom 15 minutes to an hour a day unless deep learning methods can automate the\nsegmentation process. This paper discusses an automated segmentation process\nusing deep learning to make this process faster and allow more patients to get\neffective treatment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sharma_M/0/1/0/all/0/1\">Manhar Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Topology Divergence: a Framework for Comparing Data Manifolds. (arXiv:2106.04024v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2106.04024","description":"<p>We develop a framework for comparing data manifolds, aimed, in particular,\ntowards the evaluation of deep generative models. We describe a novel tool,\nCross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional\nspace, tracks multiscale topology spacial discrepancies between manifolds on\nwhich the distributions are concentrated. Based on the Cross-Barcode, we\nintroduce the Manifold Topology Divergence score (MTop-Divergence) and apply it\nto assess the performance of deep generative models in various domains: images,\n3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,\nCIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate\nthat the MTop-Divergence accurately detects various degrees of mode-dropping,\nintra-mode collapse, mode invention, and image disturbance. Our algorithm\nscales well (essentially linearly) with the increase of the dimension of the\nambient high-dimensional space. It is one of the first TDA-based practical\nmethodologies that can be applied universally to datasets of different sizes\nand dimensions, including the ones on which the most recent GANs in the visual\ndomain are trained. The proposed method is domain agnostic and does not rely on\npre-trained networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1\">Ilya Trofimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotnikov_G/0/1/0/all/0/1\">Grigorii Sotnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimbach_E/0/1/0/all/0/1\">Ekaterina Trimbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1\">Alexander Korotin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1\">Alexander Filippov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}