{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Cross-Platform Difference in Facebook and Text Messages Language Use: Illustrated by Depression Diagnosis. (arXiv:2202.01802v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01802","description":"<p>How does language differ across one's Facebook status updates vs. one's text\nmessages (SMS)? In this study, we show how Facebook and SMS use differs in\npsycho-linguistic characteristics and how these differences drive downstream\nanalyses with an illustration of depression diagnosis. We use a sample of\nconsenting participants who shared Facebook status updates, SMS data, and\nanswered a standard psychological depression screener. We quantify domain\ndifferences using psychologically driven lexical methods and find that language\non Facebook involves more personal concerns, experiences, and content features\nwhile the language in SMS contains more informal and style features. Next, we\nestimate depression from both text domains, using a depression model trained on\nFacebook data, and find a drop in accuracy when predicting self-reported\ndepression assessments from the SMS-based depression estimates. Finally, we\nevaluate a simple domain adaption correction based on words driving the\ncross-platform differences and applied it to the SMS-derived depression\nestimates, resulting in significant improvement in prediction. Our work shows\nthe Facebook vs. SMS difference in language use and suggests the necessity of\ncross-domain adaption for text-based predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1\">Salvatore Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xiangyu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellew_D/0/1/0/all/0/1\">Douglas Bellew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curtis_B/0/1/0/all/0/1\">Brenda Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning with Random-projection Quantizer for Speech Recognition. (arXiv:2202.01855v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01855","description":"<p>We present a simple and effective self-supervised learning approach for\nspeech recognition. The approach learns a model to predict the masked speech\nsignals, in the form of discrete labels generated with a random-projection\nquantizer. In particular the quantizer projects speech inputs with a randomly\ninitialized matrix, and does a nearest-neighbor lookup in a\nrandomly-initialized codebook. Neither the matrix nor the codebook is updated\nduring self-supervised learning. Since the random-projection quantizer is not\ntrained and is separated from the speech recognition model, the design makes\nthe approach flexible and is compatible with universal speech recognition\narchitecture. On LibriSpeech our approach achieves similar word-error-rates as\nprevious work using self-supervised learning with non-streaming models, and\nprovides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with\nstreaming models. On multilingual tasks the approach also provides significant\nimprovement over wav2vec 2.0 and w2v-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Aspect-Based Sentiment Analysis. (arXiv:2202.01924v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01924","description":"<p>Aspect-based sentiment analysis (ABSA) typically requires in-domain annotated\ndata for supervised training/fine-tuning. It is a big challenge to scale ABSA\nto a large number of new domains. This paper aims to train a unified model that\ncan perform zero-shot ABSA without using any annotated data for a new domain.\nWe propose a method called contrastive post-training on review Natural Language\nInference (CORN). Later ABSA tasks can be cast into NLI for zero-shot transfer.\nWe evaluate CORN on ABSA tasks, ranging from aspect extraction (AE), aspect\nsentiment classification (ASC), to end-to-end aspect-based sentiment analysis\n(E2E ABSA), which show ABSA can be conducted without any human annotated ABSA\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01993","description":"<p>Visual question answering is the task of answering questions about images. We\nintroduce the VizWiz-VQA-Grounding dataset, the first dataset that visually\ngrounds answers to visual questions asked by people with visual impairments. We\nanalyze our dataset and compare it with five VQA-Grounding datasets to\ndemonstrate what makes it similar and different. We then evaluate the SOTA VQA\nand VQA-Grounding models and demonstrate that current SOTA algorithms often\nfail to identify the correct visual evidence where the answer is located. These\nmodels regularly struggle when the visual evidence occupies a small fraction of\nthe image, for images that are higher quality, as well as for visual questions\nthat require skills in text recognition. The dataset, evaluation server, and\nleaderboard all can be found at the following link:\nhttps://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_S/0/1/0/all/0/1\">Samreen Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Scaling Laws in NMT: The Effect of Noise and Architecture. (arXiv:2202.01994v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01994","description":"<p>In this work, we study the effect of varying the architecture and training\ndata quality on the data scaling properties of Neural Machine Translation\n(NMT). First, we establish that the test loss of encoder-decoder transformer\nmodels scales as a power law in the number of training samples, with a\ndependence on the model size. Then, we systematically vary aspects of the\ntraining setup to understand how they impact the data scaling laws. In\nparticular, we change the following (1) Architecture and task setup: We compare\nto a transformer-LSTM hybrid, and a decoder-only transformer with a language\nmodeling loss (2) Noise level in the training distribution: We experiment with\nfiltering, and adding iid synthetic noise. In all the above cases, we find that\nthe data scaling exponents are minimally impacted, suggesting that marginally\nworse architectures or training data can be compensated for by adding more\ndata. Lastly, we find that using back-translated data instead of parallel data,\ncan significantly degrade the scaling exponent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_Y/0/1/0/all/0/1\">Yamini Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ankush Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications. (arXiv:2202.02013v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02013","description":"<p>Automatic text generation based on neural language models has achieved\nperformance levels that make the generated text almost indistinguishable from\nthose written by humans. Despite the value that text generation can have in\nvarious applications, it can also be employed for malicious tasks. The\ndiffusion of such practices represent a threat to the quality of academic\npublishing. To address these problems, we propose in this paper two datasets\ncomprised of artificially generated research content: a completely synthetic\ndataset and a partial text substitution dataset. In the first case, the content\nis completely generated by the GPT-2 model after a short prompt extracted from\noriginal papers. The partial or hybrid dataset is created by replacing several\nsentences of abstracts with sentences that are generated by the Arxiv-NLP\nmodel. We evaluate the quality of the datasets comparing the generated texts to\naligned original texts using fluency metrics such as BLEU and ROUGE. The more\nnatural the artificial texts seem, the more difficult they are to detect and\nthe better is the benchmark. We also evaluate the difficulty of the task of\ndistinguishing original from generated text by using state-of-the-art\nclassification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liyanage_V/0/1/0/all/0/1\">Vijini Liyanage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buscaldi_D/0/1/0/all/0/1\">Davide Buscaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazarenko_A/0/1/0/all/0/1\">Adeline Nazarenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Discourse Influence in Darknet Forums. (arXiv:2202.02081v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02081","description":"<p>This technical report documents our efforts in addressing the tasks set forth\nby the 2021 AMoC (Advanced Modelling of Cyber Criminal Careers) Hackathon. Our\nmain contribution is a joint visualisation of semantic and temporal features,\ngenerating insight into the supplied data on darknet cybercrime through the\naspects of novelty, transience, and resonance, which describe the potential\nimpact a message might have on the overall discourse in darknet communities.\nAll code and data produced by us as part of this hackathon is publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akiki_C/0/1/0/all/0/1\">Christopher Akiki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gienapp_L/0/1/0/all/0/1\">Lukas Gienapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Attention for Language Models. (arXiv:2202.02093v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02093","description":"<p>Pretrained language models based on the transformer architecture have shown\ngreat success in NLP. Textual training data often comes from the web and is\nthus tagged with time-specific information, but most language models ignore\nthis information. They are trained on the textual data alone, limiting their\nability to generalize temporally. In this work, we extend the key component of\nthe transformer architecture, i.e., the self-attention mechanism, and propose\ntemporal attention - a time-aware self-attention mechanism. Temporal attention\ncan be applied to any transformer model and requires the input texts to be\naccompanied with their relevant time points. It allows the transformer to\ncapture this temporal information and create time-specific contextualized word\nrepresentations. We leverage these representations for the task of semantic\nchange detection; we apply our proposed mechanism to BERT and experiment on\nthree datasets in different languages (English, German, and Latin) that also\nvary in time, size, and genre. Our proposed model achieves state-of-the-art\nresults on all the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_G/0/1/0/all/0/1\">Guy D. Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02113","description":"<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKGC/tree/main/GenKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Ecological Footprint of Neural Machine Translation Systems. (arXiv:2202.02170v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02170","description":"<p>Over the past decade, deep learning (DL) has led to significant advancements\nin various fields of artificial intelligence, including machine translation\n(MT). These advancements would not be possible without the ever-growing volumes\nof data and the hardware that allows large DL models to be trained efficiently.\nDue to the large amount of computing cores as well as dedicated memory,\ngraphics processing units (GPUs) are a more effective hardware solution for\ntraining and inference with DL models than central processing units (CPUs).\nHowever, the former is very power demanding. The electrical power consumption\nhas economical as well as ecological implications.\n</p>\n<p>This chapter focuses on the ecological footprint of neural MT systems. It\nstarts from the power drain during the training of and the inference with\nneural MT models and moves towards the environment impact, in terms of carbon\ndioxide emissions. Different architectures (RNN and Transformer) and different\nGPUs (consumer-grate NVidia 1080Ti and workstation-grade NVidia P100) are\ncompared. Then, the overall CO2 offload is calculated for Ireland and the\nNetherlands. The NMT models and their ecological impact are compared to common\nhousehold appliances to draw a more clear picture.\n</p>\n<p>The last part of this chapter analyses quantization, a technique for reducing\nthe size and complexity of models, as a way to reduce power consumption. As\nquantized models can run on CPUs, they present a power-efficient inference\nsolution without depending on a GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sherionov_D/0/1/0/all/0/1\">Dimitar Sherionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?. (arXiv:2202.02268v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02268","description":"<p>To answer this question, we fine-tune transformer-based language models,\nincluding BERT, on different sources of company-related text data for a\nclassification task to predict the one-year stock price performance. We use\nthree different types of text data: News articles, blogs, and annual reports.\nThis allows us to analyze to what extent the performance of language models is\ndependent on the type of the underlying document. StonkBERT, our\ntransformer-based stock performance classifier, shows substantial improvement\nin predictive accuracy compared to traditional language models. The highest\nperformance was achieved with news articles as text source. Performance\nsimulations indicate that these improvements in classification accuracy also\ntranslate into above-average stock market returns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasch_S/0/1/0/all/0/1\">Stefan Pasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehnes_D/0/1/0/all/0/1\">Daniel Ehnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Neural Language Models for Automatic Mobile App User Feedback Answer Generation. (arXiv:2202.02294v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02294","description":"<p>Studies show that developers' answers to the mobile app users' feedbacks on\napp stores can increase the apps' star rating. To help app developers generate\nanswers that are related to the users' issues, recent studies develop models to\ngenerate the answers automatically. Aims: The app response generation models\nuse deep neural networks and require training data. Pre-Trained neural language\nModels (PTM) used in Natural Language Processing (NLP) take advantage of the\ninformation they learned from a large corpora in an unsupervised manner, and\ncan reduce the amount of required training data. In this paper, we evaluate\nPTMs to generate replies to the mobile app user feedbacks. Method: We train a\nTransformer model from scratch and fine-tune two PTMs to evaluate the generated\nresponses, which are compared to RRGEN, a current app response model. We also\nevaluate the models with different portions of the training data. Results: The\nresults on a large dataset evaluated by automatic metrics show that PTMs obtain\nlower scores than the baselines. However, our human evaluation confirms that\nPTMs can generate more relevant and meaningful responses to the posted\nfeedbacks. Moreover, the performance of PTMs has less drop compared to other\nmodels when the amount of training data is reduced to 1/3. Conclusion: PTMs are\nuseful in generating responses to app reviews and are more robust models to the\namount of training data provided. However, the prediction time is 19X than\nRRGEN. This study can provide new avenues for research in adapting the PTMs for\nanalyzing mobile app user feedbacks. Index Terms-mobile app user feedback\nanalysis, neural pre-trained language models, automatic answer generation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1\">Fatemeh H. Fard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Mobile App Navigation with Uncertain or Under-specified Natural Language Commands. (arXiv:2202.02312v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02312","description":"<p>We introduce Mobile app Tasks with Iterative Feedback (MoTIF), a new dataset\nwhere the goal is to complete a natural language query in a mobile app. Current\ndatasets for related tasks in interactive question answering, visual common\nsense reasoning, and question-answer plausibility prediction do not support\nresearch in resolving ambiguous natural language requests or operating in\ndiverse digital domains. As a result, they fail to capture complexities of real\nquestion answering or interactive tasks. In contrast, MoTIF contains natural\nlanguage requests that are not satisfiable, the first such work to investigate\nthis issue for interactive vision-language tasks. MoTIF also contains follow up\nquestions for ambiguous queries to enable research on task uncertainty\nresolution. We introduce task feasibility prediction and propose an initial\nmodel which obtains an F1 score of 61.1. We next benchmark task automation with\nour dataset and find adaptations of prior work perform poorly due to our\nrealistic language requests, obtaining an accuracy of only 20.2% when mapping\ncommands to grounded actions. We analyze performance and gain insight for\nfuture work that may bridge the gap between current model ability and what is\nneeded for successful use in application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arsan_D/0/1/0/all/0/1\">Deniz Arsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sanjna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ranjitha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02317","description":"<p>General purpose vision (GPV) systems are models that are designed to solve a\nwide array of visual tasks without requiring architectural changes. Today, GPVs\nprimarily learn both skills and concepts from large fully supervised datasets.\nScaling GPVs to tens of thousands of concepts by acquiring data to learn each\nconcept for every skill quickly becomes prohibitive. This work presents an\neffective and inexpensive alternative: learn skills from fully supervised\ndatasets, learn concepts from web image search results, and leverage a key\ncharacteristic of GPVs -- the ability to transfer visual knowledge across\nskills. We use a dataset of 1M+ images spanning 10k+ visual concepts to\ndemonstrate webly-supervised concept expansion for two existing GPVs (GPV-1 and\nVL-T5) on 3 benchmarks - 5 COCO based datasets (80 primary concepts), a newly\ncurated series of 5 datasets based on the OpenImages and VisualGenome\nrepositories (~500 concepts) and the Web-derived dataset (10k+ concepts). We\nalso propose a new architecture, GPV-2 that supports a variety of tasks -- from\nvision tasks like classification and localization to vision+language tasks like\nQA and captioning to more niche ones like human-object interaction recognition.\nGPV-2 benefits hugely from web data, outperforms GPV-1 and VL-T5 across these\nbenchmarks, and does well in a 0-shot setting at action and attribute\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Amita Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanmay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolve_E/0/1/0/all/0/1\">Eric Kolve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Incorrectness Logic and Kleene Algebra with Top and Tests. (arXiv:2108.07707v3 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2108.07707","description":"<p>Kleene algebra with tests (KAT) is a foundational equational framework for\nreasoning about programs, which has found applications in program\ntransformations, networking and compiler optimizations, among many other areas.\nIn his seminal work, Kozen proved that KAT subsumes propositional Hoare logic,\nshowing that one can reason about the (partial) correctness of while programs\nby means of the equational theory of KAT. In this work, we investigate the\nsupport that KAT provides for reasoning about incorrectness, instead, as\nembodied by Ohearn's recently proposed incorrectness logic. We show that KAT\ncannot directly express incorrectness logic. The main reason for this\nlimitation can be traced to the fact that KAT cannot express explicitly the\nnotion of codomain, which is essential to express incorrectness triples. To\naddress this issue, we study Kleene Algebra with Top and Tests (TopKAT), an\nextension of KAT with a top element. We show that TopKAT is powerful enough to\nexpress a codomain operation, to express incorrectness triples, and to prove\nall the rules of incorrectness logic sound. This shows that one can reason\nabout the incorrectness of while-like programs by means of the equational\ntheory of TopKAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amorim_A/0/1/0/all/0/1\">Arthur Azevedo de Amorim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering. (arXiv:2109.12264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12264","description":"<p>Textual Question Answering (QA) aims to provide precise answers to user's\nquestions in natural language using unstructured data. One of the most popular\napproaches to this goal is machine reading comprehension(MRC). In recent years,\nmany novel datasets and evaluation metrics based on classical MRC tasks have\nbeen proposed for broader textual QA tasks. In this paper, we survey 47 recent\ntextual QA benchmark datasets and propose a new taxonomy from an application\npoint of view. In addition, We summarize 8 evaluation metrics of textual QA\ntasks. Finally, we discuss current trends in constructing textual QA benchmarks\nand suggest directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-conditioning pre-trained language models. (arXiv:2110.02802v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02802","description":"<p>In this paper we aim to investigate the mechanisms that guide text generation\nwith pre-trained Transformer-based Language Models (TLMs). Grounded on the\nProduct of Experts formulation by Hinton (1999), we describe a generative\nmechanism that exploits expert units which naturally exist in TLMs. Such units\nare responsible for detecting concepts in the input and conditioning text\ngeneration on such concepts. We describe how to identify expert units and how\nto activate them during inference in order to induce any desired concept in the\ngenerated output. We find that the activation of a surprisingly small amount of\nunits is sufficient to steer text generation (as little as 3 units in a model\nwith 345M parameters). While the objective of this work is to learn more about\nhow TLMs work, we show that our method is effective for conditioning without\nfine-tuning or using extra parameters, even on fine-grained homograph concepts.\nAdditionally, we show that our method can be used to correct gender bias\npresent in the output of TLMs and achieves gender parity for all evaluated\ncontexts. We compare our method with FUDGE and PPLM-BoW, and show that our\napproach is able to achieve gender parity at a lower perplexity. The proposed\nmethod is accessible to a wide audience thanks to its simplicity and minimal\ncompute needs. The findings in this paper are a step forward in understanding\nthe generative mechanisms of TLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1\">Xavier Suau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1\">Luca Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phone-to-audio alignment without text: A Semi-supervised Approach. (arXiv:2110.03876v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03876","description":"<p>The task of phone-to-audio alignment has many applications in speech\nresearch. Here we introduce two Wav2Vec2-based models for both text-dependent\nand text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a\nsemi-supervised model, directly learns phone-to-audio alignment through\ncontrastive learning and a forward sum loss, and can be coupled with a\npretrained phone recognizer to achieve text-independent alignment. The other\nmodel, Wav2Vec2-FC, is a frame classification model trained on forced aligned\nlabels that can both perform forced alignment and text-independent\nsegmentation. Evaluation results suggest that both proposed methods, even when\ntranscriptions are not available, generate highly close results to existing\nforced alignment tools. Our work presents a neural pipeline of fully automated\nphone-to-audio alignment. Code and pretrained models are available at\nhttps://github.com/lingjzhu/charsiu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04260","description":"<p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\neasily scale to have outrageously large amounts of parameters without\nsignificant increase in computational cost. However, SAMs are reported to be\nparameter inefficient such that larger models do not always lead to better\nperformance. While most on-going research focuses on improving SAMs models by\nexploring methods of routing inputs to experts, our analysis reveals that such\nresearch might not lead to the solution we expect, i.e., the commonly-used\nrouting methods based on gating mechanisms do not work better than randomly\nrouting inputs to experts. In this paper, we propose a new expert-based model,\nTHOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,\nsuch as the Switch Transformer, experts in THOR are randomly activated for each\ninput during training and inference. THOR models are trained using a\nconsistency regularized loss, where experts learn not only from training data\nbut also from other experts as teachers, such that all the experts make\nconsistent predictions. We validate the effectiveness of THOR on machine\ntranslation tasks. Results show that THOR models are more parameter efficient\nin that they significantly outperform the Transformer and MoE models across\nvarious settings. For example, in multilingual translation, THOR outperforms\nthe Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as\nthat of a state-of-the-art MoE model that is 18 times larger. Our code is\npublicly available at:\nhttps://github.com/microsoft/Stochastic-Mixture-of-Experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Heterogeneous Characteristics of Layers in ASR Models for More Efficient Training. (arXiv:2110.04267v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.04267","description":"<p>Transformer-based architectures have been the subject of research aimed at\nunderstanding their overparameterization and the non-uniform importance of\ntheir layers. Applying these approaches to Automatic Speech Recognition, we\ndemonstrate that the state-of-the-art Conformer models generally have multiple\nambient layers. We study the stability of these layers across runs and model\nsizes, propose that group normalization may be used without disrupting their\nformation, and examine their correlation with model weight updates in each\nlayer. Finally, we apply these findings to Federated Learning in order to\nimprove the training procedure, by targeting Federated Dropout to layers by\nimportance. This allows us to reduce the model size optimized by clients\nwithout quality degradation, and shows potential for future exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lillian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guliani_D/0/1/0/all/0/1\">Dhruv Guliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabel_A/0/1/0/all/0/1\">Andreas Kabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motta_G/0/1/0/all/0/1\">Giovanni Motta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. (arXiv:2201.11990v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11990","description":"<p>Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Shaden Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norick_B/0/1/0/all/0/1\">Brandon Norick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1\">Patrick LeGresley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerveas_G/0/1/0/all/0/1\">George Zerveas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Elton Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Child_R/0/1/0/all/0/1\">Rewon Child</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1\">Reza Yazdani Aminabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julie Bernauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houston_M/0/1/0/all/0/1\">Michael Houston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_S/0/1/0/all/0/1\">Saurabh Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFA: TDNN with Multi-scale Frequency-channel Attention for Text-independent Speaker Verification with Short Utterances. (arXiv:2202.01624v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2202.01624","description":"<p>The time delay neural network (TDNN) represents one of the state-of-the-art\nof neural solutions to text-independent speaker verification. However, they\nrequire a large number of filters to capture the speaker characteristics at any\nlocal frequency region. In addition, the performance of such systems may\ndegrade under short utterance scenarios. To address these issues, we propose a\nmulti-scale frequency-channel attention (MFA), where we characterize speakers\nat different scales through a novel dual-path design which consists of a\nconvolutional neural network and TDNN. We evaluate the proposed MFA on the\nVoxCeleb database and observe that the proposed framework with MFA can achieve\nstate-of-the-art performance while reducing parameters and computation\ncomplexity. Further, the MFA mechanism is found to be effective for speaker\nverification with short test utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rohan Kumar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Predicting the impact of urban change in pedestrian and road safety. (arXiv:2202.01781v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01781","description":"<p>Increased interaction between and among pedestrians and vehicles in the\ncrowded urban environments of today gives rise to a negative side-effect: a\ngrowth in traffic accidents, with pedestrians being the most vulnerable\nelements. Recent work has shown that Convolutional Neural Networks are able to\naccurately predict accident rates exploiting Street View imagery along urban\nroads. The promising results point to the plausibility of aided design of safe\nurban landscapes, for both pedestrians and vehicles. In this paper, by\nconsidering historical accident data and Street View images, we detail how to\nautomatically predict the impact (increase or decrease) of urban interventions\non accident incidence. The results are positive, rendering an accuracies\nranging from 60 to 80%. We additionally provide an interpretability analysis to\nunveil which specific categories of urban features impact accident rates\npositively or negatively. Considering the transportation network substrates\n(sidewalk and road networks) and their demand, we integrate these results to a\ncomplex network framework, to estimate the effective impact of urban change on\nthe safety of pedestrians and vehicles. Results show that public authorities\nmay leverage on machine learning tools to prioritize targeted interventions,\nsince our analysis show that limited improvement is obtained with current\ntools. Further, our findings have a wider application range such as the design\nof safe urban routes for pedestrians or to the field of driver-assistance\ntechnologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bustos_C/0/1/0/all/0/1\">Cristina Bustos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhoads_D/0/1/0/all/0/1\">Daniel Rhoads</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borge_Holthoefer_J/0/1/0/all/0/1\">Javier Borge-Holthoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sole_Ribalta_A/0/1/0/all/0/1\">Albert Sol&#xe9;-Ribalta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retinal Vessel Segmentation with Pixel-wise Adaptive Filters. (arXiv:2202.01782v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01782","description":"<p>Accurate retinal vessel segmentation is challenging because of the complex\ntexture of retinal vessels and low imaging contrast. Previous methods generally\nrefine segmentation results by cascading multiple deep networks, which are\ntime-consuming and inefficient. In this paper, we propose two novel methods to\naddress these challenges. First, we devise a light-weight module, named\nmulti-scale residual similarity gathering (MRSG), to generate pixel-wise\nadaptive filters (PA-Filters). Different from cascading multiple deep networks,\nonly one PA-Filter layer can improve the segmentation results. Second, we\nintroduce a response cue erasing (RCE) strategy to enhance the segmentation\naccuracy. Experimental results on the DRIVE, CHASE_DB1, and STARE datasets\ndemonstrate that our proposed method outperforms state-of-the-art methods while\nmaintaining a compact structure. Code is available at\nhttps://github.com/Limingxing00/Retinal-Vessel-Segmentation-ISBI20222.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mingxing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shenglong Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yueyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oral cancer detection and interpretation: Deep multiple instance learning versus conventional deep single instance learning. (arXiv:2202.01783v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01783","description":"<p>The current medical standard for setting an oral cancer (OC) diagnosis is\nhistological examination of a tissue sample from the oral cavity. This process\nis time consuming and more invasive than an alternative approach of acquiring a\nbrush sample followed by cytological analysis. Skilled cytotechnologists are\nable to detect changes due to malignancy, however, to introduce this approach\ninto clinical routine is associated with challenges such as a lack of experts\nand labour-intensive work. To design a trustworthy OC detection system that\nwould assist cytotechnologists, we are interested in AI-based methods that\nreliably can detect cancer given only per-patient labels (minimizing annotation\nbias), and also provide information on which cells are most relevant for the\ndiagnosis (enabling supervision and understanding). We, therefore, perform a\ncomparison of a conventional single instance learning (SIL) approach and a\nmodern multiple instance learning (MIL) method suitable for OC detection and\ninterpretation, utilizing three different neural network architectures. To\nfacilitate systematic evaluation of the considered approaches, we introduce a\nsynthetic PAP-QMNIST dataset, that serves as a model of OC data, while offering\naccess to per-instance ground truth. Our study indicates that on PAP-QMNIST,\nthe SIL performs better, on average, than the MIL approach. Performance at the\nbag level on real-world cytological data is similar for both methods, yet the\nsingle instance approach performs better on average. Visual examination by\ncytotechnologist indicates that the methods manage to identify cells which\ndeviate from normality, including malignant cells as well as those suspicious\nfor dysplasia. We share the code as open source at\nhttps://github.com/MIDA-group/OralCancerMILvsSIL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Koriakina_N/0/1/0/all/0/1\">Nadezhda Koriakina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sladoje_N/0/1/0/all/0/1\">Nata&#x161;a Sladoje</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basic_V/0/1/0/all/0/1\">Vladimir Ba&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindblad_J/0/1/0/all/0/1\">Joakim Lindblad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Surface Reconstruction from Point Clouds with Visibility Information. (arXiv:2202.01810v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01810","description":"<p>Most current neural networks for reconstructing surfaces from point clouds\nignore sensor poses and only operate on raw point locations. Sensor visibility,\nhowever, holds meaningful information regarding space occupancy and surface\norientation. In this paper, we present two simple ways to augment raw point\nclouds with visibility information, so it can directly be leveraged by surface\nreconstruction networks with minimal adaptation. Our proposed modifications\nconsistently improve the accuracy of generated surfaces as well as the\ngeneralization ability of the networks to unseen shape domains. Our code and\ndata is available at https://github.com/raphaelsulzer/dsrv-data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sulzer_R/0/1/0/all/0/1\">Raphael Sulzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marlet_R/0/1/0/all/0/1\">Renaud Marlet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallet_B/0/1/0/all/0/1\">Bruno Vallet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking. (arXiv:2202.01811v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01811","description":"<p>Object detectors, which are widely deployed in security-critical systems such\nas autonomous vehicles, have been found vulnerable to physical-world patch\nhiding attacks. The attacker can use a single physically-realizable adversarial\npatch to make the object detector miss the detection of victim objects and\ncompletely undermines the functionality of object detection applications. In\nthis paper, we propose ObjectSeeker as a defense framework for building\ncertifiably robust object detectors against patch hiding attacks. The core\noperation of ObjectSeeker is patch-agnostic masking: we aim to mask out the\nentire adversarial patch without any prior knowledge of the shape, size, and\nlocation of the patch. This masking operation neutralizes the adversarial\neffect and allows any vanilla object detector to safely detect objects on the\nmasked images. Remarkably, we develop a certification procedure to determine if\nObjectSeeker can detect certain objects with a provable guarantee against any\nadaptive attacker within the threat model. Our evaluation with two object\ndetectors and three datasets demonstrates a significant (~10%-40% absolute and\n~2-6x relative) improvement in certified robustness over the prior work, as\nwell as high clean performance (~1% performance drop compared with vanilla\nundefended models).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valtchanov_A/0/1/0/all/0/1\">Alexander Valtchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAFE-OCC: A Novelty Detection Framework for Convolutional Neural Network Sensors and its Application in Process Control. (arXiv:2202.01816v1 [math.OC])","link":"http://arxiv.org/abs/2202.01816","description":"<p>We present a novelty detection framework for Convolutional Neural Network\n(CNN) sensors that we call Sensor-Activated Feature Extraction One-Class\nClassification (SAFE-OCC). We show that this framework enables the safe use of\ncomputer vision sensors in process control architectures. Emergent control\napplications use CNN models to map visual data to a state signal that can be\ninterpreted by the controller. Incorporating such sensors introduces a\nsignificant system operation vulnerability because CNN sensors can exhibit high\nprediction errors when exposed to novel (abnormal) visual data. Unfortunately,\nidentifying such novelties in real-time is nontrivial. To address this issue,\nthe SAFE-OCC framework leverages the convolutional blocks of the CNN to create\nan effective feature space to conduct novelty detection using a desired\none-class classification technique. This approach engenders a feature space\nthat directly corresponds to that used by the CNN sensor and avoids the need to\nderive an independent latent space. We demonstrate the effectiveness of\nSAFE-OCC via simulated control environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Pulsipher_J/0/1/0/all/0/1\">Joshua L. Pulsipher</a>, <a href=\"http://arxiv.org/find/math/1/au:+Coutinho_L/0/1/0/all/0/1\">Luke D. J. Coutinho</a>, <a href=\"http://arxiv.org/find/math/1/au:+Soderstrom_T/0/1/0/all/0/1\">Tyler A. Soderstrom</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zavala_V/0/1/0/all/0/1\">Victor M. Zavala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Danish Airs and Grounds: A Dataset for Aerial-to-Street-Level Place Recognition and Localization. (arXiv:2202.01821v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01821","description":"<p>Place recognition and visual localization are particularly challenging in\nwide baseline configurations. In this paper, we contribute with the\n\\emph{Danish Airs and Grounds} (DAG) dataset, a large collection of\nstreet-level and aerial images targeting such cases. Its main challenge lies in\nthe extreme viewing-angle difference between query and reference images with\nconsequent changes in illumination and perspective. The dataset is larger and\nmore diverse than current publicly available data, including more than 50 km of\nroad in urban, suburban and rural areas. All images are associated with\naccurate 6-DoF metadata that allows the benchmarking of visual localization\nmethods.\n</p>\n<p>We also propose a map-to-image re-localization pipeline, that first estimates\na dense 3D reconstruction from the aerial images and then matches query\nstreet-level images to street-level renderings of the 3D model. The dataset can\nbe downloaded at: https://frederikwarburg.github.io/DAG\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vallone_A/0/1/0/all/0/1\">Andrea Vallone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1\">Frederik Warburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_H/0/1/0/all/0/1\">Hans Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1\">S&#xf8;ren Hauberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Javier Civera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRBF-Fusion: Accurate 3D reconstruction from RGB-D data using on-the-fly implicits. (arXiv:2202.01829v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01829","description":"<p>Reconstruction of high-fidelity 3D objects or scenes is a fundamental\nresearch problem. Recent advances in RGB-D fusion have demonstrated the\npotential of producing 3D models from consumer-level RGB-D cameras. However,\ndue to the discrete nature and limited resolution of their surface\nrepresentations (e.g., point- or voxel-based), existing approaches suffer from\nthe accumulation of errors in camera tracking and distortion in the\nreconstruction, which leads to an unsatisfactory 3D reconstruction. In this\npaper, we present a method using on-the-fly implicits of Hermite Radial Basis\nFunctions (HRBFs) as a continuous surface representation for camera tracking in\nan existing RGB-D fusion framework. Furthermore, curvature estimation and\nconfidence evaluation are coherently derived from the inherent surface\nproperties of the on-the-fly HRBF implicits, which devote to a data fusion with\nbetter quality. We argue that our continuous but on-the-fly surface\nrepresentation can effectively mitigate the impact of noise with its robustness\nand constrain the reconstruction with inherent surface smoothness when being\ncompared with discrete representations. Experimental results on various\nreal-world and synthetic datasets demonstrate that our HRBF-fusion outperforms\nthe state-of-the-art approaches in terms of tracking robustness and\nreconstruction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yabin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Laishui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Charlie C.L. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain Cancer Survival Prediction on Treatment-na ive MRI using Deep Anchor Attention Learning with Vision Transformer. (arXiv:2202.01857v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01857","description":"<p>Image-based brain cancer prediction models, based on radiomics, quantify the\nradiologic phenotype from magnetic resonance imaging (MRI). However, these\nfeatures are difficult to reproduce because of variability in acquisition and\npreprocessing pipelines. Despite evidence of intra-tumor phenotypic\nheterogeneity, the spatial diversity between different slices within an MRI\nscan has been relatively unexplored using such methods. In this work, we\npropose a deep anchor attention aggregation strategy with a Vision Transformer\nto predict survival risk for brain cancer patients. A Deep Anchor Attention\nLearning (DAAL) algorithm is proposed to assign different weights to\nslice-level representations with trainable distance measurements. We evaluated\nour method on N = 326 MRIs. Our results outperformed attention multiple\ninstance learning-based techniques. DAAL highlights the importance of critical\nslices and corroborates the clinical intuition that inter-slice spatial\ndiversity can reflect disease severity and is implicated in outcome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xuan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best Practices and Scoring System on Reviewing A.I. based Medical Imaging Papers: Part 1 Classification. (arXiv:2202.01863v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01863","description":"<p>With the recent advances in A.I. methodologies and their application to\nmedical imaging, there has been an explosion of related research programs\nutilizing these techniques to produce state-of-the-art classification\nperformance. Ultimately, these research programs culminate in submission of\ntheir work for consideration in peer reviewed journals. To date, the criteria\nfor acceptance vs. rejection is often subjective; however, reproducible science\nrequires reproducible review. The Machine Learning Education Sub-Committee of\nSIIM has identified a knowledge gap and a serious need to establish guidelines\nfor reviewing these studies. Although there have been several recent papers\nwith this goal, this present work is written from the machine learning\npractitioners standpoint. In this series, the committee will address the best\npractices to be followed in an A.I.-based study and present the required\nsections in terms of examples and discussion of what should be included to make\nthe studies cohesive, reproducible, accurate, and self-contained. This first\nentry in the series focuses on the task of image classification. Elements such\nas dataset curation, data pre-processing steps, defining an appropriate\nreference standard, data partitioning, model architecture and training are\ndiscussed. The sections are presented as they would be detailed in a typical\nmanuscript, with content describing the necessary information that should be\nincluded to make sure the study is of sufficient quality to be considered for\npublication. The goal of this series is to provide resources to not only help\nimprove the review process for A.I.-based medical imaging papers, but to\nfacilitate a standard for the information that is presented within all\ncomponents of the research study. We hope to provide quantitative metrics in\nwhat otherwise may be a qualitative review process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kline_T/0/1/0/all/0/1\">Timothy L. Kline</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kitamura_F/0/1/0/all/0/1\">Felipe Kitamura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_I/0/1/0/all/0/1\">Ian Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korchi_A/0/1/0/all/0/1\">Amine M. Korchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tenenholtz_N/0/1/0/all/0/1\">Neil Tenenholtz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moy_L/0/1/0/all/0/1\">Linda Moy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1\">Judy Wawira Gichoya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Santos_I/0/1/0/all/0/1\">Igor Santos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumer_S/0/1/0/all/0/1\">Steven Blumer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hwang_M/0/1/0/all/0/1\">Misha Ysabel Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Git_K/0/1/0/all/0/1\">Kim-Ann Git</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shroff_A/0/1/0/all/0/1\">Abishek Shroff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walach_E/0/1/0/all/0/1\">Elad Walach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shih_G/0/1/0/all/0/1\">George Shih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Langer_S/0/1/0/all/0/1\">Steve Langer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Organ at Risk Segmentation with Improved Deep Neural Networks. (arXiv:2202.01866v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01866","description":"<p>Organ at risk (OAR) segmentation is a crucial step for treatment planning and\noutcome determination in radiotherapy treatments of cancer patients. Several\ndeep learning based segmentation algorithms have been developed in recent\nyears, however, U-Net remains the de facto algorithm designed specifically for\nbiomedical image segmentation and has spawned many variants with known\nweaknesses. In this study, our goal is to present simple architectural changes\nin U-Net to improve its accuracy and generalization properties. Unlike many\nother available studies evaluating their algorithms on single center data, we\nthoroughly evaluate several variations of U-Net as well as our proposed\nenhanced architecture on multiple data sets for an extensive and reliable study\nof the OAR segmentation problem. Our enhanced segmentation model includes\n(a)architectural changes in the loss function, (b)optimization framework, and\n(c)convolution type. Testing on three publicly available multi-object\nsegmentation data sets, we achieved an average of 80% dice score compared to\nthe baseline U-Net performance of 63%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Isler_I/0/1/0/all/0/1\">Ilkin Isler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lisle_C/0/1/0/all/0/1\">Curtis Lisle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rineer_J/0/1/0/all/0/1\">Justin Rineer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kelly_P/0/1/0/all/0/1\">Patrick Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turgut_D/0/1/0/all/0/1\">Damla Turgut</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ricci_J/0/1/0/all/0/1\">Jacob Ricci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Research on Patch Attentive Neural Process. (arXiv:2202.01884v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01884","description":"<p>Attentive Neural Process (ANP) improves the fitting ability of Neural Process\n(NP) and improves its prediction accuracy, but the higher time complexity of\nthe model imposes a limitation on the length of the input sequence. Inspired by\nmodels such as Vision Transformer (ViT) and Masked Auto-Encoder (MAE), we\npropose Patch Attentive Neural Process (PANP) using image patches as input and\nimprove the structure of deterministic paths based on ANP, which allows the\nmodel to extract image features more accurately and efficiently reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaochen Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances in MetaDL: AAAI 2021 challenge and workshop. (arXiv:2202.01890v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01890","description":"<p>To stimulate advances in metalearning using deep learning techniques\n(MetaDL), we organized in 2021 a challenge and an associated workshop. This\npaper presents the design of the challenge and its results, and summarizes\npresentations made at the workshop. The challenge focused on few-shot learning\nclassification tasks of small images. Participants' code submissions were run\nin a uniform manner, under tight computational constraints. This put pressure\non solution designs to use existing architecture backbones and/or pre-trained\nnetworks. Winning methods featured various classifiers trained on top of the\nsecond last layer of popular CNN backbones, fined-tuned on the meta-training\ndata (not necessarily in an episodic manner), then trained on the labeled\nsupport and tested on the unlabeled query sets of the meta-test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baz_A/0/1/0/all/0/1\">Adrian El Baz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1\">Isabelle Guyon</a> (TAU), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengying Liu</a> (TAU), <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_J/0/1/0/all/0/1\">Jan van Rijn</a> (LIACS), <a href=\"http://arxiv.org/find/cs/1/au:+Treguer_S/0/1/0/all/0/1\">Sebastien Treguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1\">Joaquin Vanschoren</a> (TU/e)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modified ResNet Model for MSI and MSS Classification of Gastrointestinal Cancer. (arXiv:2202.01905v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01905","description":"<p>In this work, a modified ResNet model is proposed for the classification of\nMicrosatellite instability(MSI) and Microsatellite stability(MSS) of\ngastrointestinal cancer. The performance of this model is analyzed and compared\nwith existing models. The proposed model surpassed the existing models with an\naccuracy of 0.8981 and F1 score of 0.9178.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Venkatesh_C/0/1/0/all/0/1\">CH Sai Venkatesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meriga_C/0/1/0/all/0/1\">Caleb Meriga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geethika_M/0/1/0/all/0/1\">M.G.V.L Geethika</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gayatri_T/0/1/0/all/0/1\">T Lakshmi Gayatri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aruna_V/0/1/0/all/0/1\">V.B.K.L Aruna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projection-based Point Convolution for Efficient Point Cloud Segmentation. (arXiv:2202.01991v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01991","description":"<p>Understanding point cloud has recently gained huge interests following the\ndevelopment of 3D scanning devices and the accumulation of large-scale 3D data.\nMost point cloud processing algorithms can be classified as either point-based\nor voxel-based methods, both of which have severe limitations in processing\ntime or memory, or both. To overcome these limitations, we propose\nProjection-based Point Convolution (PPConv), a point convolutional module that\nuses 2D convolutions and multi-layer perceptrons (MLPs) as its components. In\nPPConv, point features are processed through two branches: point branch and\nprojection branch. Point branch consists of MLPs, while projection branch\ntransforms point features into a 2D feature map and then apply 2D convolutions.\nAs PPConv does not use point-based or voxel-based convolutions, it has\nadvantages in fast point cloud processing. When combined with a learnable\nprojection and effective feature fusion strategy, PPConv achieves superior\nefficiency compared to state-of-the-art methods, even with a simple\narchitecture based on PointNet++. We demonstrate the efficiency of PPConv in\nterms of the trade-off between inference time and segmentation performance. The\nexperimental results on S3DIS and ShapeNetPart show that PPConv is the most\nefficient method among the compared ones. The code is available at\ngithub.com/pahn04/PPConv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_P/0/1/0/all/0/1\">Pyunghwan Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Juyoung Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_E/0/1/0/all/0/1\">Eojindl Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chanho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01993","description":"<p>Visual question answering is the task of answering questions about images. We\nintroduce the VizWiz-VQA-Grounding dataset, the first dataset that visually\ngrounds answers to visual questions asked by people with visual impairments. We\nanalyze our dataset and compare it with five VQA-Grounding datasets to\ndemonstrate what makes it similar and different. We then evaluate the SOTA VQA\nand VQA-Grounding models and demonstrate that current SOTA algorithms often\nfail to identify the correct visual evidence where the answer is located. These\nmodels regularly struggle when the visual evidence occupies a small fraction of\nthe image, for images that are higher quality, as well as for visual questions\nthat require skills in text recognition. The dataset, evaluation server, and\nleaderboard all can be found at the following link:\nhttps://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chongyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_S/0/1/0/all/0/1\">Samreen Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Dual Contouring. (arXiv:2202.01999v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01999","description":"<p>We introduce neural dual contouring (NDC), a new data-driven approach to mesh\nreconstruction based on dual contouring (DC). Like traditional DC, it produces\nexactly one vertex per grid cell and one quad for each grid edge intersection,\na natural and efficient structure for reproducing sharp features. However,\nrather than computing vertex locations and edge crossings with hand-crafted\nfunctions that depend directly on difficult-to-obtain surface gradients, NDC\nuses a neural network to predict them. As a result, NDC can be trained to\nproduce meshes from signed or unsigned distance fields, binary voxel grids, or\npoint clouds (with or without normals); and it can produce open surfaces in\ncases where the input represents a sheet or partial surface. During experiments\nwith five prominent datasets, we find that NDC, when trained on one of the\ndatasets, generalizes well to the others. Furthermore, NDC provides better\nsurface reconstruction accuracy, feature preservation, output complexity,\ntriangle quality, and inference time in comparison to previous learned (e.g.,\nneural marching cubes, convolutional occupancy networks) and traditional (e.g.,\nPoisson) methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality Multi-Atlas Segmentation Using Deep Neural Networks. (arXiv:2202.02000v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02000","description":"<p>Multi-atlas segmentation (MAS) is a promising framework for medical image\nsegmentation. Generally, MAS methods register multiple atlases, i.e., medical\nimages with corresponding labels, to a target image; and the transformed atlas\nlabels can be combined to generate target segmentation via label fusion\nschemes. Many conventional MAS methods employed the atlases from the same\nmodality as the target image. However, the number of atlases with the same\nmodality may be limited or even missing in many clinical applications. Besides,\nconventional MAS methods suffer from the computational burden of registration\nor label fusion procedures. In this work, we design a novel cross-modality MAS\nframework, which uses available atlases from a certain modality to segment a\ntarget image from another modality. To boost the computational efficiency of\nthe framework, both the image registration and label fusion are achieved by\nwell-designed deep neural networks. For the atlas-to-target image registration,\nwe propose a bi-directional registration network (BiRegNet), which can\nefficiently align images from different modalities. For the label fusion, we\ndesign a similarity estimation network (SimNet), which estimates the fusion\nweight of each atlas by measuring its similarity to the target image. SimNet\ncan learn multi-scale information for similarity estimation to improve the\nperformance of label fusion. The proposed framework was evaluated by the left\nventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets,\nrespectively. Results have shown that the framework is effective for\ncross-modality MAS in both registration and label fusion. The code will be\nreleased publicly on \\url{https://github.com/NanYoMy/cmmas} once the manuscript\nis accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wangbin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liqin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The devil is in the labels: Semantic segmentation from sentences. (arXiv:2202.02002v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02002","description":"<p>We propose an approach to semantic segmentation that achieves\nstate-of-the-art supervised performance when applied in a zero-shot setting. It\nthus achieves results equivalent to those of the supervised methods, on each of\nthe major semantic segmentation datasets, without training on those datasets.\nThis is achieved by replacing each class label with a vector-valued embedding\nof a short paragraph that describes the class. The generality and simplicity of\nthis approach enables merging multiple datasets from different domains, each\nwith varying class labels and semantics. The resulting merged semantic\nsegmentation dataset of over 2 Million images enables training a model that\nachieves performance equal to that of state-of-the-art supervised methods on 7\nbenchmark datasets, despite not using any images therefrom. By fine-tuning the\nmodel on standard semantic segmentation datasets, we also achieve a significant\nimprovement over the state-of-the-art supervised segmentation on NYUD-V2 and\nPASCAL-context at 60% and 65% mIoU, respectively. Based on the closeness of\nlanguage embeddings, our method can even segment unseen labels. Extensive\nexperiments demonstrate strong generalization to unseen image domains and\nunseen labels, and that the method enables impressive performance improvements\nin downstream applications, including depth estimation and instance\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baichuan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-to-Image MLP-mixer for Image Reconstruction. (arXiv:2202.02018v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02018","description":"<p>Neural networks are highly effective tools for image reconstruction problems\nsuch as denoising and compressive sensing. To date, neural networks for image\nreconstruction are almost exclusively convolutional. The most popular\narchitecture is the U-Net, a convolutional network with a multi-resolution\narchitecture. In this work, we show that a simple network based on the\nmulti-layer perceptron (MLP)-mixer enables state-of-the art image\nreconstruction performance without convolutions and without a multi-resolution\narchitecture, provided that the training set and the size of the network are\nmoderately large. Similar to the original MLP-mixer, the image-to-image\nMLP-mixer is based exclusively on MLPs operating on linearly-transformed image\npatches. Contrary to the original MLP-mixer, we incorporate structure by\nretaining the relative positions of the image patches. This imposes an\ninductive bias towards natural images which enables the image-to-image\nMLP-mixer to learn to denoise images based on fewer examples than the original\nMLP-mixer. Moreover, the image-to-image MLP-mixer requires fewer parameters to\nachieve the same denoising performance than the U-Net and its parameters scale\nlinearly in the image resolution instead of quadratically as for the original\nMLP-mixer. If trained on a moderate amount of examples for denoising, the\nimage-to-image MLP-mixer outperforms the U-Net by a slight margin. It also\noutperforms the vision transformer tailored for image reconstruction and\nclassical un-trained methods such as BM3D, making it a very effective tool for\nimage reconstruction problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1\">Youssef Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckel_R/0/1/0/all/0/1\">Reinhard Heckel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss. (arXiv:2202.02063v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02063","description":"<p>In this paper, we study color image inpainting as a pure quaternion matrix\ncompletion problem. In the literature, the theoretical guarantee for quaternion\nmatrix completion is not well-established. Our main aim is to propose a new\nminimization problem with an objective combining nuclear norm and a quadratic\nloss weighted among three channels. To fill the theoretical vacancy, we obtain\nthe error bound in both clean and corrupted regimes, which relies on some new\nresults of quaternion matrices. A general Gaussian noise is considered in\nrobust completion where all observations are corrupted. Motivated by the error\nbound, we propose to handle unbalanced or correlated noise via a cross-channel\nweight in the quadratic loss, with the main purpose of rebalancing noise level,\nor removing noise correlation. Extensive experimental results on synthetic and\ncolor image data are presented to confirm and demonstrate our theoretical\nfindings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junren Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1\">Michael K. Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CGS-Net: Aggregating Colour, Geometry and Semantic Features for Large-Scale Indoor Place Recognition. (arXiv:2202.02070v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02070","description":"<p>We describe an approach to large-scale indoor place recognition that\naggregates low-level colour and geometric features with high-level semantic\nfeatures. We use a deep learning network that takes in RGB point clouds and\nextracts local features with five 3-D kernel point convolutional (KPConv)\nlayers. We specifically train the KPConv layers on the semantic segmentation\ntask to ensure that the extracted local features are semantically meaningful.\nThen, feature maps from all the five KPConv layers are concatenated together\nand fed into the NetVLAD layer to generate the global descriptors. The approach\nis trained and evaluated using a large-scale indoor place recognition dataset\nderived from the ScanNet dataset, with a test set comprising 3,608 point clouds\ngenerated from 100 different rooms. Comparison with a traditional feature based\nmethod and three state-of-the-art deep learning methods demonstrate that the\napproach significantly outperforms all four methods, achieving, for example, a\ntop-3 average recall rate of 75% compared with 41% for the closest rival\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1\">Yuhang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingrui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1\">Andrew Calway</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heed the Noise in Performance Evaluations in Neural Architecture Search. (arXiv:2202.02078v1 [cs.NE])","link":"http://arxiv.org/abs/2202.02078","description":"<p>Neural Architecture Search (NAS) has recently become a topic of great\ninterest. However, there is a potentially impactful issue within NAS that\nremains largely unrecognized: noise. Due to stochastic factors in neural\nnetwork initialization, training, and the chosen train/validation dataset\nsplit, the performance evaluation of a neural network architecture, which is\noften based on a single learning run, is also stochastic. This may have a\nparticularly large impact if a dataset is small. We therefore propose to reduce\nthe noise by having architecture evaluations comprise averaging of scores over\nmultiple network training runs using different random seeds and\ncross-validation. We perform experiments for a combinatorial optimization\nformulation of NAS in which we vary noise reduction levels. We use the same\ncomputational budget for each noise level in terms of network training runs,\ni.e., we allow less architecture evaluations when averaging over more training\nruns. Multiple search algorithms are considered, including evolutionary\nalgorithms which generally perform well for NAS. We use two publicly available\ndatasets from the medical image segmentation domain where datasets are often\nlimited and variability among samples is often high. Our results show that\nreducing noise in architecture evaluations enables finding better architectures\nby all considered search algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dushatskiy_A/0/1/0/all/0/1\">Arkadiy Dushatskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-Selective Feature Weaving for Point Cloud Matching. (arXiv:2202.02149v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02149","description":"<p>This paper tackles the problem of accurately matching the points of two 3D\npoint clouds. Most conventional methods improve their performance by extracting\nrepresentative features from each point via deep-learning-based algorithms. On\nthe other hand, the correspondence calculation between the extracted features\nhas not been examined in depth, and non-trainable algorithms (e.g. the Sinkhorn\nalgorithm) are frequently applied. As a result, the extracted features may be\nforcibly fitted to a non-trainable algorithm. Furthermore, the extracted\nfeatures frequently contain stochastically unavoidable errors, which degrades\nthe matching accuracy. In this paper, instead of using a non-trainable\nalgorithm, we propose a differentiable matching network that can be jointly\noptimized with the feature extraction procedure. Our network first constructs\ngraphs with edges connecting the points of each point cloud and then extracts\ndiscriminative edge features by using two main components: a shared set-encoder\nand an edge-selective cross-concatenation. These components enable us to\nsymmetrically consider two point clouds and to extract discriminative edge\nfeatures, respectively. By using the extracted discriminative edge features,\nour network can accurately calculate the correspondence between points. Our\nexperimental results show that the proposed network can significantly improve\nthe performance of point cloud matching. Our code is available at\nhttps://github.com/yanarin/ESFW\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yanagi_R/0/1/0/all/0/1\">Rintaro Yanagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1\">Atsushi Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sone_S/0/1/0/all/0/1\">Shusaku Sone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1\">Naoya Chiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1\">Yoshitaka Ushiku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeAT: Neural Adaptive Tomography. (arXiv:2202.02171v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02171","description":"<p>In this paper, we present Neural Adaptive Tomography (NeAT), the first\nadaptive, hierarchical neural rendering pipeline for multi-view inverse\nrendering. Through a combination of neural features with an adaptive explicit\nrepresentation, we achieve reconstruction times far superior to existing neural\ninverse rendering methods. The adaptive explicit representation improves\nefficiency by facilitating empty space culling and concentrating samples in\ncomplex regions, while the neural features act as a neural regularizer for the\n3D reconstruction. The NeAT framework is designed specifically for the\ntomographic setting, which consists only of semi-transparent volumetric scenes\ninstead of opaque objects. In this setting, NeAT outperforms the quality of\nexisting optimization-based tomography solvers while being substantially\nfaster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_D/0/1/0/all/0/1\">Darius R&#xfc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idoughi_R/0/1/0/all/0/1\">Ramzi Idoughi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidrich_W/0/1/0/all/0/1\">Wolfgang Heidrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-Style Encoder for Style-Based GAN Inversion. (arXiv:2202.02183v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02183","description":"<p>We propose a novel architecture for GAN inversion, which we call\nFeature-Style encoder. The style encoder is key for the manipulation of the\nobtained latent codes, while the feature encoder is crucial for optimal image\nreconstruction. Our model achieves accurate inversion of real images from the\nlatent space of a pre-trained style-based GAN model, obtaining better\nperceptual quality and lower reconstruction error than existing methods. Thanks\nto its encoder structure, the model allows fast and accurate image editing.\nAdditionally, we demonstrate that the proposed encoder is especially\nwell-suited for inversion and editing on videos. We conduct extensive\nexperiments for several style-based generators pre-trained on different data\ndomains. Our proposed method yields state-of-the-art results for style-based\nGAN inversion, significantly outperforming competing approaches. Source codes\nare available at https://github.com/InterDigitalInc/FeatureStyleEncoder .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newson_A/0/1/0/all/0/1\">Alasdair Newson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gousseau_Y/0/1/0/all/0/1\">Yann Gousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Neighbor Consistency for Noisy Labels. (arXiv:2202.02200v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02200","description":"<p>Recent advances in deep learning have relied on large, labelled datasets to\ntrain high-capacity models. However, collecting large datasets in a time- and\ncost-efficient manner often results in label noise. We present a method for\nlearning from noisy labels that leverages similarities between training\nexamples in feature space, encouraging the prediction of each example to be\nsimilar to its nearest neighbours. Compared to training algorithms that use\nmultiple models or distinct stages, our approach takes the form of a simple,\nadditional regularization term. It can be interpreted as an inductive version\nof the classical, transductive label propagation algorithm. We thoroughly\nevaluate our method on datasets evaluating both synthetic (CIFAR-10, CIFAR-100)\nand realistic (mini-WebVision, Clothing1M, mini-ImageNet-Red) noise, and\nachieve competitive or state-of-the-art accuracies across all of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1\">Ahmet Iscen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valmadre_J/0/1/0/all/0/1\">Jack Valmadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Violence Recognition and Localization using a Semi-Supervised Hard-Attention Model. (arXiv:2202.02212v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02212","description":"<p>Empowering automated violence monitoring and surveillance systems amid the\ngrowing social violence and extremist activities worldwide could keep\ncommunities safe and save lives. The questionable reliability of human\nmonitoring personnel and the increasing number of surveillance cameras makes\nautomated artificial intelligence-based solutions compelling. Improving the\ncurrent state-of-the-art deep learning approaches to video violence recognition\nto higher levels of accuracy and performance could enable surveillance systems\nto be more reliable and scalable. The main contribution of the proposed deep\nreinforcement learning method is to achieve state-of-the-art accuracy on RWF,\nHockey, and Movies datasets while removing some of the computationally\nexpensive processes and input features used in the previous solutions. The\nimplementation of hard attention using a semi-supervised learning method made\nthe proposed method capable of rough violence localization and added increased\nagent interpretability to the violence detection system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_H/0/1/0/all/0/1\">Hamid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nazerfard_E/0/1/0/all/0/1\">Ehsan Nazerfard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapped Representation Learning for Skeleton-Based Action Recognition. (arXiv:2202.02232v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02232","description":"<p>In this work, we study self-supervised representation learning for 3D\nskeleton-based action recognition. We extend Bootstrap Your Own Latent (BYOL)\nfor representation learning on skeleton sequence data and propose a new data\naugmentation strategy including two asymmetric transformation pipelines. We\nalso introduce a multi-viewpoint sampling method that leverages multiple\nviewing angles of the same action captured by different cameras. In the\nsemi-supervised setting, we show that the performance can be further improved\nby knowledge distillation from wider networks, leveraging once more the\nunlabeled samples. We conduct extensive experiments on the NTU-60 and NTU-120\ndatasets to demonstrate the performance of our proposed method. Our method\nconsistently outperforms the current state of the art on both linear evaluation\nand semi-supervised benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moliner_O/0/1/0/all/0/1\">Olivier Moliner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sangxia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7B%5CAA%7Dstrom_K/0/1/0/all/0/1\">Kalle &#xc5;str&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized visual encoding model construction with small data. (arXiv:2202.02245v1 [q-bio.QM])","link":"http://arxiv.org/abs/2202.02245","description":"<p>Encoding models that predict brain response patterns to stimuli are one way\nto capture this relationship between variability in bottom-up neural systems\nand individual's behavior or pathological state. However, they generally need a\nlarge amount of training data to achieve optimal accuracy. Here, we propose and\ntest an alternative personalized ensemble encoding model approach to utilize\nexisting encoding models, to create encoding models for novel individuals with\nrelatively little stimuli-response data. We show that these personalized\nensemble encoding models trained with small amounts of data for a specific\nindividual, i.e. ~400 image-response pairs, achieve accuracy not different from\nmodels trained on ~24,000 image-response pairs for the same individual.\nImportantly, the personalized ensemble encoding models preserve patterns of\ninter-individual variability in the image-response relationship. Additionally,\nwe use our personalized ensemble encoding model within the recently developed\nNeuroGen framework to generate optimal stimuli designed to maximize specific\nregions' activations for a specific individual. We show that the\ninter-individual differences in face area responses to images of dog vs human\nfaces observed previously is replicated using NeuroGen with the ensemble\nencoding model. Finally, and most importantly, we show the proposed approach is\nrobust against domain shift by validating on a prospectively collected set of\nimage-response data in novel individuals with a different scanner and\nexperimental setup. Our approach shows the potential to use previously\ncollected, deeply sampled data to efficiently create accurate, personalized\nencoding models and, subsequently, personalized optimal synthetic images for\nnew individuals scanned under different experimental conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Gu_Z/0/1/0/all/0/1\">Zijin Gu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jamison_K/0/1/0/all/0/1\">Keith Jamison</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert Sabuncu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kuceyeski_A/0/1/0/all/0/1\">Amy Kuceyeski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Self Knowledge Distillation -- From Pothole Classification to Fine-Grained and COVID Recognition. (arXiv:2202.02265v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02265","description":"<p>Pothole classification has become an important task for road inspection\nvehicles to save drivers from potential car accidents and repair bills. Given\nthe limited computational power and fixed number of training epochs, we propose\niterative self knowledge distillation (ISKD) to train lightweight pothole\nclassifiers. Designed to improve both the teacher and student models over time\nin knowledge distillation, ISKD outperforms the state-of-the-art self knowledge\ndistillation method on three pothole classification datasets across four\nlightweight network architectures, which supports that self knowledge\ndistillation should be done iteratively instead of just once. The accuracy\nrelation between the teacher and student models shows that the student model\ncan still benefit from a moderately trained teacher model. Implying that better\nteacher models generally produce better student models, our results justify the\ndesign of ISKD. In addition to pothole classification, we also demonstrate the\nefficacy of ISKD on six additional datasets associated with generic\nclassification, fine-grained classification, and medical imaging application,\nwhich supports that ISKD can serve as a general-purpose performance booster\nwithout the need of a given teacher model and extra trainable parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kuan-Chuan Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality Assessment of Low Light Restored Images: A Subjective Study and an Unsupervised Model. (arXiv:2202.02277v1 [eess.IV])","link":"http://arxiv.org/abs/2202.02277","description":"<p>The quality assessment (QA) of restored low light images is an important tool\nfor benchmarking and improving low light restoration (LLR) algorithms. While\nseveral LLR algorithms exist, the subjective perception of the restored images\nhas been much less studied. Challenges in capturing aligned low light and\nwell-lit image pairs and collecting a large number of human opinion scores of\nquality for training, warrant the design of unsupervised (or opinion unaware)\nno-reference (NR) QA methods. This work studies the subjective perception of\nlow light restored images and their unsupervised NR QA. Our contributions are\ntwo-fold. We first create a dataset of restored low light images using various\nLLR methods, conduct a subjective QA study and benchmark the performance of\nexisting QA methods. We then present a self-supervised contrastive learning\ntechnique to extract distortion aware features from the restored low light\nimages. We show that these features can be effectively used to build an opinion\nunaware image quality analyzer. Detailed experiments reveal that our\nunsupervised NR QA model achieves state-of-the-art performance among all such\nquality measures for low light restored images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kannan_V/0/1/0/all/0/1\">Vignesh Kannan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_S/0/1/0/all/0/1\">Sameer Malik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soundararajan_R/0/1/0/all/0/1\">Rajiv Soundararajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task head pose estimation in-the-wild. (arXiv:2202.02299v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02299","description":"<p>We present a deep learning-based multi-task approach for head pose estimation\nin images. We contribute with a network architecture and training strategy that\nharness the strong dependencies among face pose, alignment and visibility, to\nproduce a top performing model for all three tasks. Our architecture is an\nencoder-decoder CNN with residual blocks and lateral skip connections. We show\nthat the combination of head pose estimation and landmark-based face alignment\nsignificantly improve the performance of the former task. Further, the location\nof the pose task at the bottleneck layer, at the end of the encoder, and that\nof tasks depending on spatial information, such as visibility and alignment, in\nthe final decoder layer, also contribute to increase the final performance. In\nthe experiments conducted the proposed model outperforms the state-of-the-art\nin the face pose and visibility tasks. By including a final landmark regression\nstep it also produces face alignment results on par with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valle_R/0/1/0/all/0/1\">Roberto Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1\">Jos&#xe9; Miguel Buenaposada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1\">Luis Baumela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Mobile App Navigation with Uncertain or Under-specified Natural Language Commands. (arXiv:2202.02312v1 [cs.CL])","link":"http://arxiv.org/abs/2202.02312","description":"<p>We introduce Mobile app Tasks with Iterative Feedback (MoTIF), a new dataset\nwhere the goal is to complete a natural language query in a mobile app. Current\ndatasets for related tasks in interactive question answering, visual common\nsense reasoning, and question-answer plausibility prediction do not support\nresearch in resolving ambiguous natural language requests or operating in\ndiverse digital domains. As a result, they fail to capture complexities of real\nquestion answering or interactive tasks. In contrast, MoTIF contains natural\nlanguage requests that are not satisfiable, the first such work to investigate\nthis issue for interactive vision-language tasks. MoTIF also contains follow up\nquestions for ambiguous queries to enable research on task uncertainty\nresolution. We introduce task feasibility prediction and propose an initial\nmodel which obtains an F1 score of 61.1. We next benchmark task automation with\nour dataset and find adaptations of prior work perform poorly due to our\nrealistic language requests, obtaining an accuracy of only 20.2% when mapping\ncommands to grounded actions. We analyze performance and gain insight for\nfuture work that may bridge the gap between current model ability and what is\nneeded for successful use in application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arsan_D/0/1/0/all/0/1\">Deniz Arsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sanjna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ranjitha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards To-a-T Spatio-Temporal Focus for Skeleton-Based Action Recognition. (arXiv:2202.02314v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02314","description":"<p>Graph Convolutional Networks (GCNs) have been widely used to model the\nhigh-order dynamic dependencies for skeleton-based action recognition. Most\nexisting approaches do not explicitly embed the high-order spatio-temporal\nimportance to joints' spatial connection topology and intensity, and they do\nnot have direct objectives on their attention module to jointly learn when and\nwhere to focus on in the action sequence. To address these problems, we propose\nthe To-a-T Spatio-Temporal Focus (STF), a skeleton-based action recognition\nframework that utilizes the spatio-temporal gradient to focus on relevant\nspatio-temporal features. We first propose the STF modules with learnable\ngradient-enforced and instance-dependent adjacency matrices to model the\nhigh-order spatio-temporal dynamics. Second, we propose three loss terms\ndefined on the gradient-based spatio-temporal focus to explicitly guide the\nclassifier when and where to look at, distinguish confusing classes, and\noptimize the stacked STF modules. STF outperforms the state-of-the-art methods\non the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets in all\n15 settings over different views, subjects, setups, and input modalities, and\nSTF also shows better accuracy on scarce data and dataset shifting settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Lipeng Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kuan-Chuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v1 [cs.CV])","link":"http://arxiv.org/abs/2202.02317","description":"<p>General purpose vision (GPV) systems are models that are designed to solve a\nwide array of visual tasks without requiring architectural changes. Today, GPVs\nprimarily learn both skills and concepts from large fully supervised datasets.\nScaling GPVs to tens of thousands of concepts by acquiring data to learn each\nconcept for every skill quickly becomes prohibitive. This work presents an\neffective and inexpensive alternative: learn skills from fully supervised\ndatasets, learn concepts from web image search results, and leverage a key\ncharacteristic of GPVs -- the ability to transfer visual knowledge across\nskills. We use a dataset of 1M+ images spanning 10k+ visual concepts to\ndemonstrate webly-supervised concept expansion for two existing GPVs (GPV-1 and\nVL-T5) on 3 benchmarks - 5 COCO based datasets (80 primary concepts), a newly\ncurated series of 5 datasets based on the OpenImages and VisualGenome\nrepositories (~500 concepts) and the Web-derived dataset (10k+ concepts). We\nalso propose a new architecture, GPV-2 that supports a variety of tasks -- from\nvision tasks like classification and localization to vision+language tasks like\nQA and captioning to more niche ones like human-object interaction recognition.\nGPV-2 benefits hugely from web data, outperforms GPV-1 and VL-T5 across these\nbenchmarks, and does well in a 0-shot setting at action and attribute\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Amita Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_C/0/1/0/all/0/1\">Christopher Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanmay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolve_E/0/1/0/all/0/1\">Eric Kolve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ocular Recognition Databases and Competitions: A Survey. (arXiv:1911.09646v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.09646","description":"<p>The use of the iris and periocular region as biometric traits has been\nextensively investigated, mainly due to the singularity of the iris features\nand the use of the periocular region when the image resolution is not\nsufficient to extract iris information. In addition to providing information\nabout an individual's identity, features extracted from these traits can also\nbe explored to obtain other information such as the individual's gender, the\ninfluence of drug use, the use of contact lenses, spoofing, among others. This\nwork presents a survey of the databases created for ocular recognition,\ndetailing their protocols and how their images were acquired. We also describe\nand discuss the most popular ocular recognition competitions (contests),\nhighlighting the submitted algorithms that achieved the best results using only\niris trait and also fusing iris and periocular region information. Finally, we\ndescribe some relevant works applying deep learning techniques to ocular\nrecognition and point out new challenges and future directions. Considering\nthat there are a large number of ocular databases, and each one is usually\ndesigned for a specific problem, we believe this survey can provide a broad\noverview of the challenges in ocular biometrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zanlorensi_L/0/1/0/all/0/1\">Luiz A. Zanlorensi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luz_E/0/1/0/all/0/1\">Eduardo Luz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1\">Alceu S. Britto Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Luiz S. Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Image-to-Image Translation via a Single Generative Adversarial Network. (arXiv:2008.01681v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.01681","description":"<p>Despite significant advances in image-to-image (I2I) translation with\ngenerative adversarial networks (GANs), it remains challenging to effectively\ntranslate an image to a set of diverse images in multiple target domains using\na pair of generators and discriminators. Existing multimodal I2I translation\nmethods adopt multiple domain-specific content encoders for different domains,\nwhere each domain-specific content encoder is trained with images from the same\ndomain only. Nevertheless, we argue that the content (domain-invariance)\nfeatures should be learned from images among all of the domains. Consequently,\neach domain-specific content encoder of existing schemes fails to extract the\ndomain-invariant features efficiently. To address this issue, we present a\nflexible and general SoloGAN model for efficient multimodal I2I translation\namong multiple domains with unpaired data. In contrast to existing methods, the\nSoloGAN algorithm uses a single projection discriminator with an additional\nauxiliary classifier and shares the encoder and generator for all domains. As\nsuch, the SoloGAN model can be trained effectively with images from all domains\nso that the domain-invariance content representation can be efficiently\nextracted. Qualitative and quantitative results over a wide range of datasets\nagainst several counterparts and variants of the SoloGAN model demonstrate the\nmerits of the method, especially for challenging I2I translation tasks, i.e.\ntasks that involve extreme shape variations or need to keep the complex\nbackgrounds unchanged after translations. Furthermore, we demonstrate the\ncontribution of each component using ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shihua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Cheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From noisy point clouds to complete ear shapes: unsupervised pipeline. (arXiv:2008.09831v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.09831","description":"<p>Ears are a particularly difficult region of the human face to model, not only\ndue to the non-rigid deformations existing between shapes but also to the\nchallenges in processing the retrieved data. The first step towards obtaining a\ngood model is to have complete scans in correspondence, but these usually\npresent a higher amount of occlusions, noise and outliers when compared to most\nface regions, thus requiring a specific procedure. Therefore, we propose a\ncomplete pipeline taking as input unordered 3D point clouds with the\naforementioned problems, and producing as output a dataset in correspondence,\nwith completion of the missing data. We provide a comparison of several\nstate-of-the-art registration methods and propose a new approach for one of the\nsteps of the pipeline, with better performance for our data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valdeira_F/0/1/0/all/0/1\">Filipa Valdeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_R/0/1/0/all/0/1\">Ricardo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheletti_A/0/1/0/all/0/1\">Alessandra Micheletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1\">Cl&#xe1;udia Soares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.15081","description":"<p>We propose Deep Estimators of Features (DEFs), a learning-based framework for\npredicting sharp geometric features in sampled 3D shapes. Differently from\nexisting data-driven methods, which reduce this problem to feature\nclassification, we propose to regress a scalar field representing the distance\nfrom point samples to the closest feature line on local patches. Our approach\nis the first that scales to massive point clouds by fusing distance-to-feature\nestimates obtained on individual patches. We extensively evaluate our approach\nagainst related state-of-the-art methods on newly proposed synthetic and\nreal-world 3D CAD model benchmarks. Our approach not only outperforms these\n(with improvements in Recall and False Positives Rates), but generalizes to\nreal-world scans after training our model on synthetic data and fine-tuning it\non a small dataset of scanned data. We demonstrate a downstream application,\nwhere we reconstruct an explicit representation of straight and curved sharp\nfeature lines from range scan data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matveev_A/0/1/0/all/0/1\">Albert Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhimov_R/0/1/0/all/0/1\">Ruslan Rakhimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobrovskikh_G/0/1/0/all/0/1\">Gleb Bobrovskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egiazarian_V/0/1/0/all/0/1\">Vage Egiazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1\">Emil Bogomolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panozzo_D/0/1/0/all/0/1\">Daniele Panozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. (arXiv:2103.04922v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.04922","description":"<p>Deep generative models are a class of techniques that train deep neural\nnetworks to model the distribution of training samples. Research has fragmented\ninto various interconnected approaches, each of which make trade-offs including\nrun-time, diversity, and architectural restrictions. In particular, this\ncompendium covers energy-based models, variational autoencoders, generative\nadversarial networks, autoregressive models, normalizing flows, in addition to\nnumerous hybrid approaches. These techniques are compared and contrasted,\nexplaining the premises behind each and how they are interrelated, while\nreviewing current state-of-the-art advances and implementations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bond_Taylor_S/0/1/0/all/0/1\">Sam Bond-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_A/0/1/0/all/0/1\">Adam Leach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lottery Jackpots Exist in Pre-trained Models. (arXiv:2104.08700v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08700","description":"<p>Network pruning is an effective approach to reduce network complexity with\nacceptable performance compromise. Existing studies achieve the sparsity of\nneural networks via time-consuming weight tuning or complex search on networks\nwith expanded width, which greatly limits the applications of network pruning.\nIn this paper, we show that high-performing and sparse sub-networks without the\ninvolvement of weight tuning, termed \"lottery jackpots\", exist in pre-trained\nmodels with unexpanded width. For example, we obtain a lottery jackpot that has\nonly 10% parameters and still reaches the performance of the original dense\nVGGNet-19 without any modifications on the pre-trained weights on CIFAR-10.\nFurthermore, we observe that the sparse masks derived from many existing\npruning criteria have a high overlap with the searched mask of our lottery\njackpot, among which, the magnitude-based pruning results in the most similar\nmask with ours. Based on this insight, we initialize our sparse mask using the\nmagnitude-based pruning, resulting in at least 3x cost reduction on the lottery\njackpot search while achieving comparable or even better performance.\nSpecifically, our magnitude-based lottery jackpot removes 90% weights in\nResNet-50, while it easily obtains more than 70% top-1 accuracy using only 10\nsearching epochs on ImageNet. Our project is available at\nhttps://github.com/lottery-jackpot/lottery-jackpot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Model Checking of Continuous Space. (arXiv:2105.06194v2 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2105.06194","description":"<p>Topological Spatial Model Checking is a recent paradigm where model checking\ntechniques are developed for the topological interpretation of Modal Logic. The\nSpatial Logic of Closure Spaces, SLCS, extends Modal Logic with reachability\nconnectives that, in turn, can be used for expressing interesting spatial\nproperties, such as \"being near to\" or \"being surrounded by\". SLCS constitutes\nthe kernel of a solid logical framework for reasoning about discrete space,\nsuch as graphs and digital images, interpreted as quasi discrete closure\nspaces. Following a recently developed geometric semantics of Modal Logic, we\npropose an interpretation of SLCS in continuous space, admitting a geometric\nspatial model checking procedure, by resorting to models based on polyhedra.\nSuch representations of space are increasingly relevant in many domains of\napplication, due to recent developments of 3D scanning and visualisation\ntechniques that exploit mesh processing. We introduce PolyLogicA, a geometric\nspatial model checker for SLCS formulas on polyhedra and demonstrate\nfeasibility of our approach on two 3D polyhedral models of realistic size.\nFinally, we introduce a geometric definition of bisimilarity, proving that it\ncharacterises logical equivalence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bezhanishvili_N/0/1/0/all/0/1\">Nick Bezhanishvili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciancia_V/0/1/0/all/0/1\">Vincenzo Ciancia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabelaia_D/0/1/0/all/0/1\">David Gabelaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grilletti_G/0/1/0/all/0/1\">Gianluca Grilletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latella_D/0/1/0/all/0/1\">Diego Latella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massink_M/0/1/0/all/0/1\">Mieke Massink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Q-attention: Enabling Efficient Learning for Vision-based Robotic Manipulation. (arXiv:2105.14829v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2105.14829","description":"<p>Despite the success of reinforcement learning methods, they have yet to have\ntheir breakthrough moment when applied to a broad range of robotic manipulation\ntasks. This is partly due to the fact that reinforcement learning algorithms\nare notoriously difficult and time consuming to train, which is exacerbated\nwhen training from images rather than full-state inputs. As humans perform\nmanipulation tasks, our eyes closely monitor every step of the process with our\ngaze focusing sequentially on the objects being manipulated. With this in mind,\nwe present our Attention-driven Robotic Manipulation (ARM) algorithm, which is\na general manipulation algorithm that can be applied to a range of\nsparse-rewarded tasks, given only a small number of demonstrations. ARM splits\nthe complex task of manipulation into a 3 stage pipeline: (1) a Q-attention\nagent extracts relevant pixel locations from RGB and point cloud inputs, (2) a\nnext-best pose agent that accepts crops from the Q-attention agent and outputs\nposes, and (3) a control agent that takes the goal pose and outputs joint\nactions. We show that current learning algorithms fail on a range of RLBench\ntasks, whilst ARM is successful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervision & Meta-Learning for One-Shot Unsupervised Cross-Domain Detection. (arXiv:2106.03496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03496","description":"<p>Deep detection approaches are powerful in controlled conditions, but appear\nbrittle and fail when source models are used off-the-shelf on unseen domains.\nMost of the existing works on domain adaptation simplify the setting and access\njointly both a large source dataset and a sizable amount of target samples.\nHowever this scenario is unrealistic in many practical cases as when monitoring\nimage feeds from social media: only a pretrained source model is available and\nevery target image uploaded by the users belongs to a different domain not\nforeseen during training. We address this challenging setting by presenting an\nobject detection algorithm able to exploit a pre-trained source model and\nperform unsupervised adaptation by using only one target sample seen at test\ntime. Our multi-task architecture includes a self-supervised branch that we\nexploit to meta-train the whole model with single-sample cross-domain episodes,\nand prepare to the test condition. At deployment time the self-supervised task\nis iteratively solved on any incoming sample to one-shot adapt on it. We\nintroduce a new dataset of social media image feeds and present a thorough\nbenchmark with the most recent cross-domain detection methods showing the\nadvantages of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1\">F. Cappio Borlino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polizzotto_S/0/1/0/all/0/1\">S. Polizzotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DInnocente_A/0/1/0/all/0/1\">A. D&#x27;Innocente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1\">S. Bucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">B. Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">T. Tommasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06959","description":"<p>The discovery of the disentanglement properties of the latent space in GANs\nmotivated a lot of research to find the semantically meaningful directions on\nit. In this paper, we suggest that the disentanglement property is closely\nrelated to the geometry of the latent space. In this regard, we propose an\nunsupervised method for finding the semantic-factorizing directions on the\nintermediate latent space of GANs based on the local geometry. Intuitively, our\nproposed method, called Local Basis, finds the principal variation of the\nlatent space in the neighborhood of the base latent variable. Experimental\nresults show that the local principal variation corresponds to the semantic\nfactorization and traversing along it provides strong robustness to image\ntraversal. Moreover, we suggest an explanation for the limited success in\nfinding the global traversal directions in the latent space, especially W-space\nof StyleGAN2. We show that W-space is warped globally by comparing the local\ngeometry, discovered from Local Basis, through the metric on Grassmannian\nManifold. The global warpage implies that the latent space is not well-aligned\nglobally and therefore the global traversal directions are bound to show\nlimited success on it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaewoong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_C/0/1/0/all/0/1\">Changyeon Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jung Ho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Geonho Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungjoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11810","description":"<p>In this work, we propose the world's first closed-loop ML-based planning\nbenchmark for autonomous driving. While there is a growing body of ML-based\nmotion planners, the lack of established datasets and metrics has limited the\nprogress in this area. Existing benchmarks for autonomous vehicle motion\nprediction have focused on short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop evaluation with L2-based\nmetrics, which are not suitable for fairly evaluating long-term planning. Our\nbenchmark overcomes these limitations by introducing a large-scale driving\ndataset, lightweight closed-loop simulator, and motion-planning-specific\nmetrics. We provide a high-quality dataset with 1500h of human driving data\nfrom 4 cities across the US and Asia with widely varying traffic patterns\n(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop\nsimulation framework with reactive agents and provide a large set of both\ngeneral and scenario-specific planning metrics. We plan to release the dataset\nat NeurIPS 2021 and organize benchmark challenges starting in early 2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1\">Juraj Kabzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kok Seang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1\">Alex Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1\">Luke Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1\">Sammy Omari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Striking the Right Balance: Recall Loss for Semantic Segmentation. (arXiv:2106.14917v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14917","description":"<p>Class imbalance is a fundamental problem in computer vision applications such\nas semantic segmentation. Specifically, uneven class distributions in a\ntraining dataset often result in unsatisfactory performance on\nunder-represented classes. Many works have proposed to weight the standard\ncross entropy loss function with pre-computed weights based on class\nstatistics, such as the number of samples and class margins. There are two\nmajor drawbacks to these methods: 1) constantly up-weighting minority classes\ncan introduce excessive false positives in semantic segmentation; 2) a minority\nclass is not necessarily a hard class. The consequence is low precision due to\nexcessive false positives. In this regard, we propose a hard-class mining loss\nby reshaping the vanilla cross entropy loss such that it weights the loss for\neach class dynamically based on instantaneous recall performance. We show that\nthe novel recall loss changes gradually between the standard cross entropy loss\nand the inverse frequency weighted loss. Recall loss also leads to improved\nmean accuracy while offering competitive mean Intersection over Union (IoU)\nperformance. On Synthia dataset, recall loss achieves $9\\%$ relative\nimprovement on mean accuracy with competitive mean IoU using DeepLab-ResNet18\ncompared to the cross entropy loss. Code available at\nhttps://github.com/PotatoTian/recall-semseg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seymour_Z/0/1/0/all/0/1\">Zach Seymour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Han-Pang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. (arXiv:2108.01775v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01775","description":"<p>This paper presents solo-learn, a library of self-supervised methods for\nvisual representation learning. Implemented in Python, using Pytorch and\nPytorch lightning, the library fits both research and industry needs by\nfeaturing distributed training pipelines with mixed-precision, faster data\nloading via Nvidia DALI, online linear evaluation for better prototyping, and\nmany additional training tricks. Our goal is to provide an easy-to-use library\ncomprising a large amount of Self-supervised Learning (SSL) methods, that can\nbe easily extended and fine-tuned by the community. solo-learn opens up avenues\nfor exploiting large-budget SSL solutions on inexpensive smaller\ninfrastructures and seeks to democratize SSL by making it accessible to all.\nThe source code is available at https://github.com/vturrisi/solo-learn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1\">Victor G. Turrisi da Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIGRoC: Boosting Image Generation via a Robust Classifier. (arXiv:2108.03702v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03702","description":"<p>The interest of the machine learning community in image synthesis has grown\nsignificantly in recent years, with the introduction of a wide range of deep\ngenerative models and means for training them. In this work, we propose a\ngeneral model-agnostic technique for improving the image quality and the\ndistribution fidelity of generated images, obtained by any generative model.\nOur method, termed BIGRoC (Boosting Image Generation via a Robust Classifier),\nis based on a post-processing procedure via the guidance of a given robust\nclassifier and without a need for additional training of the generative model.\nGiven a synthesized image, we propose to update it through projected gradient\nsteps over the robust classifier, in an attempt to refine its recognition. We\ndemonstrate this post-processing algorithm on various image synthesis methods\nand show a significant improvement of the generated images, both quantitatively\nand qualitatively, on CIFAR-10 and ImageNet. Specifically, BIGRoC improves the\nimage synthesis state of the art on ImageNet 128x128 by 14.81%, attaining an\nFID score of 2.53 and on 256x256 by 7.87%, achieving an FID of 3.63.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03201","description":"<p>Transformer, the model of choice for natural language processing, has drawn\nscant attention from the medical imaging community. Given the ability to\nexploit long-term dependencies, transformers are promising to help atypical\nconvolutional neural networks to overcome their inherent shortcomings of\nspatial inductive bias. However, most of recently proposed transformer-based\nsegmentation approaches simply treated transformers as assisted modules to help\nencode global context into convolutional representations. To address this\nissue, we introduce nnFormer, a 3D transformer for volumetric medical image\nsegmentation. nnFormer not only exploits the combination of interleaved\nconvolution and self-attention operations, but also introduces local and global\nvolume-based self-attention mechanism to learn volume representations.\nMoreover, nnFormer proposes to use skip attention to replace the traditional\nconcatenation/summation operations in skip connections in U-Net like\narchitecture. Experiments show that nnFormer significantly outperforms previous\ntransformer-based counterparts by large margins on three public datasets.\nCompared to nnUNet, nnFormer produces significantly lower HD95 and comparable\nDSC results. Furthermore, we show that nnFormer and nnUNet are highly\ncomplementary to each other in model ensembling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiansen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus on Impact: Indoor Exploration with Intrinsic Motivation. (arXiv:2109.08521v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.08521","description":"<p>Exploration of indoor environments has recently experienced a significant\ninterest, also thanks to the introduction of deep neural agents built in a\nhierarchical fashion and trained with Deep Reinforcement Learning (DRL) on\nsimulated environments. Current state-of-the-art methods employ a dense\nextrinsic reward that requires the complete a priori knowledge of the layout of\nthe training environment to learn an effective exploration policy. However,\nsuch information is expensive to gather in terms of time and resources. In this\nwork, we propose to train the model with a purely intrinsic reward signal to\nguide exploration, which is based on the impact of the robot's actions on its\ninternal representation of the environment. So far, impact-based rewards have\nbeen employed for simple tasks and in procedurally generated synthetic\nenvironments with countable states. Since the number of states observable by\nthe agent in realistic indoor environments is non-countable, we include a\nneural-based density model and replace the traditional count-based\nregularization with an estimated pseudo-count of previously visited states. The\nproposed exploration approach outperforms DRL-based competitors relying on\nintrinsic rewards and surpasses the agents trained with a dense extrinsic\nreward computed with the environment layouts. We also show that a robot\nequipped with the proposed approach seamlessly adapts to point-goal navigation\nand real-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bigazzi_R/0/1/0/all/0/1\">Roberto Bigazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Convolutional Neural Network for Automated AMD Classification using Retinal OCT Images. (arXiv:2110.03002v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.03002","description":"<p>Age-related macular degeneration (AMD) is the most common cause of blindness\nin developed countries, especially in people over 60 years of age. The workload\nof specialists and the healthcare system in this field has increased in recent\nyears mainly due to the prevalence of population aging worldwide and the\nchronic nature of AMD. Recent developments in deep learning have provided a\nunique opportunity to develop fully automated diagnosis frameworks. Considering\nthe presence of AMD-related retinal pathologies in varying sizes in OCT images,\nour objective was to propose a multi-scale convolutional neural network (CNN)\ncapable of distinguishing pathologies using receptive fields with various\nsizes. The multi-scale CNN was designed based on the feature pyramid network\n(FPN) structure and was used to diagnose normal and two common clinical\ncharacteristics of dry and wet AMD, namely drusen and choroidal\nneovascularization (CNV). The proposed method was evaluated on a national\ndataset gathered at Noor Eye Hospital (NEH) and the UCSD public dataset.\nExperimental results show the superior performance of our proposed multi-scale\nstructure over several well-known OCT classification frameworks. This feature\ncombination strategy has proved to be effective on all tested backbone models,\nwith improvements ranging from 0.4% to 3.3%. In addition, gradual learning has\nproven to improve performance in two consecutive stages. In the first stage,\nthe performance was boosted from 87.2%+-2.5% to 92.0%+-1.6% using pre-trained\nImageNet weights. In the second stage, another performance boost from\n92.0%+-1.6% to 93.4%+-1.4% was observed due to fine-tuning the previous model\non the UCSD dataset. Lastly, generating heatmaps provided additional proof for\nthe effectiveness of our multi-scale structure, enabling the detection of\nretinal pathologies appearing in different sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sotoudeh_Paima_S/0/1/0/all/0/1\">Saman Sotoudeh-Paima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jodeiri_A/0/1/0/all/0/1\">Ata Jodeiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hajizadeh_F/0/1/0/all/0/1\">Fedra Hajizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltanian_Zadeh_H/0/1/0/all/0/1\">Hamid Soltanian-Zadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Gradient Non-sign Methods. (arXiv:2110.12734v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12734","description":"<p>Adversarial attacks make their success in DNNs, and among them,\ngradient-based algorithms become one of the mainstreams. Based on the linearity\nhypothesis, under $\\ell_\\infty$ constraint, $sign$ operation applied to the\ngradients is a good choice for generating perturbations. However, side-effects\nfrom such operation exist since it leads to the bias of direction between real\ngradients and perturbations. In other words, current methods contain a gap\nbetween real gradients and actual noises, which leads to biased and inefficient\nattacks. Therefore in this paper, based on the Taylor expansion, the bias is\nanalyzed theoretically, and the correction of $sign$, i.e., Fast Gradient\nNon-sign Method (FGNM), is further proposed. Notably, FGNM is a general routine\nthat seamlessly replaces the conventional $sign$ operation in gradient-based\nattacks with negligible extra computational cost. Extensive experiments\ndemonstrate the effectiveness of our methods. Specifically, for untargeted\nblack-box attacks, ours outperform them by 27.5% at most and 9.5% on average.\nFor targeted attacks against defense models, it is 15.1% and 12.7%. Our\nanonymous code is publicly available at https://github.com/yaya-cheng/FGNM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaosu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Non-Compression Auto-Encoder for Driving Noise-based Road Surface Anomaly Detection. (arXiv:2111.10985v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10985","description":"<p>Wet weather makes water film over the road and that film causes lower\nfriction between tire and road surface. When a vehicle passes the low-friction\nroad, the accident can occur up to 35% higher frequency than a normal condition\nroad. In order to prevent accidents as above, identifying the road condition in\nreal-time is essential. Thus, we propose a convolutional auto-encoder-based\nanomaly detection model for taking both less computational resources and\nachieving higher anomaly detection performance. The proposed model adopts a\nnon-compression method rather than a conventional bottleneck structured\nauto-encoder. As a result, the computational cost of the neural network is\nreduced up to 1 over 25 compared to the conventional models and the anomaly\ndetection performance is improved by up to 7.72%. Thus, we conclude the\nproposed model as a cutting-edge algorithm for real-time anomaly detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">YeongHyeon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">JongHee Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdiBERT, a generative model for image editing. (arXiv:2111.15264v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15264","description":"<p>Advances in computer vision are pushing the limits of im-age manipulation,\nwith generative models sampling detailed images on various tasks. However, a\nspecialized model is often developed and trained for each specific task, even\nthough many image edition tasks share similarities. In denoising, inpainting,\nor image compositing, one always aims at generating a realistic image from a\nlow-quality one. In this paper, we aim at making a step towards a unified\napproach for image editing. To do so, we propose EdiBERT, a bi-directional\ntransformer trained in the discrete latent space built by a vector-quantized\nauto-encoder. We argue that such a bidirectional model is suited for image\nmanipulation since any patch can be re-sampled conditionally to the whole\nimage. Using this unique and straightforward training objective, we show that\nthe resulting model matches state-of-the-art performances on a wide variety of\ntasks: image denoising, image completion, and image composition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Issenhuth_T/0/1/0/all/0/1\">Thibaut Issenhuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanielian_U/0/1/0/all/0/1\">Ugo Tanielian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mary_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Mary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1\">David Picard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. (arXiv:2112.05139v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05139","description":"<p>We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural\nradiance fields (NeRF). By leveraging the joint language-image embedding space\nof the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose\na unified framework that allows manipulating NeRF in a user-friendly way, using\neither a short text prompt or an exemplar image. Specifically, to combine the\nnovel view synthesis capability of NeRF and the controllable manipulation\nability of latent representations from generative models, we introduce a\ndisentangled conditional NeRF architecture that allows individual control over\nboth shape and appearance. This is achieved by performing the shape\nconditioning via applying a learned deformation field to the positional\nencoding and deferring color conditioning to the volumetric rendering stage. To\nbridge this disentangled latent representation to the CLIP embedding, we design\ntwo code mappers that take a CLIP embedding as input and update the latent\ncodes to reflect the targeted editing. The mappers are trained with a\nCLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we\npropose an inverse optimization method that accurately projects an input image\nto the latent codes for manipulation to enable editing on real images. We\nevaluate our approach by extensive experiments on a variety of text prompts and\nexemplar images and also provide an intuitive interface for interactive\nediting. Our implementation is available at\nhttps://cassiepython.github.io/clipnerf/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Inception Attention for Image Synthesis and Image Recognition. (arXiv:2112.14804v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14804","description":"<p>Image synthesis and image recognition have witnessed remarkable progress, but\noften at the expense of computationally expensive training and inference.\nLearning lightweight yet expressive deep model has emerged as an important and\ninteresting direction. Inspired by the well-known split-transform-aggregate\ndesign heuristic in the Inception building block, this paper proposes a\nSkip-Layer Inception Module (SLIM) that facilitates efficient learning of image\nsynthesis models, and a same-layer variant (dubbed as SLIM too) as a stronger\nalternative to the well-known ResNeXts for image recognition. In SLIM, the\ninput feature map is first split into a number of groups (e.g., 4).Each group\nis then transformed to a latent style vector(via channel-wise attention) and a\nlatent spatial mask (via spatial attention). The learned latent masks and\nlatent style vectors are aggregated to modulate the target feature map. For\ngenerative learning, SLIM is built on a recently proposed lightweight\nGenerative Adversarial Networks (i.e., FastGANs) which present a skip-layer\nexcitation(SLE) module. For few-shot image synthesis tasks, the proposed SLIM\nachieves better performance than the SLE work and other related methods. For\none-shot image synthesis tasks, it shows stronger capability of preserving\nimages structures than prior arts such as the SinGANs. For image classification\ntasks, the proposed SLIM is used as a drop-in replacement for convolution\nlayers in ResNets (resulting in ResNeXt-like models) and achieves better\naccuracy in theImageNet-1000 dataset, with significantly smaller model\ncomplexity\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianghao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Semantic Ambiguities for Zero-Shot Learning. (arXiv:2201.01823v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01823","description":"<p>Zero-shot learning (ZSL) aims at recognizing classes for which no visual\nsample is available at training time. To address this issue, one can rely on a\nsemantic description of each class. A typical ZSL model learns a mapping\nbetween the visual samples of seen classes and the corresponding semantic\ndescriptions, in order to do the same on unseen classes at test time. State of\nthe art approaches rely on generative models that synthesize visual features\nfrom the prototype of a class, such that a classifier can then be learned in a\nsupervised manner. However, these approaches are usually biased towards seen\nclasses whose visual instances are the only one that can be matched to a given\nclass prototype. We propose a regularization method that can be applied to any\nconditional generative-based ZSL method, by leveraging only the semantic class\nprototypes. It learns to synthesize discriminative features for possible\nsemantic description that are not available at training time, that is the\nunseen ones. The approach is evaluated for ZSL and GZSL on four datasets\ncommonly used in the literature, either in inductive and transductive settings,\nwith results on-par or above state of the art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hanouti_C/0/1/0/all/0/1\">Celina Hanouti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1\">Herv&#xe9; Le Borgne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Leaning-Based Ultra-Fast Stair Detection. (arXiv:2201.05275v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05275","description":"<p>Staircases are some of the most common building structures in urban\nenvironments. Stair detection is an important task for various applications,\nincluding the environmental perception of exoskeleton robots, humanoid robots,\nand rescue robots and the navigation of visually impaired people. Most existing\nstair detection algorithms have difficulty dealing with the diversity of stair\nstructure materials, extreme light and serious occlusion. Inspired by human\nperception, we propose an end-to-end method based on deep learning.\nSpecifically, we treat the process of stair line detection as a multitask\ninvolving coarse-grained semantic segmentation and object detection. The input\nimages are divided into cells, and a simple neural network is used to judge\nwhether each cell contains stair lines. For cells containing stair lines, the\nlocations of the stair lines relative to each cell are regressed. Extensive\nexperiments on our dataset show that our method can achieve high performance in\nterms of both speed and accuracy. A lightweight version can even achieve 300+\nframes per second with the same resolution. Our code and dataset will be soon\navailable at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Z/0/1/0/all/0/1\">Zhongcai Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shuang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiyong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-consistent Generative Adversarial Networks for multi-modal Medical segmentation maps. (arXiv:2201.09693v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.09693","description":"<p>Image translation across domains for unpaired datasets has gained interest\nand great improvement lately. In medical imaging, there are multiple imaging\nmodalities, with very different characteristics. Our goal is to use\ncross-modality adaptation between CT and MRI whole cardiac scans for semantic\nsegmentation. We present a segmentation network using synthesised cardiac\nvolumes for extremely limited datasets. Our solution is based on a 3D\ncross-modality generative adversarial network to share information between\nmodalities and generate synthesized data using unpaired datasets. Our network\nutilizes semantic segmentation to improve generator shape consistency, thus\ncreating more realistic synthesised volumes to be used when re-training the\nsegmentation network. We show that improved segmentation can be achieved on\nsmall datasets when using spatial augmentations to improve a generative\nadversarial network. These augmentations improve the generator capabilities,\nthus enhancing the performance of the Segmentor. Using only 16 CT and 16 MRI\ncardiovascular volumes, improved results are shown over other segmentation\nmethods while using the suggested architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Segre_L/0/1/0/all/0/1\">Leo Segre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirschorn_O/0/1/0/all/0/1\">Or Hirschorn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginzburg_D/0/1/0/all/0/1\">Dvir Ginzburg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raviv_D/0/1/0/all/0/1\">Dan Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning. (arXiv:2201.09765v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.09765","description":"<p>Standard model-free reinforcement learning algorithms optimize a policy that\ngenerates the action to be taken in the current time step in order to maximize\nexpected future return. While flexible, it faces difficulties arising from the\ninefficient exploration due to its single step nature. In this work, we present\nGenerative Planning method (GPM), which can generate actions not only for the\ncurrent step, but also for a number of future steps (thus termed as generative\nplanning). This brings several benefits to GPM. Firstly, since GPM is trained\nby maximizing value, the plans generated from it can be regarded as intentional\naction sequences for reaching high value regions. GPM can therefore leverage\nits generated multi-step plans for temporally coordinated exploration towards\nhigh value regions, which is potentially more effective than a sequence of\nactions generated by perturbing each action at single step level, whose\nconsistent movement decays exponentially with the number of exploration steps.\nSecondly, starting from a crude initial plan generator, GPM can refine it to be\nadaptive to the task, which, in return, benefits future explorations. This is\npotentially more effective than commonly used action-repeat strategy, which is\nnon-adaptive in its form of plans. Additionally, since the multi-step plan can\nbe interpreted as the intent of the agent from now to a span of time period\ninto the future, it offers a more informative and intuitive signal for\ninterpretation. Experiments are conducted on several benchmark environments and\nthe results demonstrated its effectiveness compared with several baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haonan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network. (arXiv:2201.10147v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10147","description":"<p>The end-to-end image fusion framework has achieved promising performance,\nwith dedicated convolutional networks aggregating the multi-modal local\nappearance. However, long-range dependencies are directly neglected in existing\nCNN fusion approaches, impeding balancing the entire image-level perception for\ncomplex scenario fusion. In this paper, therefore, we propose an infrared and\nvisible image fusion algorithm based on a lightweight transformer module and\nadversarial learning. Inspired by the global interaction power, we use the\ntransformer technique to learn the effective global fusion relations. In\nparticular, shallow features extracted by CNN are interacted in the proposed\ntransformer fusion module to refine the fusion relationship within the spatial\nscope and across channels simultaneously. Besides, adversarial learning is\ndesigned in the training process to improve the output discrimination via\nimposing competitive consistency from the inputs, reflecting the specific\ncharacteristics in infrared and visible images. The experimental performance\ndemonstrates the effectiveness of the proposed modules, with superior\nimprovement against the state-of-the-art, generalising a novel paradigm via\ntransformer and adversarial learning in the fusion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1\">Dongyu Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks. (arXiv:2202.00838v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00838","description":"<p>Recent work suggests that representations learned by adversarially robust\nnetworks are more human perceptually-aligned than non-robust networks via image\nmanipulations. Despite appearing closer to human visual perception, it is\nunclear if the constraints in robust DNN representations match biological\nconstraints found in human vision. Human vision seems to rely on\ntexture-based/summary statistic representations in the periphery, which have\nbeen shown to explain phenomena such as crowding and performance on visual\nsearch tasks. To understand how adversarially robust\noptimizations/representations compare to human vision, we performed a\npsychophysics experiment using a set of metameric discrimination tasks where we\nevaluated how well human observers could distinguish between images synthesized\nto match adversarially robust representations compared to non-robust\nrepresentations and a texture synthesis model of peripheral vision (Texforms).\nWe found that the discriminability of robust representation and texture model\nimages decreased to near chance performance as stimuli were presented farther\nin the periphery. Moreover, performance on robust and texture-model images\nshowed similar trends within participants, while performance on non-robust\nrepresentations changed minimally across the visual field. These results\ntogether suggest that (1) adversarially robust representations capture\nperipheral computation better than non-robust representations and (2) robust\nrepresentations capture peripheral computation similar to current\nstate-of-the-art texture peripheral vision models. More broadly, our findings\nsupport the idea that localized texture summary statistic representations may\ndrive human invariance to adversarial perturbations and that the incorporation\nof such representations in DNNs could give rise to useful properties like\nadversarial robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrington_A/0/1/0/all/0/1\">Anne Harrington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deza_A/0/1/0/all/0/1\">Arturo Deza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extension -- Adaptive Sampling with Implicit Radiance Field. (arXiv:2202.00855v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2202.00855","description":"<p>This paper aims to explore and summarize the state-of-the-art progress in\nMonte Carlo adaptive light field sampling and reconstruction using deep\nreinforcement learning, with possible extension to it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuchi Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Transfer: Learning to Route Transferrable Representations. (arXiv:2202.01011v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01011","description":"<p>Knowledge transfer between heterogeneous source and target networks and tasks\nhas received a lot of attention in recent times as large amounts of quality\nlabelled data can be difficult to obtain in many applications. Existing\napproaches typically constrain the target deep neural network (DNN) feature\nrepresentations to be close to the source DNNs feature representations, which\ncan be limiting. We, in this paper, propose a novel adversarial multi-armed\nbandit approach which automatically learns to route source representations to\nappropriate target representations following which they are combined in\nmeaningful ways to produce accurate target models. We see upwards of 5%\naccuracy improvements compared with the state-of-the-art knowledge transfer\nmethods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67,\nand Stanford40 where the source dataset is ImageNet. We qualitatively analyze\nthe goodness of our transfer scheme by showing individual examples of the\nimportant features our target network focuses on in different layers compared\nwith the (closest) competitors. We also observe that our improvement over other\nmethods is higher for smaller target datasets making it an effective tool for\nsmall data applications that may benefit from transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1\">Keerthiram Murugesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadashivaiah_V/0/1/0/all/0/1\">Vijay Sadashivaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1\">Ronny Luss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1\">Amit Dhurandhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VOS: Learning What You Don't Know by Virtual Outlier Synthesis. (arXiv:2202.01197v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01197","description":"<p>Out-of-distribution (OOD) detection has received much attention lately due to\nits importance in the safe deployment of neural networks. One of the key\nchallenges is that models lack supervision signals from unknown data, and as a\nresult, can produce overconfident predictions on OOD data. Previous approaches\nrely on real outlier datasets for model regularization, which can be costly and\nsometimes infeasible to obtain in practice. In this paper, we present VOS, a\nnovel framework for OOD detection by adaptively synthesizing virtual outliers\nthat can meaningfully regularize the model's decision boundary during training.\nSpecifically, VOS samples virtual outliers from the low-likelihood region of\nthe class-conditional distribution estimated in the feature space. Alongside,\nwe introduce a novel unknown-aware training objective, which contrastively\nshapes the uncertainty space between the ID data and synthesized outlier data.\nVOS achieves state-of-the-art performance on both object detection and image\nclassification models, reducing the FPR95 by up to 7.87% compared to the\nprevious best method. Code is available at\nhttps://github.com/deeplearning-wisc/vos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}