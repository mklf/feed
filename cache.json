{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Local dynamic mode of Cognitive Behavioral Therapy. (arXiv:2205.09752v1 [cs.AI])","link":"http://arxiv.org/abs/2205.09752","description":"<p>In order to increase mental health equity among the most vulnerable and\nmarginalized communities, it is important to increase access to high-quality\ntherapists. One facet of addressing these needs, is to provide timely feedback\nto clinicians as they interact with their clients, in a way that is also\ncontextualized to specific clients and interactions they have had. Dynamical\nsystems provide a framework through which to analyze interactions. The present\nwork applies these methods to the domain of automated psychotherapist\nevaluation for Cognitive Behavioral Therapy (CBT). Our methods extract local\ndynamic modes from short windows of conversation and learns to correlate the\nobserved dynamics to CBT competence. The results demonstrate the value of this\nparadigm and outlines the way in which these methods can be used to study and\nimprove therapeutic strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ardulov_V/0/1/0/all/0/1\">Victor Ardulov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creed_T/0/1/0/all/0/1\">Torrey A. Creed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1\">David C. Atkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Holistic View on Argument Quality Prediction. (arXiv:2205.09803v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09803","description":"<p>Argumentation is one of society's foundational pillars, and, sparked by\nadvances in NLP and the vast availability of text data, automated mining of\narguments receives increasing attention. A decisive property of arguments is\ntheir strength or quality. While there are works on the automated estimation of\nargument strength, their scope is narrow: they focus on isolated datasets and\nneglect the interactions with related argument mining tasks, such as argument\nidentification, evidence detection, or emotional appeal. In this work, we close\nthis gap by approaching argument quality estimation from multiple different\nangles: Grounded on rich results from thorough empirical evaluations, we assess\nthe generalization capabilities of argument quality estimation across diverse\ndomains, the interplay with related argument mining tasks, and the impact of\nemotions on perceived argument strength. We find that generalization depends on\na sufficient representation of different domains in the training part. In\nzero-shot transfer and multi-task experiments, we reveal that argument quality\nis among the more challenging tasks but can improve others. Finally, we show\nthat emotions play a minor role in argument quality than is often assumed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fromm_M/0/1/0/all/0/1\">Michael Fromm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrendorf_M/0/1/0/all/0/1\">Max Berrendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiml_J/0/1/0/all/0/1\">Johanna Reiml</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayerhofer_I/0/1/0/all/0/1\">Isabelle Mayerhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargava_S/0/1/0/all/0/1\">Siddharth Bhargava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faerman_E/0/1/0/all/0/1\">Evgeniy Faerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidl_T/0/1/0/all/0/1\">Thomas Seidl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiDAS: Multi-integrated Domain Adaptive Supervision for Fake News Detection. (arXiv:2205.09817v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09817","description":"<p>COVID-19 related misinformation and fake news, coined an 'infodemic', has\ndramatically increased over the past few years. This misinformation exhibits\nconcept drift, where the distribution of fake news changes over time, reducing\neffectiveness of previously trained models for fake news detection. Given a set\nof fake news models trained on multiple domains, we propose an adaptive\ndecision module to select the best-fit model for a new sample. We propose\nMiDAS, a multi-domain adaptative approach for fake news detection that ranks\nrelevancy of existing models to new samples. MiDAS contains 2 components: a\ndoman-invariant encoder, and an adaptive model selector. MiDAS integrates\nmultiple pre-trained and fine-tuned models with their training data to create a\ndomain-invariant representation. Then, MiDAS uses local Lipschitz smoothness of\nthe invariant embedding space to estimate each model's relevance to a new\nsample. Higher ranked models provide predictions, and lower ranked models\nabstain. We evaluate MiDAS on generalization to drifted data with 9 fake news\ndatasets, each obtained from different domains and modalities. MiDAS achieves\nnew state-of-the-art performance on multi-domain adaptation for\nout-of-distribution fake news classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suprem_A/0/1/0/all/0/1\">Abhijit Suprem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_C/0/1/0/all/0/1\">Calton Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation. (arXiv:2205.09830v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09830","description":"<p>Women are often perceived as junior to their male counterparts, even within\nthe same job titles. While there has been significant progress in the\nevaluation of gender bias in natural language processing (NLP), existing\nstudies seldom investigate how biases toward gender groups change when\ncompounded with other societal biases. In this work, we investigate how\nseniority impacts the degree of gender bias exhibited in pretrained neural\ngeneration models by introducing a novel framework for probing compound bias.\nWe contribute a benchmark robustness-testing dataset spanning two domains, U.S.\nsenatorship and professorship, created using a distant-supervision method. Our\ndataset includes human-written text with underlying ground truth and paired\ncounterfactuals. We then examine GPT-2 perplexity and the frequency of gendered\nlanguage in generated text. Our results show that GPT-2 amplifies bias by\nconsidering women as junior and men as senior more often than the ground truth\nin both domains. These results suggest that NLP applications built using GPT-2\nmay harm women in professional capacities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honnavalli_S/0/1/0/all/0/1\">Samhita Honnavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_A/0/1/0/all/0/1\">Aesha Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Lily Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groenwold_S/0/1/0/all/0/1\">Sophie Groenwold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarization as Indirect Supervision for Relation Extraction. (arXiv:2205.09837v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09837","description":"<p>Relation extraction (RE) models have been challenged by their reliance on\ntraining data with expensive annotations. Considering that summarization tasks\naim at acquiring concise expressions of synoptical information from the longer\ncontext, these tasks naturally align with the objective of RE, i.e., extracting\na kind of synoptical information that describes the relation of entity\nmentions. We present SuRE, which converts RE into a summarization formulation.\nSuRE leads to more precise and resource-efficient RE based on indirect\nsupervision from summarization tasks. To achieve this goal, we develop sentence\nand relation conversion techniques that essentially bridge the formulation of\nsummarization and RE tasks. We also incorporate constraint decoding techniques\nwith Trie scoring to further enhance summarization-based RE with robust\ninference. Experiments on three RE datasets demonstrate the effectiveness of\nSuRE in both full-dataset and low-resource settings, showing that summarization\nis a promising source of indirect supervision to improve RE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A toolbox for idea generation and evaluation: Machine learning, data-driven, and contest-driven approaches to support idea generation. (arXiv:2205.09840v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09840","description":"<p>The significance and abundance of data are increasing due to the growing\ndigital data generated from social media, sensors, scholarly literature,\npatents, different forms of documents published online, databases, product\nmanuals, etc. Various data sources can be used to generate ideas, yet, in\naddition to bias, the size of the available digital data is a major challenge\nwhen it comes to manual analysis. Hence, human-machine interaction is essential\nfor generating valuable ideas where machine learning and data-driven techniques\ngenerate patterns from data and serve human sense-making. However, the use of\nmachine learning and data-driven approaches to generate ideas is a relatively\nnew area. Moreover, it is also possible to stimulate innovation using\ncontest-driven idea generation and evaluation. The results and contributions of\nthis thesis can be viewed as a toolbox of idea-generation techniques, including\na list of data-driven and machine learning techniques with corresponding data\nsources and models to support idea generation. In addition, the results include\ntwo models, one method and one framework, to better support data-driven and\ncontest- driven idea generation. The beneficiaries of these artefacts are\npractitioners in data and knowledge engineering, data mining project managers,\nand innovation agents. Innovation agents include incubators, contest\norganizers, consultants, innovation accelerators, and industries. Since the\nproposed artefacts consist of process models augmented with AI techniques,\nhuman-centred AI is a promising area of research that can contribute to the\nartefacts' further development and promote creativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayele_W/0/1/0/all/0/1\">Workneh Yilma Ayele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table Retrieval May Not Necessitate Table-specific Model Design. (arXiv:2205.09843v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09843","description":"<p>Tables are an important form of structured data for both human and machine\nreaders alike, providing answers to questions that cannot, or cannot easily, be\nfound in texts. Recent work has designed special models and training paradigms\nfor table-related tasks such as table-based question answering and table\nretrieval. Though effective, they add complexity in both modeling and data\nacquisition compared to generic text solutions and obscure which elements are\ntruly beneficial. In this work, we focus on the task of table retrieval, and\nask: \"is table-specific model design necessary for table retrieval, or can a\nsimpler text-based model be effectively used to achieve a similar result?\"\nFirst, we perform an analysis on a table-based portion of the Natural Questions\ndataset (NQ-table), and find that structure plays a negligible role in more\nthan 70% of the cases. Based on this, we experiment with a general Dense\nPassage Retriever (DPR) based on text and a specialized Dense Table Retriever\n(DTR) that uses table-specific model designs. We find that DPR performs well\nwithout any table-specific design and training, and even achieves superior\nresults compared to DTR when fine-tuned on properly linearized tables. We then\nexperiment with three modules to explicitly encode table structures, namely\nauxiliary row/column embeddings, hard attention masks, and soft relation-based\nattention biases. However, none of these yielded significant improvements,\nsuggesting that table-specific model design may not be necessary for table\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiruo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengbao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Bias in Meta-Embeddings. (arXiv:2205.09867v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09867","description":"<p>Combining multiple source embeddings to create meta-embeddings is considered\neffective to obtain more accurate embeddings. Different methods have been\nproposed to develop meta-embeddings from a given set of source embeddings.\nHowever, the source embeddings can contain unfair gender bias, and the bias in\nthe combination of multiple embeddings and debiasing it effectively have not\nbeen studied yet. In this paper, we investigate the bias in three types of\nmeta-embeddings: (1) Multi-Source No-Debiasing: meta-embedding from multiple\nsource embeddings without any debiasing. The experimental results show that\nmeta-embedding amplifies the gender bias compared to those of input source\nembeddings; (2) Multi-Source Single-Debiasing: meta-embedding from multiple\nsource embeddings debiased by a single method and it can be created in three\nways: debiasing each source embedding, debiasing the learned meta-embeddings,\nand debiasing both source embeddings and meta-embeddings. The results show that\ndebiasing both is the best in two out of three bias evaluation benchmarks; (3)\nSingle-Source Multi-Debiasing: meta-embedding from the same source embedding\ndebiased by different methods. It performed more effectively than its source\nembeddings debiased with a single method in all three bias evaluation\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let the Model Decide its Curriculum for Multitask Learning. (arXiv:2205.09898v1 [cs.LG])","link":"http://arxiv.org/abs/2205.09898","description":"<p>Curriculum learning strategies in prior multi-task learning approaches\narrange datasets in a difficulty hierarchy either based on human perception or\nby exhaustively searching the optimal arrangement. However, human perception of\ndifficulty may not always correlate well with machine interpretation leading to\npoor performance and exhaustive search is computationally expensive. Addressing\nthese concerns, we propose two classes of techniques to arrange training\ninstances into a learning curriculum based on difficulty scores computed via\nmodel-based approaches. The two classes i.e Dataset-level and Instance-level\ndiffer in granularity of arrangement. Through comprehensive experiments with 12\ndatasets, we show that instance-level and dataset-level techniques result in\nstrong representations as they lead to an average performance improvement of\n4.17% and 3.15% over their respective baselines. Furthermore, we find that most\nof this improvement comes from correctly answering the difficult instances,\nimplying a greater efficacy of our techniques on difficult tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. (arXiv:2205.09921v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09921","description":"<p>Relative positional embeddings (RPE) have received considerable attention\nsince RPEs effectively model the relative distance among tokens and enable\nlength extrapolation. We propose KERPLE, a framework that generalizes relative\nposition embedding for extrapolation by kernelizing positional differences. We\nachieve this goal using conditionally positive definite (CPD) kernels, a class\nof functions known for generalizing distance metrics. To maintain the inner\nproduct interpretation of self-attention, we show that a CPD kernel can be\ntransformed into a PD kernel by adding a constant offset. This offset is\nimplicitly absorbed in the Softmax normalization during self-attention. The\ndiversity of CPD kernels allows us to derive various RPEs that enable length\nextrapolation in a principled way. Experiments demonstrate that the logarithmic\nvariant achieves excellent extrapolation performance on three large language\nmodeling datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Ting-Han Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1\">Peter J. Ramadge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander I. Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SALTED: A Framework for SAlient Long-Tail Translation Error Detection. (arXiv:2205.09988v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09988","description":"<p>Traditional machine translation (MT) metrics provide an average measure of\ntranslation quality that is insensitive to the long tail of behavioral problems\nin MT. Examples include translation of numbers, physical units, dropped content\nand hallucinations. These errors, which occur rarely and unpredictably in\nNeural Machine Translation (NMT), greatly undermine the reliability of\nstate-of-the-art MT systems. Consequently, it is important to have visibility\ninto these problems during model development. Towards this direction, we\nintroduce SALTED, a specifications-based framework for behavioral testing of MT\nmodels that provides fine-grained views of salient long-tail errors, permitting\ntrustworthy visibility into previously invisible problems. At the core of our\napproach is the development of high-precision detectors that flag errors (or\nalternatively, verify output correctness) between a source sentence and a\nsystem output. We demonstrate that such detectors could be used not just to\nidentify salient long-tail errors in MT systems, but also for higher-recall\nfiltering of the training data, fixing targeted errors with model fine-tuning\nin NMT and generating novel data for metamorphic testing to elicit further bugs\nin models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raunak_V/0/1/0/all/0/1\">Vikas Raunak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Descartes: Generating Short Descriptions of Wikipedia Articles. (arXiv:2205.10012v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10012","description":"<p>We introduce and tackle the problem of automatically generating short\ndescriptions of Wikipedia articles (e.g., Belgium has a short description\nCountry in Western Europe). We introduce Descartes, a model that can generate\ndescriptions performing on par with human editors. Our human evaluation results\nindicate that Descartes is preferred over editor-written descriptions about 50%\nof time. Further manual analysis show that Descartes generates descriptions\nconsidered as \"valid\" for 91.3% of articles, this is the as same editor-written\ndescriptions. Such performances are made possible by integrating other signals\nnaturally existing in Wikipedia: (i) articles about the same entity in\ndifferent languages, (ii) existing short descriptions in other languages, and\n(iii) structural information from Wikidata. Our work has direct practical\napplications in helping Wikipedia editors to provide short descriptions for the\nmore than 9 million articles still missing one. Finally, our proposed\narchitecture can easily be re-purposed to address other information gaps in\nWikipedia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakota_M/0/1/0/all/0/1\">Marija Sakota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating Hanja historical documents to understandable Korean and English. (arXiv:2205.10019v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10019","description":"<p>The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of\nJoseon, the 500-year kingdom preceding the modern nation of Korea. The Annals\nwere originally written in an archaic Korean writing system, `Hanja', and\ntranslated into Korean from 1968 to 1993. However, this translation was literal\nand contained many archaic Korean words; thus, a new expert translation effort\nbegan in 2012, completing the records of only one king in a decade. Also,\nexpert translators are working on an English translation, of which only one\nking's records are available because of the high cost and slow progress. Thus,\nwe propose H2KE, the neural machine translation model that translates Hanja\nhistorical documents to understandable Korean and English. Based on the\nmultilingual neural machine translation approach, it translates the historical\ndocument written in Hanja, using both the full dataset of outdated Korean\ntranslation and a small dataset of recently translated Korean and English. We\ncompare our method with two baselines: one is a recent model that\nsimultaneously learns to restore and translate Hanja historical document and\nthe other is the transformer that trained on newly translated corpora only. The\nresults show that our method significantly outperforms the baselines in terms\nof BLEU score in both modern Korean and English translations. We also conduct a\nhuman evaluation that shows that our translation is preferred over the original\nexpert translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Juhee Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiho Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haneul Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_J/0/1/0/all/0/1\">JinYeong Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transition-based Semantic Role Labeling with Pointer Networks. (arXiv:2205.10023v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10023","description":"<p>Semantic role labeling (SRL) focuses on recognizing the predicate-argument\nstructure of a sentence and plays a critical role in many natural language\nprocessing tasks such as machine translation and question answering.\nPractically all available methods do not perform full SRL, since they rely on\npre-identified predicates, and most of them follow a pipeline strategy, using\nspecific models for undertaking one or several SRL subtasks. In addition,\nprevious approaches have a strong dependence on syntactic information to\nachieve state-of-the-art performance, despite being syntactic trees equally\nhard to produce. These simplifications and requirements make the majority of\nSRL systems impractical for real-world applications. In this article, we\npropose the first transition-based SRL approach that is capable of completely\nprocessing an input sentence in a single left-to-right pass, with neither\nleveraging syntactic information nor resorting to additional modules. Thanks to\nour implementation based on Pointer Networks, full SRL can be accurately and\nefficiently done in $O(n^2)$, achieving the best performance to date on the\nmajority of languages from the CoNLL-2009 shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Extreme Parameter Compression for Pre-trained Language Models. (arXiv:2205.10036v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10036","description":"<p>Recent work explored the potential of large-scale Transformer-based\npre-trained models, especially Pre-trained Language Models (PLMs) in natural\nlanguage processing. This raises many concerns from various perspectives, e.g.,\nfinancial costs and carbon emissions. Compressing PLMs like BERT with\nnegligible performance loss for faster inference and cheaper deployment has\nattracted much attention. In this work, we aim to explore larger compression\nratios for PLMs, among which tensor decomposition is a potential but\nunder-investigated one. Two decomposition and reconstruction protocols are\nfurther proposed to improve the effectiveness and efficiency during\ncompression. Our compressed BERT with ${1}/{7}$ parameters in Transformer\nlayers performs on-par with, sometimes slightly better than the original BERT\nin GLUE benchmark. A tiny version achieves $96.7\\%$ performance of BERT-base\nwith $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding\nthe embedding layer) and $2.7 \\times$ faster on inference. To show that the\nproposed method is orthogonal to existing compression methods like knowledge\ndistillation, we also explore the benefit of the proposed method on a distilled\nBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking. (arXiv:2205.10059v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10059","description":"<p>In dialogue state tracking, dialogue history is a crucial material, and its\nutilization varies between different models. However, no matter how the\ndialogue history is used, each existing model uses its own consistent dialogue\nhistory during the entire state tracking process, regardless of which slot is\nupdated. Apparently, it requires different dialogue history to update different\nslots in different turns. Therefore, using consistent dialogue contents may\nlead to insufficient or redundant information for different slots, which\naffects the overall performance. To address this problem, we devise DiCoS-DST\nto dynamically select the relevant dialogue contents corresponding to each slot\nfor state updating. Specifically, it first retrieves turn-level utterances of\ndialogue history and evaluates their relevance to the slot from a combination\nof three perspectives: (1) its explicit connection to the slot name; (2) its\nrelevance to the current turn dialogue; (3) Implicit Mention Oriented\nReasoning. Then these perspectives are combined to yield a decision, and only\nthe selected dialogue contents are fed into State Generator, which explicitly\nminimizes the distracting information passed to the downstream state\nprediction. Experimental results show that our approach achieves new\nstate-of-the-art performance on MultiWOZ 2.1 and MultiWOZ 2.2, and achieves\nsuperior performance on multiple mainstream benchmark datasets (including\nSim-M, Sim-R, and DSTC2).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jinyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_K/0/1/0/all/0/1\">Kai Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Mitigating the Uncertainty in Zero-Shot Translation. (arXiv:2205.10068v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10068","description":"<p>Zero-shot translation is a promising direction for building a comprehensive\nmultilingual neural machine translation (MNMT) system. However, its quality is\nstill not satisfactory due to off-target issues. In this paper, we aim to\nunderstand and alleviate the off-target issues from the perspective of\nuncertainty in zero-shot translation. By carefully examining the translation\noutput and model confidence, we identify two uncertainties that are responsible\nfor the off-target issues, namely, extrinsic data uncertainty and intrinsic\nmodel uncertainty. Based on the observations, we propose two light-weight and\ncomplementary approaches to denoise the training data for model training, and\nmask out the vocabulary of the off-target languages in inference. Extensive\nexperiments on both balanced and unbalanced datasets show that our approaches\nsignificantly improve the performance of zero-shot translation over strong MNMT\nbaselines. Qualitative analyses provide insights into where our approaches\nreduce off-target translations\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1\">Wenxiang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uzbek affix finite state machine for stemming. (arXiv:2205.10078v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10078","description":"<p>This work presents a morphological analyzer for the Uzbek language using a\nfinite state machine. The proposed methodology is a morphologic analysis of\nUzbek words by using an affix striping to find a root and without including any\nlexicon. This method helps to perform morphological analysis of words from a\nlarge amount of text at high speed as well as it is not required using of\nmemory for keeping vocabulary. According to Uzbek, an agglutinative language\ncan be designed with finite state machines (FSMs). In contrast to the previous\nworks, this study modeled the completed FSMs for all word classes by using the\nUzbek language's morphotactic rules in right to left order. This paper shows\nthe stages of this methodology including the classification of the affixes, the\ngeneration of the FSMs for each affix class, and the combination into a head\nmachine to make analysis a word.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharipov_M/0/1/0/all/0/1\">Maksud Sharipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salaev_U/0/1/0/all/0/1\">Ulugbek Salaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-self-supervised Automated ICD Coding. (arXiv:2205.10088v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10088","description":"<p>Clinical Text Notes (CTNs) contain physicians' reasoning process, written in\nan unstructured free text format, as they examine and interview patients. In\nrecent years, several studies have been published that provide evidence for the\nutility of machine learning for predicting doctors' diagnoses from CTNs, a task\nknown as ICD coding. Data annotation is time consuming, particularly when a\ndegree of specialization is needed, as is the case for medical data. This paper\npresents a method of augmenting a sparsely annotated dataset of Icelandic CTNs\nwith a machine-learned imputation in a semi-self-supervised manner. We train a\nneural network on a small set of annotated CTNs and use it to extract clinical\nfeatures from a set of un-annotated CTNs. These clinical features consist of\nanswers to about a thousand potential questions that a physician might find the\nanswers to during a consultation of a patient. The features are then used to\ntrain a classifier for the diagnosis of certain types of diseases. We report\nthe results of an evaluation of this data augmentation method over three tiers\nof data availability to the physician. Our data augmentation method shows a\nsignificant positive effect which is diminished when clinical features from the\nexamination of the patient and diagnostics are made available. We recommend our\nmethod for augmenting scarce datasets for systems that take decisions based on\nclinical features that do not include examinations or tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hlynsson_H/0/1/0/all/0/1\">Hlynur D. Hlynsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellertsson_S/0/1/0/all/0/1\">Steind&#xf3;r Ellertsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da%7B%5Cdh%7Dason_J/0/1/0/all/0/1\">J&#xf3;n F. Da&#xf0;ason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigurdsson_E/0/1/0/all/0/1\">Emil L. Sigurdsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loftsson_H/0/1/0/all/0/1\">Hrafn Loftsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to keep text private? A systematic review of deep learning methods for privacy-preserving natural language processing. (arXiv:2205.10095v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10095","description":"<p>Deep learning (DL) models for natural language processing (NLP) tasks often\nhandle private data, demanding protection against breaches and disclosures.\nData protection laws, such as the European Union's General Data Protection\nRegulation (GDPR), thereby enforce the need for privacy. Although many\nprivacy-preserving NLP methods have been proposed in recent years, no\ncategories to organize them have been introduced yet, making it hard to follow\nthe progress of the literature. To close this gap, this article systematically\nreviews over sixty DL methods for privacy-preserving NLP published between 2016\nand 2020, covering theoretical foundations, privacy-enhancing technologies, and\nanalysis of their suitability for real-world scenarios. First, we introduce a\nnovel taxonomy for classifying the existing methods into three categories: data\nsafeguarding methods, trusted methods, and verification methods. Second, we\npresent an extensive summary of privacy threats, datasets for applications, and\nmetrics for privacy evaluation. Third, throughout the review, we describe\nprivacy issues in the NLP pipeline in a holistic view. Further, we discuss open\nchallenges in privacy-preserving NLP regarding data traceability, computation\noverhead, dataset size, the prevalence of human biases in embeddings, and the\nprivacy-utility tradeoff. Finally, this review presents future research\ndirections to guide successive research and development of privacy-preserving\nNLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_S/0/1/0/all/0/1\">Samuel Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kern_R/0/1/0/all/0/1\">Roman Kern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually-Augmented Language Modeling. (arXiv:2205.10178v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10178","description":"<p>Human language is grounded on multimodal knowledge including visual knowledge\nlike colors, sizes, and shapes. However, current large-scale pre-trained\nlanguage models rely on the text-only self-supervised training with massive\ntext data, which precludes them from utilizing relevant visual information when\nnecessary. To address this, we propose a novel pre-training framework, named\nVaLM, to Visually-augment text tokens with retrieved relevant images for\nLanguage Modeling. Specifically, VaLM builds on a novel text-vision alignment\nmethod via an image retrieval module to fetch corresponding images given a\ntextual context. With the visually-augmented context, VaLM uses a visual\nknowledge fusion layer to enable multimodal grounded language modeling by\nattending on both text context and visual knowledge in images. We evaluate the\nproposed model on various multimodal commonsense reasoning tasks, which require\nvisual information to excel. VaLM outperforms the text-only baseline with\nsubstantial gains of +8.66% and +37.81% accuracy on object color and size\nreasoning, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haoyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototypical Calibration for Few-shot Learning of Language Models. (arXiv:2205.10183v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10183","description":"<p>In-context learning of GPT-like models has been recognized as fragile across\ndifferent hand-crafted templates, and demonstration permutations. In this work,\nwe propose prototypical calibration to adaptively learn a more robust decision\nboundary for zero- and few-shot classification, instead of greedy decoding.\nConcretely, our method first adopts Gaussian mixture distribution to estimate\nthe prototypical clusters for all categories. Then we assign each cluster to\nthe corresponding label by solving a weighted bipartite matching problem. Given\nan example, its prediction is calibrated by the likelihood of prototypical\nclusters. Experimental results show that prototypical calibration yields a 15%\nabsolute improvement on a diverse set of tasks. Extensive analysis across\ndifferent scales also indicates that our method calibrates the decision\nboundary as expected, greatly improving the robustness of GPT to templates,\npermutations, and class imbalance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhixiong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Class Semantic Matching for Semi-supervised Text Classification. (arXiv:2205.10189v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10189","description":"<p>Semi-supervised learning is a promising way to reduce the annotation cost for\ntext-classification. Combining with pre-trained language models (PLMs), e.g.,\nBERT, recent semi-supervised learning methods achieved impressive performance.\nIn this work, we further investigate the marriage between semi-supervised\nlearning and a pre-trained language model. Unlike existing approaches that\nutilize PLMs only for model parameter initialization, we explore the inherent\ntopic matching capability inside PLMs for building a more powerful\nsemi-supervised learning approach. Specifically, we propose a joint\nsemi-supervised learning process that can progressively build a standard\n$K$-way classifier and a matching network for the input text and the Class\nSemantic Representation (CSR). The CSR will be initialized from the given\nlabeled sentences and progressively updated through the training process. By\nmeans of extensive experiments, we show that our method can not only bring\nremarkable improvement to baselines, but also overall be more stable, and\nachieves state-of-the-art performance in semi-supervised text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hai-Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1\">Ehsan Abbasnejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Trade-off between Redundancy and Local Coherence in Summarization. (arXiv:2205.10192v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10192","description":"<p>Extractive summarization systems are known to produce poorly coherent and, if\nnot accounted for, highly redundant text. In this work, we tackle the problem\nof summary redundancy in unsupervised extractive summarization of long,\nhighly-redundant documents. For this, we leverage a psycholinguistic theory of\nhuman reading comprehension which directly models local coherence and\nredundancy. Implementing this theory, our system operates at the proposition\nlevel and exploits properties of human memory representations to rank similarly\ncontent units that are coherent and non-redundant, hence encouraging the\nextraction of less redundant final summaries. Because of the impact of the\nsummary length on automatic measures, we control for it by formulating content\nselection as an optimization problem with soft constraints in the budget of\ninformation retrieved. Using summarization of scientific articles as a case\nstudy, extensive experiments demonstrate that the proposed systems extract\nconsistently less redundant summaries across increasing levels of document\nredundancy, whilst maintaining comparable performance (in terms of relevancy\nand local coherence) against strong unsupervised baselines according to\nautomated evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cardenas_R/0/1/0/all/0/1\">Ronald Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Galle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?. (arXiv:2205.10226v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10226","description":"<p>Learned self-attention functions in state-of-the-art NLP models often\ncorrelate with human attention. We investigate whether self-attention in\nlarge-scale pre-trained language models is as predictive of human eye fixation\npatterns during task-reading as classical cognitive models of human attention.\nWe compare attention functions across two task-specific reading datasets for\nsentiment analysis and relation extraction. We find the predictiveness of\nlarge-scale pre-trained self-attention for human attention depends on `what is\nin the tail', e.g., the syntactic nature of rare contexts. Further, we observe\nthat task-specific fine-tuning does not increase the correlation with human\ntask-specific reading. Through an input reduction experiment we give\ncomplementary insights on the sparsity and fidelity trade-off, showing that\nlower-entropy attention vectors are more faithful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eberle_O/0/1/0/all/0/1\">Oliver Eberle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilot_J/0/1/0/all/0/1\">Jonas Pilot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Anchored Contrastive Learning for Language Understanding. (arXiv:2205.10227v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10227","description":"<p>Contrastive learning (CL) has achieved astonishing progress in computer\nvision, speech, and natural language processing fields recently with\nself-supervised learning. However, CL approach to the supervised setting is not\nfully explored, especially for the natural language understanding\nclassification task. Intuitively, the class label itself has the intrinsic\nability to perform hard positive/negative mining, which is crucial for CL.\nMotivated by this, we propose a novel label anchored contrastive learning\napproach (denoted as LaCon) for language understanding. Specifically, three\ncontrastive objectives are devised, including a multi-head instance-centered\ncontrastive loss (ICL), a label-centered contrastive loss (LCL), and a label\nembedding regularizer (LER). Our approach does not require any specialized\nnetwork architecture or any extra data augmentation, thus it can be easily\nplugged into existing powerful pre-trained language models. Compared to the\nstate-of-the-art baselines, LaCon obtains up to 4.1% improvement on the popular\ndatasets of GLUE and CLUE benchmarks. Besides, LaCon also demonstrates\nsignificant advantages under the few-shot and data imbalance settings, which\nobtains up to 9.4% improvement on the FewGLUE and FewCLUE benchmarking tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas. (arXiv:2205.10228v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10228","description":"<p>Social chatbots, also known as chit-chat chatbots, evolve rapidly with large\npretrained language models. Despite the huge progress, privacy concerns have\narisen recently: training data of large language models can be extracted via\nmodel inversion attacks. On the other hand, the datasets used for training\nchatbots contain many private conversations between two individuals. In this\nwork, we further investigate the privacy leakage of the hidden states of\nchatbots trained by language modeling which has not been well studied yet. We\nshow that speakers' personas can be inferred through a simple neural network\nwith high accuracy. To this end, we propose effective defense objectives to\nprotect persona leakage from hidden states. We conduct extensive experiments to\ndemonstrate that our proposed defense objectives can greatly reduce the attack\naccuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve\nlanguage models' powerful generation ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lixin Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RigoBERTa: A State-of-the-Art Language Model For Spanish. (arXiv:2205.10233v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10233","description":"<p>This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.\nRigoBERTa is trained over RigoCorpus, a well-curated corpus formed up from\ndifferent subcorpora with key features. It follows the DeBERTa architecture,\nwhich has several advantages over other architectures of similar size as BERT\nor RoBERTa.\n</p>\n<p>RigoBERTa performance is assessed over 13 NLU tasks in comparison with other\navailable Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa\noutperformed the three models in 10 out of the 13 tasks, achieving new\n\"State-of-the-Art\" results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serrano_A/0/1/0/all/0/1\">Alejandro Vaca Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subies_G/0/1/0/all/0/1\">Guillem Garcia Subies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamorano_H/0/1/0/all/0/1\">Helena Montoro Zamorano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Nuria Aldama Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samy_D/0/1/0/all/0/1\">Doaa Samy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1\">David Betancur Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandoval_A/0/1/0/all/0/1\">Antonio Moreno Sandoval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1\">Marta Guerrero Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_A/0/1/0/all/0/1\">Alvaro Barbero Jimenez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated learning for violence incident prediction in a simulated cross-institutional psychiatric setting. (arXiv:2205.10234v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10234","description":"<p>Inpatient violence is a common and severe problem within psychiatry. Knowing\nwho might become violent can influence staffing levels and mitigate severity.\nPredictive machine learning models can assess each patient's likelihood of\nbecoming violent based on clinical notes. Yet, while machine learning models\nbenefit from having more data, data availability is limited as hospitals\ntypically do not share their data for privacy preservation. Federated Learning\n(FL) can overcome the problem of data limitation by training models in a\ndecentralised manner, without disclosing data between collaborators. However,\nalthough several FL approaches exist, none of these train Natural Language\nProcessing models on clinical notes. In this work, we investigate the\napplication of Federated Learning to clinical Natural Language Processing,\napplied to the task of Violence Risk Assessment by simulating a\ncross-institutional psychiatric setting. We train and compare four models: two\nlocal models, a federated model and a data-centralised model. Our results\nindicate that the federated model outperforms the local models and has similar\nperformance as the data-centralised model. These findings suggest that\nFederated Learning can be used successfully in a cross-institutional setting\nand is a step towards new applications of Federated Learning based on clinical\nnotes\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borger_T/0/1/0/all/0/1\">Thomas Borger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosteiro_P/0/1/0/all/0/1\">Pablo Mosteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaya_H/0/1/0/all/0/1\">Heysem Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijcken_E/0/1/0/all/0/1\">Emil Rijcken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salah_A/0/1/0/all/0/1\">Albert Ali Salah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheepers_F/0/1/0/all/0/1\">Floortje Scheepers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spruit_M/0/1/0/all/0/1\">Marco Spruit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database. (arXiv:2205.10237v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10237","description":"<p>The emotional state of a speaker can be influenced by many different factors\nin dialogues, such as dialogue scene, dialogue topic, and interlocutor\nstimulus. The currently available data resources to support such multimodal\naffective analysis in dialogues are however limited in scale and diversity. In\nthis work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue\ndataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different\nTV series, a total of 9,082 turns and 24,449 utterances. M3 ED is annotated\nwith 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and\nneutral) at utterance level, and encompasses acoustic, visual, and textual\nmodalities. To the best of our knowledge, M3ED is the first multimodal\nemotional dialogue dataset in Chinese. It is valuable for cross-culture emotion\nanalysis and recognition. We apply several state-of-the-art methods on the M3ED\ndataset to verify the validity and quality of the dataset. We also propose a\ngeneral Multimodal Dialogue-aware Interaction framework, MDI, to model the\ndialogue context for emotion recognition, which achieves comparable performance\nto the state-of-the-art methods on the M3ED. The full dataset and codes are\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tenggan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing and Explaining Language Models. (arXiv:2205.10238v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10238","description":"<p>During the last decade, Natural Language Processing has become, after\nComputer Vision, the second field of Artificial Intelligence that was massively\nchanged by the advent of Deep Learning. Regardless of the architecture, the\nlanguage models of the day need to be able to process or generate text, as well\nas predict missing words, sentences or relations depending on the task. Due to\ntheir black-box nature, such models are difficult to interpret and explain to\nthird parties. Visualization is often the bridge that language model designers\nuse to explain their work, as the coloring of the salient words and phrases,\nclustering or neuron activations can be used to quickly understand the\nunderlying models. This paper showcases the techniques used in some of the most\npopular Deep Learning for NLP visualizations, with a special focus on\ninterpretability and explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brasoveanu_A/0/1/0/all/0/1\">Adrian M.P. Bra&#x15f;oveanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonie_R/0/1/0/all/0/1\">R&#x103;zvan Andonie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterformer: A Transformer Architecture for Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10282","description":"<p>We study node representation learning on heterogeneous text-rich networks,\nwhere nodes and edges are multi-typed and some types of nodes are associated\nwith text information. Although recent studies on graph neural networks (GNNs)\nand pretrained language models (PLMs) have demonstrated their power in encoding\nnetwork and text signals, respectively, less focus has been given to delicately\ncoupling these two types of models on heterogeneous text-rich networks.\nSpecifically, existing GNNs rarely model text in each node in a contextualized\nway; existing PLMs can hardly be applied to characterize graph structures due\nto their sequence architecture. In this paper, we propose Heterformer, a\nHeterogeneous GNN-nested transformer that blends GNNs and PLMs into a unified\nmodel. Different from previous \"cascaded architectures\" that directly add GNN\nlayers upon a PLM, our Heterformer alternately stacks two modules - a\ngraph-attention-based neighbor aggregation module and a transformer-based text\nand neighbor joint encoding module - to facilitate thorough mutual enhancement\nbetween network and text signals. Meanwhile, Heterformer is capable of\ncharacterizing network heterogeneity and nodes without text information.\nComprehensive experiments on three large-scale datasets from different domains\ndemonstrate the superiority of Heterformer over state-of-the-art baselines in\nlink prediction, transductive/inductive node classification, node clustering,\nand semantics-based retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bowen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClusterEA: Scalable Entity Alignment with Stochastic Training and Normalized Mini-batch Similarities. (arXiv:2205.10312v1 [cs.DB])","link":"http://arxiv.org/abs/2205.10312","description":"<p>Entity alignment (EA) aims at finding equivalent entities in different\nknowledge graphs (KGs). Embedding-based approaches have dominated the EA task\nin recent years. Those methods face problems that come from the geometric\nproperties of embedding vectors, including hubness and isolation. To solve\nthese geometric problems, many normalization approaches have been adopted to\nEA. However, the increasing scale of KGs renders it is hard for EA models to\nadopt the normalization processes, thus limiting their usage in real-world\napplications. To tackle this challenge, we present ClusterEA, a general\nframework that is capable of scaling up EA models and enhancing their results\nby leveraging normalization methods on mini-batches with a high entity\nequivalent rate. ClusterEA contains three components to align entities between\nlarge-scale KGs, including stochastic training, ClusterSampler, and\nSparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic\nfashion to produce entity embeddings. Based on the embeddings, a novel\nClusterSampler strategy is proposed for sampling highly overlapped\nmini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes\nlocal and global similarity and then fuses all similarity matrices to obtain\nthe final similarity matrix. Extensive experiments with real-life datasets on\nEA benchmarks offer insight into the proposed framework, and suggest that it is\ncapable of outperforming the state-of-the-art scalable EA framework by up to 8\ntimes in terms of Hits@1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lossless Acceleration for Seq2seq Generation with Aggressive Decoding. (arXiv:2205.10350v1 [cs.CL])","link":"http://arxiv.org/abs/2205.10350","description":"<p>We study lossless acceleration for seq2seq generation with a novel decoding\nalgorithm -- Aggressive Decoding. Unlike the previous efforts (e.g.,\nnon-autoregressive decoding) speeding up seq2seq generation at the cost of\nquality loss, our approach aims to yield the identical (or better) generation\ncompared with autoregressive decoding but in a significant speedup, achieved by\ninnovative cooperation of aggressive decoding and verification that are both\nefficient due to parallel computing.\n</p>\n<p>We propose two Aggressive Decoding paradigms for 2 kinds of seq2seq tasks: 1)\nFor the seq2seq tasks whose inputs and outputs are highly similar (e.g.,\nGrammatical Error Correction), we propose Input-guided Aggressive Decoding\n(IAD) that aggressively copies from the input sentence as drafted decoded\ntokens to verify in parallel; 2) For other general seq2seq tasks (e.g., Machine\nTranslation), we propose Generalized Aggressive Decoding (GAD) that first\nemploys an additional non-autoregressive decoding model for aggressive decoding\nand then verifies in parallel in the autoregressive manner.\n</p>\n<p>We test Aggressive Decoding on the most popular 6-layer Transformer model on\nGPU in multiple seq2seq tasks: 1) For IAD, we show that it can introduce a\n7x-9x speedup for the Transformer in Grammatical Error Correction and Text\nSimplification tasks with the identical results as greedy decoding; 2) For GAD,\nwe observe a 3x-5x speedup with the identical or even better quality in two\nimportant seq2seq tasks: Machine Translation and Abstractive Summarization.\nMoreover, Aggressive Decoding can benefit even more from stronger computing\ndevices that are better at parallel computing. Given the lossless quality as\nwell as significant and promising speedup, we believe Aggressive Decoding may\npotentially evolve into a de facto standard for efficient and lossless seq2seq\ngeneration in the near future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Heming Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si-Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selecting Informative Contexts Improves Language Model Finetuning. (arXiv:2005.00175v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00175","description":"<p>Language model fine-tuning is essential for modern natural language\nprocessing, but is computationally expensive and time-consuming. Further, the\neffectiveness of fine-tuning is limited by the inclusion of training examples\nthat negatively affect performance. Here we present a general fine-tuning\nmethod that we call information gain filtration for improving the overall\ntraining efficiency and final performance of language model fine-tuning. We\ndefine the information gain of an example as the improvement on a test metric\nafter training on that example. A secondary learner is then trained to\napproximate this quantity. During fine-tuning, this learner selects informative\nexamples and skips uninformative ones. We show that our method has consistent\nimprovement across datasets, fine-tuning tasks, and language model\narchitectures. For example, we achieve a median perplexity of 54.0 on a books\ndataset compared to 57.3 for standard fine-tuning. We present statistical\nevidence that offers insight into the improvements of our method over standard\nfine-tuning. The generality of our method leads us to propose a new paradigm\nfor language model fine-tuning -- we encourage researchers to release\npretrained secondary learners on common corpora to promote efficient and\neffective fine-tuning, thereby improving the performance and reducing the\noverall energy footprint of language model fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antonello_R/0/1/0/all/0/1\">Richard Antonello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beckage_N/0/1/0/all/0/1\">Nicole Beckage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turek_J/0/1/0/all/0/1\">Javier Turek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huth_A/0/1/0/all/0/1\">Alexander Huth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Text Simplification. (arXiv:2008.08612v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.08612","description":"<p>Text Simplification (TS) aims to reduce the linguistic complexity of content\nto make it easier to understand. Research in TS has been of keen interest,\nespecially as approaches to TS have shifted from manual, hand-crafted rules to\nautomated simplification. This survey seeks to provide a comprehensive overview\nof TS, including a brief description of earlier approaches used, discussion of\nvarious aspects of simplification (lexical, semantic and syntactic), and latest\ntechniques being utilized in the field. We note that the research in the field\nhas clearly shifted towards utilizing deep learning techniques to perform TS,\nwith a specific focus on developing solutions to combat the lack of data\navailable for simplification. We also include a discussion of datasets and\nevaluations metrics commonly used, along with discussion of related fields\nwithin Natural Language Processing (NLP), like semantic similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sikka_P/0/1/0/all/0/1\">Punardeep Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mago_V/0/1/0/all/0/1\">Vijay Mago</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models. (arXiv:2104.08666v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08666","description":"<p>Numerous works have analyzed biases in vision and pre-trained language models\nindividually - however, less attention has been paid to how these biases\ninteract in multimodal settings. This work extends text-based bias analysis\nmethods to investigate multimodal language models, and analyzes intra- and\ninter-modality associations and biases learned by these models. Specifically,\nwe demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often\npreferring to reinforce a stereotype over faithfully describing the visual\nscene. We demonstrate these findings on a controlled case-study and extend them\nfor a larger set of stereotypically gendered entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1\">Tejas Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Out-of-Domain Detection via Pre-trained Transformers. (arXiv:2106.00948v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00948","description":"<p>Deployed real-world machine learning applications are often subject to\nuncontrolled and even potentially malicious inputs. Those out-of-domain inputs\ncan lead to unpredictable outputs and sometimes catastrophic safety issues.\nPrior studies on out-of-domain detection require in-domain task labels and are\nlimited to supervised classification scenarios. Our work tackles the problem of\ndetecting out-of-domain samples with only unsupervised in-domain data. We\nutilize the latent representations of pre-trained transformers and propose a\nsimple yet effective method to transform features across all layers to\nconstruct out-of-domain detectors efficiently. Two domain-specific fine-tuning\napproaches are further proposed to boost detection accuracy. Our empirical\nevaluations of related methods on two datasets validate that our method greatly\nimproves out-of-domain detection ability in a more general scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Keyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1\">Tongzheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yihao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WALNUT: A Benchmark on Weakly Supervised Learning for Natural Language Understanding. (arXiv:2108.12603v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12603","description":"<p>Building machine learning models for natural language understanding (NLU)\ntasks relies heavily on labeled data. Weak supervision has been proven valuable\nwhen large amount of labeled data is unavailable or expensive to obtain.\nExisting works studying weak supervision for NLU either mostly focus on a\nspecific task or simulate weak supervision signals from ground-truth labels. It\nis thus hard to compare different approaches and evaluate the benefit of weak\nsupervision without access to a unified and systematic benchmark with diverse\ntasks and real-world weak labeling rules. In this paper, we propose such a\nbenchmark, named WALNUT (semi-WeAkly supervised Learning for Natural language\nUnderstanding Testbed), to advocate and facilitate research on weak supervision\nfor NLU. WALNUT consists of NLU tasks with different types, including\ndocument-level and token-level prediction tasks. WALNUT is the first\nsemi-weakly supervised learning benchmark for NLU, where each task contains\nweak labels generated by multiple real-world weak sources, together with a\nsmall set of clean labels. We conduct baseline evaluations on WALNUT to\nsystematically evaluate the effectiveness of various weak supervision methods\nand model architectures. Our results demonstrate the benefit of weak\nsupervision for low-resource NLU tasks and highlight interesting patterns\nacross tasks. We expect WALNUT to stimulate further research on methodologies\nto leverage weak supervision more effectively. The benchmark and code for\nbaselines are available at \\url{aka.ms/walnut_benchmark}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamanolakis_G/0/1/0/all/0/1\">Giannis Karamanolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1\">Kai Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11797","description":"<p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for large amounts of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, CPT\nenables strong few-shot and even zero-shot visual grounding capabilities of\nVL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs\noutperform their fine-tuned counterparts by a large margin (e.g., 17.3%\nabsolute accuracy improvement, and 73.8% relative standard deviation reduction\non average with one shot in RefCOCO evaluation). We make the data and code for\nthis paper publicly available at https://github.com/thunlp/CPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in Abstractive Summarization. (arXiv:2110.07166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07166","description":"<p>Hallucination is a known issue for neural abstractive summarization models.\nRecent work suggests that the degree of hallucination may depend on errors in\nthe training data. In this work, we propose a new method called Contrastive\nParameter Ensembling (CaPE) to use training data more effectively, utilizing\nvariations in noise in training samples to reduce hallucination. We first\nselect clean and noisy subsets from the training data using different automatic\nfactual metrics. Then, we fine-tune a base summarization model, which is\ntrained on all training samples, on the clean (noisy) subset to obtain an\n\\textit{expert} (\\textit{anti-expert}) model. Finally, we adjust the parameters\nof base model by the difference between parameters of the \\textit{expert} and\n\\textit{anti-expert} models, steering the base model towards the\n\\textit{expert} model and away from the \\textit{anti-expert} model.\nExperimental results show that CaPE improves performance across different\nautomatic factual metrics and human evaluation, with the maximum improvement of\n16.69\\% and 15.78\\% on summary-level dependency-arc entailment accuracy for the\nXSUM and CNN/DM datasets. The improvement in factual performance does not\ndegrade the performance on other metrics of informativeness such as ROUGE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. (arXiv:2110.08412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08412","description":"<p>To explain NLP models, importance measures such as attention inform which\ninputs tokens are important for a prediction are popular. However, an open\nquestion is how well these explanations accurately reflect a model's logic, a\nproperty called faithfulness.\n</p>\n<p>To answer this question, we propose an new faithfulness benchmark called\nRecursive ROAR. This works by recursively masking allegedly important tokens\nand then retrain the model. The principle is, that this should result in worse\nmodel performance compared to masking random tokens. The result is a\nperformance curve given a masking-ratio. Furthermore, we propose a summarizing\nmetric using the area-between-curves, which allows for easy comparison across\npapers, models, and tasks.\n</p>\n<p>To provide a thorough review, we evaluate 4 different importance measures on\n8 different datasets, using both LSTM-attention models and RoBERTa models.\n</p>\n<p>We find that the faithfulness of importance measures is both model-dependent\nand task-dependent. This conclusion contradicts previous evaluations in both\ncomputer vision and faithfulness of attention literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meade_N/0/1/0/all/0/1\">Nicholas Meade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Character-level HyperNetworks for Hate Speech Detection. (arXiv:2111.06336v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06336","description":"<p>The massive spread of hate speech, hateful content targeted at specific\nsubpopulations, is a problem of critical social importance. Automated methods\nof hate speech detection typically employ state-of-the-art deep learning\n(DL)-based text classifiers-large pretrained neural language models of over 100\nmillion parameters, adapting these models to the task of hate speech detection\nusing relevant labeled datasets. Unfortunately, there are only a few public\nlabeled datasets of limited size that are available for this purpose. We make\nseveral contributions with high potential for advancing this state of affairs.\nWe present HyperNetworks for hate speech detection, a special class of DL\nnetworks whose weights are regulated by a small-scale auxiliary network. These\narchitectures operate at character-level, as opposed to word or subword-level,\nand are several orders of magnitude smaller compared to the popular DL\nclassifiers. We further show that training hate detection classifiers using\nadditional large amounts of automatically generated examples is beneficial in\ngeneral, yet this practice especially boosts the performance of the proposed\nHyperNetworks. We report the results of extensive experiments, assessing the\nperformance of multiple neural architectures on hate detection using five\npublic datasets. The assessed methods include the pretrained language models of\nBERT, RoBERTa, ALBERT, MobileBERT and CharBERT, a variant of BERT that\nincorporates character alongside subword embeddings. In addition to the\ntraditional setup of within-dataset evaluation, we perform cross-dataset\nevaluation experiments, testing the generalization of the various models in\nconditions of data shift. Our results show that the proposed HyperNetworks\nachieve performance that is competitive, and better in some cases, than these\npretrained language models, while being smaller by orders of magnitude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1\">Tomer Wullach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Amir Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minkov_E/0/1/0/all/0/1\">Einat Minkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proposition-Level Clustering for Multi-Document Summarization. (arXiv:2112.08770v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08770","description":"<p>Text clustering methods were traditionally incorporated into multi-document\nsummarization (MDS) as a means for coping with considerable information\nrepetition. Particularly, clusters were leveraged to indicate information\nsaliency as well as to avoid redundancy. Such prior methods focused on\nclustering sentences, even though closely related sentences usually contain\nalso non-aligned parts. In this work, we revisit the clustering approach,\ngrouping together sub-sentential propositions, aiming at more precise\ninformation alignment. Specifically, our method detects salient propositions,\nclusters them into paraphrastic clusters, and generates a representative\nsentence for each cluster via text fusion. Our summarization method improves\nover the previous state-of-the-art MDS method in the DUC 2004 and TAC 2011\ndatasets, both in automatic ROUGE scores and human preference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Ori Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_O/0/1/0/all/0/1\">Ori Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1\">Jacob Goldberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond. (arXiv:2201.11443v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11443","description":"<p>The shift towards publicly available text sources has enabled language\nprocessing at unprecedented scale, yet leaves under-serviced the domains where\npublic and openly licensed data is scarce. Proactively collecting text data for\nresearch is a viable strategy to address this scarcity, but lacks systematic\nmethodology taking into account the many ethical, legal and\nconfidentiality-related aspects of data collection. Our work presents a case\nstudy on proactive data collection in peer review -- a challenging and\nunder-resourced NLP domain. We outline ethical and legal desiderata for\nproactive data collection and introduce \"Yes-Yes-Yes\", the first donation-based\npeer reviewing data collection workflow that meets these requirements. We\nreport on the implementation of Yes-Yes-Yes at ACL Rolling Review and\nempirically study the implications of proactive data collection for dataset\nsize and the biases induced by the donation behavior on the peer reviewing\nplatform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dycke_N/0/1/0/all/0/1\">Nils Dycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptor: Objective-Centric Adaptation Framework for Language Models. (arXiv:2203.03989v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03989","description":"<p>Progress in natural language processing research is catalyzed by the\npossibilities given by the widespread software frameworks. This paper\nintroduces Adaptor library that transposes the traditional model-centric\napproach composed of pre-training + fine-tuning steps to objective-centric\napproach, composing the training process by applications of selected\nobjectives. We survey research directions that can benefit from enhanced\nobjective-centric experimentation in multitask training, custom objectives\ndevelopment, dynamic training curricula, or domain adaptation. Adaptor aims to\nease reproducibility of these research directions in practice. Finally, we\ndemonstrate the practical applicability of Adaptor in selected unsupervised\ndomain adaptation scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groverova_N/0/1/0/all/0/1\">Nikola Groverov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09590","description":"<p>Within the emerging research efforts to combine structured and unstructured\nknowledge, many approaches incorporate factual knowledge, e.g., available in\nform of structured knowledge graphs (KGs), into pre-trained language models\n(PLMs) and then apply the knowledge-enhanced PLMs to downstream NLP tasks.\nHowever, (1) they typically only consider \\textit{static} factual knowledge,\nwhereas, e.g., knowledge graphs (KGs) also contain \\textit{temporal facts} or\n\\textit{events} indicating evolutionary relationships among entities at\ndifferent timestamps. (2) PLMs cannot be directly applied to many KG tasks,\nsuch as temporal KG completion. In this paper, we focus on \\textbf{e}nhancing\ntemporal knowledge embeddings with \\textbf{co}ntextualized \\textbf{la}nguage\nrepresentations (ECOLA). We align structured knowledge, contained in temporal\nknowledge graphs, with their textual descriptions extracted from news articles,\nand propose a novel knowledge-text prediction task to inject the abundant\ninformation from descriptions into temporal knowledge embeddings. ECOLA jointly\noptimizes the knowledge-text prediction objective and the temporal knowledge\nembeddings, which can simultaneously take full advantage of textual and\nknowledge information. The proposed fusion method is model-agnostic and can be\ncombined with potentially any temporal KG model. For training ECOLA, we\nintroduce three temporal KG datasets with aligned textual descriptions.\nExperimental results on the temporal knowledge graph completion task show that\nECOLA outperforms state-of-the-art temporal KG models by a large margin. The\nproposed datasets can serve as new temporal KG benchmarks and facilitate future\nresearch on structured and unstructured knowledge integration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruotong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Beiyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jindong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppl_H/0/1/0/all/0/1\">Heinz K&#xf6;ppl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linearizing Transformer with Key-Value Memory. (arXiv:2203.12644v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12644","description":"<p>Efficient transformer variants with linear time complexity have been\ndeveloped to mitigate the quadratic computational overhead of the vanilla\ntransformer. Among them are low-rank projection methods such as Linformer and\nkernel-based Transformers. Despite their unique merits, they usually suffer\nfrom a performance drop comparing with the vanilla transformer on many sequence\ngeneration tasks, and often fail to obtain computation gain when the generation\nis short. We propose MemSizer, an approach towards closing the performance gap\nwhile improving the efficiency even with short generation. It projects the\nsource sequences into lower dimension representations like Linformer, while\nenjoying efficient recurrent-style incremental computation similar to\nkernel-based transformers. This yields linear computation time and constant\nmemory complexity at inference time. MemSizer also employs a lightweight\nmulti-head mechanism which renders the computation as light as a single-head\nmodel. We demonstrate that MemSizer provides an improved balance between\nefficiency and accuracy over the vanilla transformer and other efficient\ntransformer variants in three typical sequence generation tasks, including\nmachine translation, abstractive text summarization, and language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STaR: Bootstrapping Reasoning With Reasoning. (arXiv:2203.14465v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.14465","description":"<p>Generating step-by-step \"chain-of-thought\" rationales improves language model\nperformance on complex reasoning tasks like mathematics or commonsense\nquestion-answering. However, inducing language model rationale generation\ncurrently requires either constructing massive rationale datasets or\nsacrificing accuracy by using only few-shot inference. We propose a technique\nto iteratively leverage a small number of rationale examples and a large\ndataset without rationales, to bootstrap the ability to perform successively\nmore complex reasoning. This technique, the \"Self-Taught Reasoner\" (STaR),\nrelies on a simple loop: generate rationales to answer many questions, prompted\nwith a few rationale examples; if the generated answers are wrong, try again to\ngenerate a rationale given the correct answer; fine-tune on all the rationales\nthat ultimately yielded correct answers; repeat. We show that STaR\nsignificantly improves performance on multiple datasets compared to a model\nfine-tuned to directly predict final answers, and performs comparably to\nfine-tuning a 30$\\times$ larger state-of-the-art language model on\nCommensenseQA. Thus, STaR lets a model improve itself by learning from its own\ngenerated reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1\">Eric Zelikman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jesse Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah D. Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16487","description":"<p>Different from previous work accelerating translation at the cost of quality\nloss, we propose Generalized Aggressive Decoding (GAD) -- a novel decoding\nparadigm for lossless speedup of autoregressive translation, through the\ncollaboration of autoregressive and non-autoregressive translation (NAT) of the\nTransformer. At each decoding iteration, GAD aggressively decodes a number of\ntokens with NAT as a draft and then verifies them in the autoregressive manner,\nwhere only the tokens that pass the verification are accepted as decoded\ntokens. GAD can achieve the same results as autoregressive translation but much\nmore efficiently because both NAT drafting and autoregressive verification\ncompute in parallel.\n</p>\n<p>We conduct experiments in four standard WMT benchmarks and confirm that the\nvanilla GAD yields exactly the same results as greedy decoding with an around\n$3\\times$ speedup, and that its variant (GAD++) with an advanced verification\nstrategy not only outperforms the greedy translation and even achieves the\ncomparable translation quality with the beam search result, but also further\nimproves the decoding speed, resulting in an around $5\\times$ speedup over\nautoregressive translation. Moreover, GAD can be easily generalized for\nlossless speedup of other seq2seq tasks like Abstractive Summarization, and\nbenefit more from stronger computing devices, demonstrating its potential to\nbecome a de facto decoding paradigm in the future. Our models and codes are\navailable at https://github.com/hemingkx/GAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1\">Heming Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Readability Model for Cebuano. (arXiv:2203.17225v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.17225","description":"<p>In this study, we developed the first baseline readability model for the\nCebuano language. Cebuano is the second most-used native language in the\nPhilippines with about 27.5 million speakers. As the baseline, we extracted\ntraditional or surface-based features, syllable patterns based from Cebuano's\ndocumented orthography, and neural embeddings from the multilingual BERT model.\nResults show that the use of the first two handcrafted linguistic features\nobtained the best performance trained on an optimized Random Forest model with\napproximately 87% across all metrics. The feature sets and algorithm used also\nis similar to previous results in readability assessment for the Filipino\nlanguage showing potential of crosslingual application. To encourage more work\nfor readability assessment in Philippine languages such as Cebuano, we\nopen-sourced both code and data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reyes_L/0/1/0/all/0/1\">Lloyd Lois Antonie Reyes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibanez_M/0/1/0/all/0/1\">Michael Antonio Iba&#xf1;ez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapinit_R/0/1/0/all/0/1\">Ranz Sapinit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1\">Mohammed Hussien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can language models learn from explanations in context?. (arXiv:2204.02329v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02329","description":"<p>Large language models can perform new tasks by adapting to a few in-context\nexamples. For humans, rapid learning from examples can benefit from\nexplanations that connect examples to task principles. We therefore investigate\nwhether explanations of few-shot examples can allow language models to adapt\nmore effectively. We annotate a set of 40 challenging tasks from BIG-Bench with\nexplanations of answers to a small subset of questions, as well as a variety of\nmatched control explanations. We evaluate the effects of various zero-shot and\nfew-shot prompts that include different types of explanations, instructions,\nand controls on the performance of a range of large language models. We analyze\nthese results using statistical multilevel modeling techniques that account for\nthe nested dependencies among conditions, tasks, prompts, and models. We find\nthat explanations of examples can improve performance. Adding untuned\nexplanations to a few-shot prompt offers a modest improvement in performance;\nabout 1/3 the effect size of adding few-shot examples, but twice the effect\nsize of task instructions. We then show that explanations tuned for performance\non a small validation set offer substantially larger benefits; building a\nprompt by selecting examples and explanations together substantially improves\nperformance over selecting examples alone. Hand-tuning explanations can\nsubstantially improve performance on challenging tasks. Furthermore, even\nuntuned explanations outperform carefully matched controls, suggesting that the\nbenefits are due to the link between an example and its explanation, rather\nthan lower-level features of the language used. However, only large models can\nbenefit from explanations. In summary, explanations can support the in-context\nlearning abilities of large language models on challenging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C. Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthewson_K/0/1/0/all/0/1\">Kory Matthewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creswell_A/0/1/0/all/0/1\">Antonia Creswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval. (arXiv:2204.07441v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07441","description":"<p>Large-scale single-stream pre-training has shown dramatic performance in\nimage-text retrieval. Regrettably, it faces low inference efficiency due to\nheavy attention layers. Recently, two-stream methods like CLIP and ALIGN with\nhigh inference efficiency have also shown promising performance, however, they\nonly consider instance-level alignment between the two streams (thus there is\nstill room for improvement). To overcome these limitations, we propose a novel\nCOllaborative Two-Stream vision-language pretraining model termed COTS for\nimage-text retrieval by enhancing cross-modal interaction. In addition to\ninstance level alignment via momentum contrastive learning, we leverage two\nextra levels of cross-modal interactions in our COTS: (1) Token-level\ninteraction - a masked visionlanguage modeling (MVLM) learning objective is\ndevised without using a cross-stream network module, where variational\nautoencoder is imposed on the visual encoder to generate visual tokens for each\nimage. (2) Task-level interaction - a KL-alignment learning objective is\ndevised between text-to-image and image-to-text retrieval tasks, where the\nprobability distribution per task is computed with the negative queues in\nmomentum contrastive learning. Under a fair comparison setting, our COTS\nachieves the highest performance among all two-stream methods and comparable\nperformance (but with 10,800X faster in inference) w.r.t. the latest\nsingle-stream methods. Importantly, our COTS is also applicable to\ntext-to-video retrieval, yielding new state-ofthe-art on the widely-used\nMSR-VTT dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_N/0/1/0/all/0/1\">Nanyi Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Representation Collapse of Sparse Mixture of Experts. (arXiv:2204.09179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.09179","description":"<p>Sparse mixture of experts provides larger model capacity while requiring a\nconstant computational overhead. It employs the routing mechanism to distribute\ninput tokens to the best-matched experts according to their hidden\nrepresentations. However, learning such a routing mechanism encourages token\nclustering around expert centroids, implying a trend toward representation\ncollapse. In this work, we propose to estimate the routing scores between\ntokens and experts on a low-dimensional hypersphere. We conduct extensive\nexperiments on cross-lingual language model pre-training and fine-tuning on\ndownstream tasks. Experimental results across seven multilingual benchmarks\nshow that our method achieves consistent gains. We also present a comprehensive\nanalysis on the representation and routing behaviors of our models. Our method\nalleviates the representation collapse issue and achieves more consistent\nrouting than the baseline mixture-of-experts methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks. (arXiv:2204.10615v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10615","description":"<p>Logical approaches to representing language have developed and evaluated\ncomputational models of quantifier words since the 19th century, but today's\nNLU models still struggle to capture their semantics. We rely on Generalized\nQuantifier Theory for language-independent representations of the semantics of\nquantifier words, to quantify their contribution to the errors of NLU models.\nWe find that quantifiers are pervasive in NLU benchmarks, and their occurrence\nat test time is associated with performance drops. Multilingual models also\nexhibit unsatisfying quantifier reasoning abilities, but not necessarily worse\nfor non-English languages. To facilitate directly-targeted probing, we present\nan adversarial generalized quantifier NLI task (GQNLI) and show that\npre-trained language models have a clear lack of robustness in generalized\nquantifier reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptable Text Matching via Meta-Weight Regulator. (arXiv:2204.12668v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.12668","description":"<p>Neural text matching models have been used in a range of applications such as\nquestion answering and natural language inference, and have yielded a good\nperformance. However, these neural models are of a limited adaptability,\nresulting in a decline in performance when encountering test examples from a\ndifferent dataset or even a different task. The adaptability is particularly\nimportant in the few-shot setting: in many cases, there is only a limited\namount of labeled data available for a target dataset or task, while we may\nhave access to a richly labeled source dataset or task. However, adapting a\nmodel trained on the abundant source data to a few-shot target dataset or task\nis challenging. To tackle this challenge, we propose a Meta-Weight Regulator\n(MWR), which is a meta-learning approach that learns to assign weights to the\nsource examples based on their relevance to the target loss. Specifically, MWR\nfirst trains the model on the uniformly weighted source examples, and measures\nthe efficacy of the model on the target examples via a loss function. By\niteratively performing a (meta) gradient descent, high-order gradients are\npropagated to the source examples. These gradients are then used to update the\nweights of source examples, in a way that is relevant to the target\nperformance. As MWR is model-agnostic, it can be applied to any backbone neural\nmodel. Extensive experiments are conducted with various backbone text matching\nmodels, on four widely used datasets and two tasks. The results demonstrate\nthat our proposed approach significantly outperforms a number of existing\nadaptation methods and effectively improves the cross-dataset and cross-task\nadaptability of the neural text matching models in the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Language Models with Language Feedback. (arXiv:2204.14146v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.14146","description":"<p>Pretrained language models often do not perform tasks in ways that are in\nline with our preferences, e.g., generating offensive text or factually\nincorrect summaries. Recent work approaches the above issue by learning from a\nsimple form of human evaluation: comparisons between pairs of model-generated\ntask outputs. Comparison feedback conveys limited information about human\npreferences per human evaluation. Here, we propose to learn from natural\nlanguage feedback, which conveys more information per human evaluation. We\nlearn from language feedback on model outputs using a three-step learning\nalgorithm. First, we condition the language model on the initial output and\nfeedback to generate many refinements. Second, we choose the refinement with\nthe highest similarity to the feedback. Third, we finetune a language model to\nmaximize the likelihood of the chosen refinement given the input. In synthetic\nexperiments, we first evaluate whether language models accurately incorporate\nfeedback to produce refinements, finding that only large language models (175B\nparameters) do so. Using only 100 samples of human-written feedback, our\nlearning algorithm finetunes a GPT-3 model to roughly human-level\nsummarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;my Scheurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_J/0/1/0/all/0/1\">Jon Ander Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Jun Shern Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1\">Ethan Perez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling. (arXiv:2205.05862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05862","description":"<p>Variational Auto-Encoder (VAE) has become the de-facto learning paradigm in\nachieving both representation learning and generation for natural language.\nHowever, existing VAE-based language models either employ elementary RNNs,\nwhich is not powerful to handle complex situations, or fine-tunes two\npre-trained language models (PLMs) for any downstream task, which is a huge\ndrain on resources. In this paper, we introduce the first VAE framework\nempowered with adaptive GPT-2s (AdaVAE). Different from existing systems, we\nunify both the encoder\\&amp;decoder of VAE model using GPT-2s with adaptive\nparameter-efficient components. Experiments from multiple dimensions validate\nthat AdaVAE is competent to better organize language in generation task and\nrepresentation modeling, even with less than $15\\%$ activated parameters in\ntraining. Our code is available at \\url{https://github.com/ImKeTT/adavae}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1\">Haoqin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongliang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinshuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Induction for Efficient Semi-Supervised Learning. (arXiv:2205.09067v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09067","description":"<p>Semi-supervised learning has shown promise in allowing NLP models to\ngeneralize from small amounts of labeled data. Meanwhile, pretrained\ntransformer models act as black-box correlation engines that are difficult to\nexplain and sometimes behave unreliably. In this paper, we propose tackling\nboth of these challenges via Automatic Rule Induction (ARI), a simple and\ngeneral-purpose framework for the automatic discovery and integration of\nsymbolic rules into pretrained transformer models. First, we extract weak\nsymbolic rules from low-capacity machine learning models trained on small\namounts of labeled data. Next, we use an attention mechanism to integrate these\nrules into high-capacity pretrained transformer models. Last, the\nrule-augmented system becomes part of a self-training framework to boost\nsupervision signal on unlabeled data. These steps can be layered beneath a\nvariety of existing weak supervision and semi-supervised NLP algorithms in\norder to improve performance and interpretability. Experiments across nine\nsequence classification and relation extraction tasks suggest that ARI can\nimprove state-of-the-art methods with no manual effort and minimal\ncomputational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Prompt-based Models Clueless?. (arXiv:2205.09295v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09295","description":"<p>Finetuning large pre-trained language models with a task-specific head has\nadvanced the state-of-the-art on many natural language understanding\nbenchmarks. However, models with a task-specific head require a lot of training\ndata, making them susceptible to learning and exploiting dataset-specific\nsuperficial cues that do not generalize to other datasets. Prompting has\nreduced the data requirement by reusing the language model head and formatting\nthe task input to match the pre-training objective. Therefore, it is expected\nthat few-shot prompt-based models do not exploit superficial cues. This paper\npresents an empirical examination of whether few-shot prompt-based models also\nexploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI,\nHANS, and COPA has revealed that prompt-based models also exploit superficial\ncues. While the models perform well on instances with superficial cues, they\noften underperform or only marginally outperform random accuracy on instances\nwithout superficial cues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kavumba_P/0/1/0/all/0/1\">Pride Kavumba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_R/0/1/0/all/0/1\">Ryo Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oda_Y/0/1/0/all/0/1\">Yusuke Oda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Inflection as a Data Augmentation Method for Parsing. (arXiv:2205.09350v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09350","description":"<p>We propose a morphology-based method for low-resource (LR) dependency\nparsing. We train a morphological inflector for target LR languages, and apply\nit to related rich-resource (RR) treebanks to create cross-lingual\n(x-inflected) treebanks that resemble the target LR language. We use such\ninflected treebanks to train parsers in zero- (training on x-inflected\ntreebanks) and few-shot (training on x-inflected and target language treebanks)\nsetups. The results show that the method sometimes improves the baselines, but\nnot consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Ortiz_A/0/1/0/all/0/1\">Alberto Mu&#xf1;oz-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1\">David Vilares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding. (arXiv:2205.09753v1 [cs.AI])","link":"http://arxiv.org/abs/2205.09753","description":"<p>One essential task for autonomous driving is to encode the information of a\ndriving scene into vector representations so that the downstream task such as\ntrajectory prediction could perform well. The driving scene is complicated, and\nthere exists heterogeneity within elements, where they own diverse types of\ninformation i.e., agent dynamics, map routing, road lines, etc. Meanwhile,\nthere also exist relativity across elements - meaning they have spatial\nrelations with each other; such relations should be canonically represented\nregarding the relative measurements since the absolute value of the coordinate\nis meaningless. Taking these two observations into consideration, we propose a\nnovel backbone, namely Heterogeneous Driving Graph Transformer (HDGT), which\nmodels the driving scene as a heterogeneous graph with different types of nodes\nand edges. For graph construction, each node represents either an agent or a\nroad element and each edge represents their semantics relations such as\nPedestrian-To-Crosswalk, Lane-To-Left-Lane. As for spatial relation encoding,\ninstead of setting a fixed global reference, the coordinate information of the\nnode as well as its in-edges is transformed to the local node-centric\ncoordinate system. For the aggregation module in the graph neural network\n(GNN), we adopt the transformer structure in a hierarchical way to fit the\nheterogeneous nature of inputs. Experimental results show that the proposed\nmethod achieves new state-of-the-art on INTERACTION Prediction Challenge and\nWaymo Open Motion Challenge, in which we rank 1st and 2nd respectively\nregarding the minADE/minFDE metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaosong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Penghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying outliers in astronomical images with unsupervised machine learning. (arXiv:2205.09760v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09760","description":"<p>Astronomical outliers, such as unusual, rare or unknown types of astronomical\nobjects or phenomena, constantly lead to the discovery of genuinely unforeseen\nknowledge in astronomy. More unpredictable outliers will be uncovered in\nprinciple with the increment of the coverage and quality of upcoming survey\ndata. However, it is a severe challenge to mine rare and unexpected targets\nfrom enormous data with human inspection due to a significant workload.\nSupervised learning is also unsuitable for this purpose since designing proper\ntraining sets for unanticipated signals is unworkable. Motivated by these\nchallenges, we adopt unsupervised machine learning approaches to identify\noutliers in the data of galaxy images to explore the paths for detecting\nastronomical outliers. For comparison, we construct three methods, which are\nbuilt upon the k-nearest neighbors (KNN), Convolutional Auto-Encoder (CAE)+\nKNN, and CAE + KNN + Attention Mechanism (attCAE KNN) separately. Testing sets\nare created based on the Galaxy Zoo image data published online to evaluate the\nperformance of the above methods. Results show that attCAE KNN achieves the\nbest recall (78%), which is 53% higher than the classical KNN method and 22%\nhigher than CAE+KNN. The efficiency of attCAE KNN (10 minutes) is also superior\nto KNN (4 hours) and equal to CAE+KNN(10 minutes) for accomplishing the same\ntask. Thus, we believe it is feasible to detect astronomical outliers in the\ndata of galaxy images in an unsupervised manner. Next, we will apply attCAE KNN\nto available survey datasets to assess its applicability and reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhiqiang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanli Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Peek at Peak Emotion Recognition. (arXiv:2205.09791v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09791","description":"<p>Despite much progress in the field of facial expression recognition, little\nattention has been paid to the recognition of peak emotion. Aviezer et al. [1]\nshowed that humans have trouble discerning between positive and negative peak\nemotions. In this work we analyze how deep learning fares on this challenge. We\nfind that (i) despite using very small datasets, features extracted from deep\nlearning models can achieve results significantly better than humans. (ii) We\nfind that deep learning models, even when trained only on datasets tagged by\nhumans, still outperform humans in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michelson_T/0/1/0/all/0/1\">Tzvi Michelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviezer_H/0/1/0/all/0/1\">Hillel Aviezer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peleg_S/0/1/0/all/0/1\">Shmuel Peleg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-invariant Augmentation for Semi-Supervised Graph Classification. (arXiv:2205.09802v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09802","description":"<p>Recently, contrastiveness-based augmentation surges a new climax in the\ncomputer vision domain, where some operations, including rotation, crop, and\nflip, combined with dedicated algorithms, dramatically increase the model\ngeneralization and robustness. Following this trend, some pioneering attempts\nemploy the similar idea to graph data. Nevertheless, unlike images, it is much\nmore difficult to design reasonable augmentations without changing the nature\nof graphs. Although exciting, the current graph contrastive learning does not\nachieve as promising performance as visual contrastive learning. We conjecture\nthe current performance of graph contrastive learning might be limited by the\nviolation of the label-invariant augmentation assumption. In light of this, we\npropose a label-invariant augmentation for graph-structured data to address\nthis challenge. Different from the node/edge modification and subgraph\nextraction, we conduct the augmentation in the representation space and\ngenerate the augmented samples in the most difficult direction while keeping\nthe label of augmented data the same as the original samples. In the\nsemi-supervised scenario, we demonstrate our proposed method outperforms the\nclassical graph neural network based methods and recent graph contrastive\nlearning on eight benchmark graph-structured data, followed by several in-depth\nexperiments to further explore the label-invariant augmentation in several\naspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1\">Han Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of Depth, Camera Pose and Optical Flow from Monocular Video. (arXiv:2205.09821v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09821","description":"<p>We propose DFPNet -- an unsupervised, joint learning system for monocular\nDepth, Optical Flow and egomotion (Camera Pose) estimation from monocular image\nsequences. Due to the nature of 3D scene geometry these three components are\ncoupled. We leverage this fact to jointly train all the three components in an\nend-to-end manner. A single composite loss function -- which involves image\nreconstruction-based loss for depth &amp; optical flow, bidirectional consistency\nchecks and smoothness loss components -- is used to train the network. Using\nhyperparameter tuning, we are able to reduce the model size to less than 5%\n(8.4M parameters) of state-of-the-art DFP models. Evaluation on KITTI and\nCityscapes driving datasets reveals that our model achieves results comparable\nto state-of-the-art in all of the three tasks, even with the significantly\nsmaller model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_D/0/1/0/all/0/1\">Dipan Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Abhilash Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramoney_S/0/1/0/all/0/1\">Sreenivas Subramoney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subcellular Protein Localisation in the Human Protein Atlas using Ensembles of Diverse Deep Architectures. (arXiv:2205.09841v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09841","description":"<p>Automated visual localisation of subcellular proteins can accelerate our\nunderstanding of cell function in health and disease. Despite recent advances\nin machine learning (ML), humans still attain superior accuracy by using\ndiverse clues. We show how this gap can be narrowed by addressing three key\naspects: (i) automated improvement of cell annotation quality, (ii) new\nConvolutional Neural Network (CNN) architectures supporting unbalanced and\nnoisy data, and (iii) informed selection and fusion of multiple &amp; diverse\nmachine learning models. We introduce a new \"AI-trains-AI\" method for improving\nthe quality of weak labels and propose novel CNN architectures exploiting\nwavelet filters and Weibull activations. We also explore key factors in the\nmulti-CNN ensembling process by analysing correlations between image-level and\ncell-level predictions. Finally, in the context of the Human Protein Atlas, we\ndemonstrate that our system achieves state-of-the-art performance in the\nmulti-label single-cell classification of protein localisation patterns. It\nalso significantly improves generalisation ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Husain_S/0/1/0/all/0/1\">Syed Sameed Husain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Eng-Jon Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minskiy_D/0/1/0/all/0/1\">Dmitry Minskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_Irizar_M/0/1/0/all/0/1\">Mikel Bober-Irizar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irizar_A/0/1/0/all/0/1\">Amaia Irizar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of Artificial CT Images using Patch-based Conditional Generative Adversarial Networks. (arXiv:2205.09842v1 [eess.IV])","link":"http://arxiv.org/abs/2205.09842","description":"<p>Deep learning has a great potential to alleviate diagnosis and prognosis for\nvarious clinical procedures. However, the lack of a sufficient number of\nmedical images is the most common obstacle in conducting image-based analysis\nusing deep learning. Due to the annotations scarcity, semi-supervised\ntechniques in the automatic medical analysis are getting high attention.\nArtificial data augmentation and generation techniques such as generative\nadversarial networks (GANs) may help overcome this obstacle. In this work, we\npresent an image generation approach that uses generative adversarial networks\nwith a conditional discriminator where segmentation masks are used as\nconditions for image generation. We validate the feasibility of GAN-enhanced\nmedical image generation on whole heart computed tomography (CT) images and its\nseven substructures, namely: left ventricle, right ventricle, left atrium,\nright atrium, myocardium, pulmonary arteries, and aorta. Obtained results\ndemonstrate the suitability of the proposed adversarial approach for the\naccurate generation of high-quality CT images. The presented method shows great\npotential to facilitate further research in the domain of artificial medical\nimage generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Habijan_M/0/1/0/all/0/1\">Marija Habijan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galic_I/0/1/0/all/0/1\">Irena Galic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Gender Prediction Based on Deep Transfer Learning from Panoramic Radiograph Images. (arXiv:2205.09850v1 [eess.IV])","link":"http://arxiv.org/abs/2205.09850","description":"<p>Panoramic Dental Radiography (PDR) image processing is one of the most\nextensively used manual methods for gender determination in forensic medicine.\nManual approaches require a wide range of mandibular parameter measurements in\nmetric units. Besides being time-consuming, these methods also necessitate the\nemployment of experienced professionals. In this context, deep learning models\nare widely utilized in the auto-analysis of radiological images nowadays, owing\nto their high processing speed, accuracy, and stability. In our study, a data\nset consisting of 24,000 dental panoramic images was prepared for binary\nclassification, and the transfer learning method was used to accelerate the\ntraining and increase the performance of our proposed DenseNet121 deep learning\nmodel. With the transfer learning method, instead of starting the learning\nprocess from scratch, the existing patterns learned beforehand were used.\nExtensive comparisons were made using deep transfer learning (DTL) models\nVGG16, ResNet50, and EfficientNetB6 to assess the classification performance of\nthe proposed model in PDR images. According to the findings of the comparative\nanalysis, the proposed model outperformed the other approaches by achieving a\nsuccess rate of 97.25% in gender classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Atas_I/0/1/0/all/0/1\">I. Atas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation. (arXiv:2205.09853v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09853","description":"<p>Video prediction is a challenging task. The quality of video frames from\ncurrent state-of-the-art (SOTA) generative models tends to be poor and\ngeneralization beyond the training data is difficult. Furthermore, existing\nprediction frameworks are typically not capable of simultaneously handling\nother video-related tasks such as unconditional generation or interpolation. In\nthis work, we devise a general-purpose framework called Masked Conditional\nVideo Diffusion (MCVD) for all of these video synthesis tasks using a\nprobabilistic conditional score-based denoising diffusion model, conditioned on\npast and/or future frames. We train the model in a manner where we randomly and\nindependently mask all the past frames or all the future frames. This novel but\nstraightforward setup allows us to train a single model that is capable of\nexecuting a broad range of video tasks, specifically: future/past prediction --\nwhen only future/past frames are masked; unconditional generation -- when both\npast and future frames are masked; and interpolation -- when neither past nor\nfuture frames are masked. Our experiments show that this approach can generate\nhigh-quality frames for diverse types of videos. Our MCVD models are built from\nsimple non-recurrent 2D-convolutional architectures, conditioning on blocks of\nframes and generating blocks of frames. We generate videos of arbitrary lengths\nautoregressively in a block-wise manner. Our approach yields SOTA results\nacross standard video prediction and interpolation benchmarks, with computation\ntimes for training models measured in 1-12 days using $\\le$ 4 GPUs.\nhttps://mask-cond-video-diffusion.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jolicoeur_Martineau_A/0/1/0/all/0/1\">Alexia Jolicoeur-Martineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real Time Multi-Object Detection for Helmet Safety. (arXiv:2205.09878v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09878","description":"<p>The National Football League and Amazon Web Services teamed up to develop the\nbest sports injury surveillance and mitigation program via the Kaggle\ncompetition. Through which the NFL wants to assign specific players to each\nhelmet, which would help accurately identify each player's \"exposures\"\nthroughout a football play. We are trying to implement a computer vision based\nML algorithms capable of assigning detected helmet impacts to correct players\nvia tracking information. Our paper will explain the approach to automatically\ntrack player helmets and their collisions. This will also allow them to review\nprevious plays and explore the trends in exposure over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_M/0/1/0/all/0/1\">Mrinal Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrashekhar_A/0/1/0/all/0/1\">Archana Benkkallpalli Chandrashekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuthalapati_V/0/1/0/all/0/1\">Venkata Krishna Chaithanya Nuthalapati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Labels: Visual Representations for Bone Marrow Cell Morphology Recognition. (arXiv:2205.09880v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09880","description":"<p>Analyzing and inspecting bone marrow cell cytomorphology is a critical but\nhighly complex and time-consuming component of hematopathology diagnosis.\nRecent advancements in artificial intelligence have paved the way for the\napplication of deep learning algorithms to complex medical tasks. Nevertheless,\nthere are many challenges in applying effective learning algorithms to medical\nimage analysis, such as the lack of sufficient and reliably annotated training\ndatasets and the highly class-imbalanced nature of most medical data. Here, we\nimprove on the state-of-the-art methodologies of bone marrow cell recognition\nby deviating from sole reliance on labeled data and leveraging self-supervision\nin training our learning models. We investigate our approach's effectiveness in\nidentifying bone marrow cell types. Our experiments demonstrate significant\nperformance improvements in conducting different bone marrow cell recognition\ntasks compared to the current state-of-the-art methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fazeli_S/0/1/0/all/0/1\">Shayan Fazeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samiei_A/0/1/0/all/0/1\">Alireza Samiei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Thomas D. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1\">Majid Sarrafzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep transfer learning for image classification: a survey. (arXiv:2205.09904v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09904","description":"<p>Deep neural networks such as convolutional neural networks (CNNs) and\ntransformers have achieved many successes in image classification in recent\nyears. It has been consistently demonstrated that best practice for image\nclassification is when large deep models can be trained on abundant labelled\ndata. However there are many real world scenarios where the requirement for\nlarge amounts of training data to get the best performance cannot be met. In\nthese scenarios transfer learning can help improve performance. To date there\nhave been no surveys that comprehensively review deep transfer learning as it\nrelates to image classification overall. However, several recent general\nsurveys of deep transfer learning and ones that relate to particular\nspecialised target image classification tasks have been published. We believe\nit is important for the future progress in the field that all current knowledge\nis collated and the overarching patterns analysed and discussed. In this survey\nwe formally define deep transfer learning and the problem it attempts to solve\nin relation to image classification. We survey the current state of the field\nand identify where recent progress has been made. We show where the gaps in\ncurrent knowledge are and make suggestions for how to progress the field to\nfill in these knowledge gaps. We present a new taxonomy of the applications of\ntransfer learning for image classification. This taxonomy makes it easier to\nsee overarching patterns of where transfer learning has been effective and,\nwhere it has failed to fulfill its potential. This also allows us to suggest\nwhere the problems lie and how it could be used more effectively. We show that\nunder this new taxonomy, many of the applications where transfer learning has\nbeen shown to be ineffective or even hinder performance are to be expected when\ntaking into account the source and target datasets and the techniques used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plested_J/0/1/0/all/0/1\">Jo Plested</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral Unmixing Based on Nonnegative Matrix Factorization: A Comprehensive Review. (arXiv:2205.09933v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09933","description":"<p>Hyperspectral unmixing has been an important technique that estimates a set\nof endmembers and their corresponding abundances from a hyperspectral image\n(HSI). Nonnegative matrix factorization (NMF) plays an increasingly significant\nrole in solving this problem. In this article, we present a comprehensive\nsurvey of the NMF-based methods proposed for hyperspectral unmixing. Taking the\nNMF model as a baseline, we show how to improve NMF by utilizing the main\nproperties of HSIs (e.g., spectral, spatial, and structural information). We\ncategorize three important development directions including constrained NMF,\nstructured NMF, and generalized NMF. Furthermore, several experiments are\nconducted to illustrate the effectiveness of associated algorithms. Finally, we\nconclude the article with possible future directions with the purposes of\nproviding guidelines and inspiration to promote the development of\nhyperspectral unmixing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xin-Ru Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Heng-Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiuping Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1\">Antonio Plaza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PGDP5K: A Diagram Parsing Dataset for Plane Geometry Problems. (arXiv:2205.09947v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09947","description":"<p>Diagram parsing is an important foundation for geometry problem solving,\nattracting increasing attention in the field of intelligent education and\ndocument image understanding. Due to the complex layout and between-primitive\nrelationship, plane geometry diagram parsing (PGDP) is still a challenging task\ndeserving further research and exploration. An appropriate dataset is critical\nfor the research of PGDP. Although some datasets with rough annotations have\nbeen proposed to solve geometric problems, they are either small in scale or\nnot publicly available. The rough annotations also make them not very useful.\nThus, we propose a new large-scale geometry diagram dataset named PGDP5K and a\nnovel annotation method. Our dataset consists of 5000 diagram samples composed\nof 16 shapes, covering 5 positional relations, 22 symbol types and 6 text\ntypes. Different from previous datasets, our PGDP5K dataset is labeled with\nmore fine-grained annotations at primitive level, including primitive classes,\nlocations and relationships. What is more, combined with above annotations and\ngeometric prior knowledge, it can generate intelligible geometric propositions\nautomatically and uniquely. We performed experiments on PGDP5K and\nIMP-Geometry3K datasets reveal that the state-of-the-art (SOTA) method achieves\nonly 66.07% F1 value. This shows that PGDP5K presents a challenge for future\nresearch. Our dataset is available at\n<a href=\"http://www.nlpr.ia.ac.cn/databases/CASIA-PGDP5K/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yihan Hao</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingliang Zhang</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Fei Yin</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linlin Huang</a> (1) ((1) Beijing Jiaotong University, (2) Institute of Automation of Chinese Academy of Science, (3) University of Chinese Academy of Sciences)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering as Attention: Unified Image Segmentation with Hierarchical Clustering. (arXiv:2205.09949v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09949","description":"<p>We propose a hierarchical clustering-based image segmentation scheme for deep\nneural networks, called HCFormer. We interpret image segmentation, including\nsemantic, instance, and panoptic segmentation, as a pixel clustering problem,\nand accomplish it by bottom-up, hierarchical clustering with deep neural\nnetworks. Our hierarchical clustering removes the pixel decoder from\nconventional segmentation models and simplifies the segmentation pipeline,\nresulting in improved segmentation accuracies and interpretability. HCFormer\ncan address semantic, instance, and panoptic segmentation with the same\narchitecture because the pixel clustering is a common approach for various\nimage segmentation. In experiments, HCFormer achieves comparable or superior\nsegmentation accuracies compared to baseline methods on semantic segmentation\n(55.5 mIoU on ADE20K), instance segmentation (47.1 AP on COCO), and panoptic\nsegmentation (55.7 PQ on COCO).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Teppei Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Attention Composition for Temporal Action Localization. (arXiv:2205.09956v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09956","description":"<p>Temporal action localization aims at localizing action instances from\nuntrimmed videos. Existing works have designed various effective modules to\nprecisely localize action instances based on appearance and motion features.\nHowever, by treating these two kinds of features with equal importance,\nprevious works cannot take full advantage of each modality feature, making the\nlearned model still sub-optimal. To tackle this issue, we make an early effort\nto study temporal action localization from the perspective of multi-modality\nfeature learning, based on the observation that different actions exhibit\nspecific preferences to appearance or motion modality. Specifically, we build a\nnovel structured attention composition module. Unlike conventional attention,\nthe proposed module would not infer frame attention and modality attention\nindependently. Instead, by casting the relationship between the modality\nattention and the frame attention as an attention assignment process, the\nstructured attention composition module learns to encode the frame-modality\nstructure and uses it to regularize the inferred frame attention and modality\nattention, respectively, upon the optimal transport theory. The final\nframe-modality attention is obtained by the composition of the two individual\nattentions. The proposed structured attention composition module can be\ndeployed as a plug-and-play module into existing action localization\nframeworks. Extensive experiments on two widely used benchmarks show that the\nproposed structured attention composition consistently improves four\nstate-of-the-art temporal action localization methods and builds new\nstate-of-the-art performance on THUMOS14. Code is availabel at\nhttps://github.com/VividLe/Online-Action-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Le Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advanced Feature Learning on Point Clouds using Multi-resolution Features and Learnable Pooling. (arXiv:2205.09962v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09962","description":"<p>Existing point cloud feature learning networks often incorporate sequences of\nsampling, neighborhood grouping, neighborhood-wise feature learning, and\nfeature aggregation to learn high-semantic point features that represent the\nglobal context of a point cloud. Unfortunately, the compounded loss of\ninformation concerning granularity and non-maximum point features due to\nsampling and max pooling could adversely affect the high-semantic point\nfeatures from existing networks such that they are insufficient to represent\nthe local context of a point cloud, which in turn may hinder the network in\ndistinguishing fine shapes. To cope with this problem, we propose a novel point\ncloud feature learning network, PointStack, using multi-resolution feature\nlearning and learnable pooling (LP). The multi-resolution feature learning is\nrealized by aggregating point features of various resolutions in the multiple\nlayers, so that the final point features contain both high-semantic and\nhigh-resolution information. On the other hand, the LP is used as a generalized\npooling function that calculates the weighted sum of multi-resolution point\nfeatures through the attention mechanism with learnable queries, in order to\nextract all possible information from all available point features.\nConsequently, PointStack is capable of extracting high-semantic point features\nwith minimal loss of information concerning granularity and non-maximum point\nfeatures. Therefore, the final aggregated point features can effectively\nrepresent both global and local contexts of a point cloud. In addition, both\nthe global structure and the local shape details of a point cloud can be well\ncomprehended by the network head, which enables PointStack to advance the\nstate-of-the-art of feature learning on point clouds. The codes are available\nat https://github.com/kaist-avelab/PointStack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_K/0/1/0/all/0/1\">Kevin Tirta Wijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paek_D/0/1/0/all/0/1\">Dong-Hee Paek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1\">Seung-Hyun Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Font Generation by Learning Fine-Grained Local Styles. (arXiv:2205.09965v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09965","description":"<p>Few-shot font generation (FFG), which aims to generate a new font with a few\nexamples, is gaining increasing attention due to the significant reduction in\nlabor cost. A typical FFG pipeline considers characters in a standard font\nlibrary as content glyphs and transfers them to a new target font by extracting\nstyle information from the reference glyphs. Most existing solutions explicitly\ndisentangle content and style of reference glyphs globally or component-wisely.\nHowever, the style of glyphs mainly lies in the local details, i.e. the styles\nof radicals, components, and strokes together depict the style of a glyph.\nTherefore, even a single character can contain different styles distributed\nover spatial locations. In this paper, we propose a new font generation\napproach by learning 1) the fine-grained local styles from references, and 2)\nthe spatial correspondence between the content and reference glyphs. Therefore,\neach spatial location in the content glyph can be assigned with the right\nfine-grained style. To this end, we adopt cross-attention over the\nrepresentation of the content glyphs as the queries and the representations of\nthe reference glyphs as the keys and values. Instead of explicitly\ndisentangling global or component-wise modeling, the cross-attention mechanism\ncan attend to the right local styles in the reference glyphs and aggregate the\nreference styles into a fine-grained style representation for the given content\nglyphs. The experiments show that the proposed method outperforms the\nstate-of-the-art methods in FFG. In particular, the user studies also\ndemonstrate the style consistency of our approach significantly outperforms\nprevious methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Licheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yiyang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhibin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Minhu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jingdong Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning. (arXiv:2205.09995v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09995","description":"<p>Learning with little data is challenging but often inevitable in various\napplication scenarios where the labeled data is limited and costly. Recently,\nfew-shot learning (FSL) gained increasing attention because of its\ngeneralizability of prior knowledge to new tasks that contain only a few\nsamples. However, for data-intensive models such as vision transformer (ViT),\ncurrent fine-tuning based FSL approaches are inefficient in knowledge\ngeneralization and thus degenerate the downstream task performances. In this\npaper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve an\neffective and efficient FSL on ViT model. The key idea is to apply a mask on\nimage patches to screen out the task-irrelevant ones and to guide the ViT to\nfocus on task-relevant and discriminative patches during FSL. Particularly,\nMG-ViT only introduces an additional mask operation and a residual connection,\nenabling the inheritance of parameters from pre-trained ViT without any other\ncost. To optimally select representative few-shot samples, we also include an\nactive learning based sample selection method to further improve the\ngeneralizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on both\nAgri-ImageNet classification task and ACFR apple detection task with\ngradient-weighted class activation mapping (Grad-CAM) as the mask. The\nexperimental results show that the MG-ViT model significantly improves the\nperformance when compared with general fine-tuning based ViT models, providing\nnovel insights and a concrete approach towards generalizing data-intensive and\nlarge-scale deep learning models for FSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuzhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhenxiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Haixing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">David Weizhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zihao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dajiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InDistill: Transferring Knowledge From Pruned Intermediate Layers. (arXiv:2205.10003v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10003","description":"<p>Deploying deep neural networks on hardware with limited resources, such as\nsmartphones and drones, constitutes a great challenge due to their\ncomputational complexity. Knowledge distillation approaches aim at transferring\nknowledge from a large model to a lightweight one, also known as teacher and\nstudent respectively, while distilling the knowledge from intermediate layers\nprovides an additional supervision to that task. The capacity gap between the\nmodels, the information encoding that collapses its architectural alignment,\nand the absence of appropriate learning schemes for transferring multiple\nlayers restrict the performance of existing methods. In this paper, we propose\na novel method, termed InDistill, that can drastically improve the performance\nof existing single-layer knowledge distillation methods by leveraging the\nproperties of channel pruning to both reduce the capacity gap between the\nmodels and retain the architectural alignment. Furthermore, we propose a\ncurriculum learning based scheme for enhancing the effectiveness of\ntransferring knowledge from multiple intermediate layers. The proposed method\nsurpasses state-of-the-art performance on three benchmark image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarridis_I/0/1/0/all/0/1\">Ioannis Sarridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1\">Christos Koutlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Depth Estimation with Isometric-Self-Sample-Based Learning. (arXiv:2205.10006v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10006","description":"<p>Managing the dynamic regions in the photometric loss formulation has been a\nmain issue for handling the self-supervised depth estimation problem. Most\nprevious methods have alleviated this issue by removing the dynamic regions in\nthe photometric loss formulation based on the masks estimated from another\nmodule, making it difficult to fully utilize the training images. In this\npaper, to handle this problem, we propose an isometric self-sample-based\nlearning (ISSL) method to fully utilize the training images in a simple yet\neffective way. The proposed method provides additional supervision during\ntraining using self-generated images that comply with pure static scene\nassumption. Specifically, the isometric self-sample generator synthesizes\nself-samples for each training image by applying random rigid transformations\non the estimated depth. Thus both the generated self-samples and the\ncorresponding training image always follow the static scene assumption. We show\nthat plugging our ISSL module into several existing models consistently\nimproves the performance by a large margin. In addition, it also boosts the\ndepth accuracy over different types of scene, i.e., outdoor scenes (KITTI and\nMake3D) and indoor scene (NYUv2), validating its high effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_G/0/1/0/all/0/1\">Geonho Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Ho-Deok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wee_D/0/1/0/all/0/1\">Dongyoon Wee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action parsing using context features. (arXiv:2205.10008v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10008","description":"<p>We propose an action parsing algorithm to parse a video sequence containing\nan unknown number of actions into its action segments. We argue that context\ninformation, particularly the temporal information about other actions in the\nvideo sequence, is valuable for action segmentation. The proposed parsing\nalgorithm temporally segments the video sequence into action segments. The\noptimal temporal segmentation is found using a dynamic programming search\nalgorithm that optimizes the overall classification confidence score. The\nclassification score of each segment is determined using local features\ncalculated from that segment as well as context features calculated from other\ncandidate action segments of the sequence. Experimental results on the\nBreakfast activity data-set showed improved segmentation accuracy compared to\nexisting state-of-the-art parsing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehrseresht_N/0/1/0/all/0/1\">Nagita Mehrseresht</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructive Interpretability with CoLabel: Corroborative Integration, Complementary Features, and Collaborative Learning. (arXiv:2205.10011v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10011","description":"<p>Machine learning models with explainable predictions are increasingly sought\nafter, especially for real-world, mission-critical applications that require\nbias detection and risk mitigation. Inherent interpretability, where a model is\ndesigned from the ground-up for interpretability, provides intuitive insights\nand transparent explanations on model prediction and performance. In this\npaper, we present CoLabel, an approach to build interpretable models with\nexplanations rooted in the ground truth. We demonstrate CoLabel in a vehicle\nfeature extraction application in the context of vehicle make-model recognition\n(VMMR). CoLabel performs VMMR with a composite of interpretable features such\nas vehicle color, type, and make, all based on interpretable annotations of the\nground truth labels. First, CoLabel performs corroborative integration to join\nmultiple datasets that each have a subset of desired annotations of color,\ntype, and make. Then, CoLabel uses decomposable branches to extract\ncomplementary features corresponding to desired annotations. Finally, CoLabel\nfuses them together for final predictions. During feature fusion, CoLabel\nharmonizes complementary branches so that VMMR features are compatible with\neach other and can be projected to the same semantic space for classification.\nWith inherent interpretability, CoLabel achieves superior performance to the\nstate-of-the-art black-box models, with accuracy of 0.98, 0.95, and 0.94 on\nCompCars, Cars196, and BoxCars116K, respectively. CoLabel provides intuitive\nexplanations due to constructive interpretability, and subsequently achieves\nhigh accuracy and usability in mission-critical situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suprem_A/0/1/0/all/0/1\">Abhijit Suprem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_S/0/1/0/all/0/1\">Sanjyot Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherkadi_S/0/1/0/all/0/1\">Suma Cherkadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Purva Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_J/0/1/0/all/0/1\">Joao Eduardo Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_C/0/1/0/all/0/1\">Calton Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Demographic Bias Transfer from Dataset to Model: A Case Study in Facial Expression Recognition. (arXiv:2205.10049v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10049","description":"<p>The increasing amount of applications of Artificial Intelligence (AI) has led\nresearchers to study the social impact of these technologies and evaluate their\nfairness. Unfortunately, current fairness metrics are hard to apply in\nmulti-class multi-demographic classification problems, such as Facial\nExpression Recognition (FER). We propose a new set of metrics to approach these\nproblems. Of the three metrics proposed, two focus on the representational and\nstereotypical bias of the dataset, and the third one on the residual bias of\nthe trained model. These metrics combined can potentially be used to study and\ncompare diverse bias mitigation methods. We demonstrate the usefulness of the\nmetrics by applying them to a FER problem based on the popular Affectnet\ndataset. Like many other datasets for FER, Affectnet is a large\nInternet-sourced dataset with 291,651 labeled images. Obtaining images from the\nInternet raises some concerns over the fairness of any system trained on this\ndata and its ability to generalize properly to diverse populations. We first\nanalyze the dataset and some variants, finding substantial racial bias and\ngender stereotypes. We then extract several subsets with different demographic\nproperties and train a model on each one, observing the amount of residual bias\nin the different setups. We also provide a second analysis on a different\ndataset, FER+.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_Catena_I/0/1/0/all/0/1\">Iris Dominguez-Catena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paternain_D/0/1/0/all/0/1\">Daniel Paternain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galar_M/0/1/0/all/0/1\">Mikel Galar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality. (arXiv:2205.10063v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10063","description":"<p>Masked AutoEncoder (MAE) has recently led the trends of visual\nself-supervision area by an elegant asymmetric encoder-decoder design, which\nsignificantly optimizes both the pre-training efficiency and fine-tuning\naccuracy. Notably, the success of the asymmetric structure relies on the\n\"global\" property of Vanilla Vision Transformer (ViT), whose self-attention\nmechanism reasons over arbitrary subset of discrete image patches. However, it\nis still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be\nadopted in MAE pre-training as they commonly introduce operators within \"local\"\nwindows, making it difficult to handle the random sequence of partial vision\ntokens. In this paper, we propose Uniform Masking (UM), successfully enabling\nMAE pre-training for Pyramid-based ViTs with locality (termed \"UM-MAE\" for\nshort). Specifically, UM includes a Uniform Sampling (US) that strictly samples\n$1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM)\nwhich randomly masks a portion of (usually $25\\%$) the already sampled regions\nas learnable tokens. US preserves equivalent elements across multiple\nnon-overlapped local windows, resulting in the smooth support for popular\nPyramid-based ViTs; whilst SM is designed for better transferable visual\nrepresentations since US reduces the difficulty of pixel recovery pre-task that\nhinders the semantic learning. We demonstrate that UM-MAE significantly\nimproves the pre-training efficiency (e.g., it speeds up and reduces the GPU\nmemory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive\nfine-tuning performance across downstream tasks. For example using HTC++\ndetector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only\nin ImageNet-1K can even outperform the one supervised in ImageNet-22K. The\ncodes are available at https://github.com/implus/UM-MAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Cross-Modal Knowledge Mining for Multimodal Human Activity Recognition. (arXiv:2205.10071v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10071","description":"<p>Human Activity Recognition is a field of research where input data can take\nmany forms. Each of the possible input modalities describes human behaviour in\na different way, and each has its own strengths and weaknesses. We explore the\nhypothesis that leveraging multiple modalities can lead to better recognition.\nSince manual annotation of input data is expensive and time-consuming, the\nemphasis is made on self-supervised methods which can learn useful feature\nrepresentations without any ground truth labels. We extend a number of recent\ncontrastive self-supervised approaches for the task of Human Activity\nRecognition, leveraging inertial and skeleton data. Furthermore, we propose a\nflexible, general-purpose framework for performing multimodal self-supervised\nlearning, named Contrastive Multiview Coding with Cross-Modal Knowledge Mining\n(CMC-CMKM). This framework exploits modality-specific knowledge in order to\nmitigate the limitations of typical self-supervised frameworks. The extensive\nexperiments on two widely-used datasets demonstrate that the suggested\nframework significantly outperforms contrastive unimodal and multimodal\nbaselines on different scenarios, including fully-supervised fine-tuning,\nactivity retrieval and semi-supervised learning. Furthermore, it shows\nperformance competitive even compared to supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brinzea_R/0/1/0/all/0/1\">Razvan Brinzea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaertdinov_B/0/1/0/all/0/1\">Bulat Khaertdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asteriadis_S/0/1/0/all/0/1\">Stylianos Asteriadis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unintended memorisation of unique features in neural networks. (arXiv:2205.10079v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10079","description":"<p>Neural networks pose a privacy risk due to their propensity to memorise and\nleak training data. We show that unique features occurring only once in\ntraining data are memorised by discriminative multi-layer perceptrons and\nconvolutional neural networks trained on benchmark imaging datasets. We design\nour method for settings where sensitive training data is not available, for\nexample medical imaging. Our setting knows the unique feature, but not the\ntraining data, model weights or the unique feature's label. We develop a score\nestimating a model's sensitivity to a unique feature by comparing the KL\ndivergences of the model's output distributions given modified\nout-of-distribution images. We find that typical strategies to prevent\noverfitting do not prevent unique feature memorisation. And that images\ncontaining a unique feature are highly influential, regardless of the influence\nthe images's other features. We also find a significant variation in\nmemorisation with training seed. These results imply that neural networks pose\na privacy risk to rarely occurring private information. This risk is more\npronounced in healthcare applications since sensitive patient information can\nbe memorised when it remains in training data due to an imperfect data\nsanitisation process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartley_J/0/1/0/all/0/1\">John Hartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergence of Double-slit Interference by Representing Visual Space in Artificial Neural Networks. (arXiv:2205.10081v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10081","description":"<p>Artificial neural networks have realized incredible successes at image\nrecognition, but the underlying mechanism of visual space representation\nremains a huge mystery. Grid cells (2014 Nobel Prize) in the entorhinal cortex\nsupport a periodic representation as a metric for coding space. Here, we\ndevelop a self-supervised convolutional neural network to perform visual space\nlocation, leading to the emergence of single-slit diffraction and double-slit\ninterference patterns of waves. Our discoveries reveal the nature of CNN\nencoding visual space to a certain extent. CNN is no longer a black box in\nterms of visual spatial encoding, it is interpretable. Our findings indicate\nthat the periodicity property of waves provides a space metric, suggesting a\ngeneral role of spatial coordinate frame in artificial neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiuxiu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yongqiang Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"People Tracking and Re-Identifying in Distributed Contexts: Extension of PoseTReID. (arXiv:2205.10086v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10086","description":"<p>In our previous paper, we introduced PoseTReID which is a generic framework\nfor real-time 2D multi-person tracking in distributed interaction spaces where\nlong-term people's identities are important for other studies such as behavior\nanalysis, etc. In this paper, we introduce a further study of PoseTReID\nframework in order to give a more complete comprehension of the framework. We\nuse a well-known bounding box detector YOLO (v4) for the detection to compare\nto OpenPose which was used in our last paper, and we use SORT and DeepSORT to\ncompare to centroid which was also used previously, and most importantly for\nthe re-identification, we use a bunch of deep leaning methods such as MLFN,\nOSNet, and OSNet-AIN with our custom classification layer to compare to FaceNet\nwhich was also used earlier in our last paper. By evaluating on our PoseTReID\ndatasets, even though those deep learning re-identification methods are\ndesigned for only short-term re-identification across multiple cameras or\nvideos, it is worth showing that they give impressive results which boost the\noverall tracking performance of PoseTReID framework regardless the type of\ntracking method. At the same time, we also introduce our research-friendly and\nopen source Python toolbox pyppbox, which is pure written in Python and\ncontains all sub-modules which are used this study along with real-time online\nand offline evaluations for our PoseTReID datasets. This pyppbox is available\non GitHub https://github.com/rathaumons/pyppbox .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siv_R/0/1/0/all/0/1\">Ratha Siv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancas_M/0/1/0/all/0/1\">Matei Mancas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_B/0/1/0/all/0/1\">Bernard Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valy_D/0/1/0/all/0/1\">Dona Valy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreng_S/0/1/0/all/0/1\">Sokchenda Sreng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel Normalized Convolutional Networks. (arXiv:2205.10089v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10089","description":"<p>Existing deep convolutional neural network (CNN) architectures frequently\nrely upon batch normalization (BatchNorm) to effectively train the model.\nBatchNorm significantly improves model performance, but performs poorly with\nsmaller batch sizes. To address this limitation, we propose kernel\nnormalization and kernel normalized convolutional layers, and incorporate them\ninto kernel normalized convolutional networks (KNConvNets) as the main building\nblocks. We implement KNConvNets corresponding to the state-of-the-art CNNs such\nas ResNet and DenseNet while forgoing BatchNorm layers. Through extensive\nexperiments, we illustrate that KNConvNets consistently outperform their batch,\ngroup, and layer normalized counterparts in terms of both accuracy and\nconvergence rate while maintaining competitive computational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasirigerdeh_R/0/1/0/all/0/1\">Reza Nasirigerdeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torkzadehmahani_R/0/1/0/all/0/1\">Reihaneh Torkzadehmahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Concepts Tokenization. (arXiv:2205.10093v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10093","description":"<p>Obtaining the human-like perception ability of abstracting visual concepts\nfrom concrete pixels has always been a fundamental and important target in\nmachine learning research fields such as disentangled representation learning\nand scene decomposition. Towards this goal, we propose an unsupervised\ntransformer-based Visual Concepts Tokenization framework, dubbed VCT, to\nperceive an image into a set of disentangled visual concept tokens, with each\nconcept token responding to one type of independent visual concept.\nParticularly, to obtain these concept tokens, we only use cross-attention to\nextract visual information from the image tokens layer by layer without\nself-attention between concept tokens, preventing information leakage across\nconcept tokens. We further propose a Concept Disentangling Loss to facilitate\nthat different concept tokens represent independent visual concepts. The\ncross-attention and disentangling loss play the role of induction and mutual\nexclusion for the concept tokens, respectively. Extensive experiments on\nseveral popular datasets verify the effectiveness of VCT on the tasks of\ndisentangled representation learning and scene decomposition. VCT achieves the\nstate of the art results by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion. (arXiv:2205.10101v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10101","description":"<p>Measuring the perceptual quality of images automatically is an essential task\nin the area of computer vision, as degradations on image quality can exist in\nmany processes from image acquisition, transmission to enhancing. Many Image\nQuality Assessment(IQA) algorithms have been designed to tackle this problem.\nHowever, it still remains un settled due to the various types of image\ndistortions and the lack of large-scale human-rated datasets. In this paper, we\npropose a novel algorithm based on the Swin Transformer [31] with fused\nfeatures from multiple stages, which aggregates information from both local and\nglobal features to better predict the quality. To address the issues of\nsmall-scale datasets, relative rankings of images have been taken into account\ntogether with regression loss to simultaneously optimize the model.\nFurthermore, effective data augmentation strategies are also used to improve\nthe performance. In comparisons with previous works, experiments are carried\nout on two standard IQA datasets and a challenge dataset. The results\ndemonstrate the effectiveness of our work. The proposed method outperforms\nother methods on standard datasets and ranks 2nd in the no-reference track of\nNTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that\nour method is promising in solving diverse IQA problems and thus can be used to\nreal-word applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fa_H/0/1/0/all/0/1\">Haotian Fa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xiaoxia Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yitian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuechao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Lean Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging. (arXiv:2205.10102v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10102","description":"<p>In coded aperture snapshot spectral compressive imaging (CASSI) systems,\nhyperspectral image (HSI) reconstruction methods are employed to recover the\nspatial-spectral signal from a compressed measurement. Among these algorithms,\ndeep unfolding methods demonstrate promising performance but suffer from two\nissues. Firstly, they do not estimate the degradation patterns and\nill-posedness degree from the highly related CASSI to guide the iterative\nlearning. Secondly, they are mainly CNN-based, showing limitations in capturing\nlong-range dependencies. In this paper, we propose a principled\nDegradation-Aware Unfolding Framework (DAUF) that estimates parameters from the\ncompressed image and physical mask, and then uses these parameters to control\neach iteration. Moreover, we customize a novel Half-Shuffle Transformer (HST)\nthat simultaneously captures local contents and non-local dependencies. By\nplugging HST into DAUF, we establish the first Transformer-based deep unfolding\nmethod, Degradation-Aware Unfolding Half-Shuffle Transformer (DAUHST), for HSI\nreconstruction. Experiments show that DAUHST significantly surpasses\nstate-of-the-art methods while requiring cheaper computational and memory\ncosts. Code and models will be released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Preserving Image Registration. (arXiv:2205.10120v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10120","description":"<p>Image registration is a key task in medical imaging applications, allowing to\nrepresent medical images in a common spatial reference frame. Current\nliterature on image registration is generally based on the assumption that\nimages are usually accessible to the researcher, from which the spatial\ntransformation is subsequently estimated. This common assumption may not be met\nin current practical applications, since the sensitive nature of medical images\nmay ultimately require their analysis under privacy constraints, preventing to\nshare the image content in clear form. In this work, we formulate the problem\nof image registration under a privacy preserving regime, where images are\nassumed to be confidential and cannot be disclosed in clear. We derive our\nprivacy preserving image registration framework by extending classical\nregistration paradigms to account for advanced cryptographic tools, such as\nsecure multi-party computation and homomorphic encryption, that enable the\nexecution of operations without leaking the underlying data. To overcome the\nproblem of performance and scalability of cryptographic tools in high\ndimensions, we first propose to optimize the underlying image registration\noperations using gradient approximations. We further revisit the use of\nhomomorphic encryption and use a packing method to allow the encryption and\nmultiplication of large matrices more efficiently. We demonstrate our privacy\npreserving framework in linear and non-linear registration problems, evaluating\nits accuracy and scalability with respect to standard image registration. Our\nresults show that privacy preserving image registration is feasible and can be\nadopted in sensitive medical imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taiello_R/0/1/0/all/0/1\">Riccardo Taiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onen_M/0/1/0/all/0/1\">Melek &#xd6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humbert_O/0/1/0/all/0/1\">Olivier Humbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliability-based Mesh-to-Grid Image Reconstruction. (arXiv:2205.10138v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10138","description":"<p>This paper presents a novel method for the reconstruction of images from\nsamples located at non-integer positions, called mesh. This is a common\nscenario for many image processing applications, such as super-resolution,\nwarping or virtual view generation in multi-camera systems. The proposed method\nrelies on a set of initial estimates that are later refined by a new\nreliability-based content-adaptive framework that employs denoising in order to\nreduce the reconstruction error. The reliability of the initial estimate is\ncomputed so stronger denoising is applied to less reliable estimates. The\nproposed technique can improve the reconstruction quality by more than 2 dB (in\nterms of PSNR) with respect to the initial estimate and it outperforms the\nstate-of-the-art denoising-based refinement by up to 0.7 dB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koloda_J/0/1/0/all/0/1\">J&#xe1;n Koloda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seiler_J/0/1/0/all/0/1\">J&#xfc;rgen Seiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The developmental trajectory of object recognition robustness: children are like small adults but unlike big deep neural networks. (arXiv:2205.10144v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10144","description":"<p>In laboratory object recognition tasks based on undistorted photographs, both\nadult humans and Deep Neural Networks (DNNs) perform close to ceiling. Unlike\nadults', whose object recognition performance is robust against a wide range of\nimage distortions, DNNs trained on standard ImageNet (1.3M images) perform\npoorly on distorted images. However, the last two years have seen impressive\ngains in DNN distortion robustness, predominantly achieved through\never-increasing large-scale datasets$\\unicode{x2014}$orders of magnitude larger\nthan ImageNet. While this simple brute-force approach is very effective in\nachieving human-level robustness in DNNs, it raises the question of whether\nhuman robustness, too, is simply due to extensive experience with (distorted)\nvisual input during childhood and beyond. Here we investigate this question by\ncomparing the core object recognition performance of 146 children (aged\n4$\\unicode{x2013}$15) against adults and against DNNs. We find, first, that\nalready 4$\\unicode{x2013}$6 year-olds showed remarkable robustness to image\ndistortions and outperform DNNs trained on ImageNet. Second, we estimated the\nnumber of $\\unicode{x201C}$images$\\unicode{x201D}$ children have been exposed\nto during their lifetime. Compared to various DNNs, children's high robustness\nrequires relatively little data. Third, when recognizing objects\nchildren$\\unicode{x2014}$like adults but unlike DNNs$\\unicode{x2014}$rely\nheavily on shape but not on texture cues. Together our results suggest that the\nremarkable robustness to distortions emerges early in the developmental\ntrajectory of human object recognition and is unlikely the result of a mere\naccumulation of experience with distorted visual input. Even though current\nDNNs match human performance regarding robustness they seem to rely on\ndifferent and more data-hungry strategies to do so.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_L/0/1/0/all/0/1\">Lukas S. Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1\">Felix A. Wichmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swapping Semantic Contents for Mixing Images. (arXiv:2205.10158v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10158","description":"<p>Deep architecture have proven capable of solving many tasks provided a\nsufficient amount of labeled data. In fact, the amount of available labeled\ndata has become the principal bottleneck in low label settings such as\nSemi-Supervised Learning. Mixing Data Augmentations do not typically yield new\nlabeled samples, as indiscriminately mixing contents creates between-class\nsamples. In this work, we introduce the SciMix framework that can learn to\ngenerator to embed a semantic style code into image backgrounds, we obtain new\nmixing scheme for data augmentation. We then demonstrate that SciMix yields\nnovel mixed samples that inherit many characteristics from their non-semantic\nparents. Afterwards, we verify those samples can be used to improve the\nperformance semi-supervised frameworks like Mean Teacher or Fixmatch, and even\nfully supervised learning on a small labeled dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">R&#xe9;my Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masson_C/0/1/0/all/0/1\">Cl&#xe9;ment Masson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henaff_G/0/1/0/all/0/1\">Gilles H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1\">Nicolas Thome</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Generation of Synthetic Images of Palm Vein Patterns: A Review. (arXiv:2205.10179v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10179","description":"<p>With the recent success of computer vision and deep learning, remarkable\nprogress has been achieved on automatic personal recognition using vein\nbiometrics. However, collecting large-scale real-world training data for palm\nvein recognition has turned out to be challenging, mainly due to the noise and\nirregular variations included at the time of acquisition. Meanwhile, existing\npalm vein recognition datasets are usually collected under near-infrared light,\nlacking detailed annotations on attributes (e.g., pose), so the influences of\ndifferent attributes on vein recognition have been poorly investigated.\nTherefore, this paper examines the suitability of synthetic vein images\ngenerated to compensate for the urgent lack of publicly available large-scale\ndatasets. Firstly, we present an overview of recent research progress on palm\nvein recognition, from the basic background knowledge to vein anatomical\nstructure, data acquisition, public database, and quality assessment\nprocedures. Then, we focus on the state-of-the-art methods that have allowed\nthe generation of vascular structures for biometric purposes and the modeling\nof biological networks with their respective application domains. In addition,\nwe review the existing research on the generation of style transfer and\nbiological nature-based synthetic palm vein image algorithms. Afterward, we\nformalize a general flowchart for the creation of a synthetic database\ncomparing real palm vein images and generated synthetic samples to obtain some\nunderstanding into the development of the realistic vein imaging system.\nUltimately, we conclude by discussing the challenges, insights, and future\nperspectives in generating synthetic palm vein images for further works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salazar_Jurado_E/0/1/0/all/0/1\">Edwin H. Salazar-Jurado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Garcia_R/0/1/0/all/0/1\">Ruber Hern&#xe1;ndez-Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilches_Ponce_K/0/1/0/all/0/1\">Karina Vilches-Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrientos_R/0/1/0/all/0/1\">Ricardo J. Barrientos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mora_M/0/1/0/all/0/1\">Marco Mora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaswal_G/0/1/0/all/0/1\">Gaurav Jaswal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-Scooter Rider Detection and Classification in Dense Urban Environments. (arXiv:2205.10184v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10184","description":"<p>Accurate detection and classification of vulnerable road users is a safety\ncritical requirement for the deployment of autonomous vehicles in heterogeneous\ntraffic. Although similar in physical appearance to pedestrians, e-scooter\nriders follow distinctly different characteristics of movement and can reach\nspeeds of up to 45kmph. The challenge of detecting e-scooter riders is\nexacerbated in urban environments where the frequency of partial occlusion is\nincreased as riders navigate between vehicles, traffic infrastructure and other\nroad users. This can lead to the non-detection or mis-classification of\ne-scooter riders as pedestrians, providing inaccurate information for accident\nmitigation and path planning in autonomous vehicle applications. This research\nintroduces a novel benchmark for partially occluded e-scooter rider detection\nto facilitate the objective characterization of detection models. A novel,\nocclusion-aware method of e-scooter rider detection is presented that achieves\na 15.93% improvement in detection performance over the current state of the\nart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilroy_S/0/1/0/all/0/1\">Shane Gilroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_D/0/1/0/all/0/1\">Darragh Mullins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_E/0/1/0/all/0/1\">Edward Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsi_A/0/1/0/all/0/1\">Ashkan Parsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavin_M/0/1/0/all/0/1\">Martin Glavin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration. (arXiv:2205.10195v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10195","description":"<p>How to properly model the inter-frame relation within the video sequence is\nan important but unsolved challenge for video restoration (VR). In this work,\nwe propose an unsupervised flow-aligned sequence-to-sequence model (S2SVR) to\naddress this problem. On the one hand, the sequence-to-sequence model, which\nhas proven capable of sequence modeling in the field of natural language\nprocessing, is explored for the first time in VR. Optimized serialization\nmodeling shows potential in capturing long-range dependencies among frames. On\nthe other hand, we equip the sequence-to-sequence model with an unsupervised\noptical flow estimator to maximize its potential. The flow estimator is trained\nwith our proposed unsupervised distillation loss, which can alleviate the data\ndiscrepancy and inaccurate degraded optical flow issues of previous flow-based\nmethods. With reliable optical flow, we can establish accurate correspondence\namong multiple frames, narrowing the domain difference between 1D language and\n2D misaligned frames and improving the potential of the sequence-to-sequence\nmodel. S2SVR shows superior performance in multiple VR tasks, including video\ndeblurring, video super-resolution, and compressed video quality enhancement.\nCode and models are publicly available at\nhttps://github.com/linjing7/VR-Baseline\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xueyi Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Underwater Image Enhancement and Improved Underwater Biological Detection Pipeline. (arXiv:2205.10199v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10199","description":"<p>For aquaculture resource evaluation and ecological environment monitoring,\nautomatic detection and identification of marine organisms is critical.\nHowever, due to the low quality of underwater images and the characteristics of\nunderwater biological, a lack of abundant features may impede traditional\nhand-designed feature extraction approaches or CNN-based object detection\nalgorithms, particularly in complex underwater environment. Therefore, the goal\nof this paper is to perform object detection in the underwater environment.\nThis paper proposed a novel method for capturing feature information, which\nadds the convolutional block attention module (CBAM) to the YOLOv5 backbone.\nThe interference of underwater creature characteristics on object\ncharacteristics is decreased, and the output of the backbone network to object\ninformation is enhanced. In addition, the self-adaptive global histogram\nstretching algorithm (SAGHS) is designed to eliminate the degradation problems\nsuch as low contrast and color loss caused by underwater environmental\ninformation to better restore image quality. Extensive experiments and\ncomprehensive evaluation on the URPC2021 benchmark dataset demonstrate the\neffectiveness and adaptivity of our methods. Beyond that, this paper conducts\nan exhaustive analysis of the role of training data on performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yaoming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_P/0/1/0/all/0/1\">Pengrun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chengdong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongli Xu ang Zhanlin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Guide Adaptive Depth Sampling?. (arXiv:2205.10202v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10202","description":"<p>Recent advances in depth sensing technologies allow fast electronic\nmaneuvering of the laser beam, as opposed to fixed mechanical rotations. This\nwill enable future sensors, in principle, to vary in real-time the sampling\npattern. We examine here the abstract problem of whether adapting the sampling\npattern for a given frame can reduce the reconstruction error or allow a\nsparser pattern. We propose a constructive generic method to guide adaptive\ndepth sampling algorithms.\n</p>\n<p>Given a sampling budget B, a depth predictor P and a desired quality measure\nM, we propose an Importance Map that highlights important sampling locations.\nThis map is defined for a given frame as the per-pixel expected value of M\nproduced by the predictor P, given a pattern of B random samples. This map can\nbe well estimated in a training phase. We show that a neural network can learn\nto produce a highly faithful Importance Map, given an RGB image. We then\nsuggest an algorithm to produce a sampling pattern for the scene, which is\ndenser in regions that are harder to reconstruct. The sampling strategy of our\nmodular framework can be adjusted according to hardware limitations, type of\ndepth predictor, and any custom reconstruction error measure that should be\nminimized. We validate through simulations that our approach outperforms grid\nand random sampling patterns as well as recent state-of-the-art adaptive\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tcenov_I/0/1/0/all/0/1\">Ilya Tcenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1\">Guy Gilboa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Count Anything: Reference-less Class-agnostic Counting with Weak Supervision. (arXiv:2205.10203v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10203","description":"<p>Object counting is a seemingly simple task with diverse real-world\napplications. Most counting methods focus on counting instances of specific,\nknown classes. While there are class-agnostic counting methods that can\ngeneralise to unseen classes, these methods require reference images to define\nthe type of object to be counted, as well as instance annotations during\ntraining. We identify that counting is, at its core, a repetition-recognition\ntask and show that a general feature space, with global context, is sufficient\nto enumerate instances in an image without a prior on the object type present.\nSpecifically, we demonstrate that self-supervised vision transformer features\ncombined with a lightweight count regression head achieve competitive results\nwhen compared to other class-agnostic counting tasks without the need for\npoint-level supervision or reference images. Our method thus facilitates\ncounting on a constantly changing set composition. To the best of our\nknowledge, we are both the first reference-less class-agnostic counting method\nas well as the first weakly-supervised class-agnostic counting method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hobley_M/0/1/0/all/0/1\">Michael Hobley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1\">Victor Prisacariu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time Batch Normalization. (arXiv:2205.10210v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10210","description":"<p>Deep neural networks often suffer the data distribution shift between\ntraining and testing, and the batch statistics are observed to reflect the\nshift. In this paper, targeting of alleviating distribution shift in test time,\nwe revisit the batch normalization (BN) in the training process and reveals two\nkey insights benefiting test-time optimization: $(i)$ preserving the same\ngradient backpropagation form as training, and $(ii)$ using dataset-level\nstatistics for robust optimization and inference. Based on the two insights, we\npropose a novel test-time BN layer design, GpreBN, which is optimized during\ntesting by minimizing Entropy loss. We verify the effectiveness of our method\non two typical settings with distribution shift, i.e., domain generalization\nand robustness tasks. Our GpreBN significantly improves the test-time\nperformance and achieves the state of the art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shenglong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions. (arXiv:2205.10218v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10218","description":"<p>Generalization across different environments with the same tasks is critical\nfor successful applications of visual reinforcement learning (RL) in real\nscenarios. However, visual distractions -- which are common in real scenes --\nfrom high-dimensional observations can be hurtful to the learned\nrepresentations in visual RL, thus degrading the performance of generalization.\nTo tackle this problem, we propose a novel approach, namely Characteristic\nReward Sequence Prediction (CRESP), to extract the task-relevant information by\nlearning reward sequence distributions (RSDs), as the reward signals are\ntask-relevant in RL and invariant to visual distractions. Specifically, to\neffectively capture the task-relevant information via RSDs, CRESP introduces an\nauxiliary task -- that is, predicting the characteristic functions of RSDs --\nto learn task-relevant representations, because we can well approximate the\nhigh-dimensional distributions by leveraging the corresponding characteristic\nfunctions. Experiments demonstrate that CRESP significantly improves the\nperformance of generalization on unseen environments, outperforming several\nstate-of-the-arts on DeepMind Control tasks with different visual distractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mingxuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mosaic Zonotope Shadow Matching for Risk-Aware Autonomous Localization in Harsh Urban Environments. (arXiv:2205.10223v1 [cs.AI])","link":"http://arxiv.org/abs/2205.10223","description":"<p>Risk-aware urban localization with the Global Navigation Satellite System\n(GNSS) remains an unsolved problem with frequent misdetection of the user's\nstreet or side of the street. Significant advances in 3D map-aided GNSS use\ngrid-based GNSS shadow matching alongside AI-driven line-of-sight (LOS)\nclassifiers and server-based processing to improve localization accuracy,\nespecially in the cross-street direction. Our prior work introduces a new\nparadigm for shadow matching that proposes set-valued localization with\ncomputationally efficient zonotope set representations. While existing\nliterature improved accuracy and efficiency, the current state of shadow\nmatching theory does not address the needs of risk-aware autonomous systems. We\nextend our prior work to propose Mosaic Zonotope Shadow Matching (MZSM) that\nemploys a classifier-agnostic polytope mosaic architecture to provide\nrisk-awareness and certifiable guarantees on urban positioning. We formulate a\nrecursively expanding binary tree that refines an initial location estimate\nwith set operations into smaller polytopes. Together, the smaller polytopes\nform a mosaic. We weight the tree branches with the probability that the user\nis in line of sight of the satellite and expand the tree with each new\nsatellite observation. Our method yields an exact shadow matching distribution\nfrom which we guarantee uncertainty bounds on the user localization. We perform\nhigh-fidelity simulations using a 3D building map of San Francisco to validate\nour algorithm's risk-aware improvements. We demonstrate that MZSM provides\ncertifiable guarantees across varied data-driven LOS classifier accuracies and\nyields a more precise understanding of the uncertainty over existing methods.\nWe validate that our tree-based construction is efficient and tractable,\ncomputing a mosaic from 14 satellites in 0.63 seconds and growing quadratically\nin the satellite number.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neamati_D/0/1/0/all/0/1\">Daniel Neamati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhamidipati_S/0/1/0/all/0/1\">Sriramya Bhamidipati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Grace Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Demographic Attribute Guided Approach to Age Estimation. (arXiv:2205.10254v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10254","description":"<p>Face-based age estimation has attracted enormous attention due to wide\napplications to public security surveillance, human-computer interaction, etc.\nWith vigorous development of deep learning, age estimation based on deep neural\nnetwork has become the mainstream practice. However, seeking a more suitable\nproblem paradigm for age change characteristics, designing the corresponding\nloss function and designing a more effective feature extraction module still\nneeds to be studied. What is more, change of face age is also related to\ndemographic attributes such as ethnicity and gender, and the dynamics of\ndifferent age groups is also quite different. This problem has so far not been\npaid enough attention to. How to use demographic attribute information to\nimprove the performance of age estimation remains to be further explored. In\nlight of these issues, this research makes full use of auxiliary information of\nface attributes and proposes a new age estimation approach with an attribute\nguidance module. We first design a multi-scale attention residual convolution\nunit (MARCU) to extract robust facial features other than simply using other\nstandard feature modules such as VGG and ResNet. Then, after being especially\ntreated through full connection (FC) layers, the facial demographic attributes\nare weight-summed by 1*1 convolutional layer and eventually merged with the age\nfeatures by a global FC layer. Lastly, we propose a new error compression\nranking (ECR) loss to better converge the age regression value. Experimental\nresults on three public datasets of UTKFace, LAP2016 and Morph show that our\nproposed approach achieves superior performance compared to other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhicheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaituo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liaojun Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Heng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Co-Laughter Gesture Relationship on RGB videos in Dyadic Conversation Contex. (arXiv:2205.10266v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10266","description":"<p>The development of virtual agents has enabled human-avatar interactions to\nbecome increasingly rich and varied. Moreover, an expressive virtual agent i.e.\nthat mimics the natural expression of emotions, enhances social interaction\nbetween a user (human) and an agent (intelligent machine). The set of\nnon-verbal behaviors of a virtual character is, therefore, an important\ncomponent in the context of human-machine interaction. Laughter is not just an\naudio signal, but an intrinsic relationship of multimodal non-verbal\ncommunication, in addition to audio, it includes facial expressions and body\nmovements. Motion analysis often relies on a relevant motion capture dataset,\nbut the main issue is that the acquisition of such a dataset is expensive and\ntime-consuming. This work studies the relationship between laughter and body\nmovements in dyadic conversations. The body movements were extracted from\nvideos using deep learning based pose estimator model. We found that, in the\nexplored NDC-ME dataset, a single statistical feature (i.e, the maximum value,\nor the maximum of Fourier transform) of a joint movement weakly correlates with\nlaughter intensity by 30%. However, we did not find a direct correlation\nbetween audio features and body movements. We discuss about the challenges to\nuse such dataset for the audio-driven co-laughter motion synthesis task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohy_H/0/1/0/all/0/1\">Hugo Bohy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammoudeh_A/0/1/0/all/0/1\">Ahmad Hammoudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_A/0/1/0/all/0/1\">Antoine Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupont_S/0/1/0/all/0/1\">St&#xe9;phane Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"B-cos Networks: Alignment is All We Need for Interpretability. (arXiv:2205.10268v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10268","description":"<p>We present a new direction for increasing the interpretability of deep neural\nnetworks (DNNs) by promoting weight-input alignment during training. For this,\nwe propose to replace the linear transforms in DNNs by our B-cos transform. As\nwe show, a sequence (network) of such transforms induces a single linear\ntransform that faithfully summarises the full model computations. Moreover, the\nB-cos transform introduces alignment pressure on the weights during\noptimisation. As a result, those induced linear transforms become highly\ninterpretable and align with task-relevant features. Importantly, the B-cos\ntransform is designed to be compatible with existing architectures and we show\nthat it can easily be integrated into common models such as VGGs, ResNets,\nInceptionNets, and DenseNets, whilst maintaining similar performance on\nImageNet. The resulting explanations are of high visual quality and perform\nwell under quantitative metrics for interpretability. Code available at\nhttps://www.github.com/moboehle/B-cos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohle_M/0/1/0/all/0/1\">Moritz B&#xf6;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compression ensembles quantify aesthetic complexity and the evolution of visual art. (arXiv:2205.10271v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10271","description":"<p>The quantification of visual aesthetics and complexity have a long history,\nthe latter previously operationalized via the application of compression\nalgorithms. Here we generalize and extend the compression approach beyond\nsimple complexity measures to quantify algorithmic distance in historical and\ncontemporary visual media. The proposed \"ensemble\" approach works by\ncompressing a large number of transformed versions of a given input image,\nresulting in a vector of associated compression ratios. This approach is more\nefficient than other compression-based algorithmic distances, and is\nparticularly suited for the quantitative analysis of visual artifacts, because\nhuman creative processes can be understood as algorithms in the broadest sense.\nUnlike comparable image embedding methods using machine learning, our approach\nis fully explainable through the transformations. We demonstrate that the\nmethod is cognitively plausible and fit for purpose by evaluating it against\nhuman complexity judgments, and on automated detection tasks of authorship and\nstyle. We show how the approach can be used to reveal and quantify trends in\nart historical data, both on the scale of centuries and in rapidly evolving\ncontemporary NFT art markets. We further quantify temporal resemblance to\ndisambiguate artists outside the documented mainstream from those who are\ndeeply embedded in Zeitgeist. Finally, we note that compression ensembles\nconstitute a quantitative representation of the concept of visual family\nresemblance, as distinct sets of dimensions correspond to shared visual\ncharacteristics otherwise hard to pin down. Our approach provides a new\nperspective for the study of visual art, algorithmic image analysis, and\nquantitative aesthetics more generally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karjus_A/0/1/0/all/0/1\">Andres Karjus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sola_M/0/1/0/all/0/1\">Mar Canet Sol&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohm_T/0/1/0/all/0/1\">Tillmann Ohm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahnert_S/0/1/0/all/0/1\">Sebastian E. Ahnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schich_M/0/1/0/all/0/1\">Maximilian Schich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Skin Lesion Segmentation via Dilated Scale-Wise Feature Fusion Network. (arXiv:2205.10272v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10272","description":"<p>Skin lesion detection in dermoscopic images is essential in the accurate and\nearly diagnosis of skin cancer by a computerized apparatus. Current skin lesion\nsegmentation approaches show poor performance in challenging circumstances such\nas indistinct lesion boundaries, low contrast between the lesion and the\nsurrounding area, or heterogeneous background that causes over/under\nsegmentation of the skin lesion. To accurately recognize the lesion from the\nneighboring regions, we propose a dilated scale-wise feature fusion network\nbased on convolution factorization. Our network is designed to simultaneously\nextract features at different scales which are systematically fused for better\ndetection. The proposed model has satisfactory accuracy and efficiency. Various\nexperiments for lesion segmentation are performed along with comparisons with\nthe state-of-the-art models. Our proposed model consistently showcases\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1\">Pourya Shamsolmoali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1\">Masoumeh Zareapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors. (arXiv:2205.10279v1 [cs.LG])","link":"http://arxiv.org/abs/2205.10279","description":"<p>Deep learning is increasingly moving towards a transfer learning paradigm\nwhereby large foundation models are fine-tuned on downstream tasks, starting\nfrom an initialization learned on the source task. But an initialization\ncontains relatively little information about the source task. Instead, we show\nthat we can learn highly informative posteriors from the source task, through\nsupervised or self-supervised approaches, which then serve as the basis for\npriors that modify the whole loss surface on the downstream task. This simple\nmodular approach enables significant performance gains and more data-efficient\nlearning on a variety of downstream classification and segmentation tasks,\nserving as a drop-in replacement for standard pre-training strategies. These\nhighly informative priors also can be saved for future use, similar to\npre-trained weights, and stand in contrast to the zero-mean isotropic\nuninformative priors that are typically used in Bayesian deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_Ziv_R/0/1/0/all/0/1\">Ravid Shwartz-Ziv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souri_H/0/1/0/all/0/1\">Hossein Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapoor_S/0/1/0/all/0/1\">Sanyam Kapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Localization using RF Sensing: A Performance comparison between LIS and mmWave Radars. (arXiv:2205.10321v1 [eess.SP])","link":"http://arxiv.org/abs/2205.10321","description":"<p>Since electromagnetic signals are omnipresent, Radio Frequency (RF)-sensing\nhas the potential to become a universal sensing mechanism with applications in\nlocalization, smart-home, retail, gesture recognition, intrusion detection,\netc. Two emerging technologies in RF-sensing, namely sensing through Large\nIntelligent Surfaces (LISs) and mmWave Frequency-Modulated Continuous-Wave\n(FMCW) radars, have been successfully applied to a wide range of applications.\nIn this work, we compare LIS and mmWave radars for localization in real-world\nand simulated environments. In our experiments, the mmWave radar achieves 0.71\nIntersection Over Union (IOU) and 3cm error for bounding boxes, while LIS has\n0.56 IOU and 10cm distance error. Although the radar outperforms the LIS in\nterms of accuracy, LIS features additional applications in communication in\naddition to sensing scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vaca_Rubio_C/0/1/0/all/0/1\">Cristian J. Vaca-Rubio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salami_D/0/1/0/all/0/1\">Dariush Salami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Popovski_P/0/1/0/all/0/1\">Petar Popovski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carvalho_E/0/1/0/all/0/1\">Elisabeth de Carvalho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_Z/0/1/0/all/0/1\">Zheng-Hua Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sigg_S/0/1/0/all/0/1\">Stephan Sigg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UCC: Uncertainty guided Cross-head Co-training for Semi-Supervised Semantic Segmentation. (arXiv:2205.10334v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10334","description":"<p>Deep neural networks (DNNs) have witnessed great successes in semantic\nsegmentation, which requires a large number of labeled data for training. We\npresent a novel learning framework called Uncertainty guided Cross-head\nCo-training (UCC) for semi-supervised semantic segmentation. Our framework\nintroduces weak and strong augmentations within a shared encoder to achieve\nco-training, which naturally combines the benefits of consistency and\nself-training. Every segmentation head interacts with its peers and, the weak\naugmentation result is used for supervising the strong. The consistency\ntraining samples' diversity can be boosted by Dynamic Cross-Set Copy-Paste\n(DCSCP), which also alleviates the distribution mismatch and class imbalance\nproblems. Moreover, our proposed Uncertainty Guided Re-weight Module (UGRM)\nenhances the self-training pseudo labels by suppressing the effect of the\nlow-quality pseudo labels from its peer via modeling uncertainty. Extensive\nexperiments on Cityscapes and PASCAL VOC 2012 demonstrate the effectiveness of\nour UCC. Our approach significantly outperforms other state-of-the-art\nsemi-supervised semantic segmentation methods. It achieves 77.17$\\%$, 76.49$\\%$\nmIoU on Cityscapes and PASCAL VOC 2012 datasets respectively under 1/16\nprotocols, which are +10.1$\\%$, +7.91$\\%$ better than the supervised baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiashuo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1\">Bin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Huan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lihui Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes. (arXiv:2205.10337v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10337","description":"<p>We introduce UViM, a unified approach capable of modeling a wide range of\ncomputer vision tasks. In contrast to previous models, UViM has the same\nfunctional form for all tasks; it requires no task-specific modifications which\nrequire extensive human expertise. The approach involves two components: (I) a\nbase model (feed-forward) which is trained to directly predict raw vision\noutputs, guided by a learned discrete code and (II) a language model\n(autoregressive) that is trained to generate the guiding code. These components\ncomplement each other: the language model is well-suited to modeling structured\ninterdependent data, while the base model is efficient at dealing with\nhigh-dimensional outputs. We demonstrate the effectiveness of UViM on three\ndiverse and challenging vision tasks: panoptic segmentation, depth prediction\nand image colorization, where we achieve competitive and near state-of-the-art\nresults. Our experimental results suggest that UViM is a promising candidate\nfor a unified modeling approach in computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_A/0/1/0/all/0/1\">Andr&#xe9; Susano Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmsen_J/0/1/0/all/0/1\">Jeremiah Harmsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient visual object representation using a biologically plausible spike-latency code and winner-take-all inhibition. (arXiv:2205.10338v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10338","description":"<p>Deep neural networks have surpassed human performance in key visual\nchallenges such as object recognition, but require a large amount of energy,\ncomputation, and memory. In contrast, spiking neural networks (SNNs) have the\npotential to improve both the efficiency and biological plausibility of object\nrecognition systems. Here we present a SNN model that uses spike-latency coding\nand winner-take-all inhibition (WTA-I) to efficiently represent visual stimuli\nfrom the Fashion MNIST dataset. Stimuli were preprocessed with center-surround\nreceptive fields and then fed to a layer of spiking neurons whose synaptic\nweights were updated using spike-timing-dependent-plasticity (STDP). We\ninvestigate how the quality of the represented objects changes under different\nWTA-I schemes and demonstrate that a network of 150 spiking neurons can\nefficiently represent objects with as little as 40 spikes. Studying how core\nobject recognition may be implemented using biologically plausible learning\nrules in SNNs may not only further our understanding of the brain, but also\nlead to novel and efficient artificial vision systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Garcia_M/0/1/0/all/0/1\">Melani Sanchez-Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyeler_M/0/1/0/all/0/1\">Michael Beyeler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT). (arXiv:2205.10342v1 [eess.IV])","link":"http://arxiv.org/abs/2205.10342","description":"<p>Vision transformers, with their ability to more efficiently model long-range\ncontext, have demonstrated impressive accuracy gains in several computer vision\nand medical image analysis tasks including segmentation. However, such methods\nneed large labeled datasets for training, which is hard to obtain for medical\nimage analysis. Self-supervised learning (SSL) has demonstrated success in\nmedical image segmentation using convolutional networks. In this work, we\ndeveloped a \\underline{s}elf-distillation learning with \\underline{m}asked\n\\underline{i}mage modeling method to perform SSL for vision\n\\underline{t}ransformers (SMIT) applied to 3D multi-organ segmentation from CT\nand MRI. Our contribution is a dense pixel-wise regression within masked\npatches called masked image prediction, which we combined with masked patch\ntoken distillation as pretext task to pre-train vision transformers. We show\nour approach is more accurate and requires fewer fine tuning datasets than\nother pretext tasks. Unlike prior medical image methods, which typically used\nimage sets arising from disease sites and imaging modalities corresponding to\nthe target tasks, we used 3,643 CT scans (602,708 images) arising from head and\nneck, lung, and kidney cancers as well as COVID-19 for pre-training and applied\nit to abdominal organs segmentation from MRI pancreatic cancer patients as well\nas publicly available 13 different abdominal organs segmentation from CT. Our\nmethod showed clear accuracy improvement (average DSC of 0.875 from MRI and\n0.878 from CT) with reduced requirement for fine-tuning datasets over commonly\nused pretext tasks. Extensive comparisons against multiple current SSL methods\nwere done. Code will be made available upon acceptance for publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1\">Jue Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tyagi_N/0/1/0/all/0/1\">Neelam Tyagi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tringale_K/0/1/0/all/0/1\">Kathryn Tringale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crane_C/0/1/0/all/0/1\">Christopher Crane</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_H/0/1/0/all/0/1\">Harini Veeraraghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse super-resolution with pretrained deep hiererarchical VAEs. (arXiv:2205.10347v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10347","description":"<p>Image super-resolution is a one-to-many problem, but most deep-learning based\nmethods only provide one single solution to this problem. In this work, we\ntackle the problem of diverse super-resolution by reusing VD-VAE, a\nstate-of-the art variational autoencoder (VAE). We find that the hierarchical\nlatent representation learned by VD-VAE naturally separates the image\nlow-frequency information, encoded in the latent groups at the top of the\nhierarchy, from the image high-frequency details, determined by the latent\ngroups at the bottom of the latent hierarchy. Starting from this observation,\nwe design a super-resolution model exploiting the specific structure of VD-VAE\nlatent space. Specifically, we train an encoder to encode low-resolution images\nin the subset of VD-VAE latent space encoding the low-frequency information,\nand we combine this encoder with VD-VAE generative model to sample diverse\nsuper-resolved version of a low-resolution input. We demonstrate the ability of\nour method to generate diverse solutions to the super-resolution problem on\nface super-resolution with upsampling factors x4, x8, and x16.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prost_J/0/1/0/all/0/1\">Jean Prost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houdard_A/0/1/0/all/0/1\">Antoine Houdard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1\">Nicolas Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching StyleGAN with Illumination Physics. (arXiv:2205.10351v1 [cs.CV])","link":"http://arxiv.org/abs/2205.10351","description":"<p>StyleGAN generates novel images of a scene from latent codes which are\nimpressively disentangled. But StyleGAN generates images that are \"like\" its\ntraining set. This paper shows how to use simple physical properties of images\nto enrich StyleGAN's generation capacity. We use an intrinsic image method to\ndecompose an image, then search the latent space of a pretrained StyleGAN to\nfind novel directions that fix one component (say, albedo) and vary another\n(say, shading). Therefore, we can change the lighting of a complex scene\nwithout changing the scene layout, object colors, and shapes. Or we can change\nthe colors of objects without changing shading intensity or their scene layout.\nOur experiments suggest the proposed method, StyLitGAN, can add and remove\nluminaires in the scene and generate images with realistic lighting effects --\ncast shadows, soft shadows, inter-reflections, glossy effects -- requiring no\nlabeled paired relighting data or any other geometric supervision. Qualitative\nevaluation confirms that our generated images are realistic and that we can\nchange or fix components at will. Quantitative evaluation shows that\npre-trained StyleGAN could not produce the images StyLitGAN produces; we can\nautomatically generate realistic out-of-distribution images, and so can\nsignificantly enrich the range of images StyleGAN can produce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1\">Anand Bhattad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">D.A. Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing License Plates in Real-Time. (arXiv:1906.04376v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1906.04376","description":"<p>License plate detection and recognition (LPDR) is of growing importance for\nenabling intelligent transportation and ensuring the security and safety of the\ncities. However, LPDR faces a big challenge in a practical environment. The\nlicense plates can have extremely diverse sizes, fonts and colors, and the\nplate images are usually of poor quality caused by skewed capturing angles,\nuneven lighting, occlusion, and blurring. In applications such as surveillance,\nit often requires fast processing. To enable real-time and accurate license\nplate recognition, in this work, we propose a set of techniques: 1) a contour\nreconstruction method along with edge-detection to quickly detect the candidate\nplates; 2) a simple zero-one-alternation scheme to effectively remove the fake\ntop and bottom borders around plates to facilitate more accurate segmentation\nof characters on plates; 3) a set of techniques to augment the training data,\nincorporate SIFT features into the CNN network, and exploit transfer learning\nto obtain the initial parameters for more effective training; and 4) a\ntwo-phase verification procedure to determine the correct plate at low cost, a\nstatistical filtering in the plate detection stage to quickly remove unwanted\ncandidates, and the accurate CR results after the CR process to perform further\nplate verification without additional processing. We implement a complete LPDR\nsystem based on our algorithms. The experimental results demonstrate that our\nsystem can accurately recognize license plate in real-time. Additionally, it\nworks robustly under various levels of illumination and noise, and in the\npresence of car movement. Compared to peer schemes, our system is not only\namong the most accurate ones but is also the fastest, and can be easily applied\nto other scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-Wise Data-Free CNN Compression. (arXiv:2011.09058v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09058","description":"<p>We present a computationally efficient method for compressing a trained\nneural network without using real data. We break the problem of data-free\nnetwork compression into independent layer-wise compressions. We show how to\nefficiently generate layer-wise training data using only a pretrained network.\nWe use this data to perform independent layer-wise compressions on the\npretrained network. We also show how to precondition the network to improve the\naccuracy of our layer-wise compression method. We present results for\nlayer-wise compression using quantization and pruning. When quantizing, we\ncompress with higher accuracy than related works while using orders of\nmagnitude less compute. When compressing MobileNetV2 and evaluating on\nImageNet, our method outperforms existing methods for quantization at all\nbit-widths, achieving a $+0.34\\%$ improvement in $8$-bit quantization, and a\nstronger improvement at lower bit-widths (up to a $+28.50\\%$ improvement at $5$\nbits). When pruning, we outperform baselines of a similar compute envelope,\nachieving $1.5$ times the sparsity rate at the same accuracy. We also show how\nto combine our efficient method with high-compute generative methods to improve\nupon their results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1\">Maxwell Horton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yanzi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow-based Spatio-Temporal Structured Prediction of Dynamics. (arXiv:2104.04391v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04391","description":"<p>Conditional Normalizing Flows (CNFs) are flexible generative models capable\nof representing complicated distributions with high dimensionality and large\ninterdimensional correlations, making them appealing for structured output\nlearning. Their effectiveness in modelling multivariates spatio-temporal\nstructured data has yet to be completely investigated. We propose MotionFlow as\na novel normalizing flows approach that autoregressively conditions the output\ndistributions on the spatio-temporal input features. It combines deterministic\nand stochastic representations with CNFs to create a probabilistic neural\ngenerative approach that can model the variability seen in high-dimensional\nstructured spatio-temporal data. We specifically propose to use conditional\npriors to factorize the latent space for the time dependent modeling. We also\nexploit the use of masked convolutions as autoregressive conditionals in CNFs.\nAs a result, our method is able to define arbitrarily expressive output\nprobability distributions under temporal dynamics in multivariate prediction\ntasks. We apply our method to different tasks, including trajectory prediction,\nmotion prediction, time series forecasting, and binary segmentation, and\ndemonstrate that our model is able to leverage normalizing flows to learn\ncomplicated time dependent conditional distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zand_M/0/1/0/all/0/1\">Mohsen Zand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1\">Michael Greenspan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement. (arXiv:2104.08223v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08223","description":"<p>This paper presents a generic method for generating full facial 3D animation\nfrom speech. Existing approaches to audio-driven facial animation exhibit\nuncanny or static upper face animation, fail to produce accurate and plausible\nco-articulation or rely on person-specific models that limit their scalability.\nTo improve upon existing models, we propose a generic audio-driven facial\nanimation approach that achieves highly realistic motion synthesis results for\nthe entire face. At the core of our approach is a categorical latent space for\nfacial animation that disentangles audio-correlated and audio-uncorrelated\ninformation based on a novel cross-modality loss. Our approach ensures highly\naccurate lip motion, while also synthesizing plausible animation of the parts\nof the face that are uncorrelated to the audio signal, such as eye blinks and\neye brow motion. We demonstrate that our approach outperforms several baselines\nand obtains state-of-the-art quality both qualitatively and quantitatively. A\nperceptual user study demonstrates that our approach is deemed more realistic\nthan the current state-of-the-art in over 75% of cases. We recommend watching\nthe supplemental video before reading the paper:\nhttps://github.com/facebookresearch/meshtalk\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1\">Alexander Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1\">Fernando de la Torre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Algorithmic Stability in Unsupervised Representation Learning. (arXiv:2106.05238v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05238","description":"<p>In this paper, we investigate the algorithmic stability of unsupervised\nrepresentation learning with deep generative models, as a function of repeated\nre-training on the same input data. Algorithms for learning low dimensional\nlinear representations -- for example principal components analysis (PCA), or\nlinear independent components analysis (ICA) -- come with guarantees that they\nwill always reveal the same latent representations (perhaps up to an arbitrary\nrotation or permutation). Unfortunately, for non-linear representation\nlearning, such as in a variational auto-encoder (VAE) model trained by\nstochastic gradient descent, we have no such guarantees. Recent work on\nidentifiability in non-linear ICA have introduced a family of deep generative\nmodels that have identifiable latent representations, achieved by conditioning\non side information (e.g. informative labels). We empirically evaluate the\nstability of these models under repeated re-estimation of parameters, and\ncompare them to both standard VAEs and deep generative models which learn to\ncluster in their latent space. Surprisingly, we discover side information is\nnot necessary for algorithmic stability: using standard quantitative measures\nof identifiability, we find deep generative models with latent clusterings are\nempirically identifiable to the same degree as models which rely on auxiliary\nlabels. We relate these results to the possibility of identifiable non-linear\nICA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Willetts_M/0/1/0/all/0/1\">Matthew Willetts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paige_B/0/1/0/all/0/1\">Brooks Paige</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data. (arXiv:2107.06777v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06777","description":"<p>One of the most pressing problems in the automated analysis of historical\ndocuments is the availability of annotated training data. The problem is that\nlabeling samples is a time-consuming task because it requires human expertise\nand thus, cannot be automated well. In this work, we propose a novel method to\nconstruct synthetic labeled datasets for historical documents where no\nannotations are available. We train a StyleGAN model to synthesize document\nimages that capture the core features of the original documents. While\noriginally, the StyleGAN architecture was not intended to produce labels, it\nindirectly learns the underlying semantics to generate realistic images. Using\nour approach, we can extract the semantic information from the intermediate\nfeature maps and use it to generate ground truth labels. To investigate if our\nsynthetic dataset can be used to segment the text in historical documents, we\nuse it to train multiple supervised segmentation models and evaluate their\nperformance. We also train these models on another dataset created by a\nstate-of-the-art synthesis approach to show that the models trained on our\ndataset achieve better results while requiring even less human annotation\neffort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_C/0/1/0/all/0/1\">Christian Bartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raetz_H/0/1/0/all/0/1\">Hendrik Raetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otholt_J/0/1/0/all/0/1\">Jona Otholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1\">Christoph Meinel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haojin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Dependency Guided Contrastive Learning for Multiple Labeling in Prenatal Ultrasound. (arXiv:2108.05055v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05055","description":"<p>Standard plane recognition plays an important role in prenatal ultrasound\n(US) screening. Automatically recognizing the standard plane along with the\ncorresponding anatomical structures in US image can not only facilitate US\nimage interpretation but also improve diagnostic efficiency. In this study, we\nbuild a novel multi-label learning (MLL) scheme to identify multiple standard\nplanes and corresponding anatomical structures of fetus simultaneously. Our\ncontribution is three-fold. First, we represent the class correlation by word\nembeddings to capture the fine-grained semantic and latent statistical\nconcurrency. Second, we equip the MLL with a graph convolutional network to\nexplore the inner and outer relationship among categories. Third, we propose a\nnovel cluster relabel-based contrastive learning algorithm to encourage the\ndivergence among ambiguous classes. Extensive validation was performed on our\nlarge in-house dataset. Our approach reports the highest accuracy as 90.25% for\nstandard planes labeling, 85.59% for planes and structures labeling and mAP as\n94.63%. The proposed MLL scheme provides a novel perspective for standard plane\nrecognition and can be easily extended to other medical image classification\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shuangchi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_X/0/1/0/all/0/1\">Xue Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Ziwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiduo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruobing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11797","description":"<p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for large amounts of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, CPT\nenables strong few-shot and even zero-shot visual grounding capabilities of\nVL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs\noutperform their fine-tuned counterparts by a large margin (e.g., 17.3%\nabsolute accuracy improvement, and 73.8% relative standard deviation reduction\non average with one shot in RefCOCO evaluation). We make the data and code for\nthis paper publicly available at https://github.com/thunlp/CPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Track Boosting and Synthetic Data Aided Drone Detection. (arXiv:2111.12389v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12389","description":"<p>This is the paper for the first place winning solution of the Drone vs. Bird\nChallenge, organized by AVSS 2021. As the usage of drones increases with\nlowered costs and improved drone technology, drone detection emerges as a vital\nobject detection task. However, detecting distant drones under unfavorable\nconditions, namely weak contrast, long-range, low visibility, requires\neffective algorithms. Our method approaches the drone detection problem by\nfine-tuning a YOLOv5 model with real and synthetically generated data using a\nKalman-based object tracker to boost detection confidence. Our results indicate\nthat augmenting the real data with an optimal subset of synthetic data can\nincrease the performance. Moreover, temporal information gathered by object\ntracking methods can increase performance further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akyon_F/0/1/0/all/0/1\">Fatih Cagatay Akyon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eryuksel_O/0/1/0/all/0/1\">Ogulcan Eryuksel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozfuttu_K/0/1/0/all/0/1\">Kamil Anil Ozfuttu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altinuc_S/0/1/0/all/0/1\">Sinan Onur Altinuc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Federated Learning with Adaptive Batchnorm for Healthcare. (arXiv:2112.00734v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.00734","description":"<p>There is a growing interest in applying machine learning techniques to\nhealthcare. Recently, federated learning (FL) is gaining popularity since it\nallows researchers to train powerful models without compromising data privacy\nand security. However, the performance of existing FL approaches often\ndeteriorates when encountering non-iid situations where there exist\ndistribution gaps among clients, and few previous efforts focus on\npersonalization in healthcare. In this article, we propose FedAP to tackle\ndomain shifts and then obtain personalized models for local clients. FedAP\nlearns the similarity between clients based on the statistics of the batch\nnormalization layers while preserving the specificity of each client with\ndifferent local batch normalization. Comprehensive experiments on five\nhealthcare benchmarks demonstrate that FedAP achieves better accuracy compared\nto state-of-the-art methods (e.g., 10% accuracy improvement for PAMAP2) with\nfaster convergence speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1\">Dimitrios Dimitriadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalisation effects of predictive uncertainty estimation in deep learning for digital pathology. (arXiv:2112.09693v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.09693","description":"<p>Deep learning (DL) has shown great potential in digital pathology\napplications. The robustness of a diagnostic DL-based solution is essential for\nsafe clinical deployment. In this work we evaluate if adding uncertainty\nestimates for DL predictions in digital pathology could result in increased\nvalue for the clinical applications, by boosting the general predictive\nperformance or by detecting mispredictions. We compare the effectiveness of\nmodel-integrated methods (MC dropout and Deep ensembles) with a model-agnostic\napproach (Test time augmentation, TTA). Moreover, four uncertainty metrics are\ncompared. Our experiments focus on two domain shift scenarios: a shift to a\ndifferent medical center and to an underrepresented subtype of cancer. Our\nresults show that uncertainty estimates increase reliability by reducing a\nmodel's sensitivity to classification threshold selection as well as by\ndetecting between 70\\% and 90\\% of the mispredictions done by the model.\nOverall, the deep ensembles method achieved the best performance closely\nfollowed by TTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poceviciute_M/0/1/0/all/0/1\">Milda Pocevi&#x10d;i&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eilertsen_G/0/1/0/all/0/1\">Gabriel Eilertsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarkman_S/0/1/0/all/0/1\">Sofia Jarkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundstrom_C/0/1/0/all/0/1\">Claes Lundstr&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow-Guided Sparse Transformer for Video Deblurring. (arXiv:2201.01893v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.01893","description":"<p>Exploiting similar and sharper scene patches in spatio-temporal neighborhoods\nis critical for video deblurring. However, CNN-based methods show limitations\nin capturing long-range dependencies and modeling non-local self-similarity. In\nthis paper, we propose a novel framework, Flow-Guided Sparse Transformer\n(FGST), for video deblurring. In FGST, we customize a self-attention module,\nFlow-Guided Sparse Window-based Multi-head Self-Attention (FGSW-MSA). For each\n$query$ element on the blurry reference frame, FGSW-MSA enjoys the guidance of\nthe estimated optical flow to globally sample spatially sparse yet highly\nrelated $key$ elements corresponding to the same scene patch in neighboring\nframes. Besides, we present a Recurrent Embedding (RE) mechanism to transfer\ninformation from past frames and strengthen long-range temporal dependencies.\nComprehensive experiments demonstrate that our proposed FGST outperforms\nstate-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields\nmore visually pleasing results in real video deblurring. Code and pre-trained\nmodels are publicly available at https://github.com/linjing7/VR-Baseline\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jing Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yuanhao Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_X/0/1/0/all/0/1\">Xueyi Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task Specific Attention is one more thing you need for object detection. (arXiv:2202.09048v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09048","description":"<p>Various models have been proposed to solve the object detection problem.\nHowever, most of them require many hand-designed components to demonstrate good\nperformance. To mitigate these issues, Transformer based DETR and its variant\nDeformable DETR were suggested. They solved much of the complex issue of\ndesigning a head of object detection model but it has not been generally clear\nthat the Transformer-based models could be considered as the state-of-the-art\nmethod in object detection without doubt. Furthermore, as DETR adapted\nTransformer method only for the detection head, but still with including CNN\nfor the backbone body, it has not been certain that it would be possible to\nbuild the competent end-to-end pipeline with the combination of attention\nmodules. In this paper, we propose that combining several attention modules\nwith our new Task Specific Split Transformer(TSST) is a fairly good enough\nmethod to produce the best COCO results without traditionally hand-designed\ncomponents. By splitting generally purposed attention module into two separated\nmission specific attention module, the proposed method addresses the way to\ndesign simpler object detection models than before. Extensive experiments on\nthe COCO benchmark demonstrate the effectiveness of our approach. Code is\nreleased at https://github.com/navervision/tsst\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang Yon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HMD-EgoPose: Head-Mounted Display-Based Egocentric Marker-Less Tool and Hand Pose Estimation for Augmented Surgical Guidance. (arXiv:2202.11891v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11891","description":"<p>The success or failure of modern computer-assisted surgery procedures hinges\non the precise six-degree-of-freedom (6DoF) position and orientation (pose)\nestimation of tracked instruments and tissue. In this paper, we present\nHMD-EgoPose, a single-shot learning-based approach to hand and object pose\nestimation and demonstrate state-of-the-art performance on a benchmark dataset\nfor monocular red-green-blue (RGB) 6DoF marker-less hand and surgical\ninstrument pose tracking. Further, we reveal the capacity of our HMD-EgoPose\nframework for performant 6DoF pose estimation on a commercially available\noptical see-through head-mounted display (OST-HMD) through a low-latency\nstreaming approach. Our framework utilized an efficient convolutional neural\nnetwork (CNN) backbone for multi-scale feature extraction and a set of\nsubnetworks to jointly learn the 6DoF pose representation of the rigid surgical\ndrill instrument and the grasping orientation of the hand of a user. To make\nour approach accessible to a commercially available OST-HMD, the Microsoft\nHoloLens 2, we created a pipeline for low-latency video and data communication\nwith a high-performance computing workstation capable of optimized network\ninference. HMD-EgoPose outperformed current state-of-the-art approaches on a\nbenchmark dataset for surgical tool pose estimation, achieving an average tool\n3D vertex error of 11.0 mm on real data and furthering the progress towards a\nclinically viable marker-free tracking strategy. Through our low-latency\nstreaming approach, we achieved a round trip latency of 199.1 ms for pose\nestimation and augmented visualization of the tracked model when integrated\nwith the OST-HMD. Our single-shot learned approach was robust to occlusion and\ncomplex surfaces and improved on current state-of-the-art approaches to\nmarker-less tool and hand pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doughty_M/0/1/0/all/0/1\">Mitchell Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghugre_N/0/1/0/all/0/1\">Nilesh R. Ghugre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TwistSLAM: Constrained SLAM in Dynamic Environment. (arXiv:2202.12384v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.12384","description":"<p>Classical visual simultaneous localization and mapping (SLAM) algorithms\nusually assume the environment to be rigid. This assumption limits the\napplicability of those algorithms as they are unable to accurately estimate the\ncamera poses and world structure in real life scenes containing moving objects\n(e.g. cars, bikes, pedestrians, etc.). To tackle this issue, we propose\nTwistSLAM: a semantic, dynamic and stereo SLAM system that can track dynamic\nobjects in the environment. Our algorithm creates clusters of points according\nto their semantic class. Thanks to the definition of inter-cluster constraints\nmodeled by mechanical joints (function of the semantic class), a novel\nconstrained bundle adjustment is then able to jointly estimate both poses and\nvelocities of moving objects along with the classical world structure and\ncamera trajectory. We evaluate our approach on several sequences from the\npublic KITTI dataset and demonstrate quantitatively that it improves camera and\nobject tracking compared to state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mathieu Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchand_E/0/1/0/all/0/1\">Eric Marchand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacete_A/0/1/0/all/0/1\">Amine Kacete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royan_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Royan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04275","description":"<p>This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural\nNetwork (CNN) for pose estimation of noncooperative spacecraft across domain\ngap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared\nmulti-scale feature encoder and multiple prediction heads that perform\ndifferent tasks on a shared feature output. These tasks are all related to\ndetection and pose estimation of a target spacecraft from an image, such as\nprediction of pre-defined satellite keypoints, direct pose regression, and\nbinary segmentation of the satellite foreground. It is shown that by jointly\ntraining on different yet related tasks with extensive data augmentations on\nsynthetic images only, the shared encoder learns features that are common\nacross image domains that have fundamentally different visual characteristics\ncompared to synthetic images. This work also introduces Online Domain\nRefinement (ODR) which refines the parameters of the normalization layers of\nSPNv2 on the target domain images online at deployment. Specifically, ODR\nperforms self-supervised entropy minimization of the predicted satellite\nforeground, thereby improving the CNN's performance on the target domain images\nwithout their pose labels and with minimal computational efforts. The GitHub\nrepository for SPNv2 is available at \\url{https://github.com/tpark94/spnv2}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Tae Ha Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_S/0/1/0/all/0/1\">Simone D&#x27;Amico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Autofocusing using Tiny Transformer Networks for Digital Holographic Microscopy. (arXiv:2203.07772v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.07772","description":"<p>The numerical wavefront backpropagation principle of digital holography\nconfers unique extended focus capabilities, without mechanical displacements\nalong z-axis. However, the determination of the correct focusing distance is a\nnon-trivial and time consuming issue. A deep learning (DL) solution is proposed\nto cast the autofocusing as a regression problem and tested over both\nexperimental and simulated holograms. Single wavelength digital holograms were\nrecorded by a Digital Holographic Microscope (DHM) with a 10$\\mathrm{x}$\nmicroscope objective from a patterned target moving in 3D over an axial range\nof 92 $\\mu$m. Tiny DL models are proposed and compared such as a tiny Vision\nTransformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The\nproposed tiny networks are compared with their original versions (ViT/B16,\nVGG16 and Swin-Transformer Tiny) and the main neural networks used in digital\nholography such as LeNet and AlexNet. The experiments show that the predicted\nfocusing distance $Z_R^{\\mathrm{Pred}}$ is accurately inferred with an accuracy\nof 1.2 $\\mu$m in average in comparison with the DHM depth of field of 15\n$\\mu$m. Numerical simulations show that all tiny models give the\n$Z_R^{\\mathrm{Pred}}$ with an error below 0.3 $\\mu$m. Such a prospect would\nsignificantly improve the current capabilities of computer vision position\nsensing in applications such as 3D microscopy for life sciences or\nmicro-robotics. Moreover, all models reach an inference time on CPU, inferior\nto 25 ms per inference. In terms of occlusions, TViT based on its Transformer\narchitecture is the most robust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cuenat_S/0/1/0/all/0/1\">St&#xe9;phane Cuenat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andreoli_L/0/1/0/all/0/1\">Louis Andr&#xe9;oli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andre_A/0/1/0/all/0/1\">Antoine N. Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sandoz_P/0/1/0/all/0/1\">Patrick Sandoz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laurent_G/0/1/0/all/0/1\">Guillaume J. Laurent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Couturier_R/0/1/0/all/0/1\">Rapha&#xeb;l Couturier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacquot_M/0/1/0/all/0/1\">Maxime Jacquot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation. (arXiv:2203.14098v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14098","description":"<p>A fundamental and challenging problem in deep learning is catastrophic\nforgetting, i.e. the tendency of neural networks to fail to preserve the\nknowledge acquired from old tasks when learning new tasks. This problem has\nbeen widely investigated in the research community and several Incremental\nLearning (IL) approaches have been proposed in the past years. While earlier\nworks in computer vision have mostly focused on image classification and object\ndetection, more recently some IL approaches for semantic segmentation have been\nintroduced. These previous works showed that, despite its simplicity, knowledge\ndistillation can be effectively employed to alleviate catastrophic forgetting.\nIn this paper, we follow this research direction and, inspired by recent\nliterature on contrastive learning, we propose a novel distillation framework,\nUncertainty-aware Contrastive Distillation (\\method). In a nutshell, \\method~is\noperated by introducing a novel distillation loss that takes into account all\nthe images in a mini-batch, enforcing similarity between features associated to\nall the pixels from the same classes, and pulling apart those corresponding to\npixels from different classes. In order to mitigate catastrophic forgetting, we\ncontrast features of the new model with features extracted by a frozen model\nlearned at the previous incremental step. Our experimental results demonstrate\nthe advantage of the proposed distillation technique, which can be used in\nsynergy with previous IL approaches, and leads to state-of-art performance on\nthree commonly adopted benchmarks for incremental semantic segmentation. The\ncode is available at \\url{https://github.com/ygjwd12345/UCD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanglei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1\">Paolo Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingli Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. The image convolution operation\nhelps CNNs to get good performance on image-related tasks. However, the image\nconvolution has high computation complexity and hard to be implemented. This\npaper proposes the CEMNet, which can be trained in the frequency domain. The\nmost important motivation of this research is that we can use the\nstraightforward element-wise multiplication operation to replace the image\nconvolution in the frequency domain based on the Cross-Correlation Theorem,\nwhich obviously reduces the computation complexity. We further introduce a\nWeight Fixation mechanism to alleviate the problem of over-fitting, and analyze\nthe working behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval. (arXiv:2204.07441v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07441","description":"<p>Large-scale single-stream pre-training has shown dramatic performance in\nimage-text retrieval. Regrettably, it faces low inference efficiency due to\nheavy attention layers. Recently, two-stream methods like CLIP and ALIGN with\nhigh inference efficiency have also shown promising performance, however, they\nonly consider instance-level alignment between the two streams (thus there is\nstill room for improvement). To overcome these limitations, we propose a novel\nCOllaborative Two-Stream vision-language pretraining model termed COTS for\nimage-text retrieval by enhancing cross-modal interaction. In addition to\ninstance level alignment via momentum contrastive learning, we leverage two\nextra levels of cross-modal interactions in our COTS: (1) Token-level\ninteraction - a masked visionlanguage modeling (MVLM) learning objective is\ndevised without using a cross-stream network module, where variational\nautoencoder is imposed on the visual encoder to generate visual tokens for each\nimage. (2) Task-level interaction - a KL-alignment learning objective is\ndevised between text-to-image and image-to-text retrieval tasks, where the\nprobability distribution per task is computed with the negative queues in\nmomentum contrastive learning. Under a fair comparison setting, our COTS\nachieves the highest performance among all two-stream methods and comparable\nperformance (but with 10,800X faster in inference) w.r.t. the latest\nsingle-stream methods. Importantly, our COTS is also applicable to\ntext-to-video retrieval, yielding new state-ofthe-art on the widely-used\nMSR-VTT dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_N/0/1/0/all/0/1\">Nanyi Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation. (arXiv:2205.01271v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01271","description":"<p>Pose estimation plays a critical role in human-centered vision applications.\nHowever, it is difficult to deploy state-of-the-art HRNet-based pose estimation\nmodels on resource-constrained edge devices due to the high computational cost\n(more than 150 GMACs per frame). In this paper, we study efficient architecture\ndesign for real-time multi-person pose estimation on edge. We reveal that\nHRNet's high-resolution branches are redundant for models at the\nlow-computation region via our gradual shrinking experiments. Removing them\nimproves both efficiency and performance. Inspired by this finding, we design\nLitePose, an efficient single-branch architecture for pose estimation, and\nintroduce two simple approaches to enhance the capacity of LitePose, including\nFusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the\nredundancy in high-resolution branches, allowing scale-aware feature fusion\nwith low overhead. Large Kernel Convs significantly improve the model's\ncapacity and receptive field while maintaining a low computational cost. With\nonly 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3\nkernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the\nlatency by up to 5.0x without sacrificing performance, compared with prior\nstate-of-the-art efficient pose estimation models, pushing the frontier of\nreal-time multi-person pose estimation on edge. Our code and pre-trained models\nare released at https://github.com/mit-han-lab/litepose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Han Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPortraitDrawing: Generating Human Body Images from Freehand Sketches. (arXiv:2205.02070v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02070","description":"<p>Researchers have explored various ways to generate realistic images from\nfreehand sketches, e.g., for objects and human faces. However, how to generate\nrealistic human body images from sketches is still a challenging problem. It\nis, first because of the sensitivity to human shapes, second because of the\ncomplexity of human images caused by body shape and pose changes, and third\nbecause of the domain gap between realistic images and freehand sketches. In\nthis work, we present DeepPortraitDrawing, a deep generative framework for\nconverting roughly drawn sketches to realistic human body images. To encode\ncomplicated body shapes under various poses, we take a local-to-global\napproach. Locally, we employ semantic part auto-encoders to construct\npart-level shape spaces, which are useful for refining the geometry of an input\npre-segmented hand-drawn sketch. Globally, we employ a cascaded spatial\ntransformer network to refine the structure of body parts by adjusting their\nspatial locations and relative proportions. Finally, we use a global synthesis\nnetwork for the sketch-to-image translation task, and a face refinement network\nto enhance facial details. Extensive experiments have shown that given roughly\nsketched human portraits, our method produces more realistic images than the\nstate-of-the-art sketch-to-image synthesis techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Song-Hai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shi-Min Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2 segmentation models. (arXiv:2205.02152v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.02152","description":"<p>Recent studies indicate that detecting radiographic patterns on CT scans can\nyield high sensitivity and specificity for Covid-19 localization. In this\npaper, we investigate the appropriateness of deep learning models\ntransferability, for semantic segmentation of pneumonia-infected areas in CT\nimages. Transfer learning allows for the fast initialization/reutilization of\ndetection models, given that large volumes of training data are not available.\nOur work explores the efficacy of using pre-trained U-Net architectures, on a\nspecific CT data set, for identifying Covid-19 side-effects over images from\ndifferent datasets. Experimental results indicate improvement in the\nsegmentation accuracy of identifying Covid-19 infected regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maganaris_C/0/1/0/all/0/1\">Constantine Maganaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakalos_N/0/1/0/all/0/1\">Nikolaos Bakalos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalogeras_D/0/1/0/all/0/1\">Dimitris Kalogeras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angeli_A/0/1/0/all/0/1\">Aikaterini Angeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise-Tolerant Learning for Audio-Visual Action Recognition. (arXiv:2205.07611v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.07611","description":"<p>Recently, video recognition is emerging with the help of multi-modal\nlearning, which focuses on integrating multiple modalities to improve the\nperformance or robustness of a model. Although various multi-modal learning\nmethods have been proposed and offer remarkable recognition results, almost all\nof these methods rely on high-quality manual annotations and assume that\nmodalities among multi-modal data provide relevant semantic information.\nUnfortunately, most widely used video datasets are collected from the Internet\nand inevitably contain noisy labels and noisy correspondence. To solve this\nproblem, we use the audio-visual action recognition task as a proxy and propose\na noise-tolerant learning framework to find anti-interference model parameters\nto both noisy labels and noisy correspondence. Our method consists of two\nphases and aims to rectify noise by the inherent correlation between\nmodalities. A noise-tolerant contrastive training phase is performed first to\nlearn robust model parameters unaffected by the noisy labels. To reduce the\ninfluence of noisy correspondence, we propose a cross-modal noise estimation\ncomponent to adjust the consistency between different modalities. Since the\nnoisy correspondence existed at the instance level, a category-level\ncontrastive loss is proposed to further alleviate the interference of noisy\ncorrespondence. Then in the hybrid supervised training phase, we calculate the\ndistance metric among features to obtain corrected labels, which are used as\ncomplementary supervision. In addition, we investigate the noisy correspondence\nin real-world datasets and conduct comprehensive experiments with synthetic and\nreal noise data. The results verify the advantageous performance of our method\ncompared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haochen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_K/0/1/0/all/0/1\">Kaiyao Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1\">Feng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning on 3D point clouds with random compressed rehearsal. (arXiv:2205.08013v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.08013","description":"<p>Contemporary deep neural networks offer state-of-the-art results when applied\nto visual reasoning, e.g., in the context of 3D point cloud data. Point clouds\nare important datatype for precise modeling of three-dimensional environments,\nbut effective processing of this type of data proves to be challenging. In the\nworld of large, heavily-parameterized network architectures and\ncontinuously-streamed data, there is an increasing need for machine learning\nmodels that can be trained on additional data. Unfortunately, currently\navailable models cannot fully leverage training on additional data without\nlosing their past knowledge. Combating this phenomenon, called catastrophic\nforgetting, is one of the main objectives of continual learning. Continual\nlearning for deep neural networks has been an active field of research,\nprimarily in 2D computer vision, natural language processing, reinforcement\nlearning, and robotics. However, in 3D computer vision, there are hardly any\ncontinual learning solutions specifically designed to take advantage of point\ncloud structure. This work proposes a novel neural network architecture capable\nof continual learning on 3D point cloud data. We utilize point cloud structure\nproperties for preserving a heavily compressed set of past data. By using\nrehearsal and reconstruction as regularization methods of the learning process,\nour approach achieves a significant decrease of catastrophic forgetting\ncompared to the existing solutions on several most popular point cloud datasets\nconsidering two continual learning settings: when a task is known beforehand,\nand in the challenging scenario of when task information is unknown to the\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamorski_M/0/1/0/all/0/1\">Maciej Zamorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stypulkowski_M/0/1/0/all/0/1\">Micha&#x142; Stypu&#x142;kowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanowski_K/0/1/0/all/0/1\">Konrad Karanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1\">Maciej Zi&#x119;ba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers. (arXiv:2205.08078v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.08078","description":"<p>Vision transformers using self-attention or its proposed alternatives have\ndemonstrated promising results in many image related tasks. However, the\nunderpinning inductive bias of attention is not well understood. To address\nthis issue, this paper analyzes attention through the lens of convex duality.\nFor the non-linear dot-product self-attention, and alternative mechanisms such\nas MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent\nfinite-dimensional convex problems that are interpretable and solvable to\nglobal optimality. The convex programs lead to {\\it block nuclear-norm\nregularization} that promotes low rank in the latent feature and token\ndimensions. In particular, we show how self-attention networks implicitly\nclusters the tokens, based on their latent similarity. We conduct experiments\nfor transferring a pre-trained transformer backbone for CIFAR-100\nclassification by fine-tuning a variety of convex attention heads. The results\nindicate the merits of the bias induced by attention compared with the existing\nMLP or linear heads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahiner_A/0/1/0/all/0/1\">Arda Sahiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1\">Tolga Ergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturkler_B/0/1/0/all/0/1\">Batu Ozturkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardani_M/0/1/0/all/0/1\">Morteza Mardani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning. (arXiv:2205.09542v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09542","description":"<p>In this work, we tackle the challenging problem of arbitrary image style\ntransfer using a novel style feature representation learning method. A suitable\nstyle representation, as a key component in image stylization tasks, is\nessential to achieve satisfactory results. Existing deep neural network based\napproaches achieve reasonable results with the guidance from second-order\nstatistics such as Gram matrix of content features. However, they do not\nleverage sufficient style information, which results in artifacts such as local\ndistortions and style inconsistency. To address these issues, we propose to\nlearn style representation directly from image features instead of their\nsecond-order statistics, by analyzing the similarities and differences between\nmultiple styles and considering the style distribution. Specifically, we\npresent Contrastive Arbitrary Style Transfer (CAST), which is a new style\nrepresentation learning and style transfer method via contrastive learning. Our\nframework consists of three key components, i.e., a multi-layer style projector\nfor style code encoding, a domain enhancement module for effective learning of\nstyle distribution, and a generative network for image style transfer. We\nconduct qualitative and quantitative evaluations comprehensively to demonstrate\nthat our approach achieves significantly better results compared to those\nobtained via state-of-the-art methods. Code and models are available at\nhttps://github.com/zyxElsa/CAST_pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tong-Yee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network. (arXiv:2205.09612v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.09612","description":"<p>In this paper, we propose a Classification Confidence Network (CLCNet) that\ncan determine whether the classification model classifies input samples\ncorrectly. It can take a classification result in the form of vector in any\ndimension, and return a confidence score as output, which represents the\nprobability of an instance being classified correctly. We can utilize CLCNet in\na simple cascade structure system consisting of several SOTA (state-of-the-art)\nclassification models, and our experiments show that the system can achieve the\nfollowing advantages: 1. The system can customize the average computation\nrequirement (FLOPs) per image while inference. 2. Under the same computation\nrequirement, the performance of the system can exceed any model that has\nidentical structure with the model in the system, but different in size. In\nfact, this is a new type of ensemble modeling. Like general ensemble modeling,\nit can achieve higher performance than single classification model, yet our\nsystem requires much less computation than general ensemble modeling. We have\nuploaded our code to a github repository:\nhttps://github.com/yaoching0/CLCNet-Rethinking-of-Ensemble-Modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yao-Ching Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1\">Shi-Jinn Horng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}