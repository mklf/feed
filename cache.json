{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Conformer Based Acoustic Model for Robust Automatic Speech Recognition. (arXiv:2203.00725v1 [cs.SD])","link":"http://arxiv.org/abs/2203.00725","description":"<p>This study addresses robust automatic speech recognition (ASR) by introducing\na Conformer-based acoustic model. The proposed model builds on a\nstate-of-the-art recognition system using a bi-directional long short-term\nmemory (BLSTM) model with utterance-wise dropout and iterative speaker\nadaptation, but employs a Conformer encoder instead of the BLSTM network. The\nConformer encoder uses a convolution-augmented attention mechanism for acoustic\nmodeling. The proposed system is evaluated on the monaural ASR task of the\nCHiME-4 corpus. Coupled with utterance-wise normalization and speaker\nadaptation, our model achieves $6.25\\%$ word error rate, which outperforms the\nprevious best system by $8.4\\%$ relatively. In addition, the proposed\nConformer-based model is $18.3\\%$ smaller in model size and reduces training\ntime by $88.5\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">DeLiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots. (arXiv:2203.00732v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00732","description":"<p>Few-shot table-to-text generation is a task of composing fluent and faithful\nsentences to convey table content using limited data. Despite many efforts\nhaving been made towards generating impressive fluent sentences by fine-tuning\npowerful pre-trained language models, the faithfulness of generated content\nstill needs to be improved. To this end, this paper proposes a novel approach\nAttend, Memorize and Generate (called AMG), inspired by the text generation\nprocess of humans. In particular, AMG (1) attends over the multi-granularity of\ncontext using a novel strategy based on table slot level and traditional\ntoken-by-token level attention to exploit both the table structure and natural\nlinguistic information; (2) dynamically memorizes the table slot allocation\nstates; and (3) generates faithful sentences according to both the context and\nmemory allocation states. Comprehensive experiments with human evaluation on\nthree domains (i.e., humans, songs, and books) of the Wiki dataset show that\nour model can generate higher qualified texts when compared with several\nstate-of-the-art baselines, in both fluency and faithfulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models. (arXiv:2203.00748v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00748","description":"<p>Building huge and highly capable language models has been a trend in the past\nyears. Despite their great performance, they incur high computational cost. A\ncommon solution is to apply model compression or choose light-weight\narchitectures, which often need a separate fixed-size model for each desirable\ncomputational budget, and may lose performance in case of heavy compression.\nThis paper proposes an effective dynamic inference approach, called E-LANG,\nwhich distributes the inference between large accurate Super-models and\nlight-weight Swift models. To this end, a decision making module routes the\ninputs to Super or Swift models based on the energy characteristics of the\nrepresentations in the latent space. This method is easily adoptable and\narchitecture agnostic. As such, it can be applied to black-box pre-trained\nmodels without a need for architectural manipulations, reassembling of modules,\nor re-training. Unlike existing methods that are only applicable to\nencoder-only backbones and classification tasks, our method also works for\nencoder-decoder structures and sequence-to-sequence tasks such as translation.\nThe E-LANG performance is verified through a set of experiments with T5 and\nBERT backbones on GLUE, SuperGLUE, and WMT. In particular, we outperform T5-11B\nwith an average computations speed-up of 3.3$\\times$ on GLUE and 2.9$\\times$ on\nSuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2$\\times$ less\ncomputations. Code and demo are available in the supplementary materials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1\">Amin Banitalebi-Dehkordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperPrompt: Prompt-based Task-Conditioning of Transformers. (arXiv:2203.00759v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00759","description":"<p>Prompt-Tuning is a new paradigm for finetuning pre-trained language models in\na parameter-efficient way. Here, we explore the use of HyperNetworks to\ngenerate hyper-prompts: we propose HyperPrompt, a novel architecture for\nprompt-based task-conditioning of self-attention in Transformers. The\nhyper-prompts are end-to-end learnable via generation by a HyperNetwork.\nHyperPrompt allows the network to learn task-specific feature maps where the\nhyper-prompts serve as task global memories for the queries to attend to, at\nthe same time enabling flexible information sharing among tasks. We show that\nHyperPrompt is competitive against strong multi-task learning baselines with as\nfew as $0.14\\%$ of additional task-conditioning parameters, achieving great\nparameter and computational efficiency. Through extensive empirical\nexperiments, we demonstrate that HyperPrompt can achieve superior performances\nover strong T5 multi-task learning baselines and parameter-efficient adapter\nvariants including Prompt-Tuning and HyperFormer++ on Natural Language\nUnderstanding benchmarks of GLUE and SuperGLUE across many model sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huaixiu Steven Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aribandi_V/0/1/0/all/0/1\">Vamsi Aribandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">YaGuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Heng-Tze Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed H. Chi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Analysis for Text with Side Data. (arXiv:2203.00762v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00762","description":"<p>Although latent factor models (e.g., matrix factorization) obtain good\nperformance in predictions, they suffer from several problems including\ncold-start, non-transparency, and suboptimal recommendations. In this paper, we\nemploy text with side data to tackle these limitations. We introduce a hybrid\ngenerative probabilistic model that combines a neural network with a latent\ntopic model, which is a four-level hierarchical Bayesian model. In the model,\neach document is modeled as a finite mixture over an underlying set of topics\nand each topic is modeled as an infinite mixture over an underlying set of\ntopic probabilities. Furthermore, each topic probability is modeled as a finite\nmixture over side data. In the context of text, the neural network provides an\noverview distribution about side data for the corresponding text, which is the\nprior distribution in LDA to help perform topic grouping. The approach is\nevaluated on several different datasets, where the model is shown to outperform\nstandard LDA and Dirichlet-multinomial regression (DMR) in terms of topic\ngrouping, model perplexity, classification and comment generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Biyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajshekhar_K/0/1/0/all/0/1\">Kripa Rajshekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1\">Diego Klabjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Sentence Knowledge Selection in Open-Domain Dialogue. (arXiv:2203.00763v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00763","description":"<p>Incorporating external knowledge sources effectively in conversations is a\nlongstanding problem in open-domain dialogue research. The existing literature\non open-domain knowledge selection is limited and makes certain brittle\nassumptions on knowledge sources to simplify the overall task (Dinan et al.,\n2019), such as the existence of a single relevant knowledge sentence per\ncontext. In this work, we evaluate the existing state of open-domain\nconversation knowledge selection, showing where the existing methodologies\nregarding data and evaluation are flawed. We then improve on them by proposing\na new framework for collecting relevant knowledge, and create an augmented\ndataset based on the Wizard of Wikipedia (WOW) corpus, which we call WOW++.\nWOW++ averages 8 relevant knowledge sentences per dialogue context, embracing\nthe inherent ambiguity of open-domain dialogue knowledge selection. We then\nbenchmark various knowledge ranking algorithms on this augmented dataset with\nboth intrinsic evaluation and extrinsic measures of response quality, showing\nthat neural rerankers that use WOW++ can outperform rankers trained on standard\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eric_M/0/1/0/all/0/1\">Mihail Eric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chartier_N/0/1/0/all/0/1\">Nicole Chartier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_P/0/1/0/all/0/1\">Pankaj Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Level Supervised Contrastive Learning for Response Selection in Multi-Turn Dialogue. (arXiv:2203.00793v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00793","description":"<p>Selecting an appropriate response from many candidates given the utterances\nin a multi-turn dialogue is the key problem for a retrieval-based dialogue\nsystem. Existing work formalizes the task as matching between the utterances\nand a candidate and uses the cross-entropy loss in learning of the model. This\npaper applies contrastive learning to the problem by using the supervised\ncontrastive loss. In this way, the learned representations of positive examples\nand representations of negative examples can be more distantly separated in the\nembedding space, and the performance of matching can be enhanced. We further\ndevelop a new method for supervised contrastive learning, referred to as\ntwo-level supervised contrastive learning, and employ the method in response\nselection in multi-turn dialogue. Our method exploits two techniques: sentence\ntoken shuffling (STS) and sentence re-ordering (SR) for supervised contrastive\nlearning. Experimental results on three benchmark datasets demonstrate that the\nproposed method significantly outperforms the contrastive learning baseline and\nthe state-of-the-art methods for the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoran Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSAM: A Two-Stream Attention Model for Causal Emotion Entailment. (arXiv:2203.00819v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00819","description":"<p>Causal Emotion Entailment (CEE) aims to discover the potential causes behind\nan emotion in a conversational utterance. Previous works formalize CEE as\nindependent utterance pair classification problems, with emotion and speaker\ninformation neglected. From a new perspective, this paper considers CEE in a\njoint framework. We classify multiple utterances synchronously to capture the\ncorrelations between utterances in a global view and propose a Two-Stream\nAttention Model (TSAM) to effectively model the speaker's emotional influences\nin the conversational history. Specifically, the TSAM comprises three modules:\nEmotion Attention Network (EAN), Speaker Attention Network (SAN), and\ninteraction module. The EAN and SAN incorporate emotion and speaker information\nin parallel, and the subsequent interaction module effectively interchanges\nrelevant information between the EAN and SAN via a mutual BiAffine\ntransformation. Experimental results on a benchmark dataset demonstrate that\nour model achieves new State-Of-The-Art (SOTA) performance and outperforms\nbaselines remarkably.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Duzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Contextual Spelling Correction for Customization of End-to-end Speech Recognition Systems. (arXiv:2203.00888v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00888","description":"<p>Contextual biasing is an important and challenging task for end-to-end\nautomatic speech recognition (ASR) systems, which aims to achieve better\nrecognition performance by biasing the ASR system to particular context phrases\nsuch as person names, music list, proper nouns, etc. Existing methods mainly\ninclude contextual LM biasing and adding bias encoder into end-to-end ASR\nmodels. In this work, we introduce a novel approach to do contextual biasing by\nadding a contextual spelling correction model on top of the end-to-end ASR\nsystem. We incorporate contextual information into a sequence-to-sequence\nspelling correction model with a shared context encoder. Our proposed model\nincludes two different mechanisms: autoregressive (AR) and non-autoregressive\n(NAR). We propose filtering algorithms to handle large-size context lists, and\nperformance balancing mechanisms to control the biasing degree of the model. We\ndemonstrate the proposed model is a general biasing solution which is\ndomain-insensitive and can be adopted in different scenarios. Experiments show\nthat the proposed method achieves as much as 51% relative word error rate (WER)\nreduction over ASR system and outperforms traditional biasing methods. Compared\nto the AR solution, the proposed NAR model reduces model size by 43.2% and\nspeeds up inference by 2.1 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miljanic_V/0/1/0/all/0/1\">Veljko Miljanic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_H/0/1/0/all/0/1\">Hosam Khalil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Prompts Solve NLP Tasks Using Natural Language?. (arXiv:2203.00902v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00902","description":"<p>Thanks to the advanced improvement of large pre-trained language models,\nprompt-based fine-tuning is shown to be effective on a variety of downstream\ntasks. Though many prompting methods have been investigated, it remains unknown\nwhich type of prompts are the most effective among three types of prompts\n(i.e., human-designed prompts, schema prompts and null prompts). In this work,\nwe empirically compare the three types of prompts under both few-shot and\nfully-supervised settings. Our experimental results show that schema prompts\nare the most effective in general. Besides, the performance gaps tend to\ndiminish when the scale of training data grows large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for Chinese Spell Checking. (arXiv:2203.00991v1 [cs.CL])","link":"http://arxiv.org/abs/2203.00991","description":"<p>Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling\nerrors, which are mainly caused by the phonological or visual similarity.\nRecently, pre-trained language models (PLMs) promote the progress of CSC task.\nHowever, there exists a gap between the learned knowledge of PLMs and the goal\nof CSC task. PLMs focus on the semantics in text and tend to correct the\nerroneous characters to semantically proper or commonly used ones, but these\naren't the ground-truth corrections. To address this issue, we propose an\nError-driven COntrastive Probability Optimization (ECOPO) framework for CSC\ntask. ECOPO refines the knowledge representations of PLMs, and guides the model\nto avoid predicting these common characters through an error-driven way.\nParticularly, ECOPO is model-agnostic and it can be combined with existing CSC\nmethods to achieve better performance. Extensive experiments and detailed\nanalyses on SIGHAN datasets demonstrate that ECOPO is simple yet effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rongyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zizhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs. (arXiv:2203.01044v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01044","description":"<p>Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing Web-scale\nKGs. Over the course of its development, the label supervision has been\nconsidered necessary for accurate alignments. Inspired by the recent progress\nof self-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Commonly, the label information (positive\nentity pairs) is used to supervise the process of pulling the aligned entities\nin each positive pair closer. However, our theoretical analysis suggests that\nthe learning of entity alignment can actually benefit more from pushing\nunlabeled negative pairs far away from each other than pulling labeled positive\npairs close. By leveraging this discovery, we develop the self-supervised\nlearning objective for entity alignment. We present SelfKG with efficient\nstrategies to optimize this objective for aligning entities without label\nsupervision. Extensive experiments on benchmark datasets demonstrate that\nSelfKG without supervision can match or achieve comparable results with\nstate-of-the-art supervised baselines. The performance of SelfKG suggests that\nself-supervised learning offers great potential for entity alignment in KGs.\nThe code and data are available at https://github.com/THUDM/SelfKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges. (arXiv:2203.01054v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01054","description":"<p>As an important fine-grained sentiment analysis problem, aspect-based\nsentiment analysis (ABSA), aiming to analyze and understand people's opinions\nat the aspect level, has been attracting considerable interest in the last\ndecade. To handle ABSA in different scenarios, various tasks have been\nintroduced for analyzing different sentiment elements and their relations,\nincluding the aspect term, aspect category, opinion term, and sentiment\npolarity. Unlike early ABSA works focusing on a single sentiment element, many\ncompound ABSA tasks involving multiple elements have been studied in recent\nyears for capturing more complete aspect-level sentiment information. However,\na systematic review of various ABSA tasks and their corresponding solutions is\nstill lacking, which we aim to fill in this survey. More specifically, we\nprovide a new taxonomy for ABSA which organizes existing studies from the axes\nof concerned sentiment elements, with an emphasis on recent advances of\ncompound ABSA tasks. From the perspective of solutions, we summarize the\nutilization of pre-trained language models for ABSA, which improved the\nperformance of ABSA to a new stage. Besides, techniques for building more\npractical ABSA systems in cross-domain/lingual scenarios are discussed.\nFinally, we review some emerging topics and discuss some open challenges to\noutlook potential future directions of ABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discontinuous Constituency and BERT: A Case Study of Dutch. (arXiv:2203.01063v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01063","description":"<p>In this paper, we set out to quantify the syntactic capacity of BERT in the\nevaluation regime of non-context free patterns, as occurring in Dutch. We\ndevise a test suite based on a mildly context-sensitive formalism, from which\nwe derive grammars that capture the linguistic phenomena of control verb\nnesting and verb raising. The grammars, paired with a small lexicon, provide us\nwith a large collection of naturalistic utterances, annotated with verb-subject\npairings, that serve as the evaluation test bed for an attention-based span\nselection probe. Our results, backed by extensive analysis, suggest that the\nmodels investigated fail in the implicit acquisition of the dependencies\nexamined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winholds_G/0/1/0/all/0/1\">Gijs Winholds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models. (arXiv:2203.01104v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01104","description":"<p>The state-of-the-art Mixture-of-Experts (short as MoE) architecture has\nachieved several remarkable successes in terms of increasing model capacity.\nHowever, MoE has been hindered widespread adoption due to complexity,\ncommunication costs, and training instability. Here we present a novel MoE\narchitecture based on matrix product operators (MPO) from quantum many-body\nphysics. It can decompose an original matrix into central tensors (containing\nthe core information) and auxiliary tensors (with only a small proportion of\nparameters). With the decomposed MPO structure, we can reduce the parameters of\nthe original MoE architecture by sharing a global central tensor across experts\nand keeping expert-specific auxiliary tensors. We also design the gradient mask\nstrategy for the tensor structure of MPO to alleviate the overfitting problem.\nExperiments on the three well-known downstream natural language datasets based\non GPT2 show improved performance and efficiency in increasing model capacity\n(7.26x fewer parameters with the same amount of experts). We additionally\ndemonstrate an improvement in the positive transfer effects of our approach for\nmulti-task learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Ze-Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhong-Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Hate Speech Detection with Cross-Domain Transfer. (arXiv:2203.01111v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01111","description":"<p>The performance of hate speech detection models relies on the datasets on\nwhich the models are trained. Existing datasets are mostly prepared with a\nlimited number of instances or hate domains that define hate topics. This\nhinders large-scale analysis and transfer learning with respect to hate\ndomains. In this study, we construct large-scale tweet datasets for hate speech\ndetection in English and a low-resource language, Turkish, consisting of\nhuman-labeled 100k tweets per each. Our datasets are designed to have equal\nnumber of tweets distributed over five domains. The experimental results\nsupported by statistical tests show that Transformer-based language models\noutperform conventional bag-of-words and neural models by at least 5% in\nEnglish and 10% in Turkish for large-scale hate speech detection. The\nperformance is also scalable to different training sizes, such that 98% of\nperformance in English, and 97% in Turkish, are recovered when 20% of training\ninstances are used. We further examine the generalization ability of\ncross-domain transfer among hate domains. We show that 96% of the performance\nof a target domain in average is recovered by other domains for English, and\n92% for Turkish. Gender and religion are more successful to generalize to other\ndomains, while sports fail most.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahinuc_F/0/1/0/all/0/1\">Furkan &#x15e;ahinu&#xe7;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Eyup Halit Y&#x131;lmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mukayese: Turkish NLP Strikes Back. (arXiv:2203.01215v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01215","description":"<p>Having sufficient resources for language X lifts it from the under-resourced\nlanguages class, but not necessarily from the under-researched class. In this\npaper, we address the problem of the absence of organized benchmarks in the\nTurkish language. We demonstrate that languages such as Turkish are left behind\nthe state-of-the-art in NLP applications. As a solution, we present Mukayese, a\nset of NLP benchmarks for the Turkish language that contains several NLP tasks.\nWe work on one or more datasets for each benchmark and present two or more\nbaselines. Moreover, we present four new benchmarking datasets in Turkish for\nlanguage modeling, sentence segmentation, and spell checking. All datasets and\nbaselines are available under: https://github.com/alisafaya/mukayese\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safaya_A/0/1/0/all/0/1\">Ali Safaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtulus_E/0/1/0/all/0/1\">Emirhan Kurtulu&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goktogan_A/0/1/0/all/0/1\">Arda G&#xf6;kto&#x11f;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuret_D/0/1/0/all/0/1\">Deniz Yuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\texttt{py-irt}$: A Scalable Item Response Theory Library for Python. (arXiv:2203.01282v1 [cs.CL])","link":"http://arxiv.org/abs/2203.01282","description":"<p>$\\texttt{py-irt}$ is a Python library for fitting Bayesian Item Response\nTheory (IRT) models. $\\texttt{py-irt}$ estimates latent traits of subjects and\nitems, making it appropriate for use in IRT tasks as well as ideal-point\nmodels. $\\texttt{py-irt}$ is built on top of the Pyro and PyTorch frameworks\nand uses GPU-accelerated training to scale to large data sets. Code,\ndocumentation, and examples can be found at https://github.com/nd-ball/py-irt.\n$\\texttt{py-irt}$ can be installed from the GitHub page or the Python Package\nIndex (PyPI).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lalor_J/0/1/0/all/0/1\">John P. Lalor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pedro Rodriguez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering. (arXiv:2203.01294v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01294","description":"<p>Teachers often conduct surveys in order to collect data from a predefined\ngroup of students to gain insights into topics of interest. When analyzing\nsurveys with open-ended textual responses, it is extremely time-consuming,\nlabor-intensive, and difficult to manually process all the responses into an\ninsightful and comprehensive report. In the analysis step, traditionally, the\nteacher has to read each of the responses and decide on how to group them in\norder to extract insightful information. Even though it is possible to group\nthe responses only using certain keywords, such an approach would be limited\nsince it not only fails to account for embedded contexts but also cannot detect\npolysemous words or phrases and semantics that are not expressible in single\nwords. In this work, we present a novel end-to-end context-aware framework that\nextracts, aggregates, and abbreviates embedded semantic patterns in\nopen-response survey data. Our framework relies on a pre-trained natural\nlanguage model in order to encode the textual data into semantic vectors. The\nencoded vectors then get clustered either into an optimally tuned number of\ngroups or into a set of groups with pre-specified titles. In the former case,\nthe clusters are then further analyzed to extract a representative set of\nkeywords or summary sentences that serve as the labels of the clusters. In our\nframework, for the designated clusters, we finally provide context-aware\nwordclouds that demonstrate the semantically prominent keywords within each\ngroup. Honoring user privacy, we have successfully built the on-device\nimplementation of our framework suitable for real-time analysis on mobile\ndevices and have tested it on a synthetic dataset. Our framework reduces the\ncosts at-scale by automating the process of extracting the most insightful\ninformation pieces from survey data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilzadeh_S/0/1/0/all/0/1\">Soheil Esmaeilzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Brian Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsi_D/0/1/0/all/0/1\">Davood Shamsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vikingstad_O/0/1/0/all/0/1\">Onar Vikingstad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01311","description":"<p>Learning multimodal representations involves discovering correspondences and\nintegrating information from multiple heterogeneous sources of data. While\nrecent research has begun to explore the design of more general-purpose\nmultimodal models (contrary to prior focus on domain and modality-specific\narchitectures), these methods are still largely focused on a small set of\nmodalities in the language, vision, and audio space. In order to accelerate\ngeneralization towards diverse and understudied modalities, we investigate\nmethods for high-modality (a large set of diverse modalities) and\npartially-observable (each task only defined on a small subset of modalities)\nscenarios. To tackle these challenges, we design a general multimodal model\nthat enables multitask and transfer learning: multitask learning with shared\nparameters enables stable parameter counts (addressing scalability), and\ncross-modal transfer learning enables information sharing across modalities and\ntasks (addressing partial observability). Our resulting model generalizes\nacross text, image, video, audio, time-series, sensors, tables, and set\nmodalities from different research areas, improves the tradeoff between\nperformance and efficiency, transfers to new modalities and tasks, and reveals\nsurprising insights on the nature of information sharing in multitask models.\nWe release our code and benchmarks which we hope will present a unified\nplatform for subsequent theoretical and empirical analysis:\nhttps://github.com/pliang279/HighMMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shengtong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Institutional Grammar 2.0 Codebook. (arXiv:2008.08937v4 [cs.MA] UPDATED)","link":"http://arxiv.org/abs/2008.08937","description":"<p>The Grammar of Institutions, or Institutional Grammar, is an established\napproach to encode policy information in terms of institutional statements\nbased on a set of pre-defined syntactic components. This codebook provides\ncoding guidelines for a revised version of the Institutional Grammar, the\nInstitutional Grammar 2.0 (IG 2.0). IG 2.0 is a specification that aims at\nfacilitating the encoding of policy to meet varying analytical objectives. To\nthis end, it revises the grammar with respect to comprehensiveness,\nflexibility, and specificity by offering multiple levels of expressiveness (IG\nCore, IG Extended, IG Logico). In addition to the encoding of regulative\nstatements, it further introduces the encoding of constitutive institutional\nstatements, as well as statements that exhibit both constitutive and regulative\ncharacteristics. Introducing those aspects, the codebook initially covers\nfundamental concepts of IG 2.0, before providing an overview of pre-coding\nsteps relevant for document preparation. Detailed coding guidelines are\nprovided for both regulative and constitutive statements across all levels of\nexpressiveness, along with the encoding guidelines for statements of mixed form\n-- hybrid and polymorphic institutional statements. The document further\nprovides an overview of taxonomies used in the encoding process and referred to\nthroughout the codebook. The codebook concludes with a summary and discussion\nof relevant considerations to facilitate the coding process. An initial\nReader's Guide helps the reader tailor the content to her interest.\n</p>\n<p>Note that this codebook specifically focuses on operational aspects of IG 2.0\nin the context of policy coding. Links to additional resources such as the\nunderlying scientific literature (that offers a comprehensive treatment of the\nunderlying theoretical concepts) are referred to in the DOI and the concluding\nsection of the codebook.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frantz_C/0/1/0/all/0/1\">Christopher K. Frantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiki_S/0/1/0/all/0/1\">Saba N. Siddiki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing deep neural networks with morphological information. (arXiv:2011.12432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.12432","description":"<p>Deep learning approaches are superior in NLP due to their ability to extract\ninformative features and patterns from languages. The two most successful\nneural architectures are LSTM and transformers, used in large pretrained\nlanguage models such as BERT. While cross-lingual approaches are on the rise,\nmost current NLP techniques are designed and applied to English, and\nless-resourced languages are lagging behind. In morphologically rich languages,\ninformation is conveyed through morphology, e.g., through affixes modifying\nstems of words. Existing neural approaches do not explicitly use the\ninformation on word morphology. We analyse the effect of adding morphological\nfeatures to LSTM and BERT models. As a testbed, we use three tasks available in\nmany less-resourced languages: named entity recognition (NER), dependency\nparsing (DP), and comment filtering (CF). We construct baselines involving LSTM\nand BERT models, which we adjust by adding additional input in the form of part\nof speech (POS) tags and universal features. We compare models across several\nlanguages from different language families. Our results suggest that adding\nmorphological features has mixed effects depending on the quality of features\nand the task. The features improve the performance of LSTM-based models on the\nNER and DP tasks, while they do not benefit the performance on the CF task. For\nBERT-based models, the morphological features only improve the performance on\nDP when they are of high quality while not showing practical improvement when\nthey are predicted. Even for high-quality features, the improvements are less\npronounced in language-specific BERT variants compared to massively\nmultilingual BERT models. As in NER and CF datasets manually checked features\nare not available, we only experiment with predicted features and find that\nthey do not cause any practical improvement in performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klemen_M/0/1/0/all/0/1\">Matej Klemen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krsnik_L/0/1/0/all/0/1\">Luka Krsnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariance, encodings, and generalization: learning identity effects with neural networks. (arXiv:2101.08386v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.08386","description":"<p>Often in language and other areas of cognition, whether two components of an\nobject are identical or not determines if it is well formed. We call such\nconstraints identity effects. When developing a system to learn well-formedness\nfrom examples, it is easy enough to build in an identify effect. But can\nidentity effects be learned from the data without explicit guidance? We provide\na framework in which we can rigorously prove that algorithms satisfying simple\ncriteria cannot make the correct inference. We then show that a broad class of\nlearning algorithms including deep feedforward neural networks trained via\ngradient-based algorithms (such as stochastic gradient descent or the Adam\nmethod) satisfy our criteria, dependent on the encoding of inputs. In some\nbroader circumstances we are able to provide adversarial examples that the\nnetwork necessarily classifies incorrectly. Finally, we demonstrate our theory\nwith computational experiments in which we explore the effect of different\ninput encodings on the ability of algorithms to generalize to novel inputs.\nThis allows us to show similar effects to those predicted by theory for more\nrealistic methods that violate some of the conditions of our theoretical\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brugiapaglia_S/0/1/0/all/0/1\">S. Brugiapaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">M. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tupper_P/0/1/0/all/0/1\">P. Tupper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Effectively Adapting Pretrained Language Models for Active Learning. (arXiv:2104.08320v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08320","description":"<p>Recent Active Learning (AL) approaches in Natural Language Processing (NLP)\nproposed using off-the-shelf pretrained language models (LMs). In this paper,\nwe argue that these LMs are not adapted effectively to the downstream task\nduring AL and we explore ways to address this issue. We suggest to first adapt\nthe pretrained LM to the target task by continuing training with all the\navailable unlabeled data and then use it for AL. We also propose a simple yet\neffective fine-tuning method to ensure that the adapted LM is properly trained\nin both low and high resource scenarios during AL. Our experiments demonstrate\nthat our approach provides substantial data efficiency improvements compared to\nthe standard fine-tuning approach, suggesting that a poor training strategy can\nbe catastrophic for AL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting intermediate convolutional layers of CNNs trained on raw speech. (arXiv:2104.09489v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2104.09489","description":"<p>This paper presents a technique to interpret and visualize intermediate\nlayers in CNNs trained on raw speech data in an unsupervised manner. We argue\nthat averaging over feature maps after ReLU activation in each convolutional\nlayer yields interpretable time-series data. By linearly interpolating\nindividual latent variables to marginal levels outside of the training range,\nwe further argue that we are able to observe a causal relationship between\nindividual latent variables that encode linguistically meaningful units and\nactivations in intermediate convolutional layers. The proposed technique allows\nacoustic analysis of intermediate layers that parallels the acoustic analysis\nof human speech data: we can extract F0, intensity, duration, formants, and\nother acoustic properties from intermediate layers in order to test where and\nhow CNNs encode various types of information. Observing the causal effect\nbetween linear interpolation and the resulting changes in intermediate layers\ncan reveal how individual variables get transformed into spikes in activation\nin intermediate layers.We train and probe internal representations on two\nmodels -- a bare WaveGAN architecture and a ciwGAN extension which forces the\nGenerator to output informative data and results in emergence of linguistically\nmeaningful representations. Interpretation and visualization is performed for\nthree basic acoustic properties of speech: periodic vibration (corresponding to\nvowels), aperiodic noise vibration (corresponding to fricatives), and silence\n(corresponding to stops). The proposal also allows testing of higher-level\nmorphophonological alternations such as reduplication (copying). In short,\nusing the proposed technique, we can analyze how linguistically meaningful\nunits in speech get encoded in different convolutional layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07306","description":"<p>A major challenge in structured prediction is to represent the\ninterdependencies within output structures. When outputs are structured as\nsequences, linear-chain conditional random fields (CRFs) are a widely used\nmodel class which can learn \\textit{local} dependencies in the output. However,\nthe CRF's Markov assumption makes it impossible for CRFs to represent\ndistributions with \\textit{nonlocal} dependencies, and standard CRFs are unable\nto respect nonlocal constraints of the data (such as global arity constraints\non output labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show\nempirically that it can be substantially better in practice. Additionally, we\ndemonstrate a practical benefit on downstream tasks by incorporating a RegCCRF\ninto a deep neural model for semantic role labeling, exceeding state-of-the-art\nresults on a standard dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1\">Sean Papay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14586","description":"<p>Recent advances in deep learning have drastically improved performance on\nmany Natural Language Understanding (NLU) tasks. However, the data used to\ntrain NLU models may contain private information such as addresses or phone\nnumbers, particularly when drawn from human subjects. It is desirable that\nunderlying models do not expose private information contained in the training\ndata. Differentially Private Stochastic Gradient Descent (DP-SGD) has been\nproposed as a mechanism to build privacy-preserving models. However, DP-SGD can\nbe prohibitively slow to train. In this work, we propose a more efficient\nDP-SGD for training using a GPU infrastructure and apply it to fine-tuning\nmodels based on LSTM and transformer architectures. We report faster training\ntimes, alongside accuracy, theoretical privacy guarantees and success of\nMembership inference attacks for our models and observe that fine-tuning with\nproposed variant of DP-SGD can yield competitive models without significant\ndegradation in training time and improvement in privacy protection. We also\nmake observations such as looser theoretical $\\epsilon, \\delta$ can translate\ninto significant practical privacy gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arava_R/0/1/0/all/0/1\">Radhika Arava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1\">Anna Rumshisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's not Rocket Science : Interpreting Figurative Language in Narratives. (arXiv:2109.00087v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00087","description":"<p>Figurative language is ubiquitous in English. Yet, the vast majority of NLP\nresearch focuses on literal language. Existing text representations by design\nrely on compositionality, while figurative language is often non-compositional.\nIn this paper, we study the interpretation of two non-compositional figurative\nlanguages (idioms and similes). We collected datasets of fictional narratives\ncontaining a figurative expression along with crowd-sourced plausible and\nimplausible continuations relying on the correct interpretation of the\nexpression. We then trained models to choose or generate the plausible\ncontinuation. Our experiments show that models based solely on pre-trained\nlanguage models perform substantially worse than humans on these tasks. We\nadditionally propose knowledge-enhanced models, adopting human strategies for\ninterpreting figurative language types : inferring meaning from the context and\nrelying on the constituent words' literal meanings. The knowledge-enhanced\nmodels improve the performance on both the discriminative and generative tasks,\nfurther bridging the gap from human performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2109.08186","description":"<p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.\nFaST-VGS is a Transformer-based model for learning the associations between raw\nspeech waveforms and visual images. The model unifies dual-encoder and\ncross-attention architectures into a single model, reaping the superior\nretrieval speed of the former along with the accuracy of the latter. FaST-VGS\nachieves state-of-the-art speech-image retrieval accuracy on benchmark\ndatasets, and its learned representations exhibit strong performance on the\nZeroSpeech 2021 phonetic and semantic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over. (arXiv:2110.03342v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.03342","description":"<p>In this paper, we formulate a novel task to synthesize speech in sync with a\nsilent pre-recorded video, denoted as automatic voice over (AVO). Unlike\ntraditional speech synthesis, AVO seeks to generate not only human-sounding\nspeech, but also perfect lip-speech synchronization. A natural solution to AVO\nis to condition the speech rendering on the temporal progression of lip\nsequence in the video. We propose a novel text-to-speech model that is\nconditioned on visual input, named VisualTTS, for accurate lip-speech\nsynchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)\ntextual-visual attention, and 2) visual fusion strategy during acoustic\ndecoding, which both contribute to forming accurate alignment between the input\ntext content and lip motion in input lip sequence. Experimental results show\nthat VisualTTS achieves accurate lip-speech synchronization and outperforms all\nbaseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Junchen Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Chinese Biomedical Language Models via Multi-Level Text Discrimination. (arXiv:2110.07244v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07244","description":"<p>Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized\nthe field of NLP, not only in the general domain but also in the biomedical\ndomain. Most prior efforts in building biomedical PLMs have resorted simply to\ndomain adaptation and focused mainly on English. In this work we introduce\neHealth, a Chinese biomedical PLM built from scratch with a new pre-training\nframework. This new framework pre-trains eHealth as a discriminator through\nboth token- and sequence-level discrimination. The former is to detect input\ntokens corrupted by a generator and recover their original identities from\nplausible candidates, while the latter is to further distinguish corruptions of\na same original sequence from those of others. As such, eHealth can learn\nlanguage semantics at both token and sequence levels. Extensive experiments on\n11 Chinese biomedical language understanding tasks of various forms verify the\neffectiveness and superiority of our approach. We release the pre-trained model\nat \\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and\nwill also release the code later.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Songtai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Benfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Descriptive Patterns and Their Application to Characterizing Classification Errors. (arXiv:2110.09599v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09599","description":"<p>State-of-the-art deep learning methods achieve human-like performance on many\ntasks, but make errors nevertheless. Characterizing these errors in easily\ninterpretable terms gives insight into whether a classifier is prone to making\nsystematic errors, but also gives a way to act and improve the classifier. We\npropose to discover those feature-value combinations (ie. patterns) that\nstrongly correlate with correct resp. erroneous predictions to obtain a global\nand interpretable description for arbitrary classifiers. We show this is an\ninstance of the more general label description problem, which we formulate in\nterms of the Minimum Description Length principle. To discover a good pattern\nset, we develop the efficient Premise algorithm. Through an extensive set of\nexperiments we show it performs very well in practice on both synthetic and\nreal-world data. Unlike existing solutions, it ably recovers ground truth\npatterns, even on highly imbalanced data over many features. Through two case\nstudies on Visual Question Answering and Named Entity Recognition, we confirm\nthat Premise gives clear and actionable insight into the systematic errors made\nby modern NLP classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1\">Jonas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1\">Jilles Vreeken</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Siamese Bi-encoder Neural Ranking Model Using Lightweight Fine-Tuning. (arXiv:2110.14943v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14943","description":"<p>A BERT-based Neural Ranking Model (NRM) can be either a crossencoder or a\nbi-encoder. Between the two, bi-encoder is highly efficient because all the\ndocuments can be pre-processed before the actual query time. In this work, we\nshow two approaches for improving the performance of BERT-based bi-encoders.\nThe first approach is to replace the full fine-tuning step with a lightweight\nfine-tuning. We examine lightweight fine-tuning methods that are adapter-based,\nprompt-based, and hybrid of the two. The second approach is to develop\nsemi-Siamese models where queries and documents are handled with a limited\namount of difference. The limited difference is realized by learning two\nlightweight fine-tuning modules, where the main language model of BERT is kept\ncommon for both query and document. We provide extensive experiment results for\nmonoBERT, TwinBERT, and ColBERT where three performance metrics are evaluated\nover Robust04, ClueWeb09b, and MS-MARCO datasets. The results confirm that both\nlightweight fine-tuning and semi-Siamese are considerably helpful for improving\nBERT-based bi-encoders. In fact, lightweight fine-tuning is helpful for\ncrossencoder, too\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1\">Euna Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaekeol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval. (arXiv:2201.05409v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2201.05409","description":"<p>Ad-hoc search calls for the selection of appropriate answers from a\nmassive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a\npromising solution, where deep learning based document representation and ANN\nsearch techniques are allied to handle this task. However, a major challenge is\nthat the ANN index can be too large to fit into memory, given the considerable\nsize of answer corpus. In this work, we tackle this problem with Bi-Granular\nDocument Representation, where the lightweight sparse embeddings are indexed\nand standby in memory for coarse-grained candidate search, and the heavyweight\ndense embeddings are hosted in disk for fine-grained post verification. For the\nbest of retrieval accuracy, a Progressive Optimization framework is designed.\nThe sparse embeddings are learned ahead for high-quality search of candidates.\nConditioned on the candidate distribution induced by the sparse embeddings, the\ndense embeddings are continuously learned to optimize the discrimination of\nground-truth from the shortlisted candidates. Besides, two techniques: the\ncontrastive quantization and the locality-centric sampling are introduced for\nthe learning of sparse and dense embeddings, which substantially contribute to\ntheir performances. Thanks to the above features, our method effectively\nhandles massive-scale EBR with strong advantages in accuracy: with up to +4.3%\nrecall gain on million-scale corpus, and up to +17.5% recall gain on\nbillion-scale corpus. Besides, Our method is applied to a major sponsored\nsearch platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and\nCTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Weihao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianjin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Denvy Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples. (arXiv:2201.05979v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05979","description":"<p>Unsupervised sentence embedding aims to obtain the most appropriate embedding\nfor a sentence to reflect its semantic. Contrastive learning has been\nattracting developing attention. For a sentence, current models utilize diverse\ndata augmentation methods to generate positive samples, while consider other\nindependent sentences as negative samples. Then they adopt InfoNCE loss to pull\nthe embeddings of positive pairs gathered, and push those of negative pairs\nscattered. Although these models have made great progress on sentence\nembedding, we argue that they may suffer from feature suppression. The models\nfail to distinguish and decouple textual similarity and semantic similarity.\nAnd they may overestimate the semantic similarity of any pairs with similar\ntextual regardless of the actual semantic difference between them. This is\nbecause positive pairs in unsupervised contrastive learning come with similar\nand even the same textual through data augmentation. To alleviate feature\nsuppression, we propose contrastive learning for unsupervised sentence\nembedding with soft negative samples (SNCSE). Soft negative samples share\nhighly similar textual but have surely and apparently different semantic with\nthe original samples. Specifically, we take the negation of original sentences\nas soft negative samples, and propose Bidirectional Margin Loss (BML) to\nintroduce them into traditional contrastive learning framework, which merely\ninvolves positive and negative samples. Our experimental results show that\nSNCSE can obtain state-of-the-art performance on semantic textual similarity\n(STS) task with average Spearman's correlation coefficient of 78.97% on\nBERTbase and 79.23% on RoBERTabase. Besides, we adopt rank-based error analysis\nmethod to detect the weakness of SNCSE for future study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yong Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection. (arXiv:2201.12806v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12806","description":"<p>Nowadays, most methods in end-to-end contextual speech recognition bias the\nrecognition process towards contextual knowledge. Since all-neural contextual\nbiasing methods rely on phrase-level contextual modeling and attention-based\nrelevance modeling, they may encounter confusion between similar\ncontext-specific phrases, which hurts predictions at the token level. In this\nwork, we focus on mitigating confusion problems with fine-grained contextual\nknowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge\nto reduce the uncertainty of token predictions. Specifically, we first apply\nphrase selection to narrow the range of phrase candidates, and then conduct\ntoken attention on the tokens in the selected phrase candidates. Moreover, we\nre-normalize the attention weights of most relevant phrases in inference to\nobtain more focused phrase-level contextual representations, and inject\nposition information to better discriminate phrases or tokens. On LibriSpeech\nand an in-house 160,000-hour dataset, we explore the proposed methods based on\na controllable all-neural biasing method, collaborative decoding (ColDec). The\nproposed methods provide at most 6.1% relative word error rate reduction on\nLibriSpeech and 16.4% relative character error rate reduction on the in-house\ndataset over ColDec.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Minglun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Linhao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhenlin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Meng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02394","description":"<p>Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakhotiya_Y/0/1/0/all/0/1\">Yash Jakhotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1\">Ashwin Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling. (arXiv:2202.03543v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.03543","description":"<p>In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge\nand SUPERB benchmark. Our submissions are based on the recently proposed\nFaST-VGS model, which is a Transformer-based model that learns to associate raw\nspeech waveforms with semantically related images, all without the use of any\ntranscriptions of the speech. Additionally, we introduce a novel extension of\nthis model, FaST-VGS+, which is learned in a multi-task fashion with a masked\nlanguage modeling objective in addition to the visual grounding objective. On\nZeroSpeech 2021, we show that our models perform competitively on the ABX task,\noutperform all other concurrent submissions on the Syntactic and Semantic\ntasks, and nearly match the best system on the Lexical task. On the SUPERB\nbenchmark, we show that our models also achieve strong performance, in some\ncases even outperforming the popular wav2vec2.0 model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Self Shuffle Language. (arXiv:2202.07988v2 [math.CO] UPDATED)","link":"http://arxiv.org/abs/2202.07988","description":"<p>The shuffle product \\(u\\shuffle v\\) of two words \\(u\\) and \\(v\\) is the set\nof all words which can be obtained by interleaving \\(u\\) and \\(v\\). Motivated\nby the paper \\emph{The Shuffle Product: New Research Directions} by Restivo\n(2015) we investigate a special case of the shuffle product. In this work we\nconsider the shuffle of a word with itself called the \\emph{self shuffle} or\n\\emph{shuffle square}, showing first that the self shuffle language and the\nshuffle of the language are in general different sets. We prove that the\nlanguage of all words arising as a self shuffle of some word is context\nsensitive but not context free. Furthermore, we show that the self shuffle \\(w\n\\shuffle w\\) uniquely determines \\(w\\).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Fleischmann_P/0/1/0/all/0/1\">Pamela Fleischmann</a>, <a href=\"http://arxiv.org/find/math/1/au:+Harju_T/0/1/0/all/0/1\">Tero Harju</a>, <a href=\"http://arxiv.org/find/math/1/au:+Haschke_L/0/1/0/all/0/1\">Lukas Haschke</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hofer_J/0/1/0/all/0/1\">Jonas H&#xf6;fer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nowotka_D/0/1/0/all/0/1\">Dirk Nowotka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Systematic Evaluation of Large Language Models of Code. (arXiv:2202.13169v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2202.13169","description":"<p>Large language models (LMs) of code have recently shown tremendous promise in\ncompleting code and synthesizing code from natural language descriptions.\nHowever, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,\n2021)) are not publicly available, leaving many questions about their model and\ndata design decisions. We aim to fill in some of these blanks through a\nsystematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,\nGPT-NeoX-20B, and CodeParrot, across various programming languages. Although\nCodex itself is not open-source, we find that existing open-source models do\nachieve close results in some programming languages, although targeted mainly\nfor natural language modeling. We further identify an important missing piece\nin the form of a large open-source model trained exclusively on a multi-lingual\ncorpus of code. We release a new model, PolyCoder, with 2.7B parameters based\non the GPT-2 architecture, which was trained on 249GB of code across 12\nprogramming languages on a single machine. In the C programming language,\nPolyCoder outperforms all models including Codex. Our trained models are\nopen-source and publicly available at https://github.com/VHellendoorn/Code-LMs,\nwhich enables future research and application in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Beauty in Songs: Neural Singing Voice Beautifier. (arXiv:2202.13277v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.13277","description":"<p>We are interested in a novel task, singing voice beautifying (SVB). Given the\nsinging voice of an amateur singer, SVB aims to improve the intonation and\nvocal tone of the voice, while keeping the content and vocal timbre. Current\nautomatic pitch correction techniques are immature, and most of them are\nrestricted to intonation but ignore the overall aesthetic quality. Hence, we\nintroduce Neural Singing Voice Beautifier (NSVB), the first generative model to\nsolve the SVB task, which adopts a conditional variational autoencoder as the\nbackbone and learns the latent representations of vocal tone. In NSVB, we\npropose a novel time-warping approach for pitch correction: Shape-Aware Dynamic\nTime Warping (SADTW), which ameliorates the robustness of existing time-warping\napproaches, to synchronize the amateur recording with the template pitch curve.\nFurthermore, we propose a latent-mapping algorithm in the latent space to\nconvert the amateur vocal tone to the professional one. To achieve this, we\nalso propose a new dataset containing parallel singing recordings of both\namateur and professional versions. Extensive experiments on both Chinese and\nEnglish songs demonstrate the effectiveness of our methods in terms of both\nobjective and subjective metrics. Audio samples are available\nat~\\url{https://neuralsvb.github.io}. Codes:\n\\url{https://github.com/MoonInTheRiver/NeuralSVB}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring and Adapting Chinese GPT to Pinyin Input Method. (arXiv:2203.00249v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00249","description":"<p>While GPT has become the de-facto method for text generation tasks, its\napplication to pinyin input method remains unexplored. In this work, we make\nthe first exploration to leverage Chinese GPT for pinyin input method. We find\nthat a frozen GPT achieves state-of-the-art performance on perfect pinyin.\nHowever, the performance drops dramatically when the input includes abbreviated\npinyin. A reason is that an abbreviated pinyin can be mapped to many perfect\npinyin, which links to even larger number of Chinese characters. We mitigate\nthis issue with two strategies, including enriching the context with pinyin and\noptimizing the training process to help distinguish homophones. To further\nfacilitate the evaluation of pinyin input method, we create a dataset\nconsisting of 270K instances from 15 domains. Results show that our approach\nimproves performance on abbreviated pinyin across all domains. Model analysis\ndemonstrates that both strategies contribute to the performance boost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Minghuan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Is Whole Word Masking Always Better for Chinese BERT?\": Probing on Chinese Grammatical Error Correction. (arXiv:2203.00286v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00286","description":"<p>Whole word masking (WWM), which masks all subwords corresponding to a word at\nonce, makes a better English BERT model. For the Chinese language, however,\nthere is no subword because each token is an atomic character. The meaning of a\nword in Chinese is different in that a word is a compositional unit consisting\nof multiple characters. Such difference motivates us to investigate whether WWM\nleads to better context understanding ability for Chinese BERT. To achieve\nthis, we introduce two probing tasks related to grammatical error correction\nand ask pretrained models to revise or insert tokens in a masked language\nmodeling manner. We construct a dataset including labels for 19,075 tokens in\n10,448 sentences. We train three Chinese BERT models with standard\ncharacter-level masking (CLM), WWM, and a combination of CLM and WWM,\nrespectively. Our major findings are as follows: First, when one character\nneeds to be inserted or replaced, the model trained with CLM performs the best.\nSecond, when more than one character needs to be handled, WWM is the key to\nbetter performance. Finally, when being fine-tuned on sentence-level downstream\ntasks, models trained with different masking strategies perform comparably.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1\">Enbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Impact of Individual Domain Factors in Self-Supervised Pre-Training. (arXiv:2203.00648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00648","description":"<p>Human speech data comprises a rich set of domain factors such as accent,\nsyntactic and semantic variety, or acoustic environment. Previous work explores\nthe effect of domain mismatch in automatic speech recognition between\npre-training and fine-tuning as a whole but does not dissect the contribution\nof individual factors. In this paper, we present a controlled study to better\nunderstand the effect of such factors on the performance of pre-trained\nrepresentations. To do so, we pre-train models either on modified natural\nspeech or synthesized audio, with a single domain factor modified, and then\nmeasure performance on automatic speech recognition after fine tuning. Results\nshow that phonetic domain factors play an important role during pre-training\nwhile grammatical and syntactic factors are far less important. To our\nknowledge, this is the first study to better understand the domain\ncharacteristics in self-supervised pre-training for speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data. (arXiv:2203.00734v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00734","description":"<p>Automatic player identification is an essential and complex task in sports\nvideo analysis. Different strategies have been devised over the years, but\nidentification based on jersey numbers is one of the most common approaches\ngiven its versatility and relative simplicity. However, automatic detection of\njersey numbers is still challenging due to changing camera angles, low video\nresolution, small object size in wide-range shots and transient changes in the\nplayer's posture and movement. In this paper we present a novel approach for\njersey number identification in a small, highly imbalanced dataset from the\nSeattle Seahawks practice videos. Our results indicate that simple models can\nachieve an acceptable performance on the jersey number detection task and that\nsynthetic data can improve the performance dramatically (accuracy increase of\n~9% overall, ~18% on low frequency numbers) making our approach achieve state\nof the art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhargavi_D/0/1/0/all/0/1\">Divya Bhargavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coyotl_E/0/1/0/all/0/1\">Erika Pelaez Coyotl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_S/0/1/0/all/0/1\">Sia Gholami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Skeleton-based Human Motion Prediction with Manifold-Aware GAN. (arXiv:2203.00736v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00736","description":"<p>In this work we propose a novel solution for 3D skeleton-based human motion\nprediction. The objective of this task consists in forecasting future human\nposes based on a prior skeleton pose sequence. This involves solving two main\nchallenges still present in recent literature; (1) discontinuity of the\npredicted motion which results in unrealistic motions and (2) performance\ndeterioration in long-term horizons resulting from error accumulation across\ntime. We tackle these issues by using a compact manifold-valued representation\nof 3D human skeleton motion. Specifically, we model the temporal evolution of\nthe 3D poses as trajectory, what allows us to map human motions to single\npoints on a sphere manifold. Using such a compact representation avoids error\naccumulation and provides robust representation for long-term prediction while\nensuring the smoothness and the coherence of the whole motion. To learn these\nnon-Euclidean representations, we build a manifold-aware Wasserstein generative\nadversarial model that captures the temporal and spatial dependencies of human\nmotion through different losses. Experiments have been conducted on CMU MoCap\nand Human 3.6M datasets and demonstrate the superiority of our approach over\nthe state-of-the-art both in short and long term horizons. The smoothness of\nthe generated motion is highlighted in the qualitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopin_B/0/1/0/all/0/1\">Baptiste Chopin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otberdout_N/0/1/0/all/0/1\">Naima Otberdout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_A/0/1/0/all/0/1\">Angela Bartolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Runtime Detection of Executional Errors in Robot-Assisted Surgery. (arXiv:2203.00737v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00737","description":"<p>Despite significant developments in the design of surgical robots and\nautomated techniques for objective evaluation of surgical skills, there are\nstill challenges in ensuring safety in robot-assisted minimally-invasive\nsurgery (RMIS). This paper presents a runtime monitoring system for the\ndetection of executional errors during surgical tasks through the analysis of\nkinematic data. The proposed system incorporates dual Siamese neural networks\nand knowledge of surgical context, including surgical tasks and gestures, their\ndistributional similarities, and common error modes, to learn the differences\nbetween normal and erroneous surgical trajectories from small training\ndatasets. We evaluate the performance of the error detection using Siamese\nnetworks compared to single CNN and LSTM networks trained with different levels\nof contextual knowledge and training data, using the dry-lab demonstrations of\nthe Suturing and Needle Passing tasks from the JIGSAWS dataset. Our results\nshow that gesture specific task nonspecific Siamese networks obtain micro F1\nscores of 0.94 (Siamese-CNN) and 0.95 (Siamese-LSTM), and perform better than\nsingle CNN (0.86) and LSTM (0.87) networks. These Siamese networks also\noutperform gesture nonspecific task specific Siamese-CNN and Siamese-LSTM\nmodels for Suturing and Needle Passing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_K/0/1/0/all/0/1\">Kay Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1\">Homa Alemzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"There is a Time and Place for Reasoning Beyond the Image. (arXiv:2203.00758v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00758","description":"<p>Images are often more significant than only the pixels to human eyes, as we\ncan infer, associate, and reason with contextual information from other sources\nto establish a more complete picture. For example, in Figure 1, we can find a\nway to identify the news articles related to the picture through segment-wise\nunderstandings on the signs, the buildings, the crowds, and more. This tells us\nthe time when and the location where the image is taken, which will help us in\nsubsequent tasks, such as evidence retrieval for criminal activities, automatic\nstoryline construction, and upper-stream processing such as image clustering.\nIn this work, we formulate this problem and introduce TARA: a dataset with 16k\nimages with their associated news, time and location automatically extracted\nfrom New York Times (NYT), and an additional 61k examples as distant\nsupervision from WIT. On top of the extractions, we present a crowdsourced\nsubset in which images are believed to be feasible to find their\nspatio-temporal information for evaluation purpose. We show that there exists a\n70% gap between a state-of-the-art joint model and human performance, which is\nslightly filled by our proposed model that uses segment-wise reasoning,\nmotivating higher-level vision-language joint models that can conduct\nopen-ended reasoning with world knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xingyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Ben Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandratreya_I/0/1/0/all/0/1\">Ishaan Preetam Chandratreya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tricks and Plugins to GBM on Images and Sequences. (arXiv:2203.00761v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00761","description":"<p>Convolutional neural networks (CNNs) and transformers, which are composed of\nmultiple processing layers and blocks to learn the representations of data with\nmultiple abstract levels, are the most successful machine learning models in\nrecent years. However, millions of parameters and many blocks make them\ndifficult to be trained, and sometimes several days or weeks are required to\nfind an ideal architecture or tune the parameters. Within this paper, we\npropose a new algorithm for boosting Deep Convolutional Neural Networks\n(BoostCNN) to combine the merits of dynamic feature selection and BoostCNN, and\nanother new family of algorithms combining boosting and transformers. To learn\nthese new models, we introduce subgrid selection and importance sampling\nstrategies and propose a set of algorithms to incorporate boosting weights into\na deep learning architecture based on a least squares objective function. These\nalgorithms not only reduce the required manual effort for finding an\nappropriate network architecture but also result in superior performance and\nlower running time. Experiments show that the proposed methods outperform\nbenchmarks on several fine-grained classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_B/0/1/0/all/0/1\">Biyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utke_J/0/1/0/all/0/1\">Jean Utke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1\">Diego Klabjan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Cost On-device Partial Domain Adaptation (LoCO-PDA): Enabling efficient CNN retraining on edge devices. (arXiv:2203.00772v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00772","description":"<p>With the increased deployment of Convolutional Neural Networks (CNNs) on edge\ndevices, the uncertainty of the observed data distribution upon deployment has\nled researchers to to utilise large and extensive datasets such as ILSVRC'12 to\ntrain CNNs. Consequently, it is likely that the observed data distribution upon\ndeployment is a subset of the training data distribution. In such cases, not\nadapting a network to the observed data distribution can cause performance\ndegradation due to negative transfer and alleviating this is the focus of\nPartial Domain Adaptation (PDA). Current works targeting PDA do not focus on\nperforming the domain adaptation on an edge device, adapting to a changing\ntarget distribution or reducing the cost of deploying the adapted network. This\nwork proposes a novel PDA methodology that targets all of these directions and\nopens avenues for on-device PDA. LoCO-PDA adapts a deployed network to the\nobserved data distribution by enabling it to be retrained on an edge device.\nAcross subsets of the ILSVRC12 dataset, LoCO-PDA improves classification\naccuracy by 3.04pp on average while achieving up to 15.1x reduction in\nretraining memory consumption and 2.07x improvement in inference latency on the\nNVIDIA Jetson TX2. The work is open-sourced at \\emph{link removed for\nanonymity}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_A/0/1/0/all/0/1\">Aditya Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouganis_C/0/1/0/all/0/1\">Christos-Savvas Bouganis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image analysis for automatic measurement of crustose lichens. (arXiv:2203.00787v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00787","description":"<p>Lichens, organisms resulting from a symbiosis between a fungus and an algae,\nare frequently used as age estimators, especially in recent geological deposits\nand archaeological structures, using the correlation between lichen size and\nage. Current non-automated manual lichen and measurement (with ruler, calipers\nor using digital image processing tools) is a time-consuming and laborious\nprocess, especially when the number of samples is high.\n</p>\n<p>This work presents a workflow and set of image acquisition and processing\ntools developed to efficiently identify lichen thalli in flat rocky surfaces,\nand to produce relevant lichen size statistics (percentage cover, number of\nthalli, their area and perimeter).\n</p>\n<p>The developed workflow uses a regular digital camera for image capture along\nwith specially designed targets to allow for automatic image correction and\nscale assignment. After this step, lichen identification is done in a flow\ncomprising assisted image segmentation and classification based on interactive\nforeground extraction tool (GrabCut) and automatic classification of images\nusing Simple Linear Iterative Clustering (SLIC) for image segmentation and\nSupport Vector Machines (SV) and Random Forest classifiers.\n</p>\n<p>Initial evaluation shows promising results. The manual classification of\nimages (for training) using GrabCut show an average speedup of 4 if compared\nwith currently used techniques and presents an average precision of 95\\%. The\nautomatic classification using SLIC and SVM with default parameters produces\nresults with average precision higher than 70\\%. The developed system is\nflexible and allows a considerable reduction of processing time, the workflow\nallows it applicability to data sets of new lichen populations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guedes_P/0/1/0/all/0/1\">Pedro Guedes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_M/0/1/0/all/0/1\">Maria Alexandra Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branquinho_C/0/1/0/all/0/1\">Cristina Branquinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jo&#xe3;o Nuno Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Physical Threat Monitoring System Aided by Virtual Building Simulation. (arXiv:2203.00789v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00789","description":"<p>With increasing physical threats in recent years targeted at critical\ninfrastructures, it is crucial to establish a reliable threat monitoring system\nintegrating video surveillance and digital sensors based on cutting-edge\ntechnologies. A physical threat monitoring solution unifying the floorplan,\ncameras, and sensors for smart buildings has been set up in our study. Computer\nvision and deep learning models are used for video streams analysis. When a\nthreat is detected by a rule engine based on the real-time analysis results\ncombining with feedback from related digital sensors, an alert is sent to the\nVideo Management System so that human operators can take further action. A\nphysical threat monitoring system typically needs to address complex and even\ndestructive incidents, such as fire, which is unrealistic to simulate in real\nlife. Restrictions imposed during the Covid-19 pandemic and privacy concerns\nhave added to the challenges. Our study utilises the Unreal Engine to simulate\nsome typical suspicious and intrusion scenes with photorealistic qualities in\nthe context of a virtual building. Add-on programs are implemented to transfer\nthe video stream from virtual PTZ cameras to the Milestone Video Management\nSystem and enable users to control those cameras from the graphic client\napplication. Virtual sensors such as fire alarms, temperature sensors and door\naccess controls are implemented similarly, fulfilling the same programmatic VMS\ninterface as real-life sensors. Thanks to this simulation system's\nextensibility and repeatability, we have consolidated this unified physical\nthreat monitoring system and verified its effectiveness and user-friendliness.\nBoth the simulated Unreal scenes and the software add-ons developed during this\nstudy are highly modulated and thereby are ready for reuse in future projects\nin this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norton_B/0/1/0/all/0/1\">Barry Norton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stable, accurate and efficient deep neural networks for inverse problems with analysis-sparse models. (arXiv:2203.00804v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00804","description":"<p>Solving inverse problems is a fundamental component of science, engineering\nand mathematics. With the advent of deep learning, deep neural networks have\nsignificant potential to outperform existing state-of-the-art, model-based\nmethods for solving inverse problems. However, it is known that current\ndata-driven approaches face several key issues, notably instabilities and\nhallucinations, with potential impact in critical tasks such as medical\nimaging. This raises the key question of whether or not one can construct\nstable and accurate deep neural networks for inverse problems. In this work, we\npresent a novel construction of an accurate, stable and efficient neural\nnetwork for inverse problems with general analysis-sparse models. To construct\nthe network, we unroll NESTA, an accelerated first-order method for convex\noptimization. Combined with a compressed sensing analysis, we prove accuracy\nand stability. Finally, a restart scheme is employed to enable exponential\ndecay of the required network depth, yielding a shallower, and consequently\nmore efficient, network. We showcase this approach in the case of Fourier\nimaging, and verify its stability and performance via a series of numerical\nexperiments. The key impact of this work is to provide theoretical guarantees\nfor computing and developing stable neural networks in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neyra_Nesterenko_M/0/1/0/all/0/1\">Maksym Neyra-Nesterenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adcock_B/0/1/0/all/0/1\">Ben Adcock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InCloud: Incremental Learning for Point Cloud Place Recognition. (arXiv:2203.00807v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00807","description":"<p>Place recognition is a fundamental component of robotics, and has seen\ntremendous improvements through the use of deep learning models in recent\nyears. Networks can experience significant drops in performance when deployed\nin unseen or highly dynamic environments, and require additional training on\nthe collected data. However naively fine-tuning on new training distributions\ncan cause severe degradation of performance on previously visited domains, a\nphenomenon known as catastrophic forgetting. In this paper we address the\nproblem of incremental learning for point cloud place recognition and introduce\nInCloud, a structure-aware distillation-based approach which preserves the\nhigher-order structure of the network's embedding space. We introduce several\nchallenging new benchmarks on four popular and large-scale LiDAR datasets\n(Oxford, MulRan, In-house and KITTI) showing broad improvements in point cloud\nplace recognition performance over a variety of network architectures. To the\nbest of our knowledge, this work is the first to effectively apply incremental\nlearning for point cloud place recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knights_J/0/1/0/all/0/1\">Joshua Knights</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1\">Peyman Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Milad Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-aware multi-object self-supervision for monocular depth prediction. (arXiv:2203.00809v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00809","description":"<p>This paper proposes a self-supervised monocular image-to-depth prediction\nframework that is trained with an end-to-end photometric loss that handles not\nonly 6-DOF camera motion but also 6-DOF moving object instances.\nSelf-supervision is performed by warping the images across a video sequence\nusing depth and scene motion including object instances. One novelty of the\nproposed method is the use of a multi-head attention of the transformer network\nthat matches moving objects across time and models their interaction and\ndynamics. This enables accurate and robust pose estimation for each object\ninstance. Most image-to-depth predication frameworks make the assumption of\nrigid scenes, which largely degrades their performance with respect to dynamic\nobjects. Only a few SOTA papers have accounted for dynamic objects. The\nproposed method is shown to largely outperform these methods on standard\nbenchmarks and the impact of the dynamic motion on these benchmarks is exposed.\nFurthermore, the proposed image-to-depth prediction framework is also shown to\noutperform SOTA video-to-depth prediction frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boulahbal_H/0/1/0/all/0/1\">Houssem eddine Boulahbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voicila_A/0/1/0/all/0/1\">Adrian Voicila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew Comport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Seatbelt Detection and Usage Recognition for Driver Monitoring Systems. (arXiv:2203.00810v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00810","description":"<p>Wearing a seatbelt appropriately while driving can reduce serious\ncrash-related injuries or deaths by about half. However, current seatbelt\nreminder system has multiple shortcomings, such as can be easily fooled by a\n\"Seatbelt Warning Stopper\", and cannot recognize incorrect usages for example\nseating in front of a buckled seatbelt or wearing a seatbelt under the arm.\nGeneral seatbelt usage recognition has many challenges, to name a few, lacking\nof color information in Infrared (IR) cameras, strong distortion caused by wide\nField of View (FoV) fisheye lens, low contrast between belt and its background,\nocclusions caused by hands or hair, and imaging blurry. In this paper, we\nintroduce a novel general seatbelt detection and usage recognition framework to\nresolve the above challenges. Our method consists of three components: a local\npredictor, a global assembler, and a shape modeling process. Our approach can\nbe applied to the driver in the Driver Monitoring System (DMS) or general\npassengers in the Occupant Monitoring System (OMS) for various camera\nmodalities. Experiment results on both DMS and OMS are provided to demonstrate\nthe accuracy and robustness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Feng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification. (arXiv:2203.00828v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00828","description":"<p>Although accurate and fast point cloud classification is a fundamental task\nin 3D applications, it is difficult to achieve this purpose due to the\nirregularity and disorder of point clouds that make it challenging to achieve\neffective and efficient global discriminative feature learning. Lately, 3D\nTransformers have been adopted to improve point cloud processing. Nevertheless,\nmassive Transformer layers tend to incur huge computational and memory costs.\nThis paper presents a novel hierarchical framework that incorporates\nconvolution with Transformer for point cloud classification, named 3D\nConvolution-Transformer Network (3DCTN), to combine the strong and efficient\nlocal feature learning ability of convolution with the remarkable global\ncontext modeling capability of Transformer. Our method has two main modules\noperating on the downsampling point sets, and each module consists of a\nmulti-scale local feature aggregating (LFA) block and a global feature learning\n(GFL) block, which are implemented by using Graph Convolution and Transformer\nrespectively. We also conduct a detailed investigation on a series of\nTransformer variants to explore better performance for our network. Various\nexperiments on ModelNet40 demonstrate that our method achieves state-of-the-art\nclassification performance, in terms of both accuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dening Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linlin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSC Loss: A Gaussian Score Calibrating Loss for Deep Learning. (arXiv:2203.00833v1 [cs.LG])","link":"http://arxiv.org/abs/2203.00833","description":"<p>Cross entropy (CE) loss integrated with softmax is an orthodox component in\nmost classification-based frameworks, but it fails to obtain an accurate\nprobability distribution of predicted scores that is critical for further\ndecision-making of poor-classified samples. The prediction score calibration\nprovides a solution to learn the distribution of predicted scores which can\nexplicitly make the model obtain a discriminative representation. Considering\nthe entropy function can be utilized to measure the uncertainty of predicted\nscores. But, the gradient variation of it is not in line with the expectations\nof model optimization. To this end, we proposed a general Gaussian Score\nCalibrating (GSC) loss to calibrate the predicted scores produced by the deep\nneural networks (DNN). Extensive experiments on over 10 benchmark datasets\ndemonstrate that the proposed GSC loss can yield consistent and significant\nperformance boosts in a variety of visual tasks. Notably, our label-independent\nGSC loss can be embedded into common improved methods based on the CE loss\neasily.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingsong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1\">Shuguang Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaopeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Cairong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion. (arXiv:2203.00838v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00838","description":"<p>A well-known challenge in applying deep-learning methods to omnidirectional\nimages is spherical distortion. In dense regression tasks such as depth\nestimation, where structural details are required, using a vanilla CNN layer on\nthe distorted 360 image results in undesired information loss. In this paper,\nwe propose a 360 monocular depth estimation pipeline, \\textit{OmniFusion}, to\ntackle the spherical distortion issue. Our pipeline transforms a 360 image into\nless-distorted perspective patches (i.e. tangent images) to obtain patch-wise\npredictions via CNN, and then merge the patch-wise results for final output. To\nhandle the discrepancy between patch-wise predictions which is a major issue\naffecting the merging quality, we propose a new framework with the following\nkey components. First, we propose a geometry-aware feature fusion mechanism\nthat combines 3D geometric features with 2D image features to compensate for\nthe patch-wise discrepancy. Second, we employ the self-attention-based\ntransformer architecture to conduct a global aggregation of patch-wise\ninformation, which further improves the consistency. Last, we introduce an\niterative depth refinement mechanism, to further refine the estimated depth\nbased on the more accurate geometric features. Experiments show that our method\ngreatly mitigates the distortion issue, and achieves state-of-the-art\nperformances on several 360 monocular depth estimation benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhixin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Ye Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. (arXiv:2203.00843v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00843","description":"<p>3D dense captioning aims to describe individual objects by natural language\nin 3D scenes, where 3D scenes are usually represented as RGB-D scans or point\nclouds. However, only exploiting single modal information, e.g., point cloud,\nprevious approaches fail to produce faithful descriptions. Though aggregating\n2D features into point clouds may be beneficial, it introduces an extra\ncomputational burden, especially in inference phases. In this study, we\ninvestigate a cross-modal knowledge transfer using Transformer for 3D dense\ncaptioning, X-Trans2Cap, to effectively boost the performance of single-modal\n3D caption through knowledge distillation using a teacher-student framework. In\npractice, during the training phase, the teacher network exploits auxiliary 2D\nmodality and guides the student network that only takes point clouds as input\nthrough the feature consistency constraints. Owing to the well-designed\ncross-modal feature fusion module and the feature alignment in the training\nphase, X-Trans2Cap acquires rich appearance information embedded in 2D images\nwith ease. Thus, a more faithful caption can be generated only using point\nclouds during the inference. Qualitative and quantitative results confirm that\nX-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e.,\nabout +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can No-reference features help in Full-reference image quality estimation?. (arXiv:2203.00845v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00845","description":"<p>Development of perceptual image quality assessment (IQA) metrics has been of\nsignificant interest to computer vision community. The aim of these metrics is\nto model quality of an image as perceived by humans. Recent works in\nFull-reference IQA research perform pixelwise comparison between deep features\ncorresponding to query and reference images for quality prediction. However,\npixelwise feature comparison may not be meaningful if distortion present in\nquery image is severe. In this context, we explore utilization of no-reference\nfeatures in Full-reference IQA task. Our model consists of both full-reference\nand no-reference branches. Full-reference branches use both distorted and\nreference images, whereas No-reference branch only uses distorted image. Our\nexperiments show that use of no-reference features boosts performance of image\nquality assessment. Our model achieves higher SRCC and KRCC scores than a\nnumber of state-of-the-art algorithms on KADID-10K and PIPAL datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dutta_S/0/1/0/all/0/1\">Saikat Dutta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sourya Dipta Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg A. Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clean-Annotation Backdoor Attack against Lane Detection Systems in the Wild. (arXiv:2203.00858v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00858","description":"<p>We present the first backdoor attack against the lane detection systems in\nthe physical world. Modern autonomous vehicles adopt various deep learning\nmethods to train lane detection models, making it challenging to devise a\nuniversal backdoor attack technique. In our solution, (1) we propose a novel\nsemantic trigger design, which leverages the traffic cones with specific poses\nand locations to activate the backdoor. Such trigger can be easily realized\nunder the physical setting, and looks very natural not to be detected. (2) We\nintroduce a new clean-annotation approach to generate poisoned samples. These\nsamples have correct annotations but are still capable of embedding the\nbackdoor to the model. Comprehensive evaluations on public datasets and\nphysical autonomous vehicles demonstrate that our backdoor attack is effective,\nstealthy and robust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xingshuo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guowen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuehuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00859","description":"<p>Recent transformer-based solutions have been introduced to estimate 3D human\npose from 2D keypoint sequence by considering body joints among all frames\nglobally to learn spatio-temporal correlation. We observe that the motions of\ndifferent joints differ significantly. However, the previous methods cannot\nefficiently model the solid inter-frame correspondence of each joint, leading\nto insufficient learning of spatial-temporal correlation. We propose MixSTE\n(Mixed Spatio-Temporal Encoder), which has a temporal transformer block to\nseparately model the temporal motion of each joint and a spatial transformer\nblock to learn inter-joint spatial correlation. These two blocks are utilized\nalternately to obtain better spatio-temporal feature encoding. In addition, the\nnetwork output is extended from the central frame to entire frames of the input\nvideo, thereby improving the coherence between the input and output sequences.\nExtensive experiments are conducted on three benchmarks (i.e. Human3.6M,\nMPI-INF-3DHP, and HumanEva) to evaluate the proposed method. The results show\nthat our model outperforms the state-of-the-art approach by 10.9% P-MPJPE and\n7.6% MPJPE on the Human3.6M dataset. Code is available in our supplementary\nmaterials.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhigang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yujin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale Attention. (arXiv:2203.00860v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00860","description":"<p>DETR is the first fully end-to-end detector that predicts a final set of\npredictions without post-processing. However, it suffers from problems such as\nlow performance and slow convergence. A series of works aim to tackle these\nissues in different ways, but the computational cost is yet expensive due to\nthe sophisticated encoder-decoder architecture. To alleviate this issue, we\npropose a decoder-only detector called D^2ETR. In the absence of encoder, the\ndecoder directly attends to the fine-fused feature maps generated by the\nTransformer backbone with a novel computationally efficient cross-scale\nattention module. D^2ETR demonstrates low computational complexity and high\ndetection accuracy in evaluations on the COCO benchmark, outperforming DETR and\nits variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaofeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Styleverse: Towards Identity Stylization across Heterogeneous Domains. (arXiv:2203.00861v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00861","description":"<p>We propose a new challenging task namely IDentity Stylization (IDS) across\nheterogeneous domains. IDS focuses on stylizing the content identity, rather\nthan completely swapping it using the reference identity. We use an effective\nheterogeneous-network-based framework $Styleverse$ that uses a single\ndomain-aware generator to exploit the Metaverse of diverse heterogeneous faces,\nbased on the proposed dataset FS13 with limited data. FS13 means 13 kinds of\nFace Styles considering diverse lighting conditions, art representations and\nlife dimensions. Previous similar tasks, \\eg, image style transfer can handle\ntextural style transfer based on a reference image. This task usually ignores\nthe high structure-aware facial area and high-fidelity preservation of the\ncontent. However, Styleverse intends to controllably create topology-aware\nfaces in the Parallel Style Universe, where the source facial identity is\nadaptively styled via AdaIN guided by the domain-aware and reference-aware\nstyle embeddings from heterogeneous pretrained models. We first establish the\nIDS quantitative benchmark as well as the qualitative Styleverse matrix.\nExtensive experiments demonstrate that Styleverse achieves higher-fidelity\nidentity stylization compared with other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">JunXian Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEA: Bridging the Gap Between One- and Two-stage Detector Distillation via SEmantic-aware Alignment. (arXiv:2203.00862v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00862","description":"<p>We revisit the one- and two-stage detector distillation tasks and present a\nsimple and efficient semantic-aware framework to fill the gap between them. We\naddress the pixel-level imbalance problem by designing the category anchor to\nproduce a representative pattern for each category and regularize the\ntopological distance between pixels and category anchors to further tighten\ntheir semantic bonds. We name our method SEA (SEmantic-aware Alignment)\ndistillation given the nature of abstracting dense fine-grained information by\nsemantic reliance to well facilitate distillation efficacy. SEA is well adapted\nto either detection pipeline and achieves new state-of-the-art results on the\nchallenging COCO object detection task on both one- and two-stage detectors.\nIts superior performance on instance segmentation further manifests the\ngeneralization ability. Both 2x-distilled RetinaNet and FCOS with ResNet50-FPN\noutperform their corresponding 3x ResNet101-FPN teacher, arriving 40.64 and\n43.06 AP, respectively. Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding. (arXiv:2203.00867v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00867","description":"<p>Image inpainting has made significant advances in recent years. However, it\nis still challenging to recover corrupted images with both vivid textures and\nreasonable structures. Some specific methods only tackle regular textures while\nlosing holistic structures due to the limited receptive fields of convolutional\nneural networks (CNNs). On the other hand, attention-based models can learn\nbetter long-range dependency for the structure recovery, but they are limited\nby the heavy computation for inference with large image sizes. To address these\nissues, we propose to leverage an additional structure restorer to facilitate\nthe image inpainting incrementally. The proposed model restores holistic image\nstructures with a powerful attention-based transformer model in a fixed\nlow-resolution sketch space. Such a grayscale space is easy to be upsampled to\nlarger scales to convey correct structural information. Our structure restorer\ncan be integrated with other pretrained inpainting models efficiently with the\nzero-initialized residual addition. Furthermore, a masking positional encoding\nstrategy is utilized to improve the performance with large irregular masks.\nExtensive experiments on various datasets validate the efficacy of our model\ncompared with other competitors. Our codes are released in\nhttps://github.com/DQiaole/ZITS_inpainting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiaole Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chenjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Optimized Deep Convolution Neural Network based Learning Model for Object Detection. (arXiv:2203.00869v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00869","description":"<p>Object identification is one of the most fundamental and difficult issues in\ncomputer vision. It aims to discover object instances in real pictures from a\nhuge number of established categories. In recent years, deep learning-based\nobject detection techniques that developed from computer vision have grabbed\nthe public's interest. Object recognition methods based on deep learning\nframeworks have quickly become a popular way to interpret moving images\nacquired by various sensors. Due to its vast variety of applications for\nvarious computer vision tasks such as activity or event detection,\ncontent-based image retrieval, and scene understanding, academics have spent\ndecades attempting to solve this problem. With this goal in mind, a unique deep\nlearning classification technique is used to create an autonomous object\ndetecting system. The noise destruction and normalising operations, which are\ncarried out using gaussian filter and contrast normalisation techniques,\nrespectively, are the first steps in the study activity. The pre-processed\npicture is next subjected to entropy-based segmentation algorithms, which\nseparate the image's significant areas in order to distinguish between distinct\noccurrences. The classification challenge is completed by the suggested Hybrid\nOptimized Dense Convolutional Neural Network (HODCNN). The major goal of this\nframework is to aid in the precise recognition of distinct items from the\ngathered input frames. The suggested system's performance is assessed by\ncomparing it to existing machine learning and deep learning methodologies. The\nexperimental findings reveal that the suggested framework has a detection\naccuracy of 0.9864, which is greater than current techniques. As a result, the\nsuggested object detection model outperforms other current methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beri_V/0/1/0/all/0/1\">Venkata Beri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Voxel Fusion for 3D Object Detection. (arXiv:2203.00871v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00871","description":"<p>Camera and LiDAR sensor modalities provide complementary appearance and\ngeometric information useful for detecting 3D objects for autonomous vehicle\napplications. However, current fusion models underperform state-of-art\nLiDAR-only methods on 3D object detection benchmarks. Our proposed solution,\nDense Voxel Fusion (DVF) is a sequential fusion method that generates\nmulti-scale multi-modal dense voxel feature representations, improving\nexpressiveness in low point density regions. To enhance multi-modal learning,\nwe train directly with ground truth 2D bounding box labels, avoiding noisy,\ndetector-specific, 2D predictions. Additionally, we use LiDAR ground truth\nsampling to simulate missed 2D detections and to accelerate training\nconvergence. Both DVF and the multi-modal training approaches can be applied to\nany voxel-based LiDAR backbone without introducing additional learnable\nparameters. DVF outperforms existing sparse fusion detectors, ranking $1^{st}$\namong all published fusion methods on KITTI's 3D car detection benchmark at the\ntime of submission and significantly improves 3D vehicle detection performance\nof voxel-based methods on the Waymo Open Dataset. We also show that our\nproposed multi-modal training strategy results in better generalization\ncompared to training using erroneous 2D predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoud_A/0/1/0/all/0/1\">Anas Mahmoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jordan S. K. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven L. Waslander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Split Semantic Detection Algorithm for Psychological Sandplay Image. (arXiv:2203.00907v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00907","description":"<p>Psychological sandplay, as an important psychological analysis tool, is a\nvisual scene constructed by the tester selecting and placing sand objects\n(e.g., sand, river, human figures, animals, vegetation, buildings, etc.). As\nthe projection of the tester's inner world, it contains high-level semantic\ninformation reflecting the tester's thoughts and feelings. Most of the existing\ncomputer vision technologies focus on the objective basic semantics (e.g.,\nobject's name, attribute, boundingbox, etc.) in the natural image, while few\nrelated works pay attention to the subjective psychological semantics (e.g.,\nemotion, thoughts, feelings, etc.) in the artificial image. We take the latter\nsemantics as the research object, take \"split\" (a common psychological\nsemantics reflecting the inner integration of testers) as the research goal,\nand use the method of machine learning to realize the automatic detection of\nsplit semantics, so as to explore the application of machine learning in the\ndetection of subjective psychological semantics of sandplay images. To this\nend, we present a feature dimensionality reduction and extraction algorithm to\nobtain a one-dimensional vector representing the split feature, and build the\nsplit semantic detector based on Multilayer Perceptron network to get the\ndetection results. Experimental results on the real sandplay datasets show the\neffectiveness of our proposed algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaokun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiqi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence. (arXiv:2203.00911v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00911","description":"<p>Deep learning based single image super-resolution models have been widely\nstudied and superb results are achieved in upscaling low-resolution images with\nfixed scale factor and downscaling degradation kernel. To improve real world\napplicability of such models, there are growing interests to develop models\noptimized for arbitrary upscaling factors. Our proposed method is the first to\ntreat arbitrary rescaling, both upscaling and downscaling, as one unified\nprocess. Using joint optimization of both directions, the proposed model is\nable to learn upscaling and downscaling simultaneously and achieve\nbidirectional arbitrary image rescaling. It improves the performance of current\narbitrary upscaling models by a large margin while at the same time learns to\nmaintain visual perception quality in downscaled images. The proposed model is\nfurther shown to be robust in cycle idempotence test, free of severe\ndegradations in reconstruction accuracy when the downscaling-to-upscaling cycle\nis applied repetitively. This robustness is beneficial for image rescaling in\nthe wild when this cycle could be applied to one image for multiple times. It\nalso performs well on tests with arbitrary large scales and asymmetric scales,\neven when the model is not trained with such tasks. Extensive experiments are\nconducted to demonstrate the superior performance of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_Z/0/1/0/all/0/1\">Zhihong Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_T/0/1/0/all/0/1\">Tianwei Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Principled Design of Image Representation: Towards Forensic Tasks. (arXiv:2203.00913v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00913","description":"<p>Image forensics is a rising topic as the trustworthy multimedia content is\ncritical for modern society. Like other vision-related applications, forensic\nanalysis relies heavily on the proper image representation. Despite the\nimportance, current theoretical understanding for such representation remains\nlimited, with varying degrees of neglect for its key role. For this gap, we\nattempt to investigate the forensic-oriented image representation as a distinct\nproblem, from the perspectives of theory, implementation, and application. Our\nwork starts from the abstraction of basic principles that the representation\nfor forensics should satisfy, especially revealing the criticality of\nrobustness, interpretability, and coverage. At the theoretical level, we\npropose a new representation framework for forensics, called Dense Invariant\nRepresentation (DIR), which is characterized by stable description with\nmathematical guarantees. At the implementation level, the discrete calculation\nproblems of DIR are discussed, and the corresponding accurate and fast\nsolutions are designed with generic nature and constant complexity. We\ndemonstrate the above arguments on the dense-domain pattern detection and\nmatching experiments, providing comparison results with state-of-the-art\ndescriptors. Also, at the application level, the proposed DIR is initially\nexplored in passive and active forensics, namely copy-move forgery detection\nand perceptual hashing, exhibiting the benefits in fulfilling the requirements\nof such forensic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1\">Shuren Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yushu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiantao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PUFA-GAN: A Frequency-Aware Generative Adversarial Network for 3D Point Cloud Upsampling. (arXiv:2203.00914v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00914","description":"<p>We propose a generative adversarial network for point cloud upsampling, which\ncan not only make the upsampled points evenly distributed on the underlying\nsurface but also efficiently generate clean high frequency regions. The\ngenerator of our network includes a dynamic graph hierarchical residual\naggregation unit and a hierarchical residual aggregation unit for point feature\nextraction and upsampling, respectively. The former extracts multiscale\npoint-wise descriptive features, while the latter captures rich feature details\nwith hierarchical residuals. To generate neat edges, our discriminator uses a\ngraph filter to extract and retain high frequency points. The generated high\nresolution point cloud and corresponding high frequency points help the\ndiscriminator learn the global and high frequency properties of the point\ncloud. We also propose an identity distribution loss function to make sure that\nthe upsampled points remain on the underlying surface of the input low\nresolution point cloud. To assess the regularity of the upsampled points in\nhigh frequency regions, we introduce two evaluation metrics. Objective and\nsubjective results demonstrate that the visual quality of the upsampled point\nclouds generated by our method is better than that of the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamzaoui_R/0/1/0/all/0/1\">Raouf Hamzaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translation Invariant Global Estimation of Heading Angle Using Sinogram of LiDAR Point Cloud. (arXiv:2203.00924v1 [cs.RO])","link":"http://arxiv.org/abs/2203.00924","description":"<p>Global point cloud registration is an essential module for localization, of\nwhich the main difficulty exists in estimating the rotation globally without\ninitial value. With the aid of gravity alignment, the degree of freedom in\npoint cloud registration could be reduced to 4DoF, in which only the heading\nangle is required for rotation estimation. In this paper, we propose a fast and\naccurate global heading angle estimation method for gravity-aligned point\nclouds. Our key idea is that we generate a translation invariant representation\nbased on Radon Transform, allowing us to solve the decoupled heading angle\nglobally with circular cross-correlation. Besides, for heading angle estimation\nbetween point clouds with different distributions, we implement this heading\nangle estimator as a differentiable module to train a feature extraction\nnetwork end- to-end. The experimental results validate the effectiveness of the\nproposed method in heading angle estimation and show better performance\ncompared with other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaqing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuecheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Sha Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yanmei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mengwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Huanjun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameterized Image Quality Score Distribution Prediction. (arXiv:2203.00926v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00926","description":"<p>Recently, image quality has been generally describedby a mean opinion score\n(MOS). However, we observe that thequality scores of an image given by a group\nof subjects are verysubjective and diverse. Thus it is not enough to use a MOS\ntodescribe the image quality. In this paper, we propose to describeimage\nquality using a parameterized distribution rather thana MOS, and an objective\nmethod is also proposed to predictthe image quality score distribution (IQSD).\nAt first, the LIVEdatabase is re-recorded. Specifically, we have invited a\nlargegroup of subjects to evaluate the quality of all images in theLIVE\ndatabase, and each image is evaluated by a large numberof subjects (187 valid\nsubjects), whose scores can form a reliableIQSD. By analyzing the obtained\nsubjective quality scores, wefind that the IQSD can be well modeled by an alpha\nstable model,and it can reflect much more information than a single MOS, suchas\nthe skewness of opinion score, the subject diversity and themaximum probability\nscore for an image. Therefore, we proposeto model the IQSD using the alpha\nstable model. Moreover, wepropose a framework and an algorithm to predict the\nalphastable model based IQSD, where quality features are extractedfrom each\nimage based on structural information and statisticalinformation, and support\nvector regressors are trained to predictthe alpha stable model parameters.\nExperimental results verifythe feasibility of using alpha stable model to\ndescribe the IQSD,and prove the effectiveness of objective alpha stable model\nbasedIQSD prediction method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yixuan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhan Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration. (arXiv:2203.00927v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00927","description":"<p>Traditional video-based human activity recognition has experienced remarkable\nprogress linked to the rise of deep learning, but this effect was slower as it\ncomes to the downstream task of driver behavior understanding. Understanding\nthe situation inside the vehicle cabin is essential for Advanced Driving\nAssistant System (ADAS) as it enables identifying distraction, predicting\ndriver's intent and leads to more convenient human-vehicle interaction. At the\nsame time, driver observation systems face substantial obstacles as they need\nto capture different granularities of driver states, while the complexity of\nsuch secondary activities grows with the rising automation and increased driver\nfreedom. Furthermore, a model is rarely deployed under conditions identical to\nthe ones in the training set, as sensor placements and types vary from vehicle\nto vehicle, constituting a substantial obstacle for real-life deployment of\ndata-driven models. In this work, we present a novel vision-based framework for\nrecognizing secondary driver behaviours based on visual transformers and an\nadditional augmented feature distribution calibration module. This module\noperates in the latent feature-space enriching and diversifying the training\nset at feature-level in order to improve generalization to novel data\nappearances, (e.g., sensor changes) and general feature quality. Our framework\nconsistently leads to better recognition rates, surpassing previous\nstate-of-the-art results of the public Drive&amp;Act benchmark on all granularity\nlevels. Our code will be made publicly available at\nhttps://github.com/KPeng9510/TransDARC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaPose: Parameter and Domain Randomization Optimization for Pose Estimation using Synthetic Data. (arXiv:2203.00945v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00945","description":"<p>Pose estimation is the task of determining the 6D position of an object in a\nscene. Pose estimation aid the abilities and flexibility of robotic set-ups.\nHowever, the system must be configured towards the use case to perform\nadequately. This configuration is time-consuming and limits the usability of\npose estimation and, thereby, robotic systems.\n</p>\n<p>Deep learning is a method to overcome this configuration procedure by\nlearning parameters directly from the dataset. However, obtaining this training\ndata can also be very time-consuming. The use of synthetic training data avoids\nthis data collection problem, but a configuration of the training procedure is\nnecessary to overcome the domain gap problem. Additionally, the pose estimation\nparameters also need to be configured. This configuration is jokingly known as\ngrad student descent as parameters are manually adjusted until satisfactory\nresults are obtained.\n</p>\n<p>This paper presents a method for automatic configuration using only synthetic\ndata. This is accomplished by learning the domain randomization during network\ntraining, and then using the domain randomization to optimize the pose\nestimation parameters. The developed approach shows state-of-the-art\nperformance of 82.0 % recall on the challenging OCCLUSION dataset,\noutperforming all previous methods with a large margin. These results prove the\nvalidity of automatic set-up of pose estimation using purely synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagelskjaer_F/0/1/0/all/0/1\">Frederik Hagelskjaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_A/0/1/0/all/0/1\">Anders Glent Buch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CD-GAN: a robust fusion-based generative adversarial network for unsupervised change detection between heterogeneous images. (arXiv:2203.00948v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00948","description":"<p>In the context of Earth observation, the detection of changes is performed\nfrom multitemporal images acquired by sensors with possibly different\ncharacteristics and modalities. Even when restricting to the optical modality,\nthis task has proved to be challenging as soon as the sensors provide images of\ndifferent spatial and/or spectral resolutions. This paper proposes a novel\nunsupervised change detection method dedicated to such so-called heterogeneous\noptical images. This method capitalizes on recent advances which frame the\nchange detection problem into a robust fusion framework. More precisely, we\nshow that a deep adversarial network designed and trained beforehand to fuse a\npair of multiband images can be easily complemented by a network with the same\narchitecture to perform change detection. The resulting overall architecture\nitself follows an adversarial strategy where the fusion network and the\nadditional network are interpreted as essential building blocks of a generator.\nA comparison with state-of-the-art change detection methods demonstrate the\nversatility and the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jin-Ju Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dobigeon_N/0/1/0/all/0/1\">Nicolas Dobigeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chabert_M/0/1/0/all/0/1\">Marie Chabert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Ding-Cheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Zhu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketched RT3D: How to reconstruct billions of photons per second. (arXiv:2203.00952v1 [eess.IV])","link":"http://arxiv.org/abs/2203.00952","description":"<p>Single-photon light detection and ranging (lidar) captures depth and\nintensity information of a 3D scene. Reconstructing a scene from observed\nphotons is a challenging task due to spurious detections associated with\nbackground illumination sources. To tackle this problem, there is a plethora of\n3D reconstruction algorithms which exploit spatial regularity of natural scenes\nto provide stable reconstructions. However, most existing algorithms have\ncomputational and memory complexity proportional to the number of recorded\nphotons. This complexity hinders their real-time deployment on modern lidar\narrays which acquire billions of photons per second. Leveraging a recent lidar\nsketching framework, we show that it is possible to modify existing\nreconstruction algorithms such that they only require a small sketch of the\nphoton information. In particular, we propose a sketched version of a recent\nstate-of-the-art algorithm which uses point cloud denoisers to provide\nspatially regularized reconstructions. A series of experiments performed on\nreal lidar datasets demonstrates a significant reduction of execution time and\nmemory requirements, while achieving the same reconstruction performance than\nin the full data case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tachella_J/0/1/0/all/0/1\">Juli&#xe1;n Tachella</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheehan_M/0/1/0/all/0/1\">Michael P. Sheehan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Davies_M/0/1/0/all/0/1\">Mike E. Davies</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRASP EARTH: Intuitive Software for Discovering Changes on the Planet. (arXiv:2203.00955v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00955","description":"<p>Detecting changes on the Earth, such as urban development, deforestation, or\nnatural disaster, is one of the research fields that is attracting a great deal\nof attention. One promising tool to solve these problems is satellite imagery.\nHowever, satellite images require huge amount of storage, therefore users are\nrequired to set Area of Interests first, which was not suitable for detecting\npotential areas for disaster or development. To tackle with this problem, we\ndevelop the novel tool, namely GRASP EARTH, which is the simple change\ndetection application based on Google Earth Engine. GRASP EARTH allows us to\nhandle satellite imagery easily and it has used for disaster monitoring and\nurban development monitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hatakeyama_W/0/1/0/all/0/1\">Waku Hatakeyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawakita_S/0/1/0/all/0/1\">Shirou Kawakita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izawa_R/0/1/0/all/0/1\">Ryohei Izawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_M/0/1/0/all/0/1\">Masanari Kimura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Moving-Object Tracking with FMCW LiDAR. (arXiv:2203.00959v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00959","description":"<p>In this paper, we propose a learning-based moving-object tracking method\nutilizing our newly developed LiDAR sensor, Frequency Modulated Continuous Wave\n(FMCW) LiDAR. Compared with most existing commercial LiDAR sensors, our FMCW\nLiDAR can provide additional Doppler velocity information to each 3D point of\nthe point clouds. Benefiting from this, we can generate instance labels as\nground truth in a semi-automatic manner. Given the labels, we propose a\ncontrastive learning framework, which pulls together the features from the same\ninstance in embedding space and pushes apart the features from different\ninstances, to improve the tracking quality. Extensive experiments are conducted\non our recorded driving data, and the results show that our method outperforms\nthe baseline methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hongzhi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kafeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1\">Hui Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aggregated Pyramid Vision Transformer: Split-transform-merge Strategy for Image Recognition without Convolutions. (arXiv:2203.00960v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00960","description":"<p>With the achievements of Transformer in the field of natural language\nprocessing, the encoder-decoder and the attention mechanism in Transformer have\nbeen applied to computer vision. Recently, in multiple tasks of computer vision\n(image classification, object detection, semantic segmentation, etc.),\nstate-of-the-art convolutional neural networks have introduced some concepts of\nTransformer. This proves that Transformer has a good prospect in the field of\nimage recognition. After Vision Transformer was proposed, more and more works\nbegan to use self-attention to completely replace the convolutional layer. This\nwork is based on Vision Transformer, combined with the pyramid architecture,\nusing Split-transform-merge to propose the group encoder and name the network\narchitecture Aggregated Pyramid Vision Transformer (APVT). We perform image\nclassification tasks on the CIFAR-10 dataset and object detection tasks on the\nCOCO 2017 dataset. Compared with other network architectures that use\nTransformer as the backbone, APVT has excellent results while reducing the\ncomputational cost. We hope this improved strategy can provide a reference for\nfuture Transformer research in computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1\">Rui-Yang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jen-Shiun Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jia-Hao Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Shian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liu-Rui-Yi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation. (arXiv:2203.00962v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00962","description":"<p>Extracting class activation maps (CAM) is arguably the most standard step of\ngenerating pseudo masks for weakly-supervised semantic segmentation (WSSS).\nYet, we find that the crux of the unsatisfactory pseudo masks is the binary\ncross-entropy loss (BCE) widely used in CAM. Specifically, due to the\nsum-over-class pooling nature of BCE, each pixel in CAM may be responsive to\nmultiple classes co-occurring in the same receptive field. As a result, given a\nclass, its hot CAM pixels may wrongly invade the area belonging to other\nclasses, or the non-hot ones may be actually a part of the class. To this end,\nwe introduce an embarrassingly simple yet surprisingly effective method:\nReactivating the converged CAM with BCE by using softmax cross-entropy loss\n(SCE), dubbed \\textbf{ReCAM}. Given an image, we use CAM to extract the feature\npixels of each single class, and use them with the class label to learn another\nfully-connected layer (after the backbone) with SCE. Once converged, we extract\nReCAM in the same way as in CAM. Thanks to the contrastive nature of SCE, the\npixel response is disentangled into different classes and hence less mask\nambiguity is expected. The evaluation on both PASCAL VOC and MS~COCO shows that\nReCAM not only generates high-quality masks, but also supports plug-and-play in\nany CAM variant with little overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaozheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiongwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qianru Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training. (arXiv:2203.00972v1 [cs.CV])","link":"http://arxiv.org/abs/2203.00972","description":"<p>The paper presents a simple and effective learning-based method for computing\na discriminative 3D point cloud descriptor for place recognition purposes.\nRecent state-of-the-art methods have relatively complex architectures such as\nmulti-scale oyramid of point Transformers combined with a pyramid of feature\naggregation modules. Our method uses a simple and efficient 3D convolutional\nfeature extraction, based on a sparse voxelized representation, enhanced with\nchannel attention blocks. We employ recent advances in image retrieval and\npropose a modified version of a loss function based on a differentiable average\nprecision approximation. Such loss function requires training with very large\nbatches for the best results. This is enabled by using multistaged\nbackpropagation. Experimental evaluation on the popular benchmarks proves the\neffectiveness of our approach, with a consistent improvement over the state of\nthe art\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Komorowski_J/0/1/0/all/0/1\">Jacek Komorowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TableFormer: Table Structure Understanding with Transformers. (arXiv:2203.01017v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01017","description":"<p>Tables organize valuable content in a concise and compact representation.\nThis content is extremely valuable for systems such as search engines,\nKnowledge Graph's, etc, since they enhance their predictive capabilities.\nUnfortunately, tables come in a large variety of shapes and sizes. Furthermore,\nthey can have complex column/row-header configurations, multiline rows,\ndifferent variety of separation lines, missing entries, etc. As such, the\ncorrect identification of the table-structure from an image is a non-trivial\ntask. In this paper, we present a new table-structure identification model. The\nlatter improves the latest end-to-end deep learning model (i.e.\nencoder-dual-decoder from PubTabNet) in two significant ways. First, we\nintroduce a new object detection decoder for table-cells. In this way, we can\nobtain the content of the table-cells from programmatic PDF's directly from the\nPDF source and avoid the training of the custom OCR decoders. This\narchitectural change leads to more accurate table-content extraction and allows\nus to tackle non-english tables. Second, we replace the LSTM decoders with\ntransformer based decoders. This upgrade improves significantly the previous\nstate-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple\ntables and from 88.7% to 95% on complex tables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Ahmed Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livathinos_N/0/1/0/all/0/1\">Nikolaos Livathinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lysak_M/0/1/0/all/0/1\">Maksym Lysak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staar_P/0/1/0/all/0/1\">Peter Staar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asynchronous Optimisation for Event-based Visual Odometry. (arXiv:2203.01037v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01037","description":"<p>Event cameras open up new possibilities for robotic perception due to their\nlow latency and high dynamic range. On the other hand, developing effective\nevent-based vision algorithms that fully exploit the beneficial properties of\nevent cameras remains work in progress. In this paper, we focus on event-based\nvisual odometry (VO). While existing event-driven VO pipelines have adopted\ncontinuous-time representations to asynchronously process event data, they\neither assume a known map, restrict the camera to planar trajectories, or\nintegrate other sensors into the system. Towards map-free event-only monocular\nVO in SE(3), we propose an asynchronous structure-from-motion optimisation\nback-end. Our formulation is underpinned by a principled joint optimisation\nproblem involving non-parametric Gaussian Process motion modelling and\nincremental maximum a posteriori inference. A high-performance incremental\ncomputation engine is employed to reason about the camera trajectory with every\nincoming event. We demonstrate the robustness of our asynchronous back-end in\ncomparison to frame-based methods which depend on accurate temporal\naccumulation of measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_A/0/1/0/all/0/1\">Alvaro Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latif_Y/0/1/0/all/0/1\">Yasir Latif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-based material analysis of ancient historical documents. (arXiv:2203.01042v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01042","description":"<p>Researchers continually perform corroborative tests to classify ancient\nhistorical documents based on the physical materials of their writing surfaces.\nHowever, these tests, often performed on-site, requires actual access to the\nmanuscript objects. The procedures involve a considerable amount of time and\ncost, and can damage the manuscripts. Developing a technique to classify such\ndocuments using only digital images can be very useful and efficient. In order\nto tackle this problem, this study uses images of a famous historical\ncollection, the Dead Sea Scrolls, to propose a novel method to classify the\nmaterials of the manuscripts. The proposed classifier uses the two-dimensional\nFourier Transform to identify patterns within the manuscript surfaces.\nCombining a binary classification system employing the transform with a\nmajority voting process is shown to be effective for this classification task.\nThis pilot study shows a successful classification percentage of up to 97% for\na confined amount of manuscripts produced from either parchment or papyrus\nmaterial. Feature vectors based on Fourier-space grid representation\noutperformed a concentric Fourier-space format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_T/0/1/0/all/0/1\">Thomas Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhali_M/0/1/0/all/0/1\">Maruf A. Dhali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D object reconstruction and 6D-pose estimation from 2D shape for robotic grasping of objects. (arXiv:2203.01051v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01051","description":"<p>We propose a method for 3D object reconstruction and 6D-pose estimation from\n2D images that uses knowledge about object shape as the primary key. In the\nproposed pipeline, recognition and labeling of objects in 2D images deliver 2D\nsegment silhouettes that are compared with the 2D silhouettes of projections\nobtained from various views of a 3D model representing the recognized object\nclass. By computing transformation parameters directly from the 2D images, the\nnumber of free parameters required during the registration process is reduced,\nmaking the approach feasible. Furthermore, 3D transformations and projective\ngeometry are employed to arrive at a full 3D reconstruction of the object in\ncamera space using a calibrated set up. Inclusion of a second camera allows\nresolving remaining ambiguities. The method is quantitatively evaluated using\nsynthetic data and tested with real data, and additional results for the\nwell-known Linemod data set are shown. In robot experiments, successful\ngrasping of objects demonstrates its usability in real-world environments, and,\nwhere possible, a comparison with other methods is provided. The method is\napplicable to scenarios where 3D object models, e.g., CAD-models or point\nclouds, are available and precise pixel-wise segmentation maps of 2D images can\nbe obtained. Different from other methods, the method does not use 3D depth for\ntraining, widening the domain of application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolnitza_M/0/1/0/all/0/1\">Marcell Wolnitza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaya_O/0/1/0/all/0/1\">Osman Kaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulvicius_T/0/1/0/all/0/1\">Tomas Kulvicius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worgotter_F/0/1/0/all/0/1\">Florentin W&#xf6;rg&#xf6;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dellen_B/0/1/0/all/0/1\">Babette Dellen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Anomaly Detection from Time-of-Flight Depth Images. (arXiv:2203.01052v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01052","description":"<p>Video anomaly detection (VAD) addresses the problem of automatically finding\nanomalous events in video data. The primary data modalities on which current\nVAD systems work on are monochrome or RGB images. Using depth data in this\ncontext instead is still hardly explored in spite of depth images being a\npopular choice in many other computer vision research areas and the increasing\navailability of inexpensive depth camera hardware. We evaluate the application\nof existing autoencoder-based methods on depth video and propose how the\nadvantages of using depth data can be leveraged by integration into the loss\nfunction. Training is done unsupervised using normal sequences without need for\nany additional annotations. We show that depth allows easy extraction of\nauxiliary information for scene analysis in the form of a foreground mask and\ndemonstrate its beneficial effect on the anomaly detection performance through\nevaluation on a large public dataset, for which we are also the first ones to\npresent results on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pascal Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirbach_B/0/1/0/all/0/1\">Bruno Mirbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colar: Effective and Efficient Online Action Detection by Consulting Exemplars. (arXiv:2203.01057v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01057","description":"<p>Online action detection has attracted increasing research interests in recent\nyears. Current works model historical dependencies and anticipate future to\nperceive the action evolution within a video segment and improve the detection\naccuracy. However, the existing paradigm ignores category-level modeling and\ndoes not pay sufficient attention to efficiency. Considering a category, its\nrepresentative frames exhibit various characteristics. Thus, the category-level\nmodeling can provide complementary guidance to the temporal dependencies\nmodeling. In this paper, we develop an effective exemplar-consultation\nmechanism that first measures the similarity between a frame and exemplary\nframes, and then aggregates exemplary features based on the similarity weights.\nThis is also an efficient mechanism as both similarity measurement and feature\naggregation require limited computations. Based on the exemplar-consultation\nmechanism, the long-term dependencies can be captured by regarding historical\nframes as exemplars, and the category-level modeling can be achieved by\nregarding representative frames from a category as exemplars. Due to the\ncomplementarity from the category-level modeling, our method employs a\nlightweight architecture but achieves new high performance on three benchmarks.\nIn addition, using a spatio-temporal network to tackle video frames, our method\nspends 9.8 seconds to dispose of a one-minute video and achieves comparable\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Le Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation. (arXiv:2203.01072v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01072","description":"<p>This paper proposes a universal framework, called OVE6D, for model-based 6D\nobject pose estimation from a single depth image and a target object mask. Our\nmodel is trained using purely synthetic data rendered from ShapeNet, and,\nunlike most of the existing methods, it generalizes well on new real-world\nobjects without any fine-tuning. We achieve this by decomposing the 6D pose\ninto viewpoint, in-plane rotation around the camera optical axis and\ntranslation, and introducing novel lightweight modules for estimating each\ncomponent in a cascaded manner. The resulting network contains less than 4M\nparameters while demonstrating excellent performance on the challenging T-LESS\nand Occluded LINEMOD datasets without any dataset-specific training. We show\nthat OVE6D outperforms some contemporary deep learning-based pose estimation\nmethods specifically trained for individual objects or datasets with real-world\ntraining data.\n</p>\n<p>The implementation and the pre-trained model will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Dingding Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkil&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation. (arXiv:2203.01074v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01074","description":"<p>Environment perception in autonomous driving vehicles often heavily relies on\ndeep neural networks (DNNs), which are subject to domain shifts, leading to a\nsignificantly decreased performance during DNN deployment. Usually, this\nproblem is addressed by unsupervised domain adaptation (UDA) approaches trained\neither simultaneously on source and target domain datasets or even source-free\nonly on target data in an offline fashion. In this work, we further expand a\nsource-free UDA approach to a continual and therefore online-capable UDA on a\nsingle-image basis for semantic segmentation. Accordingly, our method only\nrequires the pre-trained model from the supplier (trained in the source domain)\nand the current (unlabeled target domain) camera image. Our method Continual\nBatchNorm Adaptation (CBNA) modifies the source domain statistics in the batch\nnormalization layers, using target domain images in an unsupervised fashion,\nwhich yields consistent performance improvements during inference. Thereby, in\ncontrast to existing works, our approach can be applied to improve a DNN\ncontinuously on a single-image basis during deployment without access to source\ndata, without algorithmic delay, and nearly without computational overhead. We\nshow the consistent effectiveness of our method across a wide variety of\nsource/target domain settings for semantic segmentation. As part of this work,\nour code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klingner_M/0/1/0/all/0/1\">Marvin Klingner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayache_M/0/1/0/all/0/1\">Mouadh Ayache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-based Large-scale 3D Semantic Mapping for Autonomous Driving Applications. (arXiv:2203.01087v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01087","description":"<p>In this paper, we present a complete pipeline for 3D semantic mapping solely\nbased on a stereo camera system. The pipeline comprises a direct sparse visual\nodometry front-end as well as a back-end for global optimization including GNSS\nintegration, and semantic 3D point cloud labeling. We propose a simple but\neffective temporal voting scheme which improves the quality and consistency of\nthe 3D point labels. Qualitative and quantitative evaluations of our pipeline\nare performed on the KITTI-360 dataset. The results show the effectiveness of\nour proposed voting scheme and the capability of our pipeline for efficient\nlarge-scale 3D semantic mapping. The large-scale mapping capabilities of our\npipeline is furthermore demonstrated by presenting a very large-scale semantic\nmap covering 8000 km of roads generated from data collected by a fleet of\nvehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeller_N/0/1/0/all/0/1\">Niclas Zeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape constrained CNN for segmentation guided prediction of myocardial shape and pose parameters in cardiac MRI. (arXiv:2203.01089v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01089","description":"<p>Semantic segmentation using convolutional neural networks (CNNs) is the\nstate-of-the-art for many medical image segmentation tasks including myocardial\nsegmentation in cardiac MR images. However, the predicted segmentation maps\nobtained from such standard CNN do not allow direct quantification of regional\nshape properties such as regional wall thickness. Furthermore, the CNNs lack\nexplicit shape constraints, occasionally resulting in unrealistic\nsegmentations. In this paper, we use a CNN to predict shape parameters of an\nunderlying statistical shape model of the myocardium learned from a training\nset of images. Additionally, the cardiac pose is predicted, which allows to\nreconstruct the myocardial contours. The integrated shape model regularizes the\npredicted contours and guarantees realistic shapes. We enforce robustness of\nshape and pose prediction by simultaneously performing pixel-wise semantic\nsegmentation during training and define two loss functions to impose\nconsistency between the two predicted representations: one distance-based loss\nand one overlap-based loss. We evaluated the proposed method in a 5-fold cross\nvalidation on an in-house clinical dataset with 75 subjects and on the ACDC and\nLVQuan19 public datasets. We show the benefits of simultaneous semantic\nsegmentation and the two newly defined loss functions for the prediction of\nshape parameters. Our method achieved a correlation of 99% for left ventricular\n(LV) area on the three datasets, between 91% and 97% for myocardial area,\n98-99% for LV dimensions and between 80% and 92% for regional wall thickness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tilborghs_S/0/1/0/all/0/1\">Sofie Tilborghs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogaert_J/0/1/0/all/0/1\">Jan Bogaert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_F/0/1/0/all/0/1\">Frederik Maes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generalized Approach for Cancellable Template and Its Realization for Minutia Cylinder-Code. (arXiv:2203.01095v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01095","description":"<p>Hashing technology gains much attention in protecting the biometric template\nlately. For instance, Index-of-Max (IoM), a recent reported hashing technique,\nis a ranking-based locality sensitive hashing technique, which illustrates the\nfeasibility to protect the ordered and fixed-length biometric template.\nHowever, biometric templates are not always in the form of ordered and\nfixed-length, rather it may be an unordered and variable size point set e.g.\nfingerprint minutiae, which restricts the usage of the traditional hashing\ntechnology. In this paper, we proposed a generalized version of IoM hashing\nnamely gIoM, and therefore the unordered and variable size biometric template\ncan be used. We demonstrate a realization using a well-known variable size\nfeature vector, fingerprint Minutia Cylinder-Code (MCC). The gIoM transforms\nMCC into index domain to form indexing-based feature representation.\nConsequently, the inversion of MCC from the transformed representation is\ncomputational infeasible, thus to achieve non-invertibility while the\nperformance is preserved. Public fingerprint databases FVC2002 and FVC2004 are\nemployed for experiment as benchmark to demonstrate a fair comparison with\nother methods. Moreover, the security and privacy analysis suggest that gIoM\nmeets the criteria of template protection: non-invertibility, revocability, and\nnon-linkability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xingbo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhe Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">KokSheik Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Scene Flow Estimation with 4D Automotive Radar. (arXiv:2203.01137v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01137","description":"<p>Scene flow allows autonomous vehicles to reason about the arbitrary motion of\nmultiple independent objects which is the key to long-term mobile autonomy.\nWhile estimating the scene flow from LiDAR has progressed recently, it remains\nlargely unknown how to estimate the scene flow from a 4D radar - an\nincreasingly popular automotive sensor for its robustness against adverse\nweather and lighting conditions. Compared with the LiDAR point clouds, radar\ndata are drastically sparser, noisier and in much lower resolution. Annotated\ndatasets for radar scene flow are also in absence and costly to acquire in the\nreal world. These factors jointly pose the radar scene flow estimation as a\nchallenging problem. This work aims to address the above challenges and\nestimate scene flow from 4D radar point clouds by leveraging self-supervised\nlearning. A robust scene flow estimation architecture and three novel losses\nare bespoken designed to cope with intractable radar data. Real-world\nexperimental results validate that our method is able to robustly estimate the\nradar scene flow in the wild and effectively supports the downstream task of\nmotion segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fangqiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhijun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yimin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jianning Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Lidar-Based Semantic Segmentation of Top-View Grid Maps by Learning Features in Complementary Representations. (arXiv:2203.01151v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01151","description":"<p>In this paper we introduce a novel way to predict semantic information from\nsparse, single-shot LiDAR measurements in the context of autonomous driving. In\nparticular, we fuse learned features from complementary representations. The\napproach is aimed specifically at improving the semantic segmentation of\ntop-view grid maps. Towards this goal the 3D LiDAR point cloud is projected\nonto two orthogonal 2D representations. For each representation a tailored deep\nlearning architecture is developed to effectively extract semantic information\nwhich are fused by a superordinate deep neural network. The contribution of\nthis work is threefold: (1) We examine different stages within the segmentation\nnetwork for fusion. (2) We quantify the impact of embedding different features.\n(3) We use the findings of this survey to design a tailored deep neural network\narchitecture leveraging respective advantages of different representations. Our\nmethod is evaluated using the SemanticKITTI dataset which provides a point-wise\nsemantic annotation of more than 23.000 LiDAR measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Link_M/0/1/0/all/0/1\">Maximilian Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanski_S/0/1/0/all/0/1\">Simon Romanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haohao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisARM: Displacement Aware Relation Module for 3D Detection. (arXiv:2203.01152v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01152","description":"<p>We introduce Displacement Aware Relation Module (DisARM), a novel neural\nnetwork module for enhancing the performance of 3D object detection in point\ncloud scenes. The core idea of our method is that contextual information is\ncritical to tell the difference when the instance geometry is incomplete or\nfeatureless. We find that relations between proposals provide a good\nrepresentation to describe the context. However, adopting relations between all\nthe object or patch proposals for detection is inefficient, and an imbalanced\ncombination of local and global relations brings extra noise that could mislead\nthe training. Rather than working with all relations, we found that training\nwith relations only between the most representative ones, or anchors, can\nsignificantly boost the detection performance. A good anchor should be\nsemantic-aware with no ambiguity and independent with other anchors as well. To\nfind the anchors, we first perform a preliminary relation anchor module with an\nobjectness-aware sampling approach and then devise a displacement-based module\nfor weighing the relation importance for better utilization of contextual\ninformation. This lightweight relation module leads to significantly higher\naccuracy of object instance detection when being plugged into the\nstate-of-the-art detectors. Evaluations on the public benchmarks of real-world\nscenes show that our method achieves state-of-the-art performance on both SUN\nRGB-D and ScanNet V2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yao Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yuqing Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Renjiao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinwang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying multi-angled parallelism to Spanish topographical maps. (arXiv:2203.01169v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01169","description":"<p>Multi-Angled Parallelism (MAP) is a method to recognize lines in binary\nimages. It is suitable to be implemented in parallel processing and image\nprocessing hardware. The binary image is transformed into directional planes,\nupon which, directional operators of erosion-dilation are iteratively applyed.\nFrom a set of basic operators, more complex ones are created, which let to\nextract the several types of lines. Each type is extracted with a different set\nof operations and so the lines are identified when extracted. In this paper, an\noverview of MAP is made, and it is adapted to line recognition in Spanish\ntopographical maps, with the double purpose of testing the method in a real\ncase and studying the process of adapting it to a custom application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cusco_J/0/1/0/all/0/1\">Josep-Maria Cusco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Adversarial Perturbations in Multi-Task Perception. (arXiv:2203.01177v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01177","description":"<p>While deep neural networks (DNNs) achieve impressive performance on\nenvironment perception tasks, their sensitivity to adversarial perturbations\nlimits their use in practical applications. In this paper, we (i) propose a\nnovel adversarial perturbation detection scheme based on multi-task perception\nof complex vision tasks (i.e., depth estimation and semantic segmentation).\nSpecifically, adversarial perturbations are detected by inconsistencies between\nextracted edges of the input image, the depth output, and the segmentation\noutput. To further improve this technique, we (ii) develop a novel edge\nconsistency loss between all three modalities, thereby improving their initial\nconsistency which in turn supports our detection scheme. We verify our\ndetection scheme's effectiveness by employing various known attacks and image\nnoises. In addition, we (iii) develop a multi-task adversarial attack, aiming\nat fooling both tasks as well as our detection scheme. Experimental evaluation\non the Cityscapes and KITTI datasets shows that under an assumption of a 5%\nfalse positive rate up to 100% of images are correctly detected as\nadversarially perturbed, depending on the strength of the perturbation. Code\nwill be available on github. A short video at https://youtu.be/KKa6gOyWmH4\nprovides qualitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klingner_M/0/1/0/all/0/1\">Marvin Klingner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1\">Andreas B&#xe4;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fingscheidt_T/0/1/0/all/0/1\">Tim Fingscheidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Robust Ground Surface Estimation from LIDAR Measurements using Uniform B-Splines. (arXiv:2203.01180v1 [cs.RO])","link":"http://arxiv.org/abs/2203.01180","description":"<p>We propose a fast and robust method to estimate the ground surface from LIDAR\nmeasurements on an automated vehicle. The ground surface is modeled as a UBS\nwhich is robust towards varying measurement densities and with a single\nparameter controlling the smoothness prior. We model the estimation process as\na robust LS optimization problem which can be reformulated as a linear problem\nand thus solved efficiently. Using the SemanticKITTI data set, we conduct a\nquantitative evaluation by classifying the point-wise semantic annotations into\nground and non-ground points. Finally, we validate the approach on our research\nvehicle in real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wirges_S/0/1/0/all/0/1\">Sascha Wirges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosch_K/0/1/0/all/0/1\">Kevin R&#xf6;sch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Feature Encoding for GNNs on Road Networks. (arXiv:2203.01187v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01187","description":"<p>In this work, we present a novel approach to learning an encoding of visual\nfeatures into graph neural networks with the application on road network data.\nWe propose an architecture that combines state-of-the-art vision backbone\nnetworks with graph neural networks. More specifically, we perform a road type\nclassification task on an Open Street Map road network through encoding of\nsatellite imagery using various ResNet architectures. Our architecture further\nenables fine-tuning and a transfer-learning approach is evaluated by\npretraining on the NWPU-RESISC45 image classification dataset for remote\nsensing and comparing them to purely ImageNet-pretrained ResNet models as\nvisual feature encoders. The results show not only that the visual feature\nencoders are superior to low-level visual features, but also that the\nfine-tuning of the visual feature encoder to a general remote sensing dataset\nsuch as NWPU-RESISC45 can further improve the performance of a GNN on a machine\nlearning task like road type classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_A/0/1/0/all/0/1\">Alireza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization of Deep Networks for Estimating Physical Properties of Containers and Fillings. (arXiv:2203.01192v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01192","description":"<p>We present methods to estimate the physical properties of household\ncontainers and their fillings manipulated by humans. We use a lightweight,\npre-trained convolutional neural network with coordinate attention as a\nbackbone model of the pipelines to accurately locate the object of interest and\nestimate the physical properties in the CORSMAL Containers Manipulation (CCM)\ndataset. We address the filling type classification with audio data and then\ncombine this information from audio with video modalities to address the\nfilling level classification. For the container capacity, dimension, and mass\nestimation, we present a data augmentation and consistency measurement to\nalleviate the over-fitting issue in the CCM dataset caused by the limited\nnumber of containers. We augment the training data using an\nobject-of-interest-based re-scaling that increases the variety of physical\nvalues of the containers. We then perform the consistency measurement to choose\na model with low prediction variance in the same containers under different\nscenes, which ensures the generalization ability of the model. Our method\nimproves the generalization ability of the models to estimate the property of\nthe containers that were not previously seen in the training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chaoran Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changjae Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAE-iForest: Auto-encoding Reconstruction and Isolation-based Anomalies Detecting Fallen Objects on Road Surface. (arXiv:2203.01193v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01193","description":"<p>In road monitoring, it is an important issue to detect changes in the road\nsurface at an early stage to prevent damage to third parties. The target of the\nfalling object may be a fallen tree due to the external force of a flood or an\nearthquake, and falling rocks from a slope. Generative deep learning is\npossible to flexibly detect anomalies of the falling objects on the road\nsurface. We prototype a method that combines auto-encoding reconstruction and\nisolation-based anomaly detector in application for road surface monitoring.\nActually, we apply our method to a set of test images that fallen objects is\nlocated on the raw inputs added with fallen stone and plywood, and that snow is\ncovered on the winter road. Finally we mention the future works for practical\npurpose application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasuno_T/0/1/0/all/0/1\">Takato Yasuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_J/0/1/0/all/0/1\">Junichiro Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogata_R/0/1/0/all/0/1\">Riku Ogata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okano_M/0/1/0/all/0/1\">Masahiro Okano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Container Localisation and Mass Estimation with an RGB-D Camera. (arXiv:2203.01207v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01207","description":"<p>In the research area of human-robot interactions, the automatic estimation of\nthe mass of a container manipulated by a person leveraging only visual\ninformation is a challenging task. The main challenges consist of occlusions,\ndifferent filling materials and lighting conditions. The mass of an object\nconstitutes key information for the robot to correctly regulate the force\nrequired to grasp the container. We propose a single RGB-D camera-based method\nto locate a manipulated container and estimate its empty mass i.e.,\nindependently of the presence of the content. The method first automatically\nselects a number of candidate containers based on the distance with the fixed\nfrontal view, then averages the mass predictions of a lightweight model to\nprovide the final estimation. Results on the CORSMAL Containers Manipulation\ndataset show that the proposed method estimates empty container mass obtaining\na score of 71.08% under different lighting or filling conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Apicella_T/0/1/0/all/0/1\">Tommaso Apicella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slavic_G/0/1/0/all/0/1\">Giulia Slavic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragusa_E/0/1/0/all/0/1\">Edoardo Ragusa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gastaldo_P/0/1/0/all/0/1\">Paolo Gastaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcenaro_L/0/1/0/all/0/1\">Lucio Marcenaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A simple and universal rotation equivariant point-cloud network. (arXiv:2203.01216v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01216","description":"<p>Equivariance to permutations and rigid motions is an important inductive bias\nfor various 3D learning problems. Recently it has been shown that the\nequivariant Tensor Field Network architecture is universal -- it can\napproximate any equivariant function. In this paper we suggest a much simpler\narchitecture, prove that it enjoys the same universality guarantees and\nevaluate its performance on Modelnet40. The code to reproduce our experiments\nis available at \\url{https://github.com/simpleinvariance/UniversalNetwork}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finkelshtein_B/0/1/0/all/0/1\">Ben Finkelshtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1\">Chaim Baskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1\">Nadav Dym</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Tracker with Pixel and Instance for Video Panoptic Segmentation. (arXiv:2203.01217v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01217","description":"<p>Video Panoptic Segmentation (VPS) requires generating consistent panoptic\nsegmentation and tracking identities to all pixels across video frames.\nExisting methods are mainly based on the trained instance embedding to maintain\nconsistent panoptic segmentation. However, they inevitably struggle to cope\nwith the challenges of small objects, similar appearance but inconsistent\nidentities, occlusion, and strong instance contour deformations. To address\nthese problems, we present HybridTracker, a lightweight and joint tracking\nmodel attempting to eliminate the limitations of the single tracker.\nHybridTracker performs pixel tracker and instance tracker in parallel to obtain\nthe association matrices, which are fused into a matching matrix. In the\ninstance tracker, we design a differentiable matching layer, ensuring the\nstability of inter-frame matching. In the pixel tracker, we compute the dice\ncoefficient of the same instance of different frames given the estimated\noptical flow, forming the Intersection Over Union (IoU) matrix. We additionally\npropose mutual check and temporal consistency constraints during inference to\nsettle the occlusion and contour deformation challenges. Extensive experiments\ndemonstrate that HybridTracker outperforms state-of-the-art methods on\nCityscapes-VPS and VIPER datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Weicai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xinyue Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Ge Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Question Answering: Datasets, Algorithms and Challenges. (arXiv:2203.01225v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01225","description":"<p>Video Question Answering (VideoQA) aims to answer natural language questions\naccording to the given videos. It has earned increasing attention with recent\nresearch trends in joint vision and language understanding. Yet, compared with\nImageQA, VideoQA is largely underexplored and progresses slowly. Although\ndifferent algorithms have continually been proposed and shown success on\ndifferent VideoQA datasets, we find that there lacks a meaningful survey to\ncategorize them, which seriously impedes its advancements. This paper thus\nprovides a clear taxonomy and comprehensive analyses to VideoQA, focusing on\nthe datasets, algorithms, and unique challenges. We then point out the research\ntrend of studying beyond factoid QA to inference QA towards the cognition of\nvideo contents, Finally, we conclude some promising directions for future\nexploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yaoyao Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Junbin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yicong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable IFS Fractals. (arXiv:2203.01231v1 [cs.GR])","link":"http://arxiv.org/abs/2203.01231","description":"<p>I present my explorations in rendering Iterated Function System (IFS)\nfractals using a differentiable rendering pipeline. Differentiable rendering is\na recent innovation at the intersection of graphics and machine learning. This\nopens up many possibilities for generating fractals that meet particular\ncriteria. In this paper I show how my method can be used to generate an IFS\nfractal that resembles a target image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Cory Braker Scott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"H4D: Human 4D Modeling by Learning Neural Compositional Representation. (arXiv:2203.01247v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01247","description":"<p>Despite the impressive results achieved by deep learning based 3D\nreconstruction, the techniques of directly learning to model the 4D human\ncaptures with detailed geometry have been less studied. This work presents a\nnovel framework that can effectively learn a compact and compositional\nrepresentation for dynamic human by exploiting the human body prior from the\nwidely-used SMPL parametric model. Particularly, our representation, named H4D,\nrepresents dynamic 3D human over a temporal span into the latent spaces\nencoding shape, initial pose, motion and auxiliary information. A simple yet\neffective linear motion model is proposed to provide a rough and regularized\nmotion estimation, followed by per-frame compensation for pose and geometry\ndetails with the residual encoded in the auxiliary code. Technically, we\nintroduce novel GRU-based architectures to facilitate learning and improve the\nrepresentation capability. Extensive experiments demonstrate our method is not\nonly efficacy in recovering dynamic human with accurate motion and detailed\ngeometry, but also amenable to various 4D human related tasks, including motion\nretargeting, motion completion and future prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Boyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingkui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Query-based Paradigm for Point Cloud Understanding. (arXiv:2203.01252v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01252","description":"<p>3D point cloud understanding is an important component in autonomous driving\nand robotics. In this paper, we present a novel Embedding-Querying paradigm\n(EQ-Paradigm) for 3D understanding tasks including detection, segmentation and\nclassification. EQ-Paradigm is a unified paradigm that enables the combination\nof any existing 3D backbone architectures with different task heads. Under the\nEQ-Paradigm, the input is firstly encoded in the embedding stage with an\narbitrary feature extraction architecture, which is independent of tasks and\nheads. Then, the querying stage enables the encoded features to be applicable\nfor diverse task heads. This is achieved by introducing an intermediate\nrepresentation, i.e., Q-representation, in the querying stage to serve as a\nbridge between the embedding stage and task heads. We design a novel Q-Net as\nthe querying stage network. Extensive experimental results on various 3D tasks\nshow that EQ-Paradigm in tandem with Q-Net is a general and effective pipeline,\nwhich enables a flexible collaboration of backbones and heads, and further\nboosts the performance of the state-of-the-art methods. All codes and models\nwill be published soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zetong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yanan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Transformer for Deepfake Detection. (arXiv:2203.01265v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01265","description":"<p>The fast evolution and widespread of deepfake techniques in real-world\nscenarios require stronger generalization abilities of face forgery detectors.\nSome works capture the features that are unrelated to method-specific\nartifacts, such as clues of blending boundary, accumulated up-sampling, to\nstrengthen the generalization ability. However, the effectiveness of these\nmethods can be easily corrupted by post-processing operations such as\ncompression. Inspired by transfer learning, neural networks pre-trained on\nother large-scale face-related tasks may provide useful features for deepfake\ndetection. For example, lip movement has been proved to be a kind of robust and\ngood-transferring highlevel semantic feature, which can be learned from the\nlipreading task. However, the existing method pre-trains the lip feature\nextraction model in a supervised manner, which requires plenty of human\nresources in data annotation and increases the difficulty of obtaining training\ndata. In this paper, we propose a self-supervised transformer based\naudio-visual contrastive learning method. The proposed method learns mouth\nmotion representations by encouraging the paired video and audio\nrepresentations to be close while unpaired ones to be diverse. After\npre-training with our method, the model will then be partially fine-tuned for\ndeepfake detection task. Extensive experiments show that our self-supervised\nmethod performs comparably or even better than the supervised pre-training\ncounterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Temporal Interpolation of Radar-based Precipitation. (arXiv:2203.01277v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01277","description":"<p>When providing the boundary conditions for hydrological flood models and\nestimating the associated risk, interpolating precipitation at very high\ntemporal resolutions (e.g. 5 minutes) is essential not to miss the cause of\nflooding in local regions. In this paper, we study optical flow-based\ninterpolation of globally available weather radar images from satellites. The\nproposed approach uses deep neural networks for the interpolation of multiple\nvideo frames, while terrain information is combined with temporarily\ncoarse-grained precipitation radar observation as inputs for self-supervised\ntraining. An experiment with the Meteonet radar precipitation dataset for the\nflood risk simulation in Aude, a department in Southern France (2018),\ndemonstrated the advantage of the proposed method over a linear interpolation\nbaseline, with up to 20% error reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tatsubori_M/0/1/0/all/0/1\">Michiaki Tatsubori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moriyama_T/0/1/0/all/0/1\">Takao Moriyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_T/0/1/0/all/0/1\">Tatsuya Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraccaro_P/0/1/0/all/0/1\">Paolo Fraccaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Anne Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_B/0/1/0/all/0/1\">Blair Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehnert_J/0/1/0/all/0/1\">Julian Kuehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remy_S/0/1/0/all/0/1\">Sekou L. Remy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADVISE: ADaptive Feature Relevance and VISual Explanations for Convolutional Neural Networks. (arXiv:2203.01289v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01289","description":"<p>To equip Convolutional Neural Networks (CNNs) with explainability, it is\nessential to interpret how opaque models take specific decisions, understand\nwhat causes the errors, improve the architecture design, and identify unethical\nbiases in the classifiers. This paper introduces ADVISE, a new explainability\nmethod that quantifies and leverages the relevance of each unit of the feature\nmap to provide better visual explanations. To this end, we propose using\nadaptive bandwidth kernel density estimation to assign a relevance score to\neach unit of the feature map with respect to the predicted class. We also\npropose an evaluation protocol to quantitatively assess the visual\nexplainability of CNN models. We extensively evaluate our idea in the image\nclassification task using AlexNet, VGG16, ResNet50, and Xception pretrained on\nImageNet. We compare ADVISE with the state-of-the-art visual explainable\nmethods and show that the proposed method outperforms competing approaches in\nquantifying feature-relevance and visual explainability while maintaining\ncompetitive time complexity. Our experiments further show that ADVISE fulfils\nthe sensitivity and implementation independence axioms while passing the sanity\nchecks. The implementation is accessible for reproducibility purposes on\nhttps://github.com/dehshibi/ADVISE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehshibi_M/0/1/0/all/0/1\">Mohammad Mahdi Dehshibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashtari_Majlan_M/0/1/0/all/0/1\">Mona Ashtari-Majlan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adhane_G/0/1/0/all/0/1\">Gereziher Adhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masip_D/0/1/0/all/0/1\">David Masip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement. (arXiv:2203.01296v1 [eess.IV])","link":"http://arxiv.org/abs/2203.01296","description":"<p>Low-Light Image Enhancement is a computer vision task which intensifies the\ndark images to appropriate brightness. It can also be seen as an ill-posed\nproblem in image restoration domain. With the success of deep neural networks,\nthe convolutional neural networks surpass the traditional algorithm-based\nmethods and become the mainstream in the computer vision area. To advance the\nperformance of enhancement algorithms, we propose an image enhancement network\n(HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use\na half wavelet attention block on M-Net+ to enrich the features from wavelet\ndomain. Furthermore, our HWMNet has competitive performance results on two\nimage enhancement datasets in terms of quantitative metrics and visual quality.\nThe source code and pretrained model are available at\nhttps://github.com/FanChiMao/HWMNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_C/0/1/0/all/0/1\">Chi-Mao Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tsung-Jung Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kuan-Hsien Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01305","description":"<p>We present in this paper a novel denoising training method to speedup DETR\n(DEtection TRansformer) training and offer a deepened understanding of the slow\nconvergence issue of DETR-like methods. We show that the slow convergence\nresults from the instability of bipartite graph matching which causes\ninconsistent optimization goals in early training stages. To address this\nissue, except for the Hungarian loss, our method additionally feeds\nground-truth bounding boxes with noises into Transformer decoder and trains the\nmodel to reconstruct the original boxes, which effectively reduces the\nbipartite graph matching difficulty and leads to a faster convergence. Our\nmethod is universal and can be easily plugged into any DETR-like methods by\nadding dozens of lines of code to achieve a remarkable improvement. As a\nresult, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the\nsame setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and\n$50$ epochs of training respectively) among DETR-like methods with ResNet-$50$\nbackbone. Compared with the baseline under the same setting, DN-DETR achieves\ncomparable performance with $50\\%$ training epochs. Code is available at\n\\url{https://github.com/FengLi-ust/DN-DETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v1 [cs.LG])","link":"http://arxiv.org/abs/2203.01311","description":"<p>Learning multimodal representations involves discovering correspondences and\nintegrating information from multiple heterogeneous sources of data. While\nrecent research has begun to explore the design of more general-purpose\nmultimodal models (contrary to prior focus on domain and modality-specific\narchitectures), these methods are still largely focused on a small set of\nmodalities in the language, vision, and audio space. In order to accelerate\ngeneralization towards diverse and understudied modalities, we investigate\nmethods for high-modality (a large set of diverse modalities) and\npartially-observable (each task only defined on a small subset of modalities)\nscenarios. To tackle these challenges, we design a general multimodal model\nthat enables multitask and transfer learning: multitask learning with shared\nparameters enables stable parameter counts (addressing scalability), and\ncross-modal transfer learning enables information sharing across modalities and\ntasks (addressing partial observability). Our resulting model generalizes\nacross text, image, video, audio, time-series, sensors, tables, and set\nmodalities from different research areas, improves the tradeoff between\nperformance and efficiency, transfers to new modalities and tasks, and reveals\nsurprising insights on the nature of information sharing in multitask models.\nWe release our code and benchmarks which we hope will present a unified\nplatform for subsequent theoretical and empirical analysis:\nhttps://github.com/pliang279/HighMMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shengtong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protecting Celebrities with Identity Consistency Transformer. (arXiv:2203.01318v1 [cs.CV])","link":"http://arxiv.org/abs/2203.01318","description":"<p>In this work we propose Identity Consistency Transformer, a novel face\nforgery detection method that focuses on high-level semantics, specifically\nidentity information, and detecting a suspect face by finding identity\ninconsistency in inner and outer face regions. The Identity Consistency\nTransformer incorporates a consistency loss for identity consistency\ndetermination. We show that Identity Consistency Transformer exhibits superior\ngeneralization ability not only across different datasets but also across\nvarious types of image degradation forms found in real-world applications\nincluding deepfake videos. The Identity Consistency Transformer can be easily\nenhanced with additional identity information when such information is\navailable, and for this reason it is especially well-suited for detecting face\nforgeries involving celebrities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Methods and Applications for Region of Interest Detection in Dermoscopic Images. (arXiv:1807.10711v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1807.10711","description":"<p>Rapid growth in the development of medical imaging analysis technology has\nbeen propelled by the great interest in improving computer-aided diagnosis and\ndetection (CAD) systems for three popular image visualization tasks:\nclassification, segmentation, and Region of Interest (ROI) detection. However,\na limited number of datasets with ground truth annotations are available for\ndeveloping segmentation and ROI detection of lesions, as expert annotations are\nlaborious and expensive. Detecting the ROI is vital to locate lesions\naccurately. In this paper, we propose the use of two deep object detection\nmeta-architectures (Faster R-CNN Inception-V2 and SSD Inception-V2) to develop\nrobust ROI detection of skin lesions in dermoscopic datasets (2017 ISIC\nChallenge, PH2, and HAM10000), and compared the performance with\nstate-of-the-art segmentation algorithm (DeeplabV3+). To further demonstrate\nthe potential of our work, we built a smartphone application for real-time\nautomated detection of skin lesions based on this methodology. In addition, we\ndeveloped an automated natural data-augmentation method from ROI detection to\nproduce augmented copies of dermoscopic images, as a pre-processing step in the\nsegmentation of skin lesions to further improve the performance of the current\nstate-of-the-art deep learning algorithm. Our proposed ROI detection has the\npotential to more appropriately streamline dermatology referrals and reduce\nunnecessary biopsies in the diagnosis of skin cancer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_M/0/1/0/all/0/1\">Manu Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanpour_S/0/1/0/all/0/1\">Saeed Hassanpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ricci Curvature Based Volumetric Segmentation of the Auditory Ossicles. (arXiv:2006.14788v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.14788","description":"<p>The auditory ossicles that are located in the middle ear are the smallest\nbones in the human body. Their damage will result in hearing loss. It is\ntherefore important to be able to automatically diagnose ossicles' diseases\nbased on Computed Tomography (CT) 3D imaging. However CT images usually include\nthe whole head area, which is much larger than the bones of interest, thus the\nlocalization of the ossicles, followed by segmentation, both play a significant\nrole in automatic diagnosis. The commonly employed local segmentation methods\nrequire manually selected initial points, which is a highly time consuming\nprocess. We therefore propose a completely automatic method to locate the\nossicles which requires neither templates, nor manual labels. It relies solely\non the connective properties of the auditory ossicles themselves, and their\nrelationship with the surrounding tissue fluid. For the segmentation task, we\ndefine a novel energy function and obtain the shape of the ossicles from the 3D\nCT image by minimizing this new energy. Compared to the state-of-the-art\nmethods which usually use the gradient operator and some normalization terms,\nwe propose to add a Ricci curvature term to the commonly employed energy\nfunction. We compare our proposed method with the state-of-the-art methods and\nshow that the performance of discrete Forman-Ricci curvature is superior to the\nothers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_N/0/1/0/all/0/1\">Na Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jisui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yuxue Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saucan_E/0/1/0/all/0/1\">Emil Saucan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenchang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manipulation-Oriented Object Perception in Clutter through Affordance Coordinate Frames. (arXiv:2010.08202v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2010.08202","description":"<p>In order to enable robust operation in unstructured environments, robots\nshould be able to generalize manipulation actions to novel object instances.\nFor example, to pour and serve a drink, a robot should be able to recognize\nnovel containers which afford the task. Most importantly, robots should be able\nto manipulate these novel containers to fulfill the task. To achieve this, we\naim to provide robust and generalized perception of object affordances and\ntheir associated manipulation poses for reliable manipulation. In this work, we\ncombine the notions of affordance and category-level pose, and introduce the\nAffordance Coordinate Frame (ACF). With ACF, we represent each object class in\nterms of individual affordance parts and the compatibility between them, where\neach part is associated with a part category-level pose for robot manipulation.\nIn our experiments, we demonstrate that ACF outperforms state-of-the-art\nmethods for object detection, as well as category-level pose estimation for\nobject parts. We further demonstrate the applicability of ACF to robot\nmanipulation tasks through experiments in a simulated environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaotong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaizhi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Shreshtha Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooney_J/0/1/0/all/0/1\">James Cooney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlasek_J/0/1/0/all/0/1\">Jana Pavlasek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_O/0/1/0/all/0/1\">Odest Chadwicke Jenkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Efficient GANs for Image Translation via Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.08382","description":"<p>Generative Adversarial Networks (GANs) have been widely-used in image\ntranslation, but their high computation and storage costs impede the deployment\non mobile devices. Prevalent methods for CNN compression cannot be directly\napplied to GANs due to the peculiarties of GAN tasks and the unstable\nadversarial training. To solve these, in this paper, we introduce a novel GAN\ncompression method, termed DMAD, by proposing a Differentiable Mask and a\nco-Attention Distillation. The former searches for a light-weight generator\narchitecture in a training-adaptive manner. To overcome channel inconsistency\nwhen pruning the residual connections, an adaptive cross-block group sparsity\nis further incorporated. The latter simultaneously distills informative\nattention maps from both the generator and discriminator of a pre-trained model\nto the searched generator, effectively stabilizing the adversarial training of\nour light-weight model. Experiments show that DMAD can reduce the Multiply\nAccumulate Operations (MACs) of CycleGAN by 13x and that of Pix2Pix by 4x while\nretaining a comparable performance against the full model. Our code can be\navailable at https://github.com/SJLeo/DMAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05567","description":"<p>Model explanations such as saliency maps can improve user trust in AI by\nhighlighting important features for a prediction. However, these become\ndistorted and misleading when explaining predictions of images that are subject\nto systematic error (bias) by perturbations and corruptions. Furthermore, the\ndistortions persist despite model fine-tuning on images biased by different\nfactors (blur, color temperature, day/night). We present Debiased-CAM to\nrecover explanation faithfulness across various bias types and levels by\ntraining a multi-input, multi-task model with auxiliary tasks for explanation\nand bias level predictions. In simulation studies, the approach not only\nenhanced prediction accuracy, but also generated highly faithful explanations\nabout these predictions as if the images were unbiased. In user studies,\ndebiased explanations improved user task performance, perceived truthfulness\nand perceived helpfulness. Debiased training can provide a versatile platform\nfor robust performance and explanation faithfulness for a wide range of\napplications with data biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wencan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1\">Brian Y. Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Class-Agnostic Pseudo Mask Generation for Box-Supervised Semantic Segmentation. (arXiv:2103.05463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05463","description":"<p>Recently, several weakly supervised learning methods have been devoted to\nutilize bounding box supervision for training deep semantic segmentation\nmodels. Most existing methods usually leverage the generic proposal generators\n(e.g., dense CRF and MCG) to produce enhanced segmentation masks for further\ntraining segmentation models. These proposal generators, however, are generic\nand not specifically designed for box-supervised semantic segmentation, thereby\nleaving some leeway for improving segmentation performance. In this paper, we\naim at seeking for a more accurate learning-based class-agnostic pseudo mask\ngenerator tailored to box-supervised semantic segmentation. To this end, we\nresort to a pixel-level annotated auxiliary dataset where the class labels are\nnon-overlapped with those of the box-annotated dataset. For learning pseudo\nmask generator from the auxiliary dataset, we present a bi-level optimization\nformulation. In particular, the lower subproblem is used to learn\nbox-supervised semantic segmentation, while the upper subproblem is used to\nlearn an optimal class-agnostic pseudo mask generator. The learned pseudo\nsegmentation mask generator can then be deployed to the box-annotated dataset\nfor improving weakly supervised semantic segmentation. Experiments on PASCAL\nVOC 2012 dataset show that the learned pseudo mask generator is effective in\nboosting segmentation performance, and our method can further close the\nperformance gap between box-supervised and fully-supervised models. Our code\nwill be made publicly available at\nhttps://github.com/Vious/LPG_BBox_Segmentation .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chaohao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepChange: A Large Long-Term Person Re-Identification Benchmark with Clothes Change. (arXiv:2105.14685v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14685","description":"<p>Existing person re-identification (re-id) works mostly consider short-term\napplication scenarios without clothes change. In real-world, however, we often\ndress differently across space and time. To solve this contrast, a few recent\nattempts have been made on long-term re-id with clothes change. Currently, one\nof the most significant limitations in this field is the lack of a large\nrealistic benchmark. In this work, we contribute a large, realistic long-term\nperson re-identification benchmark, named as DeepChange. It has several unique\ncharacteristics: (1) Realistic and rich personal appearance (e.g., clothes and\nhair style) and variations: Highly diverse clothes change and styles, with\nvarying reappearing gaps in time from minutes to seasons, different weather\nconditions (e.g., sunny, cloudy, windy, rainy, snowy, extremely cold) and\nevents (e.g., working, leisure, daily activities). (2) Rich camera setups: Raw\nvideos were recorded by 17 outdoor varying resolution cameras operating in a\nreal-world surveillance system. (3) The currently largest number of (17)\ncameras, (1, 121) identities, and (178, 407) bounding boxes, over the longest\ntime span (12 months). Further, we investigate multimodal fusion strategies for\ntackling the clothes change challenge. Extensive experiments show that our\nfusion models outperform a wide variety of state-of-the-art models on\nDeepChange. Our dataset and documents are available at\nhttps://github.com/PengBoXiangShang/deepchange.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse PointPillars: Maintaining and Exploiting Input Sparsity to Improve Runtime on Embedded Systems. (arXiv:2106.06882v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06882","description":"<p>Bird's Eye View (BEV) is a popular representation for processing 3D point\nclouds, and by its nature is fundamentally sparse. Motivated by the\ncomputational limitations of mobile robot platforms, we create a fast,\nhigh-performance BEV 3D object detector that maintains and exploits this input\nsparsity to decrease runtimes over non-sparse baselines and avoids the tradeoff\nbetween pseudoimage area and runtime. We present results on KITTI, a canonical\n3D detection dataset, and Matterport-Chair, a novel Matterport3D-derived chair\ndetection dataset from scenes in real furnished homes. We evaluate runtime\ncharacteristics using a desktop GPU, an embedded ML accelerator, and a robot\nCPU, demonstrating that our method results in significant detection speedups\n(2X or more) for embedded systems with only a modest decrease in detection\nquality. Our work represents a new approach for practitioners to optimize\nmodels for embedded systems by maintaining and exploiting input sparsity\nthroughout their entire pipeline to reduce runtime and resource usage while\npreserving detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vedder_K/0/1/0/all/0/1\">Kyle Vedder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1\">Eric Eaton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration. (arXiv:2107.02433v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02433","description":"<p>In order to tackle the difficulty associated with the ill-posed nature of the\nimage registration problem, regularization is often used to constrain the\nsolution space. For most learning-based registration approaches, the\nregularization usually has a fixed weight and only constrains the spatial\ntransformation. Such convention has two limitations: (i) Besides the laborious\ngrid search for the optimal fixed weight, the regularization strength of a\nspecific image pair should be associated with the content of the images, thus\nthe \"one value fits all\" training scheme is not ideal; (ii) Only spatially\nregularizing the transformation may neglect some informative clues related to\nthe ill-posedness. In this study, we propose a mean-teacher based registration\nframework, which incorporates an additional temporal consistency regularization\nterm by encouraging the teacher model's prediction to be consistent with that\nof the student model. More importantly, instead of searching for a fixed\nweight, the teacher enables automatically adjusting the weights of the spatial\nregularization and the temporal consistency regularization by taking advantage\nof the transformation uncertainty and appearance uncertainty. Extensive\nexperiments on the challenging abdominal CT-MRI registration show that our\ntraining strategy can promisingly advance the original learning-based method in\nterms of efficient hyperparameter tuning and a better tradeoff between accuracy\nand smoothness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Donghuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiangpeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frisken_S/0/1/0/all/0/1\">Sarah Frisken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_J/0/1/0/all/0/1\">Jayender Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William Wells III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Raymond Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Local-Global Contextual Adaptation for Multi-Person Pose Estimation. (arXiv:2109.03622v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03622","description":"<p>This paper studies the problem of multi-person pose estimation in a bottom-up\nfashion. With a new and strong observation that the localization issue of the\ncenter-offset formulation can be remedied in a local-window search scheme in an\nideal situation, we propose a multi-person pose estimation approach, dubbed as\nLOGO-CAP, by learning the LOcal-GlObal Contextual Adaptation for human Pose.\nSpecifically, our approach learns the keypoint attraction maps (KAMs) from the\nlocal keypoints expansion maps (KEMs) in small local windows in the first step,\nwhich are subsequently treated as dynamic convolutional kernels on the\nkeypoints-focused global heatmaps for contextual adaptation, achieving accurate\nmulti-person pose estimation. Our method is end-to-end trainable with near\nreal-time inference speed in a single forward pass, obtaining state-of-the-art\nperformance on the COCO keypoint benchmark for bottom-up human pose estimation.\nWith the COCO trained model, our method also outperforms prior arts by a large\nmargin on the challenging OCHuman dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionHint: Self-Supervised Monocular Visual Odometry with Motion Constraints. (arXiv:2109.06768v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06768","description":"<p>We present a novel self-supervised algorithm named MotionHint for monocular\nvisual odometry (VO) that takes motion constraints into account. A key aspect\nof our approach is to use an appropriate motion model that can help existing\nself-supervised monocular VO (SSM-VO) algorithms to overcome issues related to\nthe local minima within their self-supervised loss functions. The motion model\nis expressed with a neural network named PPnet. It is trained to coarsely\npredict the next pose of the camera and the uncertainty of this prediction. Our\nself-supervised approach combines the original loss and the motion loss, which\nis the weighted difference between the prediction and the generated ego-motion.\nTaking two existing SSM-VO systems as our baseline, we evaluate our MotionHint\nalgorithm on the standard KITTI benchmark. Experimental results show that our\nMotionHint algorithm can be easily applied to existing open-sourced\nstate-of-the-art SSM-VO systems to greatly improve the performance by reducing\nthe resulting ATE by up to 28.73%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Ping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PnP-DETR: Towards Efficient Visual Analysis with Transformers. (arXiv:2109.07036v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07036","description":"<p>Recently, DETR pioneered the solution of vision tasks with transformers, it\ndirectly translates the image feature map into the object detection result.\nThough effective, translating the full feature map can be costly due to\nredundant computation on some area like the background. In this work, we\nencapsulate the idea of reducing spatial redundancy into a novel poll and pool\n(PnP) sampling module, with which we build an end-to-end PnP-DETR architecture\nthat adaptively allocates its computation spatially to be more efficient.\nConcretely, the PnP module abstracts the image feature map into fine foreground\nobject feature vectors and a small number of coarse background contextual\nfeature vectors. The transformer models information interaction within the\nfine-coarse feature space and translates the features into the detection\nresult. Moreover, the PnP-augmented model can instantly achieve various desired\ntrade-offs between performance and computation with a single model by varying\nthe sampled feature length, without requiring to train multiple models as\nexisting methods. Thus it offers greater flexibility for deployment in diverse\nscenarios with varying computation constraint. We further validate the\ngeneralizability of the PnP module on panoptic segmentation and the recent\ntransformer-based image recognition model ViT and show consistent efficiency\ngain. We believe our method makes a step for efficient visual analysis with\ntransformers, wherein spatial redundancy is commonly observed. Code will be\navailable at \\url{https://github.com/twangnh/pnp-detr}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S3LAM: Structured Scene SLAM. (arXiv:2109.07339v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.07339","description":"<p>We propose a new SLAM system that uses the semantic segmentation of objects\nand structures in the scene. Semantic information is relevant as it contains\nhigh level information which may make SLAM more accurate and robust. Our\ncontribution is twofold: i) A new SLAM system based on ORB-SLAM2 that creates a\nsemantic map made of clusters of points corresponding to objects instances and\nstructures in the scene. ii) A modification of the classical Bundle Adjustment\nformulation to constrain each cluster using geometrical priors, which improves\nboth camera localization and reconstruction and enables a better understanding\nof the scene. We evaluate our approach on sequences from several public\ndatasets and show that it improves camera pose estimation with respect to state\nof the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mathieu Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchand_E/0/1/0/all/0/1\">Eric Marchand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacete_A/0/1/0/all/0/1\">Amine Kacete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royan_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Royan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Human Pose Triangulation. (arXiv:2110.00280v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00280","description":"<p>We address the problem of generalizability for multi-view 3D human pose\nestimation. The standard approach is to first detect 2D keypoints in images and\nthen apply triangulation from multiple views. Even though the existing methods\nachieve remarkably accurate 3D pose estimation on public benchmarks, most of\nthem are limited to a single spatial camera arrangement and their number.\nSeveral methods address this limitation but demonstrate significantly degraded\nperformance on novel views. We propose a stochastic framework for human pose\ntriangulation and demonstrate a superior generalization across different camera\narrangements on two public datasets. In addition, we apply the same approach to\nthe fundamental matrix estimation problem, showing that the proposed method can\nsuccessfully apply to other computer vision problems. The stochastic framework\nachieves more than 8.8% improvement on the 3D pose estimation task, compared to\nthe state-of-the-art, and more than 30% improvement for fundamental matrix\nestimation, compared to a standard algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartol_K/0/1/0/all/0/1\">Kristijan Bartol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanic_D/0/1/0/all/0/1\">David Bojani&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_T/0/1/0/all/0/1\">Tomislav Petkovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pribanic_T/0/1/0/all/0/1\">Tomislav Pribani&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04627","description":"<p>Pretraining language models with next-token prediction on massive text\ncorpora has delivered phenomenal zero-shot, few-shot, transfer learning and\nmulti-tasking capabilities on both generative and discriminative language\ntasks. Motivated by this success, we explore a Vector-quantized Image Modeling\n(VIM) approach that involves pretraining a Transformer to predict rasterized\nimage tokens autoregressively. The discrete image tokens are encoded from a\nlearned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple\nimprovements over vanilla VQGAN from architecture to codebook learning,\nyielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN\nfurther improves vector-quantized image modeling tasks, including\nunconditional, class-conditioned image generation and unsupervised\nrepresentation learning. When trained on ImageNet at 256x256 resolution, we\nachieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of\n4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and\n17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised\npretraining, we further evaluate the pretrained Transformer by averaging\nintermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained\nVIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 72.2%\nfor a similar model size. ViM-L also outperforms iGPT-XL which is trained with\nextra web image data and larger model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_A/0/1/0/all/0/1\">Alexander Ku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Optimal Correlational Object Search. (arXiv:2110.09991v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2110.09991","description":"<p>In realistic applications of object search, robots will need to locate target\nobjects in complex environments while coping with unreliable sensors,\nespecially for small or hard-to-detect objects. In such settings, correlational\ninformation can be valuable for planning efficiently. Previous approaches that\nconsider correlational information typically resort to ad-hoc, greedy search\nstrategies. We introduce the Correlational Object Search POMDP (COS-POMDP),\nwhich models correlations while preserving optimal solutions with a reduced\nstate space. We propose a hierarchical planning algorithm to scale up\nCOS-POMDPs for practical domains. Our evaluation, conducted with the AI2-THOR\nhousehold simulator and the YOLOv5 object detector, shows that our method finds\nobjects more successfully and efficiently compared to baselines,particularly\nfor hard-to-detect objects such as srub brush and remote control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiyu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1\">Rohan Chitnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yoonchang Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Image Patch is a Wave: Quantum Inspired Vision MLP. (arXiv:2111.12294v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12294","description":"<p>In the field of computer vision, recent works show that a pure MLP\narchitecture mainly stacked by fully-connected layers can achieve competing\nperformance with CNN and transformer. An input image of vision MLP is usually\nsplit into multiple tokens (patches), while the existing MLP models directly\naggregate them with fixed weights, neglecting the varying semantic information\nof tokens from different images. To dynamically aggregate tokens, we propose to\nrepresent each token as a wave function with two parts, amplitude and phase.\nAmplitude is the original feature and the phase term is a complex value\nchanging according to the semantic contents of input images. Introducing the\nphase term can dynamically modulate the relationship between tokens and fixed\nweights in MLP. Based on the wave-like token representation, we establish a\nnovel Wave-MLP architecture for vision tasks. Extensive experiments demonstrate\nthat the proposed Wave-MLP is superior to the state-of-the-art MLP\narchitectures on various vision tasks such as image classification, object\ndetection and semantic segmentation. The source code will be available at\nhttps://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch and\nhttps://gitee.com/mindspore/models/tree/master/research/cv/wave_mlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional, and Incremental Robot Learning. (arXiv:2111.14693v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.14693","description":"<p>Building general-purpose robots to perform a diverse range of tasks in a\nlarge variety of environments in the physical world at the human level is\nextremely challenging. It requires the robot learning to be sample-efficient,\ngeneralizable, compositional, and incremental. In this work, we introduce a\nsystematic learning framework called SAGCI-system towards achieving these above\nfour requirements. Our system first takes the raw point clouds gathered by the\ncamera mounted on the robot's wrist as the inputs and produces initial modeling\nof the surrounding environment represented as a file of Unified Robot\nDescription Format (URDF). Our system adopts a learning-augmented\ndifferentiable simulation that loads the URDF. The robot then utilizes the\ninteractive perception to interact with the environment to online verify and\nmodify the URDF. Leveraging the differentiable simulation, we propose a\nmodel-based learning algorithm combining object-centric and robot-centric\nstages to efficiently produce policies to accomplish manipulation tasks. We\napply our system to perform articulated object manipulation tasks, both in the\nsimulation and the real world. Extensive experiments demonstrate the\neffectiveness of our proposed learning framework. Supplemental materials and\nvideos are available on https://sites.google.com/view/egci.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiaojun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Lin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Camera Self-Calibration from Video. (arXiv:2112.03325v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03325","description":"<p>Camera calibration is integral to robotics and computer vision algorithms\nthat seek to infer geometric properties of the scene from visual input streams.\nIn practice, calibration is a laborious procedure requiring specialized data\ncollection and careful tuning. This process must be repeated whenever the\nparameters of the camera change, which can be a frequent occurrence for mobile\nrobots and autonomous vehicles. In contrast, self-supervised depth and\nego-motion estimation approaches can bypass explicit calibration by inferring\nper-frame projection models that optimize a view synthesis objective. In this\npaper, we extend this approach to explicitly calibrate a wide range of cameras\nfrom raw videos in the wild. We propose a learning algorithm to regress\nper-sequence calibration parameters using an efficient family of general camera\nmodels. Our procedure achieves self-calibration results with sub-pixel\nreprojection error, outperforming other learning-based methods. We validate our\napproach on a wide variety of camera geometries, including perspective,\nfisheye, and catadioptric. Finally, we show that our approach leads to\nimprovements in the downstream task of depth estimation, achieving\nstate-of-the-art results on the EuRoC dataset with greater computational\nefficiency than contemporary methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiading Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasiljevic_I/0/1/0/all/0/1\">Igor Vasiljevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1\">Matthew R.Walter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04312","description":"<p>In this work we develop a generalizable and efficient Neural Radiance Field\n(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under\nsettings with sparse camera views. Though existing NeRF-based methods can\nsynthesize rather realistic details for human body, they tend to produce poor\nresults when the input has self-occlusion, especially for unseen humans under\nsparse views. Moreover, these methods often require a large number of sampling\npoints for rendering, which leads to low efficiency and limits their real-world\napplicability. To address these challenges, we propose a Geometry-guided\nProgressive NeRF~(GP-NeRF). In particular, to better tackle self-occlusion, we\ndevise a geometry-guided multi-view feature integration approach that utilizes\nthe estimated geometry prior to integrate the incomplete information from input\nviews and construct a complete geometry volume for the target human body.\nMeanwhile, for achieving higher rendering efficiency, we introduce a\ngeometry-guided progressive rendering pipeline, which leverages the geometric\nfeature volume and the predicted density values to progressively reduce the\nnumber of sampling points and speed up the rendering process. Experiments on\nthe ZJU-MoCap and THUman datasets show that our method outperforms the\nstate-of-the-arts significantly across multiple generalization settings, while\nthe time cost is reduced &gt;70% via applying our efficient progressive rendering\npipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lijuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yujun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. (arXiv:2112.05139v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05139","description":"<p>We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural\nradiance fields (NeRF). By leveraging the joint language-image embedding space\nof the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose\na unified framework that allows manipulating NeRF in a user-friendly way, using\neither a short text prompt or an exemplar image. Specifically, to combine the\nnovel view synthesis capability of NeRF and the controllable manipulation\nability of latent representations from generative models, we introduce a\ndisentangled conditional NeRF architecture that allows individual control over\nboth shape and appearance. This is achieved by performing the shape\nconditioning via applying a learned deformation field to the positional\nencoding and deferring color conditioning to the volumetric rendering stage. To\nbridge this disentangled latent representation to the CLIP embedding, we design\ntwo code mappers that take a CLIP embedding as input and update the latent\ncodes to reflect the targeted editing. The mappers are trained with a\nCLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we\npropose an inverse optimization method that accurately projects an input image\nto the latent codes for manipulation to enable editing on real images. We\nevaluate our approach by extensive experiments on a variety of text prompts and\nexemplar images and also provide an intuitive interface for interactive\nediting. Our implementation is available at\nhttps://cassiepython.github.io/clipnerf/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Can Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HairCLIP: Design Your Hair by Text and Reference Image. (arXiv:2112.05142v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05142","description":"<p>Hair editing is an interesting and challenging problem in computer vision and\ngraphics. Many existing methods require well-drawn sketches or masks as\nconditional inputs for editing, however these interactions are neither\nstraightforward nor efficient. In order to free users from the tedious\ninteraction process, this paper proposes a new hair editing interaction mode,\nwhich enables manipulating hair attributes individually or jointly based on the\ntexts or reference images provided by users. For this purpose, we encode the\nimage and text conditions in a shared embedding space and propose a unified\nhair editing framework by leveraging the powerful image text representation\ncapability of the Contrastive Language-Image Pre-Training (CLIP) model. With\nthe carefully designed network structures and loss functions, our framework can\nperform high-quality hair editing in a disentangled manner. Extensive\nexperiments demonstrate the superiority of our approach in terms of\nmanipulation accuracy, visual realism of editing results, and irrelevant\nattribute preservation. Project repo is https://github.com/wty-ustc/HairCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tianyi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhentao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to integrate vision data into road network data. (arXiv:2112.10624v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10624","description":"<p>Road networks are the core infrastructure for connected and autonomous\nvehicles, but creating meaningful representations for machine learning\napplications is a challenging task. In this work, we propose to integrate\nremote sensing vision data into road network data for improved embeddings with\ngraph neural networks. We present a segmentation of road edges based on\nspatio-temporal road and traffic characteristics, which allows to enrich the\nattribute set of road networks with visual features of satellite imagery and\ndigital surface models. We show that both, the segmentation and the integration\nof vision data can increase performance on a road type classification task, and\nwe achieve state-of-the-art performance on the OSM+DiDi Chuxing dataset on\nChengdu, China.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_A/0/1/0/all/0/1\">Alireza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond. (arXiv:2201.03176v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03176","description":"<p>Pedestrian detection is the cornerstone of many vision based applications,\nstarting from object tracking to video surveillance and more recently,\nautonomous driving. With the rapid development of deep learning in object\ndetection, pedestrian detection has achieved very good performance in\ntraditional single-dataset training and evaluation setting. However, in this\nstudy on generalizable pedestrian detectors, we show that, current pedestrian\ndetectors poorly handle even small domain shifts in cross-dataset evaluation.\nWe attribute the limited generalization to two main factors, the method and the\ncurrent sources of data. Regarding the method, we illustrate that biasness\npresent in the design choices (e.g anchor settings) of current pedestrian\ndetectors are the main contributing factor to the limited generalization. Most\nmodern pedestrian detectors are tailored towards target dataset, where they do\nachieve high performance in traditional single training and testing pipeline,\nbut suffer a degrade in performance when evaluated through cross-dataset\nevaluation. Consequently, a general object detector performs better in\ncross-dataset evaluation compared with state of the art pedestrian detectors,\ndue to its generic design. As for the data, we show that the autonomous driving\nbenchmarks are monotonous in nature, that is, they are not diverse in scenarios\nand dense in pedestrians. Therefore, benchmarks curated by crawling the web\n(which contain diverse and dense scenarios), are an efficient source of\npre-training for providing a more robust representation. Accordingly, we\npropose a progressive fine-tuning strategy which improves generalization. Code\nand models can accessed at https://github.com/hasanirtiza/Pedestron.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_I/0/1/0/all/0/1\">Irtiza Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akram_S/0/1/0/all/0/1\">Saad Ullah Akram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A ConvNet for the 2020s. (arXiv:2201.03545v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03545","description":"<p>The \"Roaring 20s\" of visual recognition began with the introduction of Vision\nTransformers (ViTs), which quickly superseded ConvNets as the state-of-the-art\nimage classification model. A vanilla ViT, on the other hand, faces\ndifficulties when applied to general computer vision tasks such as object\ndetection and semantic segmentation. It is the hierarchical Transformers (e.g.,\nSwin Transformers) that reintroduced several ConvNet priors, making\nTransformers practically viable as a generic vision backbone and demonstrating\nremarkable performance on a wide variety of vision tasks. However, the\neffectiveness of such hybrid approaches is still largely credited to the\nintrinsic superiority of Transformers, rather than the inherent inductive\nbiases of convolutions. In this work, we reexamine the design spaces and test\nthe limits of what a pure ConvNet can achieve. We gradually \"modernize\" a\nstandard ResNet toward the design of a vision Transformer, and discover several\nkey components that contribute to the performance difference along the way. The\noutcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt.\nConstructed entirely from standard ConvNet modules, ConvNeXts compete favorably\nwith Transformers in terms of accuracy and scalability, achieving 87.8%\nImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection\nand ADE20K segmentation, while maintaining the simplicity and efficiency of\nstandard ConvNets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1\">Hanzi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chao-Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.13392","description":"<p>The mortality of lung cancer has ranked high among cancers for many years.\nEarly detection of lung cancer is critical for disease prevention, cure, and\nmortality rate reduction. However, existing detection methods on pulmonary\nnodules introduce an excessive number of false positive proposals in order to\nachieve high sensitivity, which is not practical in clinical situations. In\nthis paper, we propose the multi-head detection and spatial\nsqueeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to\naid doctors in the early diagnosis of lung cancers. Specifically, we first\nintroduce multi-head detectors and skip connections to customize for the\nvariety of nodules in sizes, shapes and types and capture multi-scale features.\nThen, we implement a spatial attention module to enable the network to focus on\ndifferent regions differently inspired by how experienced clinicians screen CT\nimages, which results in fewer false positive proposals. Lastly, we present a\nlightweight but effective false positive reduction module with the Linear\nRegression model to cut down the number of false positive proposals, without\nany constraints on the front network. Extensive experimental results compared\nwith the state-of-the-art models have shown the superiority of the MHSnet in\nterms of the average FROC, sensitivity and especially false discovery rate\n(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%\nand 28.33% decrease in terms of false discovery rate and average candidates per\nscan). The false positive reduction module significantly decreases the average\nnumber of candidates generated per scan by 68.11% and the false discovery rate\nby 13.48%, which is promising to reduce distracted proposals for the downstream\ntasks based on the detection results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Juanyun Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Minghao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_J/0/1/0/all/0/1\">Jiayin Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yanbo Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diao_Z/0/1/0/all/0/1\">Zhaoqi Diao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_X/0/1/0/all/0/1\">Xinliang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yulong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jianyu Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_J/0/1/0/all/0/1\">Jian You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_A/0/1/0/all/0/1\">Airu Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_X/0/1/0/all/0/1\">Xiangcheng Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_J/0/1/0/all/0/1\">Jinsheng Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hua Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crafting Better Contrastive Views for Siamese Representation Learning. (arXiv:2202.03278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03278","description":"<p>Recent self-supervised contrastive learning methods greatly benefit from the\nSiamese structure that aims at minimizing distances between positive pairs. For\nhigh performance Siamese representation learning, one of the keys is to design\ngood contrastive pairs. Most previous works simply apply random sampling to\nmake different crops of the same image, which overlooks the semantic\ninformation that may degrade the quality of views. In this work, we propose\nContrastiveCrop, which could effectively generate better crops for Siamese\nrepresentation learning. Firstly, a semantic-aware object localization strategy\nis proposed within the training process in a fully unsupervised manner. This\nguides us to generate contrastive views which could avoid most false positives\n(i.e., object vs. background). Moreover, we empirically find that views with\nsimilar appearances are trivial for the Siamese model training. Thus, a\ncenter-suppressed sampling is further designed to enlarge the variance of\ncrops. Remarkably, our method takes a careful consideration of positive pairs\nfor contrastive learning with negligible extra training overhead. As a\nplug-and-play and framework-agnostic module, ContrastiveCrop consistently\nimproves SimCLR, MoCo, BYOL, SimSiam by 0.4% ~ 2.0% classification accuracy on\nCIFAR-10, CIFAR-100, Tiny ImageNet and STL-10. Superior results are also\nachieved on downstream detection and segmentation tasks when pre-trained on\nImageNet-1K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling. (arXiv:2202.03543v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.03543","description":"<p>In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge\nand SUPERB benchmark. Our submissions are based on the recently proposed\nFaST-VGS model, which is a Transformer-based model that learns to associate raw\nspeech waveforms with semantically related images, all without the use of any\ntranscriptions of the speech. Additionally, we introduce a novel extension of\nthis model, FaST-VGS+, which is learned in a multi-task fashion with a masked\nlanguage modeling objective in addition to the visual grounding objective. On\nZeroSpeech 2021, we show that our models perform competitively on the ABX task,\noutperform all other concurrent submissions on the Syntactic and Semantic\ntasks, and nearly match the best system on the Lexical task. On the SUPERB\nbenchmark, we show that our models also achieve strong performance, in some\ncases even outperforming the popular wav2vec2.0 model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Texture Information into Dimensionality Reduction for High-Dimensional Images. (arXiv:2202.09179v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09179","description":"<p>High-dimensional imaging is becoming increasingly relevant in many fields\nfrom astronomy and cultural heritage to systems biology. Visual exploration of\nsuch high-dimensional data is commonly facilitated by dimensionality reduction.\nHowever, common dimensionality reduction methods do not include spatial\ninformation present in images, such as local texture features, into the\nconstruction of low-dimensional embeddings. Consequently, exploration of such\ndata is typically split into a step focusing on the attribute space followed by\na step focusing on spatial information, or vice versa. In this paper, we\npresent a method for incorporating spatial neighborhood information into\ndistance-based dimensionality reduction methods, such as t-Distributed\nStochastic Neighbor Embedding (t-SNE). We achieve this by modifying the\ndistance measure between high-dimensional attribute vectors associated with\neach pixel such that it takes the pixel's spatial neighborhood into account.\nBased on a classification of different methods for comparing image patches, we\nexplore a number of different approaches. We compare these approaches from a\ntheoretical and experimental point of view. Finally, we illustrate the value of\nthe proposed methods by qualitative and quantitative evaluation on synthetic\ndata and two real-world use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vieth_A/0/1/0/all/0/1\">Alexander Vieth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilanova_A/0/1/0/all/0/1\">Anna Vilanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lelieveldt_B/0/1/0/all/0/1\">Boudewijn Lelieveldt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisemann_E/0/1/0/all/0/1\">Elmar Eisemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollt_T/0/1/0/all/0/1\">Thomas H&#xf6;llt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Language Pre-Training with Triple Contrastive Learning. (arXiv:2202.10401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10401","description":"<p>Vision-language representation learning largely benefits from image-text\nalignment through contrastive losses (e.g., InfoNCE loss). The success of this\nalignment strategy is attributed to its capability in maximizing the mutual\ninformation (MI) between an image and its matched text. However, simply\nperforming cross-modal alignment (CMA) ignores data potential within each\nmodality, which may result in degraded representations. For instance, although\nCMA-based models are able to map image-text pairs close together in the\nembedding space, they fail to ensure that similar inputs from the same modality\nstay close by. This problem can get even worse when the pre-training data is\nnoisy. In this paper, we propose triple contrastive learning (TCL) for\nvision-language pre-training by leveraging both cross-modal and intra-modal\nself-supervision. Besides CMA, TCL introduces an intra-modal contrastive\nobjective to provide complementary benefits in representation learning. To take\nadvantage of localized and structural information from image and text input,\nTCL further maximizes the average MI between local regions of image/text and\ntheir global summary. To the best of our knowledge, ours is the first work that\ntakes into account local structure information for multi-modality\nrepresentation learning. Experimental evaluations show that our approach is\ncompetitive and achieve the new state of the art on various common down-stream\nvision-language tasks such as image-text retrieval and visual question\nanswering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1\">Sampath Chanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilimbi_T/0/1/0/all/0/1\">Trishul Chilimbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?. (arXiv:2202.12426v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12426","description":"<p>While several works have studied the vulnerability of automated FRS and have\nproposed morphing attack detection (MAD) methods, very few have focused on\nstudying the human ability to detect morphing attacks. The examiner/observer's\nface morph detection ability is based on their observation, domain knowledge,\nexperience, and familiarity with the problem, and no works report the detailed\nfindings from observers who check identity documents as a part of their\neveryday professional life. This work creates a new benchmark database of\nrealistic morphing attacks from 48 unique subjects leading to 400 morphed\nimages presented to the observers in a Differential-MAD (D-MAD) setting. Unlike\nthe existing databases, the newly created morphed image database has been\ncreated with careful considerations to age, gender and ethnicity to create\nrealistic morph attacks. Further, unlike the previous works, we also capture\nten images from Automated Border Control (ABC) gates to mimic the realistic\nD-MAD setting leading to 400 probe images in border crossing scenarios. The\nnewly created dataset is further used to study the ability of human observers'\nability to detect morphed images. In addition, a new dataset of 180 morphed\nimages is also created using the FRGCv2 dataset under the Single Image-MAD\n(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a\nnew evaluation platform is created to conduct S-MAD and D-MAD analysis. The\nbenchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who\nare primarily governmental employees from more than 40 countries. The analysis\nprovides interesting insights and points to expert observers' missing\ncompetence and failure to detect a considerable amount of morphing attacks.\nHuman observers tend to detect morphed images to a lower accuracy as compared\nto the automated MAD algorithms evaluated in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Godage_S/0/1/0/all/0/1\">Sankini Rancha Godage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovaasda_F/0/1/0/all/0/1\">Fr&#xf8;y L&#xf8;v&#xe5;sda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1\">Sushma Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12785","description":"<p>Calibrated confidence estimates obtained from neural networks are crucial,\nparticularly for safety-critical applications such as autonomous driving or\nmedical image diagnosis. However, although the task of confidence calibration\nhas been investigated on classification problems, thorough investigations on\nobject detection and segmentation problems are still missing. Therefore, we\nfocus on the investigation of confidence calibration for object detection and\nsegmentation models in this chapter. We introduce the concept of multivariate\nconfidence calibration that is an extension of well-known calibration methods\nto the task of object detection and segmentation. This allows for an extended\nconfidence calibration that is also aware of additional features such as\nbounding box/pixel position, shape information, etc. Furthermore, we extend the\nexpected calibration error (ECE) to measure miscalibration of object detection\nand segmentation models. We examine several network architectures on MS COCO as\nwell as on Cityscapes and show that especially object detection as well as\ninstance segmentation models are intrinsically miscalibrated given the\nintroduced definition of calibration. Using our proposed calibration methods,\nwe have been able to improve calibration so that it also has a positive impact\non the quality of segmentation masks as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuppers_F/0/1/0/all/0/1\">Fabian K&#xfc;ppers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haselhoff_A/0/1/0/all/0/1\">Anselm Haselhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronenberger_J/0/1/0/all/0/1\">Jan Kronenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jonas Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Multi-scale SwinTransformer-HTC with Data augmentation in CoNIC Challenge. (arXiv:2202.13588v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.13588","description":"<p>Colorectal cancer is one of the most common cancers worldwide, so early\npathological examination is very important. However, it is time-consuming and\nlabor-intensive to identify the number and type of cells on H&amp;E images in\nclinical. Therefore, automatic segmentation and classification task and\ncounting the cellular composition of H&amp;E images from pathological sections is\nproposed by CoNIC Challenge 2022. We proposed a multi-scale Swin transformer\nwith HTC for this challenge, and also applied the known normalization methods\nto generate more augmentation data. Finally, our strategy showed that the\nmulti-scale played a crucial role to identify different scale features and the\naugmentation arose the recognition of model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Yen Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chien_H/0/1/0/all/0/1\">Hsiang-Chin Chien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Ching-Ping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yen_H/0/1/0/all/0/1\">Hong Yen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhen_K/0/1/0/all/0/1\">Kai-Wen Zhen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hong-Kun Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Recurrent Fusion for Indoor Localization. (arXiv:2203.00510v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2203.00510","description":"<p>This paper considers indoor localization using multi-modal wireless signals\nincluding Wi-Fi, inertial measurement unit (IMU), and ultra-wideband (UWB). By\nformulating the localization as a multi-modal sequence regression problem, a\nmulti-stream recurrent fusion method is proposed to combine the current hidden\nstate of each modality in the context of recurrent neural networks while\naccounting for the modality uncertainty which is directly learned from its own\nimmediate past states. The proposed method was evaluated on the large-scale\nSPAWC2021 multi-modal localization dataset and compared with a wide range of\nbaseline methods including the trilateration method, traditional fingerprinting\nmethods, and convolution network-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jianyuan Yu</a>, Pu (Perry) <a href=\"http://arxiv.org/find/eess/1/au:+Wang/0/1/0/all/0/1\">Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koike_Akino_T/0/1/0/all/0/1\">Toshiaki Koike-Akino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orlik_P/0/1/0/all/0/1\">Philip V. Orlik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding. (arXiv:2203.00680v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00680","description":"<p>Manual annotation of large-scale point cloud dataset for varying tasks such\nas 3D object classification, segmentation and detection is often laborious\nowing to the irregular structure of point clouds. Self-supervised learning,\nwhich operates without any human labeling, is a promising approach to address\nthis issue. We observe in the real world that humans are capable of mapping the\nvisual concepts learnt from 2D images to understand the 3D world. Encouraged by\nthis insight, we propose CrossPoint, a simple cross-modal contrastive learning\napproach to learn transferable 3D point cloud representations. It enables a\n3D-2D correspondence of objects by maximizing agreement between point clouds\nand the corresponding rendered 2D image in the invariant space, while\nencouraging invariance to transformations in the point cloud modality. Our\njoint training objective combines the feature correspondences within and across\nmodalities, thus ensembles a rich learning signal from both 3D point cloud and\n2D image modalities in a self-supervised fashion. Experimental results show\nthat our approach outperforms the previous unsupervised learning methods on a\ndiverse range of downstream tasks including 3D object classification and\nsegmentation. Further, the ablation studies validate the potency of our\napproach for a better point cloud understanding. Code and pretrained models are\navailable at <a href=\"http://github.com/MohamedAfham/CrossPoint.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afham_M/0/1/0/all/0/1\">Mohamed Afham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dissanayake_I/0/1/0/all/0/1\">Isuru Dissanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dissanayake_D/0/1/0/all/0/1\">Dinithi Dissanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharmasiri_A/0/1/0/all/0/1\">Amaya Dharmasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thilakarathna_K/0/1/0/all/0/1\">Kanchana Thilakarathna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}