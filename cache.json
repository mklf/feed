{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-30T01:30:00Z","channels":[{"title":"cs.AI updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.AI","description":"Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Cascading Neural Network Methodology for Artificial Intelligence-Assisted Radiographic Detection and Classification of Lead-Less Implanted Electronic Devices within the Chest. (arXiv:2108.11954v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11954","description":"<p>Background &amp; Purpose: Chest X-Ray (CXR) use in pre-MRI safety screening for\nLead-Less Implanted Electronic Devices (LLIEDs), easily overlooked or\nmisidentified on a frontal view (often only acquired), is common. Although most\nLLIED types are \"MRI conditional\": 1. Some are stringently conditional; 2.\nDifferent conditional types have specific patient- or device- management\nrequirements; and 3. Particular types are \"MRI unsafe\". This work focused on\ndeveloping CXR interpretation-assisting Artificial Intelligence (AI)\nmethodology with: 1. 100% detection for LLIED presence/location; and 2. High\nclassification in LLIED typing. Materials &amp; Methods: Data-mining\n(03/1993-02/2021) produced an AI Model Development Population (1,100\npatients/4,871 images) creating 4,924 LLIED Region-Of-Interests (ROIs) (with\nimage-quality grading) used in Training, Validation, and Testing. For\ndeveloping the cascading neural network (detection via Faster R-CNN and\nclassification via Inception V3), \"ground-truth\" CXR annotation (ROI labeling\nper LLIED), as well as inference display (as Generated Bounding Boxes (GBBs)),\nrelied on a GPU-based graphical user interface. Results: To achieve 100% LLIED\ndetection, probability threshold reduction to 0.00002 was required by Model 1,\nresulting in increasing GBBs per LLIED-related ROI. Targeting LLIED-type\nclassification following detection of all LLIEDs, Model 2 multi-classified to\nreach high-performance while decreasing falsely positive GBBs. Despite 24%\nsuboptimal ROI image quality, classification was correct in 98.9% and AUCs for\nthe 9 LLIED-types were 1.00 for 8 and 0.92 for 1. For all misclassification\ncases: 1. None involved stringently conditional or unsafe LLIEDs; and 2. Most\nwere attributable to suboptimal images. Conclusion: This project successfully\ndeveloped a LLIED-related AI methodology supporting: 1. 100% detection; and 2.\nTypically 100% type classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Demirer_M/0/1/0/all/0/1\">Mutlu Demirer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+White_R/0/1/0/all/0/1\">Richard D. White</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_V/0/1/0/all/0/1\">Vikash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sebro_R/0/1/0/all/0/1\">Ronnie A. Sebro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erdal_B/0/1/0/all/0/1\">Barbaros S. Erdal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Sentence Ordering Method Using BERT Pretrained Model. (arXiv:2108.11994v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11994","description":"<p>Building systems with capability of natural language understanding (NLU) has\nbeen one of the oldest areas of AI. An essential component of NLU is to detect\nlogical succession of events contained in a text. The task of sentence ordering\nis proposed to learn succession of events with applications in AI tasks. The\nperformance of previous works employing statistical methods is poor, while the\nneural networks-based approaches are in serious need of large corpora for model\nlearning. In this paper, we propose a method for sentence ordering which does\nnot need a training phase and consequently a large corpus for learning. To this\nend, we generate sentence embedding using BERT pre-trained model and measure\nsentence similarity using cosine similarity score. We suggest this score as an\nindicator of sequential events' level of coherence. We finally sort the\nsentences through brute-force search to maximize overall similarities of the\nsequenced sentences. Our proposed method outperformed other baselines on\nROCStories, a corpus of 5-sentence human-made stories. The method is\nspecifically more efficient than neural network-based methods when no huge\ncorpus is available. Among other advantages of this method are its\ninterpretability and needlessness to linguistic knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golestani_M/0/1/0/all/0/1\">Melika Golestani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_S/0/1/0/all/0/1\">Seyedeh Zahra Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Heshaam Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Logit Distributions of Adversarially-Trained Deep Neural Networks. (arXiv:2108.12001v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12001","description":"<p>Adversarial defenses train deep neural networks to be invariant to the input\nperturbations from adversarial attacks. Almost all defense strategies achieve\nthis invariance through adversarial training i.e. training on inputs with\nadversarial perturbations. Although adversarial training is successful at\nmitigating adversarial attacks, the behavioral differences between\nadversarially-trained (AT) models and standard models are still poorly\nunderstood. Motivated by a recent study on learning robustness without input\nperturbations by distilling an AT model, we explore what is learned during\nadversarial training by analyzing the distribution of logits in AT models. We\nidentify three logit characteristics essential to learning adversarial\nrobustness. First, we provide a theoretical justification for the finding that\nadversarial training shrinks two important characteristics of the logit\ndistribution: the max logit values and the \"logit gaps\" (difference between the\nlogit max and next largest values) are on average lower for AT models. Second,\nwe show that AT and standard models differ significantly on which samples are\nhigh or low confidence, then illustrate clear qualitative differences by\nvisualizing samples with the largest confidence difference. Finally, we find\nlearning information about incorrect classes to be essential to learning\nrobustness by manipulating the non-max logit information during distillation\nand measuring the impact on the student's robustness. Our results indicate that\nlearning some adversarial robustness without input perturbations requires a\nmodel to learn specific sample-wise confidences and incorrect class orderings\nthat follow complex distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seguin_L/0/1/0/all/0/1\">Landan Seguin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ndirango_A/0/1/0/all/0/1\">Anthony Ndirango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_N/0/1/0/all/0/1\">Neeli Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1\">SueYeon Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tyler Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-based Self-Critical Training For Question Generation. (arXiv:2108.12026v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12026","description":"<p>We present in this work a fully Transformer-based reinforcement learning\ngenerator-evaluator architecture for neural question generation. Question\ngeneration is a task that consists in generating questions given a context and\nanswer. To improve the quality of the generated question, we came up with a\nsemantic-based self-critical training layout in generator-evaluator\narchitecture, which goes beyond typical maximum likelihood training. Evaluation\nmetrics for language modeling only based on n-gram overlapping do not consider\nsemantic relations between reference and candidate strings. To improve the\nevaluation step, we assess our model for both n-gram overlap using BLEU and\nsemantically using BERTScore and NUBIA, a novel state-of-the-art evaluation\nmetric for text generation. Question generation could be used in many\ndownstream applications, including in extending question answering datasets,\nconversational systems, and educational assessment systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo%5C%22ic/0/1/0/all/0/1\">Lo&#xef;c</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dassi_K/0/1/0/all/0/1\">Kwate Dassi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12056","description":"<p>Existing machines are functionally specific tools that were made for easy\nprediction and control. Tomorrow's machines may be closer to biological systems\nin their mutability, resilience, and autonomy. But first they must be capable\nof learning, and retaining, new information without repeated exposure to it.\nPast efforts to engineer such systems have sought to build or regulate\nartificial neural networks using task-specific modules with constrained\ncircumstances of application. This has not yet enabled continual learning over\nlong sequences of previously unseen data without corrupting existing knowledge:\na problem known as catastrophic forgetting. In this paper, we introduce a\nsystem that can learn sequentially over previously unseen datasets (ImageNet,\nCIFAR-100) with little forgetting over time. This is accomplished by regulating\nthe activity of weights in a convolutional neural network on the basis of\ninputs using top-down modulation generated by a second feed-forward neural\nnetwork. We find that our method learns continually under domain transfer with\nsparse bursts of activity in weights that are recycled across tasks, rather\nthan by maintaining task-specific modules. Sparse synaptic bursting is found to\nbalance enhanced and diminished activity in a way that facilitates adaptation\nto new inputs without corrupting previously acquired functions. This behavior\nemerges during a prior meta-learning phase in which regulated synapses are\nselectively disinhibited, or grown, from an initial state of uniform\nsuppression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1\">Shawn L. Beaulieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1\">Nick Cheney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12084","description":"<p>Gender is widely discussed in the context of language tasks and when\nexamining the stereotypes propagated by language models. However, current\ndiscussions primarily treat gender as binary, which can perpetuate harms such\nas the cyclical erasure of non-binary gender identities. These harms are driven\nby model and dataset biases, which are consequences of the non-recognition and\nlack of understanding of non-binary genders in society. In this paper, we\nexplain the complexity of gender and language around it, and survey non-binary\npersons to understand harms associated with the treatment of gender as binary\nin English language technologies. We also detail how current language\nrepresentations (e.g., GloVe, BERT) capture and perpetuate these harms and\nrelated challenges that need to be acknowledged and addressed for\nrepresentations to equitably encode gender information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Give Checkable Answers with Prover-Verifier Games. (arXiv:2108.12099v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12099","description":"<p>Our ability to know when to trust the decisions made by machine learning\nsystems has not kept up with the staggering improvements in their performance,\nlimiting their applicability in high-stakes domains. We introduce\nProver-Verifier Games (PVGs), a game-theoretic framework to encourage learning\nagents to solve decision problems in a verifiable manner. The PVG consists of\ntwo learners with competing objectives: a trusted verifier network tries to\nchoose the correct answer, and a more powerful but untrusted prover network\nattempts to persuade the verifier of a particular answer, regardless of its\ncorrectness. The goal is for a reliable justification protocol to emerge from\nthis game. We analyze variants of the framework, including simultaneous and\nsequential games, and narrow the space down to a subset of games which provably\nhave the desired equilibria. We develop instantiations of the PVG for two\nalgorithmic tasks, and show that in practice, the verifier learns a robust\ndecision rule that is able to receive useful and reliable information from an\nuntrusted prover. Importantly, the protocol still works even when the verifier\nis frozen and the prover's messages are directly optimized to convince the\nverifier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1\">Cem Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guodong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1\">Roger Grosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-aware Warping Factors in Mask-based Speech Enhancement. (arXiv:2108.12128v1 [cs.SD])","link":"http://arxiv.org/abs/2108.12128","description":"<p>This paper proposes the use of two task-aware warping factors in mask-based\nspeech enhancement (SE). One controls the balance between speech-maintenance\nand noise-removal in training phases, while the other controls SE power applied\nto specific downstream tasks in testing phases. Our intention is to alleviate\nthe problem that SE systems trained to improve speech quality often fail to\nimprove other downstream tasks, such as automatic speaker verification (ASV)\nand automatic speech recognition (ASR), because they do not share the same\nobjects. It is easy to apply the proposed dual-warping factors approach to any\nmask-based SE method, and it allows a single SE system to handle multiple tasks\nwithout task-dependent training. The effectiveness of our proposed approach has\nbeen confirmed on the SITW dataset for ASV evaluation and the LibriSpeech\ndataset for ASR and speech quality evaluations of 0-20dB. We show that\ndifferent warping values are necessary for a single SE to achieve optimal\nperformance w.r.t. the three tasks. With the use of task-dependent warping\nfactors, speech quality was improved by an 84.7% PESQ increase, ASV had a 22.4%\nEER reduction, and ASR had a 52.2% WER reduction, on 0dB speech. The\neffectiveness of the task-dependent warping factors were also cross-validated\non VoxCeleb-1 test set for ASV and LibriSpeech dev-clean set for ASV and\nquality evaluations. The proposed method is highly effective and easy to apply\nin practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiongqiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koshinaka_T/0/1/0/all/0/1\">Takafumi Koshinaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okabe_K/0/1/0/all/0/1\">Koji Okabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_H/0/1/0/all/0/1\">Hitoshi Yamamoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WAD: A Deep Reinforcement Learning Agent for Urban Autonomous Driving. (arXiv:2108.12134v1 [cs.AI])","link":"http://arxiv.org/abs/2108.12134","description":"<p>Urban autonomous driving is an open and challenging problem to solve as the\ndecision-making system has to account for several dynamic factors like\nmulti-agent interactions, diverse scene perceptions, complex road geometries,\nand other rarely occurring real-world events. On the other side, with deep\nreinforcement learning (DRL) techniques, agents have learned many complex\npolicies. They have even achieved super-human-level performances in various\nAtari Games and Deepmind's AlphaGo. However, current DRL techniques do not\ngeneralize well on complex urban driving scenarios. This paper introduces the\nDRL driven Watch and Drive (WAD) agent for end-to-end urban autonomous driving.\nMotivated by recent advancements, the study aims to detect important\nobjects/states in high dimensional spaces of CARLA and extract the latent state\nfrom them. Further, passing on the latent state information to WAD agents based\non TD3 and SAC methods to learn the optimal driving policy. Our novel approach\nutilizing fewer resources, step-by-step learning of different driving tasks,\nhard episode termination policy, and reward mechanism has led our agents to\nachieve a 100% success rate on all driving tasks in the original CARLA\nbenchmark and set a new record of 82% on further complex NoCrash benchmark,\noutperforming the state-of-the-art model by more than +30% on NoCrash\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Arjit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sahil Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lyra: A Benchmark for Turducken-Style Code Generation. (arXiv:2108.12144v1 [cs.SE])","link":"http://arxiv.org/abs/2108.12144","description":"<p>Code generation is crucial to reduce manual software development efforts.\nRecently, neural techniques have been used to generate source code\nautomatically. While promising, these approaches are evaluated on tasks for\ngenerating code in single programming languages. However, in actual\ndevelopment, one programming language is often embedded in another. For\nexample, SQL statements are often embedded as strings in base programming\nlanguages such as Python and Java, and JavaScript programs are often embedded\nin sever-side programming languages, such as PHP, Java, and Python. We call\nthis a turducken-style programming. In this paper, we define a new code\ngeneration task: given a natural language comment, this task aims to generate a\nprogram in a base language with an embedded language. To our knowledge, this is\nthe first turducken-style code generation task. For this task, we present Lyra:\na dataset in Python with embedded SQL. This dataset contains 2,000 carefully\nannotated database manipulation programs from real usage projects. Each program\nis paired with both a Chinese comment and an English comment. In our\nexperiment, we adopted Transformer, a state-of-the-art technique, as the\nbaseline. In the best setting, Transformer achieves 0.5% and 1.5% AST exact\nmatching accuracy using Chinese and English comments, respectively. Therefore,\nwe believe that Lyra provides a new challenge for code generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1\">Qingyuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zeyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yingfei Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cleaning Inconsistent Data in Temporal DL-Lite Under Best Repair Semantics. (arXiv:2108.12149v1 [cs.AI])","link":"http://arxiv.org/abs/2108.12149","description":"<p>In this paper, we address the problem of handling inconsistent data in\nTemporal Description Logic (TDL) knowledge bases. Considering the data part of\nthe Knowledge Base as the source of inconsistency over time, we propose an ABox\nrepair approach. This is the first work handling the repair in TDL Knowledge\nbases. To do so, our goal is twofold: 1) detect temporal inconsistencies and 2)\npropose a data temporal reparation. For the inconsistency detection, we propose\na reduction approach from TDL to DL which allows to provide a tight NP-complete\nupper bound for TDL concept satisfiability and to use highly optimised DL\nreasoners that can bring precise explanation (the set of inconsistent data\nassertions). Thereafter, from the obtained explanation, we propose a method for\nautomatically computing the best repair in the temporal setting based on the\nallowed rigid predicates and the time order of assertions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouziri_M/0/1/0/all/0/1\">Mourad Ouziri</a> (LIPADE), <a href=\"http://arxiv.org/find/cs/1/au:+Tahrat_S/0/1/0/all/0/1\">Sabiha Tahrat</a> (LIPADE), <a href=\"http://arxiv.org/find/cs/1/au:+Benbernou_S/0/1/0/all/0/1\">Salima Benbernou</a> (LIPADE), <a href=\"http://arxiv.org/find/cs/1/au:+Ouzirri_M/0/1/0/all/0/1\">Mourad Ouzirri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLocal-K: Global and Local Kernels for Recommender Systems. (arXiv:2108.12184v1 [cs.IR])","link":"http://arxiv.org/abs/2108.12184","description":"<p>Recommender systems typically operate on high-dimensional sparse user-item\nmatrices. Matrix completion is a very challenging task to predict one's\ninterest based on millions of other users having each seen a small subset of\nthousands of items. We propose a Global-Local Kernel-based matrix completion\nframework, named GLocal-K, that aims to generalise and represent a\nhigh-dimensional sparse user-item matrix entry into a low dimensional space\nwith a small number of important features. Our GLocal-K can be divided into two\nmajor stages. First, we pre-train an auto encoder with the local kernelised\nweight matrix, which transforms the data from one space into the feature space\nby using a 2d-RBF kernel. Then, the pre-trained auto encoder is fine-tuned with\nthe rating matrix, produced by a convolution-based global kernel, which\ncaptures the characteristics of each item. We apply our GLocal-K model under\nthe extreme low-resource setting, which includes only a user-item rating\nmatrix, with no side information. Our model outperforms the state-of-the-art\nbaselines on three collaborative filtering benchmarks: ML-100K, ML-1M, and\nDouban.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_T/0/1/0/all/0/1\">Taejun Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgstaller_B/0/1/0/all/0/1\">Bernd Burgstaller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_J/0/1/0/all/0/1\">Josiah Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Robustness of Neural Language Models to Input Perturbations. (arXiv:2108.12237v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12237","description":"<p>High-performance neural language models have obtained state-of-the-art\nresults on a wide range of Natural Language Processing (NLP) tasks. However,\nresults for common benchmark datasets often do not reflect model reliability\nand robustness when applied to noisy, real-world data. In this study, we design\nand implement various types of character-level and word-level perturbation\nmethods to simulate realistic scenarios in which input texts may be slightly\nnoisy or different from the data distribution on which NLP systems were\ntrained. Conducting comprehensive experiments on different NLP tasks, we\ninvestigate the ability of high-performance language models such as BERT,\nXLNet, RoBERTa, and ELMo in handling different types of input perturbations.\nThe results suggest that language models are sensitive to input perturbations\nand their performance can decrease even when small changes are introduced. We\nhighlight that models need to be further improved and that current benchmarks\nare not reflecting model robustness well. We argue that evaluations on\nperturbed inputs should routinely complement widely-used benchmarks in order to\nyield a more realistic understanding of NLP systems robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Models for (Temporally) Attributed Description Logics. (arXiv:2108.12239v1 [cs.LO])","link":"http://arxiv.org/abs/2108.12239","description":"<p>In the search for knowledge graph embeddings that could capture ontological\nknowledge, geometric models of existential rules have been recently introduced.\nIt has been shown that convex geometric regions capture the so-called\nquasi-chained rules. Attributed description logics (DL) have been defined to\nbridge the gap between DL languages and knowledge graphs, whose facts often\ncome with various kinds of annotations that may need to be taken into account\nfor reasoning. In particular, temporally attributed DLs are enriched by\nspecific attributes whose semantics allows for some temporal reasoning.\nConsidering that geometric models and (temporally) attributed DLs are promising\ntools designed for knowledge graphs, this paper investigates their\ncompatibility, focusing on the attributed version of a Horn dialect of the\nDL-Lite family. We first adapt the definition of geometric models to attributed\nDLs and show that every satisfiable ontology has a convex geometric model. Our\nsecond contribution is a study of the impact of temporal attributes. We show\nthat a temporally attributed DL may not have a convex geometric model in\ngeneral but we can recover geometric satisfiability by imposing some\nrestrictions on the use of the temporal attributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bourgaux_C/0/1/0/all/0/1\">Camille Bourgaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_A/0/1/0/all/0/1\">Ana Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning models are not robust against noise in clinical text. (arXiv:2108.12242v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12242","description":"<p>Artificial Intelligence (AI) systems are attracting increasing interest in\nthe medical domain due to their ability to learn complicated tasks that require\nhuman intelligence and expert knowledge. AI systems that utilize\nhigh-performance Natural Language Processing (NLP) models have achieved\nstate-of-the-art results on a wide variety of clinical text processing\nbenchmarks. They have even outperformed human accuracy on some tasks. However,\nperformance evaluation of such AI systems have been limited to accuracy\nmeasures on curated and clean benchmark datasets that may not properly reflect\nhow robustly these systems can operate in real-world situations. In order to\naddress this challenge, we introduce and implement a wide variety of\nperturbation methods that simulate different types of noise and variability in\nclinical text data. While noisy samples produced by these perturbation methods\ncan often be understood by humans, they may cause AI systems to make erroneous\ndecisions. Conducting extensive experiments on several clinical text processing\ntasks, we evaluated the robustness of high-performance NLP models against\nvarious types of character-level and word-level noise. The results revealed\nthat the NLP models performance degrades when the input contains small amounts\nof noise. This study is a significant step towards exposing vulnerabilities of\nAI models utilized in clinical text processing systems. The proposed\nperturbation methods can be used in performance evaluation tests to assess how\nrobustly clinical NLP models can operate on noisy data, in real-world settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-To-End Anomaly Detection for Identifying Malicious Cyber Behavior through NLP-Based Log Embeddings. (arXiv:2108.12276v1 [cs.AI])","link":"http://arxiv.org/abs/2108.12276","description":"<p>Rule-based IDS (intrusion detection systems) are being replaced by more\nrobust neural IDS, which demonstrate great potential in the field of\nCybersecurity. However, these ML approaches continue to rely on ad-hoc feature\nengineering techniques, which lack the capacity to vectorize inputs in ways\nthat are fully relevant to the discovery of anomalous cyber activity. We\npropose a deep end-to-end framework with NLP-inspired components for\nidentifying potentially malicious behaviors on enterprise computer networks. We\nalso demonstrate the efficacy of this technique on the recently released DARPA\nOpTC data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golczynski_A/0/1/0/all/0/1\">Andrew Golczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emanuello_J/0/1/0/all/0/1\">John A. Emanuello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process. (arXiv:2108.12278v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12278","description":"<p>Recent research efforts in lifelong learning propose to grow a mixture of\nmodels to adapt to an increasing number of tasks. The proposed methodology\nshows promising results in overcoming catastrophic forgetting. However, the\ntheory behind these successful models is still not well understood. In this\npaper, we perform the theoretical analysis for lifelong learning models by\nderiving the risk bounds based on the discrepancy distance between the\nprobabilistic representation of data generated by the model and that\ncorresponding to the target dataset. Inspired by the theoretical analysis, we\nintroduce a new lifelong learning approach, namely the Lifelong Infinite\nMixture (LIMix) model, which can automatically expand its network architectures\nor choose an appropriate component to adapt its parameters for learning a new\ntask, while preserving its previously learnt information. We propose to\nincorporate the knowledge by means of Dirichlet processes by using a gating\nmechanism which computes the dependence between the knowledge learnt previously\nand stored in each component, and a new set of data. Besides, we train a\ncompact Student model which can accumulate cross-domain representations over\ntime and make quick inferences. The code is available at\nhttps://github.com/dtuzi123/Lifelong-infinite-mixture-model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers. (arXiv:2108.12284v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12284","description":"<p>Recently, many datasets have been proposed to test the systematic\ngeneralization ability of neural networks. The companion baseline Transformers,\ntypically trained with default hyper-parameters from standard tasks, are shown\nto fail dramatically. Here we demonstrate that by revisiting model\nconfigurations as basic as scaling of embeddings, early stopping, relative\npositional embedding, and Universal Transformer variants, we can drastically\nimprove the performance of Transformers on systematic generalization. We report\nimprovements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics\ndataset. Our models improve accuracy from 50% to 85% on the PCFG productivity\nsplit, and from 35% to 81% on COGS. On SCAN, relative positional embedding\nlargely mitigates the EOS decision problem (Newman et al., 2020), yielding 100%\naccuracy on the length split with a cutoff at 26. Importantly, performance\ndifferences between these models are typically invisible on the IID data split.\nThis calls for proper generalization validation sets for developing neural\nnetworks that generalize systematically. We publicly release the code to\nreproduce our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1\">R&#xf3;bert Csord&#xe1;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Music Composition with Deep Learning: A Review. (arXiv:2108.12290v1 [cs.SD])","link":"http://arxiv.org/abs/2108.12290","description":"<p>Generating a complex work of art such as a musical composition requires\nexhibiting true creativity that depends on a variety of factors that are\nrelated to the hierarchy of musical language. Music generation have been faced\nwith Algorithmic methods and recently, with Deep Learning models that are being\nused in other fields such as Computer Vision. In this paper we want to put into\ncontext the existing relationships between AI-based music composition models\nand human musical composition and creativity processes. We give an overview of\nthe recent Deep Learning models for music composition and we compare these\nmodels to the music composition process from a theoretical point of view. We\nhave tried to answer some of the most relevant open questions for this task by\nanalyzing the ability of current Deep Learning models to generate music with\ncreativity or the similarity between AI and human composition processes, among\nothers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Olivan_C/0/1/0/all/0/1\">Carlos Hernandez-Olivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltran_J/0/1/0/all/0/1\">Jose R. Beltran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Supervised Heterogeneous Transfer Learning using Dynamic Distribution Adaptation and Manifold Regularization. (arXiv:2108.12293v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12293","description":"<p>Transfer learning aims to learn classifiers for a target domain by\ntransferring knowledge from a source domain. However, due to two main issues:\nfeature discrepancy and distribution divergence, transfer learning can be a\nvery difficult problem in practice. In this paper, we present a framework\ncalled TLF that builds a classifier for the target domain having only few\nlabeled training records by transferring knowledge from the source domain\nhaving many labeled records. While existing methods often focus on one issue\nand leave the other one for the further work, TLF is capable of handling both\nissues simultaneously. In TLF, we alleviate feature discrepancy by identifying\nshared label distributions that act as the pivots to bridge the domains. We\nhandle distribution divergence by simultaneously optimizing the structural risk\nfunctional, joint distributions between domains, and the manifold consistency\nunderlying marginal distributions. Moreover, for the manifold consistency we\nexploit its intrinsic properties by identifying k nearest neighbors of a\nrecord, where the value of k is determined automatically in TLF. Furthermore,\nsince negative transfer is not desired, we consider only the source records\nthat are belonging to the source pivots during the knowledge transfer. We\nevaluate TLF on seven publicly available natural datasets and compare the\nperformance of TLF against the performance of eleven state-of-the-art\ntechniques. We also evaluate the effectiveness of TLF in some challenging\nsituations. Our experimental results, including statistical sign test and\nNemenyi test analyses, indicate a clear superiority of the proposed framework\nover the state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Geaur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Zahidul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TE-YOLOF: Tiny and efficient YOLOF for blood cell detection. (arXiv:2108.12313v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12313","description":"<p>Blood cell detection in microscopic images is an essential branch of medical\nimage processing research. Since disease detection based on manual checking of\nblood cells is time-consuming and full of errors, testing of blood cells using\nobject detectors with Deep Convolutional Neural Network can be regarded as a\nfeasible solution. In this work, an object detector based on YOLOF has been\nproposed to detect blood cell objects such as red blood cells, white blood\ncells and platelets. This object detector is called TE-YOLOF, Tiny and\nEfficient YOLOF, and it is a One-Stage detector using dilated encoder to\nextract information from single-level feature maps. For increasing efficiency\nand flexibility, the EfficientNet Convolutional Neural Network is utilized as\nthe backbone for the proposed object detector. Furthermore, the Depthwise\nSeparable Convolution is applied to enhance the performance and minimize the\nparameters of the network. In addition, the Mish activation function is\nemployed to increase the precision. Extensive experiments on the BCCD dataset\nprove the effectiveness of the proposed model, which is more efficient than\nother existing studies for blood cell detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fanxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangkui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMT-Based Safety Verification of Data-Aware Processes under Ontologies (Extended Version). (arXiv:2108.12330v1 [cs.AI])","link":"http://arxiv.org/abs/2108.12330","description":"<p>In the context of verification of data-aware processes (DAPs), a formal\napproach based on satisfiability modulo theories (SMT) has been considered to\nverify parameterised safety properties of so-called artifact-centric systems.\nThis approach requires a combination of model-theoretic notions and algorithmic\ntechniques based on backward reachability. We introduce here a variant of one\nof the most investigated models in this spectrum, namely simple artifact\nsystems (SASs), where, instead of managing a database, we operate over a\ndescription logic (DL) ontology expressed in (a slight extension of) RDFS. This\nDL, enjoying suitable model-theoretic properties, allows us to define DL-based\nSASs to which backward reachability can still be applied, leading to\ndecidability in PSPACE of the corresponding safety problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calvanese_D/0/1/0/all/0/1\">Diego Calvanese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gianola_A/0/1/0/all/0/1\">Alessandro Gianola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzullo_A/0/1/0/all/0/1\">Andrea Mazzullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1\">Marco Montali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Heuristics and Learning in a Computational Architecture for Cognitive Trading. (arXiv:2108.12333v1 [cs.AI])","link":"http://arxiv.org/abs/2108.12333","description":"<p>The successes of Artificial Intelligence in recent years in areas such as\nimage analysis, natural language understanding and strategy games have sparked\ninterest from the world of finance. Specifically, there are high expectations,\nand ongoing engineering projects, regarding the creation of artificial agents,\nknown as robotic traders, capable of juggling the financial markets with the\nskill of experienced human traders. Obvious economic implications aside, this\nis certainly an area of great scientific interest, due to the challenges that\nsuch a real context poses to the use of AI techniques. Precisely for this\nreason, we must be aware that artificial agents capable of operating at such\nlevels are not just round the corner, and that there will be no simple answers,\nbut rather a concurrence of various technologies and methods to the success of\nthe effort. In the course of this article, we review the issues inherent in the\ndesign of effective robotic traders as well as the consequently applicable\nsolutions, having in view the general objective of bringing the current state\nof the art of robo-trading up to the next level of intelligence, which we refer\nto as Cognitive Trading. Key to our approach is the joining of two\nmethodological and technological directions which, although both deeply rooted\nin the disciplinary field of artificial intelligence, have so far gone their\nseparate ways: heuristics and learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pareschi_R/0/1/0/all/0/1\">Remo Pareschi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappone_F/0/1/0/all/0/1\">Federico Zappone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning. (arXiv:2108.12370v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12370","description":"<p>We demonstrate a library for the integration of domain knowledge in deep\nlearning architectures. Using this library, the structure of the data is\nexpressed symbolically via graph declarations and the logical constraints over\noutputs or latent variables can be seamlessly added to the deep models. The\ndomain knowledge can be defined explicitly, which improves the models'\nexplainability in addition to the performance and generalizability in the\nlow-data regime. Several approaches for such an integration of symbolic and\nsub-symbolic models have been introduced; however, there is no library to\nfacilitate the programming for such an integration in a generic way while\nvarious underlying algorithms can be used. Our library aims to simplify\nprogramming for such an integration in both training and inference phases while\nseparating the knowledge representation from learning algorithms. We showcase\nvarious NLP benchmark tasks and beyond. The framework is publicly available at\nGithub(https://github.com/HLR/DomiKnowS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faghihi_H/0/1/0/all/0/1\">Hossein Rajaby Faghihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Quan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszok_A/0/1/0/all/0/1\">Andrzej Uszok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nafar_A/0/1/0/all/0/1\">Aliakbar Nafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raisi_E/0/1/0/all/0/1\">Elaheh Raisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pedestrian Detection and Tracking Framework for Autonomous Cars: Efficient Fusion of Camera and LiDAR Data. (arXiv:2108.12375v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12375","description":"<p>This paper presents a novel method for pedestrian detection and tracking by\nfusing camera and LiDAR sensor data. To deal with the challenges associated\nwith the autonomous driving scenarios, an integrated tracking and detection\nframework is proposed. The detection phase is performed by converting LiDAR\nstreams to computationally tractable depth images, and then, a deep neural\nnetwork is developed to identify pedestrian candidates both in RGB and depth\nimages. To provide accurate information, the detection phase is further\nenhanced by fusing multi-modal sensor information using the Kalman filter. The\ntracking phase is a combination of the Kalman filter prediction and an optical\nflow algorithm to track multiple pedestrians in a scene. We evaluate our\nframework on a real public driving dataset. Experimental results demonstrate\nthat the proposed method achieves significant performance improvement over a\nbaseline method that solely uses image-based pedestrian detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Muhammad Mobaidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newaz_A/0/1/0/all/0/1\">Abdullah Al Redwan Newaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimoddini_A/0/1/0/all/0/1\">Ali Karimoddini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Effect Identification from Multiple Incomplete Data Sources: A General Search-based Approach. (arXiv:1902.01073v5 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/1902.01073","description":"<p>Causal effect identification considers whether an interventional probability\ndistribution can be uniquely determined without parametric assumptions from\nmeasured source distributions and structural knowledge on the generating\nsystem. While complete graphical criteria and procedures exist for many\nidentification problems, there are still challenging but important extensions\nthat have not been considered in the literature. To tackle these new settings,\nwe present a search algorithm directly over the rules of do-calculus. Due to\ngenerality of do-calculus, the search is capable of taking more advanced\ndata-generating mechanisms into account along with an arbitrary type of both\nobservational and experimental source distributions. The search is enhanced via\na heuristic and search space reduction techniques. The approach, called\ndo-search, is provably sound, and it is complete with respect to\nidentifiability problems that have been shown to be completely characterized by\ndo-calculus. When extended with additional rules, the search is capable of\nhandling missing data problems as well. With the versatile search, we are able\nto approach new problems such as combined transportability and selection bias,\nor multiple sources of selection bias. We perform a systematic analysis of\nbivariate missing data problems and study causal inference under case-control\ndesign. We also present the R package dosearch that provides an interface for a\nC++ implementation of the search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Tikka_S/0/1/0/all/0/1\">Santtu Tikka</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hyttinen_A/0/1/0/all/0/1\">Antti Hyttinen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karvanen_J/0/1/0/all/0/1\">Juha Karvanen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces. (arXiv:2012.08859v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.08859","description":"<p>Current state-of-the-art Neural Architecture Search (NAS) methods neither\nefficiently scale to multiple hardware platforms, nor handle diverse\narchitectural search-spaces. To remedy this, we present DONNA (Distilling\nOptimal Neural Network Architectures), a novel pipeline for rapid, scalable and\ndiverse NAS, that scales to many user scenarios. DONNA consists of three\nphases. First, an accuracy predictor is built using blockwise knowledge\ndistillation from a reference model. This predictor enables searching across\ndiverse networks with varying macro-architectural parameters such as layer\ntypes and attention mechanisms, as well as across micro-architectural\nparameters such as block repeats and expansion rates. Second, a rapid\nevolutionary search finds a set of pareto-optimal architectures for any\nscenario using the accuracy predictor and on-device measurements. Third,\noptimal models are quickly finetuned to training-from-scratch accuracy. DONNA\nis up to 100x faster than MNasNet in finding state-of-the-art architectures\non-device. Classifying ImageNet, DONNA architectures are 20% faster than\nEfficientNet-B0 and MobileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5%\nhigher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition\nto NAS, DONNA is used for search-space extension and exploration, as well as\nhardware-aware model compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moons_B/0/1/0/all/0/1\">Bert Moons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noorzad_P/0/1/0/all/0/1\">Parham Noorzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skliar_A/0/1/0/all/0/1\">Andrii Skliar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariani_G/0/1/0/all/0/1\">Giovanni Mariani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Dushyant Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lott_C/0/1/0/all/0/1\">Chris Lott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1\">Tijmen Blankevoort</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00433","description":"<p>Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nThis is problematic, as previous work has demonstrated possible trade-offs and\nnegative consequences to disclosive transparency, such as a confusion effect,\nwhere 'too much information' clouds a reader's understanding of what a system\ndescription means. Disclosive transparency's subjective nature has rendered\ndeep study into these problems and their remedies difficult. To improve this\nstate of affairs, We introduce neural language model-based probabilistic\nmetrics to directly model disclosive transparency, and demonstrate that they\ncorrelate with user and expert opinions of system transparency, making them a\nvalid objective proxy. Finally, we demonstrate the use of these metrics in a\npilot study quantifying the relationships between transparency, confusion, and\nuser perceptions in a corpus of real NLP system descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Right Kind of Fairness in AI. (arXiv:2102.08453v6 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2102.08453","description":"<p>Fairness is a concept of justice. Various definitions exist, some of them\nconflicting with each other. In the absence of an uniformly accepted notion of\nfairness, choosing the right kind for a specific situation has always been a\ncentral issue in human history. When it comes to implementing sustainable\nfairness in artificial intelligence systems, this old question plays a key role\nonce again: How to identify the most appropriate fairness metric for a\nparticular application? The answer is often a matter of context, and the best\nchoice depends on ethical standards and legal requirements. Since ethics\nguidelines on this topic are kept rather general for now, we aim to provide\nmore hands-on guidance with this document. Therefore, we first structure the\ncomplex landscape of existing fairness metrics and explain the different\noptions by example. Furthermore, we propose the \"Fairness Compass\", a tool\nwhich formalises the selection process and makes identifying the most\nappropriate fairness definition for a given system a simple, straightforward\nprocedure. Because this process also allows to document the reasoning behind\nthe respective decisions, we argue that this approach can help to build trust\nfrom the user through explaining and justifying the implemented fairness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruf_B/0/1/0/all/0/1\">Boris Ruf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1\">Marcin Detyniecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Navigation of an Ultrasound Probe Towards Standard Scan Planes with Deep Reinforcement Learning. (arXiv:2103.00718v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.00718","description":"<p>Autonomous ultrasound (US) acquisition is an important yet challenging task,\nas it involves interpretation of the highly complex and variable images and\ntheir spatial relationships. In this work, we propose a deep reinforcement\nlearning framework to autonomously control the 6-D pose of a virtual US probe\nbased on real-time image feedback to navigate towards the standard scan planes\nunder the restrictions in real-world US scans. Furthermore, we propose a\nconfidence-based approach to encode the optimization of image quality in the\nlearning process. We validate our method in a simulation environment built with\nreal-world data collected in the US imaging of the spine. Experimental results\ndemonstrate that our method can perform reproducible US probe navigation\ntowards the standard scan plane with an accuracy of $4.91mm/4.65^\\circ$ in the\nintra-patient setting, and accomplish the task in the intra- and inter-patient\nsettings with a success rate of $92\\%$ and $46\\%$, respectively. The results\nalso show that the introduction of image quality optimization in our method can\neffectively improve the navigation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Keyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongsheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Max Q.-H. Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform Network Architectures for Deep Learning based End-to-End Image/Video Coding in Subsampled Color Spaces. (arXiv:2103.01760v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.01760","description":"<p>Most of the existing deep learning based end-to-end image/video coding (DLEC)\narchitectures are designed for non-subsampled RGB color format. However, in\norder to achieve a superior coding performance, many state-of-the-art\nblock-based compression standards such as High Efficiency Video Coding\n(HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for\nYUV 4:2:0 format, where U and V components are subsampled by considering the\nhuman visual system. This paper investigates various DLEC designs to support\nYUV 4:2:0 format by comparing their performance against the main profiles of\nHEVC and VVC standards under a common evaluation framework. Moreover, a new\ntransform network architecture is proposed to improve the efficiency of coding\nYUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the\nproposed architecture significantly outperforms naive extensions of existing\narchitectures designed for RGB format and achieves about 10% average BD-rate\nimprovement over the intra-frame coding in HEVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Egilmez_H/0/1/0/all/0/1\">Hilmi E. Egilmez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1\">Ankitesh K. Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coban_M/0/1/0/all/0/1\">Muhammed Coban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karczewicz_M/0/1/0/all/0/1\">Marta Karczewicz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yinhao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Said_A/0/1/0/all/0/1\">Amir Said</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco S. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation. (arXiv:2103.16219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16219","description":"<p>For unsupervised image-to-image translation, we propose a discriminator\narchitecture which focuses on the statistical features instead of individual\npatches. The network is stabilized by distribution matching of key statistical\nfeatures at multiple scales. Unlike the existing methods which impose more and\nmore constraints on the generator, our method facilitates the shape deformation\nand enhances the fine details with a greatly simplified framework. We show that\nthe proposed method outperforms the existing state-of-the-art models in various\nchallenging applications including selfie-to-anime, male-to-female and glasses\nremoval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xuning Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weidong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximate Computing for Robotic path planning -- Experimentation, Case Study and Practical Implications. (arXiv:2104.05773v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2104.05773","description":"<p>Approximate computing is a computation domain which can be used to trade time\nand energy with quality and therefore is useful in embedded systems. Energy is\nthe prime resource in battery-driven embedded systems, like robots. Approximate\ncomputing can be used as a technique to generate approximate version of the\ncontrol functionalities of a robot, enabling it to ration energy for\ncomputation at the cost of degraded quality. Usually, the programmer of the\nfunction specifies the extent of degradation that is safe for the overall\nsafety of the system. However, in a collaborative environment, where several\nsub-systems co-exist and some of the functionality of each of them have been\napproximated, the safety of the overall system may be compromised. In this\npaper, we consider multiple identical robots operate in a warehouse, and the\npath planning function of the robot is approximated. Although the planned paths\nare safe for individual robots (i.e. they do not collide with the racks), we\nshow that this leads to a collision among the robots. So, a controlled\napproximation needs to be carried out in such situations to harness the full\npower of this new paradigm if it needs to be a mainstream paradigm in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1\">Hrishav Bakul Barua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06669","description":"<p>We propose the task of Narrative Reordering (NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot. We\npresent a dataset, NAREORC, with human rewritings of stories within ROCStories\nin non-linear orders, and conduct a detailed analysis of it. Further, we\npropose novel task-specific training methods with suitable evaluation metrics.\nWe perform experiments on NAREORC using state-of-the-art models such as BART\nand T5 and conduct extensive automatic and human evaluations. We demonstrate\nthat NAREOR is a challenging task with potential for further exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Guided Curriculum Learning for Neural Machine Translation. (arXiv:2105.04475v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.04475","description":"<p>In the field of machine learning, the well-trained model is assumed to be\nable to recover the training labels, i.e. the synthetic labels predicted by the\nmodel should be as close to the ground-truth labels as possible. Inspired by\nthis, we propose a self-guided curriculum strategy to encourage the learning of\nneural machine translation (NMT) models to follow the above recovery criterion,\nwhere we cast the recovery degree of each training example as its learning\ndifficulty. Specifically, we adopt the sentence level BLEU score as the proxy\nof recovery degree. Different from existing curricula relying on linguistic\nprior knowledge or third-party language models, our chosen learning difficulty\nis more suitable to measure the degree of knowledge mastery of the NMT models.\nExperiments on translation benchmarks, including WMT14\nEnglish$\\Rightarrow$German and WMT17 Chinese$\\Rightarrow$English, demonstrate\nthat our approach can consistently improve translation performance against\nstrong baseline Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1\">Ryohei Sasano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koichi Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenter: Transformer for Semantic Segmentation. (arXiv:2105.05633v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05633","description":"<p>Image segmentation is often ambiguous at the level of individual image\npatches and requires contextual information to reach label consensus. In this\npaper we introduce Segmenter, a transformer model for semantic segmentation. In\ncontrast to convolution-based methods, our approach allows to model global\ncontext already at the first layer and throughout the network. We build on the\nrecent Vision Transformer (ViT) and extend it to semantic segmentation. To do\nso, we rely on the output embeddings corresponding to image patches and obtain\nclass labels from these embeddings with a point-wise linear decoder or a mask\ntransformer decoder. We leverage models pre-trained for image classification\nand show that we can fine-tune them on moderate sized datasets available for\nsemantic segmentation. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a mask transformer\ngenerating class masks. We conduct an extensive ablation study to show the\nimpact of the different parameters, in particular the performance is better for\nlarge models and small patch sizes. Segmenter attains excellent results for\nsemantic segmentation. It outperforms the state of the art on both ADE20K and\nPascal Context datasets and is competitive on Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1\">Ricardo Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Precision Training (AdaPT): A dynamic fixed point quantized training approach for DNNs. (arXiv:2107.13490v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.13490","description":"<p>Quantization is a technique for reducing deep neural networks (DNNs) training\nand inference times, which is crucial for training in resource constrained\nenvironments or applications where inference is time critical. State-of-the-art\n(SOTA) quantization approaches focus on post-training quantization, i.e.,\nquantization of pre-trained DNNs for speeding up inference. While work on\nquantized training exists, most approaches require refinement in full precision\n(usually single precision) in the final training phase or enforce a global word\nlength across the entire DNN. This leads to suboptimal assignments of\nbit-widths to layers and, consequently, suboptimal resource usage. In an\nattempt to overcome such limitations, we introduce AdaPT, a new fixed-point\nquantized sparsifying training strategy. AdaPT decides about precision switches\nbetween training epochs based on information theoretic conditions. The goal is\nto determine on a per-layer basis the lowest precision that causes no\nquantization-induced information loss while keeping the precision high enough\nsuch that future learning steps do not suffer from vanishing gradients. The\nbenefits of the resulting fully quantized DNN are evaluated based on an\nanalytical performance model which we develop. We illustrate that an average\nspeedup of 1.27 compared to standard training in float32 with an average\naccuracy increase of 0.98% can be achieved for AlexNet/ResNet on CIFAR10/100\nand we further demonstrate these AdaPT trained models achieve an average\ninference speedup of 2.33 with a model size reduction of 0.52.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1\">Lorenz Kummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1\">Kevin Sidak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1\">Tabea Reichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1\">Wilfried Gansterer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.04409","description":"<p>Deep Neural Networks (DNNs) are vulnerable to adversarial examples which\nwould inveigle neural networks to make prediction errors with small\nperturbations on the input images. Researchers have been devoted to promoting\nthe research on the universal adversarial perturbations (UAPs) which are\ngradient-free and have little prior knowledge on data distributions. Procedural\nadversarial noise attack is a data-free universal perturbation generation\nmethod. In this paper, we propose two universal adversarial perturbation (UAP)\ngeneration methods based on procedural noise functions: Simplex noise and\nWorley noise. In our framework, the shading which disturbs visual\nclassification is generated with rendering technology. Without changing the\nsemantic representations, the adversarial examples generated via our methods\nshow superior performance on the attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08988","description":"<p>Social media platforms provide convenient means for users to participate in\nmultiple online activities on various contents and create fast widespread\ninteractions. However, this rapidly growing access has also increased the\ndiverse information, and characterizing user types to understand people's\nlifestyle decisions shared in social media is challenging. In this paper, we\npropose a weakly supervised graph embedding based framework for understanding\nuser types. We evaluate the user embedding learned using weak supervision over\nwell-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.\nExperiments on real-world datasets demonstrate that the proposed framework\noutperforms the baselines for detecting user types. Finally, we illustrate data\nanalysis on different types of users (e.g., practitioner vs. promotional) from\nour dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our\nmethod for constructing user representation readily generalizes to other\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CMML: Contextual Modulation Meta Learning for Cold-Start Recommendation. (arXiv:2108.10511v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.10511","description":"<p>Practical recommender systems experience a cold-start problem when observed\nuser-item interactions in the history are insufficient. Meta learning,\nespecially gradient based one, can be adopted to tackle this problem by\nlearning initial parameters of the model and thus allowing fast adaptation to a\nspecific task from limited data examples. Though with significant performance\nimprovement, it commonly suffers from two critical issues: the\nnon-compatibility with mainstream industrial deployment and the heavy\ncomputational burdens, both due to the inner-loop gradient operation. These two\nissues make them hard to be applied in practical recommender systems. To enjoy\nthe benefits of meta learning framework and mitigate these problems, we propose\na recommendation framework called Contextual Modulation Meta Learning (CMML).\nCMML is composed of fully feed-forward operations so it is computationally\nefficient and completely compatible with the mainstream industrial deployment.\nCMML consists of three components, including a context encoder that can\ngenerate context embedding to represent a specific task, a hybrid context\ngenerator that aggregates specific user-item features with task-level context,\nand a contextual modulation network, which can modulate the recommendation\nmodel to adapt effectively. We validate our approach on both scenario-specific\nand user-specific cold-start setting on various real-world datasets, showing\nCMML can achieve comparable or even better performance with gradient based\nmethods yet with much higher computational efficiency and better\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xidong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jianye Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Artificial Intelligence"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Enhanced Seq2Seq Autoencoder via Contrastive Learning for Abstractive Text Summarization. (arXiv:2108.11992v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11992","description":"<p>In this paper, we present a denoising sequence-to-sequence (seq2seq)\nautoencoder via contrastive learning for abstractive text summarization. Our\nmodel adopts a standard Transformer-based architecture with a multi-layer\nbi-directional encoder and an auto-regressive decoder. To enhance its denoising\nability, we incorporate self-supervised contrastive learning along with various\nsentence-level document augmentation. These two components, seq2seq autoencoder\nand contrastive learning, are jointly trained through fine-tuning, which\nimproves the performance of text summarization with regard to ROUGE scores and\nhuman evaluation. We conduct experiments on two datasets and demonstrate that\nour model outperforms many existing benchmarks and even achieves comparable\nperformance to the state-of-the-art abstractive systems trained with more\ncomplex architecture and extensive computation resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Harry Jiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Ling Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Sentence Ordering Method Using BERT Pretrained Model. (arXiv:2108.11994v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11994","description":"<p>Building systems with capability of natural language understanding (NLU) has\nbeen one of the oldest areas of AI. An essential component of NLU is to detect\nlogical succession of events contained in a text. The task of sentence ordering\nis proposed to learn succession of events with applications in AI tasks. The\nperformance of previous works employing statistical methods is poor, while the\nneural networks-based approaches are in serious need of large corpora for model\nlearning. In this paper, we propose a method for sentence ordering which does\nnot need a training phase and consequently a large corpus for learning. To this\nend, we generate sentence embedding using BERT pre-trained model and measure\nsentence similarity using cosine similarity score. We suggest this score as an\nindicator of sequential events' level of coherence. We finally sort the\nsentences through brute-force search to maximize overall similarities of the\nsequenced sentences. Our proposed method outperformed other baselines on\nROCStories, a corpus of 5-sentence human-made stories. The method is\nspecifically more efficient than neural network-based methods when no huge\ncorpus is available. Among other advantages of this method are its\ninterpretability and needlessness to linguistic knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golestani_M/0/1/0/all/0/1\">Melika Golestani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_S/0/1/0/all/0/1\">Seyedeh Zahra Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Heshaam Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa. (arXiv:2108.12009v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12009","description":"<p>We present EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with\nRoBERTa, a simple yet expressive scheme of solving the ERC (emotion recognition\nin conversation) task. By simply prepending speaker names to utterances and\ninserting separation tokens between the utterances in a dialogue, EmoBERTa can\nlearn intra- and inter- speaker states and context to predict the emotion of a\ncurrent speaker, in an end-to-end manner. Our experiments show that we reach a\nnew state of the art on the two popular ERC datasets using a basic and\nstraight-forward approach. We've open sourced our code and models at\nhttps://github.com/tae898/erc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vossen_P/0/1/0/all/0/1\">Piek Vossen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-based Self-Critical Training For Question Generation. (arXiv:2108.12026v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12026","description":"<p>We present in this work a fully Transformer-based reinforcement learning\ngenerator-evaluator architecture for neural question generation. Question\ngeneration is a task that consists in generating questions given a context and\nanswer. To improve the quality of the generated question, we came up with a\nsemantic-based self-critical training layout in generator-evaluator\narchitecture, which goes beyond typical maximum likelihood training. Evaluation\nmetrics for language modeling only based on n-gram overlapping do not consider\nsemantic relations between reference and candidate strings. To improve the\nevaluation step, we assess our model for both n-gram overlap using BLEU and\nsemantically using BERTScore and NUBIA, a novel state-of-the-art evaluation\nmetric for text generation. Question generation could be used in many\ndownstream applications, including in extending question answering datasets,\nconversational systems, and educational assessment systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo%5C%22ic/0/1/0/all/0/1\">Lo&#xef;c</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dassi_K/0/1/0/all/0/1\">Kwate Dassi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using GAN-based models to sentimental analysis on imbalanced datasets in education domain. (arXiv:2108.12061v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12061","description":"<p>While the whole world is still struggling with the COVID-19 pandemic, online\nlearning and home office become more common. Many schools transfer their\ncourses teaching to the online classroom. Therefore, it is significant to mine\nthe students' feedback and opinions from their reviews towards studies so that\nboth schools and teachers can know where they need to improve. This paper\ntrains machine learning and deep learning models using both balanced and\nimbalanced datasets for sentiment classification. Two SOTA category-aware text\ngeneration GAN models: CatGAN and SentiGAN, are utilized to synthesize text\nused to balance the highly imbalanced dataset. Results on three datasets with\ndifferent imbalance degree from distinct domains show that when using generated\ntext to balance the dataset, the F1-score of machine learning and deep learning\nmodel on sentiment classification increases 2.79% ~ 9.28%. Also, the results\nindicate that the average growth degree for CR100k is higher than CR23k, the\naverage growth degree for deep learning is more increased than machine learning\nalgorithms, and the average growth degree for more complex deep learning models\nis more increased than simpler deep learning models in experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ru Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edalati_M/0/1/0/all/0/1\">Maryam Edalati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4-bit Quantization of LSTM-based Speech Recognition Models. (arXiv:2108.12074v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12074","description":"<p>We investigate the impact of aggressive low-precision representations of\nweights and activations in two families of large LSTM-based architectures for\nAutomatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM - Hidden\nMarkov Models (DBLSTM-HMMs) and Recurrent Neural Network - Transducers\n(RNN-Ts). Using a 4-bit integer representation, a na\\\"ive quantization approach\napplied to the LSTM portion of these models results in significant Word Error\nRate (WER) degradation. On the other hand, we show that minimal accuracy loss\nis achievable with an appropriate choice of quantizers and initializations. In\nparticular, we customize quantization schemes depending on the local properties\nof the network, improving recognition performance while limiting computational\ntime. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH)\ntest sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with 300 or\n2000 hours of SWB data achieves $&lt;$0.5% and $&lt;$1% average WER degradation,\nrespectively. On the more challenging RNN-T models, our quantization strategy\nlimits degradation in 4-bit inference to 1.3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fasoli_A/0/1/0/all/0/1\">Andrea Fasoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chia-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_M/0/1/0/all/0/1\">Mauricio Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataramani_S/0/1/0/all/0/1\">Swagath Venkataramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaodong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuske_Z/0/1/0/all/0/1\">Zolt&#xe1;n T&#xfc;ske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kailash Gopalakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12084","description":"<p>Gender is widely discussed in the context of language tasks and when\nexamining the stereotypes propagated by language models. However, current\ndiscussions primarily treat gender as binary, which can perpetuate harms such\nas the cyclical erasure of non-binary gender identities. These harms are driven\nby model and dataset biases, which are consequences of the non-recognition and\nlack of understanding of non-binary genders in society. In this paper, we\nexplain the complexity of gender and language around it, and survey non-binary\npersons to understand harms associated with the treatment of gender as binary\nin English language technologies. We also detail how current language\nrepresentations (e.g., GloVe, BERT) capture and perpetuate these harms and\nrelated challenges that need to be acknowledged and addressed for\nrepresentations to equitably encode gender information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lingxi: A Diversity-aware Chinese Modern Poetry Generation System. (arXiv:2108.12108v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12108","description":"<p>Poetry generation has been a difficult task in natural language processing.\nUnlike plain neural text generation tasks, poetry has a high requirement for\nnovelty, since an easily-understood sentence with too many high frequency words\nmight not be considered as poetic, while adequately ambiguous sentences with\nlow frequency words can possibly be novel and creative. Inspired by this, we\npresent Lingxi, a diversity-aware Chinese modern poetry generation system. We\npropose nucleus sampling with randomized head (NS-RH) algorithm, which\nrandomizes the high frequency part (\"head\") of the predicted distribution, in\norder to emphasize on the \"comparatively low frequency\" words. The proposed\nalgorithm can significantly increase the novelty of generated poetry compared\nwith traditional sampling methods. The permutation of distribution is\ncontrollable by tuning the filtering parameter that determines the \"head\" to\npermutate, achieving diversity-aware sampling. We find that even when a large\nportion of filtered vocabulary is randomized, it can actually generate fluent\npoetry but with notably higher novelty. We also propose a\nsemantic-similarity-based rejection sampling algorithm, which creates longer\nand more informative context on the basis of the short input poetry title while\nmaintaining high semantic similarity to the title, alleviating the off-topic\nissue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiafeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Generation of Accurate \\& Fluent Medical X-ray Reports. (arXiv:2108.12126v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12126","description":"<p>Our paper focuses on automating the generation of medical reports from chest\nX-ray image inputs, a critical yet time-consuming task for radiologists. Unlike\nexisting medical re-port generation efforts that tend to produce human-readable\nreports, we aim to generate medical reports that are both fluent and clinically\naccurate. This is achieved by our fully differentiable and end-to-end paradigm\ncontaining three complementary modules: taking the chest X-ray images and\nclinical his-tory document of patients as inputs, our classification module\nproduces an internal check-list of disease-related topics, referred to as\nenriched disease embedding; the embedding representation is then passed to our\ntransformer-based generator, giving rise to the medical reports; meanwhile, our\ngenerator also pro-duces the weighted embedding representation, which is fed to\nour interpreter to ensure consistency with respect to disease-related\ntopics.Our approach achieved promising results on commonly-used metrics\nconcerning language fluency and clinical accuracy. Moreover, noticeable\nperformance gains are consistently ob-served when additional input information\nis available, such as the clinical document and extra scans of different views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang T.N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_D/0/1/0/all/0/1\">Dong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badamdorj_T/0/1/0/all/0/1\">Taivanbat Badamdorj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_J/0/1/0/all/0/1\">Jason Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Secoco: Self-Correcting Encoding for Neural Machine Translation. (arXiv:2108.12137v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12137","description":"<p>This paper presents Self-correcting Encoding (Secoco), a framework that\neffectively deals with input noise for robust neural machine translation by\nintroducing self-correcting predictors. Different from previous robust\napproaches, Secoco enables NMT to explicitly correct noisy inputs and delete\nspecific errors simultaneously with the translation decoding process. Secoco is\nable to achieve significant improvements over strong baselines on two\nreal-world test sets and a benchmark WMT dataset with good interpretability. We\nwill make our code and dataset publicly available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving callsign recognition with air-surveillance data in air-traffic communication. (arXiv:2108.12156v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12156","description":"<p>Automatic Speech Recognition (ASR) can be used as the assistance of speech\ncommunication between pilots and air-traffic controllers. Its application can\nsignificantly reduce the complexity of the task and increase the reliability of\ntransmitted information. Evidently, high accuracy predictions are needed to\nminimize the risk of errors. Especially, high accuracy is required in\nrecognition of key information, such as commands and callsigns, used to\nnavigate pilots. Our results prove that the surveillance data containing\ncallsigns can help to considerably improve the recognition of a callsign in an\nutterance when the weights of probable callsign n-grams are reduced per\nutterance. In this paper, we investigate two approaches: (1) G-boosting, when\ncallsigns weights are adjusted at language model level (G) and followed by the\ndynamic decoder with an on-the-fly composition, and (2) lattice rescoring when\ncallsign information is introduced on top of lattices generated using a\nconventional decoder. Boosting callsign n-grams with the combination of two\nmethods allowed us to gain 28.4% of absolute improvement in callsign\nrecognition accuracy and up to 74.2% of relative improvement in WER of callsign\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_R/0/1/0/all/0/1\">Rudolf Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar Based Identification Of Speaker Role For Improving ATCO And Pilot ASR. (arXiv:2108.12175v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12175","description":"<p>Assistant Based Speech Recognition (ABSR) for air traffic control is\ngenerally trained by pooling both Air Traffic Controller (ATCO) and pilot data.\nIn practice, this is motivated by the fact that the proportion of pilot data is\nlesser compared to ATCO while their standard language of communication is\nsimilar. However, due to data imbalance of ATCO and pilot and their varying\nacoustic conditions, the ASR performance is usually significantly better for\nATCOs than pilots. In this paper, we propose to (1) split the ATCO and pilot\ndata using an automatic approach exploiting ASR transcripts, and (2) consider\nATCO and pilot ASR as two separate tasks for Acoustic Model (AM) training. For\nspeaker role classification of ATCO and pilot data, a hypothesized ASR\ntranscript is generated with a seed model, subsequently used to classify the\nspeaker role based on the knowledge extracted from grammar defined by\nInternational Civil Aviation Organization (ICAO). This approach provides an\naverage speaker role identification accuracy of 83% for ATCO and pilot.\nFinally, we show that training AMs separately for each task, or using a\nmultitask approach is well suited for this data compared to AM trained by\npooling all data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling. (arXiv:2108.12177v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12177","description":"<p>Social media has effectively become the prime hub of communication and\ndigital marketing. As these platforms enable the free manifestation of thoughts\nand facts in text, images and video, there is an extensive need to screen them\nto protect individuals and groups from offensive content targeted at them. Our\nwork intends to classify codemixed social media comments/posts in the Dravidian\nlanguages of Tamil, Kannada, and Malayalam. We intend to improve offensive\nlanguage identification by generating pseudo-labels on the dataset. A custom\ndataset is constructed by transliterating all the code-mixed texts into the\nrespective Dravidian language, either Kannada, Malayalam, or Tamil and then\ngenerating pseudo-labels for the transliterated dataset. The two datasets are\ncombined using the generated pseudo-labels to create a custom dataset called\nCMTRA. As Dravidian languages are under-resourced, our approach increases the\namount of training data for the language models. We fine-tune several recent\npretrained language models on the newly constructed dataset. We extract the\npretrained language embeddings and pass them onto recurrent neural networks. We\nobserve that fine-tuning ULMFiT on the custom dataset yields the best results\non the code-mixed test sets of all three languages. Our approach yields the\nbest results among the benchmarked models on Tamil-English, achieving a\nweighted F1-Score of 0.7934 while scoring competitive weighted F1-Scores of\n0.9624 and 0.7306 on the code-mixed test sets of Malayalam-English and\nKannada-English, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puranik_K/0/1/0/all/0/1\">Karthik Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasaswini_K/0/1/0/all/0/1\">Konthala Yasaswini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thavareesan_S/0/1/0/all/0/1\">Sajeetha Thavareesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugavadivel_K/0/1/0/all/0/1\">Kogilavani Shanmugavadivel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thenmozhi_D/0/1/0/all/0/1\">Durairaj Thenmozhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12189","description":"<p>This paper presents Macquarie University's participation to the BioASQ\nSynergy Task, and BioASQ9b Phase B. In each of these tasks, our participation\nfocused on the use of query-focused extractive summarisation to obtain the\nideal answers to medical questions. The Synergy Task is an end-to-end question\nanswering task on COVID-19 where systems are required to return relevant\ndocuments, snippets, and answers to a given question. Given the absence of\ntraining data, we used a query-focused summarisation system that was trained\nwith the BioASQ8b training data set and we experimented with methods to\nretrieve the documents and snippets. Considering the poor quality of the\ndocuments and snippets retrieved by our system, we observed reasonably good\nquality in the answers returned. For phase B of the BioASQ9b task, the relevant\ndocuments and snippets were already included in the test data. Our system split\nthe snippets into candidate sentences and used BERT variants under a sentence\nclassification setup. The system used the question and candidate sentence as\ninput and was trained to predict the likelihood of the candidate sentence being\npart of the ideal answer. The runs obtained either the best or second best\nROUGE-F1 results of all participants to all batches of BioASQ9b. This shows\nthat using BERT in a classification setup is a very strong baseline for the\nidentification of ideal answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molla_D/0/1/0/all/0/1\">Diego Moll&#xe1;</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_U/0/1/0/all/0/1\">Urvashi Khanna</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Galat_D/0/1/0/all/0/1\">Dima Galat</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vincent Nguyen</a> (2 and 3) <a href=\"http://arxiv.org/find/cs/1/au:+Rybinski_M/0/1/0/all/0/1\">Maciej Rybinski</a> (3) ( (1) Macquarie University, (2) CSIRO Data61, (3) Australian National University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translation Error Detection as Rationale Extraction. (arXiv:2108.12197v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12197","description":"<p>Recent Quality Estimation (QE) models based on multilingual pre-trained\nrepresentations have achieved very competitive results when predicting the\noverall quality of translated sentences. Predicting translation errors, i.e.\ndetecting specifically which words are incorrect, is a more challenging task,\nespecially with limited amounts of training data. We hypothesize that, not\nunlike humans, successful QE models rely on translation errors to predict\noverall sentence quality. By exploring a set of feature attribution methods\nthat assign relevance scores to the inputs to explain model predictions, we\nstudy the behaviour of state-of-the-art sentence-level QE models and show that\nexplanations (i.e. rationales) extracted from these models can indeed be used\nto detect translation errors. We therefore (i) introduce a novel\nsemi-supervised method for word-level QE and (ii) propose to use the QE task as\na new benchmark for evaluating the plausibility of feature attribution, i.e.\nhow interpretable model explanations are to humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features are dependent\nupon each other. Experiment results on five public datasets show that our model\nperforms significantly better than previous approaches. The source code can be\nfound in https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors. (arXiv:2108.12216v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12216","description":"<p>In this paper, we explore the capacity of a language model-based method for\ngrammatical error detection in detail. We first show that 5 to 10% of training\ndata are enough for a BERT-based error detection method to achieve performance\nequivalent to a non-language model-based method can achieve with the full\ntraining data; recall improves much faster with respect to training data size\nin the BERT-based method than in the non-language model method while precision\nbehaves similarly. These suggest that (i) the BERT-based method should have a\ngood knowledge of grammar required to recognize certain types of error and that\n(ii) it can transform the knowledge into error detection rules by fine-tuning\nwith a few training samples, which explains its high generalization ability in\ngrammatical error detection. We further show with pseudo error data that it\nactually exhibits such nice properties in learning rules for recognizing\nvarious types of error. Finally, based on these findings, we explore a\ncost-effective method for detecting grammatical errors with feedback comments\nexplaining relevant grammatical rules to learners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagata_R/0/1/0/all/0/1\">Ryo Nagata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_M/0/1/0/all/0/1\">Manabu Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanawa_K/0/1/0/all/0/1\">Kazuaki Hanawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Text in Self-Supervised Speech Pretraining. (arXiv:2108.12226v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12226","description":"<p>Self-supervised pretraining for Automated Speech Recognition (ASR) has shown\nvaried degrees of success. In this paper, we propose to jointly learn\nrepresentations during pretraining from two different modalities: speech and\ntext. The proposed method, tts4pretrain complements the power of contrastive\nlearning in self-supervision with linguistic/lexical representations derived\nfrom synthesized speech, effectively learning from untranscribed speech and\nunspoken text. Lexical learning in the speech encoder is enforced through an\nadditional sequence loss term that is coupled with contrastive loss during\npretraining. We demonstrate that this novel pretraining method yields Word\nError Rate (WER) reductions of 10% relative on the well-benchmarked,\nLibrispeech task over a state-of-the-art baseline pretrained with wav2vec2.0\nonly. The proposed method also serves as an effective strategy to compensate\nfor the lack of transcribed speech, effectively matching the performance of\n5000 hours of transcribed speech with just 100 hours of transcribed speech on\nthe AMI meeting transcription task. Finally, we demonstrate WER reductions of\nup to 15% on an in-house Voice Search task over traditional pretraining.\nIncorporating text into encoder pretraining is complimentary to rescoring with\na larger or in-domain language model, resulting in additional 6% relative\nreduction in WER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications since the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current algorithms can tackle such\nproblem reliably in a realistic scenario where zero OOD training data is\navailable. In this study, we propose ProtoInfoMax, a new architecture that\nextends Prototypical Networks to simultaneously process In-Domain (ID) and OOD\nsentences via Mutual Information Maximization (InfoMax) objective. Experimental\nresults show that our proposed method can substantially improve performance up\nto 20% for OOD detection in low resource settings of text classification. We\nalso show that ProtoInfoMax is less prone to typical over-confidence Error of\nNeural Networks, leading to more reliable ID and OOD prediction outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Robustness of Neural Language Models to Input Perturbations. (arXiv:2108.12237v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12237","description":"<p>High-performance neural language models have obtained state-of-the-art\nresults on a wide range of Natural Language Processing (NLP) tasks. However,\nresults for common benchmark datasets often do not reflect model reliability\nand robustness when applied to noisy, real-world data. In this study, we design\nand implement various types of character-level and word-level perturbation\nmethods to simulate realistic scenarios in which input texts may be slightly\nnoisy or different from the data distribution on which NLP systems were\ntrained. Conducting comprehensive experiments on different NLP tasks, we\ninvestigate the ability of high-performance language models such as BERT,\nXLNet, RoBERTa, and ELMo in handling different types of input perturbations.\nThe results suggest that language models are sensitive to input perturbations\nand their performance can decrease even when small changes are introduced. We\nhighlight that models need to be further improved and that current benchmarks\nare not reflecting model robustness well. We argue that evaluations on\nperturbed inputs should routinely complement widely-used benchmarks in order to\nyield a more realistic understanding of NLP systems robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning models are not robust against noise in clinical text. (arXiv:2108.12242v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12242","description":"<p>Artificial Intelligence (AI) systems are attracting increasing interest in\nthe medical domain due to their ability to learn complicated tasks that require\nhuman intelligence and expert knowledge. AI systems that utilize\nhigh-performance Natural Language Processing (NLP) models have achieved\nstate-of-the-art results on a wide variety of clinical text processing\nbenchmarks. They have even outperformed human accuracy on some tasks. However,\nperformance evaluation of such AI systems have been limited to accuracy\nmeasures on curated and clean benchmark datasets that may not properly reflect\nhow robustly these systems can operate in real-world situations. In order to\naddress this challenge, we introduce and implement a wide variety of\nperturbation methods that simulate different types of noise and variability in\nclinical text data. While noisy samples produced by these perturbation methods\ncan often be understood by humans, they may cause AI systems to make erroneous\ndecisions. Conducting extensive experiments on several clinical text processing\ntasks, we evaluated the robustness of high-performance NLP models against\nvarious types of character-level and word-level noise. The results revealed\nthat the NLP models performance degrades when the input contains small amounts\nof noise. This study is a significant step towards exposing vulnerabilities of\nAI models utilized in clinical text processing systems. The proposed\nperturbation methods can be used in performance evaluation tests to assess how\nrobustly clinical NLP models can operate on noisy data, in real-world settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Propaganda on the Sentence Level during the COVID-19 Pandemic. (arXiv:2108.12269v1 [cs.CY])","link":"http://arxiv.org/abs/2108.12269","description":"<p>The spread of misinformation, conspiracy, and questionable content and\ninformation manipulation by foreign adversaries on social media has surged\nalong with the COVID-19 pandemic. Such malicious cyber-enabled actions may\ncause increasing social polarization, health crises, and property loss. In this\npaper, using fine-tuned contextualized embedding trained on Reddit, we tackle\nthe detection of the propaganda of such user accounts and their targeted issues\non Twitter during March 2020 when the COVID-19 epidemic became recognized as a\npandemic. Our result shows that the pro-China group appeared to be tweeting 35\nto 115 times more than the neutral group. At the same time, neutral groups were\ntweeting more positive-attitude content and voicing alarm for the COVID-19\nsituation. The pro-China group was also using more call-for-action words on\npolitical issues not necessarily China-related.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1\">Rong-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chu-Hsing Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?. (arXiv:2108.12275v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12275","description":"<p>In this paper we address the problem of fine-tuned text generation with a\nlimited computational budget. For that, we use a well-performing text\ngenerative adversarial network (GAN) architecture - Diversity-Promoting GAN\n(DPGAN), and attempted a drop-in replacement of the LSTM layer with a\nself-attention-based Transformer layer in order to leverage their efficiency.\nThe resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance,\nquality and diversity of generated text and stability. Computational\nexperiments suggested that a transformer architecture is unable to drop-in\nreplace the LSTM layer, under-performing during the pre-training phase and\nundergoing a complete mode collapse during the GAN tuning phase. Our results\nsuggest that the transformer architecture need to be adapted before it can be\nused as a replacement for RNNs in text-generating GANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blin_K/0/1/0/all/0/1\">Kevin Blin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucharavy_A/0/1/0/all/0/1\">Andrei Kucharavy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree Decomposition Attention for AMR-to-Text Generation. (arXiv:2108.12300v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12300","description":"<p>Text generation from AMR requires mapping a semantic graph to a string that\nit annotates. Transformer-based graph encoders, however, poorly capture vertex\ndependencies that may benefit sequence prediction. To impose order on an\nencoder, we locally constrain vertex self-attention using a graph's tree\ndecomposition. Instead of forming a full query-key bipartite graph, we restrict\nattention to vertices in parent, subtree, and same-depth bags of a vertex. This\nhierarchical context lends both sparsity and structure to vertex state updates.\nWe apply dynamic programming to derive a forest of tree decompositions,\nchoosing the most structurally similar tree to the AMR. Our system outperforms\na self-attentive baseline by 1.6 BLEU and 1.8 chrF++.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Tree Decomposition Parsers for AMR-to-Text Generation. (arXiv:2108.12304v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12304","description":"<p>Graph encoders in AMR-to-text generation models often rely on neighborhood\nconvolutions or global vertex attention. While these approaches apply to\ngeneral graphs, AMRs may be amenable to encoders that target their tree-like\nstructure. By clustering edges into a hierarchy, a tree decomposition\nsummarizes graph structure. Our model encodes a derivation forest of tree\ndecompositions and extracts an expected tree. From tree node embeddings, it\nbuilds graph edge features used in vertex attention of the graph encoder.\nEncoding TD forests instead of shortest-pairwise paths in a self-attentive\nbaseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also\nsurpasses a convolutional baseline for molecular property prediction by 1.92%\nROC-AUC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAPE: Context-Aware Private Embeddings for Private Language Learning. (arXiv:2108.12318v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12318","description":"<p>Deep learning-based language models have achieved state-of-the-art results in\na number of applications including sentiment analysis, topic labelling, intent\nclassification and others. Obtaining text representations or embeddings using\nthese models presents the possibility of encoding personally identifiable\ninformation learned from language and context cues that may present a risk to\nreputation or privacy. To ameliorate these issues, we propose Context-Aware\nPrivate Embeddings (CAPE), a novel approach which preserves privacy during\ntraining of embeddings. To maintain the privacy of text representations, CAPE\napplies calibrated noise through differential privacy, preserving the encoded\nsemantic links while obscuring sensitive information. In addition, CAPE employs\nan adversarial training regime that obscures identified private variables.\nExperimental results demonstrate that the proposed approach reduces private\ninformation leakage better than either single intervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plant_R/0/1/0/all/0/1\">Richard Plant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giuffrida_V/0/1/0/all/0/1\">Valerio Giuffrida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning. (arXiv:2108.12370v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12370","description":"<p>We demonstrate a library for the integration of domain knowledge in deep\nlearning architectures. Using this library, the structure of the data is\nexpressed symbolically via graph declarations and the logical constraints over\noutputs or latent variables can be seamlessly added to the deep models. The\ndomain knowledge can be defined explicitly, which improves the models'\nexplainability in addition to the performance and generalizability in the\nlow-data regime. Several approaches for such an integration of symbolic and\nsub-symbolic models have been introduced; however, there is no library to\nfacilitate the programming for such an integration in a generic way while\nvarious underlying algorithms can be used. Our library aims to simplify\nprogramming for such an integration in both training and inference phases while\nseparating the knowledge representation from learning algorithms. We showcase\nvarious NLP benchmark tasks and beyond. The framework is publicly available at\nGithub(https://github.com/HLR/DomiKnowS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faghihi_H/0/1/0/all/0/1\">Hossein Rajaby Faghihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Quan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszok_A/0/1/0/all/0/1\">Andrzej Uszok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nafar_A/0/1/0/all/0/1\">Aliakbar Nafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raisi_E/0/1/0/all/0/1\">Elaheh Raisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. (arXiv:2108.12409v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12409","description":"<p>Since the introduction of the transformer model by Vaswani et al. (2017), a\nfundamental question remains open: how to achieve extrapolation at inference\ntime to longer sequences than seen during training? We first show that\nextrapolation can be improved by changing the position representation method,\nthough we find that existing proposals do not allow efficient extrapolation. We\nintroduce a simple and efficient method, Attention with Linear Biases (ALiBi),\nthat allows for extrapolation. ALiBi does not add positional embeddings to the\nword embeddings; instead, it biases the query-key attention scores with a term\nthat is proportional to their distance. We show that this method allows\ntraining a 1.3 billion parameter model on input sequences of length 1024 that\nextrapolates to input sequences of length 2048, achieving the same perplexity\nas a sinusoidal position embedding model trained on inputs of length 2048, 11%\nfaster and using 11% less memory. ALiBi's inductive bias towards recency allows\nit to outperform multiple strong position methods on the WikiText-103\nbenchmark. Finally, we provide analysis of ALiBi to understand why it leads to\nbetter performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ofir Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach. (arXiv:2003.10715v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2003.10715","description":"<p>Knowledge about the software used in scientific investigations is necessary\nfor different reasons, including provenance of the results, measuring software\nimpact to attribute developers, and bibliometric software citation analysis in\ngeneral. Additionally, providing information about whether and how the software\nand the source code are available allows an assessment about the state and role\nof open source software in science in general. While such analyses can be done\nmanually, large scale analyses require the application of automated methods of\ninformation extraction and linking. In this paper, we present SoftwareKG - a\nknowledge graph that contains information about software mentions from more\nthan 51,000 scientific articles from the social sciences. A silver standard\ncorpus, created by a distant and weak supervision approach, and a gold standard\ncorpus, created by manual annotation, were used to train an LSTM based neural\nnetwork to identify software mentions in scientific articles. The model\nachieves a recognition rate of .82 F-score in exact matches. As a result, we\nidentified more than 133,000 software mentions. For entity disambiguation, we\nused the public domain knowledge base DBpedia. Furthermore, we linked the\nentities of the knowledge graph to other knowledge bases such as the Microsoft\nAcademic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we\nillustrate, how SoftwareKG can be used to assess the role of software in the\nsocial sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schindler_D/0/1/0/all/0/1\">David Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zapilko_B/0/1/0/all/0/1\">Benjamin Zapilko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1\">Frank Kr&#xfc;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review of on-device fully neural end-to-end automatic speech recognition algorithms. (arXiv:2012.07974v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.07974","description":"<p>In this paper, we review various end-to-end automatic speech recognition\nalgorithms and their optimization techniques for on-device applications.\nConventional speech recognition systems comprise a large number of discrete\ncomponents such as an acoustic model, a language model, a pronunciation model,\na text-normalizer, an inverse-text normalizer, a decoder based on a Weighted\nFinite State Transducer (WFST), and so on. To obtain sufficiently high speech\nrecognition accuracy with such conventional speech recognition systems, a very\nlarge language model (up to 100 GB) is usually needed. Hence, the corresponding\nWFST size becomes enormous, which prohibits their on-device implementation.\nRecently, fully neural network end-to-end speech recognition algorithms have\nbeen proposed. Examples include speech recognition systems based on\nConnectionist Temporal Classification (CTC), Recurrent Neural Network\nTransducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic\nChunk-wise Attention (MoChA), transformer-based speech recognition systems, and\nso on. These fully neural network-based systems require much smaller memory\nfootprints compared to conventional algorithms, therefore their on-device\nimplementation has become feasible. In this paper, we review such end-to-end\nspeech recognition models. We extensively discuss their structures,\nperformance, and advantages compared to conventional algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowda_D/0/1/0/all/0/1\">Dhananjaya Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankur Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Abhinav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Changwoo Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00433","description":"<p>Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nThis is problematic, as previous work has demonstrated possible trade-offs and\nnegative consequences to disclosive transparency, such as a confusion effect,\nwhere 'too much information' clouds a reader's understanding of what a system\ndescription means. Disclosive transparency's subjective nature has rendered\ndeep study into these problems and their remedies difficult. To improve this\nstate of affairs, We introduce neural language model-based probabilistic\nmetrics to directly model disclosive transparency, and demonstrate that they\ncorrelate with user and expert opinions of system transparency, making them a\nvalid objective proxy. Finally, we demonstrate the use of these metrics in a\npilot study quantifying the relationships between transparency, confusion, and\nuser perceptions in a corpus of real NLP system descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semi-Supervised Learning: An Approach To Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems. (arXiv:2104.03643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03643","description":"<p>Air traffic management and specifically air-traffic control (ATC) rely mostly\non voice communications between Air Traffic Controllers (ATCos) and pilots. In\nmost cases, these voice communications follow a well-defined grammar that could\nbe leveraged in Automatic Speech Recognition (ASR) technologies. The callsign\nused to address an airplane is an essential part of all ATCo-pilot\ncommunications. We propose a two-steps approach to add contextual knowledge\nduring semi-supervised training to reduce the ASR system error rates at\nrecognizing the part of the utterance that contains the callsign. Initially, we\nrepresent in a WFST the contextual knowledge (i.e. air-surveillance data) of an\nATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the\ncontextual knowledge is added by second-pass decoding (i.e. lattice\nre-scoring). Results show that `unseen domains' (e.g. data from airports not\npresent in the supervised training data) are further aided by contextual SSL\nwhen compared to standalone SSL. For this task, we introduce the Callsign Word\nError Rate (CA-WER) as an evaluation metric, which only assesses ASR\nperformance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER\nrelative improvement applying SSL with an additional 17.5% CA-WER improvement\nby adding contextual knowledge during SSL on a challenging ATC-based test set\ngathered from LiveATC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vesely_K/0/1/0/all/0/1\">Karel Vesel&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocour_M/0/1/0/all/0/1\">Martin Kocour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szoke_I/0/1/0/all/0/1\">Igor Sz&#xf6;ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Remove: Towards Isotropic Pre-trained BERT Embedding. (arXiv:2104.05274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05274","description":"<p>Pre-trained language models such as BERT have become a more common choice of\nnatural language processing (NLP) tasks. Research in word representation shows\nthat isotropic embeddings can significantly improve performance on downstream\ntasks. However, we measure and analyze the geometry of pre-trained BERT\nembedding and find that it is far from isotropic. We find that the word vectors\nare not centered around the origin, and the average cosine similarity between\ntwo random words is much higher than zero, which indicates that the word\nvectors are distributed in a narrow cone and deteriorate the representation\ncapacity of word embedding. We propose a simple, and yet effective method to\nfix this problem: remove several dominant directions of BERT embedding with a\nset of learnable weights. We train the weights on word similarity tasks and\nshow that processed embedding is more isotropic. Our method is evaluated on\nthree standardized tasks: word similarity, word analogy, and semantic textual\nsimilarity. In all tasks, the word embedding processed by our method\nconsistently outperforms the original embedding (with average improvement of\n13% on word analogy and 16% on semantic textual similarity) and two baseline\nmethods. Our method is also proven to be more robust to changes of\nhyperparameter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06669","description":"<p>We propose the task of Narrative Reordering (NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot. We\npresent a dataset, NAREORC, with human rewritings of stories within ROCStories\nin non-linear orders, and conduct a detailed analysis of it. Further, we\npropose novel task-specific training methods with suitable evaluation metrics.\nWe perform experiments on NAREORC using state-of-the-art models such as BART\nand T5 and conduct extensive automatic and human evaluations. We demonstrate\nthat NAREOR is a challenging task with potential for further exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08808","description":"<p>The ability to continuously expand knowledge over time and utilize it to\nrapidly generalize to new tasks is a key feature of human linguistic\nintelligence. Existing models that pursue rapid generalization to new tasks\n(e.g., few-shot learning methods), however, are mostly trained in a single shot\non fixed datasets, unable to dynamically expand their knowledge; while\ncontinual learning algorithms are not specifically designed for rapid\ngeneralization. We present a new learning setup, Continual Learning of Few-Shot\nLearners (CLIF), to address the challenges of both learning settings in a\nunified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks\narriving sequentially, accumulating knowledge for improved generalization to\nnew tasks, while also retaining performance on the tasks learned earlier. We\nexamine how the generalization ability is affected in the continual learning\nsetup, evaluate a number of continual learning algorithms, and propose a novel\nregularized adapter generation approach. We find that catastrophic forgetting\naffects generalization ability to a less degree than performance on seen tasks;\nwhile continual learning algorithms can still bring considerable benefit to the\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts. (arXiv:2104.08809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08809","description":"<p>Determining coreference of concept mentions across multiple documents is a\nfundamental task in natural language understanding. Work on cross-document\ncoreference resolution (CDCR) typically considers mentions of events in the\nnews, which seldom involve abstract technical concepts that are prevalent in\nscience and technology. These complex concepts take diverse or ambiguous forms\nand have many hierarchical levels of granularity (e.g., tasks and subtasks),\nposing challenges for CDCR. We present a new task of Hierarchical CDCR (H-CDCR)\nwith the goal of jointly inferring coreference clusters and hierarchy between\nthem. We create SciCo, an expert-annotated dataset for H-CDCR in scientific\npapers, 3X larger than the prominent ECB+ resource. We study strong baseline\nmodels that we customize for H-CDCR, and highlight challenges for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08836","description":"<p>Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09106","description":"<p>Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is somewhat\nmissing. We propose an acoustic data-driven subword modeling (ADSM) approach\nthat adapts the advantages of several text-based and acoustic-based subword\nmethods into one pipeline. With a fully acoustic-oriented label design and\nlearning process, ADSM produces acoustic-structured subword units and\nacoustic-matched target sequence for further ASR training. The obtained ADSM\nlabels are evaluated with different end-to-end ASR approaches including CTC,\nRNN-Transducer and attention models. Experiments on the LibriSpeech corpus show\nthat ADSM clearly outperforms both byte pair encoding (BPE) and\npronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis\nshows that ADSM achieves acoustically more logical word segmentation and more\nbalanced sequence length, and thus, is suitable for both time-synchronous and\nlabel-synchronous models. We also briefly describe how to apply acoustic-based\nsubword regularization and unseen text segmentation using ADSM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zuoyun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Guided Curriculum Learning for Neural Machine Translation. (arXiv:2105.04475v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.04475","description":"<p>In the field of machine learning, the well-trained model is assumed to be\nable to recover the training labels, i.e. the synthetic labels predicted by the\nmodel should be as close to the ground-truth labels as possible. Inspired by\nthis, we propose a self-guided curriculum strategy to encourage the learning of\nneural machine translation (NMT) models to follow the above recovery criterion,\nwhere we cast the recovery degree of each training example as its learning\ndifficulty. Specifically, we adopt the sentence level BLEU score as the proxy\nof recovery degree. Different from existing curricula relying on linguistic\nprior knowledge or third-party language models, our chosen learning difficulty\nis more suitable to measure the degree of knowledge mastery of the NMT models.\nExperiments on translation benchmarks, including WMT14\nEnglish$\\Rightarrow$German and WMT17 Chinese$\\Rightarrow$English, demonstrate\nthat our approach can consistently improve translation performance against\nstrong baseline Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1\">Ryohei Sasano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koichi Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08988","description":"<p>Social media platforms provide convenient means for users to participate in\nmultiple online activities on various contents and create fast widespread\ninteractions. However, this rapidly growing access has also increased the\ndiverse information, and characterizing user types to understand people's\nlifestyle decisions shared in social media is challenging. In this paper, we\npropose a weakly supervised graph embedding based framework for understanding\nuser types. We evaluate the user embedding learned using weak supervision over\nwell-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.\nExperiments on real-world datasets demonstrate that the proposed framework\noutperforms the baselines for detecting user types. Finally, we illustrate data\nanalysis on different types of users (e.g., practitioner vs. promotional) from\nour dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our\nmethod for constructing user representation readily generalizes to other\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Communication with Adaptive Universal Transformer. (arXiv:2108.09119v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09119","description":"<p>With the development of deep learning (DL), natural language processing (NLP)\nmakes it possible for us to analyze and understand a large amount of language\ntexts. Accordingly, we can achieve a semantic communication in terms of joint\nsemantic source and channel coding over a noisy channel with the help of NLP.\nHowever, the existing method to realize this goal is to use a fixed transformer\nof NLP while ignoring the difference of semantic information contained in each\nsentence. To solve this problem, we propose a new semantic communication system\nbased on Universal Transformer. Compared with the traditional transformer, an\nadaptive circulation mechanism is introduced in the Universal Transformer.\nThrough the introduction of the circulation mechanism, the new semantic\ncommunication system can be more flexible to transmit sentences with different\nsemantic information, and achieve better end-to-end performance under various\nchannel conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chenghui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10750","description":"<p>Relation Extraction (RE) from tables is the task of identifying relations\nbetween pairs of columns of a table. Generally, RE models for this task require\nlabelled tables for training. These labelled tables can also be generated\nartificially from a Knowledge Graph (KG), which makes the cost to acquire them\nmuch lower in comparison to manual annotations. However, unlike real tables,\nthese synthetic tables lack associated metadata, such as, column-headers,\ncaptions, etc; this is because synthetic tables are created out of KGs that do\nnot store such metadata. Meanwhile, previous works have shown that metadata is\nimportant for accurate RE from tables. To address this issue, we propose\nmethods to artificially create some of this metadata for synthetic tables.\nAfterward, we experiment with a BERT-based model, in line with recently\npublished works, that takes as input a combination of proposed artificial\nmetadata and table content. Our empirical results show that this leads to an\nimprovement of 9\\%-45\\% in F1 score, in absolute terms, over 2 tabular\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+singh_G/0/1/0/all/0/1\">Gaurav singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Joshua Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10939","description":"<p>Offensive Language detection in social media platforms has been an active\nfield of research over the past years. In non-native English spoken countries,\nsocial media users mostly use a code-mixed form of text in their\nposts/comments. This poses several challenges in the offensive content\nidentification tasks, and considering the low resources available for Tamil,\nthe task becomes much harder. The current study presents extensive experiments\nusing multiple deep learning, and transfer learning models to detect offensive\ncontent on YouTube. We propose a novel and flexible approach of selective\ntranslation and transliteration techniques to reap better results from\nfine-tuning and ensembling multilingual transformer networks like BERT, Distil-\nBERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best\nmodel for this task. The best performing models were ULMFiT and mBERTBiLSTM for\nthis Tamil code-mix dataset instead of more popular transfer learning models\nsuch as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The\nproposed model ULMFiT and mBERTBiLSTM yielded good results and are promising\nfor effective offensive speech identification in low-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasantharajan_C/0/1/0/all/0/1\">Charangan Vasantharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayasivam_U/0/1/0/all/0/1\">Uthayasanker Thayasivam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutReader: Pre-training of Text and Layout for Reading Order Detection. (arXiv:2108.11591v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11591","description":"<p>Reading order detection is the cornerstone to understanding visually-rich\ndocuments (e.g., receipts and forms). Unfortunately, no existing work took\nadvantage of advanced deep learning models because it is too laborious to\nannotate a large enough dataset. We observe that the reading order of WORD\ndocuments is embedded in their XML metadata; meanwhile, it is easy to convert\nWORD documents to PDFs or images. Therefore, in an automated manner, we\nconstruct ReadingBank, a benchmark dataset that contains reading order, text,\nand layout information for 500,000 document images covering a wide spectrum of\ndocument types. This first-ever large-scale dataset unleashes the power of deep\nneural networks for reading order detection. Specifically, our proposed\nLayoutReader captures the text and layout information for reading order\nprediction using the seq2seq model. It performs almost perfectly in reading\norder detection and significantly improves both open-source and commercial OCR\nengines in ordering text lines in their results in our experiments. We will\nrelease the dataset and model at \\url{https://aka.ms/layoutreader}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Negative Sampling for Unlabeled Entity Problem in Named Entity Recognition. (arXiv:2108.11607v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11607","description":"<p>In many situations (e.g., distant supervision), unlabeled entity problem\nseriously degrades the performances of named entity recognition (NER) models.\nRecently, this issue has been well addressed by a notable approach based on\nnegative sampling. In this work, we perform two studies along this direction.\nFirstly, we analyze why negative sampling succeeds both theoretically and\nempirically. Based on the observation that named entities are highly sparse in\ndatasets, we show a theoretical guarantee that, for a long sentence, the\nprobability of containing no unlabeled entities in sampled negatives is high.\nMissampling tests on synthetic datasets have verified our guarantee in\npractice. Secondly, to mine hard negatives and further reduce missampling\nrates, we propose a weighted and adaptive sampling distribution for negative\nsampling. Experiments on synthetic datasets and well-annotated datasets show\nthat our method significantly improves negative sampling in robustness and\neffectiveness. We also have achieved new state-of-the-art results on real-world\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning Cross-modal Contrastive Features for Video Domain Adaptation. (arXiv:2108.11974v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11974","description":"<p>Learning transferable and domain adaptive feature representations from videos\nis important for video-relevant tasks such as action recognition. Existing\nvideo domain adaptation methods mainly rely on adversarial feature alignment,\nwhich has been derived from the RGB image space. However, video data is usually\nassociated with multi-modal information, e.g., RGB and optical flow, and thus\nit remains a challenge to design a better method that considers the cross-modal\ninputs under the cross-domain adaptation setting. To this end, we propose a\nunified framework for video domain adaptation, which simultaneously regularizes\ncross-modal and cross-domain feature representations. Specifically, we treat\neach modality in a domain as a view and leverage the contrastive learning\ntechnique with properly designed sampling strategies. As a result, our\nobjectives regularize feature spaces, which originally lack the connection\nacross modalities or have less alignment across domains. We conduct experiments\non domain adaptive action recognition benchmark datasets, i.e., UCF, HMDB, and\nEPIC-Kitchens, and demonstrate the effectiveness of our components against\nstate-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bingbing Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection in Medical Imaging -- A Mini Review. (arXiv:2108.11986v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11986","description":"<p>The increasing digitization of medical imaging enables machine learning based\nimprovements in detecting, visualizing and segmenting lesions, easing the\nworkload for medical experts. However, supervised machine learning requires\nreliable labelled data, which is is often difficult or impossible to collect or\nat least time consuming and thereby costly. Therefore methods requiring only\npartly labeled data (semi-supervised) or no labeling at all (unsupervised\nmethods) have been applied more regularly. Anomaly detection is one possible\nmethodology that is able to leverage semi-supervised and unsupervised methods\nto handle medical imaging tasks like classification and segmentation. This\npaper uses a semi-exhaustive literature review of relevant anomaly detection\npapers in medical imaging to cluster into applications, highlight important\nresults, establish lessons learned and give further advice on how to approach\nanomaly detection in medical imaging. The qualitative analysis is based on\ngoogle scholar and 4 different search terms, resulting in 120 different\nanalysed papers. The main results showed that the current research is mostly\nmotivated by reducing the need for labelled data. Also, the successful and\nsubstantial amount of research in the brain MRI domain shows the potential for\napplications in further domains like OCT and chest X-ray.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian E. Tschuchnig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transformer based Semantic Segmentation Networks for Pathological Image Segmentation. (arXiv:2108.11993v1 [eess.IV])","link":"http://arxiv.org/abs/2108.11993","description":"<p>Histopathology has played an essential role in cancer diagnosis. With the\nrapid advances in convolutional neural networks (CNN). Various CNN-based\nautomated pathological image segmentation approaches have been developed in\ncomputer-assisted pathological image analysis. In the past few years,\nTransformer neural networks (Transformer) have shown the unique merit of\ncapturing the global long distance dependencies across the entire image as a\nnew deep learning paradigm. Such merit is appealing for exploring spatially\nheterogeneous pathological images. However, there have been very few, if any,\nstudies that have systematically evaluated the current Transformer based\napproaches in pathological image segmentation. To assess the performance of\nTransformer segmentation models on whole slide images (WSI), we quantitatively\nevaluated six prevalent transformer-based models on tumor segmentation, using\nthe widely used PAIP liver histopathological dataset. For a more comprehensive\nanalysis, we also compare the transformer-based models with six major\ntraditional CNN-based models. The results show that the Transformer-based\nmodels exhibit a general superior performance over the CNN-based models. In\nparticular, Segmenter, Swin-Transformer and TransUNet, all transformer-based,\ncame out as the best performers among the twelve evaluated models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_C/0/1/0/all/0/1\">Cam Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers. (arXiv:2108.11996v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11996","description":"<p>In this work, we consider the problem of sequence-to-sequence alignment for\nsignals containing outliers. Assuming the absence of outliers, the standard\nDynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment\nbetween two (generally) variable-length sequences. While DTW is robust to\ntemporal shifts and dilations of the signal, it fails to align sequences in a\nmeaningful way in the presence of outliers that can be arbitrarily interspersed\nin the sequences. To address this problem, we introduce Drop-DTW, a novel\nalgorithm that aligns the common signal between the sequences while\nautomatically dropping the outlier elements from the matching. The entire\nprocedure is implemented as a single dynamic program that is efficient and\nfully differentiable. In our experiments, we show that Drop-DTW is a robust\nsimilarity measure for sequence retrieval and demonstrate its effectiveness as\na training loss on diverse applications. With Drop-DTW, we address temporal\nstep localization on instructional videos, representation learning from noisy\nvideos, and cross-modal representation learning for audio-visual retrieval and\nlocalization. In all applications, we take a weakly- or unsupervised approach\nand demonstrate state-of-the-art results under these settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dvornik_N/0/1/0/all/0/1\">Nikita Dvornik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadji_I/0/1/0/all/0/1\">Isma Hadji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jepson_A/0/1/0/all/0/1\">Allan D. Jepson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tutorial on Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12043","description":"<p>Disentangled representation learning has been proposed as an approach to\nlearning general representations. This can be done in the absence of, or with\nlimited, annotations. A good general representation can be readily fine-tuned\nfor new target tasks using modest amounts of data, or even be used directly in\nunseen domains achieving remarkable performance in the corresponding task. This\nalleviation of the data and annotation requirements offers tantalising\nprospects for tractable and affordable applications in computer vision and\nhealthcare. Finally, disentangled representations can offer model\nexplainability and can help us understand the underlying causal relations of\nthe factors of variation, increasing their suitability for real-world\ndeployment. In this tutorial paper, we will offer an overview of the\ndisentangled representation learning, its building blocks and criteria, and\ndiscuss applications in computer vision and medical imaging. We conclude our\ntutorial by presenting the identified opportunities for the integration of\nrecent machine learning advances into disentanglement, as well as the remaining\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1\">Pedro Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thermos_S/0/1/0/all/0/1\">Spyridon Thermos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ONeil_A/0/1/0/all/0/1\">Alison Q. O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A.Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultrafast Focus Detection for Automated Microscopy. (arXiv:2108.12050v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12050","description":"<p>Recent advances in scientific instruments have resulted in dramatic increase\nin the volumes and velocities of data being generated in every-day\nlaboratories. Scanning electron microscopy is one such example where\ntechnological advancements are now overwhelming scientists with critical data\nfor montaging, alignment, and image segmentation -- key practices for many\nscientific domains, including, for example, neuroscience, where they are used\nto derive the anatomical relationships of the brain. These instruments now\nnecessitate equally advanced computing resources and techniques to realize\ntheir full potential. Here we present a fast out-of-focus detection algorithm\nfor electron microscopy images collected serially and demonstrate that it can\nbe used to provide near-real time quality control for neurology research. Our\ntechnique, Multi-scale Histologic Feature Detection, adapts classical computer\nvision techniques and is based on detecting various fine-grained histologic\nfeatures. We further exploit the inherent parallelism in the technique by\nemploying GPGPU primitives in order to accelerate characterization. Tests are\nperformed that demonstrate near-real-time detection of out-of-focus conditions.\nWe deploy these capabilities as a funcX function and show that it can be\napplied as data are collected using an automated pipeline . We discuss\nextensions that enable scaling out to support multi-beam microscopes and\nintegration with existing focus systems for purposes of implementing\nauto-focus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Levental_M/0/1/0/all/0/1\">Maksim Levental</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chard_R/0/1/0/all/0/1\">Ryan Chard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wildenberg_G/0/1/0/all/0/1\">Gregg A. Wildenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12056","description":"<p>Existing machines are functionally specific tools that were made for easy\nprediction and control. Tomorrow's machines may be closer to biological systems\nin their mutability, resilience, and autonomy. But first they must be capable\nof learning, and retaining, new information without repeated exposure to it.\nPast efforts to engineer such systems have sought to build or regulate\nartificial neural networks using task-specific modules with constrained\ncircumstances of application. This has not yet enabled continual learning over\nlong sequences of previously unseen data without corrupting existing knowledge:\na problem known as catastrophic forgetting. In this paper, we introduce a\nsystem that can learn sequentially over previously unseen datasets (ImageNet,\nCIFAR-100) with little forgetting over time. This is accomplished by regulating\nthe activity of weights in a convolutional neural network on the basis of\ninputs using top-down modulation generated by a second feed-forward neural\nnetwork. We find that our method learns continually under domain transfer with\nsparse bursts of activity in weights that are recycled across tasks, rather\nthan by maintaining task-specific modules. Sparse synaptic bursting is found to\nbalance enhanced and diminished activity in a way that facilitates adaptation\nto new inputs without corrupting previously acquired functions. This behavior\nemerges during a prior meta-learning phase in which regulated synapses are\nselectively disinhibited, or grown, from an initial state of uniform\nsuppression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1\">Shawn L. Beaulieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1\">Jeff Clune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1\">Nick Cheney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Stable Configurations for Semantic Placement of Novel Objects. (arXiv:2108.12062v1 [cs.RO])","link":"http://arxiv.org/abs/2108.12062","description":"<p>Human environments contain numerous objects configured in a variety of\narrangements. Our goal is to enable robots to repose previously unseen objects\naccording to learned semantic relationships in novel environments. We break\nthis problem down into two parts: (1) finding physically valid locations for\nthe objects and (2) determining if those poses satisfy learned, high-level\nsemantic relationships. We build our models and training from the ground up to\nbe tightly integrated with our proposed planning algorithm for semantic\nplacement of unknown objects. We train our models purely in simulation, with no\nfine-tuning needed for use in the real world. Our approach enables motion\nplanning for semantic rearrangement of unknown objects in scenes with varying\ngeometry from only RGB-D sensing. Our experiments through a set of simulated\nablations demonstrate that using a relational classifier alone is not\nsufficient for reliable planning. We further demonstrate the ability of our\nplanner to generate and execute diverse manipulation plans through a set of\nreal-world experiments with a variety of objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chris Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_T/0/1/0/all/0/1\">Tucker Hermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Automatic Image Content Retrieval Method for better Mobile Device Display User Experiences. (arXiv:2108.12068v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12068","description":"<p>A growing number of commercially available mobile phones come with integrated\nhigh-resolution digital cameras. That enables a new class of dedicated\napplications to image analysis such as mobile visual search, image cropping,\nobject detection, content-based image retrieval, image classification. In this\npaper, a new mobile application for image content retrieval and classification\nfor mobile device display is proposed to enrich the visual experience of users.\nThe mobile application can extract a certain number of images based on the\ncontent of an image with visual saliency methods aiming at detecting the most\ncritical regions in a given image from a perceptual viewpoint. First, the most\ncritical areas from a perceptual perspective are extracted using the local\nmaxima of a 2D saliency function. Next, a salient region is cropped using the\nbounding box centred on the local maxima of the thresholded Saliency Map of the\nimage. Then, each image crop feds into an Image Classification system based on\nSVM and SIFT descriptors to detect the class of object present in the image.\nImageNet repository was used as the reference for semantic category\nclassification. Android platform was used to implement the mobile application\non a client-server architecture. A mobile client sends the photo taken by the\ncamera to the server, which processes the image and returns the results (image\ncontents such as image crops and related target classes) to the mobile client.\nThe application was run on thousands of pictures and showed encouraging results\ntowards a better user visual experience with mobile displays.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruno_A/0/1/0/all/0/1\">Alessandro Bruno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Underwater Sonar Images by the Learned Descriptor Based on Style Transfer Method. (arXiv:2108.12072v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12072","description":"<p>This paper proposes a method that combines the style transfer technique and\nthe learned descriptor to enhance the matching performances of underwater sonar\nimages. In the field of underwater vision, sonar is currently the most\neffective long-distance detection sensor, it has excellent performances in map\nbuilding and target search tasks. However, the traditional image matching\nalgorithms are all developed based on optical images. In order to solve this\ncontradiction, the style transfer method is used to convert the sonar images\ninto optical styles, and at the same time, the learned descriptor with\nexcellent expressiveness for sonar images matching is introduced. Experiments\nshow that this method significantly enhances the matching quality of sonar\nimages. In addition, it also provides new ideas for the preprocessing of\nunderwater sonar images by using the style transfer approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoteng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Citong Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection and Continual Learning of Novel Face Presentation Attacks. (arXiv:2108.12081v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12081","description":"<p>Advances in deep learning, combined with availability of large datasets, have\nled to impressive improvements in face presentation attack detection research.\nHowever, state-of-the-art face antispoofing systems are still vulnerable to\nnovel types of attacks that are never seen during training. Moreover, even if\nsuch attacks are correctly detected, these systems lack the ability to adapt to\nnewly encountered attacks. The post-training ability of continually detecting\nnew types of attacks and self-adaptation to identify these attack types, after\nthe initial detection phase, is highly appealing. In this paper, we enable a\ndeep neural network to detect anomalies in the observed input data points as\npotential new types of attacks by suppressing the confidence-level of the\nnetwork outside the training samples' distribution. We then use experience\nreplay to update the model to incorporate knowledge about new types of attacks\nwithout forgetting the past learned attack types. Experimental results are\nprovided to demonstrate the effectiveness of the proposed method on two\nbenchmark datasets as well as a newly introduced dataset which exhibits a large\nvariety of attack types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spinoulas_L/0/1/0/all/0/1\">Leonidas Spinoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_M/0/1/0/all/0/1\">Mohamed Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathai_J/0/1/0/all/0/1\">Joe Mathai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abd_Almageed_W/0/1/0/all/0/1\">Wael Abd-Almageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Denoising Method for Side Scan Sonar Images without High-quality Reference Data. (arXiv:2108.12083v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12083","description":"<p>Subsea images measured by the side scan sonars (SSSs) are necessary visual\ndata in the process of deep-sea exploration by using the autonomous underwater\nvehicles (AUVs). They could vividly reflect the topography of the seabed, but\nusually accompanied by complex and severe noise. This paper proposes a deep\ndenoising method for SSS images without high-quality reference data, which uses\none single noise SSS image to perform self-supervised denoising. Compared with\nthe classical artificially designed filters, the deep denoising method shows\nobvious advantages. The denoising experiments are performed on the real seabed\nSSS images, and the results demonstrate that our proposed method could\neffectively reduce the noise on the SSS image while minimizing the image\nquality and detail loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoteng Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_C/0/1/0/all/0/1\">Changli Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_C/0/1/0/all/0/1\">Citong Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOVEA: Foveated Image Magnification for Autonomous Navigation. (arXiv:2108.12102v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12102","description":"<p>Efficient processing of high-resolution video streams is safety-critical for\nmany robotics applications such as autonomous driving. Image downsampling is a\ncommonly adopted technique to ensure the latency constraint is met. However,\nthis naive approach greatly restricts an object detector's capability to\nidentify small objects. In this paper, we propose an attentional approach that\nelastically magnifies certain regions while maintaining a small input canvas.\nThe magnified regions are those that are believed to have a high probability of\ncontaining an object, whose signal can come from a dataset-wide prior or\nframe-level prior computed from recent object predictions. The magnification is\nimplemented by a KDE-based mapping to transform the bounding boxes into warping\nparameters, which are then fed into an image sampler with anti-cropping\nregularization. The detector is then fed with the warped image and we apply a\ndifferentiable backward mapping to get bounding box outputs in the original\nspace. Our regional magnification allows algorithms to make better use of\nhigh-resolution input without incurring the cost of high-resolution processing.\nOn the autonomous driving datasets Argoverse-HD and BDD100K, we show our\nproposed method boosts the detection AP over standard Faster R-CNN, with and\nwithout finetuning. Additionally, building on top of the previous\nstate-of-the-art in streaming detection, our method sets a new record for\nstreaming AP on Argoverse-HD (from 17.8 to 23.0 on a GTX 1080 Ti GPU),\nsuggesting that it has achieved a superior accuracy-latency tradeoff.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thavamani_C/0/1/0/all/0/1\">Chittesh Thavamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengtian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cebron_N/0/1/0/all/0/1\">Nicolas Cebron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binocular Mutual Learning for Improving Few-shot Classification. (arXiv:2108.12104v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12104","description":"<p>Most of the few-shot learning methods learn to transfer knowledge from\ndatasets with abundant labeled data (i.e., the base set). From the perspective\nof class space on base set, existing methods either focus on utilizing all\nclasses under a global view by normal pretraining, or pay more attention to\nadopt an episodic manner to train meta-tasks within few classes in a local\nview. However, the interaction of the two views is rarely explored. As the two\nviews capture complementary information, we naturally think of the\ncompatibility of them for achieving further performance gains. Inspired by the\nmutual learning paradigm and binocular parallax, we propose a unified\nframework, namely Binocular Mutual Learning (BML), which achieves the\ncompatibility of the global view and the local view through both intra-view and\ncross-view modeling. Concretely, the global view learns in the whole class\nspace to capture rich inter-class relationships. Meanwhile, the local view\nlearns in the local class space within each episode, focusing on matching\npositive pairs correctly. In addition, cross-view mutual interaction further\npromotes the collaborative learning and the implicit exploration of useful\nknowledge from each other. During meta-test, binocular embeddings are\naggregated together to support decision-making, which greatly improve the\naccuracy of classification. Extensive experiments conducted on multiple\nbenchmarks including cross-domain validation confirm the effectiveness of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangtao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition Awareness: An Application of Latent Cognizance to Open-Set Recognition. (arXiv:2108.12115v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12115","description":"<p>This study investigates an application of a new probabilistic interpretation\nof a softmax output to Open-Set Recognition (OSR). Softmax is a mechanism\nwildly used in classification and object recognition.\n</p>\n<p>However, a softmax mechanism forces a model to operate under a closed-set\nparadigm, i.e., to predict an object class out of a set of pre-defined labels.\n</p>\n<p>This characteristic contributes to efficacy in classification, but poses a\nrisk of non-sense prediction in object recognition.\n</p>\n<p>Object recognition is often operated under a dynamic and diverse condition.\n</p>\n<p>A foreign object -- an object of any unprepared class -- can be encountered\nat any time.\n</p>\n<p>OSR is intended to address an issue of identifying a foreign object in object\nrecognition.\n</p>\n<p>Based on Bayes theorem and the emphasis of conditioning on the context,\nsoftmax inference has been re-interpreted.\n</p>\n<p>This re-interpretation has led to a new approach to OSR, called Latent\nCognizance (LC). Our investigation employs various scenarios, using Imagenet\n2012 dataset as well as fooling and open-set images. The findings support LC\nhypothesis and show its effectiveness on OSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katanyukul_T/0/1/0/all/0/1\">Tatpong Katanyukul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakjai_P/0/1/0/all/0/1\">Pisit Nakjai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Densely-Populated Traffic Detection using YOLOv5 and Non-Maximum Suppression Ensembling. (arXiv:2108.12118v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12118","description":"<p>Vehicular object detection is the heart of any intelligent traffic system. It\nis essential for urban traffic management. R-CNN, Fast R-CNN, Faster R-CNN and\nYOLO were some of the earlier state-of-the-art models. Region based CNN methods\nhave the problem of higher inference time which makes it unrealistic to use the\nmodel in real-time. YOLO on the other hand struggles to detect small objects\nthat appear in groups. In this paper, we propose a method that can locate and\nclassify vehicular objects from a given densely crowded image using YOLOv5. The\nshortcoming of YOLO was solved my ensembling 4 different models. Our proposed\nmodel performs well on images taken from both top view and side view of the\nstreet in both day and night. The performance of our proposed model was\nmeasured on Dhaka AI dataset which contains densely crowded vehicular images.\nOur experiment shows that our model achieved mAP@0.5 of 0.458 with inference\ntime of 0.75 sec which outperforms other state-of-the-art models on\nperformance. Hence, the model can be implemented in the street for real-time\ntraffic detection which can be used for traffic control and data collection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_R/0/1/0/all/0/1\">Raian Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azad_Z/0/1/0/all/0/1\">Zadid Bin Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Bakhtiar Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAE-GAN: Dynamic Aspect-aware GAN for Text-to-Image Synthesis. (arXiv:2108.12141v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12141","description":"<p>Text-to-image synthesis refers to generating an image from a given text\ndescription, the key goal of which lies in photo realism and semantic\nconsistency. Previous methods usually generate an initial image with sentence\nembedding and then refine it with fine-grained word embedding. Despite the\nsignificant progress, the 'aspect' information (e.g., red eyes) contained in\nthe text, referring to several words rather than a word that depicts 'a\nparticular part or feature of something', is often ignored, which is highly\nhelpful for synthesizing image details. How to make better utilization of\naspect information in text-to-image synthesis still remains an unresolved\nchallenge. To address this problem, in this paper, we propose a Dynamic\nAspect-awarE GAN (DAE-GAN) that represents text information comprehensively\nfrom multiple granularities, including sentence-level, word-level, and\naspect-level. Moreover, inspired by human learning behaviors, we develop a\nnovel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an\nAttended Global Refinement (AGR) module and an Aspect-aware Local Refinement\n(ALR) module are alternately employed. AGR utilizes word-level embedding to\nglobally enhance the previously generated image, while ALR dynamically employs\naspect-level embedding to refine image details from a local perspective.\nFinally, a corresponding matching loss function is designed to ensure the\ntext-image semantic consistency at different levels. Extensive experiments on\ntwo well-studied and publicly available datasets (i.e., CUB-200 and COCO)\ndemonstrate the superiority and rationality of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_S/0/1/0/all/0/1\">Shulan Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Matching Algorithm based on Image Attribute Transfer and Local Features for Underwater Acoustic and Optical Images. (arXiv:2108.12151v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12151","description":"<p>In the field of underwater vision research, image matching between the sonar\nsensors and optical cameras has always been a challenging problem. Due to the\ndifference in the imaging mechanism between them, which are the gray value,\ntexture, contrast, etc. of the acoustic images and the optical images are also\nvariant in local locations, which makes the traditional matching method based\non the optical image invalid. Coupled with the difficulties and high costs of\nunderwater data acquisition, it further affects the research process of\nacousto-optic data fusion technology. In order to maximize the use of\nunderwater sensor data and promote the development of multi-sensor information\nfusion (MSIF), this study applies the image attribute transfer method based on\ndeep learning approach to solve the problem of acousto-optic image matching,\nthe core of which is to eliminate the imaging differences between them as much\nas possible. At the same time, the advanced local feature descriptor is\nintroduced to solve the challenging acousto-optic matching problem.\nExperimental results show that our proposed method could preprocess\nacousto-optic images effectively and obtain accurate matching results.\nAdditionally, the method is based on the combination of image depth semantic\nlayer, and it could indirectly display the local feature matching relationship\nbetween original image pair, which provides a new solution to the underwater\nmulti-sensor image matching problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoteng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Changli Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Citong Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection of Defect using Energy of Point Pattern Features within Random Finite Set Framework. (arXiv:2108.12159v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12159","description":"<p>In this paper, we propose an efficient approach for industrial defect\ndetection that is modeled based on anomaly detection using point pattern data.\nMost recent works use \\textit{global features} for feature extraction to\nsummarize image content. However, global features are not robust against\nlighting and viewpoint changes and do not describe the image's geometrical\ninformation to be fully utilized in the manufacturing industry. To the best of\nour knowledge, we are the first to propose using transfer learning of\nlocal/point pattern features to overcome these limitations and capture\ngeometrical information of the image regions. We model these local/point\npattern features as a random finite set (RFS). In addition we propose RFS\nenergy, in contrast to RFS likelihood as anomaly score. The similarity\ndistribution of point pattern features of the normal sample has been modeled as\na multivariate Gaussian. Parameters learning of the proposed RFS energy does\nnot require any heavy computation. We evaluate the proposed approach on the\nMVTec AD dataset, a multi-object defect detection dataset. Experimental results\nshow the outstanding performance of our proposed approach compared to the\nstate-of-the-art methods, and the proposed RFS energy outperforms the\nstate-of-the-art in the few shot learning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamoona_A/0/1/0/all/0/1\">Ammar Mansoor Kamoona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gostar_A/0/1/0/all/0/1\">Amirali Khodadadian Gostar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1\">Alireza Bab-Hadiashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoseinnezhad_R/0/1/0/all/0/1\">Reza Hoseinnezhad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LassoLayer: Nonlinear Feature Selection by Switching One-to-one Links. (arXiv:2108.12165v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12165","description":"<p>Along with the desire to address more complex problems, feature selection\nmethods have gained in importance. Feature selection methods can be classified\ninto wrapper method, filter method, and embedded method. Being a powerful\nembedded feature selection method, Lasso has attracted the attention of many\nresearchers. However, as a linear approach, the applicability of Lasso has been\nlimited. In this work, we propose LassoLayer that is one-to-one connected and\ntrained by L1 optimization, which work to drop out unnecessary units for\nprediction. For nonlinear feature selections, we build LassoMLP: the network\nequipped with LassoLayer as its first layer. Because we can insert LassoLayer\nin any network structure, it can harness the strength of neural network\nsuitable for tasks where feature selection is needed. We evaluate LassoMLP in\nfeature selection with regression and classification tasks. LassoMLP receives\nfeatures including considerable numbers of noisy factors that is harmful for\noverfitting. In the experiments using MNIST dataset, we confirm that LassoMLP\noutperforms the state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sudo_A/0/1/0/all/0/1\">Akihito Sudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1\">Teng Teck Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_M/0/1/0/all/0/1\">Masaki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tone_Y/0/1/0/all/0/1\">Yoshinori Tone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoCo DistillNet: a Cross-layer Correlation Distillation Network for Pathological Gastric Cancer Segmentation. (arXiv:2108.12173v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12173","description":"<p>In recent years, deep convolutional neural networks have made significant\nadvances in pathology image segmentation. However, pathology image segmentation\nencounters with a dilemma in which the higher-performance networks generally\nrequire more computational resources and storage. This phenomenon limits the\nemployment of high-accuracy networks in real scenes due to the inherent\nhigh-resolution of pathological images. To tackle this problem, we propose CoCo\nDistillNet, a novel Cross-layer Correlation (CoCo) knowledge distillation\nnetwork for pathological gastric cancer segmentation. Knowledge distillation, a\ngeneral technique which aims at improving the performance of a compact network\nthrough knowledge transfer from a cumbersome network. Concretely, our CoCo\nDistillNet models the correlations of channel-mixed spatial similarity between\ndifferent layers and then transfers this knowledge from a pre-trained\ncumbersome teacher network to a non-trained compact student network. In\naddition, we also utilize the adversarial learning strategy to further prompt\nthe distilling procedure which is called Adversarial Distillation (AD).\nFurthermore, to stabilize our training procedure, we make the use of the\nunsupervised Paraphraser Module (PM) to boost the knowledge paraphrase in the\nteacher network. As a result, extensive experiments conducted on the Gastric\nCancer Segmentation Dataset demonstrate the prominent ability of CoCo\nDistillNet which achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Aligned and Misaligned Features in One-stage Object Detection. (arXiv:2108.12176v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12176","description":"<p>One-stage object detectors rely on the point feature to predict the detection\nresults. However, the point feature may lack the information of the whole\nobject and lead to a misalignment between the object and the point feature.\nMeanwhile, the classification and regression tasks are sensitive to different\nobject regions, but their features are spatially aligned. In this paper, we\npropose a simple and plug-in operator that could generate aligned and\ndisentangled features for each task, respectively, without breaking the fully\nconvolutional manner. By predicting two task-aware point sets that are located\nin each sensitive region, this operator could disentangle the two tasks from\nthe spatial dimension, as well as align the point feature with the object. We\nalso reveal an interesting finding of the opposite effect of the long-range\nskip-connection for classification and regression, respectively. Based on the\nobject-aligned and task-disentangled operator (OAT), we propose OAT-Net, which\nexplicitly exploits point-set features for more accurate detection results.\nExtensive experiments on the MS-COCO dataset show that OAT can consistently\nboost different one-stage detectors by $\\sim$2 AP. Notably, OAT-Net achieves\n53.7 AP with Res2Net-101-DCN backbone and shows promising performance gain for\nsmall objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiSiam: Self-supervised Multi-instance Siamese Representation Learning for Autonomous Driving. (arXiv:2108.12178v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12178","description":"<p>Autonomous driving has attracted much attention over the years but turns out\nto be harder than expected, probably due to the difficulty of labeled data\ncollection for model training. Self-supervised learning (SSL), which leverages\nunlabeled data only for representation learning, might be a promising way to\nimprove model performance. Existing SSL methods, however, usually rely on the\nsingle-centric-object guarantee, which may not be applicable for multi-instance\ndatasets such as street scenes. To alleviate this limitation, we raise two\nissues to solve: (1) how to define positive samples for cross-view consistency\nand (2) how to measure similarity in multi-instance circumstances. We first\nadopt an IoU threshold during random cropping to transfer global-inconsistency\nto local-consistency. Then, we propose two feature alignment methods to enable\n2D feature maps for multi-instance similarity measurement. Additionally, we\nadopt intra-image clustering with self-attention for further mining intra-image\nsimilarity and translation-invariance. Experiments show that, when pre-trained\non Waymo dataset, our method called Multi-instance Siamese Network (MultiSiam)\nremarkably improves generalization ability and achieves state-of-the-art\ntransfer performance on autonomous driving benchmarks, including Cityscapes and\nBDD100K, while existing SSL counterparts like MoCo, MoCo-v2, and BYOL show\nsignificant performance drop. By pre-training on SODA10M, a large-scale\nautonomous driving dataset, MultiSiam exceeds the ImageNet pre-trained MoCo-v2,\ndemonstrating the potential of domain-specific pre-training. Code will be\navailable at https://github.com/KaiChen1998/MultiSiam.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIMo -- A Dataset for Indoor Building Monitoring with a Time-of-Flight Camera. (arXiv:2108.12196v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12196","description":"<p>We present TIMo (Time-of-flight Indoor Monitoring), a dataset for video-based\nmonitoring of indoor spaces captured using a time-of-flight (ToF) camera. The\nresulting depth videos feature people performing a set of different predefined\nactions, for which we provide detailed annotations. Person detection for people\ncounting and anomaly detection are the two targeted applications. Most existing\nsurveillance video datasets provide either grayscale or RGB videos. Depth\ninformation, on the other hand, is still a rarity in this class of datasets in\nspite of being popular and much more common in other research fields within\ncomputer vision. Our dataset addresses this gap in the landscape of\nsurveillance video datasets. The recordings took place at two different\nlocations with the ToF camera set up either in a top-down or a tilted\nperspective on the scene. The dataset is publicly available at\nhttps://vizta-tof.kl.dfki.de/timo-dataset-overview/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pascal Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anisimov_Y/0/1/0/all/0/1\">Yuriy Anisimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1\">Raisul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirbach_B/0/1/0/all/0/1\">Bruno Mirbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandidier_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Grandidier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Rule-Based Clutter Detection in Automotive Radar Data. (arXiv:2108.12224v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12224","description":"<p>Automotive radar sensors output a lot of unwanted clutter or ghost\ndetections, whose position and velocity do not correspond to any real object in\nthe sensor's field of view. This poses a substantial challenge for environment\nperception methods like object detection or tracking. Especially problematic\nare clutter detections that occur in groups or at similar locations in multiple\nconsecutive measurements. In this paper, a new algorithm for identifying such\nerroneous detections is presented. It is mainly based on the modeling of\nspecific commonly occurring wave propagation paths that lead to clutter. In\nparticular, the three effects explicitly covered are reflections at the\nunderbody of a car or truck, signals traveling back and forth between the\nvehicle on which the sensor is mounted and another object, and multipath\npropagation via specular reflection. The latter often occurs near guardrails,\nconcrete walls or similar reflective surfaces. Each of these effects is\ndescribed both theoretically and regarding a method for identifying the\ncorresponding clutter detections. Identification is done by analyzing\ndetections generated from a single sensor measurement only. The final algorithm\nis evaluated on recordings of real extra-urban traffic. For labeling, a\nsemi-automatic process is employed. The results are promising, both in terms of\nperformance and regarding the very low execution time. Typically, a large part\nof clutter is found, while only a small ratio of detections corresponding to\nreal objects are falsely classified by the algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kopp_J/0/1/0/all/0/1\">Johannes Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellner_D/0/1/0/all/0/1\">Dominik Kellner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piroli_A/0/1/0/all/0/1\">Aldi Piroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process. (arXiv:2108.12278v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12278","description":"<p>Recent research efforts in lifelong learning propose to grow a mixture of\nmodels to adapt to an increasing number of tasks. The proposed methodology\nshows promising results in overcoming catastrophic forgetting. However, the\ntheory behind these successful models is still not well understood. In this\npaper, we perform the theoretical analysis for lifelong learning models by\nderiving the risk bounds based on the discrepancy distance between the\nprobabilistic representation of data generated by the model and that\ncorresponding to the target dataset. Inspired by the theoretical analysis, we\nintroduce a new lifelong learning approach, namely the Lifelong Infinite\nMixture (LIMix) model, which can automatically expand its network architectures\nor choose an appropriate component to adapt its parameters for learning a new\ntask, while preserving its previously learnt information. We propose to\nincorporate the knowledge by means of Dirichlet processes by using a gating\nmechanism which computes the dependence between the knowledge learnt previously\nand stored in each component, and a new set of data. Besides, we train a\ncompact Student model which can accumulate cross-domain representations over\ntime and make quick inferences. The code is available at\nhttps://github.com/dtuzi123/Lifelong-infinite-mixture-model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bors_A/0/1/0/all/0/1\">Adrian G. Bors</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stop Throwing Away Discriminators! Re-using Adversaries for Test-Time Training. (arXiv:2108.12280v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12280","description":"<p>Thanks to their ability to learn data distributions without requiring paired\ndata, Generative Adversarial Networks (GANs) have become an integral part of\nmany computer vision methods, including those developed for medical image\nsegmentation. These methods jointly train a segmentor and an adversarial mask\ndiscriminator, which provides a data-driven shape prior. At inference, the\ndiscriminator is discarded, and only the segmentor is used to predict label\nmaps on test images. But should we discard the discriminator? Here, we argue\nthat the life cycle of adversarial discriminators should not end after\ntraining. On the contrary, training stable GANs produces powerful shape priors\nthat we can use to correct segmentor mistakes at inference. To achieve this, we\ndevelop stable mask discriminators that do not overfit or catastrophically\nforget. At test time, we fine-tune the segmentor on each individual test\ninstance until it satisfies the learned shape prior. Our method is simple to\nimplement and increases model performance. Moreover, it opens new directions\nfor re-using mask discriminators at inference. We release the code used for the\nexperiments at https://vios-s.github.io/adversarial-test-time-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1\">Gabriele Valvano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leo_A/0/1/0/all/0/1\">Andrea Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TE-YOLOF: Tiny and efficient YOLOF for blood cell detection. (arXiv:2108.12313v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12313","description":"<p>Blood cell detection in microscopic images is an essential branch of medical\nimage processing research. Since disease detection based on manual checking of\nblood cells is time-consuming and full of errors, testing of blood cells using\nobject detectors with Deep Convolutional Neural Network can be regarded as a\nfeasible solution. In this work, an object detector based on YOLOF has been\nproposed to detect blood cell objects such as red blood cells, white blood\ncells and platelets. This object detector is called TE-YOLOF, Tiny and\nEfficient YOLOF, and it is a One-Stage detector using dilated encoder to\nextract information from single-level feature maps. For increasing efficiency\nand flexibility, the EfficientNet Convolutional Neural Network is utilized as\nthe backbone for the proposed object detector. Furthermore, the Depthwise\nSeparable Convolution is applied to enhance the performance and minimize the\nparameters of the network. In addition, the Mish activation function is\nemployed to increase the precision. Extensive experiments on the BCCD dataset\nprove the effectiveness of the proposed model, which is more efficient than\nother existing studies for blood cell detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fanxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangkui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Pedestrian Detection and Tracking Framework for Autonomous Cars: Efficient Fusion of Camera and LiDAR Data. (arXiv:2108.12375v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12375","description":"<p>This paper presents a novel method for pedestrian detection and tracking by\nfusing camera and LiDAR sensor data. To deal with the challenges associated\nwith the autonomous driving scenarios, an integrated tracking and detection\nframework is proposed. The detection phase is performed by converting LiDAR\nstreams to computationally tractable depth images, and then, a deep neural\nnetwork is developed to identify pedestrian candidates both in RGB and depth\nimages. To provide accurate information, the detection phase is further\nenhanced by fusing multi-modal sensor information using the Kalman filter. The\ntracking phase is a combination of the Kalman filter prediction and an optical\nflow algorithm to track multiple pedestrians in a scene. We evaluate our\nframework on a real public driving dataset. Experimental results demonstrate\nthat the proposed method achieves significant performance improvement over a\nbaseline method that solely uses image-based pedestrian detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Muhammad Mobaidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newaz_A/0/1/0/all/0/1\">Abdullah Al Redwan Newaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimoddini_A/0/1/0/all/0/1\">Ali Karimoddini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISNet: Integrate Image-Level and Semantic-Level Context for Semantic Segmentation. (arXiv:2108.12382v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12382","description":"<p>Co-occurrent visual pattern makes aggregating contextual information a common\nparadigm to enhance the pixel representation for semantic image segmentation.\nThe existing approaches focus on modeling the context from the perspective of\nthe whole image, i.e., aggregating the image-level contextual information.\nDespite impressive, these methods weaken the significance of the pixel\nrepresentations of the same category, i.e., the semantic-level contextual\ninformation. To address this, this paper proposes to augment the pixel\nrepresentations by aggregating the image-level and semantic-level contextual\ninformation, respectively. First, an image-level context module is designed to\ncapture the contextual information for each pixel in the whole image. Second,\nwe aggregate the representations of the same category for each pixel where the\ncategory regions are learned under the supervision of the ground-truth\nsegmentation. Third, we compute the similarities between each pixel\nrepresentation and the image-level contextual information, the semantic-level\ncontextual information, respectively. At last, a pixel representation is\naugmented by weighted aggregating both the image-level contextual information\nand the semantic-level contextual information with the similarities as the\nweights. Integrating the image-level and semantic-level context allows this\npaper to report state-of-the-art accuracy on four benchmarks, i.e., ADE20K,\nLIP, COCOStuff and Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenchao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DC-GNet: Deep Mesh Relation Capturing Graph Convolution Network for 3D Human Shape Reconstruction. (arXiv:2108.12384v1 [cs.CV])","link":"http://arxiv.org/abs/2108.12384","description":"<p>In this paper, we aim to reconstruct a full 3D human shape from a single\nimage. Previous vertex-level and parameter regression approaches reconstruct 3D\nhuman shape based on a pre-defined adjacency matrix to encode positive\nrelations between nodes. The deep topological relations for the surface of the\n3D human body are not carefully exploited. Moreover, the performance of most\nexisting approaches often suffer from domain gap when handling more occlusion\ncases in real-world scenes.\n</p>\n<p>In this work, we propose a Deep Mesh Relation Capturing Graph Convolution\nNetwork, DC-GNet, with a shape completion task for 3D human shape\nreconstruction. Firstly, we propose to capture deep relations within mesh\nvertices, where an adaptive matrix encoding both positive and negative\nrelations is introduced. Secondly, we propose a shape completion task to learn\nprior about various kinds of occlusion cases. Our approach encodes mesh\nstructure from more subtle relations between nodes in a more distant region.\nFurthermore, our shape completion module alleviates the performance degradation\nissue in the outdoor scene. Extensive experiments on several benchmarks show\nthat our approach outperforms the previous 3D human pose and shape estimation\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shihao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mengxi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Shanshan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yunqi Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Hierarchical Light Field Coding Scheme Based on Hybrid Stacked Multiplicative Layers and Fourier Disparity Layers for Glasses-Free 3D Displays. (arXiv:2108.12399v1 [eess.IV])","link":"http://arxiv.org/abs/2108.12399","description":"<p>This paper presents a novel hierarchical coding scheme for light fields based\non transmittance patterns of low-rank multiplicative layers and Fourier\ndisparity layers. The proposed scheme identifies multiplicative layers of light\nfield view subsets optimized using a convolutional neural network for different\nscanning orders. Our approach exploits the hidden low-rank structure in the\nmultiplicative layers obtained from the subsets of different scanning patterns.\nThe spatial redundancies in the multiplicative layers can be efficiently\nremoved by performing low-rank approximation at different ranks on the Krylov\nsubspace. The intra-view and inter-view redundancies between approximated\nlayers are further removed by HEVC encoding. Next, a Fourier disparity layer\nrepresentation is constructed from the first subset of the approximated light\nfield based on the chosen hierarchical order. Subsequent view subsets are\nsynthesized by modeling the Fourier disparity layers that iteratively refine\nthe representation with improved accuracy. The critical advantage of the\nproposed hybrid layered representation and coding scheme is that it utilizes\nnot just spatial and temporal redundancies in light fields but efficiently\nexploits intrinsic similarities among neighboring sub-aperture images in both\nhorizontal and vertical directions as specified by different predication\norders. In addition, the scheme is flexible to realize a range of multiple\nbitrates at the decoder within a single integrated system. The compression\nperformance of the proposed scheme is analyzed on real light fields. We\nachieved substantial bitrate savings and maintained good light field\nreconstruction quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ravishankar_J/0/1/0/all/0/1\">Joshitha Ravishankar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_M/0/1/0/all/0/1\">Mansi Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynthIA: A Synthetic Inversion Approximation for the Stokes Vector Fusing SDO and Hinode into a Virtual Observatory. (arXiv:2108.12421v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2108.12421","description":"<p>Both NASA's Solar Dynamics Observatory (SDO) and the JAXA/NASA Hinode mission\ninclude spectropolarimetric instruments designed to measure the photospheric\nmagnetic field. SDO's Helioseismic and Magnetic Imager (HMI) emphasizes\nfull-disk high-cadence and good spatial resolution data acquisition while\nHinode's Solar Optical Telescope Spectro-Polarimeter (SOT-SP) focuses on high\nspatial resolution and spectral sampling at the cost of a limited field of view\nand slower temporal cadence. This work introduces a deep-learning system named\nSynthIA (Synthetic Inversion Approximation), that can enhance both missions by\ncapturing the best of each instrument's characteristics. We use SynthIA to\nproduce a new magnetogram data product, SynodeP (Synthetic Hinode Pipeline),\nthat mimics magnetograms from the higher spectral resolution Hinode/SOT-SP\npipeline, but is derived from full-disk, high-cadence, and lower\nspectral-resolution SDO/HMI Stokes observations. Results on held-out data show\nthat SynodeP has good agreement with the Hinode/SOT-SP pipeline inversions,\nincluding magnetic fill fraction, which is not provided by the current SDO/HMI\npipeline. SynodeP further shows a reduction in the magnitude of the 24-hour\noscillations present in the SDO/HMI data. To demonstrate SynthIA's generality,\nwe show the use of SDO/AIA data and subsets of the HMI data as inputs, which\nenables trade-offs between fidelity to the Hinode/SOT-SP inversions, number of\nobservations used, and temporal artifacts. We discuss possible generalizations\nof SynthIA and its implications for space weather modeling. This work is part\nof the NASA Heliophysics DRIVE Science Center (SOLSTICE) at the University of\nMichigan under grant NASA 80NSSC20K0600E, and will be open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Higgins_R/0/1/0/all/0/1\">Richard E.L. Higgins</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Fouhey_D/0/1/0/all/0/1\">David F. Fouhey</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Antiochos_S/0/1/0/all/0/1\">Spiro K. Antiochos</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Barnes_G/0/1/0/all/0/1\">Graham Barnes</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Cheung_M/0/1/0/all/0/1\">Mark C.M. Cheung</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hoeksema_J/0/1/0/all/0/1\">J. Todd Hoeksema</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Leka_K/0/1/0/all/0/1\">KD Leka</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Schuck_P/0/1/0/all/0/1\">Peter W. Schuck</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gombosi_T/0/1/0/all/0/1\">Tamas I. Gombosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competing Ratio Loss for Discriminative Multi-class Image Classification. (arXiv:1912.11642v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.11642","description":"<p>The development of deep convolutional neural network architecture is critical\nto the improvement of image classification task performance. Many image\nclassification studies use deep convolutional neural network and focus on\nmodifying the network structure to improve image classification performance.\nConversely, our study focuses on loss function design. Cross-entropy Loss (CEL)\nhas been widely used for training deep convolutional neural network for the\ntask of multi-class classification. Although CEL has been successfully\nimplemented in several image classification tasks, it only focuses on the\nposterior probability of the correct class. For this reason, a negative log\nlikelihood ratio loss (NLLR) was proposed to better differentiate between the\ncorrect class and the competing incorrect ones. However, during the training of\nthe deep convolutional neural network, the value of NLLR is not always positive\nor negative, which severely affects the convergence of NLLR. Our proposed\ncompeting ratio loss (CRL) calculates the posterior probability ratio between\nthe correct class and the competing incorrect classes to further enlarge the\nprobability difference between the correct and incorrect classes. We added\nhyperparameters to CRL, thereby ensuring its value to be positive and that the\nupdate size of backpropagation is suitable for the CRL's fast convergence. To\ndemonstrate the performance of CRL, we conducted experiments on general image\nclassification tasks (CIFAR10/100, SVHN, ImageNet), the fine-grained image\nclassification tasks (CUB200-2011 and Stanford Car), and the challenging face\nage estimation task (using Adience). Experimental results show the\neffectiveness and robustness of the proposed loss function on different deep\nconvolutional neural network architectures and different image classification\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yurong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhenbing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tony X.Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge Computing for Real-Time Near-Crash Detection for Smart Transportation Applications. (arXiv:2008.00549v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2008.00549","description":"<p>Traffic near-crash events serve as critical data sources for various smart\ntransportation applications, such as being surrogate safety measures for\ntraffic safety research and corner case data for automated vehicle testing.\nHowever, there are several key challenges for near-crash detection. First,\nextracting near-crashes from original data sources requires significant\ncomputing, communication, and storage resources. Also, existing methods lack\nefficiency and transferability, which bottlenecks prospective large-scale\napplications. To this end, this paper leverages the power of edge computing to\naddress these challenges by processing the video streams from existing dashcams\nonboard in a real-time manner. We design a multi-thread system architecture\nthat operates on edge devices and model the bounding boxes generated by object\ndetection and tracking in linear complexity. The method is insensitive to\ncamera parameters and backward compatible with different vehicles. The edge\ncomputing system has been evaluated with recorded videos and real-world tests\non two cars and four buses for over ten thousand hours. It filters out\nirrelevant videos in real-time thereby saving labor cost, processing time,\nnetwork bandwidth, and data storage. It collects not only event videos but also\nother valuable data such as road user type, event location, time to collision,\nvehicle trajectory, vehicle speed, brake switch, and throttle. The experiments\ndemonstrate the promising performance of the system regarding efficiency,\naccuracy, reliability, and transferability. It is among the first efforts in\napplying edge computing for real-time traffic video analytics and is expected\nto benefit multiple sub-fields in smart transportation research and\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_R/0/1/0/all/0/1\">Ruimin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiyong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Meixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinhai Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel-wise Knowledge Distillation for Dense Prediction. (arXiv:2011.13256v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13256","description":"<p>Knowledge distillation (KD) has been proven to be a simple and effective tool\nfor training compact models. Almost all KD variants for dense prediction tasks\nalign the student and teacher networks' feature maps in the spatial domain,\ntypically by minimizing point-wise and/or pair-wise discrepancy. Observing that\nin semantic segmentation, some layers' feature activations of each channel tend\nto encode saliency of scene categories (analogue to class activation mapping),\nwe propose to align features channel-wise between the student and teacher\nnetworks. To this end, we first transform the feature map of each channel into\na probabilty map using softmax normalization, and then minimize the\nKullback-Leibler (KL) divergence of the corresponding channels of the two\nnetworks. By doing so, our method focuses on mimicking the soft distributions\nof channels between networks. In particular, the KL divergence enables learning\nto pay more attention to the most salient regions of the channel-wise maps,\npresumably corresponding to the most useful signals for semantic segmentation.\nExperiments demonstrate that our channel-wise distillation outperforms almost\nall existing spatial distillation methods for semantic segmentation\nconsiderably, and requires less computational cost during training. We\nconsistently achieve superior performance on three benchmarks with various\nnetwork structures. Code is available at: https://git.io/Distiller\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Changyong Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting 32 Pedestrian Attributes for Autonomous Vehicles. (arXiv:2012.02647v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02647","description":"<p>Pedestrians are arguably one of the most safety-critical road users to\nconsider for autonomous vehicles in urban areas. In this paper, we address the\nproblem of jointly detecting pedestrians and recognizing 32 pedestrian\nattributes from a single image. These encompass visual appearance and behavior,\nand also include the forecasting of road crossing, which is a main safety\nconcern. For this, we introduce a Multi-Task Learning (MTL) model relying on a\ncomposite field framework, which achieves both goals in an efficient way. Each\nfield spatially locates pedestrian instances and aggregates attribute\npredictions over them. This formulation naturally leverages spatial context,\nmaking it well suited to low resolution scenarios such as autonomous driving.\nBy increasing the number of attributes jointly learned, we highlight an issue\nrelated to the scales of gradients, which arises in MTL with numerous tasks. We\nsolve it by normalizing the gradients coming from different objective functions\nwhen they join at the fork in the network architecture during the backward\npass, referred to as fork-normalization. Experimental validation is performed\non JAAD, a dataset providing numerous attributes for pedestrian analysis from\nautonomous vehicles, and shows competitive detection and attribute recognition\nresults, as well as a more stable MTL training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mordan_T/0/1/0/all/0/1\">Taylor Mordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces. (arXiv:2012.08859v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.08859","description":"<p>Current state-of-the-art Neural Architecture Search (NAS) methods neither\nefficiently scale to multiple hardware platforms, nor handle diverse\narchitectural search-spaces. To remedy this, we present DONNA (Distilling\nOptimal Neural Network Architectures), a novel pipeline for rapid, scalable and\ndiverse NAS, that scales to many user scenarios. DONNA consists of three\nphases. First, an accuracy predictor is built using blockwise knowledge\ndistillation from a reference model. This predictor enables searching across\ndiverse networks with varying macro-architectural parameters such as layer\ntypes and attention mechanisms, as well as across micro-architectural\nparameters such as block repeats and expansion rates. Second, a rapid\nevolutionary search finds a set of pareto-optimal architectures for any\nscenario using the accuracy predictor and on-device measurements. Third,\noptimal models are quickly finetuned to training-from-scratch accuracy. DONNA\nis up to 100x faster than MNasNet in finding state-of-the-art architectures\non-device. Classifying ImageNet, DONNA architectures are 20% faster than\nEfficientNet-B0 and MobileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5%\nhigher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition\nto NAS, DONNA is used for search-space extension and exploration, as well as\nhardware-aware model compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moons_B/0/1/0/all/0/1\">Bert Moons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noorzad_P/0/1/0/all/0/1\">Parham Noorzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skliar_A/0/1/0/all/0/1\">Andrii Skliar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariani_G/0/1/0/all/0/1\">Giovanni Mariani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Dushyant Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lott_C/0/1/0/all/0/1\">Chris Lott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1\">Tijmen Blankevoort</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics Disentangling for Generalized Zero-Shot Learning. (arXiv:2101.07978v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07978","description":"<p>Generalized zero-shot learning (GZSL) aims to classify samples under the\nassumption that some classes are not observable during training. To bridge the\ngap between the seen and unseen classes, most GZSL methods attempt to associate\nthe visual features of seen classes with attributes or to generate unseen\nsamples directly. Nevertheless, the visual features used in the prior\napproaches do not necessarily encode semantically related information that the\nshared attributes refer to, which degrades the model generalization to unseen\nclasses. To address this issue, in this paper, we propose a novel semantics\ndisentangling framework for the generalized zero-shot learning task (SDGZSL),\nwhere the visual features of unseen classes are firstly estimated by a\nconditional VAE and then factorized into semantic-consistent and\nsemantic-unrelated latent vectors. In particular, a total correlation penalty\nis applied to guarantee the independence between the two factorized\nrepresentations, and the semantic consistency of which is measured by the\nderived relation network. Extensive experiments conducted on four GZSL\nbenchmark datasets have evidenced that the semantic-consistent features\ndisentangled by the proposed SDGZSL are more generalizable in tasks of\ncanonical and generalized zero-shot learning. Our source code is available at\nhttps://github.com/uqzhichen/SDGZSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform Network Architectures for Deep Learning based End-to-End Image/Video Coding in Subsampled Color Spaces. (arXiv:2103.01760v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.01760","description":"<p>Most of the existing deep learning based end-to-end image/video coding (DLEC)\narchitectures are designed for non-subsampled RGB color format. However, in\norder to achieve a superior coding performance, many state-of-the-art\nblock-based compression standards such as High Efficiency Video Coding\n(HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for\nYUV 4:2:0 format, where U and V components are subsampled by considering the\nhuman visual system. This paper investigates various DLEC designs to support\nYUV 4:2:0 format by comparing their performance against the main profiles of\nHEVC and VVC standards under a common evaluation framework. Moreover, a new\ntransform network architecture is proposed to improve the efficiency of coding\nYUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the\nproposed architecture significantly outperforms naive extensions of existing\narchitectures designed for RGB format and achieves about 10% average BD-rate\nimprovement over the intra-frame coding in HEVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Egilmez_H/0/1/0/all/0/1\">Hilmi E. Egilmez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1\">Ankitesh K. Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coban_M/0/1/0/all/0/1\">Muhammed Coban</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karczewicz_M/0/1/0/all/0/1\">Marta Karczewicz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yinhao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Said_A/0/1/0/all/0/1\">Amir Said</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1\">Taco S. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation. (arXiv:2103.16219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16219","description":"<p>For unsupervised image-to-image translation, we propose a discriminator\narchitecture which focuses on the statistical features instead of individual\npatches. The network is stabilized by distribution matching of key statistical\nfeatures at multiple scales. Unlike the existing methods which impose more and\nmore constraints on the generator, our method facilitates the shape deformation\nand enhances the fine details with a greatly simplified framework. We show that\nthe proposed method outperforms the existing state-of-the-art models in various\nchallenging applications including selfie-to-anime, male-to-female and glasses\nremoval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xuning Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weidong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Accurate Emulation of the SDO/HMI Stokes Inversion with Uncertainty Quantification. (arXiv:2103.17273v2 [astro-ph.SR] UPDATED)","link":"http://arxiv.org/abs/2103.17273","description":"<p>The Helioseismic and Magnetic Imager (HMI) onboard NASA's Solar Dynamics\nObservatory (SDO) produces estimates of the photospheric magnetic field which\nare a critical input to many space weather modelling and forecasting systems.\nThe magnetogram products produced by HMI and its analysis pipeline are the\nresult of a per-pixel optimization that estimates solar atmospheric parameters\nand minimizes disagreement between a synthesized and observed Stokes vector. In\nthis paper, we introduce a deep learning-based approach that can emulate the\nexisting HMI pipeline results two orders of magnitude faster than the current\npipeline algorithms. Our system is a U-Net trained on input Stokes vectors and\ntheir accompanying optimization-based VFISV inversions. We demonstrate that our\nsystem, once trained, can produce high-fidelity estimates of the magnetic field\nand kinematic and thermodynamic parameters while also producing meaningful\nconfidence intervals. We additionally show that despite penalizing only\nper-pixel loss terms, our system is able to faithfully reproduce known\nsystematic oscillations in full-disk statistics produced by the pipeline. This\nemulation system could serve as an initialization for the full Stokes inversion\nor as an ultra-fast proxy inversion. This work is part of the NASA Heliophysics\nDRIVE Science Center (SOLSTICE) at the University of Michigan, under grant NASA\n80NSSC20K0600E, and has been open sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Higgins_R/0/1/0/all/0/1\">Richard E.L. Higgins</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Fouhey_D/0/1/0/all/0/1\">David F. Fouhey</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zhang_D/0/1/0/all/0/1\">Dichang Zhang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Antiochos_S/0/1/0/all/0/1\">Spiro K. Antiochos</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Barnes_G/0/1/0/all/0/1\">Graham Barnes</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Hoeksema_J/0/1/0/all/0/1\">J. Todd Hoeksema</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Leka_K/0/1/0/all/0/1\">K. D. Leka</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Schuck_P/0/1/0/all/0/1\">Peter W. Schuck</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Gombosi_T/0/1/0/all/0/1\">Tamas I. Gombosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semi-Supervised Learning: An Approach To Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems. (arXiv:2104.03643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03643","description":"<p>Air traffic management and specifically air-traffic control (ATC) rely mostly\non voice communications between Air Traffic Controllers (ATCos) and pilots. In\nmost cases, these voice communications follow a well-defined grammar that could\nbe leveraged in Automatic Speech Recognition (ASR) technologies. The callsign\nused to address an airplane is an essential part of all ATCo-pilot\ncommunications. We propose a two-steps approach to add contextual knowledge\nduring semi-supervised training to reduce the ASR system error rates at\nrecognizing the part of the utterance that contains the callsign. Initially, we\nrepresent in a WFST the contextual knowledge (i.e. air-surveillance data) of an\nATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the\ncontextual knowledge is added by second-pass decoding (i.e. lattice\nre-scoring). Results show that `unseen domains' (e.g. data from airports not\npresent in the supervised training data) are further aided by contextual SSL\nwhen compared to standalone SSL. For this task, we introduce the Callsign Word\nError Rate (CA-WER) as an evaluation metric, which only assesses ASR\nperformance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER\nrelative improvement applying SSL with an additional 17.5% CA-WER improvement\nby adding contextual knowledge during SSL on a challenging ATC-based test set\ngathered from LiveATC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vesely_K/0/1/0/all/0/1\">Karel Vesel&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocour_M/0/1/0/all/0/1\">Martin Kocour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szoke_I/0/1/0/all/0/1\">Igor Sz&#xf6;ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRiPOD: Human Trajectory and Pose Dynamics Forecasting in the Wild. (arXiv:2104.04029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04029","description":"<p>Joint forecasting of human trajectory and pose dynamics is a fundamental\nbuilding block of various applications ranging from robotics and autonomous\ndriving to surveillance systems. Predicting body dynamics requires capturing\nsubtle information embedded in the humans' interactions with each other and\nwith the objects present in the scene. In this paper, we propose a novel\nTRajectory and POse Dynamics (nicknamed TRiPOD) method based on graph\nattentional networks to model the human-human and human-object interactions\nboth in the input space and the output space (decoded future output). The model\nis supplemented by a message passing interface over the graphs to fuse these\ndifferent levels of interactions efficiently. Furthermore, to incorporate a\nreal-world challenge, we propound to learn an indicator representing whether an\nestimated body joint is visible/invisible at each frame, e.g. due to occlusion\nor being outside the sensor field of view. Finally, we introduce a new\nbenchmark for this joint task based on two challenging datasets (PoseTrack and\n3DPW) and propose evaluation metrics to measure the effectiveness of\npredictions in the global space, even when there are invisible cases of joints.\nOur evaluation shows that TRiPOD outperforms all prior work and\nstate-of-the-art specifically designed for each of the trajectory and pose\nforecasting tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adeli_V/0/1/0/all/0/1\">Vida Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsanpour_M/0/1/0/all/0/1\">Mahsa Ehsanpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1\">Juan Carlos Niebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SA-ConvONet: Sign-Agnostic Optimization of Convolutional Occupancy Networks. (arXiv:2105.03582v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03582","description":"<p>Surface reconstruction from point clouds is a fundamental problem in the\ncomputer vision and graphics community. Recent state-of-the-arts solve this\nproblem by individually optimizing each local implicit field during inference.\nWithout considering the geometric relationships between local fields, they\ntypically require accurate normals to avoid the sign conflict problem in\noverlapped regions of local fields, which severely limits their applicability\nto raw scans where surface normals could be unavailable. Although SAL breaks\nthis limitation via sign-agnostic learning, further works still need to explore\nhow to extend this technique for local shape modeling. To this end, we propose\nto learn implicit surface reconstruction by sign-agnostic optimization of\nconvolutional occupancy networks, to simultaneously achieve advanced\nscalability to large-scale scenes, generality to novel shapes, and\napplicability to raw scans in a unified framework. Concretely, we achieve this\ngoal by a simple yet effective design, which further optimizes the pre-trained\noccupancy prediction networks with an unsigned cross-entropy loss during\ninference. The learning of occupancy fields is conditioned on convolutional\nfeatures from an hourglass network architecture. Extensive experimental\ncomparisons with previous state-of-the-arts on both object-level and\nscene-level datasets demonstrate the superior accuracy of our approach for\nsurface reconstruction from un-orientated point clouds. The code is available\nat https://github.com/tangjiapeng/SA-ConvONet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiapeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jiabao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Feiying Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Explainable, Privacy-Preserved Human-Motion Affect Recognition. (arXiv:2105.03958v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03958","description":"<p>Human motion characteristics are used to monitor the progression of\nneurological diseases and mood disorders. Since perceptions of emotions are\nalso interleaved with body posture and movements, emotion recognition from\nhuman gait can be used to quantitatively monitor mood changes. Many existing\nsolutions often use shallow machine learning models with raw positional data or\nmanually extracted features to achieve this. However, gait is composed of many\nhighly expressive characteristics that can be used to identify human subjects,\nand most solutions fail to address this, disregarding the subject's privacy.\nThis work introduces a novel deep neural network architecture to disentangle\nhuman emotions and biometrics. In particular, we propose a cross-subject\ntransfer learning technique for training a multi-encoder autoencoder deep\nneural network to learn disentangled latent representations of human motion\nfeatures. By disentangling subject biometrics from the gait data, we show that\nthe subject's privacy is preserved while the affect recognition performance\noutperforms traditional methods. Furthermore, we exploit Guided Grad-CAM to\nprovide global explanations of the model's decision across gait cycles. We\nevaluate the effectiveness of our method to existing methods at recognizing\nemotions using both 3D temporal joint signals and manually extracted features.\nWe also show that this data can easily be exploited to expose a subject's\nidentity. Our method shows up to 7% improvement and highlights the joints with\nthe most significant influence across the average gait cycle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malek_Podjaski_M/0/1/0/all/0/1\">Matthew Malek-Podjaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligianni_F/0/1/0/all/0/1\">Fani Deligianni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenter: Transformer for Semantic Segmentation. (arXiv:2105.05633v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.05633","description":"<p>Image segmentation is often ambiguous at the level of individual image\npatches and requires contextual information to reach label consensus. In this\npaper we introduce Segmenter, a transformer model for semantic segmentation. In\ncontrast to convolution-based methods, our approach allows to model global\ncontext already at the first layer and throughout the network. We build on the\nrecent Vision Transformer (ViT) and extend it to semantic segmentation. To do\nso, we rely on the output embeddings corresponding to image patches and obtain\nclass labels from these embeddings with a point-wise linear decoder or a mask\ntransformer decoder. We leverage models pre-trained for image classification\nand show that we can fine-tune them on moderate sized datasets available for\nsemantic segmentation. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a mask transformer\ngenerating class masks. We conduct an extensive ablation study to show the\nimpact of the different parameters, in particular the performance is better for\nlarge models and small patch sizes. Segmenter attains excellent results for\nsemantic segmentation. It outperforms the state of the art on both ADE20K and\nPascal Context datasets and is competitive on Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_R/0/1/0/all/0/1\">Ricardo Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Emotional Dependencies with Graph Convolutional Networks for Facial Expression Recognition. (arXiv:2106.03487v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03487","description":"<p>Over the past few years, deep learning methods have shown remarkable results\nin many face-related tasks including automatic facial expression recognition\n(FER) in-the-wild. Meanwhile, numerous models describing the human emotional\nstates have been proposed by the psychology community. However, we have no\nclear evidence as to which representation is more appropriate and the majority\nof FER systems use either the categorical or the dimensional model of affect.\nInspired by recent work in multi-label classification, this paper proposes a\nnovel multi-task learning (MTL) framework that exploits the dependencies\nbetween these two models using a Graph Convolutional Network (GCN) to recognize\nfacial expressions in-the-wild. Specifically, a shared feature representation\nis learned for both discrete and continuous recognition in a MTL setting.\nMoreover, the facial expression classifiers and the valence-arousal regressors\nare learned through a GCN that explicitly captures the dependencies between\nthem. To evaluate the performance of our method under real-world conditions we\nperform extensive experiments on the AffectNet and Aff-Wild2 datasets. The\nresults of our experiments show that our method is capable of improving the\nperformance across different datasets and backbone architectures. Finally, we\nalso surpass the previous state-of-the-art methods on the categorical model of\nAffectNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antoniadis_P/0/1/0/all/0/1\">Panagiotis Antoniadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filntisis_P/0/1/0/all/0/1\">Panagiotis P. Filntisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Matching via Deep Metric Learning for Generative Modeling. (arXiv:2106.10777v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10777","description":"<p>We propose a manifold matching approach to generative models which includes a\ndistribution generator (or data generator) and a metric generator. In our\nframework, we view the real data set as some manifold embedded in a\nhigh-dimensional Euclidean space. The distribution generator aims at generating\nsamples that follow some distribution condensed around the real data manifold.\nIt is achieved by matching two sets of points using their geometric shape\ndescriptors, such as centroid and $p$-diameter, with learned distance metric;\nthe metric generator utilizes both real data and generated samples to learn a\ndistance metric which is close to some intrinsic geodesic distance on the real\ndata manifold. The produced distance metric is further used for manifold\nmatching. The two networks are learned simultaneously during the training\nprocess. We apply the approach on both unsupervised and supervised learning\ntasks: in unconditional image generation task, the proposed method obtains\ncompetitive results compared with existing generative models; in\nsuper-resolution task, we incorporate the framework in perception-based models\nand improve visual qualities by producing samples with more natural textures.\nExperiments and analysis demonstrate the feasibility and effectiveness of the\nproposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1\">Mengyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_H/0/1/0/all/0/1\">Haibin Hang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Geometric Distillation Network for Compressive Sensing MRI. (arXiv:2107.04943v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.04943","description":"<p>Compressed sensing (CS) is an efficient method to reconstruct MR image from\nsmall sampled data in $k$-space and accelerate the acquisition of MRI. In this\nwork, we propose a novel deep geometric distillation network which combines the\nmerits of model-based and deep learning-based CS-MRI methods, it can be\ntheoretically guaranteed to improve geometric texture details of a linear\nreconstruction. Firstly, we unfold the model-based CS-MRI optimization problem\ninto two sub-problems that consist of image linear approximation and image\ngeometric compensation. Secondly, geometric compensation sub-problem for\ndistilling lost texture details in approximation stage can be expanded by\nTaylor expansion to design a geometric distillation module fusing features of\ndifferent geometric characteristic domains. Additionally, we use a learnable\nversion with adaptive initialization of the step-length parameter, which allows\nmodel more flexibility that can lead to convergent smoothly. Numerical\nexperiments verify its superiority over other state-of-the-art CS-MRI\nreconstruction approaches. The source code will be available at\n\\url{https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohong Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Constrained Data Representation Learning for Human Motion Segmentation. (arXiv:2107.13362v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13362","description":"<p>Recently, transfer subspace learning based approaches have shown to be a\nvalid alternative to unsupervised subspace clustering and temporal data\nclustering for human motion segmentation (HMS). These approaches leverage prior\nknowledge from a source domain to improve clustering performance on a target\ndomain, and currently they represent the state of the art in HMS. Bucking this\ntrend, in this paper, we propose a novel unsupervised model that learns a\nrepresentation of the data and digs clustering information from the data\nitself. Our model is reminiscent of temporal subspace clustering, but presents\ntwo critical differences. First, we learn an auxiliary data matrix that can\ndeviate from the initial data, hence confer more degrees of freedom to the\ncoding matrix. Second, we introduce a regularization term for this auxiliary\ndata matrix that preserves the local geometrical structure present in the\nhigh-dimensional space. The proposed model is efficiently optimized by using an\noriginal Alternating Direction Method of Multipliers (ADMM) formulation\nallowing to learn jointly the auxiliary data representation, a nonnegative\ndictionary and a coding matrix. Experimental results on four benchmark datasets\nfor HMS demonstrate that our approach achieves significantly better clustering\nperformance then state-of-the-art methods, including both unsupervised and more\nrecent semi-supervised transfer learning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrido_L/0/1/0/all/0/1\">Llu&#xed;s Garrido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Corominas_G/0/1/0/all/0/1\">Guillem Rodriguez-Corominas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1\">Herwig Wendt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Matching Networks for Semantic Correspondence. (arXiv:2108.00211v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00211","description":"<p>Deep features have been proven powerful in building accurate dense semantic\ncorrespondences in various previous works. However, the multi-scale and\npyramidal hierarchy of convolutional neural networks has not been well studied\nto learn discriminative pixel-level features for semantic correspondence. In\nthis paper, we propose a multi-scale matching network that is sensitive to tiny\nsemantic differences between neighboring pixels. We follow the coarse-to-fine\nmatching strategy and build a top-down feature and matching enhancement scheme\nthat is coupled with the multi-scale hierarchy of deep convolutional neural\nnetworks. During feature enhancement, intra-scale enhancement fuses\nsame-resolution feature maps from multiple layers together via local\nself-attention and cross-scale enhancement hallucinates higher-resolution\nfeature maps along the top-down hierarchy. Besides, we learn complementary\nmatching details at different scales thus the overall matching score is refined\nby features of different semantic levels gradually. Our multi-scale matching\nnetwork can be trained end-to-end easily with few additional learnable\nparameters. Experimental results demonstrate that the proposed method achieves\nstate-of-the-art performance on three popular benchmarks with high\ncomputational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Ziyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhenghao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Weifeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to generate shape from global-local spectra. (arXiv:2108.02161v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02161","description":"<p>In this work, we present a new learning-based pipeline for the generation of\n3D shapes. We build our method on top of recent advances on the so called\nshape-from-spectrum paradigm, which aims at recovering the full 3D geometric\nstructure of an object only from the eigenvalues of its Laplacian operator. In\ndesigning our learning strategy, we consider the spectrum as a natural and\nready to use representation to encode variability of the shapes. Therefore, we\npropose a simple decoder-only architecture that directly maps spectra to 3D\nembeddings; in particular, we combine information from global and local\nspectra, the latter being obtained from localized variants of the manifold\nLaplacian. This combination captures the relations between the full shape and\nits local parts, leading to more accurate generation of geometric details and\nan improved semantic control in shape synthesis and novel editing applications.\nOur results confirm the improvement of the proposed approach in comparison to\nexisting and alternative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1\">Marco Pegoraro</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Castellani_U/0/1/0/all/0/1\">Umberto Castellani</a> (1) ((1) University of Verona, (2) Sapienza University of Rome)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable Image Registration using Neural ODEs. (arXiv:2108.03443v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03443","description":"<p>Deformable image registration, aiming to find spatial correspondence between\na given image pair, is one of the most critical problems in the domain of\nmedical image analysis. In this paper, we present a generic, fast, and accurate\ndiffeomorphic image registration framework that leverages neural ordinary\ndifferential equations (NODEs). We model each voxel as a moving particle and\nconsider the set of all voxels in a 3D image as a high-dimensional dynamical\nsystem whose trajectory determines the targeted deformation field. Compared\nwith traditional optimization-based methods, our framework reduces the running\ntime from tens of minutes to tens of seconds. Compared with recent data-driven\ndeep learning methods, our framework is more accessible since it does not\nrequire large amounts of training data. Our experiments show that the\nregistration results of our method outperform state-of-the-arts under various\nmetrics, indicating that our modeling approach is well fitted for the task of\ndeformable image registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z.Jiahao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiancong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A.Yushkevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">James C.Gee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">M.Ani Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.04409","description":"<p>Deep Neural Networks (DNNs) are vulnerable to adversarial examples which\nwould inveigle neural networks to make prediction errors with small\nperturbations on the input images. Researchers have been devoted to promoting\nthe research on the universal adversarial perturbations (UAPs) which are\ngradient-free and have little prior knowledge on data distributions. Procedural\nadversarial noise attack is a data-free universal perturbation generation\nmethod. In this paper, we propose two universal adversarial perturbation (UAP)\ngeneration methods based on procedural noise functions: Simplex noise and\nWorley noise. In our framework, the shading which disturbs visual\nclassification is generated with rendering technology. Without changing the\nsemantic representations, the adversarial examples generated via our methods\nshow superior performance on the attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DexMV: Imitation Learning for Dexterous Manipulation from Human Videos. (arXiv:2108.05877v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.05877","description":"<p>While we have made significant progress on understanding hand-object\ninteractions in computer vision, it is still very challenging for robots to\nperform complex dexterous manipulation. In this paper, we propose a new\nplatform and pipeline, DexMV (Dexterous Manipulation from Videos), for\nimitation learning to bridge the gap between computer vision and robot\nlearning. We design a platform with: (i) a simulation system for complex\ndexterous manipulation tasks with a multi-finger robot hand and (ii) a computer\nvision system to record large-scale demonstrations of a human hand conducting\nthe same tasks. In our new pipeline, we extract 3D hand and object poses from\nthe videos, and convert them to robot demonstrations via motion retargeting. We\nthen apply and compare multiple imitation learning algorithms with the\ndemonstrations. We show that the demonstrations can indeed improve robot\nlearning by a large margin and solve the complex tasks which reinforcement\nlearning alone cannot solve. Project page with video:\nhttps://yzqin.github.io/dexmv\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueh-Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaowei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hanwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconcile Prediction Consistency for Balanced Object Detection. (arXiv:2108.10809v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10809","description":"<p>Classification and regression are two pillars of object detectors. In most\nCNN-based detectors, these two pillars are optimized independently. Without\ndirect interactions between them, the classification loss and the regression\nloss can not be optimized synchronously toward the optimal direction in the\ntraining phase. This clearly leads to lots of inconsistent predictions with\nhigh classification score but low localization accuracy or low classification\nscore but high localization accuracy in the inference phase, especially for the\nobjects of irregular shape and occlusion, which severely hurts the detection\nperformance of existing detectors after NMS. To reconcile prediction\nconsistency for balanced object detection, we propose a Harmonic loss to\nharmonize the optimization of classification branch and localization branch.\nThe Harmonic loss enables these two branches to supervise and promote each\nother during training, thereby producing consistent predictions with high\nco-occurrence of top classification and localization in the inference phase.\nFurthermore, in order to prevent the localization loss from being dominated by\noutliers during training phase, a Harmonic IoU loss is proposed to harmonize\nthe weight of the localization loss of different IoU-level samples.\nComprehensive experiments on benchmarks PASCAL VOC and MS COCO demonstrate the\ngenerality and effectiveness of our model for facilitating existing object\ndetectors to state-of-the-art accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11250","description":"<p>A panoptic driving perception system is an essential part of autonomous\ndriving. A high-precision and real-time perception system can assist the\nvehicle in making the reasonable decision while driving. We present a panoptic\ndriving perception network (YOLOP) to perform traffic object detection,\ndrivable area segmentation and lane detection simultaneously. It is composed of\none encoder for feature extraction and three decoders to handle the specific\ntasks. Our model performs extremely well on the challenging BDD100K dataset,\nachieving state-of-the-art on all three tasks in terms of accuracy and speed.\nBesides, we verify the effectiveness of our multi-task learning model for joint\ntraining via ablative studies. To our best knowledge, this is the first work\nthat can process these three visual perception tasks simultaneously in\nreal-time on an embedded device Jetson TX2(23 FPS) and maintain excellent\naccuracy. To facilitate further research, the source codes and pre-trained\nmodels will be released at https://github.com/hustvl/YOLOP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Manwen Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weitian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifted Chunk Transformer for Spatio-Temporal Representational Learning. (arXiv:2108.11575v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11575","description":"<p>Spatio-temporal representational learning has been widely adopted in various\nfields such as action recognition, video object segmentation, and action\nanticipation. Previous spatio-temporal representational learning approaches\nprimarily employ ConvNets or sequential models,e.g., LSTM, to learn the\nintra-frame and inter-frame features. Recently, Transformer models have\nsuccessfully dominated the study of natural language processing (NLP), image\nclassification, etc. However, the pure-Transformer based spatio-temporal\nlearning can be prohibitively costly on memory and computation to extract\nfine-grained features from a tiny patch. To tackle the training difficulty and\nenhance the spatio-temporal learning, we construct a shifted chunk Transformer\nwith pure self-attention blocks. Leveraging the recent efficient Transformer\ndesign in NLP, this shifted chunk Transformer can learn hierarchical\nspatio-temporal features from a local tiny patch to a global video clip. Our\nshifted self-attention can also effectively model complicated inter-frame\nvariances. Furthermore, we build a clip encoder based on Transformer to model\nlong-term temporal dependencies. We conduct thorough ablation studies to\nvalidate each component and hyper-parameters in our shifted chunk Transformer,\nand it outperforms previous state-of-the-art approaches on Kinetics-400,\nKinetics-600, UCF101, and HMDB51. Code and trained models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_X/0/1/0/all/0/1\">Xuefan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tingxun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Diversify for Single Domain Generalization. (arXiv:2108.11726v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11726","description":"<p>Domain generalization (DG) aims to generalize a model trained on multiple\nsource (i.e., training) domains to a distributionally different target (i.e.,\ntest) domain. In contrast to the conventional DG that strictly requires the\navailability of multiple source domains, this paper considers a more realistic\nyet challenging scenario, namely Single Domain Generalization (Single-DG),\nwhere only one source domain is available for training. In this scenario, the\nlimited diversity may jeopardize the model generalization on unseen target\ndomains. To tackle this problem, we propose a style-complement module to\nenhance the generalization power of the model by synthesizing images from\ndiverse distributions that are complementary to the source ones. More\nspecifically, we adopt a tractable upper bound of mutual information (MI)\nbetween the generated and source samples and perform a two-step optimization\niteratively: (1) by minimizing the MI upper bound approximation for each sample\npair, the generated images are forced to be diversified from the source\nsamples; (2) subsequently, we maximize the MI between the samples from the same\nsemantic category, which assists the network to learn discriminative features\nfrom diverse-styled images. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of our approach, which surpasses the\nstate-of-the-art single-DG methods by up to 25.14%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}