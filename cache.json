{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Solving Quantitative Reasoning Problems with Language Models. (arXiv:2206.14858v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14858","description":"<p>Language models have achieved remarkable performance on a wide range of tasks\nthat require natural language understanding. Nevertheless, state-of-the-art\nmodels have generally struggled with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems at the college\nlevel. To help close this gap, we introduce Minerva, a large language model\npretrained on general natural language data and further trained on technical\ncontent. The model achieves state-of-the-art performance on technical\nbenchmarks without the use of external tools. We also evaluate our model on\nover two hundred undergraduate-level problems in physics, biology, chemistry,\neconomics, and other sciences that require quantitative reasoning, and find\nthat the model can correctly answer nearly a third of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Ethan Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasesh_V/0/1/0/all/0/1\">Vinay Ramasesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slone_A/0/1/0/all/0/1\">Ambrose Slone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1\">Cem Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1\">Imanol Schlag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutman_Solo_T/0/1/0/all/0/1\">Theo Gutman-Solo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_Ari_G/0/1/0/all/0/1\">Guy Gur-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Space-Efficient Representation of Entity-centric Query Language Models. (arXiv:2206.14885v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14885","description":"<p>Virtual assistants make use of automatic speech recognition (ASR) to help\nusers answer entity-centric queries. However, spoken entity recognition is a\ndifficult problem, due to the large number of frequently-changing named\nentities. In addition, resources available for recognition are constrained when\nASR is performed on-device.\n</p>\n<p>In this work, we investigate the use of probabilistic grammars as language\nmodels within the finite-state transducer (FST) framework. We introduce a\ndeterministic approximation to probabilistic grammars that avoids the explicit\nexpansion of non-terminals at model creation time, integrates directly with the\nFST framework, and is complementary to n-gram models.\n</p>\n<p>We obtain a 10% relative word error rate improvement on long tail entity\nqueries compared to when a similarly-sized n-gram model is used without our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gysel_C/0/1/0/all/0/1\">Christophe Van Gysel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannemann_M/0/1/0/all/0/1\">Mirko Hannemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pusateri_E/0/1/0/all/0/1\">Ernest Pusateri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oualil_Y/0/1/0/all/0/1\">Youssef Oualil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oparin_I/0/1/0/all/0/1\">Ilya Oparin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPTs at Factify 2022: Prompt Aided Fact-Verification. (arXiv:2206.14913v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14913","description":"<p>One of the most pressing societal issues is the fight against false news. The\nfalse claims, as difficult as they are to expose, create a lot of damage. To\ntackle the problem, fact verification becomes crucial and thus has been a topic\nof interest among diverse research communities. Using only the textual form of\ndata we propose our solution to the problem and achieve competitive results\nwith other approaches. We present our solution based on two approaches - PLM\n(pre-trained language model) based method and Prompt based method. The\nPLM-based approach uses the traditional supervised learning, where the model is\ntrained to take 'x' as input and output prediction 'y' as P(y|x). Whereas,\nPrompt-based learning reflects the idea to design input to fit the model such\nthat the original objective may be re-framed as a problem of (masked) language\nmodeling. We may further stimulate the rich knowledge provided by PLMs to\nbetter serve downstream tasks by employing extra prompts to fine-tune PLMs. Our\nexperiments showed that the proposed method performs better than just\nfine-tuning PLMs. We achieved an F1 score of 0.6946 on the FACTIFY dataset and\na 7th position on the competition leader-board.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1\">Pawan Kumar Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_S/0/1/0/all/0/1\">Saksham Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Taneesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_G/0/1/0/all/0/1\">Gyanendra Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Part-Of-Speech Model: Does Modeling Long Context Help Unsupervised POS-tagging?. (arXiv:2206.14969v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14969","description":"<p>Previous Part-Of-Speech (POS) induction models usually assume certain\nindependence assumptions (e.g., Markov, unidirectional, local dependency) that\ndo not hold in real languages. For example, the subject-verb agreement can be\nboth long-term and bidirectional. To facilitate flexible dependency modeling,\nwe propose a Masked Part-of-Speech Model (MPoSM), inspired by the recent\nsuccess of Masked Language Models (MLM). MPoSM can model arbitrary tag\ndependency and perform POS induction through the objective of masked POS\nreconstruction. We achieve competitive results on both the English Penn WSJ\ndataset as well as the universal treebank containing 10 diverse languages.\nThough modeling the long-term dependency should ideally help this task, our\nablation study shows mixed trends in different languages. To better understand\nthis phenomenon, we design a novel synthetic experiment that can specifically\ndiagnose the model's ability to learn tag agreement. Surprisingly, we find that\neven strong baselines fail to solve this problem consistently in a very\nsimplified setting: the agreement between adjacent words. Nonetheless, MPoSM\nachieves overall better performance. Lastly, we conduct a detailed error\nanalysis to shed light on other remaining challenges. Our code is available at\nhttps://github.com/owenzx/MPoSM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Multilingual Machine Translation Systems That Serve Arbitrary X-Y Translations. (arXiv:2206.14982v1 [cs.CL])","link":"http://arxiv.org/abs/2206.14982","description":"<p>Multilingual Neural Machine Translation (MNMT) enables one system to\ntranslate sentences from multiple source languages to multiple target\nlanguages, greatly reducing deployment costs compared with conventional\nbilingual systems. The MNMT training benefit, however, is often limited to\nmany-to-one directions. The model suffers from poor performance in one-to-many\nand many-to-many with zero-shot setup. To address this issue, this paper\ndiscusses how to practically build MNMT systems that serve arbitrary X-Y\ntranslation directions while leveraging multilinguality with a two-stage\ntraining strategy of pretraining and finetuning. Experimenting with the WMT'21\nmultilingual translation task, we demonstrate that our systems outperform the\nconventional baselines of direct bilingual models and pivot translation models\nfor most directions, averagely giving +6.0 and +4.1 BLEU, without the need for\narchitecture change or extra data collection. Moreover, we also examine our\nproposed approach in an extremely large-scale data setting to accommodate\npractical deployment scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eriguchi_A/0/1/0/all/0/1\">Akiko Eriguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shufang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language. (arXiv:2206.15007v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15007","description":"<p>Helping end users comprehend the abstract distribution shifts can greatly\nfacilitate AI deployment. Motivated by this, we propose a novel task, dataset\nexplanation. Given two image data sets, dataset explanation aims to\nautomatically point out their dataset-level distribution shifts with natural\nlanguage. Current techniques for monitoring distribution shifts provide\ninadequate information to understand datasets with the goal of improving data\nquality. Therefore, we introduce GSCLIP, a training-free framework to solve the\ndataset explanation task. In GSCLIP, we propose the selector as the first\nquantitative evaluation method to identify explanations that are proper to\nsummarize dataset shifts. Furthermore, we leverage this selector to demonstrate\nthe superiority of a generator based on language model generation. Systematic\nevaluation on natural data shift verifies that GSCLIP, a combined system of a\nhybrid generator group and an efficient selector is not only easy-to-use but\nalso powerful for dataset explanation at scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Diversity and Uncertainty in Moderation\" are the Key to Data Selection for Multilingual Few-shot Transfer. (arXiv:2206.15010v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15010","description":"<p>Few-shot transfer often shows substantial gain over zero-shot\ntransfer~\\cite{lauscher2020zero}, which is a practically useful trade-off\nbetween fully supervised and unsupervised learning approaches for multilingual\npretrained model-based systems. This paper explores various strategies for\nselecting data for annotation that can result in a better few-shot transfer.\nThe proposed approaches rely on multiple measures such as data entropy using\n$n$-gram language model, predictive entropy, and gradient embedding. We propose\na loss embedding method for sequence labeling tasks, which induces diversity\nand uncertainty sampling similar to gradient embedding. The proposed data\nselection strategies are evaluated and compared for POS tagging, NER, and NLI\ntasks for up to 20 languages. Our experiments show that the gradient and loss\nembedding-based strategies consistently outperform random data selection\nbaselines, with gains varying with the initial performance of the zero-shot\ntransfer. Furthermore, the proposed method shows similar trends in improvement\neven when the model is fine-tuned using a lower proportion of the original\ntask-specific labeled training data for zero-shot transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shanu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding. (arXiv:2206.15014v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15014","description":"<p>In recent years, large pre-trained Transformer networks have demonstrated\ndramatic improvements in many natural language understanding tasks. However,\nthe huge size of these models brings significant challenges to their\nfine-tuning and online deployment due to latency and cost constraints. New\nhardware supporting both N:M semi-structured sparsity and low-precision integer\ncomputation is a promising solution to boost DNN model serving efficiency.\nHowever, there have been very few studies that systematically investigate to\nwhat extent pre-trained Transformer networks benefit from the combination of\nthese techniques, as well as how to best compress each component of the\nTransformer. We propose a flexible compression framework NxMiFormer that\nperforms simultaneous sparsification and quantization using ADMM and STE-based\nQAT. Furthermore, we present and inexpensive, heuristic-driven search algorithm\nthat identifies promising heterogeneous compression configurations that meet a\ncompression ratio constraint. When evaluated across the GLUE suite of NLU\nbenchmarks, our approach can achieve up to 93% compression of the encoders of a\nBERT model while retaining 98.2% of the original model accuracy and taking full\nadvantage of the hardware's capabilities. Heterogeneous configurations found\nthe by the search heuristic maintain 99.5% of the baseline accuracy while still\ncompressing the model by 87.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1\">Connor Holmes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modern Question Answering Datasets and Benchmarks: A Survey. (arXiv:2206.15030v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15030","description":"<p>Question Answering (QA) is one of the most important natural language\nprocessing (NLP) tasks. It aims using NLP technologies to generate a\ncorresponding answer to a given question based on the massive unstructured\ncorpus. With the development of deep learning, more and more challenging QA\ndatasets are being proposed, and lots of new methods for solving them are also\nemerging. In this paper, we investigate influential QA datasets that have been\nreleased in the era of deep learning. Specifically, we begin with introducing\ntwo of the most common QA tasks - textual question answer and visual question\nanswering - separately, covering the most representative datasets, and then\ngive some current challenges of QA research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Story-thinking, computational-thinking, programming and software engineering. (arXiv:2206.15066v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15066","description":"<p>Working with stories and working with computations require very different\nmodes of thought. We call the first mode \"story-thinking\" and the second\n\"computational-thinking\". The aim of this curiosity-driven paper is to explore\nthe nature of these two modes of thinking, and to do so in relation to\nprogramming, including software engineering as programming-in-the-large. We\nsuggest that story-thinking and computational-thinking may be understood as two\nways of attending to the world, and that each both contributes and neglects the\nworld, though in different ways and for different ends. We formulate two\nfundamental problems, i.e., the problem of \"neglectful representations\" and the\nproblem of oppositional ways of thinking. We briefly suggest two ways in which\nthese problems might be tackled and identify candidate hypotheses about the\ncurrent state of the world, one assertion about a possible future state, and\nseveral research questions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rainer_A/0/1/0/all/0/1\">Austen Rainer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_C/0/1/0/all/0/1\">Catherine Menon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing. (arXiv:2206.15076v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15076","description":"<p>Training and evaluating language models increasingly requires the\nconstruction of meta-datasets --diverse collections of curated data with clear\nprovenance. Natural language prompting has recently lead to improved zero-shot\ngeneralization by transforming existing, supervised datasets into a diversity\nof novel pretraining tasks, highlighting the benefits of meta-dataset curation.\nWhile successful in general-domain text, translating these data-centric\napproaches to biomedical language modeling remains challenging, as labeled\nbiomedical datasets are significantly underrepresented in popular data hubs. To\naddress this challenge, we introduce BigBIO a community library of 126+\nbiomedical NLP datasets, currently covering 12 task categories and 10+\nlanguages. BigBIO facilitates reproducible meta-dataset curation via\nprogrammatic access to datasets and their metadata, and is compatible with\ncurrent platforms for prompt engineering and end-to-end few/zero shot language\nmodel evaluation. We discuss our process for task schema harmonization, data\nauditing, contribution guidelines, and outline two illustrative use cases:\nzero-shot evaluation of biomedical prompts and large-scale, multi-task\nlearning. BigBIO is an ongoing community effort and is available at\nhttps://github.com/bigscience-workshop/biomedical\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1\">Jason Alan Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_L/0/1/0/all/0/1\">Leon Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seelam_N/0/1/0/all/0/1\">Natasha Seelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altay_G/0/1/0/all/0/1\">Gabriel Altay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_D/0/1/0/all/0/1\">Debajyoti Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garda_S/0/1/0/all/0/1\">Samuele Garda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myungsun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruisi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusa_W/0/1/0/all/0/1\">Wojciech Kusa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_F/0/1/0/all/0/1\">Fabio Barth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanger_M/0/1/0/all/0/1\">Mario S&#xe4;nger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callahan_A/0/1/0/all/0/1\">Alison Callahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perinan_D/0/1/0/all/0/1\">Daniel Le&#xf3;n Peri&#xf1;&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gigant_T/0/1/0/all/0/1\">Th&#xe9;o Gigant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_P/0/1/0/all/0/1\">Patrick Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chim_J/0/1/0/all/0/1\">Jenny Chim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posada_J/0/1/0/all/0/1\">Jose David Posada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_J/0/1/0/all/0/1\">John Michael Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivaraman_K/0/1/0/all/0/1\">Karthik Rangasai Sivaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nezhurina_M/0/1/0/all/0/1\">Marianna Nezhurina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1\">Robert Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cullan_M/0/1/0/all/0/1\">Michael Cullan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freidank_M/0/1/0/all/0/1\">Moritz Freidank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahlberg_N/0/1/0/all/0/1\">Nathan Dahlberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bose_S/0/1/0/all/0/1\">Shamik Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broad_N/0/1/0/all/0/1\">Nicholas Michio Broad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labrak_Y/0/1/0/all/0/1\">Yanis Labrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1\">Shlok S Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiblawi_S/0/1/0/all/0/1\">Sid Kiblawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ayush Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1\">Minh Chien Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeraj_T/0/1/0/all/0/1\">Trishala Neeraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golde_J/0/1/0/all/0/1\">Jonas Golde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beilharz_B/0/1/0/all/0/1\">Benjamin Beilharz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"esCorpius: A Massive Spanish Crawling Corpus. (arXiv:2206.15147v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15147","description":"<p>In the recent years, transformer-based models have lead to significant\nadvances in language modelling for natural language processing. However, they\nrequire a vast amount of data to be (pre-)trained and there is a lack of\ncorpora in languages other than English. Recently, several initiatives have\npresented multilingual datasets obtained from automatic web crawling. However,\nthe results in Spanish present important shortcomings, as they are either too\nsmall in comparison with other languages, or present a low quality derived from\nsub-optimal cleaning and deduplication. In this paper, we introduce\n\\textsc{esCorpius}, a Spanish crawling corpus obtained from near 1 Pb of Common\nCrawl data. It is the most extensive corpus in Spanish with this level of\nquality in the extraction, purification and deduplication of web textual\ncontent. Our data curation process involves a novel highly parallel cleaning\npipeline and encompasses a series of deduplication mechanisms that together\nensure the integrity of both document and paragraph boundaries. Additionally,\nwe maintain both the source web page URL and the WARC shard origin URL in order\nto complain with EU regulations. \\textsc{esCorpius} has been released under CC\nBY-NC-ND 4.0 license and is available on HuggingFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Fernandez_D/0/1/0/all/0/1\">David P&#xe9;rez-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griol_D/0/1/0/all/0/1\">David Griol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callejas_Z/0/1/0/all/0/1\">Zoraida Callejas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Entity Candidate Generation for Low-Resource Languages. (arXiv:2206.15163v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15163","description":"<p>Candidate generation is a crucial module in entity linking. It also plays a\nkey role in multiple NLP tasks that have been proven to beneficially leverage\nknowledge bases. Nevertheless, it has often been overlooked in the monolingual\nEnglish entity linking literature, as naive approaches obtain very good\nperformance. Unfortunately, the existing approaches for English cannot be\nsuccessfully transferred to poorly resourced languages. This paper constitutes\nan in-depth analysis of the candidate generation problem in the context of\ncross-lingual entity linking with a focus on low-resource languages. Among\nother contributions, we point out limitations in the evaluation conducted in\nprevious works. We introduce a characterization of queries into types based on\ntheir difficulty, which improves the interpretability of the performance of\ndifferent methods. We also propose a light-weight and simple solution based on\nthe construction of indexes whose design is motivated by more complex transfer\nlearning based neural approaches. A thorough empirical analysis on 9 real-world\ndatasets under 2 evaluation settings shows that our simple solution outperforms\nthe state-of-the-art approach in terms of both quality and efficiency for\nalmost all datasets and query types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1\">Alberto Garc&#xed;a-Dur&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akhil Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Topological BERT: Transforming Attention into Topology for Natural Language Processing. (arXiv:2206.15195v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15195","description":"<p>In recent years, the introduction of the Transformer models sparked a\nrevolution in natural language processing (NLP). BERT was one of the first text\nencoders using only the attention mechanism without any recurrent parts to\nachieve state-of-the-art results on many NLP tasks.\n</p>\n<p>This paper introduces a text classifier using topological data analysis. We\nuse BERT's attention maps transformed into attention graphs as the only input\nto that classifier. The model can solve tasks such as distinguishing spam from\nham messages, recognizing whether a sentence is grammatically correct, or\nevaluating a movie review as negative or positive. It performs comparably to\nthe BERT baseline and outperforms it on some tasks.\n</p>\n<p>Additionally, we propose a new method to reduce the number of BERT's\nattention heads considered by the topological classifier, which allows us to\nprune the number of heads from 144 down to as few as ten with no reduction in\nperformance. Our work also shows that the topological model displays higher\nrobustness against adversarial attacks than the original BERT model, which is\nmaintained during the pruning process. To the best of our knowledge, this work\nis the first to confront topological-based models with adversarial attacks in\nthe context of NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Ilan Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinauer_R/0/1/0/all/0/1\">Raphael Reinauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Pretraining for Multilingual Acronym Extraction. (arXiv:2206.15221v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15221","description":"<p>This paper presents our findings from participating in the multilingual\nacronym extraction shared task SDU@AAAI-22. The task consists of acronym\nextraction from documents in 6 languages within scientific and legal domains.\nTo address multilingual acronym extraction we employed BiLSTM-CRF with\nmultilingual XLM-RoBERTa embeddings. We pretrained the XLM-RoBERTa model on the\nshared task corpus to further adapt XLM-RoBERTa embeddings to the shared task\ndomain(s). Our system (team: SMR-NLP) achieved competitive performance for\nacronym extraction across all the languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yaseen_U/0/1/0/all/0/1\">Usama Yaseen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_S/0/1/0/all/0/1\">Stefan Langer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FL-Tuning: Layer Tuning for Feed-Forward Network in Transformer. (arXiv:2206.15312v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15312","description":"<p>Prompt tuning is an emerging way of adapting pre-trained language models to\ndownstream tasks. However, the existing studies are mainly to add prompts to\nthe input sequence. This way would not work as expected due to the intermediate\nmulti-head self-attention and feed-forward network computation, making model\noptimization not very smooth. Hence, we propose a novel tuning way called layer\ntuning, aiming to add learnable parameters in Transformer layers. Specifically,\nwe focus on layer tuning for feed-forward network in the Transformer, namely\nFL-tuning. It introduces additional units into the hidden layer of each\nfeed-forward network. We conduct extensive experiments on the public CLUE\nbenchmark. The results show that: 1) Our FL-tuning outperforms prompt tuning\nmethods under both full-data and few-shot settings in almost all cases. In\nparticular, it improves accuracy by 17.93% (full-data setting) on WSC 1.0 and\nF1 by 16.142% (few-shot setting) on CLUENER over P-tuning v2. 2) Our FL-tuning\nis more stable and converges about 1.17 times faster than P-tuning v2. 3) With\nonly about 3% of Transformer's parameters to be trained, FL-tuning is\ncomparable with fine-tuning on most datasets, and significantly outperforms\nfine-tuning (e.g., accuracy improved by 12.9% on WSC 1.1) on several datasets.\nThe source codes are available at https://github.com/genggui001/FL-Tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_K/0/1/0/all/0/1\">Kui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongli Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiaqing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_T/0/1/0/all/0/1\">Tong Ruan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Classifier for COVID-19 Misinformation Detection Using BERT: a Study on Indonesian Tweets. (arXiv:2206.15359v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15359","description":"<p>The COVID-19 pandemic has caused globally significant impacts since the\nbeginning of 2020. This brought a lot of confusion to society, especially due\nto the spread of misinformation through social media. Although there were\nalready several studies related to the detection of misinformation in social\nmedia data, most studies focused on the English dataset. Research on COVID-19\nmisinformation detection in Indonesia is still scarce. Therefore, through this\nresearch, we collect and annotate datasets for Indonesian and build prediction\nmodels for detecting COVID-19 misinformation by considering the tweet's\nrelevance. The dataset construction is carried out by a team of annotators who\nlabeled the relevance and misinformation of the tweet data. In this study, we\npropose the two-stage classifier model using IndoBERT pre-trained language\nmodel for the Tweet misinformation detection task. We also experiment with\nseveral other baseline models for text classification. The experimental results\nshow that the combination of the BERT sequence classifier for relevance\nprediction and Bi-LSTM for misinformation detection outperformed other machine\nlearning models with an accuracy of 87.02%. Overall, the BERT utilization\ncontributes to the higher performance of most prediction models. We release a\nhigh-quality COVID-19 misinformation Tweet corpus in the Indonesian language,\nindicated by the high inter-annotator agreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faisal_D/0/1/0/all/0/1\">Douglas Raevan Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahendra_R/0/1/0/all/0/1\">Rahmad Mahendra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual grounding of abstract and concrete words: A response to G\\\"unther et al. (2020). (arXiv:2206.15381v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15381","description":"<p>Current computational models capturing words' meaning mostly rely on textual\ncorpora. While these approaches have been successful over the last decades,\ntheir lack of grounding in the real world is still an ongoing problem. In this\npaper, we focus on visual grounding of word embeddings and target two important\nquestions. First, how can language benefit from vision in the process of visual\ngrounding? And second, is there a link between visual grounding and abstract\nconcepts? We investigate these questions by proposing a simple yet effective\napproach where language benefits from vision specifically with respect to the\nmodeling of both concrete and abstract words. Our model aligns word embeddings\nwith their corresponding visual representation without deteriorating the\nknowledge captured by textual distributional information. We apply our model to\na behavioral experiment reported by G\\\"unther et al. (2020), which addresses\nthe plausibility of having visual mental representations for abstract words.\nOur evaluation results show that: (1) It is possible to predict human behaviour\nto a large degree using purely textual embeddings. (2) Our grounded embeddings\nmodel human behavior better compared to their textual counterparts. (3)\nAbstract concepts benefit from visual grounding implicitly through their\nconnections to concrete concepts, rather than from having corresponding visual\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_H/0/1/0/all/0/1\">Hassan Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heitmeier_M/0/1/0/all/0/1\">Maria Heitmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_Bajestan_E/0/1/0/all/0/1\">Elnaz Shafaei-Bajestan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_H/0/1/0/all/0/1\">Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Speech Criteria: A Modular Approach to Task-Specific Hate Speech Definitions. (arXiv:2206.15455v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15455","description":"<p>\\textbf{Offensive Content Warning}: This paper contains offensive language\nonly for providing examples that clarify this research and do not reflect the\nauthors' opinions. Please be aware that these examples are offensive and may\ncause you distress.\n</p>\n<p>The subjectivity of recognizing \\textit{hate speech} makes it a complex task.\nThis is also reflected by different and incomplete definitions in NLP. We\npresent \\textit{hate speech} criteria, developed with perspectives from law and\nsocial science, with the aim of helping researchers create more precise\ndefinitions and annotation guidelines on five aspects: (1) target groups, (2)\ndominance, (3) perpetrator characteristics, (4) type of negative group\nreference, and the (5) type of potential consequences/effects. Definitions can\nbe structured so that they cover a more broad or more narrow phenomenon. As\nsuch, conscious choices can be made on specifying criteria or leaving them\nopen. We argue that the goal and exact task developers have in mind should\ndetermine how the scope of \\textit{hate speech} is defined. We provide an\noverview of the properties of English datasets from \\url{hatespeechdata.com}\nthat may help select the most suitable dataset for a specific scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_U/0/1/0/all/0/1\">Urja Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermeulen_I/0/1/0/all/0/1\">Ivar Vermeulen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalisnick_E/0/1/0/all/0/1\">Eric Nalisnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noorloos_M/0/1/0/all/0/1\">Marloes van Noorloos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokkens_A/0/1/0/all/0/1\">Antske Fokkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15462","description":"<p>We propose a margin-based loss for vision-language model pretraining that\nencourages gradient-based explanations that are consistent with region-level\nannotations. We refer to this objective as Attention Mask Consistency (AMC) and\ndemonstrate that it produces superior visual grounding performance compared to\nmodels that rely instead on region-level annotations for explicitly training an\nobject detector such as Faster R-CNN. AMC works by encouraging gradient-based\nexplanation masks that focus their attention scores mostly within annotated\nregions of interest for images that contain such annotations. Particularly, a\nmodel trained with AMC on top of standard vision-language modeling objectives\nobtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding\nbenchmark, an absolute improvement of 5.48% when compared to the best previous\nmodel. Our approach also performs exceedingly well on established benchmarks\nfor referring expression comprehension and offers the added benefit by design\nof gradient-based explanations that better align with human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1\">Kushal Kafle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roman_V/0/1/0/all/0/1\">Vicente Ord&#xf3;&#xf1;ez Rom&#xe1;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting Future World Events with Neural Networks. (arXiv:2206.15474v1 [cs.LG])","link":"http://arxiv.org/abs/2206.15474","description":"<p>Forecasting future world events is a challenging but valuable task. Forecasts\nof climate, geopolitical conflict, pandemics and economic indicators help shape\npolicy and decision making. In these domains, the judgment of expert humans\ncontributes to the best forecasts. Given advances in language modeling, can\nthese forecasts be automated? To this end, we introduce Autocast, a dataset\ncontaining thousands of forecasting questions and an accompanying news corpus.\nQuestions are taken from forecasting tournaments, ensuring high quality,\nreal-world importance, and diversity. The news corpus is organized by date,\nallowing us to precisely simulate the conditions under which humans made past\nforecasts (avoiding leakage from the future). Motivated by the difficulty of\nforecasting numbers across orders of magnitude (e.g. global cases of COVID-19\nin 2022), we also curate IntervalQA, a dataset of numerical questions and\nmetrics for calibration. We test language models on our forecasting task and\nfind that performance is far below a human expert baseline. However,\nperformance improves with increased model size and incorporation of relevant\ninformation from the news corpus. In sum, Autocast poses a novel challenge for\nlarge language models and improved performance could bring large practical\nbenefits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tristan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ryan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joe Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Richard Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsEdits: A Dataset of Revision Histories for News Articles (Technical Report: Data Processing). (arXiv:2104.09647v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09647","description":"<p>News article revision histories have the potential to give us novel insights\nacross varied fields of linguistics and social sciences. In this work, we\npresent, to our knowledge, the first publicly available dataset of news article\nrevision histories, or NewsEdits.\n</p>\n<p>Our dataset is multilingual; it contains 1,278,804 articles with 4,609,430\nversions from over 22 English- and French-language newspaper sources based in\nthree countries. Across version pairs, we count 10.9 million added sentences;\n8.9 million changed sentences and 6.8 million removed sentences. Within the\nchanged sentences, we derive 72 million atomic edits. NewsEdits is, to our\nknowledge, the largest corpus of revision histories of any domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alexander Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StateCensusLaws.org: A Web Application for Consuming and Annotating Legal Discourse Learning. (arXiv:2104.10263v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.10263","description":"<p>In this work, we create a web application to highlight the output of NLP\nmodels trained to parse and label discourse segments in law text. Our system is\nbuilt primarily with journalists and legal interpreters in mind, and we focus\non state-level law that uses U.S. Census population numbers to allocate\nresources and organize government.\n</p>\n<p>Our system exposes a corpus we collect of 6,000 state-level laws that pertain\nto the U.S. census, using 25 scrapers we built to crawl state law websites,\nwhich we release. We also build a novel, flexible annotation framework that can\nhandle span-tagging and relation tagging on an arbitrary input text document\nand be embedded simply into any webpage. This framework allows journalists and\nresearchers to add to our annotation database by correcting and tagging new\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spangher_A/0/1/0/all/0/1\">Alexander Spangher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto Response Generation in Online Medical Chat Services. (arXiv:2104.12755v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12755","description":"<p>Telehealth helps to facilitate access to medical professionals by enabling\nremote medical services for the patients. These services have become gradually\npopular over the years with the advent of necessary technological\ninfrastructure. The benefits of telehealth have been even more apparent since\nthe beginning of the COVID-19 crisis, as people have become less inclined to\nvisit doctors in person during the pandemic. In this paper, we focus on\nfacilitating the chat sessions between a doctor and a patient. We note that the\nquality and efficiency of the chat experience can be critical as the demand for\ntelehealth services increases. Accordingly, we develop a smart auto-response\ngeneration mechanism for medical conversations that helps doctors respond to\nconsultation requests efficiently, particularly during busy sessions. We\nexplore over 900,000 anonymous, historical online messages between doctors and\npatients collected over nine months. We implement clustering algorithms to\nidentify the most frequent responses by doctors and manually label the data\naccordingly. We then train machine learning algorithms using this preprocessed\ndata to generate the responses. The considered algorithm has two steps: a\nfiltering (i.e., triggering) model to filter out infeasible patient messages\nand a response generator to suggest the top-3 doctor responses for the ones\nthat successfully pass the triggering phase. The method provides an accuracy of\n83.28\\% for precision@3 and shows robustness to its parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jahanshahi_H/0/1/0/all/0/1\">Hadi Jahanshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazmi_S/0/1/0/all/0/1\">Syed Kazmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevik_M/0/1/0/all/0/1\">Mucahit Cevik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explanations. (arXiv:2106.13876v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.13876","description":"<p>Models that generate extractive rationales (i.e., subsets of features) or\nnatural language explanations (NLEs) for their predictions are important for\nexplainable AI. While an extractive rationale provides a quick view of the\nfeatures most responsible for a prediction, an NLE allows for a comprehensive\ndescription of the decision-making process behind a prediction. However,\ncurrent models that generate the best extractive rationales or NLEs often fall\nbehind the state-of-the-art (SOTA) in terms of task performance. In this work,\nwe bridge this gap by introducing RExC, a self-rationalizing framework that\ngrounds its predictions and two complementary types of explanations (NLEs and\nextractive rationales) in background knowledge. Our framework improves over\nprevious methods by: (i) reaching SOTA task performance while also providing\nexplanations, (ii) providing two types of explanations, while existing models\nusually provide only one type, and (iii) beating by a large margin the previous\nSOTA in terms of quality of both types of explanations. Furthermore, a\nperturbation analysis in RExC shows a high degree of association between\nexplanations and predictions, a necessary property of faithful explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Neural Machine Translation with Dependency-Scaled Self-Attention Network. (arXiv:2111.11707v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.11707","description":"<p>Syntax knowledge contributes its powerful strength in Neural machine\ntranslation (NMT) tasks. Early NMT works supposed that syntax details can be\nautomatically learned from numerous texts via attention networks. However,\nsucceeding researches pointed out that limited by the uncontrolled nature of\nattention computation, the NMT model requires an external syntax to capture the\ndeep syntactic awareness. Although existing syntax-aware NMT methods have bored\ngreat fruits in combining syntax, the additional workloads they introduced\nrender the model heavy and slow. Particularly, these efforts scarcely involve\nthe Transformer-based NMT and modify its core self-attention network (SAN). To\nthis end, we propose a parameter-free, dependency-scaled self-attention network\n(Deps-SAN) for syntax-aware Transformer-based NMT. A quantified matrix of\ndependency closeness between tokens is constructed to impose explicit syntactic\nconstraints into the SAN for learning syntactic details and dispelling the\ndispersion of attention distributions. Two knowledge sparsing techniques are\nfurther integrated to avoid the model overfitting the dependency noises\nintroduce by the external parser. Experiments and analyses on IWSLT14\nGerman-to-English and WMT16 German-to-English benchmark NMT tasks verify the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Ru Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_T/0/1/0/all/0/1\">Tianyong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junbo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Visual Attention for Simultaneous Multimodal Machine Translation. (arXiv:2201.09324v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.09324","description":"<p>Recently, there has been a surge in research in multimodal machine\ntranslation (MMT), where additional modalities such as images are used to\nimprove translation quality of textual systems. A particular use for such\nmultimodal systems is the task of simultaneous machine translation, where\nvisual context has been shown to complement the partial information provided by\nthe source sentence, especially in the early phases of translation. In this\npaper, we propose the first Transformer-based simultaneous MMT architecture,\nwhich has not been previously explored in the field. Additionally, we extend\nthis model with an auxiliary supervision signal that guides its visual\nattention mechanism using labelled phrase-region alignments. We perform\ncomprehensive experiments on three language directions and conduct thorough\nquantitative and qualitative analyses using both automatic metrics and manual\ninspection. Our results show that (i) supervised visual attention consistently\nimproves the translation quality of the MMT models, and (ii) fine-tuning the\nMMT with supervision loss enabled leads to better performance than training the\nMMT from scratch. Compared to the state-of-the-art, our proposed model achieves\nimprovements of up to 2.3 BLEU and 3.5 METEOR points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haralampieva_V/0/1/0/all/0/1\">Veneta Haralampieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caglayan_O/0/1/0/all/0/1\">Ozan Caglayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning with Random-projection Quantizer for Speech Recognition. (arXiv:2202.01855v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.01855","description":"<p>We present a simple and effective self-supervised learning approach for\nspeech recognition. The approach learns a model to predict the masked speech\nsignals, in the form of discrete labels generated with a random-projection\nquantizer. In particular the quantizer projects speech inputs with a randomly\ninitialized matrix, and does a nearest-neighbor lookup in a\nrandomly-initialized codebook. Neither the matrix nor the codebook is updated\nduring self-supervised learning. Since the random-projection quantizer is not\ntrained and is separated from the speech recognition model, the design makes\nthe approach flexible and is compatible with universal speech recognition\narchitecture. On LibriSpeech our approach achieves similar word-error-rates as\nprevious work using self-supervised learning with non-streaming models, and\nprovides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with\nstreaming models. On multilingual tasks the approach also provides significant\nimprovement over wav2vec 2.0 and w2v-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review. (arXiv:2202.12205v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.12205","description":"<p>Advocates for Neuro-Symbolic Artificial Intelligence (NeSy) assert that\ncombining deep learning with symbolic reasoning will lead to stronger AI than\neither paradigm on its own. As successful as deep learning has been, it is\ngenerally accepted that even our best deep learning systems are not very good\nat abstract reasoning. And since reasoning is inextricably linked to language,\nit makes intuitive sense that Natural Language Processing (NLP), would be a\nparticularly well-suited candidate for NeSy. We conduct a structured review of\nstudies implementing NeSy for NLP, with the aim of answering the question of\nwhether NeSy is indeed meeting its promises: reasoning, out-of-distribution\ngeneralization, interpretability, learning and reasoning from small data, and\ntransferability to new domains. We examine the impact of knowledge\nrepresentation, such as rules and semantic networks, language structure and\nrelational structure, and whether implicit or explicit reasoning contributes to\nhigher promise scores. We find that systems where logic is compiled into the\nneural network lead to the most NeSy goals being satisfied, while other factors\nsuch as knowledge representation, or type of neural architecture do not exhibit\na clear correlation with goals being met. We find many discrepancies in how\nreasoning is defined, specifically in relation to human level reasoning, which\nimpact decisions about model architectures and drive conclusions which are not\nalways consistent across studies. Hence we advocate for a more methodical\napproach to the application of theories of human reasoning as well as the\ndevelopment of appropriate benchmarks, which we hope can lead to a better\nunderstanding of progress in the field. We make our data and code available on\ngithub for further analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_K/0/1/0/all/0/1\">Kyle Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1\">Aparna Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozic_B/0/1/0/all/0/1\">Bojan Bo&#x17e;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longo_L/0/1/0/all/0/1\">Luca Longo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.04592","description":"<p>Benchmarks are crucial to measuring and steering progress in artificial\nintelligence (AI). However, recent studies raised concerns over the state of AI\nbenchmarking, reporting issues such as benchmark overfitting, benchmark\nsaturation and increasing centralization of benchmark dataset creation. To\nfacilitate monitoring of the health of the AI benchmarking ecosystem, we\nintroduce methodologies for creating condensed maps of the global dynamics of\nbenchmark creation and saturation. We curated data for 1688 benchmarks covering\nthe entire domains of computer vision and natural language processing, and show\nthat a large fraction of benchmarks quickly trended towards near-saturation,\nthat many benchmarks fail to find widespread utilization, and that benchmark\nperformance gains for different AI tasks were prone to unforeseen bursts. We\nanalyze attributes associated with benchmark popularity, and conclude that\nfuture benchmarks should emphasize versatility, breadth and real-world utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_Silva_A/0/1/0/all/0/1\">Adriano Barbosa-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12886","description":"<p>Preschool evaluation is crucial because it gives teachers and parents crucial\nknowledge about a children's growth and development. The coronavirus pandemic\nhas highlighted the necessity for preschool children to be assessed online.\nThis online testing requires a variety of technologies, from web application\ndevelopment to various artificial intelligence models in diverse criteria such\nas speech recognition. Because of the acoustic fluctuations and differences in\nvoice frequencies between children and adults, employing Automatic Speech\nRecognition(ASR) systems is difficult because they are pre-trained on adults'\nvoices. In addition, training a new model requires a large amount of data. To\nsolve this issue, we constructed an ASR for our cognitive test system using the\nWav2Vec 2.0 model with a new pre-training objective, called Random Frequency\nPitch(RFP), and our new dataset, which was tested on Meaningless Words(MW) and\nRapid Automatic Naming(RAN) tests. Due to the peculiarities of these two tests,\nwe explored numerous models, including Convolutional Neural Network(CNN) and\nWav2Vec 2.0 models. Our new approach, reaches Word Error Rate(WER) of 6.45 on\nthe Persian section of CommonVoice dataset. Furthermore our novel methodology\nproduces positive outcomes in zero- and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting. (arXiv:2203.16361v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.16361","description":"<p>Catastrophic forgetting is a thorny challenge when updating keyword spotting\n(KWS) models after deployment. This problem will be more challenging if KWS\nmodels are further required for edge devices due to their limited memory. To\nalleviate such an issue, we propose a novel diversity-aware incremental\nlearning method named Rainbow Keywords (RK). Specifically, the proposed RK\napproach introduces a diversity-aware sampler to select a diverse set from\nhistorical and incoming keywords by calculating classification uncertainty. As\na result, the RK approach can incrementally learn new tasks without forgetting\nprior knowledge. Besides, the RK approach also proposes data augmentation and\nknowledge distillation loss function for efficient memory management on the\nedge device. Experimental results show that the proposed RK approach achieves\n4.2% absolute improvement in terms of average accuracy over the best baseline\non Google Speech Command dataset with less required memory. The scripts are\navailable on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_N/0/1/0/all/0/1\">Nana Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech. (arXiv:2203.17190v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.17190","description":"<p>Recently, leveraging BERT pre-training to improve the phoneme encoder in text\nto speech (TTS) has drawn increasing attention. However, the works apply\npre-training with character-based units to enhance the TTS phoneme encoder,\nwhich is inconsistent with the TTS fine-tuning that takes phonemes as input.\nPre-training only with phonemes as input can alleviate the input mismatch but\nlack the ability to model rich representations and semantic information due to\nlimited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a\nnovel variant of the BERT model that uses mixed phoneme and sup-phoneme\nrepresentations to enhance the learning capability. Specifically, we merge the\nadjacent phonemes into sup-phonemes and combine the phoneme sequence and the\nmerged sup-phoneme sequence as the model input, which can enhance the model\ncapacity to learn rich contextual representations. Experiment results\ndemonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS\nperformance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The\nMixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to\nthe previous TTS pre-trained model PnG BERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_D/0/1/0/all/0/1\">Daxin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yuzi Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Tan Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment Analysis. (arXiv:2204.00791v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00791","description":"<p>As an extensive research in the field of natural language processing (NLP),\naspect-based sentiment analysis (ABSA) is the task of predicting the sentiment\nexpressed in a text relative to the corresponding aspect. Unfortunately, most\nlanguages lack sufficient annotation resources, thus more and more recent\nresearchers focus on cross-lingual aspect-based sentiment analysis (XABSA).\nHowever, most recent researches only concentrate on cross-lingual data\nalignment instead of model alignment. To this end, we propose a novel\nframework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based\nSentiment Analysis. Based on contrastive learning, we close the distance\nbetween samples with the same label in different semantic spaces, thus\nachieving a convergence of semantic spaces of different languages.\nSpecifically, we design two contrastive strategies, token level contrastive\nlearning of token embeddings (TL-CTE) and sentiment level contrastive learning\nof token embeddings (SL-CTE), to regularize the semantic space of source and\ntarget language to be more uniform. Since our framework can receive datasets in\nmultiple languages during training, our framework can be adapted not only for\nXABSA task but also for multilingual aspect-based sentiment analysis (MABSA).\nTo further improve the performance of our model, we perform knowledge\ndistillation technology leveraging data from unlabeled target language. In the\ndistillation XABSA task, we further explore the comparative effectiveness of\ndifferent data (source dataset, translated dataset, and code-switched dataset).\nThe results demonstrate that the proposed method has a certain improvement in\nthe three tasks of XABSA, distillation XABSA and MABSA. For reproducibility,\nour code for this paper is available at https://github.com/GKLMIP/CL-XABSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiaotian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Aimin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation. (arXiv:2204.02967v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02967","description":"<p>Direct speech-to-speech translation (S2ST) models suffer from data scarcity\nissues as there exists little parallel S2ST data, compared to the amount of\ndata available for conventional cascaded systems that consist of automatic\nspeech recognition (ASR), machine translation (MT), and text-to-speech (TTS)\nsynthesis. In this work, we explore self-supervised pre-training with unlabeled\nspeech data and data augmentation to tackle this issue. We take advantage of a\nrecently proposed speech-to-unit translation (S2UT) framework that encodes\ntarget speech into discrete representations, and transfer pre-training and\nefficient partial finetuning techniques that work well for speech-to-text\ntranslation (S2T) to the S2UT domain by studying both speech encoder and\ndiscrete unit decoder pre-training. Our experiments on Spanish-English\ntranslation show that self-supervised pre-training consistently improves model\nperformance compared with multitask learning with an average 6.6-12.1 BLEU\ngain, and it can be further combined with data augmentation techniques that\napply MT to create weakly supervised training data. Audio samples are available\nat:\nhttps://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popuri_S/0/1/0/all/0/1\">Sravya Popuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Production federated keyword spotting via distillation, filtering, and joint federated-centralized training. (arXiv:2204.06322v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2204.06322","description":"<p>We trained a keyword spotting model using federated learning on real user\ndevices and observed significant improvements when the model was deployed for\ninference on phones. To compensate for data domains that are missing from\non-device training caches, we employed joint federated-centralized training.\nAnd to learn in the absence of curated labels on-device, we formulated a\nconfidence filtering strategy based on user-feedback signals for federated\ndistillation. These techniques created models that significantly improved\nquality metrics in offline evaluations and user-experience metrics in live A/B\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hard_A/0/1/0/all/0/1\">Andrew Hard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Partridge_K/0/1/0/all/0/1\">Kurt Partridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_N/0/1/0/all/0/1\">Neng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Augenstein_S/0/1/0/all/0/1\">Sean Augenstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1\">Aishanee Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_H/0/1/0/all/0/1\">Hyun Jin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_A/0/1/0/all/0/1\">Alex Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_S/0/1/0/all/0/1\">Sara Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_J/0/1/0/all/0/1\">Jessica Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Event Linking to Wikidata. (arXiv:2204.06535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06535","description":"<p>We present a task of multilingual linking of events to a knowledge base. We\nautomatically compile a large-scale dataset for this task, comprising of 1.8M\nmentions across 44 languages referring to over 10.9K events from Wikidata. We\npropose two variants of the event linking task: 1) multilingual, where event\ndescriptions are from the same language as the mention, and 2) crosslingual,\nwhere all event descriptions are in English. On the two proposed tasks, we\ncompare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and\nmultilingual adaptations of the biencoder and crossencoder architectures from\nBLINK (Wu et al., 2020). In our experiments on the two task variants, we find\nboth biencoder and crossencoder models significantly outperform the BM25+\nbaseline. Our results also indicate that the crosslingual task is in general\nmore challenging than the multilingual task. To test the out-of-domain\ngeneralization of the proposed linking systems, we additionally create a\nWikinews-based evaluation set. We present qualitative analysis highlighting\nvarious aspects captured by the proposed dataset, including the need for\ntemporal reasoning over context and tackling diverse event descriptions across\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rishubh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration strategies for articulatory synthesis of complex syllable onsets. (arXiv:2204.09381v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.09381","description":"<p>High-quality articulatory speech synthesis has many potential applications in\nspeech science and technology. However, developing appropriate mappings from\nlinguistic specification to articulatory gestures is difficult and time\nconsuming. In this paper we construct an optimisation-based framework as a\nfirst step towards learning these mappings without manual intervention. We\ndemonstrate the production of syllables with complex onsets and discuss the\nquality of the articulatory gestures with reference to coarticulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_D/0/1/0/all/0/1\">Daniel R. van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Anqi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerazov_B/0/1/0/all/0/1\">Branislav Gerazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krug_P/0/1/0/all/0/1\">Paul K. Krug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birkholz_P/0/1/0/all/0/1\">Peter Birkholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review on Multiple Plagiarism: A Performance Comparison Study. (arXiv:2206.02983v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02983","description":"<p>Plagiarism is the practice of claiming to be someone else content, thoughts\nor ideas as one own without any proper credit and citations. This paper is a\nsurvey paper that, represent the some of the great research paper and its\ncomparison that is work done on plagiarism. Now a days, plagiarism became one\nof the most interesting and crucial research points in Natural Language\nProcessing area. We review some old research paper based on different types of\nplagiarism detection and their models and algorithm, and comparison of the\naccuracy of those papers. There are many several ways which are available for\nplagiarism detection in different language. There are a few algorithms to\ndetecting plagiarism. Like, corpus, CL-CNG, LSI, Levenshtein Distance etc. We\nanalysis those papers, and learn that they used different types of algorithms\nfor detecting plagiarism. After experiment those papers, we got that some of\nthe algorithms give a better output and accuracy for detecting plagiarism. We\nare going to give a review on some papers about Plagiarism and will discuss\nabout the pros and cons of their models. And we also show a propose method for\nplagiarism detection method which based on sentience separation, word\nseparation and make sentence based on synonym and compare with any sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nahian_J/0/1/0/all/0/1\">Jabir Al Nahian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masum_A/0/1/0/all/0/1\">Abu Kaisar Mohammad Masum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Corporate Risk by Jointly Modeling Company Networks and Dialogues in Earnings Conference Calls. (arXiv:2206.06174v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.06174","description":"<p>Earnings conference calls are attracting an increasing number of researchers\ndue to their free form and rich information. Existing studies, however, do not\ntake speaker role information into account. Furthermore, current research does\nnot fully account for the impact of inter-company relationships on company\nrisk. The only study that integrates company networks and earnings conference\ncalls constructs an undirected graph for companies holding earnings conference\ncalls at different dates, failing to meet the requirement of no temporal\ninformation leakage for prediction tasks. To address the aforementioned issues,\nwe propose a new model called Temporal Virtual Graph Neural Network (TVGNN),\nwhich incorporates earnings conference calls and company networks to predict\ncompany risk. For the first time, our model incorporates participant role\ninformation in dialogue modeling. Moreover, we develop a new approach to\nconstruct company networks that ensures no temporal information leakage in the\ngraph. In experiments, our proposed model outperforms all baselines. The\nsupplementary analyses demonstrate the model's effectiveness and\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sang_Y/0/1/0/all/0/1\">Yunxin Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yang Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implementing a Chatbot Solution for Learning Management System. (arXiv:2206.13187v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.13187","description":"<p>Innovation is a key component in trying new solutions for the students to\nlearn efficiently and in ways that correspond to their own experience, where\nchatbots are one of these new solutions. One of the main problem that chatbots\nface today is to mimic human language, where they try to find the best answer\nto an input, which is not how a human conversation usually works, rather taking\ninto account the previous messages and building onto them. Extreme programming\nmethodology was chosen to use integrate ChatterBot, Pyside2, web scraping and\nTampermonkey into Blackboard as a test case. Problems occurred with the bot and\nmore training was needed for the bot to work perfectly, but the integration and\nweb scraping worked, giving us a chatbot that was able to talk with. We showed\nthe plausibility of integrating an AI bot in an educational setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaskopoulos_D/0/1/0/all/0/1\">Dimitrios Chaskopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haegdahl_J/0/1/0/all/0/1\">Jonas Eilertsen H&#xe6;gdahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagvold_P/0/1/0/all/0/1\">Petter Sagvold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trinquet_C/0/1/0/all/0/1\">Claire Trinquet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edalati_M/0/1/0/all/0/1\">Maryam Edalati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Range Language Modeling via Gated State Spaces. (arXiv:2206.13947v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.13947","description":"<p>State space models have shown to be effective at modeling long range\ndependencies, specially on sequence classification tasks. In this work we focus\non autoregressive sequence modeling over English books, Github source code and\nArXiv mathematics articles. Based on recent developments around the\neffectiveness of gated activation functions, we propose a new layer named Gated\nState Space (GSS) and show that it trains significantly faster than the\ndiagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several\nwell-tuned Transformer-based baselines and exhibits zero-shot generalization to\nlonger inputs while being straightforward to implement. Finally, we show that\nleveraging self-attention to model local dependencies improves the performance\nof GSS even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation of Transformer-based Language Models Revisited. (arXiv:2206.14366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14366","description":"<p>In the past few years, transformer-based pre-trained language models have\nachieved astounding success in both industry and academia. However, the large\nmodel size and high run-time latency are serious impediments to applying them\nin practice, especially on mobile phones and Internet of Things (IoT) devices.\nTo compress the model, considerable literature has grown up around the theme of\nknowledge distillation (KD) recently. Nevertheless, how KD works in\ntransformer-based models is still unclear. We tease apart the components of KD\nand propose a unified KD framework. Through the framework, systematic and\nextensive experiments that spent over 23,000 GPU hours render a comprehensive\nanalysis from the perspectives of knowledge types, matching strategies,\nwidth-depth trade-off, initialization, model size, etc. Our empirical results\nshed light on the distillation in the pre-train language model and with\nrelative significant improvement over previous state-of-the-arts(SOTA).\nFinally, we provide a best-practice guideline for the KD in transformer-based\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chengqiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yunfei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Strong Lensing Source Reconstruction Using Continuous Neural Fields. (arXiv:2206.14820v1 [astro-ph.CO])","link":"http://arxiv.org/abs/2206.14820","description":"<p>From the nature of dark matter to the rate of expansion of our Universe,\nobservations of distant galaxies distorted through strong gravitational lensing\nhave the potential to answer some of the major open questions in astrophysics.\nModeling galaxy-galaxy strong lensing observations presents a number of\nchallenges as the exact configuration of both the background source and\nforeground lens galaxy is unknown. A timely call, prompted by a number of\nupcoming surveys anticipating high-resolution lensing images, demands methods\nthat can efficiently model lenses at their full complexity. In this work, we\nintroduce a method that uses continuous neural fields to non-parametrically\nreconstruct the complex morphology of a source galaxy while simultaneously\ninferring a distribution over foreground lens galaxy configurations. We\ndemonstrate the efficacy of our method through experiments on simulated data\ntargeting high-resolution lensing images similar to those anticipated in\nnear-future astrophysical surveys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Mishra_Sharma_S/0/1/0/all/0/1\">Siddharth Mishra-Sharma</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causality for Inherently Explainable Transformers: CAT-XPLAIN. (arXiv:2206.14841v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14841","description":"<p>There have been several post-hoc explanation approaches developed to explain\npre-trained black-box neural networks. However, there is still a gap in\nresearch efforts toward designing neural networks that are inherently\nexplainable. In this paper, we utilize a recently proposed instance-wise\npost-hoc causal explanation method to make an existing transformer architecture\ninherently explainable. Once trained, our model provides an explanation in the\nform of top-$k$ regions in the input space of the given instance contributing\nto its decision. We evaluate our method on binary classification tasks using\nthree image datasets: MNIST, FMNIST, and CIFAR. Our results demonstrate that\ncompared to the causality-based post-hoc explainer model, our inherently\nexplainable model achieves better explainability results while eliminating the\nneed of training a separate explainer model. Our code is available at\nhttps://github.com/mvrl/CAT-XPLAIN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanal_S/0/1/0/all/0/1\">Subash Khanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brodie_B/0/1/0/all/0/1\">Benjamin Brodie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xin Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1\">Ai-Ling Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Reinforcement Learning for Small Bowel Path Tracking using Different Types of Annotations. (arXiv:2206.14847v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14847","description":"<p>Small bowel path tracking is a challenging problem considering its many folds\nand contact along its course. For the same reason, it is very costly to achieve\nthe ground-truth (GT) path of the small bowel in 3D. In this work, we propose\nto train a deep reinforcement learning tracker using datasets with different\ntypes of annotations. Specifically, we utilize CT scans that have only GT small\nbowel segmentation as well as ones with the GT path. It is enabled by designing\na unique environment that is compatible for both, including a reward definable\neven without the GT path. The performed experiments proved the validity of the\nproposed method. The proposed method holds a high degree of usability in this\nproblem by being able to utilize the scans with weak annotations, and thus by\npossibly reducing the required annotation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shin_S/0/1/0/all/0/1\">Seung Yeon Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Summers_R/0/1/0/all/0/1\">Ronald M. Summers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Motion Fields: Encoding Grasp Trajectories as Implicit Value Functions. (arXiv:2206.14854v1 [cs.RO])","link":"http://arxiv.org/abs/2206.14854","description":"<p>The pipeline of current robotic pick-and-place methods typically consists of\nseveral stages: grasp pose detection, finding inverse kinematic solutions for\nthe detected poses, planning a collision-free trajectory, and then executing\nthe open-loop trajectory to the grasp pose with a low-level tracking\ncontroller. While these grasping methods have shown good performance on\ngrasping static objects on a table-top, the problem of grasping dynamic objects\nin constrained environments remains an open problem. We present Neural Motion\nFields, a novel object representation which encodes both object point clouds\nand the relative task trajectories as an implicit value function parameterized\nby a neural network. This object-centric representation models a continuous\ndistribution over the SE(3) space and allows us to perform grasping reactively\nby leveraging sampling-based MPC to optimize this value function.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1\">Adithyavairavan Murali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaralingam_B/0/1/0/all/0/1\">Balakumar Sundaralingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage COVID19 Classification Using BERT Features. (arXiv:2206.14861v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14861","description":"<p>We propose an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages using double BERT feature extraction. In the first BERT feature\nextraction, A 3D-CNN is first used to extract CNN internal feature maps.\nInstead of using the global average pooling, a late BERT temporal pooing is\nused to aggregate the temporal information in these feature maps, followed by a\nclassification layer. This 3D-CNN-BERT classification network is first trained\non sampled fixed number of slice images from every original CT scan volume. In\nthe second stage, the 3D-CNN-BERT embedding features are extracted on all slice\nimages of every CT scan volume, and these features are averaged into a fixed\nnumber of segments. Then another BERT network is used to aggregate these\nmultiple features into a single feature followed by another classification\nlayer. The classification results of both stages are combined to generate final\noutputs. On the validation dataset, we achieve macro F1 score of 0.9164.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Q/0/1/0/all/0/1\">Qi Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teach me how to Interpolate a Myriad of Embeddings. (arXiv:2206.14868v1 [cs.LG])","link":"http://arxiv.org/abs/2206.14868","description":"<p>Mixup refers to interpolation-based data augmentation, originally motivated\nas a way to go beyond empirical risk minimization (ERM). Yet, its extensions\nfocus on the definition of interpolation and the space where it takes place,\nwhile the augmentation itself is less studied: For a mini-batch of size $m$,\nmost methods interpolate between $m$ pairs with a single scalar interpolation\nfactor $\\lambda$.\n</p>\n<p>In this work, we make progress in this direction by introducing MultiMix,\nwhich interpolates an arbitrary number $n$ of tuples, each of length $m$, with\none vector $\\lambda$ per tuple. On sequence data, we further extend to dense\ninterpolation and loss computation over all spatial positions. Overall, we\nincrease the number of tuples per mini-batch by orders of magnitude at little\nadditional cost. This is possible by interpolating at the very last layer\nbefore the classifier. Finally, to address inconsistencies due to linear target\ninterpolation, we introduce a self-distillation approach to generate and\ninterpolate synthetic targets.\n</p>\n<p>We empirically show that our contributions result in significant improvement\nover state-of-the-art mixup methods on four benchmarks. By analyzing the\nembedding space, we observe that the classes are more tightly clustered and\nuniformly spread over the embedding space, thereby explaining the improved\nbehavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_S/0/1/0/all/0/1\">Shashanka Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1\">Ewa Kijak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amsaleg_L/0/1/0/all/0/1\">Laurent Amsaleg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Unfolding of StyleGAN Latent Space. (arXiv:2206.14892v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14892","description":"<p>Generative adversarial networks (GANs) have proven to be surprisingly\nefficient for image editing by inverting and manipulating the latent code\ncorresponding to an input real image. This editing property emerges from the\ndisentangled nature of the latent space. In this paper, we identify that the\nfacial attribute disentanglement is not optimal, thus facial editing relying on\nlinear attribute separation is flawed. We thus propose to improve semantic\ndisentanglement with supervision. Our method consists in learning a proxy\nlatent representation using normalizing flows, and we show that this leads to a\nmore efficient space for face image editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1\">Mustafa Shukor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bushan Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellier_P/0/1/0/all/0/1\">Pierre Hellier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction. (arXiv:2206.14903v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14903","description":"<p>Spiculations/lobulations, sharp/curved spikes on the surface of lung nodules,\nare good predictors of lung cancer malignancy and hence, are routinely assessed\nand reported by radiologists as part of the standardized Lung-RADS clinical\nscoring criteria. Given the 3D geometry of the nodule and 2D slice-by-slice\nassessment by radiologists, manual spiculation/lobulation annotation is a\ntedious task and thus no public datasets exist to date for probing the\nimportance of these clinically-reported features in the SOTA malignancy\nprediction algorithms. As part of this paper, we release a large-scale\nClinically-Interpretable Radiomics Dataset, CIRDataset, containing 956\nradiologist QA/QC'ed spiculation/lobulation annotations on segmented lung\nnodules from two public datasets, LIDC-IDRI (N=883) and LUNGx (N=73). We also\npresent an end-to-end deep learning model based on multi-class Voxel2Mesh\nextension to segment nodules (while preserving spikes), classify spikes\n(sharp/spiculation and curved/lobulation), and perform malignancy prediction.\nPrevious methods have performed malignancy prediction for LIDC and LUNGx\ndatasets but without robust attribution to any clinically reported/actionable\nfeatures (due to known hyperparameter sensitivity issues with general\nattribution schemes). With the release of this comprehensively-annotated\nCIRDataset and end-to-end deep learning baseline, we hope that malignancy\nprediction methods can validate their explanations, benchmark against our\nbaseline, and provide clinically-actionable insights. Dataset, code, pretrained\nmodels, and docker containers are available at\nhttps://github.com/nadeemlab/CIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Choi_W/0/1/0/all/0/1\">Wookjin Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dahiya_N/0/1/0/all/0/1\">Navdeep Dahiya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Combating Bias in Segmentation Networks by leveraging multiple resolutions. (arXiv:2206.14919v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14919","description":"<p>Exploration of bias has significant impact on the transparency and\napplicability of deep learning pipelines in medical settings, yet is so far\nwoefully understudied. In this paper, we consider two separate groups for which\ntraining data is only available at differing image resolutions. For group H,\navailable images and labels are at the preferred high resolution while for\ngroup L only deprecated lower resolution data exist. We analyse how this\nresolution-bias in the data distribution propagates to systematically biased\npredictions for group L at higher resolutions. Our results demonstrate that\nsingle-resolution training settings result in significant loss of volumetric\ngroup differences that translate to erroneous segmentations as measured by DSC\nand subsequent classification failures on the low resolution group. We further\nexplore how training data across resolutions can be used to combat this\nsystematic bias. Specifically, we investigate the effect of image resampling,\nscale augmentation and resolution independence and demonstrate that biases can\neffectively be reduced with multi-resolution approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Henschel_L/0/1/0/all/0/1\">Leonie Henschel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kugler_D/0/1/0/all/0/1\">David K&#xfc;gler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Andrews_D/0/1/0/all/0/1\">Derek S Andrews</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nordahl_C/0/1/0/all/0/1\">Christine W Nordahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Non-Random Missing Labels in Semi-Supervised Learning. (arXiv:2206.14923v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14923","description":"<p>Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in\nwhich the label Missing Not At Random (MNAR) problem is more realistic and\nchallenging, compared to the widely-adopted yet naive Missing Completely At\nRandom assumption where both labeled and unlabeled data share the same class\ndistribution. Different from existing SSL solutions that overlook the role of\n\"class\" in causing the non-randomness, e.g., users are more likely to label\npopular classes, we explicitly incorporate \"class\" into SSL. Our method is\nthree-fold: 1) We propose Class-Aware Propensity (CAP) that exploits the\nunlabeled data to train an improved classifier using the biased labeled data.\n2) To encourage rare class training, whose model is low-recall but\nhigh-precision that discards too many pseudo-labeled data, we propose\nClass-Aware Imputation (CAI) that dynamically decreases (or increases) the\npseudo-label assignment threshold for rare (or frequent) classes. 3) Overall,\nwe integrate CAP and CAI into a Class-Aware Doubly Robust (CADR) estimator for\ntraining an unbiased SSL model. Under various MNAR settings and ablations, our\nmethod not only significantly outperforms existing baselines but also surpasses\nother label bias removal SSL methods. Please check our code at:\nhttps://github.com/JoyHuYY1412/CADR-FixMatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF, meet differential geometry!. (arXiv:2206.14938v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14938","description":"<p>Neural radiance fields, or NeRF, represent a breakthrough in the field of\nnovel view synthesis and 3D modeling of complex scenes from multi-view image\ncollections. Numerous recent works have been focusing on making the models more\nrobust, by means of regularization, so as to be able to train with possibly\ninconsistent and/or very sparse data. In this work, we scratch the surface of\nhow differential geometry can provide regularization tools for robustly\ntraining NeRF-like models, which are modified so as to represent continuous and\ninfinitely differentiable functions. In particular, we show how these tools\nyield a direct mathematical formalism of previously proposed NeRF variants\naimed at improving the performance in challenging conditions (i.e. RegNeRF).\nBased on this, we show how the same formalism can be used to natively encourage\nthe regularity of surfaces (by means of Gaussian and Mean Curvatures) making it\npossible, for example, to learn surfaces from a very limited number of views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ehret_T/0/1/0/all/0/1\">Thibaud Ehret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mari_R/0/1/0/all/0/1\">Roger Mar&#xed;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1\">Gabriele Facciolo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLTS-GAN: Color-Lighting-Texture-Specular Reflection Augmentation for Colonoscopy. (arXiv:2206.14951v1 [eess.IV])","link":"http://arxiv.org/abs/2206.14951","description":"<p>Automated analysis of optical colonoscopy (OC) video frames (to assist\nendoscopists during OC) is challenging due to variations in color, lighting,\ntexture, and specular reflections. Previous methods either remove some of these\nvariations via preprocessing (making pipelines cumbersome) or add diverse\ntraining data with annotations (but expensive and time-consuming). We present\nCLTS-GAN, a new deep learning model that gives fine control over color,\nlighting, texture, and specular reflection synthesis for OC video frames. We\nshow that adding these colonoscopy-specific augmentations to the training data\ncan improve state-of-the-art polyp detection/segmentation methods as well as\ndrive next generation of OC simulators for training medical students. The code\nand pre-trained models for CLTS-GAN are available on Computational Endoscopy\nPlatform GitHub (https://github.com/nadeemlab/CEP).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mathew_S/0/1/0/all/0/1\">Shawn Mathew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadeem_S/0/1/0/all/0/1\">Saad Nadeem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie Kaufman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting 3D Object Detection by Simulating Multimodality on Point Clouds. (arXiv:2206.14971v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14971","description":"<p>This paper presents a new approach to boost a single-modality (LiDAR) 3D\nobject detector by teaching it to simulate features and responses that follow a\nmulti-modality (LiDAR-image) detector. The approach needs LiDAR-image data only\nwhen training the single-modality detector, and once well-trained, it only\nneeds LiDAR data at inference. We design a novel framework to realize the\napproach: response distillation to focus on the crucial response samples and\navoid the background samples; sparse-voxel distillation to learn voxel\nsemantics and relations from the estimated crucial voxels; a fine-grained\nvoxel-to-point distillation to better attend to features of small and distant\nobjects; and instance distillation to further enhance the deep-feature\nconsistency. Experimental results on the nuScenes dataset show that our\napproach outperforms all SOTA LiDAR-only 3D detectors and even surpasses the\nbaseline LiDAR-image detector on the key NDS metric, filling 72% mAP gap\nbetween the single- and multi-modality detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Mingxuan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking the Robustness of Deep Neural Networks to Common Corruptions in Digital Pathology. (arXiv:2206.14973v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14973","description":"<p>When designing a diagnostic model for a clinical application, it is crucial\nto guarantee the robustness of the model with respect to a wide range of image\ncorruptions. Herein, an easy-to-use benchmark is established to evaluate how\ndeep neural networks perform on corrupted pathology images. Specifically,\ncorrupted images are generated by injecting nine types of common corruptions\ninto validation images. Besides, two classification and one ranking metrics are\ndesigned to evaluate the prediction and confidence performance under\ncorruption. Evaluated on two resulting benchmark datasets, we find that (1) a\nvariety of deep neural network models suffer from a significant accuracy\ndecrease (double the error on clean images) and the unreliable confidence\nestimation on corrupted images; (2) A low correlation between the validation\nand test errors while replacing the validation set with our benchmark can\nincrease the correlation. Our codes are available on\nhttps://github.com/superjamessyx/robustness_benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Honglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sunyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenglu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified End-to-End Retriever-Reader Framework for Knowledge-based VQA. (arXiv:2206.14989v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14989","description":"<p>Knowledge-based Visual Question Answering (VQA) expects models to rely on\nexternal knowledge for robust answer prediction. Though significant it is, this\npaper discovers several leading factors impeding the advancement of current\nstate-of-the-art methods. On the one hand, methods which exploit the explicit\nknowledge take the knowledge as a complement for the coarsely trained VQA\nmodel. Despite their effectiveness, these approaches often suffer from noise\nincorporation and error propagation. On the other hand, pertaining to the\nimplicit knowledge, the multi-modal implicit knowledge for knowledge-based VQA\nstill remains largely unexplored. This work presents a unified end-to-end\nretriever-reader framework towards knowledge-based VQA. In particular, we shed\nlight on the multi-modal implicit knowledge from vision-language pre-training\nmodels to mine its potential in knowledge reasoning. As for the noise problem\nencountered by the retrieval operation on explicit knowledge, we design a novel\nscheme to create pseudo labels for effective knowledge supervision. This scheme\nis able to not only provide guidance for knowledge retrieval, but also drop\nthese instances potentially error-prone towards question answering. To validate\nthe effectiveness of the proposed method, we conduct extensive experiments on\nthe benchmark dataset. The experimental results reveal that our method\noutperforms existing baselines by a noticeable margin. Beyond the reported\nnumbers, this paper further spawns several insights on knowledge utilization\nfor future research with some empirical findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-domain Federated Object Detection. (arXiv:2206.14996v1 [cs.CV])","link":"http://arxiv.org/abs/2206.14996","description":"<p>Detection models trained by one party (server) may face severe performance\ndegradation when distributed to other users (clients). For example, in\nautonomous driving scenarios, different driving environments may bring obvious\ndomain shifts, which lead to biases in model predictions. Federated learning\nthat has emerged in recent years can enable multi-party collaborative training\nwithout leaking client data. In this paper, we focus on a special cross-domain\nscenario where the server contains large-scale data and multiple clients only\ncontain a small amount of data; meanwhile, there exist differences in data\ndistributions among the clients. In this case, traditional federated learning\ntechniques cannot take into account the learning of both the global knowledge\nof all participants and the personalized knowledge of a specific client. To\nmake up for this limitation, we propose a cross-domain federated object\ndetection framework, named FedOD. In order to learn both the global knowledge\nand the personalized knowledge in different domains, the proposed framework\nfirst performs the federated training to obtain a public global aggregated\nmodel through multi-teacher distillation, and sends the aggregated model back\nto each client for finetuning its personalized local model. After very few\nrounds of communication, on each client we can perform weighted ensemble\ninference on the public global model and the personalized local model. With the\nensemble, the generalization performance of the client-side model can\noutperform a single model with the same parameter scale. We establish a\nfederated object detection dataset which has significant background differences\nand instance differences based on multiple public autonomous driving datasets,\nand then conduct extensive experiments on the dataset. The experimental results\nvalidate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shangchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingzhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Transformer Network with Transfer Learning for Small-scale Fine-grained Skeleton-based Tai Chi Action Recognition. (arXiv:2206.15002v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15002","description":"<p>Human action recognition is a quite hugely investigated area where most\nremarkable action recognition networks usually use large-scale coarse-grained\naction datasets of daily human actions as inputs to state the superiority of\ntheir networks. We intend to recognize our small-scale fine-grained Tai Chi\naction dataset using neural networks and propose a transfer-learning method\nusing NTU RGB+D dataset to pre-train our network. More specifically, the\nproposed method first uses a large-scale NTU RGB+D dataset to pre-train the\nTransformer-based network for action recognition to extract common features\namong human motion. Then we freeze the network weights except for the fully\nconnected (FC) layer and take our Tai Chi actions as inputs only to train the\ninitialized FC weights. Experimental results show that our general model\npipeline can reach a high accuracy of small-scale fine-grained Tai Chi action\nrecognition with even few inputs and demonstrate that our method achieves the\nstate-of-the-art performance compared with previous Tai Chi action recognition\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Leiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSCLIP : A Framework for Explaining Distribution Shifts in Natural Language. (arXiv:2206.15007v1 [cs.CL])","link":"http://arxiv.org/abs/2206.15007","description":"<p>Helping end users comprehend the abstract distribution shifts can greatly\nfacilitate AI deployment. Motivated by this, we propose a novel task, dataset\nexplanation. Given two image data sets, dataset explanation aims to\nautomatically point out their dataset-level distribution shifts with natural\nlanguage. Current techniques for monitoring distribution shifts provide\ninadequate information to understand datasets with the goal of improving data\nquality. Therefore, we introduce GSCLIP, a training-free framework to solve the\ndataset explanation task. In GSCLIP, we propose the selector as the first\nquantitative evaluation method to identify explanations that are proper to\nsummarize dataset shifts. Furthermore, we leverage this selector to demonstrate\nthe superiority of a generator based on language model generation. Systematic\nevaluation on natural data shift verifies that GSCLIP, a combined system of a\nhybrid generator group and an efficient selector is not only easy-to-use but\nalso powerful for dataset explanation at scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1\">Weixin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Temporally Dynamic Data Augmentation for Video Recognition. (arXiv:2206.15015v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15015","description":"<p>Data augmentation has recently emerged as an essential component of modern\ntraining recipes for visual recognition tasks. However, data augmentation for\nvideo recognition has been rarely explored despite its effectiveness. Few\nexisting augmentation recipes for video recognition naively extend the image\naugmentation methods by applying the same operations to the whole video frames.\nOur main idea is that the magnitude of augmentation operations for each frame\nneeds to be changed over time to capture the real-world video's temporal\nvariations. These variations should be generated as diverse as possible using\nfewer additional hyper-parameters during training. Through this motivation, we\npropose a simple yet effective video data augmentation framework, DynaAugment.\nThe magnitude of augmentation operations on each frame is changed by an\neffective mechanism, Fourier Sampling that parameterizes diverse, smooth, and\nrealistic temporal variations. DynaAugment also includes an extended search\nspace suitable for video for automatic data augmentation methods. DynaAugment\nexperimentally demonstrates that there are additional performance rooms to be\nimproved from static augmentations on diverse video models. Specifically, we\nshow the effectiveness of DynaAugment on various video datasets and tasks:\nlarge-scale video recognition (Kinetics-400 and Something-Something-v2),\nsmall-scale video recognition (UCF- 101 and HMDB-51), fine-grained video\nrecognition (Diving-48 and FineGym), video action segmentation on Breakfast,\nvideo action localization on THUMOS'14, and video object detection on MOT17Det.\nDynaAugment also enables video models to learn more generalized representation\nto improve the model robustness on the corrupted videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeoh Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinhyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_M/0/1/0/all/0/1\">Minho Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sangdoo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myunggu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wee_D/0/1/0/all/0/1\">Dongyoon Wee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Timestamp-Supervised Action Segmentation with Graph Convolutional Networks. (arXiv:2206.15031v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15031","description":"<p>We introduce a novel approach for temporal activity segmentation with\ntimestamp supervision. Our main contribution is a graph convolutional network,\nwhich is learned in an end-to-end manner to exploit both frame features and\nconnections between neighboring frames to generate dense framewise labels from\nsparse timestamp labels. The generated dense framewise labels can then be used\nto train the segmentation model. In addition, we propose a framework for\nalternating learning of both the segmentation model and the graph convolutional\nmodel, which first initializes and then iteratively refines the learned models.\nDetailed experiments on four public datasets, including 50 Salads, GTEA,\nBreakfast, and Desktop Assembly, show that our method is superior to the\nmulti-layer perceptron baseline, while performing on par with or better than\nthe state of the art in temporal activity segmentation with timestamp\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Hamza Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1\">Sanjay Haresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Awais Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shakeeb Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1\">Andrey Konin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1\">M. Zeeshan Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVT-COV19D: Pyramid Vision Transformer for COVID-19 Diagnosis. (arXiv:2206.15069v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15069","description":"<p>With the outbreak of COVID-19, a large number of relevant studies have\nemerged in recent years. We propose an automatic COVID-19 diagnosis framework\nbased on lung CT scan images, the PVT-COV19D. In order to accommodate the\ndifferent dimensions of the image input, we first classified the images using\nTransformer models, then sampled the images in the dataset according to normal\ndistribution, and fed the sampling results into the modified PVTv2 model for\ntraining. A large number of experiments on the COV19-CT-DB dataset demonstrate\nthe effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_L/0/1/0/all/0/1\">Lilang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiaxuan Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaorun Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hanzhang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jiaxin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1\">Rui Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zhaoyan Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Custom Pretrainings and Adapted 3D-ConvNeXt Architecture for COVID Detection and Severity Prediction. (arXiv:2206.15073v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15073","description":"<p>Since COVID strongly affects the respiratory system, lung CT scans can be\nused for the analysis of a patients health. We introduce an neural network for\nthe prediction of the severity of lung damage and the detection of infection\nusing three-dimensional CT-scans. Therefore, we adapt the recent ConvNeXt model\nto process three-dimensional data. Furthermore, we introduce different\npretraining methods specifically adjusted to improve the models ability to\nhandle three-dimensional CT-data. In order to test the performance of our\nmodel, we participate in the 2nd COV19D Competition for severity prediction and\ninfection detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kienzle_D/0/1/0/all/0/1\">Daniel Kienzle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lorenz_J/0/1/0/all/0/1\">Julian Lorenz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schon_R/0/1/0/all/0/1\">Robin Sch&#xf6;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ludwig_K/0/1/0/all/0/1\">Katja Ludwig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lienhart_R/0/1/0/all/0/1\">Rainer Lienhart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Mask Calibration for Unified Domain Adaptive Panoptic Segmentation. (arXiv:2206.15083v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15083","description":"<p>Domain adaptive panoptic segmentation aims to mitigate data annotation\nchallenge by leveraging off-the-shelf annotated data in one or multiple related\nsource domains. However, existing studies employ two networks for instance\nsegmentation and semantic segmentation separately which lead to a large amount\nof network parameters with complicated and computationally intensive training\nand inference processes. We design UniDAPS, a Unified Domain Adaptive Panoptic\nSegmentation network that is simple but capable of achieving domain adaptive\ninstance segmentation and semantic segmentation simultaneously within a single\nnetwork. UniDAPS introduces Hierarchical Mask Calibration (HMC) that rectifies\nthe predicted pseudo masks, pseudo superpixels and pseudo pixels and performs\nnetwork re-training via an online self-training process on the fly. It has\nthree unique features: 1) it enables unified domain adaptive panoptic\nadaptation; 2) it mitigates false predictions and improves domain adaptive\npanoptic segmentation effectively; 3) it is end-to-end trainable with much less\nparameters and simpler training and inference pipeline. Extensive experiments\nover multiple public benchmarks show that UniDAPS achieves superior domain\nadaptive panoptic segmentation as compared with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-based Action Recognition via Adaptive Cross-Form Learning. (arXiv:2206.15085v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15085","description":"<p>Skeleton-based action recognition aims to project skeleton sequences to\naction categories, where skeleton sequences are derived from multiple forms of\npre-detected points. Compared with earlier methods that focus on exploring\nsingle-form skeletons via Graph Convolutional Networks (GCNs), existing methods\ntend to improve GCNs by leveraging multi-form skeletons due to their\ncomplementary cues. However, these methods (either adapting structure of GCNs\nor model ensemble) require the co-existence of all forms of skeletons during\nboth training and inference stages, while a typical situation in real life is\nthe existence of only partial forms for inference. To tackle this issue, we\npresent Adaptive Cross-Form Learning (ACFL), which empowers well-designed GCNs\nto generate complementary representation from single-form skeletons without\nchanging model capacity. Specifically, each GCN model in ACFL not only learns\naction representation from the single-form skeletons, but also adaptively\nmimics useful representations derived from other forms of skeletons. In this\nway, each GCN can learn how to strengthen what has been learned, thus\nexploiting model potential and facilitating action recognition as well.\nExtensive experiments conducted on three challenging benchmarks, i.e.,\nNTU-RGB+D 120, NTU-RGB+D 60 and UAV-Human, demonstrate the effectiveness and\ngeneralizability of the proposed method. Specifically, the ACFL significantly\nimproves various GCN models (i.e., CTR-GCN, MS-G3D, and Shift-GCN), achieving a\nnew record for skeleton-based action recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuanhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yan Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MKIoU Loss: Towards Accurate Oriented Object Detection in Aerial Images. (arXiv:2206.15109v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15109","description":"<p>Oriented bounding box regression is crucial for oriented object detection.\nHowever, regression-based methods often suffer from boundary problems and the\ninconsistency between loss and evaluation metrics. In this paper, a modulated\nKalman IoU loss of approximate SkewIoU is proposed, named MKIoU. To avoid\nboundary problems, we convert the oriented bounding box to Gaussian\ndistribution, then use the Kalman filter to approximate the intersection area.\nHowever, there exists significant difference between the calculated and actual\nintersection areas. Thus, we propose a modulation factor to adjust the\nsensitivity of angle deviation and width-height offset to loss variation,\nmaking the loss more consistent with the evaluation metric. Furthermore, the\nGaussian modeling method avoids the boundary problem but causes the angle\nconfusion of square objects simultaneously. Thus, the Gaussian Angle Loss (GA\nLoss) is presented to solve this problem by adding a corrected loss for square\ntargets. The proposed GA Loss can be easily extended to other Gaussian-based\nmethods. Experiments on three publicly available aerial image datasets, DOTA,\nUCAS-AOD, and HRSC2016, show the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and Recovering Adversarial Examples from Extracting Non-robust and Highly Predictive Adversarial Perturbations. (arXiv:2206.15128v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15128","description":"<p>Deep neural networks (DNNs) have been shown to be vulnerable against\nadversarial examples (AEs) which are maliciously designed to fool target\nmodels. The normal examples (NEs) added with imperceptible adversarial\nperturbation, can be a security threat to DNNs. Although the existing AEs\ndetection methods have achieved a high accuracy, they failed to exploit the\ninformation of the AEs detected. Thus, based on high-dimension perturbation\nextraction, we propose a model-free AEs detection method, the whole process of\nwhich is free from querying the victim model. Research shows that DNNs are\nsensitive to the high-dimension features. The adversarial perturbation hiding\nin the adversarial example belongs to the high-dimension feature which is\nhighly predictive and non-robust. DNNs learn more details from high-dimension\ndata than others. In our method, the perturbation extractor can extract the\nadversarial perturbation from AEs as high-dimension feature, then the trained\nAEs discriminator determines whether the input is an AE. Experimental results\nshow that the proposed method can not only detect the adversarial examples with\nhigh accuracy, but also detect the specific category of the AEs. Meanwhile, the\nextracted perturbation can be used to recover the AEs to NEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Diqun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jingxing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rangding Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InsMix: Towards Realistic Generative Data Augmentation for Nuclei Instance Segmentation. (arXiv:2206.15134v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15134","description":"<p>Nuclei Segmentation from histology images is a fundamental task in digital\npathology analysis. However, deep-learning-based nuclei segmentation methods\noften suffer from limited annotations. This paper proposes a realistic data\naugmentation method for nuclei segmentation, named InsMix, that follows a\nCopy-Paste-Smooth principle and performs morphology-constrained generative\ninstance augmentation. Specifically, we propose morphology constraints that\nenable the augmented images to acquire luxuriant information about nuclei while\nmaintaining their morphology characteristics (e.g., geometry and location). To\nfully exploit the pixel redundancy of the background and improve the model's\nrobustness, we further propose a background perturbation method, which randomly\nshuffles the background patches without disordering the original nuclei\ndistribution. To achieve contextual consistency between original and template\ninstances, a smooth-GAN is designed with a foreground similarity encoder (FSE)\nand a triplet loss. We validated the proposed method on two datasets, i.e.,\nKumar and CPS datasets. Experimental results demonstrate the effectiveness of\neach component and the superior performance achieved by our method to the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zeyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFGC 2022: The Second DeepFake Game Competition. (arXiv:2206.15138v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15138","description":"<p>This paper presents the summary report on our DFGC 2022 competition. The\nDeepFake is rapidly evolving, and realistic face-swaps are becoming more\ndeceptive and difficult to detect. On the contrary, methods for detecting\nDeepFakes are also improving. There is a two-party game between DeepFake\ncreators and defenders. This competition provides a common platform for\nbenchmarking the game between the current state-of-the-arts in DeepFake\ncreation and detection methods. The main research question to be answered by\nthis competition is the current state of the two adversaries when competed with\neach other. This is the second edition after the last year's DFGC 2021, with a\nnew, more diverse video dataset, a more realistic game setting, and more\nreasonable evaluation metrics. With this competition, we aim to stimulate\nresearch ideas for building better defenses against the DeepFake threats. We\nalso release our DFGC 2022 dataset contributed by both our participants and\nourselves to enrich the DeepFake data resources for the research community\n(https://github.com/NiCE-X/DFGC-2022).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoxGraph: Semantic Place Recognition and Pose Estimation from 3D LiDAR. (arXiv:2206.15154v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15154","description":"<p>This paper is about extremely robust and lightweight localisation using LiDAR\npoint clouds based on instance segmentation and graph matching. We model 3D\npoint clouds as fully-connected graphs of semantically identified components\nwhere each vertex corresponds to an object instance and encodes its shape.\nOptimal vertex association across graphs allows for full 6-Degree-of-Freedom\n(DoF) pose estimation and place recognition by measuring similarity. This\nrepresentation is very concise, condensing the size of maps by a factor of 25\nagainst the state-of-the-art, requiring only 3kB to represent a 1.4MB laser\nscan. We verify the efficacy of our system on the SemanticKITTI dataset, where\nwe achieve a new state-of-the-art in place recognition, with an average of\n88.4% recall at 100% precision where the next closest competitor follows with\n64.9%. We also show accurate metric pose estimation performance - estimating\n6-DoF pose with median errors of 10 cm and 0.33 deg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramatarov_G/0/1/0/all/0/1\">Georgi Pramatarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martini_D/0/1/0/all/0/1\">Daniele De Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadd_M/0/1/0/all/0/1\">Matthew Gadd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_P/0/1/0/all/0/1\">Paul Newman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection. (arXiv:2206.15157v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15157","description":"<p>Besides standard cameras, autonomous vehicles typically include multiple\nadditional sensors, such as lidars and radars, which help acquire richer\ninformation for perceiving the content of the driving scene. While several\nrecent works focus on fusing certain pairs of sensors - such as camera and\nlidar or camera and radar - by using architectural components specific to the\nexamined setting, a generic and modular sensor fusion architecture is missing\nfrom the literature. In this work, we focus on 2D object detection, a\nfundamental high-level task which is defined on the 2D image domain, and\npropose HRFuser, a multi-resolution sensor fusion architecture that scales\nstraightforwardly to an arbitrary number of input modalities. The design of\nHRFuser is based on state-of-the-art high-resolution networks for image-only\ndense prediction and incorporates a novel multi-window cross-attention block as\nthe means to perform fusion of multiple modalities at multiple resolutions.\nEven though cameras alone provide very informative features for 2D detection,\nwe demonstrate via extensive experiments on the nuScenes and Seeing Through Fog\ndatasets that our model effectively leverages complementary features from\nadditional modalities, substantially improving upon camera-only performance and\nconsistently outperforming state-of-the-art fusion methods for 2D detection\nboth in normal and adverse conditions. The source code will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Broedermann_T/0/1/0/all/0/1\">Tim Broedermann</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a> (1 and 3) ((1) ETH Zurich, (2) MPI for Informatics, (3) KU Leuven)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDAR-as-Camera for End-to-End Driving. (arXiv:2206.15170v1 [cs.AI])","link":"http://arxiv.org/abs/2206.15170","description":"<p>The core task of any autonomous driving system is to transform sensory inputs\ninto driving commands. In end-to-end driving, this is achieved via a neural\nnetwork, with one or multiple cameras as the most commonly used input and\nlow-level driving command, e.g. steering angle, as output. However,\ndepth-sensing has been shown in simulation to make the end-to-end driving task\neasier. On a real car, combining depth and visual information can be\nchallenging, due to the difficulty of obtaining good spatial and temporal\nalignment of the sensors. To alleviate alignment problems, Ouster LiDARs can\noutput surround-view LiDAR-images with depth, intensity, and ambient radiation\nchannels. These measurements originate from the same sensor, rendering them\nperfectly aligned in time and space. We demonstrate that such LiDAR-images are\nsufficient for the real-car road-following task and perform at least equally to\ncamera-based models in the tested conditions, with the difference increasing\nwhen needing to generalize to new weather conditions. In the second direction\nof study, we reveal that the temporal smoothness of off-policy prediction\nsequences correlates equally well with actual on-policy driving ability as the\ncommonly used mean absolute error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tampuu_A/0/1/0/all/0/1\">Ardi Tampuu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aidla_R/0/1/0/all/0/1\">Romet Aidla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gent_J/0/1/0/all/0/1\">Jan Are van Gent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matiisen_T/0/1/0/all/0/1\">Tambet Matiisen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Medical Image Fusion Method based on MDLatLRRv2. (arXiv:2206.15179v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15179","description":"<p>Since MDLatLRR only considers detailed parts (salient features) of input\nimages extracted by latent low-rank representation (LatLRR), it doesn't use\nbase parts (principal features) extracted by LatLRR effectively. Therefore, we\nproposed an improved multi-level decomposition method called MDLatLRRv2 which\neffectively analyzes and utilizes all the image features obtained by LatLRR.\nThen we apply MDLatLRRv2 to medical image fusion. The base parts are fused by\naverage strategy and the detail parts are fused by nuclear-norm operation. The\ncomparison with the existing methods demonstrates that the proposed method can\nachieve state-of-the-art fusion performance in objective and subjective\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_X/0/1/0/all/0/1\">Xu Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The (de)biasing effect of GAN-based augmentation methods on skin lesion images. (arXiv:2206.15182v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15182","description":"<p>New medical datasets are now more open to the public, allowing for better and\nmore extensive research. Although prepared with the utmost care, new datasets\nmight still be a source of spurious correlations that affect the learning\nprocess. Moreover, data collections are usually not large enough and are often\nunbalanced. One approach to alleviate the data imbalance is using data\naugmentation with Generative Adversarial Networks (GANs) to extend the dataset\nwith high-quality images. GANs are usually trained on the same biased datasets\nas the target data, resulting in more biased instances. This work explored\nunconditional and conditional GANs to compare their bias inheritance and how\nthe synthetic data influenced the models. We provided extensive manual data\nannotation of possibly biasing artifacts on the well-known ISIC dataset with\nskin lesions. In addition, we examined classification models trained on both\nreal and synthetic data with counterfactual bias explanations. Our experiments\nshowed that GANs inherited biases and sometimes even amplified them, leading to\neven stronger spurious correlations. Manual data annotation and synthetic\nimages are publicly available for reproducible scientific research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mikolajczyk_A/0/1/0/all/0/1\">Agnieszka Miko&#x142;ajczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majchrowska_S/0/1/0/all/0/1\">Sylwia Majchrowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Limeros_S/0/1/0/all/0/1\">Sandra Carrasco Limeros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-Distribution Detection for Long-tailed and Fine-grained Skin Lesion Images. (arXiv:2206.15186v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15186","description":"<p>Recent years have witnessed a rapid development of automated methods for skin\nlesion diagnosis and classification. Due to an increasing deployment of such\nsystems in clinics, it has become important to develop a more robust system\ntowards various Out-of-Distribution(OOD) samples (unknown skin lesions and\nconditions). However, the current deep learning models trained for skin lesion\nclassification tend to classify these OOD samples incorrectly into one of their\nlearned skin lesion categories. To address this issue, we propose a simple yet\nstrategic approach that improves the OOD detection performance while\nmaintaining the multi-class classification accuracy for the known categories of\nskin lesion. To specify, this approach is built upon a realistic scenario of a\nlong-tailed and fine-grained OOD detection task for skin lesion images. Through\nthis approach, 1) First, we target the mixup amongst middle and tail classes to\naddress the long-tail problem. 2) Later, we combine the above mixup strategy\nwith prototype learning to address the fine-grained nature of the dataset. The\nunique contribution of this paper is two-fold, justified by extensive\nexperiments. First, we present a realistic problem setting of OOD task for skin\nlesion. Second, we propose an approach to target the long-tailed and\nfine-grained aspects of the problem setting simultaneously to increase the OOD\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1\">Deval Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yaniv Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowling_A/0/1/0/all/0/1\">Adrian Bowling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonnington_P/0/1/0/all/0/1\">Paul Bonnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Granularity Regularized Re-Balancing for Class Incremental Learning. (arXiv:2206.15189v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15189","description":"<p>Deep learning models suffer from catastrophic forgetting when learning new\ntasks incrementally. Incremental learning has been proposed to retain the\nknowledge of old classes while learning to identify new classes. A typical\napproach is to use a few exemplars to avoid forgetting old knowledge. In such a\nscenario, data imbalance between old and new classes is a key issue that leads\nto performance degradation of the model. Several strategies have been designed\nto rectify the bias towards the new classes due to data imbalance. However,\nthey heavily rely on the assumptions of the bias relation between old and new\nclasses. Therefore, they are not suitable for complex real-world applications.\nIn this study, we propose an assumption-agnostic method, Multi-Granularity\nRegularized re-Balancing (MGRB), to address this problem. Re-balancing methods\nare used to alleviate the influence of data imbalance; however, we empirically\ndiscover that they would under-fit new classes. To this end, we further design\na novel multi-granularity regularization term that enables the model to\nconsider the correlations of classes in addition to re-balancing the data. A\nclass hierarchy is first constructed by grouping the semantically or visually\nsimilar classes. The multi-granularity regularization then transforms the\none-hot label vector into a continuous label distribution, which reflects the\nrelations between the target class and other classes based on the constructed\nclass hierarchy. Thus, the model can learn the inter-class relational\ninformation, which helps enhance the learning of both old and new classes.\nExperimental results on both public datasets and a real-world fault diagnosis\ndataset verify the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huitong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit U-Net for volumetric medical image segmentation. (arXiv:2206.15217v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15217","description":"<p>U-Net has been the go-to architecture for medical image segmentation tasks,\nhowever computational challenges arise when extending the U-Net architecture to\n3D images. We propose the Implicit U-Net architecture that adapts the efficient\nImplicit Representation paradigm to supervised image segmentation tasks. By\ncombining a convolutional feature extractor with an implicit localization\nnetwork, our implicit U-Net has 40% less parameters than the equivalent U-Net.\nMoreover, we propose training and inference procedures to capitalize sparse\npredictions. When comparing to an equivalent fully convolutional U-Net,\nImplicit U-Net reduces by approximately 30% inference and training time as well\nas training memory footprint while achieving comparable results in our\nexperiments with two different abdominal CT scan datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Marimont_S/0/1/0/all/0/1\">Sergio Naval Marimont</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarroni_G/0/1/0/all/0/1\">Giacomo Tarroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTrGAN: Cycle Transformers GAN for Gait Transfer. (arXiv:2206.15248v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15248","description":"<p>We attempt for the first time to address the problem of gait transfer. In\ncontrast to motion transfer, the objective here is not to imitate the source's\nnormal motions, but rather to transform the source's motion into a typical gait\npattern for the target. Using gait recognition models, we demonstrate that\nexisting techniques yield a discrepancy that can be easily detected. We\nintroduce a novel model, Cycle Transformers GAN (CTrGAN), that can successfully\ngenerate the target's natural gait. CTrGAN's generators consist of a decoder\nand encoder, both Transformers, where the attention is on the temporal domain\nbetween complete images rather than the spatial domain between patches. While\nrecent Transformer studies in computer vision mainly focused on discriminative\ntasks, we introduce an architecture that can be applied to synthesis tasks.\nUsing a widely-used gait recognition dataset, we demonstrate that our approach\nis capable of producing over an order of magnitude more realistic personalized\ngaits than existing methods, even when used with sources that were not\navailable during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahpod_S/0/1/0/all/0/1\">Shahar Mahpod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaash_N/0/1/0/all/0/1\">Noam Gaash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Artzi_G/0/1/0/all/0/1\">G. Ben-Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localizing the Recurrent Laryngeal Nerve via Ultrasound with a Bayesian Shape Framework. (arXiv:2206.15254v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15254","description":"<p>Tumor infiltration of the recurrent laryngeal nerve (RLN) is a\ncontraindication for robotic thyroidectomy and can be difficult to detect via\nstandard laryngoscopy. Ultrasound (US) is a viable alternative for RLN\ndetection due to its safety and ability to provide real-time feedback. However,\nthe tininess of the RLN, with a diameter typically less than 3mm, poses\nsignificant challenges to the accurate localization of the RLN. In this work,\nwe propose a knowledge-driven framework for RLN localization, mimicking the\nstandard approach surgeons take to identify the RLN according to its\nsurrounding organs. We construct a prior anatomical model based on the inherent\nrelative spatial relationships between organs. Through Bayesian shape alignment\n(BSA), we obtain the candidate coordinates of the center of a region of\ninterest (ROI) that encloses the RLN. The ROI allows a decreased field of view\nfor determining the refined centroid of the RLN using a dual-path\nidentification network, based on multi-scale semantic information. Experimental\nresults indicate that the proposed method achieves superior hit rates and\nsubstantially smaller distance errors compared with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1\">Luyi Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yushuang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mann_R/0/1/0/all/0/1\">Ritse Mann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yunzhi Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery. (arXiv:2206.15255v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15255","description":"<p>Reconstruction of the soft tissues in robotic surgery from endoscopic stereo\nvideos is important for many applications such as intra-operative navigation\nand image-guided robotic surgery automation. Previous works on this task mainly\nrely on SLAM-based approaches, which struggle to handle complex surgical\nscenes. Inspired by recent progress in neural rendering, we present a novel\nframework for deformable tissue reconstruction from binocular captures in\nrobotic surgery under the single-viewpoint setting. Our framework adopts\ndynamic neural radiance fields to represent deformable surgical scenes in MLPs\nand optimize shapes and deformations in a learning-based manner. In addition to\nnon-rigid deformations, tool occlusion and poor 3D clues from a single\nviewpoint are also particular challenges in soft tissue reconstruction. To\novercome these difficulties, we present a series of strategies of tool\nmask-guided ray casting, stereo depth-cueing ray marching and stereo\ndepth-supervised optimization. With experiments on DaVinci robotic surgery\nvideos, our method significantly outperforms the current state-of-the-art\nreconstruction method for handling various complex non-rigid deformations. To\nour best knowledge, this is the first work leveraging neural rendering for\nsurgical scene 3D reconstruction with remarkable potential demonstrated. Code\nis available at: https://github.com/med-air/EndoNeRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuehao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Siu Hin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera. (arXiv:2206.15258v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15258","description":"<p>We propose Neural-DynamicReconstruction (NDR), a template-free method to\nrecover high-fidelity geometry and motions of a dynamic scene from a monocular\nRGB-D camera. In NDR, we adopt the neural implicit function for surface\nrepresentation and rendering such that the captured color and depth can be\nfully utilized to jointly optimize the surface and deformations. To represent\nand constrain the non-rigid deformations, we propose a novel neural invertible\ndeforming network such that the cycle consistency between arbitrary two frames\nis automatically satisfied. Considering that the surface topology of dynamic\nscene might change over time, we employ a topology-aware strategy to construct\nthe topology-variant correspondence for the fused frames. NDR also further\nrefines the camera poses in a global optimization manner. Experiments on public\ndatasets and our collected dataset demonstrate that NDR outperforms existing\nmonocular dynamic reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongrui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wanquan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xuetao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Submission to Generic Event Boundary Detection Challenge@CVPR 2022: Local Context Modeling and Global Boundary Decoding Approach. (arXiv:2206.15268v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15268","description":"<p>Generic event boundary detection (GEBD) is an important yet challenging task\nin video understanding, which aims at detecting the moments where humans\nnaturally perceive event boundaries. In this paper, we present a local context\nmodeling and global boundary decoding approach for GEBD task. Local context\nmodeling sub-network is proposed to perceive diverse patterns of generic event\nboundaries, and it generates powerful video representations and reliable\nboundary confidence. Based on them, global boundary decoding sub-network is\nexploited to decode event boundaries from a global view. Our proposed method\nachieves 85.13% F1-score on Kinetics-GEBD testing set, which achieves a more\nthan 22% F1-score boost compared to the baseline method. The code is available\nat https://github.com/JackyTown/GEBD_Challenge_CVPR2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposing and addressing the fragility of neural networks in digital pathology. (arXiv:2206.15274v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15274","description":"<p>Neural networks have achieved impressive results in many medical imaging\ntasks but often perform substantially worse on out-of-distribution datasets\noriginating from different medical centres or patient cohorts. Evaluating this\nlack of ability to generalise and address the underlying problem are the two\nmain challenges in developing neural networks intended for clinical practice.\n</p>\n<p>In this study, we develop a new method for evaluating neural network models'\nability to generalise by generating a large number of distribution-shifted\ndatasets, which can be used to thoroughly investigate their robustness to\nvariability encountered in clinical practice. Compared to external validation,\n\\textit{shifted evaluation} can provide explanations for why neural networks\nfail on a given dataset, thus offering guidance on how to improve model\nrobustness. With shifted evaluation, we demonstrate that neural networks,\ntrained with state-of-the-art methods, are highly fragile to even small\ndistribution shifts from training data, and in some cases lose all\ndiscrimination ability.\n</p>\n<p>To address this fragility, we develop an augmentation strategy, explicitly\ndesigned to increase neural networks' robustness to distribution shifts.\n\\texttt{StrongAugment} is evaluated with large-scale, heterogeneous\nhistopathology data including five training datasets from two tissue types, 274\ndistribution-shifted datasets and 20 external datasets from four countries.\nNeural networks trained with \\texttt{StrongAugment} retain similar performance\non all datasets, even with distribution shifts where networks trained with\ncurrent state-of-the-art methods lose all discrimination ability. We recommend\nusing strong augmentation and shifted evaluation to train and evaluate all\nneural networks intended for clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pohjonen_J/0/1/0/all/0/1\">Joona Pohjonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sturenberg_C/0/1/0/all/0/1\">Carolin St&#xfc;renberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fohr_A/0/1/0/all/0/1\">Atte F&#xf6;hr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pitkanen_E/0/1/0/all/0/1\">Esa Pitk&#xe4;nen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rannikko_A/0/1/0/all/0/1\">Antti Rannikko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirtti_T/0/1/0/all/0/1\">Tuomas Mirtti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiclass-SGCN: Sparse Graph-based Trajectory Prediction with Agent Class Embedding. (arXiv:2206.15275v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15275","description":"<p>Trajectory prediction of road users in real-world scenarios is challenging\nbecause their movement patterns are stochastic and complex. Previous\npedestrian-oriented works have been successful in modelling the complex\ninteractions among pedestrians, but fail in predicting trajectories when other\ntypes of road users are involved (e.g., cars, cyclists, etc.), because they\nignore user types. Although a few recent works construct densely connected\ngraphs with user label information, they suffer from superfluous spatial\ninteractions and temporal dependencies. To address these issues, we propose\nMulticlass-SGCN, a sparse graph convolution network based approach for\nmulti-class trajectory prediction that takes into consideration velocity and\nagent label information and uses a novel interaction mask to adaptively decide\nthe spatial and temporal connections of agents based on their interaction\nscores. The proposed approach significantly outperformed state-of-the-art\napproaches on the Stanford Drone Dataset, providing more realistic and\nplausible trajectory predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruochen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsigiannis_S/0/1/0/all/0/1\">Stamos Katsigiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TINC: Temporally Informed Non-Contrastive Learning for Disease Progression Modeling in Retinal OCT Volumes. (arXiv:2206.15282v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15282","description":"<p>Recent contrastive learning methods achieved state-of-the-art in low label\nregimes. However, the training requires large batch sizes and heavy\naugmentations to create multiple views of an image. With non-contrastive\nmethods, the negatives are implicitly incorporated in the loss, allowing\ndifferent images and modalities as pairs. Although the meta-information (i.e.,\nage, sex) in medical imaging is abundant, the annotations are noisy and prone\nto class imbalance. In this work, we exploited already existing temporal\ninformation (different visits from a patient) in a longitudinal optical\ncoherence tomography (OCT) dataset using temporally informed non-contrastive\nloss (TINC) without increasing complexity and need for negative pairs.\nMoreover, our novel pair-forming scheme can avoid heavy augmentations and\nimplicitly incorporates the temporal information in the pairs. Finally, these\nrepresentations learned from the pretraining are more successful in predicting\ndisease progression where the temporal information is crucial for the\ndownstream task. More specifically, our model outperforms existing models in\npredicting the risk of conversion within a time frame from intermediate\nage-related macular degeneration (AMD) to the late wet-AMD stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Emre_T/0/1/0/all/0/1\">Taha Emre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarty_A/0/1/0/all/0/1\">Arunava Chakravarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivail_A/0/1/0/all/0/1\">Antoine Rivail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_S/0/1/0/all/0/1\">Sophie Riedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1\">Ursula Schmidt-Erfurth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-SuperFlow: Self-supervised Scene Flow Prediction in Stereo Sequences. (arXiv:2206.15296v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15296","description":"<p>In recent years, deep neural networks showed their exceeding capabilities in\naddressing many computer vision tasks including scene flow prediction. However,\nmost of the advances are dependent on the availability of a vast amount of\ndense per pixel ground truth annotations, which are very difficult to obtain\nfor real life scenarios. Therefore, synthetic data is often relied upon for\nsupervision, resulting in a representation gap between the training and test\ndata. Even though a great quantity of unlabeled real world data is available,\nthere is a huge lack in self-supervised methods for scene flow prediction.\nHence, we explore the extension of a self-supervised loss based on the Census\ntransform and occlusion-aware bidirectional displacements for the problem of\nscene flow prediction. Regarding the KITTI scene flow benchmark, our method\noutperforms the corresponding supervised pre-training of the same network and\nshows improved generalization capabilities while achieving much faster\nconvergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bendig_K/0/1/0/all/0/1\">Katharina Bendig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1\">Ren&#xe9; Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Anomaly Detection in Echocardiograms with Dynamic Variational Trajectory Models. (arXiv:2206.15316v1 [cs.LG])","link":"http://arxiv.org/abs/2206.15316","description":"<p>We propose a novel anomaly detection method for echocardiogram videos. The\nintroduced method takes advantage of the periodic nature of the heart cycle to\nlearn different variants of a variational latent trajectory model (TVAE). The\nmodels are trained on the healthy samples of an in-house dataset of infant\nechocardiogram videos consisting of multiple chamber views to learn a normative\nprior of the healthy population. During inference, maximum a posteriori (MAP)\nbased anomaly detection is performed to detect out-of-distribution samples in\nour dataset. The proposed method reliably identifies severe congenital heart\ndefects, such as Ebstein's Anomaly or Shonecomplex. Moreover, it achieves\nsuperior performance over MAP-based anomaly detection with standard variational\nautoencoders on the task of detecting pulmonary hypertension and right\nventricular dilation. Finally, we demonstrate that the proposed method provides\ninterpretable explanations of its output through heatmaps which highlight the\nregions corresponding to anomalous heart structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryser_A/0/1/0/all/0/1\">Alain Ryser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manduchi_L/0/1/0/all/0/1\">Laura Manduchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laumer_F/0/1/0/all/0/1\">Fabian Laumer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_H/0/1/0/all/0/1\">Holger Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1\">Sven Wellmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1\">Julia E. Vogt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis. (arXiv:2206.15328v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15328","description":"<p>The human annotations are imperfect, especially when produced by junior\npractitioners. Multi-expert consensus is usually regarded as golden standard,\nwhile this annotation protocol is too expensive to implement in many real-world\nprojects. In this study, we propose a method to refine human annotation, named\nNeural Annotation Refinement (NeAR). It is based on a learnable implicit\nfunction, which decodes a latent vector into represented shape. By integrating\nthe appearance as an input of implicit functions, the appearance-aware NeAR\nfixes the annotation artefacts. Our method is demonstrated on the application\nof adrenal gland analysis. We first show that the NeAR can repair distorted\ngolden standards on a public adrenal gland segmentation dataset. Besides, we\ndevelop a new Adrenal gLand ANalysis (ALAN) dataset with the proposed NeAR,\nwhere each case consists of a 3D shape of adrenal gland and its diagnosis label\n(normal vs. abnormal) assigned by experts. We show that models trained on the\nshapes repaired by the NeAR can diagnose adrenal glands better than the\noriginal ones. The ALAN dataset will be open-source, with 1,594 shapes for\nadrenal gland diagnosis, which serves as a new benchmark for medical shape\nanalysis. Code and dataset are available at https://github.com/M3DV/NeAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Rui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1\">Udaranga Wickramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qikui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Competitive Coding Approach for Palmprint Recognition: A Linear Discriminant Analysis Perspective. (arXiv:2206.15349v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15349","description":"<p>The competitive Coding approach (CompCode) is one of the most promising\nmethods for palmprint recognition. Due to its high performance and simple\nformulation, it has been continuously studied for many years. However, although\nnumerous variations of CompCode have been proposed, a detailed analysis of the\nmethod is still absent. In this paper, we provide a detailed analysis of\nCompCode from the perspective of linear discriminant analysis (LDA) for the\nfirst time. A non-trivial sufficient condition under which the CompCode is\noptimal in the sense of Fisher's criterion is presented. Based on our analysis,\nwe examined the statistics of palmprints and concluded that CompCode deviates\nfrom the optimal condition. To mitigate the deviation, we propose a new method\ncalled Class-Specific CompCode that improves CompCode by excluding\nnon-palm-line areas from matching. A nonlinear mapping of the competitive code\nis also applied in this method to further enhance accuracy. Experiments on two\npublic databases demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Lingfei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hua Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning to See: Towards New Foundations of Computer Vision. (arXiv:2206.15351v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15351","description":"<p>The remarkable progress in computer vision over the last few years is, by and\nlarge, attributed to deep learning, fueled by the availability of huge sets of\nlabeled data, and paired with the explosive growth of the GPU paradigm. While\nsubscribing to this view, this book criticizes the supposed scientific progress\nin the field and proposes the investigation of vision within the framework of\ninformation-based laws of nature. Specifically, the present work poses\nfundamental questions about vision that remain far from understood, leading the\nreader on a journey populated by novel challenges resonating with the\nfoundations of machine learning. The central thesis is that for a deeper\nunderstanding of visual computational processes, it is necessary to look beyond\nthe applications of general purpose machine learning algorithms and focus\ninstead on appropriate learning theories that take into account the\nspatiotemporal nature of the visual signal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betti_A/0/1/0/all/0/1\">Alessandro Betti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1\">Marco Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Underrepresented Classes from Decentralized Partially Labeled Medical Images. (arXiv:2206.15353v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15353","description":"<p>Using decentralized data for federated training is one promising emerging\nresearch direction for alleviating data scarcity in the medical domain.\nHowever, in contrast to large-scale fully labeled data commonly seen in general\nobject recognition tasks, the local medical datasets are more likely to only\nhave images annotated for a subset of classes of interest due to high\nannotation costs. In this paper, we consider a practical yet under-explored\nproblem, where underrepresented classes only have few labeled instances\navailable and only exist in a few clients of the federated system. We show that\nstandard federated learning approaches fail to learn robust multi-label\nclassifiers with extreme class imbalance and address it by proposing a novel\nfederated learning framework, FedFew. FedFew consists of three stages, where\nthe first stage leverages federated self-supervised learning to learn\nclass-agnostic representations. In the second stage, the decentralized\npartially labeled data are exploited to learn an energy-based multi-label\nclassifier for the common classes. Finally, the underrepresented classes are\ndetected based on the energy and a prototype-based nearest-neighbor model is\nproposed for few-shot matching. We evaluate FedFew on multi-label thoracic\ndisease classification tasks and demonstrate that it outperforms the federated\nbaselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voiculescu_I/0/1/0/all/0/1\">Irina Voiculescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Generalization of Supervised Models. (arXiv:2206.15369v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15369","description":"<p>We consider the problem of training a deep neural network on a given\nclassification task, e.g., ImageNet-1K (IN1K), so that it excels at that task\nas well as at other (future) transfer tasks. These two seemingly contradictory\nproperties impose a trade-off between improving the model's generalization\nwhile maintaining its performance on the original task. Models trained with\nself-supervised learning (SSL) tend to generalize better than their supervised\ncounterparts for transfer learning; yet, they still lag behind supervised\nmodels on IN1K. In this paper, we propose a supervised learning setup that\nleverages the best of both worlds. We enrich the common supervised training\nframework using two key components of recent SSL models: multi-scale crops for\ndata augmentation and the use of an expendable projector head. We replace the\nlast layer of class weights with class prototypes computed on the fly using a\nmemory bank. We show that these three improvements lead to a more favorable\ntrade-off between the IN1K training task and 13 transfer tasks. Over all the\nexplored configurations, we single out two models: t-ReX that achieves a new\nstate of the art for transfer learning and outperforms top methods such as DINO\nand PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on\nIN1K while performing better on transfer tasks. Project page and pretrained\nmodels: https://europe.naverlabs.com/t-rex\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sariyildiz_M/0/1/0/all/0/1\">Mert Bulent Sariyildiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantidis_Y/0/1/0/all/0/1\">Yannis Kalantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolarFormer: Multi-camera 3D Object Detection with Polar Transformer. (arXiv:2206.15398v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15398","description":"<p>3D object detection in autonomous driving aims to reason \"what\" and \"where\"\nthe objects of interest present in a 3D world. Following the conventional\nwisdom of previous 2D object detection, existing methods often adopt the\ncanonical Cartesian coordinate system with perpendicular axis. However, we\nconjugate that this does not fit the nature of the ego car's perspective, as\neach onboard camera perceives the world in shape of wedge intrinsic to the\nimaging geometry with radical (non-perpendicular) axis. Hence, in this paper we\nadvocate the exploitation of the Polar coordinate system and propose a new\nPolar Transformer (PolarFormer) for more accurate 3D object detection in the\nbird's-eye-view (BEV) taking as input only multi-camera 2D images.\nSpecifically, we design a cross attention based Polar detection head without\nrestriction to the shape of input structure to deal with irregular Polar grids.\nFor tackling the unconstrained object scale variations along Polar's distance\ndimension, we further introduce a multi-scalePolar representation learning\nstrategy. As a result, our model can make best use of the Polar representation\nrasterized via attending to the corresponding image observation in a\nsequence-to-sequence fashion subject to the geometric constraints. Thorough\nexperiments on the nuScenes dataset demonstrate that our PolarFormer\noutperforms significantly state-of-the-art 3D object detection alternatives, as\nwell as yielding competitive performance on BEV semantic segmentation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yanqin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhenwei Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEAD: A Multi-Armed Approach for Evaluation of Adversarial Examples Detectors. (arXiv:2206.15415v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15415","description":"<p>Detection of adversarial examples has been a hot topic in the last years due\nto its importance for safely deploying machine learning algorithms in critical\napplications. However, the detection methods are generally validated by\nassuming a single implicitly known attack strategy, which does not necessarily\naccount for real-life threats. Indeed, this can lead to an overoptimistic\nassessment of the detectors' performance and may induce some bias in the\ncomparison between competing detection schemes. We propose a novel multi-armed\nframework, called MEAD, for evaluating detectors based on several attack\nstrategies to overcome this limitation. Among them, we make use of three new\nobjectives to generate attacks. The proposed performance metric is based on the\nworst-case scenario: detection is successful if and only if all different\nattacks are correctly recognized. Empirically, we show the effectiveness of our\napproach. Moreover, the poor performance obtained for state-of-the-art\ndetectors opens a new exciting line of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Granese_F/0/1/0/all/0/1\">Federica Granese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picot_M/0/1/0/all/0/1\">Marine Picot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1\">Marco Romanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_F/0/1/0/all/0/1\">Francisco Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble CNN models for Covid-19 Recognition and Severity Perdition From 3D CT-scan. (arXiv:2206.15431v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15431","description":"<p>Since the appearance of Covid-19 in late 2019, Covid-19 has become an active\nresearch topic for the artificial intelligence (AI) community. One of the most\ninteresting AI topics is Covid-19 analysis of medical imaging. CT-scan imaging\nis the most informative tool about this disease. This work is part of the 2nd\nCOV19D competition, where two challenges are set: Covid-19 Detection and\nCovid-19 Severity Detection from the CT-scans. For Covid-19 detection from\nCT-scans, we proposed an ensemble of 2D Convolution blocks with Densenet-161\nmodels. Here, each 2D convolutional block with Densenet-161 architecture is\ntrained separately and in testing phase, the ensemble model is based on the\naverage of their probabilities. On the other hand, we proposed an ensemble of\nConvolutional Layers with Inception models for Covid-19 severity detection. In\naddition to the Convolutional Layers, three Inception variants were used,\nnamely Inception-v3, Inception-v4 and Inception-Resnet. Our proposed approaches\noutperformed the baseline approach in the validation data of the 2nd COV19D\ncompetition by 11% and 16% for Covid-19 detection and Covid-19 severity\ndetection, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bougourzi_F/0/1/0/all/0/1\">Fares Bougourzi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Distante_C/0/1/0/all/0/1\">Cosimo Distante</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dornaika_F/0/1/0/all/0/1\">Fadi Dornaika</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taleb_Ahmed_A/0/1/0/all/0/1\">Abdelmalik Taleb-Ahmed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset. (arXiv:2206.15436v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15436","description":"<p>6D object pose estimation is one of the fundamental problems in computer\nvision and robotics research. While a lot of recent efforts have been made on\ngeneralizing pose estimation to novel object instances within the same\ncategory, namely category-level 6D pose estimation, it is still restricted in\nconstrained environments given the limited number of annotated data. In this\npaper, we collect Wild6D, a new unlabeled RGBD object video dataset with\ndiverse instances and backgrounds. We utilize this data to generalize\ncategory-level 6D object pose estimation in the wild with semi-supervised\nlearning. We propose a new model, called Rendering for Pose estimation network\nRePoNet, that is jointly trained using the free ground-truths with the\nsynthetic data, and a silhouette matching objective function on the real-world\ndata. Without using any 3D annotations on real data, our method outperforms\nstate-of-the-art methods on the previous dataset and our Wild6D test set (with\nmanual annotations for evaluation) by a large margin. Project page with Wild6D\ndata: https://oasisyang.github.io/semi-pose .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetry Disentanglement Network for Interpretable Acute Ischemic Stroke Infarct Segmentation in Non-Contrast CT Scans. (arXiv:2206.15445v1 [eess.IV])","link":"http://arxiv.org/abs/2206.15445","description":"<p>Accurate infarct segmentation in non-contrast CT (NCCT) images is a crucial\nstep toward computer-aided acute ischemic stroke (AIS) assessment. In clinical\npractice, bilateral symmetric comparison of brain hemispheres is usually used\nto locate pathological abnormalities. Recent research has explored asymmetries\nto assist with AIS segmentation. However, most previous symmetry-based work\nmixed different types of asymmetries when evaluating their contribution to AIS.\nIn this paper, we propose a novel Asymmetry Disentanglement Network (ADN) to\nautomatically separate pathological asymmetries and intrinsic anatomical\nasymmetries in NCCTs for more effective and interpretable AIS segmentation. ADN\nfirst performs asymmetry disentanglement based on input NCCTs, which produces\ndifferent types of 3D asymmetry maps. Then a synthetic,\nintrinsic-asymmetry-compensated and pathology-asymmetry-salient NCCT volume is\ngenerated and later used as input to a segmentation network. The training of\nADN incorporates domain knowledge and adopts a tissue-type aware regularization\nloss function to encourage clinically-meaningful pathological asymmetry\nextraction. Coupled with an unsupervised 3D transformation network, ADN\nachieves state-of-the-art AIS segmentation performance on a public NCCT\ndataset. In addition to the superior performance, we believe the learned\nclinically-interpretable asymmetry maps can also provide insights towards a\nbetter understanding of AIS assessment. Our code is available at\nhttps://github.com/nihaomiao/MICCAI22_ADN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ni_H/0/1/0/all/0/1\">Haomiao Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1\">Yuan Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1\">Kelvin Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Volpi_J/0/1/0/all/0/1\">John Volpi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_S/0/1/0/all/0/1\">Stephen T.C. Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15462","description":"<p>We propose a margin-based loss for vision-language model pretraining that\nencourages gradient-based explanations that are consistent with region-level\nannotations. We refer to this objective as Attention Mask Consistency (AMC) and\ndemonstrate that it produces superior visual grounding performance compared to\nmodels that rely instead on region-level annotations for explicitly training an\nobject detector such as Faster R-CNN. AMC works by encouraging gradient-based\nexplanation masks that focus their attention scores mostly within annotated\nregions of interest for images that contain such annotations. Particularly, a\nmodel trained with AMC on top of standard vision-language modeling objectives\nobtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding\nbenchmark, an absolute improvement of 5.48% when compared to the best previous\nmodel. Our approach also performs exceedingly well on established benchmarks\nfor referring expression comprehension and offers the added benefit by design\nof gradient-based explanations that better align with human annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1\">Kushal Kafle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roman_V/0/1/0/all/0/1\">Vicente Ord&#xf3;&#xf1;ez Rom&#xe1;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch and Match: Supercharging Imitation with Regularized Optimal Transport. (arXiv:2206.15469v1 [cs.RO])","link":"http://arxiv.org/abs/2206.15469","description":"<p>Imitation learning holds tremendous promise in learning policies efficiently\nfor complex decision making problems. Current state-of-the-art algorithms often\nuse inverse reinforcement learning (IRL), where given a set of expert\ndemonstrations, an agent alternatively infers a reward function and the\nassociated optimal policy. However, such IRL approaches often require\nsubstantial online interactions for complex control problems. In this work, we\npresent Regularized Optimal Transport (ROT), a new imitation learning algorithm\nthat builds on recent advances in optimal transport based trajectory-matching.\nOur key technical insight is that adaptively combining trajectory-matching\nrewards with behavior cloning can significantly accelerate imitation even with\nonly a few demonstrations. Our experiments on 20 visual control tasks across\nthe DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World\nBenchmark demonstrate an average of 7.8X faster imitation to reach 90% of\nexpert performance compared to prior state-of-the-art methods. On real-world\nrobotic manipulation, with just one demonstration and an hour of online\ntraining, ROT achieves an average success rate of 90.1% across 14 tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haldar_S/0/1/0/all/0/1\">Siddhant Haldar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_V/0/1/0/all/0/1\">Vaibhav Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarats_D/0/1/0/all/0/1\">Denis Yarats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1\">Lerrel Pinto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dressing Avatars: Deep Photorealistic Appearance for Physically Simulated Clothing. (arXiv:2206.15470v1 [cs.GR])","link":"http://arxiv.org/abs/2206.15470","description":"<p>Despite recent progress in developing animatable full-body avatars, realistic\nmodeling of clothing - one of the core aspects of human self-expression -\nremains an open challenge. State-of-the-art physical simulation methods can\ngenerate realistically behaving clothing geometry at interactive rate. Modeling\nphotorealistic appearance, however, usually requires physically-based rendering\nwhich is too expensive for interactive applications. On the other hand,\ndata-driven deep appearance models are capable of efficiently producing\nrealistic appearance, but struggle at synthesizing geometry of highly dynamic\nclothing and handling challenging body-clothing configurations. To this end, we\nintroduce pose-driven avatars with explicit modeling of clothing that exhibit\nboth realistic clothing dynamics and photorealistic appearance learned from\nreal-world data. The key idea is to introduce a neural clothing appearance\nmodel that operates on top of explicit geometry: at train time we use\nhigh-fidelity tracking, whereas at animation time we rely on physically\nsimulated geometry. Our key contribution is a physically-inspired appearance\nnetwork, capable of generating photorealistic appearance with view-dependent\nand dynamic shadowing effects even for unseen body-clothing configurations. We\nconduct a thorough evaluation of our model and demonstrate diverse animation\nresults on several subjects and different types of clothing. Unlike previous\nwork on photorealistic full-body avatars, our approach can produce much richer\ndynamics and more realistic deformations even for loose clothing. We also\ndemonstrate that our formulation naturally allows clothing to be used with\navatars of different people while staying fully animatable, thus enabling, for\nthe first time, photorealistic avatars with novel clothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Donglai Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuyck_T/0/1/0/all/0/1\">Tuur Stuyck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_J/0/1/0/all/0/1\">Javier Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_S/0/1/0/all/0/1\">Shunsuke Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jingfan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1\">Breannan Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgins_J/0/1/0/all/0/1\">Jessica Hodgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-Device Training Under 256KB Memory. (arXiv:2206.15472v1 [cs.CV])","link":"http://arxiv.org/abs/2206.15472","description":"<p>On-device training enables the model to adapt to new data collected from the\nsensors by fine-tuning a pre-trained model. However, the training memory\nconsumption is prohibitive for IoT devices that have tiny memory resources. We\npropose an algorithm-system co-design framework to make on-device training\npossible with only 256KB of memory. On-device training faces two unique\nchallenges: (1) the quantized graphs of neural networks are hard to optimize\ndue to mixed bit-precision and the lack of normalization; (2) the limited\nhardware resource (memory and computation) does not allow full backward\ncomputation. To cope with the optimization difficulty, we propose\nQuantization-Aware Scaling to calibrate the gradient scales and stabilize\nquantized training. To reduce the memory footprint, we propose Sparse Update to\nskip the gradient computation of less important layers and sub-tensors. The\nalgorithm innovation is implemented by a lightweight training system, Tiny\nTraining Engine, which prunes the backward computation graph to support sparse\nupdates and offloads the runtime auto-differentiation to compile time. Our\nframework is the first practical solution for on-device transfer learning of\nvisual recognition on tiny IoT devices (e.g., a microcontroller with only 256KB\nSRAM), using less than 1/100 of the memory of existing frameworks while\nmatching the accuracy of cloud training+edge deployment for the tinyML\napplication VWW. Our study enables IoT devices to not only perform inference\nbut also continuously adapt to new data for on-device lifelong learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Ji Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Ligeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei-Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A DCNN-based Arbitrarily-Oriented Object Detector for Quality Control and Inspection Application. (arXiv:2101.07383v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07383","description":"<p>Following the success of machine vision systems for on-line automated quality\ncontrol and inspection processes, an object recognition solution is presented\nin this work for two different specific applications, i.e., the detection of\nquality control items in surgery toolboxes prepared for sterilizing in a\nhospital, as well as the detection of defects in vessel hulls to prevent\npotential structural failures. The solution has two stages. First, a feature\npyramid architecture based on Single Shot MultiBox Detector (SSD) is used to\nimprove the detection performance, and a statistical analysis based on ground\ntruth is employed to select parameters of a range of default boxes. Second, a\nlightweight neural network is exploited to achieve oriented detection results\nusing a regression method. The first stage of the proposed method is capable of\ndetecting the small targets considered in the two scenarios. In the second\nstage, despite the simplicity, it is efficient to detect elongated targets\nwhile maintaining high running efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Alberto Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonnin_Pascual_F/0/1/0/all/0/1\">Francisco Bonnin-Pascual</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimPLE: Similar Pseudo Label Exploitation for Semi-Supervised Classification. (arXiv:2103.16725v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16725","description":"<p>A common classification task situation is where one has a large amount of\ndata available for training, but only a small portion is annotated with class\nlabels. The goal of semi-supervised training, in this context, is to improve\nclassification accuracy by leverage information not only from labeled data but\nalso from a large amount of unlabeled data. Recent works have developed\nsignificant improvements by exploring the consistency constrain between\ndifferently augmented labeled and unlabeled data. Following this path, we\npropose a novel unsupervised objective that focuses on the less studied\nrelationship between the high confidence unlabeled data that are similar to\neach other. The new proposed Pair Loss minimizes the statistical distance\nbetween high confidence pseudo labels with similarity above a certain\nthreshold. Combining the Pair Loss with the techniques developed by the\nMixMatch family, our proposed SimPLE algorithm shows significant performance\ngains over previous algorithms on CIFAR-100 and Mini-ImageNet, and is on par\nwith the state-of-the-art methods on CIFAR-10 and SVHN. Furthermore, SimPLE\nalso outperforms the state-of-the-art methods in the transfer learning setting,\nwhere models are initialized by the weights pre-trained on ImageNet or\nDomainNet-Real. The code is available at github.com/zijian-hu/SimPLE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zijian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuefeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nevatia_R/0/1/0/all/0/1\">Ram Nevatia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVT v2: Improved Baselines with Pyramid Vision Transformer. (arXiv:2106.13797v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13797","description":"<p>Transformer recently has presented encouraging progress in computer vision.\nIn this work, we present new baselines by improving the original Pyramid Vision\nTransformer (PVT v1) by adding three designs, including (1) linear complexity\nattention layer, (2) overlapping patch embedding, and (3) convolutional\nfeed-forward network. With these modifications, PVT v2 reduces the\ncomputational complexity of PVT v1 to linear and achieves significant\nimprovements on fundamental vision tasks such as classification, detection, and\nsegmentation. Notably, the proposed PVT v2 achieves comparable or better\nperformances than recent works such as Swin Transformer. We hope this work will\nfacilitate state-of-the-art Transformer researches in computer vision. Code is\navailable at https://github.com/whai362/PVT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised domain adaptation for clinician pose estimation and instance segmentation in the operating room. (arXiv:2108.11801v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11801","description":"<p>The fine-grained localization of clinicians in the operating room (OR) is a\nkey component to design the new generation of OR support systems. Computer\nvision models for person pixel-based segmentation and body-keypoints detection\nare needed to better understand the clinical activities and the spatial layout\nof the OR. This is challenging, not only because OR images are very different\nfrom traditional vision datasets, but also because data and annotations are\nhard to collect and generate in the OR due to privacy concerns. To address\nthese concerns, we first study how joint person pose estimation and instance\nsegmentation can be performed on low resolutions images with downsampling\nfactors from 1x to 12x. Second, to address the domain shift and the lack of\nannotations, we propose a novel unsupervised domain adaptation method, called\nAdaptOR, to adapt a model from an in-the-wild labeled source domain to a\nstatistically different unlabeled target domain. We propose to exploit explicit\ngeometric constraints on the different augmentations of the unlabeled target\ndomain image to generate accurate pseudo labels and use these pseudo labels to\ntrain the model on high- and low-resolution OR images in a self-training\nframework. Furthermore, we propose disentangled feature normalization to handle\nthe statistically different source and target domain data. Extensive\nexperimental results with detailed ablation studies on the two OR datasets\nMVOR+ and TUM-OR-test show the effectiveness of our approach against strongly\nconstructed baselines, especially on the low-resolution privacy-preserving OR\nimages. Finally, we show the generality of our method as a semi-supervised\nlearning (SSL) method on the large-scale COCO dataset, where we achieve\ncomparable results with as few as 1% of labeled supervision against a model\ntrained with 100% labeled supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vinkle Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangi_A/0/1/0/all/0/1\">Afshin Gangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAReN: A Collaborative Approach Towards Reasoning And Disentangling. (arXiv:2109.13156v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13156","description":"<p>Computational learning approaches to solving visual reasoning tests, such as\nRaven's Progressive Matrices (RPM), critically depend on the ability to\nidentify the visual concepts used in the test (i.e., the representation) as\nwell as the latent rules based on those concepts (i.e., the reasoning).\nHowever, learning of representation and reasoning is a challenging and\nill-posed task, often approached in a stage-wise manner (first representation,\nthen reasoning). In this work, we propose an end-to-end joint\nrepresentation-reasoning learning framework, which leverages a weak form of\ninductive bias to improve both tasks together. Specifically, we introduce a\ngeneral generative graphical model for RPMs, GM-RPM, and apply it to solve the\nreasoning test. We accomplish this using a novel learning framework\nDisentangling based Abstract Reasoning Network (DAReN) based on the principles\nof GM-RPM. We perform an empirical evaluation of DAReN over several benchmark\ndatasets. DAReN shows consistent improvement over state-of-the-art (SOTA)\nmodels on both the reasoning and the disentanglement tasks. This demonstrates\nthe strong correlation between disentangled latent representation and the\nability to solve abstract visual reasoning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1\">Pritish Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basioti_K/0/1/0/all/0/1\">Kalliopi Basioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Fusion Prior for Multi-Focus Image Super Resolution Fusion. (arXiv:2110.05706v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05706","description":"<p>Multi-focus image fusion (MFIF) and super-resolution (SR) are the inverse\nproblem of imaging model, purposes of MFIF and SR are obtaining all-in-focus\nand high-resolution 2D mapping of targets. Though various MFIF and SR methods\nhave been designed; almost all the them deal with MFIF and SR separately. This\npaper unifies MFIF and SR problems in the physical perspective as the\nmulti-focus image super resolution fusion (MFISRF), and we propose a novel\nunified dataset-free unsupervised framework named deep fusion prior (DFP)\nbased-on deep image prior (DIP) to address such MFISRF with single model.\nExperiments have proved that our proposed DFP approaches or even outperforms\nthose state-of-art MFIF and SR method combinations. To our best knowledge, our\nproposed work is a dataset-free unsupervised method to simultaneously implement\nthe multi-focus fusion and super-resolution task for the first time.\nAdditionally, DFP is a general framework, thus its networks and focus\nmeasurement tactics can be continuously updated to further improve the MFISRF\nperformance. DFP codes are open source available at\n<a href=\"http://github.com/GuYuanjie/DeepFusionPrior.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuanjie Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhibo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hailun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shouyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning. (arXiv:2110.11395v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11395","description":"<p>Pruning neural networks reduces inference time and memory costs. On standard\nhardware, these benefits will be especially prominent if coarse-grained\nstructures, like feature maps, are pruned. We devise two novel saliency-based\nmethods for second-order structured pruning (SOSP) which include correlations\namong all structures and layers. Our main method SOSP-H employs an innovative\nsecond-order approximation, which enables saliency evaluations by fast\nHessian-vector products. SOSP-H thereby scales like a first-order method\ndespite taking into account the full Hessian. We validate SOSP-H by comparing\nit to our second method SOSP-I that uses a well-established Hessian\napproximation, and to numerous state-of-the-art methods. While SOSP-H performs\non par or better in terms of accuracy, it has clear advantages in terms of\nscalability and efficiency. This allowed us to scale SOSP-H to large-scale\nvision tasks, even though it captures correlations across all layers of the\nnetwork. To underscore the global nature of our pruning methods, we evaluate\ntheir performance not only by removing structures from a pretrained network,\nbut also by detecting architectural bottlenecks. We show that our algorithms\nallow to systematically reveal architectural bottlenecks, which we then remove\nto further increase the accuracy of the networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nonnenmacher_M/0/1/0/all/0/1\">Manuel Nonnenmacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeil_T/0/1/0/all/0/1\">Thomas Pfeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinwart_I/0/1/0/all/0/1\">Ingo Steinwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reeb_D/0/1/0/all/0/1\">David Reeb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits. (arXiv:2112.11014v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11014","description":"<p>We obtain a personal signature of a person's learning progress in a\nself-neuromodulation task, guided by functional MRI (fMRI). The signature is\nbased on predicting the activity of the Amygdala in a second neurofeedback\nsession, given a similar fMRI-derived brain state in the first session. The\nprediction is made by a deep neural network, which is trained on the entire\ntraining cohort of patients. This signal, which is indicative of a person's\nprogress in performing the task of Amygdala modulation, is aggregated across\nmultiple prototypical brain states and then classified by a linear classifier\nto various personal and clinical indications. The predictive power of the\nobtained signature is stronger than previous approaches for obtaining a\npersonal signature from fMRI neurofeedback and provides an indication that a\nperson's learning pattern may be used as a diagnostic tool. Our code has been\nmade available, and data would be shared, subject to ethical approvals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leibovitz_R/0/1/0/all/0/1\">Rotem Leibovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osin_J/0/1/0/all/0/1\">Jhonathan Osin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevitch_G/0/1/0/all/0/1\">Guy Gurevitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_T/0/1/0/all/0/1\">Talma Hendler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equalized Focal Loss for Dense Long-Tailed Object Detection. (arXiv:2201.02593v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02593","description":"<p>Despite the recent success of long-tailed object detection, almost all\nlong-tailed object detectors are developed based on the two-stage paradigm. In\npractice, one-stage detectors are more prevalent in the industry because they\nhave a simple and fast pipeline that is easy to deploy. However, in the\nlong-tailed scenario, this line of work has not been explored so far. In this\npaper, we investigate whether one-stage detectors can perform well in this\ncase. We discover the primary obstacle that prevents one-stage detectors from\nachieving excellent performance is: categories suffer from different degrees of\npositive-negative imbalance problems under the long-tailed data distribution.\nThe conventional focal loss balances the training process with the same\nmodulating factor for all categories, thus failing to handle the long-tailed\nproblem. To address this issue, we propose the Equalized Focal Loss (EFL) that\nrebalances the loss contribution of positive and negative samples of different\ncategories independently according to their imbalance degrees. Specifically,\nEFL adopts a category-relevant modulating factor which can be adjusted\ndynamically by the training status of different categories. Extensive\nexperiments conducted on the challenging LVIS v1 benchmark demonstrate the\neffectiveness of our proposed method. With an end-to-end training pipeline, EFL\nachieves 29.2% in terms of overall AP and obtains significant performance\nimprovements on rare categories, surpassing all existing state-of-the-art\nmethods. The code is available at https://github.com/ModelTC/EOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yongqiang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jingru Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianwei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Ye Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Pretraining for Echocardiography Segmentation with Limited Data. (arXiv:2201.07219v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.07219","description":"<p>Contrastive learning has proven useful in many applications where access to\nlabelled data is limited. The lack of annotated data is particularly\nproblematic in medical image segmentation as it is difficult to have clinical\nexperts manually annotate large volumes of data such as cardiac structures in\nultrasound images of the heart. In this paper, we argue whether or not\ncontrastive pretraining is helpful for the segmentation of the left ventricle\nin echocardiography images. Furthermore, we study the effect of contrastive\npretraining on two well-known segmentation networks, UNet and DeepLabV3. Our\nresults show that contrastive pretraining helps improve the performance on left\nventricle segmentation, particularly when annotated data is scarce. We show how\nto achieve comparable results to state-of-the-art fully supervised algorithms\nwhen we train our models in a self-supervised fashion followed by fine-tuning\non just 5\\% of the data. We show that our solution outperforms what is\ncurrently published on a large public dataset (EchoNet-Dynamic) achieving a\nDice score of 0.9211. We also compare the performance of our solution on\nanother smaller dataset (CAMUS) to demonstrate the generalizability of our\nproposed solution. The code is available at\n(https://github.com/BioMedIA-MBZUAI/contrastive-echo).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saeed_M/0/1/0/all/0/1\">Mohamed Saeed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muhtaseb_R/0/1/0/all/0/1\">Rand Muhtaseb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaqub_M/0/1/0/all/0/1\">Mohammad Yaqub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification. (arXiv:2202.02832v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02832","description":"<p>Convolutional Neural Networks have demonstrated human-level performance in\nthe classification of melanoma and other skin lesions, but evident performance\ndisparities between differing skin tones should be addressed before widespread\ndeployment. In this work, we propose an efficient yet effective algorithm for\nautomatically labelling the skin tone of lesion images, and use this to\nannotate the benchmark ISIC dataset. We subsequently use these automated labels\nas the target for two leading bias unlearning techniques towards mitigating\nskin tone bias. Our experimental results provide evidence that our skin tone\ndetection algorithm outperforms existing solutions and that unlearning skin\ntone improves generalisation and can reduce the performance disparity between\nmelanoma detection in lighter and darker skin tones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ab-initio Contrast Estimation and Denoising of Cryo-EM Images. (arXiv:2202.07737v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07737","description":"<p>Background and Objective: The contrast of cryo-EM images varies from one to\nanother, primarily due to the uneven thickness of the ice layer. This contrast\nvariation can affect the quality of 2-D class averaging, 3-D ab-initio\nmodeling, and 3-D heterogeneity analysis. Contrast estimation is currently\nperformed during 3-D iterative refinement. As a result, the estimates are not\navailable at the earlier computational stages of class averaging and ab-initio\nmodeling. This paper aims to solve the contrast estimation problem directly\nfrom the picked particle images in the ab-initio stage, without estimating the\n3-D volume, image rotations, or class averages.\n</p>\n<p>Methods: The key observation underlying our analysis is that the 2-D\ncovariance matrix of the raw images is related to the covariance of the\nunderlying clean images, the noise variance, and the contrast variability\nbetween images. We show that the contrast variability can be derived from the\n2-D covariance matrix and we apply the existing Covariance Wiener Filtering\n(CWF) framework to estimate it. We also demonstrate a modification of CWF to\nestimate the contrast of individual images.\n</p>\n<p>Results: Our method improves the contrast estimation by a large margin,\ncompared to the previous CWF method. Its estimation accuracy is often\ncomparable to that of an oracle that knows the ground truth covariance of the\nclean images. The more accurate contrast estimation also improves the quality\nof image restoration as demonstrated in both synthetic and experimental\ndatasets.\n</p>\n<p>Conclusions: This paper proposes an effective method for contrast estimation\ndirectly from noisy images without using any 3-D volume information. It enables\ncontrast correction in the earlier stage of single particle analysis, and may\nimprove the accuracy of downstream processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yunpeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_A/0/1/0/all/0/1\">Amit Singer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-Attention to Textures: Hierarchical Hourglass Vision Transformer for Universal Texture Synthesis. (arXiv:2202.11703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11703","description":"<p>We present a novel U-Attention vision Transformer for universal texture\nsynthesis. We exploit the natural long-range dependencies enabled by the\nattention mechanism to allow our approach to synthesize diverse textures while\npreserving their structures in a single inference. We propose a hierarchical\nhourglass backbone that attends to the global structure and performs patch\nmapping at varying scales in a coarse-to-fine-to-coarse stream. Completed by\nskip connection and convolution designs that propagate and fuse information at\ndifferent scales, our hierarchical U-Attention architecture unifies attention\nto features from macro structures to micro details, and progressively refines\nsynthesis results at successive stages. Our method achieves stronger 2$\\times$\nsynthesis than previous work on both stochastic and structured textures while\ngeneralizing to unseen textures without fine-tuning. Ablation studies\ndemonstrate the effectiveness of each component of our architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shouchang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1\">Valentin Deschaintre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noll_D/0/1/0/all/0/1\">Douglas Noll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roullier_A/0/1/0/all/0/1\">Arthur Roullier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Disentangled Generative Adversarial Network for Zero-Shot Sketch-Based 3D Shape Retrieval. (arXiv:2202.11948v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11948","description":"<p>Sketch-based 3D shape retrieval is a challenging task due to the large domain\ndiscrepancy between sketches and 3D shapes. Since existing methods are trained\nand evaluated on the same categories, they cannot effectively recognize the\ncategories that have not been used during training. In this paper, we propose a\nnovel domain disentangled generative adversarial network (DD-GAN) for zero-shot\nsketch-based 3D retrieval, which can retrieve the unseen categories that are\nnot accessed during training. Specifically, we first generate domain-invariant\nfeatures and domain-specific features by disentangling the learned features of\nsketches and 3D shapes, where the domain-invariant features are used to align\nwith the corresponding word embeddings. Then, we develop a generative\nadversarial network that combines the domain-specific features of the seen\ncategories with the aligned domain-invariant features to synthesize samples,\nwhere the synthesized samples of the unseen categories are generated by using\nthe corresponding word embeddings. Finally, we use the synthesized samples of\nthe unseen categories combined with the real samples of the seen categories to\ntrain the network for retrieval, so that the unseen categories can be\nrecognized. In order to reduce the domain shift problem, we utilized unlabeled\nunseen samples to enhance the discrimination ability of the discriminator. With\nthe discriminator distinguishing the generated samples from the unlabeled\nunseen samples, the generator can generate more realistic unseen samples.\nExtensive experiments on the SHREC'13 and SHREC'14 datasets show that our\nmethod significantly improves the retrieval performance of the unseen\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Rui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zongyan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Le Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jianjun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jin Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction. (arXiv:2203.01703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01703","description":"<p>Reconstructing 3D objects from 2D images is both challenging for our brains\nand machine learning algorithms. To support this spatial reasoning task,\ncontextual information about the overall shape of an object is critical.\nHowever, such information is not captured by established loss terms (e.g. Dice\nloss). We propose to complement geometrical shape information by including\nmulti-scale topological features, such as connected components, cycles, and\nvoids, in the reconstruction loss. Our method uses cubical complexes to\ncalculate topological features of 3D volume data and employs an optimal\ntransport distance to guide the reconstruction process. This topology-aware\nloss is fully differentiable, computationally efficient, and can be added to\nany neural network. We demonstrate the utility of our loss by incorporating it\ninto SHAPR, a model for predicting the 3D cell shape of individual cells based\non 2D microscopy images. Using a hybrid loss that leverages both geometrical\nand topological information of single objects to assess their shape, we find\nthat topological information substantially improves the quality of\nreconstructions, thus highlighting its ability to extract more relevant\nfeatures from image datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waibel_D/0/1/0/all/0/1\">Dominik J. E. Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atwell_S/0/1/0/all/0/1\">Scott Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_M/0/1/0/all/0/1\">Matthias Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1\">Carsten Marr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1\">Bastian Rieck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.04592","description":"<p>Benchmarks are crucial to measuring and steering progress in artificial\nintelligence (AI). However, recent studies raised concerns over the state of AI\nbenchmarking, reporting issues such as benchmark overfitting, benchmark\nsaturation and increasing centralization of benchmark dataset creation. To\nfacilitate monitoring of the health of the AI benchmarking ecosystem, we\nintroduce methodologies for creating condensed maps of the global dynamics of\nbenchmark creation and saturation. We curated data for 1688 benchmarks covering\nthe entire domains of computer vision and natural language processing, and show\nthat a large fraction of benchmarks quickly trended towards near-saturation,\nthat many benchmarks fail to find widespread utilization, and that benchmark\nperformance gains for different AI tasks were prone to unforeseen bursts. We\nanalyze attributes associated with benchmark popularity, and conclude that\nfuture benchmarks should emphasize versatility, breadth and real-world utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_Silva_A/0/1/0/all/0/1\">Adriano Barbosa-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionSC: Data Set and Network for Real-Time Semantic Mapping in Dynamic Environments. (arXiv:2203.07060v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07060","description":"<p>This work addresses a gap in semantic scene completion (SSC) data by creating\na novel outdoor data set with accurate and complete dynamic scenes. Our data\nset is formed from randomly sampled views of the world at each time step, which\nsupervises generalizability to complete scenes without occlusions or traces. We\ncreate SSC baselines from state-of-the-art open source networks and construct a\nbenchmark real-time dense local semantic mapping algorithm, MotionSC, by\nleveraging recent 3D deep learning architectures to enhance SSC with temporal\ninformation. Our network shows that the proposed data set can quantify and\nsupervise accurate scene completion in the presence of dynamic objects, which\ncan lead to the development of improved dynamic mapping algorithms. All\nsoftware is available at https://github.com/UMich-CURLY/3DMapping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1\">Joey Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuewei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Arthur Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capodieci_A/0/1/0/all/0/1\">Andrew Capodieci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_P/0/1/0/all/0/1\">Paramsothy Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barton_K/0/1/0/all/0/1\">Kira Barton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel End-To-End Network for Reconstruction of Non-Regularly Sampled Image Data Using Locally Fully Connected Layers. (arXiv:2203.09180v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.09180","description":"<p>Quarter sampling and three-quarter sampling are novel sensor concepts that\nenable the acquisition of higher resolution images without increasing the\nnumber of pixels. This is achieved by non-regularly covering parts of each\npixel of a low-resolution sensor such that only one quadrant or three quadrants\nof the sensor area of each pixel is sensitive to light. Combining a properly\ndesigned mask and a high-quality reconstruction algorithm, a higher image\nquality can be achieved than using a low-resolution sensor and subsequent\nupsampling. For the latter case, the image quality can be further enhanced\nusing super resolution algorithms such as the very deep super resolution\nnetwork (VDSR). In this paper, we propose a novel end-to-end neural network to\nreconstruct high resolution images from non-regularly sampled sensor data. The\nnetwork is a concatenation of a locally fully connected reconstruction network\n(LFCR) and a standard VDSR network. Altogether, using a three-quarter sampling\nsensor with our novel neural network layout, the image quality in terms of PSNR\nfor the Urban100 dataset can be increased by 2.96 dB compared to the\nstate-of-the-art approach. Compared to a low-resolution sensor with VDSR, a\ngain of 1.11 dB is achieved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Grosche_S/0/1/0/all/0/1\">Simon Grosche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brand_F/0/1/0/all/0/1\">Fabian Brand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Intermediate-level Attack Framework on The Basis of Linear Regression. (arXiv:2203.10723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10723","description":"<p>This paper substantially extends our work published at ECCV, in which an\nintermediate-level attack was proposed to improve the transferability of some\nbaseline adversarial examples. Specifically, we advocate a framework in which a\ndirect linear mapping from the intermediate-level discrepancies (between\nadversarial features and benign features) to prediction loss of the adversarial\nexample is established. By delving deep into the core components of such a\nframework, we show that 1) a variety of linear regression models can all be\nconsidered in order to establish the mapping, 2) the magnitude of the finally\nobtained intermediate-level adversarial discrepancy is correlated with the\ntransferability, 3) further boost of the performance can be achieved by\nperforming multiple runs of the baseline attack with random initialization. In\naddition, by leveraging these findings, we achieve new state-of-the-arts on\ntransfer-based $\\ell_\\infty$ and $\\ell_2$ attacks. Our code is publicly\navailable at https://github.com/qizhangli/ila-plus-plus-lr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qizhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privileged Attribution Constrained Deep Networks for Facial Expression Recognition. (arXiv:2203.12905v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12905","description":"<p>Facial Expression Recognition (FER) is crucial in many research domains\nbecause it enables machines to better understand human behaviours. FER methods\nface the problems of relatively small datasets and noisy data that don't allow\nclassical networks to generalize well. To alleviate these issues, we guide the\nmodel to concentrate on specific facial areas like the eyes, the mouth or the\neyebrows, which we argue are decisive to recognise facial expressions. We\npropose the Privileged Attribution Loss (PAL), a method that directs the\nattention of the model towards the most salient facial regions by encouraging\nits attribution maps to correspond to a heatmap formed by facial landmarks.\nFurthermore, we introduce several channel strategies that allow the model to\nhave more degrees of freedom. The proposed method is independent of the\nbackbone architecture and doesn't need additional semantic information at test\ntime. Finally, experimental results show that the proposed PAL method\noutperforms current state-of-the-art methods on both RAF-DB and AffectNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonnard_J/0/1/0/all/0/1\">Jules Bonnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhombres_F/0/1/0/all/0/1\">Ferdinand Dhombres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1\">K&#xe9;vin Bailly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices. (arXiv:2204.02090v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02090","description":"<p>In this paper, we address the problem of lip-voice synchronisation in videos\ncontaining human face and voice. Our approach is based on determining if the\nlips motion and the voice in a video are synchronised or not, depending on\ntheir audio-visual correspondence score. We propose an audio-visual cross-modal\ntransformer-based model that outperforms several baseline models in the\naudio-visual synchronisation task on the standard lip-reading speech benchmark\ndataset LRS2. While the existing methods focus mainly on lip synchronisation in\nspeech videos, we also consider the special case of the singing voice. The\nsinging voice is a more challenging use case for synchronisation due to\nsustained vowel sounds. We also investigate the relevance of lip\nsynchronisation models trained on speech datasets in the context of singing\nvoice. Finally, we use the frozen visual features learned by our lip\nsynchronisation model in the singing voice separation task to outperform a\nbaseline audio-visual model which was trained end-to-end. The demos, source\ncode, and the pre-trained models are available on\nhttps://ipcv.github.io/VocaLiST/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadandale_V/0/1/0/all/0/1\">Venkatesh S. Kadandale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_J/0/1/0/all/0/1\">Juan F. Montesinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haro_G/0/1/0/all/0/1\">Gloria Haro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower. (arXiv:2204.02390v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.02390","description":"<p>We investigate pneumatic non-prehensile manipulation (i.e., blowing) as a\nmeans of efficiently moving scattered objects into a target receptacle. Due to\nthe chaotic nature of aerodynamic forces, a blowing controller must (i)\ncontinually adapt to unexpected changes from its actions, (ii) maintain\nfine-grained control, since the slightest misstep can result in large\nunintended consequences (e.g., scatter objects already in a pile), and (iii)\ninfer long-range plans (e.g., move the robot to strategic blowing locations).\nWe tackle these challenges in the context of deep reinforcement learning,\nintroducing a multi-frequency version of the spatial action maps framework.\nThis allows for efficient learning of vision-based policies that effectively\ncombine high-level planning and low-level closed-loop control for dynamic\nmobile manipulation. Experiments show that our system learns efficient\nbehaviors for the task, demonstrating in particular that blowing achieves\nbetter downstream performance than pushing, and that our policies improve\nperformance over baselines. Moreover, we show that our system naturally\nencourages emergent specialization between the different subpolicies spanning\nlow-level fine-grained control and high-level planning. On a real mobile robot\nequipped with a miniature air blower, we show that our simulation-trained\npolicies transfer well to a real environment and can generalize to novel\nobjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1\">Szymon Rusinkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LifeLonger: A Benchmark for Continual Disease Classification. (arXiv:2204.05737v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05737","description":"<p>Deep learning models have shown a great effectiveness in recognition of\nfindings in medical images. However, they cannot handle the ever-changing\nclinical environment, bringing newly annotated medical data from different\nsources. To exploit the incoming streams of data, these models would benefit\nlargely from sequentially learning from new samples, without forgetting the\npreviously obtained knowledge. In this paper we introduce LifeLonger, a\nbenchmark for continual disease classification on the MedMNIST collection, by\napplying existing state-of-the-art continual learning methods. In particular,\nwe consider three continual learning scenarios, namely, task and class\nincremental learning and the newly defined cross-domain incremental learning.\nTask and class incremental learning of diseases address the issue of\nclassifying new samples without re-training the models from scratch, while\ncross-domain incremental learning addresses the issue of dealing with datasets\noriginating from different institutions while retaining the previously obtained\nknowledge. We perform a thorough analysis of the performance and examine how\nthe well-known challenges of continual learning, such as the catastrophic\nforgetting exhibit themselves in this setting. The encouraging results\ndemonstrate that continual learning has a major potential to advance disease\nclassification and to produce a more robust and efficient learning framework\nfor clinical settings. The code repository, data partitions and baseline\nresults for the complete benchmark will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Derakhshani_M/0/1/0/all/0/1\">Mohammad Mahdi Derakhshani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najdenkoska_I/0/1/0/all/0/1\">Ivona Najdenkoska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonsbeek_T/0/1/0/all/0/1\">Tom van Sonsbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1\">Xiantong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object Detection. (arXiv:2204.11582v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11582","description":"<p>3D object detection from multiple image views is a fundamental and\nchallenging task for visual scene understanding. Due to its low cost and high\nefficiency, multi-view 3D object detection has demonstrated promising\napplication prospects. However, accurately detecting objects through\nperspective views in the 3D space is extremely difficult due to the lack of\ndepth information. Recently, DETR3D introduces a novel 3D-2D query paradigm in\naggregating multi-view images for 3D object detection and achieves\nstate-of-the-art performance. In this paper, with intensive pilot experiments,\nwe quantify the objects located at different regions and find that the\n\"truncated instances\" (i.e., at the border regions of each image) are the main\nbottleneck hindering the performance of DETR3D. Although it merges multiple\nfeatures from two adjacent views in the overlapping regions, DETR3D still\nsuffers from insufficient feature aggregation, thus missing the chance to fully\nboost the detection performance. In an effort to tackle the problem, we propose\nGraph-DETR3D to automatically aggregate multi-view imagery information through\ngraph structure learning (GSL). It constructs a dynamic 3D graph between each\nobject query and 2D feature maps to enhance the object representations,\nespecially at the border regions. Besides, Graph-DETR3D benefits from a novel\ndepth-invariant multi-scale training strategy, which maintains the visual depth\nconsistency by simultaneously scaling the image size and the object depth.\nExtensive experiments on the nuScenes dataset demonstrate the effectiveness and\nefficiency of our Graph-DETR3D. Notably, our best model achieves 49.5 NDS on\nthe nuScenes test leaderboard, achieving new state-of-the-art in comparison\nwith various published image-view 3D object detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network. (arXiv:2205.01550v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01550","description":"<p>In recent years, with the development of computing resources and LiDAR, point\ncloud semantic segmentation has attracted many researchers. For the sparsity of\npoint clouds, although there is already a way to deal with sparse convolution,\nmulti-scale features are not considered. In this letter, we propose a feature\nextraction module based on multi-scale sparse convolution and a feature\nselection module based on channel attention and build a point cloud\nsegmentation network framework based on this. By introducing multi-scale sparse\nconvolution, the network could capture richer feature information based on\nconvolution kernels with different sizes, improving the segmentation result of\npoint cloud segmentation. Experimental results on Stanford large-scale 3-D\nIndoor Spaces(S3DIS) dataset and outdoor dataset(SemanticKITTI), demonstrate\neffectiveness and superiority of the proposed mothod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yunzheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jie Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions. (arXiv:2205.10218v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.10218","description":"<p>Generalization across different environments with the same tasks is critical\nfor successful applications of visual reinforcement learning (RL) in real\nscenarios. However, visual distractions -- which are common in real scenes --\nfrom high-dimensional observations can be hurtful to the learned\nrepresentations in visual RL, thus degrading the performance of generalization.\nTo tackle this problem, we propose a novel approach, namely Characteristic\nReward Sequence Prediction (CRESP), to extract the task-relevant information by\nlearning reward sequence distributions (RSDs), as the reward signals are\ntask-relevant in RL and invariant to visual distractions. Specifically, to\neffectively capture the task-relevant information via RSDs, CRESP introduces an\nauxiliary task -- that is, predicting the characteristic functions of RSDs --\nto learn task-relevant representations, because we can well approximate the\nhigh-dimensional distributions by leveraging the corresponding characteristic\nfunctions. Experiments demonstrate that CRESP significantly improves the\nperformance of generalization on unseen environments, outperforming several\nstate-of-the-arts on DeepMind Control tasks with different visual distractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mingxuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pooling Revisited: Your Receptive Field is Suboptimal. (arXiv:2205.15254v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15254","description":"<p>The size and shape of the receptive field determine how the network\naggregates local information and affect the overall performance of a model\nconsiderably. Many components in a neural network, such as kernel sizes and\nstrides for convolution and pooling operations, influence the configuration of\na receptive field. However, they still rely on hyperparameters, and the\nreceptive fields of existing models result in suboptimal shapes and sizes.\nHence, we propose a simple yet effective Dynamically Optimized Pooling\noperation, referred to as DynOPool, which optimizes the scale factors of\nfeature maps end-to-end by learning the desirable size and shape of its\nreceptive field in each layer. Any kind of resizing modules in a deep neural\nnetwork can be replaced by the operations with DynOPool at a minimal cost.\nAlso, DynOPool controls the complexity of a model by introducing an additional\nloss term that constrains computational cost. Our experiments show that the\nmodels equipped with the proposed learnable resizing module outperform the\nbaseline networks on multiple datasets in image classification and semantic\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_D/0/1/0/all/0/1\">Dong-Hwan Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_S/0/1/0/all/0/1\">Sanghyeok Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joonhyuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D'ARTAGNAN: Counterfactual Video Generation. (arXiv:2206.01651v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01651","description":"<p>Causally-enabled machine learning frameworks could help clinicians to\nidentify the best course of treatments by answering counterfactual questions.\nWe explore this path for the case of echocardiograms by looking into the\nvariation of the Left Ventricle Ejection Fraction, the most essential clinical\nmetric gained from these examinations. We combine deep neural networks, twin\ncausal networks and generative adversarial methods for the first time to build\nD'ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks), a novel\ncausal generative model. We demonstrate the soundness of our approach on a\nsynthetic dataset before applying it to cardiac ultrasound videos to answer the\nquestion: \"What would this echocardiogram look like if the patient had a\ndifferent ejection fraction?\". To do so, we generate new ultrasound videos,\nretaining the video style and anatomy of the original patient, while modifying\nthe Ejection Fraction conditioned on a given input. We achieve an SSIM score of\n0.79 and an R2 score of 0.51 on the counterfactual videos. Code and models are\navailable at: https://github.com/HReynaud/dartagnan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reynaud_H/0/1/0/all/0/1\">Hadrien Reynaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dombrowski_M/0/1/0/all/0/1\">Mischa Dombrowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Ciar&#xe1;n Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeson_P/0/1/0/all/0/1\">Paul Leeson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Deformable Image Registration Network with Mutual Attention. (arXiv:2206.01863v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01863","description":"<p>Deformable image registration, estimating the spatial transformation between\ndifferent images, is an important task in medical imaging. Many previous\nstudies have used learning-based methods for multi-stage registration to\nperform 3D image registration to improve performance. The performance of the\nmulti-stage approach, however, is limited by the size of the receptive field\nwhere complex motion does not occur at a single spatial scale. We propose a new\nregistration network combining recursive network architecture and mutual\nattention mechanism to overcome these limitations. Compared with the\nstate-of-the-art deep learning methods, our network based on the recursive\nstructure achieves the highest accuracy in lung Computed Tomography (CT) data\nset (Dice score of 92\\% and average surface distance of 3.8mm for lungs) and\none of the most accurate results in abdominal CT data set with 9 organs of\nvarious sizes (Dice score of 55\\% and average surface distance of 7.8mm). We\nalso showed that adding 3 recursive networks is sufficient to achieve the\nstate-of-the-art results without a significant increase in the inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jian-Qing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baoru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_N/0/1/0/all/0/1\">Ngee Han Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_T/0/1/0/all/0/1\">Tonia Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papiez_B/0/1/0/all/0/1\">Bartlomiej W. Papiez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training of Handwritten Word Recognition for Synthetic-to-Real Adaptation. (arXiv:2206.03149v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03149","description":"<p>Performances of Handwritten Text Recognition (HTR) models are largely\ndetermined by the availability of labeled and representative training samples.\nHowever, in many application scenarios labeled samples are scarce or costly to\nobtain. In this work, we propose a self-training approach to train a HTR model\nsolely on synthetic samples and unlabeled data. The proposed training scheme\nuses an initial model trained on synthetic data to make predictions for the\nunlabeled target dataset. Starting from this initial model with rather poor\nperformance, we show that a considerable adaptation is possible by training\nagainst the predicted pseudo-labels. Moreover, the investigated self-training\nstrategy does not require any manually annotated training samples. We evaluate\nthe proposed method on four widely used benchmark datasets and show its\neffectiveness on closing the gap to a model trained in a fully-supervised\nmanner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolf_F/0/1/0/all/0/1\">Fabian Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fink_G/0/1/0/all/0/1\">Gernot A. Fink</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03820","description":"<p>Intra-voxel incoherent motion (IVIM) analysis of fetal lungs\nDiffusion-Weighted MRI (DWI) data shows potential in providing quantitative\nimaging bio-markers that reflect, indirectly, diffusion and pseudo-diffusion\nfor non-invasive fetal lung maturation assessment. However, long acquisition\ntimes, due to the large number of different ``b-value'' images required for\nIVIM analysis, precluded clinical feasibility. We introduce SUPER-IVIM-DC a\ndeep-neural-networks (DNN) approach which couples supervised loss with a\ndata-consistency term to enable IVIM analysis of DWI data acquired with a\nlimited number of b-values. We demonstrated the added-value of SUPER-IVIM-DC\nover both classical and recent DNN approaches for IVIM analysis through\nnumerical simulations, healthy volunteer study, and IVIM analysis of fetal lung\nmaturation from fetal DWI data. Our numerical simulations and healthy volunteer\nstudy show that SUPER-IVIM-DC estimates of the IVIM model parameters from\nlimited DWI data had lower normalized root mean-squared error compared to\nprevious DNN-based approaches. Further, SUPER-IVIM-DC estimates of the\npseudo-diffusion fraction parameter from limited DWI data of fetal lungs\ncorrelate better with gestational age compared to both to classical and\nDNN-based approaches (0.555 vs. 0.463 and 0.310). SUPER-IVIM-DC has the\npotential to reduce the long acquisition times associated with IVIM analysis of\nDWI data and to provide clinically feasible bio-markers for non-invasive fetal\nlung maturity assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korngut_N/0/1/0/all/0/1\">Noam Korngut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rotman_E/0/1/0/all/0/1\">Elad Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afacan_O/0/1/0/all/0/1\">Onur Afacan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurugol_S/0/1/0/all/0/1\">Sila Kurugol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaffrani_Reznikov_Y/0/1/0/all/0/1\">Yael Zaffrani-Reznikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemirovsky_Rotman_S/0/1/0/all/0/1\">Shira Nemirovsky-Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warfield_S/0/1/0/all/0/1\">Simon Warfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TBraTS: Trusted Brain Tumor Segmentation. (arXiv:2206.09309v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.09309","description":"<p>Despite recent improvements in the accuracy of brain tumor segmentation, the\nresults still exhibit low levels of confidence and robustness. Uncertainty\nestimation is one effective way to change this situation, as it provides a\nmeasure of confidence in the segmentation results. In this paper, we propose a\ntrusted brain tumor segmentation network which can generate robust segmentation\nresults and reliable uncertainty estimations without excessive computational\nburden and modification of the backbone network. In our method, uncertainty is\nmodeled explicitly using subjective logic theory, which treats the predictions\nof backbone neural network as subjective opinions by parameterizing the class\nprobabilities of the segmentation as a Dirichlet distribution. Meanwhile, the\ntrusted segmentation framework learns the function that gathers reliable\nevidence from the feature leading to the final segmentation results. Overall,\nour unified trusted segmentation framework endows the model with reliability\nand robustness to out-of-distribution samples. To evaluate the effectiveness of\nour model in robustness and reliability, qualitative and quantitative\nexperiments are conducted on the BraTS 2019 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zou_K/0/1/0/all/0/1\">Ke Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xuedong Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_X/0/1/0/all/0/1\">Xiaojing Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology. (arXiv:2206.09769v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.09769","description":"<p>Histopathology whole slide images (WSIs) can reveal significant\ninter-hospital variability such as illumination, color or optical artifacts.\nThese variations, caused by the use of different scanning protocols across\nmedical centers (staining, scanner), can strongly harm algorithms\ngeneralization on unseen protocols. This motivates development of new methods\nto limit such drop of performances. In this paper, to enhance robustness on\nunseen target protocols, we propose a new test-time data augmentation based on\nmulti domain image-to-image translation. It allows to project images from\nunseen protocol into each source domain before classifying them and ensembling\nthe predictions. This test-time augmentation method results in a significant\nboost of performances for domain generalization. To demonstrate its\neffectiveness, our method has been evaluated on 2 different histopathology\ntasks where it outperforms conventional domain generalization, standard H&amp;E\nspecific color augmentation/normalization and standard test-time augmentation\ntechniques. Our code is publicly available at\nhttps://gitlab.com/vitadx/articles/test-time-i2i-translation-ensembling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1\">Marin Scalbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1\">Florent Couzini&#xe9;-Devy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need. (arXiv:2206.11804v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.11804","description":"<p>Data diversity and volume are crucial to the success of training deep\nlearning models, while in the medical imaging field, the difficulty and cost of\ndata collection and annotation are especially huge. Specifically in robotic\nsurgery, data scarcity and imbalance have heavily affected the model accuracy\nand limited the design and deployment of deep learning-based surgical\napplications such as surgical instrument segmentation. Considering this, we\nrethink the surgical instrument segmentation task and propose a one-to-many\ndata generation solution that gets rid of the complicated and expensive process\nof data collection and annotation from robotic surgery. In our method, we only\nutilize a single surgical background tissue image and a few open-source\ninstrument images as the seed images and apply multiple augmentations and\nblending techniques to synthesize amounts of image variations. In addition, we\nalso introduce the chained augmentation mixing during training to further\nenhance the data diversities. The proposed approach is evaluated on the real\ndatasets of the EndoVis-2018 and EndoVis-2017 surgical scene segmentation. Our\nempirical analysis suggests that without the high cost of data collection and\nannotation, we can achieve decent surgical instrument segmentation performance.\nMoreover, we also observe that our method can deal with novel instrument\nprediction in the deployment domain. We hope our inspiring results will\nencourage researchers to emphasize data-centric methods to overcome demanding\ndeep learning limitations besides data shortage, such as class imbalance,\ndomain adaptation, and incremental learning. Our code is available at\nhttps://github.com/lofrienger/Single_SurgicalScene_For_Segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">An Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengya Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FetReg2021: A Challenge on Placental Vessel Segmentation and Registration in Fetoscopy. (arXiv:2206.12512v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.12512","description":"<p>Fetoscopy laser photocoagulation is a widely adopted procedure for treating\nTwin-to-Twin Transfusion Syndrome (TTTS). The procedure involves\nphotocoagulation pathological anastomoses to regulate blood exchange among\ntwins. The procedure is particularly challenging due to the limited field of\nview, poor manoeuvrability of the fetoscope, poor visibility, and variability\nin illumination. These challenges may lead to increased surgery time and\nincomplete ablation. Computer-assisted intervention (CAI) can provide surgeons\nwith decision support and context awareness by identifying key structures in\nthe scene and expanding the fetoscopic field of view through video mosaicking.\nResearch in this domain has been hampered by the lack of high-quality data to\ndesign, develop and test CAI algorithms. Through the Fetoscopic Placental\nVessel Segmentation and Registration (FetReg2021) challenge, which was\norganized as part of the MICCAI2021 Endoscopic Vision challenge, we released\nthe first largescale multicentre TTTS dataset for the development of\ngeneralized and robust semantic segmentation and video mosaicking algorithms.\nFor this challenge, we released a dataset of 2060 images, pixel-annotated for\nvessels, tool, fetus and background classes, from 18 in-vivo TTTS fetoscopy\nprocedures and 18 short video clips. Seven teams participated in this challenge\nand their model performance was assessed on an unseen test dataset of 658\npixel-annotated images from 6 fetoscopic procedures and 6 short clips. The\nchallenge provided an opportunity for creating generalized solutions for\nfetoscopic scene understanding and mosaicking. In this paper, we present the\nfindings of the FetReg2021 challenge alongside reporting a detailed literature\nreview for CAI in TTTS fetoscopy. Through this challenge, its analysis and the\nrelease of multi-centre fetoscopic data, we provide a benchmark for future\nresearch in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Casella_A/0/1/0/all/0/1\">Alessandro Casella</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qayyum_A/0/1/0/all/0/1\">Abdul Qayyum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benzinou_A/0/1/0/all/0/1\">Abdesslam Benzinou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazher_M/0/1/0/all/0/1\">Moona Mazher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meriaudeau_F/0/1/0/all/0/1\">Fabrice Meriaudeau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lena_C/0/1/0/all/0/1\">Chiara Lena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cintorrino_I/0/1/0/all/0/1\">Ilaria Anita Cintorrino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paolis_G/0/1/0/all/0/1\">Gaia Romana De Paolis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biagioli_J/0/1/0/all/0/1\">Jessica Biagioli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grechishnikova_D/0/1/0/all/0/1\">Daria Grechishnikova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiao_J/0/1/0/all/0/1\">Jing Jiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_B/0/1/0/all/0/1\">Bizhe Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yanyan Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaire_R/0/1/0/all/0/1\">Rebati Raman Gaire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subedi_R/0/1/0/all/0/1\">Ronast Subedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vazquez_E/0/1/0/all/0/1\">Eduard Vazquez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lisowska_A/0/1/0/all/0/1\">Aneta Lisowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Attilakos_G/0/1/0/all/0/1\">George Attilakos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wimalasundera_R/0/1/0/all/0/1\">Ruwan Wimalasundera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paladini_D/0/1/0/all/0/1\">Dario Paladini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Momi_E/0/1/0/all/0/1\">Elena De Momi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mattos_L/0/1/0/all/0/1\">Leonardo S Mattos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Key-frame Guided Network for Thyroid Nodule Recognition using Ultrasound Videos. (arXiv:2206.13318v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13318","description":"<p>Ultrasound examination is widely used in the clinical diagnosis of thyroid\nnodules (benign/malignant). However, the accuracy relies heavily on radiologist\nexperience. Although deep learning techniques have been investigated for\nthyroid nodules recognition. Current solutions are mainly based on static\nultrasound images, with limited temporal information used and inconsistent with\nclinical diagnosis. This paper proposes a novel method for the automated\nrecognition of thyroid nodules through an exhaustive exploration of ultrasound\nvideos and key-frames. We first propose a detection-localization framework to\nautomatically identify the clinical key-frame with a typical nodule in each\nultrasound video. Based on the localized key-frame, we develop a key-frame\nguided video classification model for thyroid nodule recognition. Besides, we\nintroduce a motion attention module to help the network focus on significant\nframes in an ultrasound video, which is consistent with clinical diagnosis. The\nproposed thyroid nodule recognition framework is validated on clinically\ncollected ultrasound videos, demonstrating superior performance compared with\nother state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiangxiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Meng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shi Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Industrial Federated Learning Framework for AIoT: A Face Recognition Application. (arXiv:2206.13398v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13398","description":"<p>Recently, the artificial intelligence of things (AIoT) has been gaining\nincreasing attention, with an intriguing vision of providing highly intelligent\nservices through the network connection of things, leading to an advanced\nAI-driven ecology. However, recent regulatory restrictions on data privacy\npreclude uploading sensitive local data to data centers and utilizing them in a\ncentralized approach. Directly applying federated learning algorithms in this\nscenario could hardly meet the industrial requirements of both efficiency and\naccuracy. Therefore, we propose an efficient industrial federated learning\nframework for AIoT in terms of a face recognition application. Specifically, we\npropose to utilize the concept of transfer learning to speed up federated\ntraining on devices and further present a novel design of a private projector\nthat helps protect shared gradients without incurring additional memory\nconsumption or computational cost. Empirical studies on a private Asian face\ndataset show that our approach can achieve high recognition accuracy in only 20\ncommunication rounds, demonstrating its effectiveness in prediction and its\nefficiency in training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Youlong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xueyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhitao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shengqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weike Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Refinement to Improve High Resolution Image Inpainting. (arXiv:2206.13644v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13644","description":"<p>In this paper, we address the problem of degradation in inpainting quality of\nneural networks operating at high resolutions. Inpainting networks are often\nunable to generate globally coherent structures at resolutions higher than\ntheir training set. This is partially attributed to the receptive field\nremaining static, despite an increase in image resolution. Although downscaling\nthe image prior to inpainting produces coherent structure, it inherently lacks\ndetail present at higher resolutions. To get the best of both worlds, we\noptimize the intermediate featuremaps of a network by minimizing a multiscale\nconsistency loss at inference. This runtime optimization improves the\ninpainting results and establishes a new state-of-the-art for high resolution\ninpainting. Code is available at:\nhttps://github.com/geomagical/lama-with-refiner/tree/refinement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_P/0/1/0/all/0/1\">Prakhar Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_B/0/1/0/all/0/1\">Brian Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiddi_S/0/1/0/all/0/1\">Salma Jiddi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation. (arXiv:2206.13737v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13737","description":"<p>An organ segmentation method that can generalize to unseen contrasts and\nscanner settings can significantly reduce the need for retraining of deep\nlearning models. Domain Generalization (DG) aims to achieve this goal. However,\nmost DG methods for segmentation require training data from multiple domains\nduring training. We propose a novel adversarial domain generalization method\nfor organ segmentation trained on data from a \\emph{single} domain. We\nsynthesize the new domains via learning an adversarial domain synthesizer (ADS)\nand presume that the synthetic domains cover a large enough area of plausible\ndistributions so that unseen domains can be interpolated from synthetic\ndomains. We propose a mutual information regularizer to enforce the semantic\nconsistency between images from the synthetic domains, which can be estimated\nby patch-level contrastive learning. We evaluate our method for various organ\nsegmentation for unseen modalities, scanning protocols, and scanner sites.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shaoan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_M/0/1/0/all/0/1\">Maxwell Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragoza_M/0/1/0/all/0/1\">Matthew Ragoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}