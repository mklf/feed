{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Interactive Learning from Natural Language and Demonstrations using Signal Temporal Logic. (arXiv:2207.00627v1 [cs.FL])","link":"http://arxiv.org/abs/2207.00627","description":"<p>Natural language is an intuitive way for humans to communicate tasks to a\nrobot. While natural language (NL) is ambiguous, real world tasks and their\nsafety requirements need to be communicated unambiguously. Signal Temporal\nLogic (STL) is a formal logic that can serve as a versatile, expressive, and\nunambiguous formal language to describe robotic tasks. On one hand, existing\nwork in using STL for the robotics domain typically requires end-users to\nexpress task specifications in STL, a challenge for non-expert users.\n</p>\n<p>On the other, translating from NL to STL specifications is currently\nrestricted to specific fragments. In this work, we propose DIALOGUESTL, an\ninteractive approach for learning correct and concise STL formulas from (often)\nambiguous NL descriptions. We use a combination of semantic parsing,\npre-trained transformer-based language models, and user-in-the-loop\nclarifications aided by a small number of user demonstrations to predict the\nbest STL formula to encode NL task descriptions. An advantage of mapping NL to\nSTL is that there has been considerable recent work on the use of reinforcement\nlearning (RL) to identify control policies for robots. We show we can use Deep\nQ-Learning techniques to learn optimal policies from the learned STL\nspecifications. We demonstrate that DIALOGUESTL is efficient, scalable, and\nrobust, and has high accuracy in predicting the correct STL formula with a few\nnumber of demonstrations and a few interactions with an oracle user.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadinejad_S/0/1/0/all/0/1\">Sara Mohammadinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1\">Jyotirmoy V. Deshmukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Low-Resource Speech Recognition with Pretrained Speech Models: Continued Pretraining vs. Semi-Supervised Training. (arXiv:2207.00659v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00659","description":"<p>Self-supervised Transformer based models, such as wav2vec 2.0 and HuBERT,\nhave produced significant improvements over existing approaches to automatic\nspeech recognition (ASR). This is evident in the performance of the wav2vec 2.0\nbased pretrained XLSR-53 model across many languages when fine-tuned with\navailable labeled data. However, the performance from finetuning these models\ncan be dependent on the amount of in-language or similar-to-in-language data\nincluded in the pretraining dataset. In this paper we investigate continued\npretraining (CoPT) with unlabeled in-language audio data on the XLSR-53\npretrained model in several low-resource languages. CoPT is more\ncomputationally efficient than semi-supervised training (SST), the standard\napproach of utilizing unlabeled data in ASR, since it omits the need for\npseudo-labeling of the unlabeled data. We show CoPT results in word error rates\n(WERs), equal to or slightly better than using SST. In addition, we show that\nusing the CoPT model for pseudo-labeling, and using these labels in SST,\nresults in further improvements in WER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeHaven_M/0/1/0/all/0/1\">Mitchell DeHaven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Billa_J/0/1/0/all/0/1\">Jayadev Billa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building African Voices. (arXiv:2207.00688v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00688","description":"<p>Modern speech synthesis techniques can produce natural-sounding speech given\nsufficient high-quality data and compute resources. However, such data is not\nreadily available for many languages. This paper focuses on speech synthesis\nfor low-resourced African languages, from corpus creation to sharing and\ndeploying the Text-to-Speech (TTS) systems. We first create a set of\ngeneral-purpose instructions on building speech synthesis systems with minimum\ntechnological resources and subject-matter expertise. Next, we create new\ndatasets and curate datasets from \"found\" data (existing recordings) through a\nparticipatory approach while considering accessibility, quality, and breadth.\nWe demonstrate that we can develop synthesizers that generate intelligible\nspeech with 25 minutes of created speech, even when recorded in suboptimal\nenvironments. Finally, we release the speech data, code, and trained voices for\n12 African languages to support researchers and developers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogayo_P/0/1/0/all/0/1\">Perez Ogayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"American == White in Multimodal Language-and-Image AI. (arXiv:2207.00691v1 [cs.CY])","link":"http://arxiv.org/abs/2207.00691","description":"<p>Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP,\nare evaluated for evidence of a bias previously observed in social and\nexperimental psychology: equating American identity with being White. Embedding\nassociation tests (EATs) using standardized images of self-identified Asian,\nBlack, Latina/o, and White individuals from the Chicago Face Database (CFD)\nreveal that White individuals are more associated with collective in-group\nwords than are Asian, Black, or Latina/o individuals. In assessments of three\ncore aspects of American identity reported by social psychologists,\nsingle-category EATs reveal that images of White individuals are more\nassociated with patriotism and with being born in America, but that, consistent\nwith prior findings in psychology, White individuals are associated with being\nless likely to treat people of all races and backgrounds equally. Three\ndownstream machine learning tasks demonstrate biases associating American with\nWhite. In a visual question answering task using BLIP, 97% of White individuals\nare identified as American, compared to only 3% of Asian individuals. When\nasked in what state the individual depicted lives in, the model responds China\n53% of the time for Asian individuals, but always with an American state for\nWhite individuals. In an image captioning task, BLIP remarks upon the race of\nAsian individuals as much as 36% of the time, but never remarks upon race for\nWhite individuals. Finally, provided with an initialization image from the CFD\nand the text \"an American person,\" a synthetic image generator (VQGAN) using\nthe text-based guidance of CLIP lightens the skin tone of individuals of all\nraces (by 35% for Black individuals, based on pixel brightness). The results\nindicate that biases equating American identity with being White are learned by\nlanguage-and-image AI, and propagate to downstream applications of such models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UserLibri: A Dataset for ASR Personalization Using Only Text. (arXiv:2207.00706v1 [eess.AS])","link":"http://arxiv.org/abs/2207.00706","description":"<p>Personalization of speech models on mobile devices (on-device\npersonalization) is an active area of research, but more often than not, mobile\ndevices have more text-only data than paired audio-text data. We explore\ntraining a personalized language model on text-only data, used during inference\nto improve speech recognition performance for that user. We experiment on a\nuser-clustered LibriSpeech corpus, supplemented with personalized text-only\ndata for each user from Project Gutenberg. We release this User-Specific\nLibriSpeech (UserLibri) dataset to aid future personalization research.\nLibriSpeech audio-transcript pairs are grouped into 55 users from the\ntest-clean dataset and 52 users from test-other. We are able to lower the\naverage word error rate per user across both sets in streaming and nonstreaming\nmodels, including an improvement of 2.5 for the harder set of test-other users\nwhen streaming.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Breiner_T/0/1/0/all/0/1\">Theresa Breiner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramaswamy_S/0/1/0/all/0/1\">Swaroop Ramaswamy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Variani_E/0/1/0/all/0/1\">Ehsan Variani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_K/0/1/0/all/0/1\">Kilol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Mingqing Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McConnaughey_L/0/1/0/all/0/1\">Lara McConnaughey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language statistics at different spatial, temporal, and grammatical scales. (arXiv:2207.00709v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00709","description":"<p>Statistical linguistics has advanced considerably in recent decades as data\nhas become available. This has allowed researchers to study how statistical\nproperties of languages change over time. In this work, we use data from\nTwitter to explore English and Spanish considering the rank diversity at\ndifferent scales: temporal (from 3 to 96 hour intervals), spatial (from 3km to\n3000+km radii), and grammatical (from monograms to pentagrams). We find that\nall three scales are relevant. However, the greatest changes come from\nvariations in the grammatical scale. At the lowest grammatical scale\n(monograms), the rank diversity curves are most similar, independently on the\nvalues of other scales, languages, and countries. As the grammatical scale\ngrows, the rank diversity curves vary more depending on the temporal and\nspatial scales, as well as on the language and country. We also study the\nstatistics of Twitter-specific tokens: emojis, hashtags, and user mentions.\nThese particular type of tokens show a sigmoid kind of behaviour as a rank\ndiversity function. Our results are helpful to quantify aspects of language\nstatistics that seem universal and what may lead to variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Puig_F/0/1/0/all/0/1\">Fernanda S&#xe1;nchez-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Aranda_R/0/1/0/all/0/1\">Rogelio Lozano-Aranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Mendez_D/0/1/0/all/0/1\">Dante P&#xe9;rez-M&#xe9;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colman_E/0/1/0/all/0/1\">Ewan Colman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_Guzman_A/0/1/0/all/0/1\">Alfredo J. Morales-Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_C/0/1/0/all/0/1\">Carlos Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gershenson_C/0/1/0/all/0/1\">Carlos Gershenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Log-Precision Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2207.00729v1 [cs.CC])","link":"http://arxiv.org/abs/2207.00729","description":"<p>We prove that transformer neural networks with logarithmic precision in the\ninput length (and where the feedforward subnetworks are computable using linear\nspace in their input length) can be simulated by constant-depth uniform\nthreshold circuits. Thus, such transformers only recognize formal languages in\n$\\mathsf{TC}^0$, the class of languages defined by constant-depth, poly-size\nthreshold circuits. This demonstrates a connection between a practical claim in\nNLP and a theoretical conjecture in computational complexity theory: \"attention\nis all you need\" (Vaswani et al., 2017), i.e., transformers are capable of all\nefficient computation, only if all efficiently computable problems can be\nsolved with log space, i.e., $\\mathsf L = \\mathsf P$. We also construct a\ntransformer that can evaluate any constant-depth threshold circuit on any\ninput, proving that transformers can follow instructions that are representable\nin $\\mathsf{TC}^0$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk. (arXiv:2207.00735v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00735","description":"<p>Language is the principal tool for human communication, in which humor is one\nof the most attractive parts. Producing natural language like humans using\ncomputers, a.k.a, Natural Language Generation (NLG), has been widely used for\ndialogue systems, chatbots, machine translation, as well as computer-aid\ncreation e.g., idea generations, scriptwriting. However, the humor aspect of\nnatural language is relatively under-investigated, especially in the age of\npre-trained language models. In this work, we aim to preliminarily test whether\nNLG can generate humor as humans do. We build a new dataset consisting of\nnumerous digitized Chinese Comical Crosstalk scripts (called C$^3$ in short),\nwhich is for a popular Chinese performing art called `Xiangsheng' since 1800s.\n(For convenience for non-Chinese speakers, we called `crosstalk' for\n`Xiangsheng' in this paper.) We benchmark various generation approaches\nincluding training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and\nlarge-scale PLMs (with and without fine-tuning). Moreover, we also conduct a\nhuman assessment, showing that 1) large-scale pretraining largely improves\ncrosstalk generation quality; and 2) even the scripts generated from the best\nPLM is far from what we expect, with only 65% quality of human-created\ncrosstalk. We conclude, humor generation could be largely improved using\nlarge-scaled PLMs, but it is still in its infancy.\n</p>\n<p>The data and benchmarking code is publicly available in\n\\url{https://github.com/anonNo2/crosstalk-generation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangbo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaokang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INSCIT: Information-Seeking Conversations with Mixed-Initiative Interactions. (arXiv:2207.00746v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00746","description":"<p>In an information-seeking conversation, a user converses with an agent to ask\na series of questions that can often be under- or over-specified. An ideal\nagent would first identify that they were in such a situation by searching\nthrough their underlying knowledge source and then appropriately interacting\nwith a user to resolve it. However, most existing studies either fail to or\nartificially incorporate such agent-side initiatives. In this work, we present\nINSCIT (pronounced Insight), a dataset for information-seeking conversations\nwith mixed-initiative interactions. It contains a total of 4.7K user-agent\nturns from 805 human-human conversations where the agent searches over\nWikipedia and either asks for clarification or provides relevant information to\naddress user queries. We define two subtasks, namely evidence passage\nidentification and response generation, as well as a new human evaluation\nprotocol to assess the model performance. We report results of two strong\nbaselines based on state-of-the-art models of conversational knowledge\nidentification and open-domain question answering. Both models significantly\nunderperform humans and fail to generate coherent and informative responses,\nsuggesting ample room for improvement in future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parish_R/0/1/0/all/0/1\">Ryu Parish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rationale-Augmented Ensembles in Language Models. (arXiv:2207.00747v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00747","description":"<p>Recent research has shown that rationales, or step-by-step chains of thought,\ncan be used to improve performance in multi-step reasoning tasks. We reconsider\nrationale-augmented prompting for few-shot in-context learning, where (input -&gt;\noutput) prompts are expanded to (input, rationale -&gt; output) prompts. For\nrationale-augmented prompting we demonstrate how existing approaches, which\nrely on manual prompt engineering, are subject to sub-optimal rationales that\nmay harm performance. To mitigate this brittleness, we propose a unified\nframework of rationale-augmented ensembles, where we identify rationale\nsampling in the output space as the key component to robustly improve\nperformance. This framework is general and can easily be extended to common\nnatural language processing tasks, even those that do not traditionally\nleverage intermediate steps, such as question answering, word sense\ndisambiguation, and sentiment analysis. We demonstrate that rationale-augmented\nensembles achieve more accurate and interpretable results than existing\nprompting approaches--including standard prompting without rationales and\nrationale-based chain-of-thought prompting--while simultaneously improving\ninterpretability of model predictions through the associated rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-aware multimodal page classification of Brazilian legal documents. (arXiv:2207.00748v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00748","description":"<p>The Brazilian Supreme Court receives tens of thousands of cases each\nsemester. Court employees spend thousands of hours to execute the initial\nanalysis and classification of those cases -- which takes effort away from\nposterior, more complex stages of the case management workflow. In this paper,\nwe explore multimodal classification of documents from Brazil's Supreme Court.\nWe train and evaluate our methods on a novel multimodal dataset of 6,510\nlawsuits (339,478 pages) with manual annotation assigning each page to one of\nsix classes. Each lawsuit is an ordered sequence of pages, which are stored\nboth as an image and as a corresponding text extracted through optical\ncharacter recognition. We first train two unimodal classifiers: a ResNet\npre-trained on ImageNet is fine-tuned on the images, and a convolutional\nnetwork with filters of multiple kernel sizes is trained from scratch on\ndocument texts. We use them as extractors of visual and textual features, which\nare then combined through our proposed Fusion Module. Our Fusion Module can\nhandle missing textual or visual input by using learned embeddings for missing\ndata. Moreover, we experiment with bi-directional Long Short-Term Memory\n(biLSTM) networks and linear-chain conditional random fields to model the\nsequential nature of the pages. The multimodal approaches outperform both\ntextual and visual classifiers, especially when leveraging the sequential\nnature of the pages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_P/0/1/0/all/0/1\">Pedro H. Luz de Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_A/0/1/0/all/0/1\">Ana Paula G. S. de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braz_F/0/1/0/all/0/1\">Fabricio A. Braz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nilton C. da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_F/0/1/0/all/0/1\">Flavio de Barros Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_T/0/1/0/all/0/1\">Teofilo E. de Campos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Set Transformer for User-Level Classification of Depression and Gambling Disorder. (arXiv:2207.00753v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00753","description":"<p>This work proposes a transformer architecture for user-level classification\nof gambling addiction and depression that is trainable end-to-end. As opposed\nto other methods that operate at the post level, we process a set of social\nmedia posts from a particular individual, to make use of the interactions\nbetween posts and eliminate label noise at the post level. We exploit the fact\nthat, by not injecting positional encodings, multi-head attention is\npermutation invariant and we process randomly sampled sets of texts from a user\nafter being encoded with a modern pretrained sentence encoder (RoBERTa /\nMiniLM). Moreover, our architecture is interpretable with modern feature\nattribution methods and allows for automatic dataset creation by identifying\ndiscriminating posts in a user's text-set. We perform ablation studies on\nhyper-parameters and evaluate our method for the eRisk 2022 Lab on early\ndetection of signs of pathological gambling and early risk detection of\ndepression. The method proposed by our team BLUE obtained the best ERDE5 score\nof 0.015, and the second-best ERDE50 score of 0.009 for pathological gambling\ndetection. For the early detection of depression, we obtained the second-best\nERDE50 of 0.027.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages. (arXiv:2207.00758v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00758","description":"<p>We present the results of the Workshop on Multilingual Information Access\n(MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question\nanswering (QA) systems in 16 typologically diverse languages. In this task, we\nadapted two large-scale cross-lingual open-retrieval QA datasets in 14\ntypologically diverse languages, and newly annotated open-retrieval QA data in\n2 underrepresented languages: Tagalog and Tamil. Four teams submitted their\nsystems. The best system leveraging iteratively mined diverse negative examples\nand larger pretrained models achieves 32.2 F1, outperforming our baseline by\n4.5 points. The second best system uses entity-aware contextualized\nrepresentations for document retrieval, and achieves significant improvements\nin Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Hsuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRAME: Evaluating Simulatability Metrics for Free-Text Rationales. (arXiv:2207.00779v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00779","description":"<p>Free-text rationales aim to explain neural language model (LM) behavior more\nflexibly and intuitively via natural language. To ensure rationale quality, it\nis important to have metrics for measuring rationales' faithfulness (reflects\nLM's actual behavior) and plausibility (convincing to humans). All existing\nfree-text rationale metrics are based on simulatability (association between\nrationale and LM's predicted label), but there is no protocol for assessing\nsuch metrics' reliability. To investigate this, we propose FRAME, a framework\nfor evaluating free-text rationale simulatability metrics. FRAME is based on\nthree axioms: (1) good metrics should yield highest scores for reference\nrationales, which maximize rationale-label association by construction; (2)\ngood metrics should be appropriately sensitive to semantic perturbation of\nrationales; and (3) good metrics should be robust to variation in the LM's task\nperformance. Across three text classification datasets, we show that existing\nsimulatability metrics cannot satisfy all three FRAME axioms, since they are\nimplemented via model pretraining which muddles the metric's signal. We\nintroduce a non-pretraining simulatability variant that improves performance on\n(1) and (3) by an average of 41.7% and 42.9%, respectively, while performing\ncompetitively on (2).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1\">Shaoliang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Liang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaochang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firooz_H/0/1/0/all/0/1\">Hamed Firooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanjabi_M/0/1/0/all/0/1\">Maziar Sanjabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANEC: An Amharic Named Entity Corpus and Transformer Based Recognizer. (arXiv:2207.00785v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00785","description":"<p>Named Entity Recognition is an information extraction task that serves as a\npreprocessing step for other natural language processing tasks, such as machine\ntranslation, information retrieval, and question answering. Named entity\nrecognition enables the identification of proper names as well as temporal and\nnumeric expressions in an open domain text. For Semitic languages such as\nArabic, Amharic, and Hebrew, the named entity recognition task is more\nchallenging due to the heavily inflected structure of these languages. In this\npaper, we present an Amharic named entity recognition system based on\nbidirectional long short-term memory with a conditional random fields layer. We\nannotate a new Amharic named entity recognition dataset (8,070 sentences, which\nhas 182,691 tokens) and apply Synthetic Minority Over-sampling Technique to our\ndataset to mitigate the imbalanced classification problem. Our named entity\nrecognition system achieves an F_1 score of 93%, which is the new\nstate-of-the-art result for Amharic named entity recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jibril_E/0/1/0/all/0/1\">Ebrahim Chekol Jibril</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantg_A/0/1/0/all/0/1\">A. C&#xfc;neyd Tant&#x11f;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking. (arXiv:2207.00828v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00828","description":"<p>Task-oriented dialogue systems often employ a Dialogue State Tracker (DST) to\nsuccessfully complete conversations. Recent state-of-the-art DST\nimplementations rely on schemata of diverse services to improve model\nrobustness and handle zero-shot generalization to new domains [1], however such\nmethods [2, 3] typically require multiple large scale transformer models and\nlong input sequences to perform well. We propose a single multi-task BERT-based\nmodel that jointly solves the three DST tasks of intent prediction, requested\nslot prediction and slot filling. Moreover, we propose an efficient and\nparsimonious encoding of the dialogue history and service schemata that is\nshown to further improve performance. Evaluation on the SGD dataset shows that\nour approach outperforms the baseline SGP-DST by a large margin and performs\nwell compared to the state-of-the-art, while being significantly more\ncomputationally efficient. Extensive ablation studies are performed to examine\nthe contributing factors to the success of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kapelonis_E/0/1/0/all/0/1\">Eleftherios Kapelonis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiou_E/0/1/0/all/0/1\">Efthymios Georgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1\">Alexandros Potamianos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition. (arXiv:2207.00857v1 [cs.SD])","link":"http://arxiv.org/abs/2207.00857","description":"<p>Incorporating biasing words obtained as contextual knowledge is critical for\nmany automatic speech recognition (ASR) applications. This paper proposes the\nuse of graph neural network (GNN) encodings in a tree-constrained pointer\ngenerator (TCPGen) component for end-to-end contextual ASR. By encoding the\nbiasing words in the prefix-tree with a tree-based GNN, lookahead for future\nwordpieces in end-to-end ASR decoding is achieved at each tree node by\nincorporating information about all wordpieces on the tree branches rooted from\nit, which allows a more accurate prediction of the generation probability of\nthe biasing words. Systems were evaluated on the Librispeech corpus using\nsimulated biasing tasks, and on the AMI corpus by proposing a novel\nvisual-grounded contextual ASR pipeline that extracts biasing words from slides\nalongside each meeting. Results showed that TCPGen with GNN encodings achieved\nabout a further 15% relative WER reduction on the biasing words compared to the\noriginal TCPGen, with a negligible increase in the computation cost for\ndecoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Linguistic Blind Spot of Value-Aligned Agency, Natural and Artificial. (arXiv:2207.00868v1 [cs.AI])","link":"http://arxiv.org/abs/2207.00868","description":"<p>The value-alignment problem for artificial intelligence (AI) asks how we can\nensure that the 'values' (i.e., objective functions) of artificial systems are\naligned with the values of humanity. In this paper, I argue that linguistic\ncommunication (natural language) is a necessary condition for robust value\nalignment. I discuss the consequences that the truth of this claim would have\nfor research programmes that attempt to ensure value alignment for AI systems;\nor, more loftily, designing robustly beneficial or ethical artificial agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LaCroix_T/0/1/0/all/0/1\">Travis LaCroix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Biomedical Pipeline to Detect Clinical and Non-Clinical Named Entities. (arXiv:2207.00876v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00876","description":"<p>There are a few challenges related to the task of biomedical named entity\nrecognition, which are: the existing methods consider a fewer number of\nbiomedical entities (e.g., disease, symptom, proteins, genes); and these\nmethods do not consider the social determinants of health (age, gender,\nemployment, race), which are the non-medical factors related to patients'\nhealth. We propose a machine learning pipeline that improves on previous\nefforts in the following ways: first, it recognizes many biomedical entity\ntypes other than the standard ones; second, it considers non-clinical factors\nrelated to patient's health. This pipeline also consists of stages, such as\npreprocessing, tokenization, mapping embedding lookup and named entity\nrecognition task to extract biomedical named entities from the free texts. We\npresent a new dataset that we prepare by curating the COVID-19 case reports.\nThe proposed approach outperforms the baseline methods on five benchmark\ndatasets with macro-and micro-average F1 scores around 90, as well as our\ndataset with a macro-and micro-average F1 score of 95.25 and 93.18\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1\">Shaina Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_B/0/1/0/all/0/1\">Brian Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism. (arXiv:2207.00883v1 [cs.SD])","link":"http://arxiv.org/abs/2207.00883","description":"<p>Transformer-based models have demonstrated their effectiveness in automatic\nspeech recognition (ASR) tasks and even shown superior performance over the\nconventional hybrid framework. The main idea of Transformers is to capture the\nlong-range global context within an utterance by self-attention layers.\nHowever, for scenarios like conversational speech, such utterance-level\nmodeling will neglect contextual dependencies that span across utterances. In\nthis paper, we propose to explicitly model the inter-sentential information in\na Transformer based end-to-end architecture for conversational speech\nrecognition. Specifically, for the encoder network, we capture the contexts of\nprevious speech and incorporate such historic information into current input by\na context-aware residual attention mechanism. For the decoder, the prediction\nof current utterance is also conditioned on the historic linguistic information\nthrough a conditional decoder framework. We show the effectiveness of our\nproposed method on several open-source dialogue corpora and the proposed method\nconsistently improved the performance from the utterance-level\nTransformer-based ASR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Repetitions with Appropriate Repeated Words. (arXiv:2207.00929v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00929","description":"<p>A repetition is a response that repeats words in the previous speaker's\nutterance in a dialogue. Repetitions are essential in communication to build\ntrust with others, as investigated in linguistic studies. In this work, we\nfocus on repetition generation. To the best of our knowledge, this is the first\nneural approach to address repetition generation. We propose Weighted Label\nSmoothing, a smoothing method for explicitly learning which words to repeat\nduring fine-tuning, and a repetition scoring method that can output more\nappropriate repetitions during decoding. We conducted automatic and human\nevaluations involving applying these methods to the pre-trained language model\nT5 for generating repetitions. The experimental results indicate that our\nmethods outperformed baselines in both evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kawamoto_T/0/1/0/all/0/1\">Toshiki Kawamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funakoshi_K/0/1/0/all/0/1\">Kotaro Funakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics. (arXiv:2207.00939v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00939","description":"<p>Long documents such as academic articles and business reports have been the\nstandard format to detail out important issues and complicated subjects that\nrequire extra attention. An automatic summarization system that can effectively\ncondense long documents into short and concise texts to encapsulate the most\nimportant information would thus be significant in aiding the reader's\ncomprehension. Recently, with the advent of neural architectures, significant\nresearch efforts have been made to advance automatic text summarization\nsystems, and numerous studies on the challenges of extending these systems to\nthe long document domain have emerged. In this survey, we provide a\ncomprehensive overview of the research on long document summarization and a\nsystematic evaluation across the three principal components of its research\nsetting: benchmark datasets, summarization models, and evaluation metrics. For\neach component, we organize the literature within the context of long document\nsummarization and conduct an empirical analysis to broaden the perspective on\ncurrent research progress. The empirical analysis includes a study on the\nintrinsic characteristics of benchmark datasets, a multi-dimensional analysis\nof summarization models, and a review of the summarization evaluation metrics.\nBased on the overall findings, we conclude by proposing possible directions for\nfuture exploration in this rapidly growing field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1\">Huan Yee Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jiaxin Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shirui Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation. (arXiv:2207.00952v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00952","description":"<p>End-to-end speech-to-text translation models are often initialized with\npre-trained speech encoder and pre-trained text decoder. This leads to a\nsignificant training gap between pre-training and fine-tuning, largely due to\nthe modality differences between speech outputs from the encoder and text\ninputs to the decoder. In this work, we aim to bridge the modality gap between\nspeech and text to improve translation quality. We propose M-Adapter, a novel\nTransformer-based module, to adapt speech representations to text. While\nshrinking the speech sequence, M-Adapter produces features desired for\nspeech-to-text translation via modelling global and local dependencies of a\nspeech sequence. Our experimental results show that our model outperforms a\nstrong baseline by up to 1 BLEU score on the Must-C En$\\rightarrow$DE\ndataset.\\footnote{Our code is available at\nhttps://github.com/mingzi151/w2v2-st.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Tieq Viet with Deep Learning Models. (arXiv:2207.00975v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00975","description":"<p>Deep learning is a powerful approach in recovering lost information as well\nas harder inverse function computation problems. When applied in natural\nlanguage processing, this approach is essentially making use of context as a\nmean to recover information through likelihood maximization. Not long ago, a\nlinguistic study called Tieq Viet was controversial among both researchers and\nsociety. We find this a great example to demonstrate the ability of deep\nlearning models to recover lost information. In the proposal of Tieq Viet, some\nconsonants in the standard Vietnamese are replaced. A sentence written in this\nproposal can be interpreted into multiple sentences in the standard version,\nwith different meanings. The hypothesis that we want to test is whether a deep\nlearning model can recover the lost information if we translate the text from\nVietnamese to Tieq Viet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thanh_N/0/1/0/all/0/1\">Nguyen Ha Thanh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mental Illness Classification on Social Media Texts using Deep Learning and Transfer Learning. (arXiv:2207.01012v1 [cs.LG])","link":"http://arxiv.org/abs/2207.01012","description":"<p>Given the current social distance restrictions across the world, most\nindividuals now use social media as their major medium of communication.\nMillions of people suffering from mental diseases have been isolated due to\nthis, and they are unable to get help in person. They have become more reliant\non online venues to express themselves and seek advice on dealing with their\nmental disorders. According to the World health organization (WHO),\napproximately 450 million people are affected. Mental illnesses, such as\ndepression, anxiety, etc., are immensely common and have affected an\nindividuals' physical health. Recently Artificial Intelligence (AI) methods\nhave been presented to help mental health providers, including psychiatrists\nand psychologists, in decision making based on patients' authentic information\n(e.g., medical records, behavioral data, social media utilization, etc.). AI\ninnovations have demonstrated predominant execution in numerous real-world\napplications broadening from computer vision to healthcare. This study analyzes\nunstructured user data on the Reddit platform and classifies five common mental\nillnesses: depression, anxiety, bipolar disorder, ADHD, and PTSD. We trained\ntraditional machine learning, deep learning, and transfer learning multi-class\nmodels to detect mental disorders of individuals. This effort will benefit the\npublic health system by automating the detection process and informing\nappropriate authorities about people who require emergency assistance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ameer_I/0/1/0/all/0/1\">Iqra Ameer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arif_M/0/1/0/all/0/1\">Muhammad Arif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidorov_G/0/1/0/all/0/1\">Grigori Sidorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Adorno_H/0/1/0/all/0/1\">Helena G&#xf2;mez-Adorno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1\">Alexander Gelbukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR. (arXiv:2207.01039v1 [eess.AS])","link":"http://arxiv.org/abs/2207.01039","description":"<p>Leveraging context information is an intuitive idea to improve performance on\nconversational automatic speech recognition(ASR). Previous works usually adopt\nrecognized hypotheses of historical utterances as preceding context, which may\nbias the current recognized hypothesis due to the inevitable\nhistoricalrecognition errors. To avoid this problem, we propose an\naudio-textual cross-modal representation extractor to learn contextual\nrepresentations directly from preceding speech. Specifically, it consists of\ntwo modal-related encoders, extracting high-level latent features from speech\nand the corresponding text, and a cross-modal encoder, which aims to learn the\ncorrelation between speech and text. We randomly mask some input tokens and\ninput sequences of each modality. Then a token-missing or modal-missing\nprediction with a modal-level CTC loss on the cross-modal encoder is performed.\nThus, the model captures not only the bi-directional context dependencies in a\nspecific modality but also relationships between different modalities. Then,\nduring the training of the conversational ASR system, the extractor will be\nfrozen to extract the textual representation of preceding speech, while such\nrepresentation is used as context fed to the ASR decoder through attention\nmechanism. The effectiveness of the proposed approach is validated on several\nMandarin conversation corpora and the highest character error rate (CER)\nreduction up to 16% is achieved on the MagicData dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_K/0/1/0/all/0/1\">Kun Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yike Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_S/0/1/0/all/0/1\">Sining Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Long Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-aspect Multilingual and Cross-lingual Parliamentary Speech Analysis. (arXiv:2207.01054v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01054","description":"<p>Parliamentary and legislative debate transcripts provide an exciting insight\ninto elected politicians' opinions, positions, and policy preferences. They are\ninteresting for political and social sciences as well as linguistics and\nnatural language processing (NLP). Exiting research covers discussions within\nindividual parliaments. In contrast, we apply advanced NLP methods to a joint\nand comparative analysis of six national parliaments (Bulgarian, Czech, French,\nSlovene, Spanish, and United Kingdom) between 2017 and 2020, whose transcripts\nare a part of the ParlaMint dataset collection. Using a uniform methodology, we\nanalyze topics discussed, emotions, and sentiment. We assess if the age,\ngender, and political orientation of speakers can be detected from speeches.\nThe results show some commonalities and many surprising differences among the\nanalyzed countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miok_K/0/1/0/all/0/1\">Kristian Miok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidalgo_Tenorio_E/0/1/0/all/0/1\">Encarnacion Hidalgo-Tenorio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osenova_P/0/1/0/all/0/1\">Petya Osenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benitez_Castro_M/0/1/0/all/0/1\">Miguel-Angel Benitez-Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-Sikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v1 [eess.AS])","link":"http://arxiv.org/abs/2207.01063","description":"<p>The majority of current TTS datasets, which are collections of individual\nutterances, contain few conversational aspects in terms of both style and\nmetadata. In this paper, we introduce DailyTalk, a high-quality conversational\nspeech dataset designed for Text-to-Speech. We sampled, modified, and recorded\n2,541 dialogues from the open-domain dialogue dataset DailyDialog which are\nadequately long to represent context of each dialogue. During the data\nconstruction step, we maintained attributes distribution originally annotated\nin DailyDialog to support diverse dialogue in DailyTalk. On top of our dataset,\nwe extend prior work as our baseline, where a non-autoregressive TTS is\nconditioned on historical information in a dialog. We gather metadata so that a\nTTS model can learn historical dialog information, the key to generating\ncontext-aware speech. From the baseline experiment results, we show that\nDailyTalk can be used to train neural text-to-speech models, and our baseline\ncan represent contextual information. The DailyTalk dataset and baseline code\nare freely available for academic use with CC-BY-SA 4.0 license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Keon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_K/0/1/0/all/0/1\">Kyumin Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Understand Depth?. (arXiv:2207.01077v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01077","description":"<p>Besides image classification, Contrastive Language-Image Pre-training (CLIP)\nhas accomplished extraordinary success for a wide range of vision tasks,\nincluding object-level and 3D space understanding. However, it's still\nchallenging to transfer semantic knowledge learned from CLIP into more\nintricate tasks of quantified targets, such as depth estimation with geometric\ninformation. In this paper, we propose to apply CLIP for zero-shot monocular\ndepth estimation, named DepthCLIP. We found that the patches of the input image\ncould respond to a certain semantic distance token and then be projected to a\nquantified depth bin for coarse estimation. Without any training, our DepthCLIP\nsurpasses existing unsupervised methods and even approaches the early\nfully-supervised networks. To our best knowledge, we are the first to conduct\nzero-shot adaptation from the semantic language knowledge to quantified\ndownstream tasks and perform zero-shot monocular depth estimation. We hope our\nwork could cast a light on future research. The code is available at\nhttps://github.com/Adonis-galaxy/DepthCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Material Science Articles. (arXiv:2207.01079v1 [cs.CL])","link":"http://arxiv.org/abs/2207.01079","description":"<p>A crucial component in the curation of KB for a scientific domain is\ninformation extraction from tables in the domain's published articles -- tables\ncarry important information (often numeric), which must be adequately extracted\nfor a comprehensive machine understanding of an article. Existing table\nextractors assume prior knowledge of table structure and format, which may not\nbe known in scientific tables. We study a specific and challenging table\nextraction problem: extracting compositions of materials (e.g., glasses,\nalloys). We first observe that material science researchers organize similar\ncompositions in a wide variety of table styles, necessitating an intelligent\nmodel for table understanding and composition extraction. Consequently, we\ndefine this novel task as a challenge for the ML community and create a\ntraining dataset comprising 4,408 distantly supervised tables, along with 1,475\nmanually annotated dev and test tables. We also present DiSCoMaT, a strong\nbaseline geared towards this specific task, which combines multiple graph\nneural networks with several task-specific regular expressions, features, and\nconstraints. We show that DiSCoMaT outperforms recent table processing\narchitectures by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohd Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">N. M. Anoop Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProoFVer: Natural Logic Theorem Proving for Fact Verification. (arXiv:2108.11357v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11357","description":"<p>Fact verification systems typically rely on neural network classifiers for\nveracity prediction which lack explainability. This paper proposes ProoFVer,\nwhich uses a seq2seq model to generate natural logic-based inferences as\nproofs. These proofs consist of lexical mutations between spans in the claim\nand the evidence retrieved, each marked with a natural logic operator. Claim\nveracity is determined solely based on the sequence of these operators. Hence,\nthese proofs are faithful explanations, and this makes ProoFVer faithful by\nconstruction. Currently, ProoFVer has the highest label accuracy and the\nsecond-best Score in the FEVER leaderboard. Furthermore, it improves by 13.21%\npoints over the next best model on a dataset with counterfactual instances,\ndemonstrating its robustness. As explanations, the proofs show better overlap\nwith human rationales than attention-based highlights and the proofs help\nhumans predict model decisions correctly more often than using the evidence\ndirectly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution. (arXiv:2108.13530v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13530","description":"<p>We consider the task of document-level entity linking (EL), where it is\nimportant to make consistent decisions for entity mentions over the full\ndocument jointly. We aim to leverage explicit \"connections\" among mentions\nwithin the document itself: we propose to join the EL task with that of\ncoreference resolution (coref). This is complementary to related works that\nexploit either (i) implicit document information (e.g., latent relations among\nentity mentions, or general language models) or (ii) connections between the\ncandidate links (e.g, as inferred from the external knowledge base).\nSpecifically, we cluster mentions that are linked via coreference, and enforce\na single EL for all of the clustered mentions together. The latter constraint\nhas the added benefit of increased coverage by joining EL candidate lists for\nthe thus clustered mentions. We formulate the coref+EL problem as a structured\nprediction task over directed trees and use a globally normalized model to\nsolve it. Experimental results on two datasets show a boost of up to +5%\nF1-score on both coref and EL tasks, compared to their standalone counterparts.\nFor a subset of hard cases, with individual mentions lacking the correct EL in\ntheir candidate entity list, we obtain a +50% increase in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaporojets_K/0/1/0/all/0/1\">Klim Zaporojets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing in-and-for Design Research. (arXiv:2111.13827v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.13827","description":"<p>We review the scholarly contributions that utilise Natural Language\nProcessing (NLP) techniques to support the design process. Using a heuristic\napproach, we gathered 223 articles that are published in 32 journals within the\nperiod 1991-present. We present state-of-the-art NLP in-and-for design research\nby reviewing these articles according to the type of natural language text\nsources: internal reports, design concepts, discourse transcripts, technical\npublications, consumer opinions, and others. Upon summarizing and identifying\nthe gaps in these contributions, we utilise an existing design innovation\nframework to identify the applications that are currently being supported by\nNLP. We then propose a few methodological and theoretical directions for future\nNLP in-and-for design research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1\">L Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1\">Lucienne T. M. Blessing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PM-MMUT: Boosted Phone-Mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition. (arXiv:2112.06721v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.06721","description":"<p>Consonant and vowel reduction are often encountered in speech, which might\ncause performance degradation in automatic speech recognition (ASR). Our\nrecently proposed learning strategy based on masking, Phone Masking Training\n(PMT), alleviates the impact of such phenomenon in Uyghur ASR. Although PMT\nachieves remarkably improvements, there still exists room for further gains due\nto the granularity mismatch between the masking unit of PMT (phoneme) and the\nmodeling unit (word-piece). To boost the performance of PMT, we propose\nmulti-modeling unit training (MMUT) architecture fusion with PMT (PM-MMUT). The\nidea of MMUT framework is to split the Encoder into two parts including\nacoustic feature sequences to phoneme-level representation (AF-to-PLR) and\nphoneme-level representation to word-piece-level representation (PLR-to-WPLR).\nIt allows AF-to-PLR to be optimized by an intermediate phoneme-based CTC loss\nto learn the rich phoneme-level context information brought by PMT.\nExperimental results on Uyghur ASR show that the proposed approaches outperform\nobviously the pure PMT. We also conduct experiments on the 960-hour Librispeech\nbenchmark using ESPnet1, which achieves about 10% relative WER reduction on all\nthe test set without LM fusion comparing with the latest official ESPnet1\npre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guodong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Pengfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yolwas_N/0/1/0/all/0/1\">Nurmemet Yolwas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.03969","description":"<p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem\ndue to the heterogeneity gap between different modalities and the ambiguity of\nhuman emotional expression. Although there have been many successful attempts\nto construct multimodal representations for MSA, there are still two challenges\nto be addressed: 1) A more robust multimodal representation needs to be\nconstructed to bridge the heterogeneity gap and cope with the complex\nmultimodal interactions, and 2) the contextual dynamics must be modeled\neffectively throughout the information flow. In this work, we propose a\nmultimodal representation model based on Mutual information Maximization and\nMinimization and Identity Embedding (MMMIE). We combine mutual information\nmaximization between modal pairs, and mutual information minimization between\ninput data and corresponding features to mine the modal-invariant and\ntask-related information. Furthermore, Identity Embedding is proposed to prompt\nthe downstream network to perceive the contextual information. Experimental\nresults on two public datasets demonstrate the effectiveness of the proposed\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiahao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.04592","description":"<p>Benchmarks are crucial to measuring and steering progress in artificial\nintelligence (AI). However, recent studies raised concerns over the state of AI\nbenchmarking, reporting issues such as benchmark overfitting, benchmark\nsaturation and increasing centralization of benchmark dataset creation. To\nfacilitate monitoring of the health of the AI benchmarking ecosystem, we\nintroduce methodologies for creating condensed maps of the global dynamics of\nbenchmark creation and saturation. We curated data for 1688 benchmarks covering\nthe entire domains of computer vision and natural language processing, and show\nthat a large fraction of benchmarks quickly trended towards near-saturation,\nthat many benchmarks fail to find widespread utilization, and that benchmark\nperformance gains for different AI tasks were prone to unforeseen bursts. We\nanalyze attributes associated with benchmark popularity, and conclude that\nfuture benchmarks should emphasize versatility, breadth and real-world utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_Silva_A/0/1/0/all/0/1\">Adriano Barbosa-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.05482","description":"<p>The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs -- we call\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on\nImageNet, achieved a new state of the art. Furthermore, we show that the model\nsoup approach extends to multiple image classification and natural language\nprocessing tasks, improves out-of-distribution performance, and improves\nzero-shot performance on new downstream tasks. Finally, we analytically relate\nthe performance similarity of weight-averaging and logit-ensembling to flatness\nof the loss and confidence of the predictions, and validate this relation\nempirically. Code is available at https://github.com/mlfoundations/model-soups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Label Is Better Than Human Label. (arXiv:2203.12668v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.12668","description":"<p>State-of-the-art automatic speech recognition (ASR) systems are trained with\ntens of thousands of hours of labeled speech data. Human transcription is\nexpensive and time consuming. Factors such as the quality and consistency of\nthe transcription can greatly affect the performance of the ASR models trained\nwith these data. In this paper, we show that we can train a strong teacher\nmodel to produce high quality pseudo labels by utilizing recent self-supervised\nand semi-supervised learning techniques. Specifically, we use JUST (Joint\nUnsupervised/Supervised Training) and iterative noisy student teacher training\nto train a 600 million parameter bi-directional teacher model. This model\nachieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively\nbetter than a baseline. We further show that by using this strong teacher model\nto generate high-quality pseudo labels for training, we can achieve 13.6%\nrelative WER reduction (5.9% to 5.1%) for a streaming model compared to using\nhuman labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12886","description":"<p>Preschool evaluation is crucial because it gives teachers and parents crucial\nknowledge about a children's growth and development. The coronavirus pandemic\nhas highlighted the necessity for preschool children to be assessed online.\nThis online testing requires a variety of technologies, from web application\ndevelopment to various artificial intelligence models in diverse criteria such\nas speech recognition. Because of the acoustic fluctuations and differences in\nvoice frequencies between children and adults, employing Automatic Speech\nRecognition(ASR) systems is difficult because they are pre-trained on adults'\nvoices. In addition, training a new model requires a large amount of data. To\nsolve this issue, we constructed an ASR for our cognitive test system using the\nWav2Vec 2.0 model with a new pre-training objective, called Random Frequency\nPitch(RFP), and our new dataset, which was tested on Meaningless Words(MW) and\nRapid Automatic Naming(RAN) tests. Due to the peculiarities of these two tests,\nwe explored numerous models, including Convolutional Neural Network(CNN) and\nWav2Vec 2.0 models. Our new approach, reaches Word Error Rate(WER) of 6.45 on\nthe Persian section of CommonVoice dataset. Furthermore our novel methodology\nproduces positive outcomes in zero- and few-shot scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filler Word Detection and Classification: A Dataset and Benchmark. (arXiv:2203.15135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.15135","description":"<p>Filler words such as `uh' or `um' are sounds or words people use to signal\nthey are pausing to think. Finding and removing filler words from recordings is\na common and tedious task in media editing. Automatically detecting and\nclassifying filler words could greatly aid in this task, but few studies have\nbeen published on this problem to date. A key reason is the absence of a\ndataset with annotated filler words for model training and evaluation. In this\nwork, we present a novel speech dataset, PodcastFillers, with 35K annotated\nfiller words and 50K annotations of other sounds that commonly occur in\npodcasts such as breaths, laughter, and word repetitions. We propose a pipeline\nthat leverages VAD and ASR to detect filler candidates and a classifier to\ndistinguish between filler word types. We evaluate our proposed pipeline on\nPodcastFillers, compare to several baselines, and present a detailed ablation\nstudy. In particular, we evaluate the importance of using ASR and how it\ncompares to a transcription-free approach resembling keyword spotting. We show\nthat our pipeline obtains state-of-the-art results, and that leveraging ASR\nstrongly outperforms a keyword spotting approach. We make PodcastFillers\npublicly available, in the hope that our work serves as a benchmark for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Ge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caceres_J/0/1/0/all/0/1\">Juan-Pablo Caceres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salamon_J/0/1/0/all/0/1\">Justin Salamon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deliberation Model for On-Device Spoken Language Understanding. (arXiv:2204.01893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01893","description":"<p>We propose a novel deliberation-based approach to end-to-end (E2E) spoken\nlanguage understanding (SLU), where a streaming automatic speech recognition\n(ASR) model produces the first-pass hypothesis and a second-pass natural\nlanguage understanding (NLU) component generates the semantic parse by\nconditioning on both ASR's text and audio embeddings. By formulating E2E SLU as\na generalized decoder, our system is able to support complex compositional\nsemantic structures. Furthermore, the sharing of parameters between ASR and NLU\nmakes the system especially suitable for resource-constrained (on-device)\nenvironments; our proposed approach consistently outperforms strong pipeline\nNLU baselines by 0.60% to 0.65% on the spoken version of the TOPv2 dataset\n(STOP). We demonstrate that the fusion of text and audio features, coupled with\nthe system's ability to rewrite the first-pass hypothesis, makes our approach\nmore robust to ASR errors. Finally, we show that our approach can significantly\nreduce the degradation when moving from natural speech to synthetic speech\ntraining, but more work is required to make text-to-speech (TTS) a viable\nsolution for scaling up E2E SLU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Suyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livshits_A/0/1/0/all/0/1\">Aleksandr Livshits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Algebraic Approach to Learning and Grounding. (arXiv:2204.02813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02813","description":"<p>We consider the problem of learning the semantics of composite algebraic\nexpressions from examples. The outcome is a versatile framework for studying\nlearning tasks that can be put into the following abstract form: The input is a\npartial algebra $\\alg$ and a finite set of examples $(\\varphi_1, O_1),\n(\\varphi_2, O_2), \\ldots$, each consisting of an algebraic term $\\varphi_i$ and\na set of objects~$O_i$. The objective is to simultaneously fill in the missing\nalgebraic operations in $\\alg$ and ground the variables of every $\\varphi_i$ in\n$O_i$, so that the combined value of the terms is optimised. We demonstrate the\napplicability of this framework through case studies in grammatical inference,\npicture-language learning, and the grounding of logic scene descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bjorklund_J/0/1/0/all/0/1\">Johanna Bj&#xf6;rklund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindstrom_A/0/1/0/all/0/1\">Adam Dahlgren Lindstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drewes_F/0/1/0/all/0/1\">Frank Drewes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ByT5 model for massively multilingual grapheme-to-phoneme conversion. (arXiv:2204.03067v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03067","description":"<p>In this study, we tackle massively multilingual grapheme-to-phoneme\nconversion through implementing G2P models based on ByT5. We have curated a G2P\ndataset from various sources that covers around 100 languages and trained\nlarge-scale multilingual G2P models based on ByT5. We found that ByT5 operating\non byte-level inputs significantly outperformed the token-based mT5 model in\nterms of multilingual G2P. Pairwise comparison with monolingual models in these\nlanguages suggests that multilingual ByT5 models generally lower the phone\nerror rate by jointly learning from a variety of languages. The pretrained\nmodel can further benefit low resource G2P through zero-shot prediction on\nunseen languages or provides pretrained weights for finetuning, which helps the\nmodel converge to a lower phone error rate than randomly initialized weights.\nTo facilitate future research on multilingual G2P, we make available our code\nand pretrained multilingual G2P models at:\nhttps://github.com/lingjzhu/CharsiuG2P.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAESTRO: Matched Speech Text Representations through Modality Matching. (arXiv:2204.03409v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03409","description":"<p>We present Maestro, a self-supervised training method to unify\nrepresentations learnt from speech and text modalities. Self-supervised\nlearning from speech signals aims to learn the latent structure inherent in the\nsignal, while self-supervised learning from text attempts to capture lexical\ninformation. Learning aligned representations from unpaired speech and text\nsequences is a challenging task. Previous work either implicitly enforced the\nrepresentations learnt from these two modalities to be aligned in the latent\nspace through multitasking and parameter sharing or explicitly through\nconversion of modalities via speech synthesis. While the former suffers from\ninterference between the two modalities, the latter introduces additional\ncomplexity. In this paper, we propose Maestro, a novel algorithm to learn\nunified representations from both these modalities simultaneously that can\ntransfer to diverse downstream tasks such as Automated Speech Recognition (ASR)\nand Speech Translation (ST). Maestro learns unified representations through\nsequence alignment, duration prediction and matching embeddings in the learned\nspace through an aligned masked-language model loss. We establish a new\nstate-of-the-art (SOTA) on VoxPopuli multilingual ASR with a 8% relative\nreduction in Word Error Rate (WER), multidomain SpeechStew ASR (3.7% relative)\nand 21 languages to English multilingual ST on CoVoST 2 with an improvement of\n2.8 BLEU averaged over 21 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Ordering of Coordinate Compounds and Elaborate Expressions in Hmong, Lahu, and Chinese. (arXiv:2204.04080v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04080","description":"<p>Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate\nconstructions common in languages of East and Southeast Asia. Mortensen (2006)\nclaims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese\ncan be predicted via phonological hierarchies and (2) these phonological\nhierarchies lack a clear phonetic rationale. These claims are significant\nbecause morphosyntax has often been seen as in a feed-forward relationship with\nphonology, and phonological generalizations have often been assumed to be\nphonetically \"natural\". We investigate whether the ordering of CCs and EEs can\nbe learned empirically and whether computational models (classifiers and\nsequence labeling models) learn unnatural hierarchies similar to those posited\nby Mortensen (2006). We find that decision trees and SVMs learn to predict the\norder of CCs/EEs on the basis of phonology, with DTs learning hierarchies\nstrikingly similar to those proposed by Mortensen. However, we also find that a\nneural sequence labeling model is able to learn the ordering of elaborate\nexpressions in Hmong very effectively without using any phonological\ninformation. We argue that EE ordering can be learned through two independent\nroutes: phonology and lexical distribution, presenting a more nuanced picture\nthan previous work. [ISO 639-3:hmn, lhu, cmn]\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chenxuan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Katherine J. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Streaming End-to-End Speech Translation with Neural Transducers. (arXiv:2204.05352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05352","description":"<p>Neural transducers have been widely used in automatic speech recognition\n(ASR). In this paper, we introduce it to streaming end-to-end speech\ntranslation (ST), which aims to convert audio signals to texts in other\nlanguages directly. Compared with cascaded ST that performs ASR followed by\ntext-based machine translation (MT), the proposed Transformer transducer\n(TT)-based ST model drastically reduces inference latency, exploits speech\ninformation, and avoids error propagation from ASR to MT. To improve the\nmodeling capacity, we propose attention pooling for the joint network in TT. In\naddition, we extend TT-based ST to multilingual ST, which generates texts of\nmultiple languages at the same time. Experimental results on a large-scale 50\nthousand (K) hours pseudo-labeled training set show that TT-based ST not only\nsignificantly reduces inference time but also outperforms non-streaming\ncascaded ST for English-German translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Learning for Natural Language Processing: A Survey. (arXiv:2205.01500v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01500","description":"<p>Deep learning has been the mainstream technique in natural language\nprocessing (NLP) area. However, the techniques require many labeled data and\nare less generalizable across domains. Meta-learning is an arising field in\nmachine learning studying approaches to learn better learning algorithms.\nApproaches aim at improving algorithms in various aspects, including data\nefficiency and generalizability. Efficacy of approaches has been shown in many\nNLP tasks, but there is no systematic survey of these approaches in NLP, which\nhinders more researchers from joining the field. Our goal with this survey\npaper is to offer researchers pointers to relevant meta-learning works in NLP\nand attract more attention from the NLP community to drive future innovation.\nThis paper first introduces the general concepts of meta-learning and the\ncommon approaches. Then we summarize task construction settings and application\nof meta-learning for various NLP problems and review the development of\nmeta-learning in NLP community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking for Knowledge: Training RL Agents to Query External Knowledge Using Language. (arXiv:2205.06111v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2205.06111","description":"<p>To solve difficult tasks, humans ask questions to acquire knowledge from\nexternal sources. In contrast, classical reinforcement learning agents lack\nsuch an ability and often resort to exploratory behavior. This is exacerbated\nas few present-day environments support querying for knowledge. In order to\nstudy how agents can be taught to query external knowledge via language, we\nfirst introduce two new environments: the grid-world-based Q-BabyAI and the\ntext-based Q-TextWorld. In addition to physical interactions, an agent can\nquery an external knowledge source specialized for these environments to gather\ninformation. Second, we propose the \"Asking for Knowledge\" (AFK) agent, which\nlearns to generate language commands to query for meaningful knowledge that\nhelps solve the tasks. AFK leverages a non-parametric memory, a pointer\nmechanism and an episodic exploration bonus to tackle (1) irrelevant\ninformation, (2) a large query language space, (3) delayed reward for making\nmeaningful queries. Extensive experiments demonstrate that the AFK agent\noutperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1\">Iou-Jen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander G. Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Crossword Solving. (arXiv:2205.09665v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.09665","description":"<p>We present the Berkeley Crossword Solver, a state-of-the-art approach for\nautomatically solving crossword puzzles. Our system works by generating answer\ncandidates for each crossword clue using neural question answering models and\nthen combines loopy belief propagation with local search to find full puzzle\nsolutions. Compared to existing approaches, our system improves exact puzzle\naccuracy from 71% to 82% on crosswords from The New York Times and obtains\n99.9% letter accuracy on themeless puzzles. Additionally, in 2021, a hybrid of\nour system and the existing Dr.Fill system outperformed all human competitors\nfor the first time at the American Crossword Puzzle Tournament. To facilitate\nresearch on question answering and crossword solving, we analyze our system's\nremaining errors and release a dataset of over six million question-answer\npairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallace_E/0/1/0/all/0/1\">Eric Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlin_N/0/1/0/all/0/1\">Nicholas Tomlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Albert Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_E/0/1/0/all/0/1\">Eshaan Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsberg_M/0/1/0/all/0/1\">Matthew Ginsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"hmBERT: Historical Multilingual Language Models for Named Entity Recognition. (arXiv:2205.15575v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15575","description":"<p>Compared to standard Named Entity Recognition (NER), identifying persons,\nlocations, and organizations in historical texts constitutes a big challenge.\nTo obtain machine-readable corpora, the historical text is usually scanned and\nOptical Character Recognition (OCR) needs to be performed. As a result, the\nhistorical corpora contain errors. Also, entities like location or organization\ncan change over time, which poses another challenge. Overall, historical texts\ncome with several peculiarities that differ greatly from modern texts and large\nlabeled corpora for training a neural tagger are hardly available for this\ndomain. In this work, we tackle NER for historical German, English, French,\nSwedish, and Finnish by training large historical language models. We\ncircumvent the need for large amounts of labeled data by using unlabeled data\nfor pretraining a language model. We propose hmBERT, a historical multilingual\nBERT-based language model, and release the model in several versions of\ndifferent sizes. Furthermore, we evaluate the capability of hmBERT by solving\ndownstream NER as part of this year's HIPE-2022 shared task and provide\ndetailed analysis and insights. For the Multilingual Classical Commentary\ncoarse-grained NER challenge, our tagger HISTeria outperforms the other teams'\nmodels for two out of three languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schweter_S/0/1/0/all/0/1\">Stefan Schweter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_K/0/1/0/all/0/1\">Katharina Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_E/0/1/0/all/0/1\">Erion &#xc7;ano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Key Event Detection from Massive Text Corpora. (arXiv:2206.04153v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04153","description":"<p>Automated event detection from news corpora is a crucial task towards mining\nfast-evolving structured knowledge. As real-world events have different\ngranularities, from the top-level themes to key events and then to event\nmentions corresponding to concrete actions, there are generally two lines of\nresearch: (1) theme detection identifies from a news corpus major themes (e.g.,\n\"2019 Hong Kong Protests\" vs. \"2020 U.S. Presidential Election\") that have very\ndistinct semantics; and (2) action extraction extracts from one document\nmention-level actions (e.g., \"the police hit the left arm of the protester\")\nthat are too fine-grained for comprehending the event. In this paper, we\npropose a new task, key event detection at the intermediate level, aiming to\ndetect from a news corpus key events (e.g., \"HK Airport Protest on Aug.\n12-14\"), each happening at a particular time/location and focusing on the same\ntopic. This task can bridge event understanding and structuring and is\ninherently challenging because of the thematic and temporal closeness of key\nevents and the scarcity of labeled data due to the fast-evolving nature of news\narticles. To address these challenges, we develop an unsupervised key event\ndetection framework, EvMine, that (1) extracts temporally frequent peak phrases\nusing a novel ttf-itf score, (2) merges peak phrases into event-indicative\nfeature sets by detecting communities from our designed peak phrase graph that\ncaptures document co-occurrences, semantic similarities, and temporal closeness\nsignals, and (3) iteratively retrieves documents related to each key event by\ntraining a classifier with automatically generated pseudo labels from the\nevent-indicative feature sets and refining the detected key events using the\nretrieved documents. Extensive experiments and case studies show EvMine\noutperforms all the baseline methods and its ablations on two real-world news\ncorpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling Transformers with LEGO: a synthetic reasoning task. (arXiv:2206.04301v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.04301","description":"<p>We propose a synthetic task, LEGO (Learning Equality and Group Operations),\nthat encapsulates the problem of following a chain of reasoning, and we study\nhow the transformer architectures learn this task. We pay special attention to\ndata effects such as pretraining (on seemingly unrelated NLP tasks) and dataset\ncomposition (e.g., differing chain length at training and test time), as well\nas architectural variants such as weight-tied layers or adding convolutional\ncomponents. We study how the trained models eventually succeed at the task, and\nin particular, we are able to understand (to some extent) some of the attention\nheads as well as how the information flows in the network. Based on these\nobservations we propose a hypothesis that here pretraining helps merely due to\nbeing a smart initialization rather than some deep knowledge stored in the\nnetwork. We also observe that in some data regime the trained transformer finds\n\"shortcut\" solutions to follow the chain of reasoning, which impedes the\nmodel's ability to generalize to simple variants of the main task, and moreover\nwe find that one can prevent such shortcut with appropriate architecture\nmodification or careful data preparation. Motivated by our findings, we begin\nto explore the task of learning to execute C programs, where a convolutional\nmodification to transformers, namely adding convolutional structures in the\nkey/query/value maps, shows an encouraging edge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1\">Tal Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.05895","description":"<p>Latent space Energy-Based Models (EBMs), also known as energy-based priors,\nhave drawn growing interests in generative modeling. Fueled by its flexibility\nin the formulation and strong modeling power of the latent space, recent works\nbuilt upon it have made interesting attempts aiming at the interpretability of\ntext modeling. However, latent space EBMs also inherit some flaws from EBMs in\ndata space; the degenerate MCMC sampling quality in practice can lead to poor\ngeneration quality and instability in training, especially on data with complex\nlatent structures. Inspired by the recent efforts that leverage diffusion\nrecovery likelihood learning as a cure for the sampling issue, we introduce a\nnovel symbiosis between the diffusion models and latent space EBMs in a\nvariational learning framework, coined as the latent diffusion energy-based\nmodel. We develop a geometric clustering-based regularization jointly with the\ninformation bottleneck to further improve the quality of the learned latent\nspace. Experiments on several challenging tasks demonstrate the superior\nperformance of our model on interpretable text modeling over strong\ncounterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peiyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sirui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1\">Baoxiong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing informativeness of an NLG chatbot vs graphical app in diet-information domain. (arXiv:2206.13435v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.13435","description":"<p>Visual representation of data like charts and tables can be challenging to\nunderstand for readers. Previous work showed that combining visualisations with\ntext can improve the communication of insights in static contexts, but little\nis known about interactive ones. In this work we present an NLG chatbot that\nprocesses natural language queries and provides insights through a combination\nof charts and text. We apply it to nutrition, a domain communication quality is\ncritical. Through crowd-sourced evaluation we compare the informativeness of\nour chatbot against traditional, static diet-apps. We find that the\nconversational context significantly improved users' understanding of dietary\ndata in various tasks, and that users considered the chatbot as more useful and\nquick to use than traditional apps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balloccu_S/0/1/0/all/0/1\">Simone Balloccu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Range Language Modeling via Gated State Spaces. (arXiv:2206.13947v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.13947","description":"<p>State space models have shown to be effective at modeling long range\ndependencies, specially on sequence classification tasks. In this work we focus\non autoregressive sequence modeling over English books, Github source code and\nArXiv mathematics articles. Based on recent developments around the\neffectiveness of gated activation functions, we propose a new layer named Gated\nState Space (GSS) and show that it trains significantly faster than the\ndiagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several\nwell-tuned Transformer-based baselines and exhibits zero-shot generalization to\nlonger inputs while being straightforward to implement. Finally, we show that\nleveraging self-attention to model local dependencies improves the performance\nof GSS even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it possible not to cheat on the Turing Test_Exploring the potential and challenges for true natural language 'understanding' by computers. (arXiv:2206.14672v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14672","description":"<p>The increasing sophistication of NLP models has renewed optimism regarding\nmachines achieving a full human-like command of natural language. Whilst work\nin NLP/NLU may have made great strides in that direction, the lack of\nconceptual clarity in how 'understanding' is used in this and other disciplines\nhave made it difficult to discern how close we actually are. A critical,\ninterdisciplinary review of current approaches and remaining challenges is yet\nto be carried out. Beyond linguistic knowledge, this requires considering our\nspecies-specific capabilities to categorize, memorize, label and communicate\nour (sufficiently similar) embodied and situated experiences. Moreover, gauging\nthe practical constraints requires critically analyzing the technical\ncapabilities of current models, as well as deeper philosophical reflection on\ntheoretical possibilities and limitations. In this paper, I unite all of these\nperspectives -- the philosophical, cognitive-linguistic, and technical -- to\nunpack the challenges involved in approaching true (human-like) language\nunderstanding. By unpacking the theoretical assumptions inherent in current\napproaches, I hope to illustrate how far we actually are from achieving this\ngoal, if indeed it is the goal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberts_L/0/1/0/all/0/1\">Lize Alberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Effects of Hyperparameters on Knowledge Graph Embedding Quality. (arXiv:2207.00473v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2207.00473","description":"<p>Embedding knowledge graphs into low-dimensional spaces is a popular method\nfor applying approaches, such as link prediction or node classification, to\nthese databases. This embedding process is very costly in terms of both\ncomputational time and space. Part of the reason for this is the optimisation\nof hyperparameters, which involves repeatedly sampling, by random, guided, or\nbrute-force selection, from a large hyperparameter space and testing the\nresulting embeddings for their quality. However, not all hyperparameters in\nthis search space will be equally important. In fact, with prior knowledge of\nthe relative importance of the hyperparameters, some could be eliminated from\nthe search altogether without significantly impacting the overall quality of\nthe outputted embeddings. To this end, we ran a Sobol sensitivity analysis to\nevaluate the effects of tuning different hyperparameters on the variance of\nembedding quality. This was achieved by performing thousands of embedding\ntrials, each time measuring the quality of embeddings produced by different\nhyperparameter configurations. We regressed the embedding quality on those\nhyperparameter configurations, using this model to generate Sobol sensitivity\nindices for each of the hyperparameters. By evaluating the correlation between\nSobol indices, we find substantial variability in the hyperparameter\nsensitivities between knowledge graphs, with differing dataset characteristics\nbeing the probable cause of these inconsistencies. As an additional\ncontribution of this work we identify several relations in the UMLS knowledge\ngraph that may cause data leakage via inverse relations, and derive and present\nUMLS-43, a leakage-robust variant of that graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lloyd_O/0/1/0/all/0/1\">Oliver Lloyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaunt_T/0/1/0/all/0/1\">Tom Gaunt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt. (arXiv:2206.07137v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2206.07137","description":"<p>Training on web-scale data can take months. But most computation and time is\nwasted on redundant and noisy points that are already learnt or not learnable.\nTo accelerate training, we introduce Reducible Holdout Loss Selection\n(RHO-LOSS), a simple but principled technique which selects approximately those\npoints for training that most reduce the model's generalization loss. As a\nresult, RHO-LOSS mitigates the weaknesses of existing data selection methods:\ntechniques from the optimization literature typically select 'hard' (e.g. high\nloss) points, but such points are often noisy (not learnable) or less\ntask-relevant. Conversely, curriculum learning prioritizes 'easy' points, but\nsuch points need not be trained on once learned. In contrast, RHO-LOSS selects\npoints that are learnable, worth learning, and not yet learnt. RHO-LOSS trains\nin far fewer steps than prior art, improves accuracy, and speeds up training on\na wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and\nBERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in\n18x fewer steps and reaches 2% higher final accuracy than uniform data\nshuffling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1\">S&#xf6;ren Mindermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_M/0/1/0/all/0/1\">Muhammed Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mrinank Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtgen_B/0/1/0/all/0/1\">Benedikt H&#xf6;ltgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Aidan N. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Feature-selected Graph Spatial Attention Network for Addictive Brain-Networks Identification. (arXiv:2207.00583v1 [q-bio.NC])","link":"http://arxiv.org/abs/2207.00583","description":"<p>Functional alterations in the relevant neural circuits occur from drug\naddiction over a certain period. And these significant alterations are also\nrevealed by analyzing fMRI. However, because of fMRI's high dimensionality and\npoor signal-to-noise ratio, it is challenging to encode efficient and robust\nbrain regional embeddings for both graph-level identification and region-level\nbiomarkers detection tasks between nicotine addiction (NA) and healthy control\n(HC) groups. In this work, we represent the fMRI of the rat brain as a graph\nwith biological attributes and propose a novel feature-selected graph spatial\nattention network(FGSAN) to extract the biomarkers of addiction and identify\nfrom these brain networks. Specially, a graph spatial attention encoder is\nemployed to capture the features of spatiotemporal brain networks with spatial\ninformation. The method simultaneously adopts a Bayesian feature selection\nstrategy to optimize the model and improve classification task by constraining\nfeatures. Experiments on an addiction-related neural imaging dataset show that\nthe proposed model can obtain superior performance and detect interpretable\nbiomarkers associated with addiction-relevant neural circuits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Gong_C/0/1/0/all/0/1\">Changwei Gong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jing_C/0/1/0/all/0/1\">Changhong Jing</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pan_J/0/1/0/all/0/1\">Junren Pan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_S/0/1/0/all/0/1\">Shuqiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrUE: Distilling Knowledge from Sparse Teacher Networks. (arXiv:2207.00586v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00586","description":"<p>Although deep neural networks have enjoyed remarkable success across a wide\nvariety of tasks, their ever-increasing size also imposes significant overhead\non deployment. To compress these models, knowledge distillation was proposed to\ntransfer knowledge from a cumbersome (teacher) network into a lightweight\n(student) network. However, guidance from a teacher does not always improve the\ngeneralization of students, especially when the size gap between student and\nteacher is large. Previous works argued that it was due to the high certainty\nof the teacher, resulting in harder labels that were difficult to fit. To\nsoften these labels, we present a pruning method termed Prediction Uncertainty\nEnlargement (PrUE) to simplify the teacher. Specifically, our method aims to\ndecrease the teacher's certainty about data, thereby generating soft\npredictions for students. We empirically investigate the effectiveness of the\nproposed method with experiments on CIFAR-10/100, Tiny-ImageNet, and ImageNet.\nResults indicate that student networks trained with sparse teachers achieve\nbetter performance. Besides, our method allows researchers to distill knowledge\nfrom deeper networks to improve students further. Our code is made public at:\n\\url{https://github.com/wangshaopu/prue}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaopu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_M/0/1/0/all/0/1\">Mengzhen Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jinqiao Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pair-Relationship Modeling for Latent Fingerprint Recognition. (arXiv:2207.00587v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00587","description":"<p>Latent fingerprints are important for identifying criminal suspects. However,\nrecognizing a latent fingerprint in a collection of reference fingerprints\nremains a challenge. Most, if not all, of existing methods would extract\nrepresentation features of each fingerprint independently and then compare the\nsimilarity of these representation features for recognition in a different\nprocess. Without the supervision of similarity for the feature extraction\nprocess, the extracted representation features are hard to optimally reflect\nthe similarity of the two compared fingerprints which is the base for matching\ndecision making. In this paper, we propose a new scheme that can model the\npair-relationship of two fingerprints directly as the similarity feature for\nrecognition. The pair-relationship is modeled by a hybrid deep network which\ncan handle the difficulties of random sizes and corrupted areas of latent\nfingerprints. Experimental results on two databases show that the proposed\nmethod outperforms the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuefei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiuping Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiankun Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoVA: Exploiting Compressed-Domain Analysis to Accelerate Video Analytics. (arXiv:2207.00588v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00588","description":"<p>Modern retrospective analytics systems leverage cascade architecture to\nmitigate bottleneck for computing deep neural networks (DNNs). However, the\nexisting cascades suffer two limitations: (1) decoding bottleneck is either\nneglected or circumvented, paying significant compute and storage cost for\npre-processing; and (2) the systems are specialized for temporal queries and\nlack spatial query support. This paper presents CoVA, a novel cascade\narchitecture that splits the cascade computation between compressed domain and\npixel domain to address the decoding bottleneck, supporting both temporal and\nspatial queries. CoVA cascades analysis into three major stages where the first\ntwo stages are performed in compressed domain while the last one in pixel\ndomain. First, CoVA detects occurrences of moving objects (called blobs) over a\nset of compressed frames (called tracks). Then, using the track results, CoVA\nprudently selects a minimal set of frames to obtain the label information and\nonly decode them to compute the full DNNs, alleviating the decoding bottleneck.\nLastly, CoVA associates tracks with labels to produce the final analysis\nresults on which users can process both temporal and spatial queries. Our\nexperiments demonstrate that CoVA offers 4.8x throughput improvement over\nmodern cascade systems, while imposing modest accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jinwoo Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1\">Seungho Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonsung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dohee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1\">Hardik Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongse Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSD-Faster Net: A Hybrid Network for Industrial Defect Inspection. (arXiv:2207.00589v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00589","description":"<p>The quality of industrial components is critical to the production of special\nequipment such as robots. Defect inspection of these components is an efficient\nway to ensure quality. In this paper, we propose a hybrid network, SSD-Faster\nNet, for industrial defect inspection of rails, insulators, commutators etc.\nSSD-Faster Net is a two-stage network, including SSD for quickly locating\ndefective blocks, and an improved Faster R-CNN for defect segmentation. For the\nformer, we propose a novel slice localization mechanism to help SSD scan\nquickly. The second stage is based on improved Faster R-CNN, using FPN,\ndeformable kernel(DK) to enhance representation ability. It fuses multi-scale\ninformation, and self-adapts the receptive field. We also propose a novel loss\nfunction and use ROI Align to improve accuracy. Experiments show that our\nSSD-Faster Net achieves an average accuracy of 84.03%, which is 13.42% higher\nthan the nearest competitor based on Faster R-CNN, 4.14% better than GAN-based\nmethods, more than 10% higher than that of DNN-based detectors. And the\ncomputing speed is improved by nearly 7%, which proves its robustness and\nsuperior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Naigong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViRel: Unsupervised Visual Relations Discovery with Graph-level Analogy. (arXiv:2207.00590v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00590","description":"<p>Visual relations form the basis of understanding our compositional world, as\nrelationships between visual objects capture key information in a scene. It is\nthen advantageous to learn relations automatically from the data, as learning\nwith predefined labels cannot capture all possible relations. However, current\nrelation learning methods typically require supervision, and are not designed\nto generalize to scenes with more complicated relational structures than those\nseen during training. Here, we introduce ViRel, a method for unsupervised\ndiscovery and learning of Visual Relations with graph-level analogy. In a\nsetting where scenes within a task share the same underlying relational\nsubgraph structure, our learning method of contrasting isomorphic and\nnon-isomorphic graphs discovers the relations across tasks in an unsupervised\nmanner. Once the relations are learned, ViRel can then retrieve the shared\nrelational graph structure for each task by parsing the predicted relational\nstructure. Using a dataset based on grid-world and the Abstract Reasoning\nCorpus, we show that our method achieves above 95% accuracy in relation\nclassification, discovers the relation graph structure for most tasks, and\nfurther generalizes to unseen tasks with more complicated relational\nstructures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Daniel Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tailin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification of Binary Neutron Star Mergers in Gravitational-Wave Data Using YOLO One-Shot Object Detection. (arXiv:2207.00591v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2207.00591","description":"<p>We demonstrate the application of the YOLOv5 model, a general purpose\nconvolution-based single-shot object detection model, in the task of detecting\nbinary neutron star (BNS) coalescence events from gravitational-wave data of\ncurrent generation interferometer detectors. We also present a thorough\nexplanation of the synthetic data generation and preparation tasks based on\napproximant waveform models used for the model training, validation and testing\nsteps. Using this approach, we achieve mean average precision\n($\\text{mAP}_{[0.50]}$) values of 0.945 for a single class validation dataset\nand as high as 0.978 for test datasets. Moreover, the trained model is\nsuccessful in identifying the GW170817 event in the LIGO H1 detector data. The\nidentification of this event is also possible for the LIGO L1 detector data\nwith an additional pre-processing step, without the need of removing the large\nglitch in the final stages of the inspiral. The detection of the GW190425 event\nis less successful, which attests to performance degradation with the\nsignal-to-noise ratio. Our study indicates that the YOLOv5 model is an\ninteresting approach for first-stage detection alarm pipelines and, when\nintegrated in more complex pipelines, for real-time inference of physical\nsource parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Aveiro_J/0/1/0/all/0/1\">Jo&#xe3;o Aveiro</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Freitas_F/0/1/0/all/0/1\">Felipe F. Freitas</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ferreira_M/0/1/0/all/0/1\">M&#xe1;rcio Ferreira</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Onofre_A/0/1/0/all/0/1\">Antonio Onofre</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Providencia_C/0/1/0/all/0/1\">Constan&#xe7;a Provid&#xea;ncia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Goncalves_G/0/1/0/all/0/1\">Gon&#xe7;alo Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Font_J/0/1/0/all/0/1\">Jos&#xe9; A. Font</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRESS: Dynamic REal-time Sparse Subnets. (arXiv:2207.00670v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00670","description":"<p>The limited and dynamically varied resources on edge devices motivate us to\ndeploy an optimized deep neural network that can adapt its sub-networks to fit\nin different resource constraints. However, existing works often build\nsub-networks through searching different network architectures in a\nhand-crafted sampling space, which not only can result in a subpar performance\nbut also may cause on-device re-configuration overhead. In this paper, we\npropose a novel training algorithm, Dynamic REal-time Sparse Subnets (DRESS).\nDRESS samples multiple sub-networks from the same backbone network through\nrow-based unstructured sparsity, and jointly trains these sub-networks in\nparallel with weighted loss. DRESS also exploits strategies including parameter\nreusing and row-based fine-grained sampling for efficient storage consumption\nand efficient on-device adaptation. Extensive experiments on public vision\ndatasets show that DRESS yields significantly higher accuracy than\nstate-of-the-art sub-networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1\">Zhongnan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Syed Shakib Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuecheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumbul_E/0/1/0/all/0/1\">Ekin Sumbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvo_B/0/1/0/all/0/1\">Barbara De Salvo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"American == White in Multimodal Language-and-Image AI. (arXiv:2207.00691v1 [cs.CY])","link":"http://arxiv.org/abs/2207.00691","description":"<p>Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP,\nare evaluated for evidence of a bias previously observed in social and\nexperimental psychology: equating American identity with being White. Embedding\nassociation tests (EATs) using standardized images of self-identified Asian,\nBlack, Latina/o, and White individuals from the Chicago Face Database (CFD)\nreveal that White individuals are more associated with collective in-group\nwords than are Asian, Black, or Latina/o individuals. In assessments of three\ncore aspects of American identity reported by social psychologists,\nsingle-category EATs reveal that images of White individuals are more\nassociated with patriotism and with being born in America, but that, consistent\nwith prior findings in psychology, White individuals are associated with being\nless likely to treat people of all races and backgrounds equally. Three\ndownstream machine learning tasks demonstrate biases associating American with\nWhite. In a visual question answering task using BLIP, 97% of White individuals\nare identified as American, compared to only 3% of Asian individuals. When\nasked in what state the individual depicted lives in, the model responds China\n53% of the time for Asian individuals, but always with an American state for\nWhite individuals. In an image captioning task, BLIP remarks upon the race of\nAsian individuals as much as 36% of the time, but never remarks upon race for\nWhite individuals. Finally, provided with an initialization image from the CFD\nand the text \"an American person,\" a synthetic image generator (VQGAN) using\nthe text-based guidance of CLIP lightens the skin tone of individuals of all\nraces (by 35% for Black individuals, based on pixel brightness). The results\nindicate that biases equating American identity with being White are learned by\nlanguage-and-image AI, and propagate to downstream applications of such models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot incremental learning in the context of solar cell quality inspection. (arXiv:2207.00693v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00693","description":"<p>In industry, Deep Neural Networks have shown high defect detection rates\nsurpassing other more traditional manual feature engineering based proposals.\nThis has been achieved mainly through supervised training where a great amount\nof data is required in order to learn good classification models. However, such\namount of data is sometimes hard to obtain in industrial scenarios, as few\ndefective pieces are produced normally. In addition, certain kinds of defects\nare very rare and usually just appear from time to time, which makes the\ngeneration of a proper dataset for training a classification model even harder.\nMoreover, the lack of available data limits the adaptation of inspection models\nto new defect types that appear in production as it might require a model\nretraining in order to incorporate the detects and detect them. In this work,\nwe have explored the technique of weight imprinting in the context of solar\ncell quality inspection where we have trained a network on three base defect\nclasses, and then we have incorporated new defect classes using few samples.\nThe results have shown that this technique allows the network to extend its\nknowledge with regard to defect classes with few samples, which can be\ninteresting for industrial practitioners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balzategui_J/0/1/0/all/0/1\">Julen Balzategui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eciolaza_L/0/1/0/all/0/1\">Luka Eciolaza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turning to a Teacher for Timestamp Supervised Temporal Action Segmentation. (arXiv:2207.00712v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00712","description":"<p>Temporal action segmentation in videos has drawn much attention recently.\nTimestamp supervision is a cost-effective way for this task. To obtain more\ninformation to optimize the model, the existing method generated pseudo\nframe-wise labels iteratively based on the output of a segmentation model and\nthe timestamp annotations. However, this practice may introduce noise and\noscillation during the training, and lead to performance degeneration. To\naddress this problem, we propose a new framework for timestamp supervised\ntemporal action segmentation by introducing a teacher model parallel to the\nsegmentation model to help stabilize the process of model optimization. The\nteacher model can be seen as an ensemble of the segmentation model, which helps\nto suppress the noise and to improve the stability of pseudo labels. We further\nintroduce a segmentally smoothing loss, which is more focused and cohesive, to\nenforce the smooth transition of the predicted probabilities within action\ninstances. The experiments on three datasets show that our method outperforms\nthe state-of-the-art method and performs comparably against the\nfully-supervised methods at a much lower annotation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise and Edge Based Dual Branch Image Manipulation Detection. (arXiv:2207.00724v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00724","description":"<p>Unlike ordinary computer vision tasks that focus more on the semantic content\nof images, the image manipulation detection task pays more attention to the\nsubtle information of image manipulation. In this paper, the noise image\nextracted by the improved constrained convolution is used as the input of the\nmodel instead of the original image to obtain more subtle traces of\nmanipulation. Meanwhile, the dual-branch network, consisting of a\nhigh-resolution branch and a context branch, is used to capture the traces of\nartifacts as much as possible. In general, most manipulation leaves\nmanipulation artifacts on the manipulation edge. A specially designed\nmanipulation edge detection module is constructed based on the dual-branch\nnetwork to identify these artifacts better. The correlation between pixels in\nan image is closely related to their distance. The farther the two pixels are,\nthe weaker the correlation. We add a distance factor to the self-attention\nmodule to better describe the correlation between pixels. Experimental results\non four publicly available image manipulation datasets demonstrate the\neffectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale Attentive Image De-raining Networks via Neural Architecture Search. (arXiv:2207.00728v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00728","description":"<p>Multi-scale architectures and attention modules have shown effectiveness in\nmany deep learning-based image de-raining methods. However, manually designing\nand integrating these two components into a neural network requires a bulk of\nlabor and extensive expertise. In this article, a high-performance multi-scale\nattentive neural architecture search (MANAS) framework is technically developed\nfor image deraining. The proposed method formulates a new multi-scale attention\nsearch space with multiple flexible modules that are favorite to the image\nde-raining task. Under the search space, multi-scale attentive cells are built,\nwhich are further used to construct a powerful image de-raining network. The\ninternal multiscale attentive architecture of the de-raining network is\nsearched automatically through a gradient-based search algorithm, which avoids\nthe daunting procedure of the manual design to some extent. Moreover, in order\nto obtain a robust image de-raining model, a practical and effective\nmulti-to-one training strategy is also presented to allow the de-raining\nnetwork to get sufficient background information from multiple rainy images\nwith the same background scene, and meanwhile, multiple loss functions\nincluding external loss, internal loss, architecture regularization loss, and\nmodel complexity loss are jointly optimized to achieve robust de-raining\nperformance and controllable model complexity. Extensive experimental results\non both synthetic and realistic rainy images, as well as the down-stream vision\napplications (i.e., objection detection and segmentation) consistently\ndemonstrate the superiority of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Lei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuli Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_W/0/1/0/all/0/1\">Wanliang Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Youjun Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huanqiang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SketchCleanNet -- A deep learning approach to the enhancement and correction of query sketches for a 3D CAD model retrieval system. (arXiv:2207.00732v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00732","description":"<p>Search and retrieval remains a major research topic in several domains,\nincluding computer graphics, computer vision, engineering design, etc. A search\nengine requires primarily an input search query and a database of items to\nsearch from. In engineering, which is the primary context of this paper, the\ndatabase consists of 3D CAD models, such as washers, pistons, connecting rods,\netc. A query from a user is typically in the form of a sketch, which attempts\nto capture the details of a 3D model. However, sketches have certain typical\ndefects such as gaps, over-drawn portions (multi-strokes), etc. Since the\nretrieved results are only as good as the input query, sketches need\ncleaning-up and enhancement for better retrieval results.\n</p>\n<p>In this paper, a deep learning approach is proposed to improve or clean the\nquery sketches. Initially, sketches from various categories are analysed in\norder to understand the many possible defects that may occur. A dataset of\ncleaned-up or enhanced query sketches is then created based on an understanding\nof these defects. Consequently, an end-to-end training of a deep neural network\nis carried out in order to provide a mapping between the defective and the\nclean sketches. This network takes the defective query sketch as the input and\ngenerates a clean or an enhanced query sketch. Qualitative and quantitative\ncomparisons of the proposed approach with other state-of-the-art techniques\nshow that the proposed approach is effective. The results of the search engine\nare reported using both the defective and enhanced query sketches, and it is\nshown that using the enhanced query sketches from the developed approach yields\nimproved search results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manda_B/0/1/0/all/0/1\">Bharadwaj Manda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kendre_P/0/1/0/all/0/1\">Prasad Kendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Subhrajit Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthuganapathy_R/0/1/0/all/0/1\">Ramanathan Muthuganapathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation Learning and Retrieval. (arXiv:2207.00733v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00733","description":"<p>Recently, the cross-modal pre-training task has been a hotspot because of its\nwide application in various down-streaming researches including retrieval,\ncaptioning, question answering and so on. However, exiting methods adopt a\none-stream pre-training model to explore the united vision-language\nrepresentation for conducting cross-modal retrieval, which easily suffer from\nthe calculation explosion. Moreover, although the conventional double-stream\nstructures are quite efficient, they still lack the vital cross-modal\ninteractions, resulting in low performances. Motivated by these challenges, we\nput forward a Contrastive Cross-Modal Knowledge Sharing Pre-training (COOKIE)\nto grasp the joint text-image representations. Structurally, COOKIE adopts the\ntraditional double-stream structure because of the acceptable time consumption.\nTo overcome the inherent defects of double-stream structure as mentioned above,\nwe elaborately design two effective modules. Concretely, the first module is a\nweight-sharing transformer that builds on the head of the visual and textual\nencoders, aiming to semantically align text and image. This design enables\nvisual and textual paths focus on the same semantics. The other one is three\nspecially designed contrastive learning, aiming to share knowledge between\ndifferent models. The shared cross-modal knowledge develops the study of\nunimodal representation greatly, promoting the single-modal retrieval tasks.\nExtensive experimental results on multi-modal matching researches that includes\ncross-modal retrieval, text matching, and image retrieval reveal the superiors\nin calculation efficiency and statistical indicators of our pre-training model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_K/0/1/0/all/0/1\">Keyu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhenshan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1\">Qingrong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaodong Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Golfer: Trajectory Prediction with Masked Goal Conditioning MnM Network. (arXiv:2207.00738v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00738","description":"<p>Transformers have enabled breakthroughs in NLP and computer vision, and have\nrecently began to show promising performance in trajectory prediction for\nAutonomous Vehicle (AV). How to efficiently model the interactive relationships\nbetween the ego agent and other road and dynamic objects remains challenging\nfor the standard attention module. In this work we propose a general\nTransformer-like architectural module MnM network equipped with novel masked\ngoal conditioning training procedures for AV trajectory prediction. The\nresulted model, named golfer, achieves state-of-the-art performance, winning\nthe 2nd place in the 2022 Waymo Open Dataset Motion Prediction Challenge and\nranked 1st place according to minADE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshkevari_S/0/1/0/all/0/1\">Soheil Sadeghi Eshkevari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weidan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_W/0/1/0/all/0/1\">Wei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoming Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaussian Kernel-based Cross Modal Network for Spatio-Temporal Video Grounding. (arXiv:2207.00744v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00744","description":"<p>Spatial-Temporal Video Grounding (STVG) is a challenging task which aims to\nlocalize the spatio-temporal tube of the interested object semantically\naccording to a natural language query. Most previous works not only severely\nrely on the anchor boxes extracted by Faster R-CNN, but also simply regard the\nvideo as a series of individual frames, thus lacking their temporal modeling.\nInstead, in this paper, we are the first to propose an anchor-free framework\nfor STVG, called Gaussian Kernel-based Cross Modal Network (GKCMN).\nSpecifically, we utilize the learned Gaussian Kernel-based heatmaps of each\nvideo frame to locate the query-related object. A mixed serial and parallel\nconnection network is further developed to leverage both spatial and temporal\nrelations among frames for better grounding. Experimental results on VidSTG\ndataset demonstrate the effectiveness of our proposed GKCMN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zeyu Xiong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a> (1) ((1) The Hubei Engineering Research Center on Big Data Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology, (2) Wangxuan Institute of Computer Technology, Peking University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhotoScene: Photorealistic Material and Lighting Transfer for Indoor Scenes. (arXiv:2207.00757v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00757","description":"<p>Most indoor 3D scene reconstruction methods focus on recovering 3D geometry\nand scene layout. In this work, we go beyond this to propose PhotoScene, a\nframework that takes input image(s) of a scene along with approximately aligned\nCAD geometry (either reconstructed automatically or manually specified) and\nbuilds a photorealistic digital twin with high-quality materials and similar\nlighting. We model scene materials using procedural material graphs; such\ngraphs represent photorealistic and resolution-independent materials. We\noptimize the parameters of these graphs and their texture scale and rotation,\nas well as the scene lighting to best match the input image via a\ndifferentiable rendering layer. We evaluate our technique on objects and layout\nreconstructions from ScanNet, SUN RGB-D and stock photographs, and demonstrate\nthat our method reconstructs high-quality, fully relightable 3D scenes that can\nbe re-rendered under arbitrary viewpoints, zooms and lighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yu-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hold_Geoffroy_Y/0/1/0/all/0/1\">Yannick Hold-Geoffroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Milo&#x161; Ha&#x161;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attack is A Devil in Federated GAN-based Medical Image Synthesis. (arXiv:2207.00762v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00762","description":"<p>Deep Learning-based image synthesis techniques have been applied in\nhealthcare research for generating medical images to support open research.\nTraining generative adversarial neural networks (GAN) usually requires large\namounts of training data. Federated learning (FL) provides a way of training a\ncentral model using distributed data from different medical institutions while\nkeeping raw data locally. However, FL is vulnerable to backdoor attack, an\nadversarial by poisoning training data, given the central server cannot access\nthe original data directly. Most backdoor attack strategies focus on\nclassification models and centralized domains. In this study, we propose a way\nof attacking federated GAN (FedGAN) by treating the discriminator with a\ncommonly used data poisoning strategy in backdoor attack classification models.\nWe demonstrate that adding a small trigger with size less than 0.5 percent of\nthe original image size can corrupt the FL-GAN model. Based on the proposed\nattack, we provide two effective defense strategies: global malicious detection\nand local training regularization. We show that combining the two defense\nstrategies yields a robust medical image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruinan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift. (arXiv:2207.00769v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00769","description":"<p>Class distribution plays an important role in learning deep classifiers. When\nthe proportion of each class in the test set differs from the training set, the\nperformance of classification nets usually degrades. Such a label distribution\nshift problem is common in medical diagnosis since the prevalence of disease\nvary over location and time. In this paper, we propose the first method to\ntackle label shift for medical image classification, which effectively adapt\nthe model learned from a single training label distribution to arbitrary\nunknown test label distribution. Our approach innovates distribution\ncalibration to learn multiple representative classifiers, which are capable of\nhandling different one-dominating-class distributions. When given a test image,\nthe diverse classifiers are dynamically aggregated via the consistency-driven\ntest-time adaptation, to deal with the unknown test label distribution. We\nvalidate our method on two important medical image classification tasks\nincluding liver fibrosis staging and COVID-19 severity prediction. Our\nexperiments clearly show the decreased model performance under label shift.\nWith our method, model performance significantly improves on all the test\ndatasets with different label shifts for both medical image diagnosis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_W/0/1/0/all/0/1\">Wenao Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Shuang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huimao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cross-Image Object Semantic Relation in Transformer for Few-Shot Fine-Grained Image Classification. (arXiv:2207.00784v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00784","description":"<p>Few-shot fine-grained learning aims to classify a query image into one of a\nset of support categories with fine-grained differences. Although learning\ndifferent objects' local differences via Deep Neural Networks has achieved\nsuccess, how to exploit the query-support cross-image object semantic relations\nin Transformer-based architecture remains under-explored in the few-shot\nfine-grained scenario. In this work, we propose a Transformer-based\ndouble-helix model, namely HelixFormer, to achieve the cross-image object\nsemantic relation mining in a bidirectional and symmetrical manner. The\nHelixFormer consists of two steps: 1) Relation Mining Process (RMP) across\ndifferent branches, and 2) Representation Enhancement Process (REP) within each\nindividual branch. By the designed RMP, each branch can extract fine-grained\nobject-level Cross-image Semantic Relation Maps (CSRMs) using information from\nthe other branch, ensuring better cross-image interaction in semantically\nrelated local object regions. Further, with the aid of CSRMs, the developed REP\ncan strengthen the extracted features for those discovered semantically-related\nlocal regions in each branch, boosting the model's ability to distinguish\nsubtle feature differences of fine-grained objects. Extensive experiments\nconducted on five public fine-grained benchmarks demonstrate that HelixFormer\ncan effectively enhance the cross-image object semantic relation matching for\nrecognizing fine-grained objects, achieving much better performance over most\nstate-of-the-art methods under 1-shot and 5-shot scenarios. Our code is\navailable at: https://github.com/JiakangYuan/HelixFormer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiakang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiayuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Botian Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Representations as Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation. (arXiv:2207.00787v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00787","description":"<p>Iterative refinement -- start with a random guess, then iteratively improve\nthe guess -- is a useful paradigm for representation learning because it offers\na way to break symmetries among equally plausible explanations for the data.\nThis property enables the application of such methods to infer representations\nof sets of entities, such as objects in physical scenes, structurally\nresembling clustering algorithms in latent space. However, most prior works\ndifferentiate through the unrolled refinement process, which can make\noptimization challenging. We observe that such methods can be made\ndifferentiable by means of the implicit function theorem, and develop an\nimplicit differentiation approach that improves the stability and tractability\nof training by decoupling the forward and backward passes. This connection\nenables us to apply advances in optimizing implicit layers to not only improve\nthe optimization of the slot attention module in SLATE, a state-of-the-art\nmethod for learning entity representations, but do so with constant space and\ntime complexity in backpropagation and only one additional line of code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Michael Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary-Guided Camouflaged Object Detection. (arXiv:2207.00794v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00794","description":"<p>Camouflaged object detection (COD), segmenting objects that are elegantly\nblended into their surroundings, is a valuable yet challenging task. Existing\ndeep-learning methods often fall into the difficulty of accurately identifying\nthe camouflaged object with complete and fine object structure. To this end, in\nthis paper, we propose a novel boundary-guided network (BGNet) for camouflaged\nobject detection. Our method explores valuable and extra object-related edge\nsemantics to guide representation learning of COD, which forces the model to\ngenerate features that highlight object structure, thereby promoting\ncamouflaged object detection of accurate boundary localization. Extensive\nexperiments on three challenging benchmark datasets demonstrate that our BGNet\nsignificantly outperforms the existing 18 state-of-the-art methods under four\nwidely-used evaluation metrics. Our code is publicly available at:\nhttps://github.com/thograce/BGNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yujia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chenglizhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tian-Zhu Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarks for Industrial Inspection Based on Structured Light. (arXiv:2207.00796v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00796","description":"<p>Robustness and accuracy are two critical metrics for industrial inspection.\nIn this paper, we propose benchmarks that can evaluate the structured light\nmethod's performance. Our evaluation metric was learning from a lot of\ninspection tasks from the factories. The metric we proposed consists of four\ndetailed criteria such as flatness, length, height and sphericity. Then we can\njudge whether the structured light method/device can be applied to a specified\ninspection task by our evaluation metric quickly. A structured light device\nbuilt for TypeC pin needles inspection performance is evaluated via our metrics\nin the final experimental section.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yuping Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis. (arXiv:2207.00807v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00807","description":"<p>Thyroid nodule classification aims at determining whether the nodule is\nbenign or malignant based on a given ultrasound image. However, the label\nobtained by the cytological biopsy which is the golden standard in clinical\nmedicine is not always consistent with the ultrasound imaging TI-RADS criteria.\nThe information difference between the two causes the existing deep\nlearning-based classification methods to be indecisive. To solve the\nInconsistent Label problem, we propose an Adaptive Curriculum Learning (ACL)\nframework, which adaptively discovers and discards the samples with\ninconsistent labels. Specifically, ACL takes both hard sample and model\ncertainty into account, and could accurately determine the threshold to\ndistinguish the samples with Inconsistent Label. Moreover, we contribute TNCD:\na Thyroid Nodule Classification Dataset to facilitate future related research\non the thyroid nodules. Extensive experimental results on TNCD based on three\ndifferent backbone networks not only demonstrate the superiority of our method\nbut also prove that the less-is-more principle which strategically discards the\nsamples with Inconsistent Label could yield performance gains. Source code and\ndata are available at https://github.com/chenghui-666/ACL/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Haifan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yifan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shuangyi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulating reaction time for Eureka effect in visual object recognition using artificial neural network. (arXiv:2207.00815v1 [q-bio.NC])","link":"http://arxiv.org/abs/2207.00815","description":"<p>The human brain can recognize objects hidden in even severely degraded images\nafter observing them for a while, which is known as a type of Eureka effect,\npossibly associated with human creativity. A previous psychological study\nsuggests that the basis of this \"Eureka recognition\" is neural processes of\ncoincidence of multiple stochastic activities. Here we constructed an\nartificial-neural-network-based model that simulated the characteristics of the\nhuman Eureka recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Hosoda_K/0/1/0/all/0/1\">Kazufumi Hosoda</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Seno_S/0/1/0/all/0/1\">Shigeto Seno</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Murata_T/0/1/0/all/0/1\">Tsutomu Murata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImLoveNet: Misaligned Image-supported Registration Network for Low-overlap Point Cloud Pairs. (arXiv:2207.00826v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00826","description":"<p>Low-overlap regions between paired point clouds make the captured features\nvery low-confidence, leading cutting edge models to point cloud registration\nwith poor quality. Beyond the traditional wisdom, we raise an intriguing\nquestion: Is it possible to exploit an intermediate yet misaligned image\nbetween two low-overlap point clouds to enhance the performance of cutting-edge\nregistration models? To answer it, we propose a misaligned image supported\nregistration network for low-overlap point cloud pairs, dubbed ImLoveNet.\nImLoveNet first learns triple deep features across different modalities and\nthen exports these features to a two-stage classifier, for progressively\nobtaining the high-confidence overlap region between the two point clouds.\nTherefore, soft correspondences are well established on the predicted overlap\nregion, resulting in accurate rigid transformations for registration. ImLoveNet\nis simple to implement yet effective, since 1) the misaligned image provides\nclearer overlap information for the two low-overlap point clouds to better\nlocate overlap parts; 2) it contains certain geometry knowledge to extract\nbetter deep features; and 3) it does not require the extrinsic parameters of\nthe imaging device with respect to the reference frame of the 3D point cloud.\nExtensive qualitative and quantitative evaluations on different kinds of\nbenchmarks demonstrate the effectiveness and superiority of our ImLoveNet over\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zeyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yabin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTD-Yolov5: A Real-time Underwater Targets Detection Method based on Attention Improved YOLOv5. (arXiv:2207.00837v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00837","description":"<p>As the treasure house of nature, the ocean contains abundant resources. But\nthe coral reefs, which are crucial to the sustainable development of marine\nlife, are facing a huge crisis because of the existence of COTS and other\norganisms. The protection of society through manual labor is limited and\ninefficient. The unpredictable nature of the marine environment also makes\nmanual operations risky. The use of robots for underwater operations has become\na trend. However, the underwater image acquisition has defects such as weak\nlight, low resolution, and many interferences, while the existing target\ndetection algorithms are not effective. Based on this, we propose an underwater\ntarget detection algorithm based on Attention Improved YOLOv5, called\nUTD-Yolov5. It can quickly and efficiently detect COTS, which in turn provides\na prerequisite for complex underwater operations. We adjusted the original\nnetwork architecture of YOLOv5 in multiple stages, including: replacing the\noriginal Backbone with a two-stage cascaded CSP (CSP2); introducing the visual\nchannel attention mechanism module SE; designing random anchor box similarity\ncalculation method etc. These operations enable UTD-Yolov5 to detect more\nflexibly and capture features more accurately. In order to make the network\nmore efficient, we also propose optimization methods such as WBF and iterative\nrefinement mechanism. This paper conducts a lot of experiments based on the\nCSIRO dataset [1]. The results show that the average accuracy of our UTD-Yolov5\nreaches 78.54%, which is a great improvement compared to the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Naigong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach. (arXiv:2207.00844v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00844","description":"<p>Medical image synthesis has attracted increasing attention because it could\ngenerate missing image data, improving diagnosis and benefits many downstream\ntasks. However, so far the developed synthesis model is not adaptive to unseen\ndata distribution that presents domain shift, limiting its applicability in\nclinical routine. This work focuses on exploring domain adaptation (DA) of 3D\nimage-to-image synthesis models. First, we highlight the technical difference\nin DA between classification, segmentation and synthesis models. Second, we\npresent a novel efficient adaptation approach based on 2D variational\nautoencoder which approximates 3D distributions. Third, we present empirical\nstudies on the effect of the amount of adaptation data and the key\nhyper-parameters. Our results show that the proposed approach can significantly\nimprove the synthesis accuracy on unseen domains in a 3D setting. The code is\npublicly available at\nhttps://github.com/WinstonHuTiger/2D_VAE_UDA_for_3D_sythesis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Q/0/1/0/all/0/1\">Qingqiao Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less Is More: A Comparison of Active Learning Strategies for 3D Medical Image Segmentation. (arXiv:2207.00845v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00845","description":"<p>Since labeling medical image data is a costly and labor-intensive process,\nactive learning has gained much popularity in the medical image segmentation\ndomain in recent years. A variety of active learning strategies have been\nproposed in the literature, but their effectiveness is highly dependent on the\ndataset and training scenario. To facilitate the comparison of existing\nstrategies and provide a baseline for evaluating novel strategies, we evaluate\nthe performance of several well-known active learning strategies on three\ndatasets from the Medical Segmentation Decathlon. Additionally, we consider a\nstrided sampling strategy specifically tailored to 3D image data. We\ndemonstrate that both random and strided sampling act as strong baselines and\ndiscuss the advantages and disadvantages of the studied methods. To allow other\nresearchers to compare their work to our results, we provide an open-source\nframework for benchmarking active learning strategies on a variety of medical\nsegmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burmeister_J/0/1/0/all/0/1\">Josafat-Mattias Burmeister</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rosas_M/0/1/0/all/0/1\">Marcel Fernandez Rosas</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hagemann_J/0/1/0/all/0/1\">Johannes Hagemann</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kordt_J/0/1/0/all/0/1\">Jonas Kordt</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Blum_J/0/1/0/all/0/1\">Jasper Blum</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shabo_S/0/1/0/all/0/1\">Simon Shabo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bergner_B/0/1/0/all/0/1\">Benjamin Bergner</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lippert_C/0/1/0/all/0/1\">Christoph Lippert</a> (1 and 2) ((1) Digital Health &amp; Machine Learning, Hasso Plattner Institute, University of Potsdam, Germany, (2) Hasso Plattner Institute for Digital Health at Mount Sinai, Icahn School of Medicine at Mount Sinai, NYC, USA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hardware architecture for high throughput event visual data filtering with matrix of IIR filters algorithm. (arXiv:2207.00860v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00860","description":"<p>Neuromorphic vision is a rapidly growing field with numerous applications in\nthe perception systems of autonomous vehicles. Unfortunately, due to the\nsensors working principle, there is a significant amount of noise in the event\nstream. In this paper we present a novel algorithm based on an IIR filter\nmatrix for filtering this type of noise and a hardware architecture that allows\nits acceleration using an SoC FPGA. Our method has a very good filtering\nefficiency for uncorrelated noise - over 99% of noisy events are removed. It\nhas been tested for several event data sets with added random noise. We\ndesigned the hardware architecture in such a way as to reduce the utilisation\nof the FPGA's internal BRAM resources. This enabled a very low latency and a\nthroughput of up to 385.8 MEPS million events per second.The proposed hardware\narchitecture was verified in simulation and in hardware on the Xilinx Zynq\nUltrascale+ MPSoC chip on the Mercury+ XU9 module with the Mercury+ ST1 base\nboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kowalczyk_M/0/1/0/all/0/1\">Marcin Kowalczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1\">Tomasz Kryjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ORA3D: Overlap Region Aware Multi-view 3D Object Detection. (arXiv:2207.00865v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00865","description":"<p>In multi-view 3D object detection tasks, disparity supervision over\noverlapping image regions substantially improves the overall detection\nperformance. However, current multi-view 3D object detection methods often fail\nto detect objects in the overlap region properly, and the network's\nunderstanding of the scene is often limited to that of a monocular detection\nnetwork. To mitigate this issue, we advocate for applying the traditional\nstereo disparity estimation method to obtain reliable disparity information for\nthe overlap region. Given the disparity estimates as a supervision, we propose\nto regularize the network to fully utilize the geometric potential of binocular\nimages, and improve the overall detection accuracy. Moreover, we propose to use\nan adversarial overlap region discriminator, which is trained to minimize the\nrepresentational gap between non-overlap regions and overlapping regions where\nobjects are often largely occluded or suffer from deformation due to camera\ndistortion, causing a domain shift. We demonstrate the effectiveness of the\nproposed method with the large-scale multi-view 3D object detection benchmark,\ncalled nuScenes. Our experiment shows that our proposed method outperforms the\ncurrent state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roh_W/0/1/0/all/0/1\">Wonseok Roh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_G/0/1/0/all/0/1\">Gyusam Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seokha Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1\">Giljoo Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangpil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinkyu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Video Object Segmentation with Adaptive Object Calibration. (arXiv:2207.00887v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00887","description":"<p>In the booming video era, video segmentation attracts increasing research\nattention in the multimedia community. Semi-supervised video object\nsegmentation (VOS) aims at segmenting objects in all target frames of a video,\ngiven annotated object masks of reference frames. Most existing methods build\npixel-wise reference-target correlations and then perform pixel-wise tracking\nto obtain target masks. Due to neglecting object-level cues, pixel-level\napproaches make the tracking vulnerable to perturbations, and even\nindiscriminate among similar objects. Towards robust VOS, the key insight is to\ncalibrate the representation and mask of each specific object to be expressive\nand discriminative. Accordingly, we propose a new deep network, which can\nadaptively construct object representations and calibrate object masks to\nachieve stronger robustness. First, we construct the object representations by\napplying an adaptive object proxy (AOP) aggregation method, where the proxies\nrepresent arbitrary-shaped segments at multi-levels for reference. Then,\nprototype masks are initially generated from the reference-target correlations\nbased on AOP. Afterwards, such proto-masks are further calibrated through\nnetwork modulation, conditioning on the object proxy representations. We\nconsolidate this conditional mask calibration process in a progressive manner,\nwhere the object representations and proto-masks evolve to be discriminative\niteratively. Extensive experiments are conducted on the standard VOS\nbenchmarks, YouTube-VOS-18/19 and DAVIS-17. Our model achieves the\nstate-of-the-art performance among existing published works, and also exhibits\nsuperior robustness against perturbations. Our project repo is at\nhttps://github.com/JerryX1110/Robust-Video-Object-Segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaohao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinglu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_X/0/1/0/all/0/1\">Xiang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Morphing Attack Detection Using Privacy-Aware Training Data. (arXiv:2207.00899v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00899","description":"<p>Images of morphed faces pose a serious threat to face recognition--based\nsecurity systems, as they can be used to illegally verify the identity of\nmultiple people with a single morphed image. Modern detection algorithms learn\nto identify such morphing attacks using authentic images of real individuals.\nThis approach raises various privacy concerns and limits the amount of publicly\navailable training data. In this paper, we explore the efficacy of detection\nalgorithms that are trained only on faces of non--existing people and their\nrespective morphs. To this end, two dedicated algorithms are trained with\nsynthetic data and then evaluated on three real-world datasets, i.e.:\nFRLL-Morphs, FERET-Morphs and FRGC-Morphs. Our results show that synthetic\nfacial images can be successfully employed for the training process of the\ndetection algorithms and generalize well to real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanovska_M/0/1/0/all/0/1\">Marija Ivanovska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronovsek_A/0/1/0/all/0/1\">Andrej Kronov&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peer_P/0/1/0/all/0/1\">Peter Peer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1\">Vitomir &#x160;truc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batagelj_B/0/1/0/all/0/1\">Borut Batagelj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Drift Reduction for Monocular Visual Odometry of Intelligent Vehicles using Feedforward Neural Networks. (arXiv:2207.00909v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00909","description":"<p>In this paper, an approach for reducing the drift in monocular visual\nodometry algorithms is proposed based on a feedforward neural network. A visual\nodometry algorithm computes the incremental motion of the vehicle between the\nsuccessive camera frames, then integrates these increments to determine the\npose of the vehicle. The proposed neural network reduces the errors in the pose\nestimation of the vehicle which results from the inaccuracies in features\ndetection and matching, camera intrinsic parameters, and so on. These\ninaccuracies are propagated to the motion estimation of the vehicle causing\nlarger amounts of estimation errors. The drift reducing neural network\nidentifies such errors based on the motion of features in the successive camera\nframes leading to more accurate incremental motion estimates. The proposed\ndrift reducing neural network is trained and validated using the KITTI dataset\nand the results show the efficacy of the proposed approach in reducing the\nerrors in the incremental orientation estimation, thus reducing the overall\nerror in the pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagih_H/0/1/0/all/0/1\">Hassan Wagih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osman_M/0/1/0/all/0/1\">Mostafa Osman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_M/0/1/0/all/0/1\">Mohamed I. Awad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammad_S/0/1/0/all/0/1\">Sherif Hammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SKIPP'D: a SKy Images and Photovoltaic Power Generation Dataset for Short-term Solar Forecasting. (arXiv:2207.00913v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00913","description":"<p>Large-scale integration of photovoltaics (PV) into electricity grids is\nchallenged by the intermittent nature of solar power. Sky-image-based solar\nforecasting using deep learning has been recognized as a promising approach to\npredicting the short-term fluctuations. However, there are few publicly\navailable standardized benchmark datasets for image-based solar forecasting,\nwhich limits the comparison of different forecasting models and the exploration\nof forecasting methods. To fill these gaps, we introduce SKIPP'D -- a SKy\nImages and Photovoltaic Power Generation Dataset. The dataset contains three\nyears (2017-2019) of quality-controlled down-sampled sky images and PV power\ngeneration data that is ready-to-use for short-term solar forecasting using\ndeep learning. In addition, to support the flexibility in research, we provide\nthe high resolution, high frequency sky images and PV power generation data as\nwell as the concurrent sky video footage. We also include a code base\ncontaining data processing scripts and baseline model implementations for\nresearchers to reproduce our previous work and accelerate their research in\nsolar forecasting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yuhao Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiatong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_A/0/1/0/all/0/1\">Andea Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuchi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopal_V/0/1/0/all/0/1\">Vignesh Venugopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_A/0/1/0/all/0/1\">Adam Brandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Sign Language Recognition via Temporal Super-Resolution Network. (arXiv:2207.00928v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00928","description":"<p>Aiming at the problem that the spatial-temporal hierarchical continuous sign\nlanguage recognition model based on deep learning has a large amount of\ncomputation, which limits the real-time application of the model, this paper\nproposes a temporal super-resolution network(TSRNet). The data is reconstructed\ninto a dense feature sequence to reduce the overall model computation while\nkeeping the final recognition accuracy loss to a minimum. The continuous sign\nlanguage recognition model(CSLR) via TSRNet mainly consists of three parts:\nframe-level feature extraction, time series feature extraction and TSRNet,\nwhere TSRNet is located between frame-level feature extraction and time-series\nfeature extraction, which mainly includes two branches: detail descriptor and\nrough descriptor. The sparse frame-level features are fused through the\nfeatures obtained by the two designed branches as the reconstructed dense\nframe-level feature sequence, and the connectionist temporal\nclassification(CTC) loss is used for training and optimization after the\ntime-series feature extraction part. To better recover semantic-level\ninformation, the overall model is trained with the self-generating adversarial\ntraining method proposed in this paper to reduce the model error rate. The\ntraining method regards the TSRNet as the generator, and the frame-level\nprocessing part and the temporal processing part as the discriminator. In\naddition, in order to unify the evaluation criteria of model accuracy loss\nunder different benchmarks, this paper proposes word error rate\ndeviation(WERD), which takes the error rate between the estimated word error\nrate (WER) and the reference WER obtained by the reconstructed frame-level\nfeature sequence and the complete original frame-level feature sequence as the\nWERD. Experiments on two large-scale sign language datasets demonstrate the\neffectiveness of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qidan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_F/0/1/0/all/0/1\">Fei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1\">Quan Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable by Design: Learning Predictors by Composing Interpretable Queries. (arXiv:2207.00938v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00938","description":"<p>There is a growing concern about typically opaque decision-making with\nhigh-performance machine learning algorithms. Providing an explanation of the\nreasoning process in domain-specific terms can be crucial for adoption in\nrisk-sensitive domains such as healthcare. We argue that machine learning\nalgorithms should be interpretable by design and that the language in which\nthese interpretations are expressed should be domain- and task-dependent.\nConsequently, we base our model's prediction on a family of user-defined and\ntask-specific binary functions of the data, each having a clear interpretation\nto the end-user. We then minimize the expected number of queries needed for\naccurate prediction on any given input. As the solution is generally\nintractable, following prior work, we choose the queries sequentially based on\ninformation gain. However, in contrast to previous work, we need not assume the\nqueries are conditionally independent. Instead, we leverage a stochastic\ngenerative model (VAE) and an MCMC algorithm (Unadjusted Langevin) to select\nthe most informative query about the input based on previous query-answers.\nThis enables the online determination of a query chain of whatever depth is\nrequired to resolve prediction ambiguities. Finally, experiments on vision and\nNLP tasks demonstrate the efficacy of our approach and its superiority over\npost-hoc explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_A/0/1/0/all/0/1\">Aditya Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slocum_S/0/1/0/all/0/1\">Stewart Slocum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1\">Benjamin D. Haeffele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Rene Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geman_D/0/1/0/all/0/1\">Donald Geman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Degradation-Guided Meta-Restoration Network for Blind Super-Resolution. (arXiv:2207.00943v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00943","description":"<p>Blind super-resolution (SR) aims to recover high-quality visual textures from\na low-resolution (LR) image, which is usually degraded by down-sampling blur\nkernels and additive noises. This task is extremely difficult due to the\nchallenges of complicated image degradations in the real-world. Existing SR\napproaches either assume a predefined blur kernel or a fixed noise, which\nlimits these approaches in challenging cases. In this paper, we propose a\nDegradation-guided Meta-restoration network for blind Super-Resolution (DMSR)\nthat facilitates image restoration for real cases. DMSR consists of a\ndegradation extractor and meta-restoration modules. The extractor estimates the\ndegradations in LR inputs and guides the meta-restoration modules to predict\nrestoration parameters for different degradations on-the-fly. DMSR is jointly\noptimized by a novel degradation consistency loss and reconstruction losses.\nThrough such an optimization, DMSR outperforms SOTA by a large margin on three\nwidely-used benchmarks. A user study including 16 subjects further validates\nthe superiority of DMSR in real-world blind SR tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fuzhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yanhong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongtao Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PS$^2$F: Polarized Spiral Point Spread Function for Single-Shot 3D Sensing. (arXiv:2207.00945v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00945","description":"<p>We propose a compact snapshot monocular depth estimation technique that\nrelies on an engineered point spread function (PSF). Traditional approaches\nused in microscopic super-resolution imaging, such as the Double-Helix PSF\n(DHPSF), are ill-suited for scenes that are more complex than a sparse set of\npoint light sources. We show, using the Cram\\'er-Rao lower bound (CRLB), that\nseparating the two lobes of the DHPSF and thereby capturing two separate images\nleads to a dramatic increase in depth accuracy. A unique property of the phase\nmask used for generating the DHPSF is that a separation of the phase mask into\ntwo halves leads to a spatial separation of the two lobes. We leverage this\nproperty to build a compact polarization-based optical setup, where we place\ntwo orthogonal linear polarizers on each half of the DHPSF phase mask and then\ncapture the resulting image with a polarization sensitive camera. Results from\nsimulations and a lab prototype demonstrate that our technique achieves up to\n$50\\%$ lower depth error compared to state-of-the-art designs including the\nDHPSF, and the Tetrapod PSF, with little to no loss in spatial resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghanekar_B/0/1/0/all/0/1\">Bhargav Ghanekar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saragadam_V/0/1/0/all/0/1\">Vishwanath Saragadam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehra_D/0/1/0/all/0/1\">Dushyant Mehra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gustavsson_A/0/1/0/all/0/1\">Anna-Karin Gustavsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sankaranarayanan_A/0/1/0/all/0/1\">Aswin Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeraraghavan_A/0/1/0/all/0/1\">Ashok Veeraraghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WaferSegClassNet -- A Light-weight Network for Classification and Segmentation of Semiconductor Wafer Defects. (arXiv:2207.00960v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00960","description":"<p>As the integration density and design intricacy of semiconductor wafers\nincrease, the magnitude and complexity of defects in them are also on the rise.\nSince the manual inspection of wafer defects is costly, an automated artificial\nintelligence (AI) based computer-vision approach is highly desired. The\nprevious works on defect analysis have several limitations, such as low\naccuracy and the need for separate models for classification and segmentation.\nFor analyzing mixed-type defects, some previous works require separately\ntraining one model for each defect type, which is non-scalable. In this paper,\nwe present WaferSegClassNet (WSCN), a novel network based on encoder-decoder\narchitecture. WSCN performs simultaneous classification and segmentation of\nboth single and mixed-type wafer defects. WSCN uses a \"shared encoder\" for\nclassification, and segmentation, which allows training WSCN end-to-end. We use\nN-pair contrastive loss to first pretrain the encoder and then use BCE-Dice\nloss for segmentation, and categorical cross-entropy loss for classification.\nUse of N-pair contrastive loss helps in better embedding representation in the\nlatent dimension of wafer maps. WSCN has a model size of only 0.51MB and\nperforms only 0.2M FLOPS. Thus, it is much lighter than other state-of-the-art\nmodels. Also, it requires only 150 epochs for convergence, compared to 4,000\nepochs needed by a previous work. We evaluate our model on the MixedWM38\ndataset, which has 38,015 images. WSCN achieves an average classification\naccuracy of 98.2% and a dice coefficient of 0.9999. We are the first to show\nsegmentation results on the MixedWM38 dataset. The source code can be obtained\nfrom https://github.com/ckmvigil/WaferSegClassNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Subhrajit Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makwana_D/0/1/0/all/0/1\">Dhruv Makwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_S/0/1/0/all/0/1\">Sai Chandra Teja R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Sparsh Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1\">C Krishna Mohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle-Interactive Generative Adversarial Network for Robust Unsupervised Low-Light Enhancement. (arXiv:2207.00965v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00965","description":"<p>Getting rid of the fundamental limitations in fitting to the paired training\ndata, recent unsupervised low-light enhancement methods excel in adjusting\nillumination and contrast of images. However, for unsupervised low light\nenhancement, the remaining noise suppression issue due to the lacking of\nsupervision of detailed signal largely impedes the wide deployment of these\nmethods in real-world applications. Herein, we propose a novel\nCycle-Interactive Generative Adversarial Network (CIGAN) for unsupervised\nlow-light image enhancement, which is capable of not only better transferring\nillumination distributions between low/normal-light images but also\nmanipulating detailed signals between two domains, e.g.,\nsuppressing/synthesizing realistic noise in the cyclic enhancement/degradation\nprocess. In particular, the proposed low-light guided transformation\nfeed-forwards the features of low-light images from the generator of\nenhancement GAN (eGAN) into the generator of degradation GAN (dGAN). With the\nlearned information of real low-light images, dGAN can synthesize more\nrealistic diverse illumination and contrast in low-light images. Moreover, the\nfeature randomized perturbation module in dGAN learns to increase the feature\nrandomness to produce diverse feature distributions, persuading the synthesized\nlow-light images to contain realistic noise. Extensive experiments demonstrate\nboth the superiority of the proposed method and the effectiveness of each\nmodule in CIGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zhangkai Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Features of a Splashing Drop on a Solid Surface and the Temporal Evolution extracted through Image-Sequence Classification using an Interpretable Feedforward Neural Network. (arXiv:2207.00971v1 [physics.flu-dyn])","link":"http://arxiv.org/abs/2207.00971","description":"<p>This paper reports the features of a splashing drop on a solid surface and\nthe temporal evolution, which are extracted through image-sequence\nclassification using a highly interpretable feedforward neural network (FNN)\nwith zero hidden layer. The image sequences used for training-validation and\ntesting of the FNN show the early-stage deformation of milli-sized ethanol\ndrops that impact a hydrophilic glass substrate with the Weber number ranges\nbetween 31-474 (splashing threshold about 173). Specific videographing\nconditions and digital image processing are performed to ensure the high\nsimilarity among the image sequences. As a result, the trained FNNs achieved a\ntest accuracy higher than 96%. Remarkably, the feature extraction shows that\nthe trained FNN identifies the temporal evolution of the ejected secondary\ndroplets around the aerodynamically lifted lamella and the relatively high\ncontour of the main body as the features of a splashing drop, while the\nrelatively short and thick lamella as the feature of a nonsplashing drop. The\nphysical interpretation for these features and their respective temporal\nevolution have been identified except for the difference in contour height of\nthe main body between splashing and nonsplashing drops. The observation\nreported in this study is important for the development of a data-driven\nsimulation for modeling the deformation of a splashing drop during the impact\non a solid surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yee_J/0/1/0/all/0/1\">Jingzu Yee</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Igarashi_D/0/1/0/all/0/1\">Daichi Igarashi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yamanaka_A/0/1/0/all/0/1\">Akinori Yamanaka</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tagawa_Y/0/1/0/all/0/1\">Yoshiyuki Tagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trichomonas Vaginalis Segmentation in Microscope Images. (arXiv:2207.00973v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00973","description":"<p>Trichomoniasis is a common infectious disease with high incidence caused by\nthe parasite Trichomonas vaginalis, increasing the risk of getting HIV in\nhumans if left untreated. Automated detection of Trichomonas vaginalis from\nmicroscopic images can provide vital information for the diagnosis of\ntrichomoniasis. However, accurate Trichomonas vaginalis segmentation (TVS) is a\nchallenging task due to the high appearance similarity between the Trichomonas\nand other cells (e.g., leukocyte), the large appearance variation caused by\ntheir motility, and, most importantly, the lack of large-scale annotated data\nfor deep model training. To address these challenges, we elaborately collected\nthe first large-scale Microscopic Image dataset of Trichomonas Vaginalis, named\nTVMI3K, which consists of 3,158 images covering Trichomonas of various\nappearances in diverse backgrounds, with high-quality annotations including\nobject-level mask labels, object boundaries, and challenging attributes.\nBesides, we propose a simple yet effective baseline, termed TVNet, to\nautomatically segment Trichomonas from microscopic images, including\nhigh-resolution fusion and foreground-background attention modules. Extensive\nexperiments demonstrate that our model achieves superior segmentation\nperformance and outperforms various cutting-edge object detection models both\nquantitatively and qualitatively, making it a promising framework to promote\nfuture research in TVS tasks. The dataset and results will be publicly\navailable at: https://github.com/CellRecog/cellRecog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xunkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tian-Zhu Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NARRATE: A Normal Assisted Free-View Portrait Stylizer. (arXiv:2207.00974v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00974","description":"<p>In this work, we propose NARRATE, a novel pipeline that enables\nsimultaneously editing portrait lighting and perspective in a photorealistic\nmanner. As a hybrid neural-physical face model, NARRATE leverages complementary\nbenefits of geometry-aware generative approaches and normal-assisted physical\nface models. In a nutshell, NARRATE first inverts the input portrait to a\ncoarse geometry and employs neural rendering to generate images resembling the\ninput, as well as producing convincing pose changes. However, inversion step\nintroduces mismatch, bringing low-quality images with less facial details. As\nsuch, we further estimate portrait normal to enhance the coarse geometry,\ncreating a high-fidelity physical face model. In particular, we fuse the neural\nand physical renderings to compensate for the imperfect inversion, resulting in\nboth realistic and view-consistent novel perspective images. In relighting\nstage, previous works focus on single view portrait relighting but ignoring\nconsistency between different perspectives as well, leading unstable and\ninconsistent lighting effects for view changes. We extend Total Relighting to\nfix this problem by unifying its multi-view input normal maps with the physical\nface model. NARRATE conducts relighting with consistent normal maps, imposing\ncross-view constraints and exhibiting stable and coherent illumination effects.\nWe experimentally demonstrate that NARRATE achieves more photorealistic,\nreliable results over prior works. We further bridge NARRATE with animation and\nstyle transfer tools, supporting pose change, light change, facial animation,\nand style transfer, either separately or in combination, all at a photographic\nquality. We showcase vivid free-view facial animations as well as 3D-aware\nrelightable stylization, which help facilitate various AR/VR applications like\nvirtual cinematography, 3D video conferencing, and post-production.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Youjia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yiwen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minzhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stabilizing Off-Policy Deep Reinforcement Learning from Pixels. (arXiv:2207.00986v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00986","description":"<p>Off-policy reinforcement learning (RL) from pixel observations is notoriously\nunstable. As a result, many successful algorithms must combine different\ndomain-specific practices and auxiliary losses to learn meaningful behaviors in\ncomplex environments. In this work, we provide novel analysis demonstrating\nthat these instabilities arise from performing temporal-difference learning\nwith a convolutional encoder and low-magnitude rewards. We show that this new\nvisual deadly triad causes unstable training and premature convergence to\ndegenerate solutions, a phenomenon we name catastrophic self-overfitting. Based\non our analysis, we propose A-LIX, a method providing adaptive regularization\nto the encoder's gradients that explicitly prevents the occurrence of\ncatastrophic self-overfitting using a dual objective. By applying A-LIX, we\nsignificantly outperform the prior state-of-the-art on the DeepMind Control and\nAtari 100k benchmarks without any data augmentation or auxiliary losses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cetin_E/0/1/0/all/0/1\">Edoardo Cetin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1\">Philip J. Ball</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Steve Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celiktutan_O/0/1/0/all/0/1\">Oya Celiktutan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic boxes fusion strategy in object detection. (arXiv:2207.00997v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00997","description":"<p>Object detection on microscopic scenarios is a popular task. As microscopes\nalways have variable magnifications, the object can vary substantially in\nscale, which burdens the optimization of detectors. Moreover, different\nsituations of camera focusing bring in the blurry images, which leads to great\nchallenge of distinguishing the boundaries between objects and background. To\nsolve the two issues mentioned above, we provide bags of useful training\nstrategies and extensive experiments on Chula-ParasiteEgg-11 dataset, bring\nnon-negligible results on ICIP 2022 Challenge: Parasitic Egg Detection and\nClassification in Microscopic Images, further more, we propose a new box\nselection strategy and an improved boxes fusion method for multi-model\nensemble, as a result our method wins 1st place(mIoU 95.28%, mF1Score 99.62%),\nwhich is also the state-of-the-art method on Chula-ParasiteEgg-11 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhijiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shichang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Manyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised learning for improving the accuracy of robot-mounted 3D camera applied to human gait analysis. (arXiv:2207.01002v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01002","description":"<p>The use of 3D cameras for gait analysis has been highly questioned due to the\nlow accuracy they have demonstrated in the past. The objective of the study\npresented in this paper is to improve the accuracy of the estimations made by\nrobot-mounted 3D cameras in human gait analysis by applying a supervised\nlearning stage. The 3D camera was mounted in a mobile robot to obtain a longer\nwalking distance. This study shows an improvement in detection of kinematic\ngait signals and gait descriptors by post-processing the raw estimations of the\ncamera using artificial neural networks trained with the data obtained from a\ncertified Vicon system. To achieve this, 37 healthy participants were recruited\nand data of 207 gait sequences were collected using an Orbbec Astra 3D camera.\nThere are two basic possible approaches for training: using kinematic gait\nsignals and using gait descriptors. The former seeks to improve the waveforms\nof kinematic gait signals by reducing the error and increasing the correlation\nwith respect to the Vicon system. The second is a more direct approach,\nfocusing on training the artificial neural networks using gait descriptors\ndirectly. The accuracy of the 3D camera was measured before and after training.\nIn both training approaches, an improvement was observed. Kinematic gait\nsignals showed lower errors and higher correlations with respect to the ground\ntruth. The accuracy of the system to detect gait descriptors also showed a\nsubstantial improvement, mostly for kinematic descriptors rather than\nspatio-temporal. When comparing both training approaches, it was not possible\nto define which was the absolute best. Therefore, we believe that the selection\nof the training approach will depend on the purpose of the study to be\nconducted. This study reveals the great potential of 3D cameras and encourages\nthe research community to continue exploring their use in gait analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guffanti_D/0/1/0/all/0/1\">Diego Guffanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brunete_A/0/1/0/all/0/1\">Alberto Brunete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernando_M/0/1/0/all/0/1\">Miguel Hernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_D/0/1/0/all/0/1\">David &#xc1;lvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueda_J/0/1/0/all/0/1\">Javier Rueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_E/0/1/0/all/0/1\">Enrique Navarro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lasers to Events: Automatic Extrinsic Calibration of Lidars and Event Cameras. (arXiv:2207.01009v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01009","description":"<p>Despite significant academic and corporate efforts, autonomous driving under\nadverse visual conditions still proves challenging. As neuromorphic technology\nhas matured, its application to robotics and autonomous vehicle systems has\nbecome an area of active research. Low-light and latency-demanding situations\ncan benefit. To enable event cameras to operate alongside staple sensors like\nlidar in perception tasks, we propose a direct, temporally-decoupled\ncalibration method between event cameras and lidars. The high dynamic range and\nlow-light operation of event cameras are exploited to directly register lidar\nlaser returns, allowing information-based correlation methods to optimize for\nthe 6-DoF extrinsic calibration between the two sensors. This paper presents\nthe first direct calibration method between event cameras and lidars, removing\ndependencies on frame-based camera intermediaries and/or highly-accurate hand\nmeasurements. Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ta_K/0/1/0/all/0/1\">Kevin Ta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruggemann_D/0/1/0/all/0/1\">David Bruggemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brodermann_T/0/1/0/all/0/1\">Tim Br&#xf6;dermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Image Reconstruction from Functional Magnetic Resonance Imaging via GAN Inversion with Improved Attribute Consistency. (arXiv:2207.01011v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01011","description":"<p>Neuroscience studies have revealed that the brain encodes visual content and\nembeds information in neural activity. Recently, deep learning techniques have\nfacilitated attempts to address visual reconstructions by mapping brain\nactivity to image stimuli using generative adversarial networks (GANs).\nHowever, none of these studies have considered the semantic meaning of latent\ncode in image space. Omitting semantic information could potentially limit the\nperformance. In this study, we propose a new framework to reconstruct facial\nimages from functional Magnetic Resonance Imaging (fMRI) data. With this\nframework, the GAN inversion is first applied to train an image encoder to\nextract latent codes in image space, which are then bridged to fMRI data using\nlinear transformation. Following the attributes identified from fMRI data using\nan attribute classifier, the direction in which to manipulate attributes is\ndecided and the attribute manipulator adjusts the latent code to improve the\nconsistency between the seen image and the reconstructed image. Our\nexperimental results suggest that the proposed framework accomplishes two\ngoals: (1) reconstructing clear facial images from fMRI data and (2)\nmaintaining the consistency of semantic characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chang_P/0/1/0/all/0/1\">Pei-Chun Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tien_Y/0/1/0/all/0/1\">Yan-Yu Tien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chia-Lin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Fen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yong-Sheng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_H/0/1/0/all/0/1\">Hui-Ling Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Single-Frame 3D Object Detection by Simulating Multi-Frame Point Clouds. (arXiv:2207.01030v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01030","description":"<p>To boost a detector for single-frame 3D object detection, we present a new\napproach to train it to simulate features and responses following a detector\ntrained on multi-frame point clouds. Our approach needs multi-frame point\nclouds only when training the single-frame detector, and once trained, it can\ndetect objects with only single-frame point clouds as inputs during the\ninference. We design a novel Simulated Multi-Frame Single-Stage object Detector\n(SMF-SSD) framework to realize the approach: multi-view dense object fusion to\ndensify ground-truth objects to generate a multi-frame point cloud;\nself-attention voxel distillation to facilitate one-to-many knowledge transfer\nfrom multi- to single-frame voxels; multi-scale BEV feature distillation to\ntransfer knowledge in low-level spatial and high-level semantic BEV features;\nand adaptive response distillation to activate single-frame responses of high\nconfidence and accurate localization. Experimental results on the Waymo test\nset show that our SMF-SSD consistently outperforms all state-of-the-art\nsingle-frame 3D object detectors for all object classes of difficulty levels 1\nand 2 in terms of both mAP and mAPH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Fanbin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yangyang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Based Label-Text Tuning for Few-Shot Class-Incremental Learning. (arXiv:2207.01036v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01036","description":"<p>Few-shot class-incremental learning(FSCIL) focuses on designing learning\nalgorithms that can continually learn a sequence of new tasks from a few\nsamples without forgetting old ones. The difficulties are that training on a\nsequence of limited data from new tasks leads to severe overfitting issues and\ncauses the well-known catastrophic forgetting problem. Existing researches\nmainly utilize the image information, such as storing the image knowledge of\nprevious tasks or limiting classifiers updating. However, they ignore analyzing\nthe informative and less noisy text information of class labels. In this work,\nwe propose leveraging the label-text information by adopting the memory prompt.\nThe memory prompt can learn new data sequentially, and meanwhile store the\nprevious knowledge. Furthermore, to optimize the memory prompt without\nundermining the stored knowledge, we propose a stimulation-based training\nstrategy. It optimizes the memory prompt depending on the image embedding\nstimulation, which is the distribution of the image embedding elements.\nExperiments show that our proposed method outperforms all prior\nstate-of-the-art approaches, significantly mitigating the catastrophic\nforgetting and overfitting problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linghu_X/0/1/0/all/0/1\">Xiongkun Linghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaoyun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Context Information for Generic Event Boundary Captioning. (arXiv:2207.01050v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01050","description":"<p>Generic Event Boundary Captioning (GEBC) aims to generate three sentences\ndescribing the status change for a given time boundary. Previous methods only\nprocess the information of a single boundary at a time, which lacks utilization\nof video context information. To tackle this issue, we design a model that\ndirectly takes the whole video as input and generates captions for all\nboundaries parallelly. The model could learn the context information for each\ntime boundary by modeling the boundary-boundary interactions. Experiments\ndemonstrate the effectiveness of context information. The proposed method\nachieved a 72.84 score on the test set, and we reached the $2^{nd}$ place in\nthis challenge. Our code is available at:\n\\url{https://github.com/zjr2000/Context-GEBC}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ran Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models. (arXiv:2207.01056v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01056","description":"<p>Vision-Language Pre-training (VLP) models have achieved state-of-the-art\nperformance in numerous cross-modal tasks. Since they are optimized to capture\nthe statistical properties of intra- and inter-modality, there remains risk to\nlearn social biases presented in the data as well. In this work, we (1)\nintroduce a counterfactual-based bias measurement \\emph{CounterBias} to\nquantify the social bias in VLP models by comparing the [MASK]ed prediction\nprobabilities of factual and counterfactual samples; (2) construct a novel\nVL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP\nmodels, from which we observed that significant gender bias is prevalent in VLP\nmodels; and (3) propose a VLP debiasing method \\emph{FairVLP} to minimize the\ndifference in the [MASK]ed prediction probabilities between factual and\ncounterfactual image-text pairs for VLP debiasing. Although CounterBias and\nFairVLP focus on social bias, they are generalizable to serve as tools and\nprovide new insights to probe and regularize more knowledge in VLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chat-to-Design: AI Assisted Personalized Fashion Design. (arXiv:2207.01058v1 [cs.AI])","link":"http://arxiv.org/abs/2207.01058","description":"<p>In this demo, we present Chat-to-Design, a new multimodal interaction system\nfor personalized fashion design. Compared to classic systems that recommend\napparel based on keywords, Chat-to-Design enables users to design clothes in\ntwo steps: 1) coarse-grained selection via conversation and 2) fine-grained\nediting via an interactive interface. It encompasses three sub-systems to\ndeliver an immersive user experience: A conversation system empowered by\nnatural language understanding to accept users' requests and manages dialogs; A\nmultimodal fashion retrieval system empowered by a large-scale pretrained\nlanguage-image network to retrieve requested apparel; A fashion design system\nempowered by emerging generative techniques to edit attributes of retrieved\nclothes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1\">Weiming Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chongjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_P/0/1/0/all/0/1\">Pengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NP-Match: When Neural Processes meet Semi-Supervised Learning. (arXiv:2207.01066v1 [cs.LG])","link":"http://arxiv.org/abs/2207.01066","description":"<p>Semi-supervised learning (SSL) has been widely explored in recent years, and\nit is an effective way of leveraging unlabeled data to reduce the reliance on\nlabeled data. In this work, we adjust neural processes (NPs) to the\nsemi-supervised image classification task, resulting in a new method named\nNP-Match. NP-Match is suited to this task for two reasons. Firstly, NP-Match\nimplicitly compares data points when making predictions, and as a result, the\nprediction of each unlabeled data point is affected by the labeled data points\nthat are similar to it, which improves the quality of pseudo-labels. Secondly,\nNP-Match is able to estimate uncertainty that can be used as a tool for\nselecting unlabeled samples with reliable pseudo-labels. Compared with\nuncertainty-based SSL methods implemented with Monte Carlo (MC) dropout,\nNP-Match estimates uncertainty with much less computational overhead, which can\nsave time at both the training and the testing phases. We conducted extensive\nexperiments on four public datasets, and NP-Match outperforms state-of-the-art\n(SOTA) results or achieves competitive results on them, which shows the\neffectiveness of NP-Match and its potential for SSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massiceti_D/0/1/0/all/0/1\">Daniela Massiceti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neophytou_A/0/1/0/all/0/1\">Alexandros Neophytou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Need One Detector: Unified Object Detector for Different Modalities based on Vision Transformers. (arXiv:2207.01071v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01071","description":"<p>Most systems use different models for different modalities, such as one model\nfor processing RGB images and one for depth images. Meanwhile, some recent\nworks discovered that an identical model for one modality can be used for\nanother modality with the help of cross modality transfer learning. In this\narticle, we further find out that by using a vision transformer together with\ncross/inter modality transfer learning, a unified detector can achieve better\nperformances when using different modalities as inputs. The unified model is\nuseful as we don't need to maintain separate models or weights for robotics,\nhence, it is more efficient. One application scenario of our unified system for\nrobotics can be: without any model architecture and model weights updating,\nrobotics can switch smoothly on using RGB camera or both RGB and Depth Sensor\nduring the day time and Depth sensor during the night time .\n</p>\n<p>Experiments on SUN RGB-D dataset show: Our unified model is not only\nefficient, but also has a similar or better performance in terms of mAP50 based\non SUNRGBD16 category: compare with the RGB only one, ours is slightly worse\n(52.3 $\\to$ 51.9). compare with the point cloud only one, we have similar\nperformance (52.7 $\\to$ 52.8); When using the novel inter modality mixing\nmethod proposed in this work, our model can achieve a significantly better\nperformance with 3.1 (52.7 $\\to$ 55.8) absolute improvement comparing with the\nprevious best result. Code (including training/inference logs and model\ncheckpoints) is available: \\url{https://github.com/liketheflower/YONOD.git}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoke Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canizales_J/0/1/0/all/0/1\">Jaime Canizales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamos_I/0/1/0/all/0/1\">Ioannis Stamos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-cluster-aware Network for Few-shot Skin Disease Classification. (arXiv:2207.01072v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01072","description":"<p>This paper studies the few-shot skin disease classification problem. Based on\na crucial observation that skin disease images often exist multiple\nsub-clusters within a class (i.e., the appearances of images within one class\nof disease vary and form multiple distinct sub-groups), we design a novel\nSub-Cluster-Aware Network, namely SCAN, for rare skin disease diagnosis with\nenhanced accuracy. As the performance of few-shot learning highly depends on\nthe quality of the learned feature encoder, the main principle guiding the\ndesign of SCAN is the intrinsic sub-clustered representation learning for each\nclass so as to better describe feature distributions. Specifically, SCAN\nfollows a dual-branch framework, where the first branch is to learn class-wise\nfeatures to distinguish different skin diseases, and the second one aims to\nlearn features which can effectively partition each class into several groups\nso as to preserve the sub-clustered structure within each class. To achieve the\nobjective of the second branch, we present a cluster loss to learn image\nsimilarities via unsupervised clustering. To ensure that the samples in each\nsub-cluster are from the same class, we further design a purity loss to refine\nthe unsupervised clustering results. We evaluate the proposed approach on two\npublic datasets for few-shot skin disease classification. The experimental\nresults validate that our framework outperforms the other state-of-the-art\nmethods by around 2% to 4% on the SD-198 and Derm7pt datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LI_S/0/1/0/all/0/1\">Shuhan LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Deep Image Restoration. (arXiv:2207.01074v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01074","description":"<p>This paper presents a new variational inference framework for image\nrestoration and a convolutional neural network (CNN) structure that can solve\nthe restoration problems described by the proposed framework. Earlier CNN-based\nimage restoration methods primarily focused on network architecture design or\ntraining strategy with non-blind scenarios where the degradation models are\nknown or assumed. For a step closer to real-world applications, CNNs are also\nblindly trained with the whole dataset, including diverse degradations.\nHowever, the conditional distribution of a high-quality image given a diversely\ndegraded one is too complicated to be learned by a single CNN. Therefore, there\nhave also been some methods that provide additional prior information to train\na CNN. Unlike previous approaches, we focus more on the objective of\nrestoration based on the Bayesian perspective and how to reformulate the\nobjective. Specifically, our method relaxes the original posterior inference\nproblem to better manageable sub-problems and thus behaves like a\ndivide-and-conquer scheme. As a result, the proposed framework boosts the\nperformance of several restoration problems compared to the previous ones.\nSpecifically, our method delivers state-of-the-art performance on Gaussian\ndenoising, real-world noise reduction, blind image super-resolution, and JPEG\ncompression artifacts reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Soh_J/0/1/0/all/0/1\">Jae Woong Soh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Patch Analysis and Mining Skills for Image Restoration Deep Neural Networks. (arXiv:2207.01075v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01075","description":"<p>There have been numerous image restoration methods based on deep\nconvolutional neural networks (CNNs). However, most of the literature on this\ntopic focused on the network architecture and loss functions, while less\ndetailed on the training methods. Hence, some of the works are not easily\nreproducible because it is required to know the hidden training skills to\nobtain the same results. To be specific with the training dataset, few works\ndiscussed how to prepare and order the training image patches. Moreover, it\nrequires a high cost to capture new datasets to train a restoration network for\nthe real-world scene. Hence, we believe it is necessary to study the\npreparation and selection of training data. In this regard, we present an\nanalysis of the training patches and explore the consequences of different\npatch extraction methods. Eventually, we propose a guideline for the patch\nextraction from given training images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Soh_J/0/1/0/all/0/1\">Jae Woong Soh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divert More Attention to Vision-Language Tracking. (arXiv:2207.01076v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01076","description":"<p>Relying on Transformer for complex visual feature learning, object tracking\nhas witnessed the new standard for state-of-the-arts (SOTAs). However, this\nadvancement accompanies by larger training data and longer training period,\nmaking tracking increasingly expensive. In this paper, we demonstrate that the\nTransformer-reliance is not necessary and the pure ConvNets are still\ncompetitive and even better yet more economical and friendly in achieving SOTA\ntracking. Our solution is to unleash the power of multimodal vision-language\n(VL) tracking, simply using ConvNets. The essence lies in learning novel\nunified-adaptive VL representations with our modality mixer (ModaMixer) and\nasymmetrical ConvNet search. We show that our unified-adaptive VL\nrepresentation, learned purely with the ConvNets, is a simple yet strong\nalternative to Transformer visual features, by unbelievably improving a\nCNN-based Siamese tracker by 14.5% in SUC on challenging LaSOT (50.7% &gt; 65.2%),\neven outperforming several Transformer-based SOTA trackers. Besides empirical\nresults, we theoretically analyze our approach to evidence its effectiveness.\nBy revealing the potential of VL representation, we expect the community to\ndivert more attention to VL tracking and hope to open more possibilities for\nfuture tracking beyond Transformer. Code and models will be released at\nhttps://github.com/JudasDie/SOTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingzhe Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Heng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Understand Depth?. (arXiv:2207.01077v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01077","description":"<p>Besides image classification, Contrastive Language-Image Pre-training (CLIP)\nhas accomplished extraordinary success for a wide range of vision tasks,\nincluding object-level and 3D space understanding. However, it's still\nchallenging to transfer semantic knowledge learned from CLIP into more\nintricate tasks of quantified targets, such as depth estimation with geometric\ninformation. In this paper, we propose to apply CLIP for zero-shot monocular\ndepth estimation, named DepthCLIP. We found that the patches of the input image\ncould respond to a certain semantic distance token and then be projected to a\nquantified depth bin for coarse estimation. Without any training, our DepthCLIP\nsurpasses existing unsupervised methods and even approaches the early\nfully-supervised networks. To our best knowledge, we are the first to conduct\nzero-shot adaptation from the semantic language knowledge to quantified\ndownstream tasks and perform zero-shot monocular depth estimation. We hope our\nwork could cast a light on future research. The code is available at\nhttps://github.com/Adonis-galaxy/DepthCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patient-specific modelling, simulation and real time processing for constrictive respiratory diseases. (arXiv:2207.01082v1 [eess.IV])","link":"http://arxiv.org/abs/2207.01082","description":"<p>Asthma is a common chronic disease of the respiratory system causing\nsignificant disability and societal burden. It affects over 500 million people\nworldwide and generates costs exceeding $USD 56 billion in 2011 in the United\nStates. Managing asthma involves controlling symptoms, preventing\nexacerbations, and maintaining lung function. Improving asthma control affects\nthe daily life of patients and is associated with a reduced risk of\nexacerbations and lung function impairment, reduces the cost of asthma care and\nindirect costs associated with reduced productivity. Understanding the complex\ndynamics of the pulmonary system and the lung's response to disease, injury,\nand treatment is fundamental to the advancement of Asthma treatment.\nComputational models of the respiratory system seek to provide a theoretical\nframework to understand the interaction between structure and function. Their\napplication can improve pulmonary medicine by a patient-specific approach to\nmedicinal methodologies optimizing the delivery given the personalized geometry\nand personalized ventilation patterns while introducing a patient-specific\ntechnique that maximizes drug delivery. A three-fold objective addressed within\nthis dissertation becomes prominent at this point. The first part refers to the\ncomprehension of pulmonary pathophysiology and the mechanics of Asthma and\nsubsequently of constrictive pulmonary conditions in general. The second part\nrefers to the design and implementation of tools that facilitate personalized\nmedicine to improve delivery and effectiveness. Finally, the third part refers\nto the self-management of the condition, meaning that medical personnel and\npatients have access to tools and methods that allow the first party to easily\ntrack the course of the condition and the second party, i.e. the patient to\neasily self-manage it alleviating the significant burden from the health\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nousias_S/0/1/0/all/0/1\">Stavros Nousias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Gesture Authoring Space: Authoring Customised Hand Gestures for Grasping Virtual Objects in Immersive Virtual Environments. (arXiv:2207.01092v1 [cs.HC])","link":"http://arxiv.org/abs/2207.01092","description":"<p>Natural user interfaces are on the rise. Manufacturers for Augmented,\nVirtual, and Mixed Reality head mounted displays are increasingly integrating\nnew sensors into their consumer grade products, allowing gesture recognition\nwithout additional hardware. This offers new possibilities for bare handed\ninteraction within virtual environments. This work proposes a hand gesture\nauthoring tool for object specific grab gestures allowing virtual objects to be\ngrabbed as in the real world. The presented solution uses template matching for\ngesture recognition and requires no technical knowledge to design and create\ncustom tailored hand gestures. In a user study, the proposed approach is\ncompared with the pinch gesture and the controller for grasping virtual\nobjects. The different grasping techniques are compared in terms of accuracy,\ntask completion time, usability, and naturalness. The study showed that\ngestures created with the proposed approach are perceived by users as a more\nnatural input modality than the others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schafer_A/0/1/0/all/0/1\">Alexander Sch&#xe4;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reis_G/0/1/0/all/0/1\">Gerd Reis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection with Adversarially Learned Perturbations of Latent Space. (arXiv:2207.01106v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01106","description":"<p>Anomaly detection is to identify samples that do not conform to the\ndistribution of the normal data. Due to the unavailability of anomalous data,\ntraining a supervised deep neural network is a cumbersome task. As such,\nunsupervised methods are preferred as a common approach to solve this task.\nDeep autoencoders have been broadly adopted as a base of many unsupervised\nanomaly detection methods. However, a notable shortcoming of deep autoencoders\nis that they provide insufficient representations for anomaly detection by\ngeneralizing to reconstruct outliers. In this work, we have designed an\nadversarial framework consisting of two competing components, an Adversarial\nDistorter, and an Autoencoder. The Adversarial Distorter is a convolutional\nencoder that learns to produce effective perturbations and the autoencoder is a\ndeep convolutional neural network that aims to reconstruct the images from the\nperturbed latent feature space. The networks are trained with opposing goals in\nwhich the Adversarial Distorter produces perturbations that are applied to the\nencoder's latent feature space to maximize the reconstruction error and the\nautoencoder tries to neutralize the effect of these perturbations to minimize\nit. When applied to anomaly detection, the proposed method learns semantically\nricher representations due to applying perturbations to the feature space. The\nproposed method outperforms the existing state-of-the-art methods in anomaly\ndetection on image and video datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khazaie_V/0/1/0/all/0/1\">Vahid Reza Khazaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Anthony Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jewell_J/0/1/0/all/0/1\">John Taylor Jewell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1\">Yalda Mohsenzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augment to Detect Anomalies with Continuous Labelling. (arXiv:2207.01112v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01112","description":"<p>Anomaly detection is to recognize samples that differ in some respect from\nthe training observations. These samples which do not conform to the\ndistribution of normal data are called outliers or anomalies. In real-world\nanomaly detection problems, the outliers are absent, not well defined, or have\na very limited number of instances. Recent state-of-the-art deep learning-based\nanomaly detection methods suffer from high computational cost, complexity,\nunstable training procedures, and non-trivial implementation, making them\ndifficult to deploy in real-world applications. To combat this problem, we\nleverage a simple learning procedure that trains a lightweight convolutional\nneural network, reaching state-of-the-art performance in anomaly detection. In\nthis paper, we propose to solve anomaly detection as a supervised regression\nproblem. We label normal and anomalous data using two separable distributions\nof continuous values. To compensate for the unavailability of anomalous samples\nduring training time, we utilize straightforward image augmentation techniques\nto create a distinct set of samples as anomalies. The distribution of the\naugmented set is similar but slightly deviated from the normal data, whereas\nreal anomalies are expected to have an even further distribution. Therefore,\ntraining a regressor on these augmented samples will result in more separable\ndistributions of labels for normal and real anomalous data points. Anomaly\ndetection experiments on image and video datasets show the superiority of the\nproposed method over the state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khazaie_V/0/1/0/all/0/1\">Vahid Reza Khazaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Anthony Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1\">Yalda Mohsenzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are 3D Face Shapes Expressive Enough for Recognising Continuous Emotions and Action Unit Intensities?. (arXiv:2207.01113v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01113","description":"<p>Recognising continuous emotions and action unit (AU) intensities from face\nvideos requires a spatial and temporal understanding of expression dynamics.\nExisting works primarily rely on 2D face appearances to extract such dynamics.\nThis work focuses on a promising alternative based on parametric 3D face shape\nalignment models, which disentangle different factors of variation, including\nexpression-induced shape variations. We aim to understand how expressive 3D\nface shapes are in estimating valence-arousal and AU intensities compared to\nthe state-of-the-art 2D appearance-based models. We benchmark four recent 3D\nface alignment models: ExpNet, 3DDFA-V2, DECA, and EMOCA. In valence-arousal\nestimation, expression features of 3D face models consistently surpassed\nprevious works and yielded an average concordance correlation of .739 and .574\non SEWA and AVEC 2019 CES corpora, respectively. We also study how 3D face\nshapes performed on AU intensity estimation on BP4D and DISFA datasets, and\nreport that 3D face features were on par with 2D appearance features in AUs 4,\n6, 10, 12, and 25, but not the entire set of AUs. To understand this\ndiscrepancy, we conduct a correspondence analysis between valence-arousal and\nAUs, which points out that accurate prediction of valence-arousal may require\nthe knowledge of only a few AUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tellamekala_M/0/1/0/all/0/1\">Mani Kumar Tellamekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumer_O/0/1/0/all/0/1\">&#xd6;mer S&#xfc;mer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giesbrecht_T/0/1/0/all/0/1\">Timo Giesbrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1\">Michel Valstar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DecisioNet -- A Binary-Tree Structured Neural Network. (arXiv:2207.01127v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01127","description":"<p>Deep neural networks (DNNs) and decision trees (DTs) are both\nstate-of-the-art classifiers. DNNs perform well due to their representational\nlearning capabilities, while DTs are computationally efficient as they perform\ninference along one route (root-to-leaf) that is dependent on the input data.\nIn this paper, we present DecisioNet (DN), a binary-tree structured neural\nnetwork. We propose a systematic way to convert an existing DNN into a DN to\ncreate a lightweight version of the original model. DecisioNet takes the best\nof both worlds - it uses neural modules to perform representational learning\nand utilizes its tree structure to perform only a portion of the computations.\nWe evaluate various DN architectures, along with their corresponding baseline\nmodels on the FashionMNIST, CIFAR10, and CIFAR100 datasets. We show that the DN\nvariants achieve similar accuracy while significantly reducing the\ncomputational cost of the original network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gottlieb_N/0/1/0/all/0/1\">Noam Gottlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1\">Michael Werman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Visual Field of View: Perceiving 3D Environment with Echoes and Vision. (arXiv:2207.01136v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01136","description":"<p>This paper focuses on perceiving and navigating 3D environments using echoes\nand RGB image. In particular, we perform depth estimation by fusing RGB image\nwith echoes, received from multiple orientations. Unlike previous works, we go\nbeyond the field of view of the RGB and estimate dense depth maps for\nsubstantially larger parts of the environment. We show that the echoes provide\nholistic and in-expensive information about the 3D structures complementing the\nRGB image. Moreover, we study how echoes and the wide field-of-view depth maps\ncan be utilised in robot navigation. We compare the proposed methods against\nrecent baselines using two sets of challenging realistic 3D environments:\nReplica and Matterport3D. The implementation and pre-trained models will be\nmade publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABAW: Learning from Synthetic Data & Multi-Task Learning Challenges. (arXiv:2207.01138v1 [cs.CV])","link":"http://arxiv.org/abs/2207.01138","description":"<p>This paper describes the fourth Affective Behavior Analysis in-the-wild\n(ABAW) Competition, held in conjunction with European Conference on Computer\nVision (ECCV), 2022. The 4th ABAW Competition is a continuation of the\nCompetitions held at IEEE CVPR 2022, ICCV 2021, IEEE FG 2020 and IEEE CVPR 2017\nConferences, and aims at automatically analyzing affect. In the previous runs\nof this Competition, the Challenges targeted Valence-Arousal Estimation,\nExpression Classification and Action Unit Detection. This year the Competition\nencompasses two different Challenges: i) a Multi-Task-Learning one in which the\ngoal is to learn at the same time (i.e., in a multi-task learning setting) all\nthe three above mentioned tasks; and ii) a Learning from Synthetic Data one in\nwhich the goal is to learn to recognise the basic expressions from artificially\ngenerated data and generalise to real data. The Aff-Wild2 database is a large\nscale in-the-wild database and the first one that contains annotations for\nvalence and arousal, expressions and action units. This database is the basis\nfor the above Challenges. In more detail: i) s-Aff-Wild2 -- a static version of\nAff-Wild2 database -- has been constructed and utilized for the purposes of the\nMulti-Task-Learning Challenge; and ii) some specific frames-images from the\nAff-Wild2 database have been used in an expression manipulation manner for\ncreating the synthetic dataset, which is the basis for the Learning from\nSynthetic Data Challenge. In this paper, at first we present the two\nChallenges, along with the utilized corpora, then we outline the evaluation\nmetrics and finally present the baseline systems per Challenge, as well as\ntheir derived results. More information regarding the Competition can be found\nin the competition's website:\nhttps://ibug.doc.ic.ac.uk/resources/eccv-2023-4th-abaw/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Train a CAT: Learning Canonical Appearance Transformations for Direct Visual Localization Under Illumination Change. (arXiv:1709.03009v6 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1709.03009","description":"<p>Direct visual localization has recently enjoyed a resurgence in popularity\nwith the increasing availability of cheap mobile computing power. The\ncompetitive accuracy and robustness of these algorithms compared to\nstate-of-the-art feature-based methods, as well as their natural ability to\nyield dense maps, makes them an appealing choice for a variety of mobile\nrobotics applications. However, direct methods remain brittle in the face of\nappearance change due to their underlying assumption of photometric\nconsistency, which is commonly violated in practice. In this paper, we propose\nto mitigate this problem by training deep convolutional encoder-decoder models\nto transform images of a scene such that they correspond to a previously-seen\ncanonical appearance. We validate our method in multiple environments and\nillumination conditions using high-fidelity synthetic RGB-D datasets, and\nintegrate the trained models into a direct visual localization pipeline,\nyielding improvements in visual odometry (VO) accuracy through time-varying\nillumination conditions, as well as improved metric relocalization performance\nunder illumination change, where conventional methods normally fail. We further\nprovide a preliminary investigation of transfer learning from synthetic to real\nenvironments in a localization context. An open-source implementation of our\nmethod using PyTorch is available at https://github.com/utiasSTARS/cat-net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clement_L/0/1/0/all/0/1\">Lee Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiamVGG: Visual Tracking using Deeper Siamese Networks. (arXiv:1902.02804v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1902.02804","description":"<p>Recently, we have seen a rapid development of Deep Neural Network (DNN) based\nvisual tracking solutions. Some trackers combine the DNN-based solutions with\nDiscriminative Correlation Filters (DCF) to extract semantic features and\nsuccessfully deliver the state-of-the-art tracking accuracy. However, these\nsolutions are highly compute-intensive, which require long processing time,\nresulting unsecured real-time performance. To deliver both high accuracy and\nreliable real-time performance, we propose a novel tracker called\nSiamVGG\\footnote{https://github.com/leeyeehoo/SiamVGG}. It combines a\nConvolutional Neural Network (CNN) backbone and a cross-correlation operator,\nand takes advantage of the features from exemplary images for more accurate\nobject tracking. The architecture of SiamVGG is customized from VGG-16 with the\nparameters shared by both exemplary images and desired input video frames. We\ndemonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017\ndatasets with the state-of-the-art accuracy while maintaining a decent\nreal-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve\n2% higher Expected Average Overlap (EAO) compared to the ECO and C-COT in\nVOT2017 Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Deming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation (VIN). (arXiv:2002.11310v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2002.11310","description":"<p>Visual Indoor Navigation (VIN) task has drawn increasing attention from the\ndata-driven machine learning communities especially with the recently reported\nsuccess from learning-based methods. Due to the innate complexity of this task,\nresearchers have tried approaching the problem from a variety of different\nangles, the full scope of which has not yet been captured within an overarching\nreport. This survey first summarizes the representative work of learning-based\napproaches for the VIN task and then identifies and discusses lingering issues\nimpeding the VIN performance, as well as motivates future research in these key\nareas worth exploring for the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis. (arXiv:2009.13008v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.13008","description":"<p>Recent advancements in the area of deep learning have shown the effectiveness\nof very large neural networks in several applications. However, as these deep\nneural networks continue to grow in size, it becomes more and more difficult to\nconfigure their many parameters to obtain good results. Presently, analysts\nmust experiment with many different configurations and parameter settings,\nwhich is labor-intensive and time-consuming. On the other hand, the capacity of\nfully automated techniques for neural network architecture search is limited\nwithout the domain knowledge of human experts. To deal with the problem, we\nformulate the task of neural network architecture optimization as a graph space\nexploration, based on the one-shot architecture search technique. In this\napproach, a super-graph of all candidate architectures is trained in one-shot\nand the optimal neural network is identified as a sub-graph. In this paper, we\npresent a framework that allows analysts to effectively build the solution\nsub-graph space and guide the network search by injecting their domain\nknowledge. Starting with the network architecture space composed of basic\nneural network components, analysts are empowered to effectively select the\nmost promising components via our one-shot search scheme. Applying this\ntechnique in an iterative manner allows analysts to converge to the best\nperforming neural network architecture for a given application. During the\nexploration, analysts can use their domain knowledge aided by cues provided\nfrom a scatterplot visualization of the search space to edit different\ncomponents and guide the search for faster convergence. We designed our\ninterface in collaboration with several deep learning researchers and its final\neffectiveness is evaluated with a user study and two case studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1\">Anjul Tyagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Scale Identity-Preserving Image-to-Image Translation Network for Low-Resolution Face Recognition. (arXiv:2010.12249v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.12249","description":"<p>State-of-the-art deep neural network models have reached near perfect face\nrecognition accuracy rates on controlled high-resolution face images. However,\ntheir performance is drastically degraded when they are tested with very\nlow-resolution face images. This is particularly critical in surveillance\nsystems, where a low-resolution probe image is to be matched with\nhigh-resolution gallery images. super-resolution techniques aim at producing\nhigh-resolution face images from low-resolution counterparts. While they are\ncapable of reconstructing images that are visually appealing, the\nidentity-related information is not preserved. Here, we propose an\nidentity-preserving end-to-end image-to-image translation deep neural network\nwhich is capable of super-resolving very low-resolution faces to their\nhigh-resolution counterparts while preserving identity-related information. We\nachieved this by training a very deep convolutional encoder-decoder network\nwith a symmetric contracting path between corresponding layers. This network\nwas trained with a combination of a reconstruction and an identity-preserving\nloss, on multi-scale low-resolution conditions. Extensive quantitative\nevaluations of our proposed model demonstrated that it outperforms competing\nsuper-resolution and low-resolution face recognition methods on natural and\nartificial low-resolution face data sets and even unseen identities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khazaie_V/0/1/0/all/0/1\">Vahid Reza Khazaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_N/0/1/0/all/0/1\">Nicky Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1\">Yalda Mohsenzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation. (arXiv:2012.10217v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.10217","description":"<p>Most existing point cloud instance and semantic segmentation methods rely\nheavily on strong supervision signals, which require point-level labels for\nevery point in the scene. However, such strong supervision suffers from large\nannotation costs, arousing the need to study efficient annotating. In this\npaper, we discover that the locations of instances matter for both instance and\nsemantic 3D scene segmentation. By fully taking advantage of locations, we\ndesign a weakly-supervised point cloud segmentation method that only requires\nclicking on one point per instance to indicate its location for annotation.\nWith over-segmentation for pre-processing, we extend these location annotations\ninto segments as seg-level labels. We further design a segment grouping network\n(SegGroup) to generate point-level pseudo labels under seg-level labels by\nhierarchically grouping the unlabeled segments into the relevant nearby labeled\nsegments, so that existing point-level supervised segmentation models can\ndirectly consume these pseudo labels for training. Experimental results show\nthat our seg-level supervised method (SegGroup) achieves comparable results\nwith the fully annotated point-level supervised methods. Moreover, it\noutperforms the recent weakly-supervised methods given a fixed annotation\nbudget. Code is available at https://github.com/AnTao97/SegGroup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1\">An Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Medical Image Registration via Appearance Adjustment Networks. (arXiv:2103.05213v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05213","description":"<p>Deformable image registration is fundamental for many medical image analyses.\nA key obstacle for accurate image registration lies in image appearance\nvariations such as the variations in texture, intensities, and noise. These\nvariations are readily apparent in medical images, especially in brain images\nwhere registration is frequently used. Recently, deep learning-based\nregistration methods (DLRs), using deep neural networks, have shown\ncomputational efficiency that is several orders of magnitude faster than\ntraditional optimization-based registration methods (ORs). DLRs rely on a\nglobally optimized network that is trained with a set of training samples to\nachieve faster registration. DLRs tend, however, to disregard the\ntarget-pair-specific optimization inherent in ORs and thus have degraded\nadaptability to variations in testing samples. This limitation is severe for\nregistering medical images with large appearance variations, especially since\nfew existing DLRs explicitly take into account appearance variations. In this\nstudy, we propose an Appearance Adjustment Network (AAN) to enhance the\nadaptability of DLRs to appearance variations. Our AAN, when integrated into a\nDLR, provides appearance transformations to reduce the appearance variations\nduring registration. In addition, we propose an anatomy-constrained loss\nfunction through which our AAN generates anatomy-preserving transformations.\nOur AAN has been purposely designed to be readily inserted into a wide range of\nDLRs and can be trained cooperatively in an unsupervised and end-to-end manner.\nWe evaluated our AAN with three state-of-the-art DLRs on three well-established\npublic datasets of 3D brain magnetic resonance imaging (MRI). The results show\nthat our AAN consistently improved existing DLRs and outperformed\nstate-of-the-art ORs on registration accuracy, while adding a fractional\ncomputational load to existing DLRs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_L/0/1/0/all/0/1\">Lei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fulham_M/0/1/0/all/0/1\">Michael Fulham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">David Dagan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of 5-year Progression-Free Survival in Advanced Nasopharyngeal Carcinoma with Pretreatment PET/CT using Multi-Modality Deep Learning-based Radiomics. (arXiv:2103.05220v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.05220","description":"<p>Objective: Deep Learning-based Radiomics (DLR) has achieved great success in\nmedical image analysis and has been considered a replacement for conventional\nradiomics that relies on handcrafted features. In this study, we aimed to\nexplore the capability of DLR for the prediction of 5-year Progression-Free\nSurvival (PFS) in Nasopharyngeal Carcinoma (NPC) using pretreatment PET/CT.\nMethods: A total of 257 patients (170/87 in internal/external cohorts) with\nadvanced NPC (TNM stage III or IVa) were enrolled. We developed an end-to-end\nmulti-modality DLR model, in which a 3D convolutional neural network was\noptimized to extract deep features from pretreatment PET/CT images and predict\nthe probability of 5-year PFS. TNM stage, as a high-level clinical feature,\ncould be integrated into our DLR model to further improve the prognostic\nperformance. To compare conventional radiomics and DLR, 1456 handcrafted\nfeatures were extracted, and optimal conventional radiomics methods were\nselected from 54 cross-combinations of 6 feature selection methods and 9\nclassification methods. In addition, risk group stratification was performed\nwith clinical signature, conventional radiomics signature, and DLR signature.\nResults: Our multi-modality DLR model using both PET and CT achieved higher\nprognostic performance than the optimal conventional radiomics method.\nFurthermore, the multi-modality DLR model outperformed single-modality DLR\nmodels using only PET or only CT. For risk group stratification, the\nconventional radiomics signature and DLR signature enabled significant\ndifferences between the high- and low-risk patient groups in both internal and\nexternal cohorts, while the clinical signature failed in the external cohort.\nConclusion: Our study identified potential prognostic tools for survival\nprediction in advanced NPC, suggesting that DLR could provide complementary\nvalues to the current TNM staging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gu_B/0/1/0/all/0/1\">Bingxin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_M/0/1/0/all/0/1\">Mingyuan Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1\">Lei Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1\">David Dagan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1\">Shaoli Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual Reality. (arXiv:2104.04794v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04794","description":"<p>Social presence, the feeling of being there with a real person, will fuel the\nnext generation of communication systems driven by digital humans in virtual\nreality (VR). The best 3D video-realistic VR avatars that minimize the uncanny\neffect rely on person-specific (PS) models. However, these PS models are\ntime-consuming to build and are typically trained with limited data\nvariability, which results in poor generalization and robustness. Major sources\nof variability that affects the accuracy of facial expression transfer\nalgorithms include using different VR headsets (e.g., camera configuration,\nslop of the headset), facial appearance changes over time (e.g., beard,\nmake-up), and environmental factors (e.g., lighting, backgrounds). This is a\nmajor drawback for the scalability of these models in VR. This paper makes\nprogress in overcoming these limitations by proposing an end-to-end\nmulti-identity architecture (MIA) trained with specialized augmentation\nstrategies. MIA drives the shape component of the avatar from three cameras in\nthe VR headset (two eyes, one mouth), in untrained subjects, using minimal\npersonalized information (i.e., neutral 3D mesh shape). Similarly, if the PS\ntexture decoder is available, MIA is able to drive the full avatar\n(shape+texture) robustly outperforming PS models in challenging scenarios. Our\nkey contribution to improve robustness and generalization, is that our method\nimplicitly decouples, in an unsupervised manner, the facial expression from\nnuisance factors (e.g., headset, environment, facial appearance). We\ndemonstrate the superior performance and robustness of the proposed method\nversus state-of-the-art PS approaches in a variety of experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jourabloo_A/0/1/0/all/0/1\">Amin Jourabloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gecer_B/0/1/0/all/0/1\">Baris Gecer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1\">Fernando De la Torre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Jason Saragih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shih-En Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Te-Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardi_S/0/1/0/all/0/1\">Stephen Lombardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belko_D/0/1/0/all/0/1\">Danielle Belko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimble_A/0/1/0/all/0/1\">Autumn Trimble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badino_H/0/1/0/all/0/1\">Hernan Badino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09124","description":"<p>While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4\\%/16.3\\% of\nResNet-101/ResNet-50. Code is available at https://github.\ncom/Yuting-Gao/DisCo-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Topology In Pancreatic Tubular Networks From Live Imaging 3D Microscopy. (arXiv:2105.09737v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09737","description":"<p>Motivated by the challenging segmentation task of pancreatic tubular\nnetworks, this paper tackles two commonly encountered problems in biomedical\nimaging: Topological consistency of the segmentation, and expensive or\ndifficult annotation. Our contributions are the following: a) We propose a\ntopological score which measures both topological and geometric consistency\nbetween the predicted and ground truth segmentations, applied to model\nselection and validation. b) We provide a full deep-learning methodology for\nthis difficult noisy task on time-series image data. In our method, we first\nuse a semisupervised U-net architecture, applicable to generic segmentation\ntasks, which jointly trains an autoencoder and a segmentation network. We then\nuse tracking of loops over time to further improve the predicted topology. This\nsemi-supervised approach allows us to utilize unannotated data to learn feature\nrepresentations that generalize to test data with high variability, in spite of\nour annotated training data having very limited variation. Our contributions\nare validated on a challenging segmentation task, locating tubular structures\nin the fetal pancreas from noisy live imaging confocal microscopy. We show that\nour semi-supervised model outperforms not only fully supervised and pre-trained\nmodels but also an approach which takes topological consistency into account\nduring training. Further, our approach achieves a mean loop score of 0.808 for\ndetecting loops in the fetal pancreas, compared to a U-net trained with clDice\nwith mean loop score 0.762.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnavaz_K/0/1/0/all/0/1\">Kasra Arnavaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zepf_K/0/1/0/all/0/1\">Kilian Zepf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krivokapic_J/0/1/0/all/0/1\">Jelena M. Krivokapic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilmann_S/0/1/0/all/0/1\">Silja Heilmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baerentzen_J/0/1/0/all/0/1\">Jakob Andreas B&#xe6;rentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyeng_P/0/1/0/all/0/1\">Pia Nyeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Don't Need u: Identifiable Non-Linear ICA Without Side Information. (arXiv:2106.05238v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05238","description":"<p>In this paper, we investigate the algorithmic stability of unsupervised\nrepresentation learning with deep generative models, as a function of repeated\nre-training on the same input data. Algorithms for learning low dimensional\nlinear representations -- for example principal components analysis (PCA), or\nlinear independent components analysis (ICA) -- come with guarantees that they\nwill always reveal the same latent representations (perhaps up to an arbitrary\nrotation or permutation). Unfortunately, for non-linear representation\nlearning, such as in a variational auto-encoder (VAE) model trained by\nstochastic gradient descent, we have no such guarantees. Recent work on\nidentifiability in non-linear ICA have introduced a family of deep generative\nmodels that have identifiable latent representations, achieved by conditioning\non side information (e.g. informative labels). We empirically evaluate the\nstability of these models under repeated re-estimation of parameters, and\ncompare them to both standard VAEs and deep generative models which learn to\ncluster in their latent space. Surprisingly, we discover side information is\nnot necessary for algorithmic stability: using standard quantitative measures\nof identifiability, we find deep generative models with latent clusterings are\nempirically identifiable to the same degree as models which rely on auxiliary\nlabels. We relate these results to the possibility of identifiable non-linear\nICA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Willetts_M/0/1/0/all/0/1\">Matthew Willetts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paige_B/0/1/0/all/0/1\">Brooks Paige</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold. (arXiv:2106.05965v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05965","description":"<p>Single image pose estimation is a fundamental problem in many vision and\nrobotics tasks, and existing deep learning approaches suffer by not completely\nmodeling and handling: i) uncertainty about the predictions, and ii) symmetric\nobjects with multiple (sometimes infinite) correct poses. To this end, we\nintroduce a method to estimate arbitrary, non-parametric distributions on\nSO(3). Our key idea is to represent the distributions implicitly, with a neural\nnetwork that estimates the probability given the input image and a candidate\npose. Grid sampling or gradient ascent can be used to find the most likely\npose, but it is also possible to evaluate the probability at any pose, enabling\nreasoning about symmetries and uncertainty. This is the most general way of\nrepresenting distributions on manifolds, and to showcase the rich expressive\npower, we introduce a dataset of challenging symmetric and nearly-symmetric\nobjects. We require no supervision on pose uncertainty -- the model trains only\nwith a single pose per example. Nonetheless, our implicit model is highly\nexpressive to handle complex distributions over 3D poses, while still obtaining\naccurate pose estimation on standard non-ambiguous environments, achieving\nstate-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1\">Kieran Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1\">Carlos Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1\">Srikumar Ramalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1\">Ameesh Makadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Cycle Pruning: Pruning ConvNets Under a Tight Training Budget. (arXiv:2107.02086v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02086","description":"<p>Introducing sparsity in a neural network has been an efficient way to reduce\nits complexity while keeping its performance almost intact. Most of the time,\nsparsity is introduced using a three-stage pipeline: 1) train the model to\nconvergence, 2) prune the model according to some criterion, 3) fine-tune the\npruned model to recover performance. The last two steps are often performed\niteratively, leading to reasonable results but also to a time-consuming and\ncomplex process. In our work, we propose to get rid of the first step of the\npipeline and to combine the two other steps in a single pruning-training cycle,\nallowing the model to jointly learn for the optimal weights while being pruned.\nWe do this by introducing a novel pruning schedule, named One-Cycle Pruning,\nwhich starts pruning from the beginning of the training, and until its very\nend. Adopting such a schedule not only leads to better performing pruned models\nbut also drastically reduces the training budget required to prune a model.\nExperiments are conducted on a variety of architectures (VGG-16 and ResNet-18)\nand datasets (CIFAR-10, CIFAR-100 and Caltech-101), and for relatively high\nsparsity values (80%, 90%, 95% of weights removed). Our results show that\nOne-Cycle Pruning consistently outperforms commonly used pruning schedules such\nas One-Shot Pruning, Iterative Pruning and Automated Gradual Pruning, on a\nfixed training budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hubens_N/0/1/0/all/0/1\">Nathan Hubens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancas_M/0/1/0/all/0/1\">Matei Mancas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_B/0/1/0/all/0/1\">Bernard Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preda_M/0/1/0/all/0/1\">Marius Preda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_T/0/1/0/all/0/1\">Titus Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Lighter and Faster Deep Neural Architecture for Tomato Leaf Disease Classification. (arXiv:2109.02394v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02394","description":"<p>To ensure global food security and the overall profit of stakeholders, the\nimportance of correctly detecting and classifying plant diseases is paramount.\nIn this connection, the emergence of deep learning-based image classification\nhas introduced a substantial number of solutions. However, the applicability of\nthese solutions in low-end devices requires fast, accurate, and computationally\ninexpensive systems. This work proposes a lightweight transfer learning-based\napproach for detecting diseases from tomato leaves. It utilizes an effective\npreprocessing method to enhance the leaf images with illumination correction\nfor improved classification. Our system extracts features using a combined\nmodel consisting of a pretrained MobileNetV2 architecture and a classifier\nnetwork for effective prediction. Traditional augmentation approaches are\nreplaced by runtime augmentation to avoid data leakage and address the class\nimbalance issue. Evaluation on tomato leaf images from the PlantVillage dataset\nshows that the proposed architecture achieves 99.30% accuracy with a model size\nof 9.60MB and 4.87M floating-point operations, making it a suitable choice for\nreal-life applications in low-end devices. Our codes and models are available\nat https://github.com/redwankarimsony/project-tomato.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sabbir Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Bakhtiar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_T/0/1/0/all/0/1\">Tasnim Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sony_R/0/1/0/all/0/1\">Redwan Karim Sony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md. Hasanul Kabir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traffic-Net: 3D Traffic Monitoring Using a Single Camera. (arXiv:2109.09165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09165","description":"<p>Computer Vision has played a major role in Intelligent Transportation Systems\n(ITS) and traffic surveillance. Along with the rapidly growing automated\nvehicles and crowded cities, the automated and advanced traffic management\nsystems (ATMS) using video surveillance infrastructures have been evolved by\nthe implementation of Deep Neural Networks. In this research, we provide a\npractical platform for real-time traffic monitoring, including 3D\nvehicle/pedestrian detection, speed detection, trajectory estimation,\ncongestion detection, as well as monitoring the interaction of vehicles and\npedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5\ndeep neural network model for vehicle/pedestrian detection and an enhanced SORT\ntracking algorithm. For the first time, a hybrid satellite-ground based inverse\nperspective mapping (SG-IPM) method for camera auto-calibration is also\ndeveloped which leads to an accurate 3D object detection and visualisation. We\nalso develop a hierarchical traffic modelling solution based on short- and\nlong-term temporal video data stream to understand the traffic flow,\nbottlenecks, and risky spots for vulnerable road users. Several experiments on\nreal-world scenarios and comparisons with state-of-the-art are conducted using\nvarious traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM\ncollected from highways, intersections, and urban areas under different\nlighting and weather conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1\">Mahdi Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarmi_M/0/1/0/all/0/1\">Mohsen Azarmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mir_F/0/1/0/all/0/1\">Farzam Mohammad Pour Mir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.09960v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09960","description":"<p>In this paper, we propose a novel mutual consistency network (MC-Net+) to\neffectively exploit the unlabeled data for semi-supervised medical image\nsegmentation. The MC-Net+ model is motivated by the observation that deep\nmodels trained with limited annotations are prone to output highly uncertain\nand easily mis-classified predictions in the ambiguous regions (e.g., adhesive\nedges or thin branches) for medical image segmentation. Leveraging these\nchallenging samples can make the semi-supervised segmentation model training\nmore effective. Therefore, our proposed MC-Net+ model consists of two new\ndesigns. First, the model contains one shared encoder and multiple slightly\ndifferent decoders (i.e., using different up-sampling strategies). The\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, we\napply a novel mutual consistency constraint between one decoder's probability\noutput and other decoders' soft pseudo labels. In this way, we minimize the\ndiscrepancy of multiple outputs (i.e., the model uncertainty) during training\nand force the model to generate invariant results in such challenging regions,\naiming at regularizing the model training. We compared the segmentation results\nof our MC-Net+ model with five state-of-the-art semi-supervised approaches on\nthree public medical datasets. Extension experiments with two standard\nsemi-supervised settings demonstrate the superior performance of our model over\nother methods, which sets a new state of the art for semi-supervised medical\nimage segmentation. Our code is released publicly at\nhttps://github.com/ycwu1997/MC-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Stacking Ensemble Approach for Supervised Video Summarization. (arXiv:2109.12581v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12581","description":"<p>Video summarization methods are usually classified into shot-level or\nframe-level methods, which are individually used in a general way. This paper\ninvestigates the underlying complementarity between the frame-level and\nshot-level methods, and a stacking ensemble approach is proposed for supervised\nvideo summarization. Firstly, we build up a stacking model to predict both the\nkey frame probabilities and the temporal interest segments simultaneously. The\ntwo components are then combined via soft decision fusion to obtain the final\nscores of each frame in the video. A joint loss function is proposed for the\nmodel training. The ablation experimental results show that the proposed method\noutperforms both the two corresponding individual method. Furthermore,\nextensive experimental results on two benchmark datasets shows its superior\nperformance in comparison with the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yubo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shenghui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation. (arXiv:2110.05092v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05092","description":"<p>This paper proposes a unified framework dubbed Multi-view and Temporal Fusing\nTransformer (MTF-Transformer) to adaptively handle varying view numbers and\nvideo length without camera calibration in 3D Human Pose Estimation (HPE). It\nconsists of Feature Extractor, Multi-view Fusing Transformer (MFT), and\nTemporal Fusing Transformer (TFT). Feature Extractor estimates 2D pose from\neach image and fuses the prediction according to the confidence. It provides\npose-focused feature embedding and makes subsequent modules computationally\nlightweight. MFT fuses the features of a varying number of views with a novel\nRelative-Attention block. It adaptively measures the implicit relative\nrelationship between each pair of views and reconstructs more informative\nfeatures. TFT aggregates the features of the whole sequence and predicts 3D\npose via a transformer. It adaptively deals with the video of arbitrary length\nand fully unitizes the temporal information. The migration of transformers\nenables our model to learn spatial geometry better and preserve robustness for\nvarying application scenarios. We report quantitative and qualitative results\non the Human3.6M, TotalCapture, and KTH Multiview Football II. Compared with\nstate-of-the-art methods with camera parameters, MTF-Transformer obtains\ncompetitive results and generalizes well to dynamic capture with an arbitrary\nnumber of unseen views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hui Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lele Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingshan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lidar with Velocity: Correcting Moving Objects Point Cloud Distortion from Oscillating Scanning Lidars by Fusion with Camera. (arXiv:2111.09497v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.09497","description":"<p>Lidar point cloud distortion from moving object is an important problem in\nautonomous driving, and recently becomes even more demanding with the emerging\nof newer lidars, which feature back-and-forth scanning patterns. Accurately\nestimating moving object velocity would not only provide a tracking capability\nbut also correct the point cloud distortion with more accurate description of\nthe moving object. Since lidar measures the time-of-flight distance but with a\nsparse angular resolution, the measurement is precise in the radial measurement\nbut lacks angularly. Camera on the other hand provides a dense angular\nresolution. In this paper, Gaussian-based lidar and camera fusion is proposed\nto estimate the full velocity and correct the lidar distortion. A probabilistic\nKalman-filter framework is provided to track the moving objects, estimate their\nvelocities and simultaneously correct the point clouds distortions. The\nframework is evaluated on real road data and the fusion method outperforms the\ntraditional ICP-based and point-cloud only method. The complete working\nframework is open-sourced\n(https://github.com/ISEE-Technology/lidar-with-velocity) to accelerate the\nadoption of the emerging lidars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaoping Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaFormer Is Actually What You Need for Vision. (arXiv:2111.11418v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11418","description":"<p>Transformers have shown great potential in computer vision tasks. A common\nbelief is their attention-based token mixer module contributes most to their\ncompetence. However, recent works show the attention-based module in\nTransformers can be replaced by spatial MLPs and the resulted models still\nperform quite well. Based on this observation, we hypothesize that the general\narchitecture of the Transformers, instead of the specific token mixer module,\nis more essential to the model's performance. To verify this, we deliberately\nreplace the attention module in Transformers with an embarrassingly simple\nspatial pooling operator to conduct only basic token mixing. Surprisingly, we\nobserve that the derived model, termed as PoolFormer, achieves competitive\nperformance on multiple computer vision tasks. For example, on ImageNet-1K,\nPoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned Vision\nTransformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with\n35%/52% fewer parameters and 50%/62% fewer MACs. The effectiveness of\nPoolFormer verifies our hypothesis and urges us to initiate the concept of\n\"MetaFormer\", a general architecture abstracted from Transformers without\nspecifying the token mixer. Based on the extensive experiments, we argue that\nMetaFormer is the key player in achieving superior results for recent\nTransformer and MLP-like models on vision tasks. This work calls for more\nfuture research dedicated to improving MetaFormer instead of focusing on the\ntoken mixer modules. Additionally, our proposed PoolFormer could serve as a\nstarting baseline for future MetaFormer architecture design. Code is available\nat https://github.com/sail-sg/poolformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenyang Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Adaptation for Implicit Object Tracking and Shape Reconstruction in the Wild. (arXiv:2111.12728v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12728","description":"<p>Tracking and reconstructing 3D objects from cluttered scenes are the key\ncomponents for computer vision, robotics and autonomous driving systems. While\nrecent progress in implicit function has shown encouraging results on\nhigh-quality 3D shape reconstruction, it is still very challenging to\ngeneralize to cluttered and partially observable LiDAR data. In this paper, we\npropose to leverage the continuity in video data. We introduce a novel and\nunified framework which utilizes a neural implicit function to simultaneously\ntrack and reconstruct 3D objects in the wild. Our approach adapts the DeepSDF\nmodel (i.e., an instantiation of the implicit function) in the video online,\niteratively improving the shape reconstruction while in return improving the\ntracking, and vice versa. We experiment with both Waymo and KITTI datasets and\nshow significant improvements over state-of-the-art methods for both tracking\nand shape reconstruction tasks. Our project page is at\nhttps://jianglongye.com/implicit-tracking .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianglong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACNet: Approaching-and-Centralizing Network for Zero-Shot Sketch-Based Image Retrieval. (arXiv:2111.12757v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12757","description":"<p>The huge domain gap between sketches and photos and the highly abstract\nsketch representations pose challenges for sketch-based image retrieval\n(\\underline{SBIR}). The zero-shot sketch-based image retrieval\n(\\underline{ZS-SBIR}) is more generic and practical but poses an even greater\nchallenge because of the additional knowledge gap between the seen and unseen\ncategories. To simultaneously mitigate both gaps, we propose an\n\\textbf{A}pproaching-and-\\textbf{C}entralizing \\textbf{Net}work (termed\n\"\\textbf{ACNet}\") to jointly optimize sketch-to-photo synthesis and the image\nretrieval. The retrieval module guides the synthesis module to generate large\namounts of diverse photo-like images which gradually approach the photo domain,\nand thus better serve the retrieval module than ever to learn domain-agnostic\nrepresentations and category-agnostic common knowledge for generalizing to\nunseen categories. These diverse images generated with retrieval guidance can\neffectively alleviate the overfitting problem troubling concrete\ncategory-specific training samples with high gradients. We also discover the\nuse of proxy-based NormSoftmax loss is effective in the zero-shot setting\nbecause its centralizing effect can stabilize our joint training and promote\nthe generalization ability to unseen categories. Our approach is simple yet\neffective, which achieves state-of-the-art performance on two widely used\nZS-SBIR datasets and surpasses previous methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Ziqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai-Kit Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Channel Encoding Transformer for Point Cloud Analysis. (arXiv:2112.02507v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02507","description":"<p>Transformer plays an increasingly important role in various computer vision\nareas and remarkable achievements have also been made in point cloud analysis.\nSince they mainly focus on point-wise transformer, an adaptive channel encoding\ntransformer is proposed in this paper. Specifically, a channel convolution\ncalled Transformer-Conv is designed to encode the channel. It can encode\nfeature channels by capturing the potential relationship between coordinates\nand features. Compared with simply assigning attention weight to each channel,\nour method aims to encode the channel adaptively. In addition, our network\nadopts the neighborhood search method of low-level and high-level dual semantic\nreceptive fields to improve the performance. Extensive experiments show that\nour method is superior to state-of-the-art point cloud classification and\nsegmentation methods on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoquan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hezhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanxin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jianwei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-Adaptive YOLO for Object Detection in Adverse Weather Conditions. (arXiv:2112.08088v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08088","description":"<p>Though deep learning-based object detection methods have achieved promising\nresults on the conventional datasets, it is still challenging to locate objects\nfrom the low-quality images captured in adverse weather conditions. The\nexisting methods either have difficulties in balancing the tasks of image\nenhancement and object detection, or often ignore the latent information\nbeneficial for detection. To alleviate this problem, we propose a novel\nImage-Adaptive YOLO (IA-YOLO) framework, where each image can be adaptively\nenhanced for better detection performance. Specifically, a differentiable image\nprocessing (DIP) module is presented to take into account the adverse weather\nconditions for YOLO detector, whose parameters are predicted by a small\nconvolutional neural net-work (CNN-PP). We learn CNN-PP and YOLOv3 jointly in\nan end-to-end fashion, which ensures that CNN-PP can learn an appropriate DIP\nto enhance the image for detection in a weakly supervised manner. Our proposed\nIA-YOLO approach can adaptively process images in both normal and adverse\nweather conditions. The experimental results are very encouraging,\ndemonstrating the effectiveness of our proposed IA-YOLO method in both foggy\nand low-light scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Gaofeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Runsheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach. (arXiv:2112.15031v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15031","description":"<p>During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to\nprevent spreading and contracting the virus. The ability to monitor the\nmask-wearing rate in the population would be useful for determining public\nhealth strategies against the virus. However, artificial intelligence\ntechnologies for detecting face masks have not been deployed at a large scale\nin real-life to measure the mask-wearing rate in public. In this paper, we\npresent a two-step face mask detection approach consisting of two separate\nmodules: 1) face detection and alignment and 2) face mask classification. This\napproach allowed us to experiment with different combinations of face detection\nand face mask classification modules. More specifically, we experimented with\nPyramidKey and RetinaFace as face detectors while maintaining a lightweight\nbackbone for the face mask classification module. Moreover, we also provide a\nrelabeled annotation of the test set of the AIZOO dataset, where we rectified\nthe incorrect labels for some face images. The evaluation results on the AIZOO\nand Moxa 3K datasets showed that the proposed face mask detection pipeline\nsurpassed the state-of-the-art methods. The proposed pipeline also yielded a\nhigher mAP on the relabeled test set of the AIZOO dataset than the original\ntest set. Since we trained the proposed model using in-the-wild face images, we\ncan successfully deploy our model to monitor the mask-wearing rate using public\nCCTV images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sommana_B/0/1/0/all/0/1\">Benjaphan Sommana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1\">Ukrit Watchareeruetai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Ankush Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Earp_S/0/1/0/all/0/1\">Samuel W.F. Earp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitiyakara_T/0/1/0/all/0/1\">Taya Kitiyakara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boonmanunt_S/0/1/0/all/0/1\">Suparee Boonmanunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thammasudjarit_R/0/1/0/all/0/1\">Ratchainant Thammasudjarit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.03969","description":"<p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem\ndue to the heterogeneity gap between different modalities and the ambiguity of\nhuman emotional expression. Although there have been many successful attempts\nto construct multimodal representations for MSA, there are still two challenges\nto be addressed: 1) A more robust multimodal representation needs to be\nconstructed to bridge the heterogeneity gap and cope with the complex\nmultimodal interactions, and 2) the contextual dynamics must be modeled\neffectively throughout the information flow. In this work, we propose a\nmultimodal representation model based on Mutual information Maximization and\nMinimization and Identity Embedding (MMMIE). We combine mutual information\nmaximization between modal pairs, and mutual information minimization between\ninput data and corresponding features to mine the modal-invariant and\ntask-related information. Furthermore, Identity Embedding is proposed to prompt\nthe downstream network to perceive the contextual information. Experimental\nresults on two public datasets demonstrate the effectiveness of the proposed\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiahao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhigang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrated Multiscale Domain Adaptive YOLO. (arXiv:2202.03527v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03527","description":"<p>The area of domain adaptation has been instrumental in addressing the domain\nshift problem encountered by many applications. This problem arises due to the\ndifference between the distributions of source data used for training in\ncomparison with target data used during realistic testing scenarios. In this\npaper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)\nframework that employs multiple domain adaptation paths and corresponding\ndomain classifiers at different scales of the recently introduced YOLOv4 object\ndetector. Building on our baseline multiscale DAYOLO framework, we introduce\nthree novel deep learning architectures for a Domain Adaptation Network (DAN)\nthat generates domain-invariant features. In particular, we propose a\nProgressive Feature Reduction (PFR), a Unified Classifier (UC), and an\nIntegrated architecture. We train and test our proposed DAN architectures in\nconjunction with YOLOv4 using popular datasets. Our experiments show\nsignificant improvements in object detection performance when training YOLOv4\nusing the proposed MS-DAYOLO architectures and when tested on target data for\nautonomous driving applications. Moreover, MS-DAYOLO framework achieves an\norder of magnitude real-time speed improvement relative to Faster R-CNN\nsolutions while providing comparable object detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hnewa_M/0/1/0/all/0/1\">Mazin Hnewa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radha_H/0/1/0/all/0/1\">Hayder Radha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attack and Defense of YOLO Detectors in Autonomous Driving Scenarios. (arXiv:2202.04781v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04781","description":"<p>Visual detection is a key task in autonomous driving, and it serves as a\ncrucial foundation for self-driving planning and control. Deep neural networks\nhave achieved promising results in various visual tasks, but they are known to\nbe vulnerable to adversarial attacks. A comprehensive understanding of deep\nvisual detectors' vulnerability is required before people can improve their\nrobustness. However, only a few adversarial attack/defense works have focused\non object detection, and most of them employed only classification and/or\nlocalization losses, ignoring the objectness aspect. In this paper, we identify\na serious objectness-related adversarial vulnerability in YOLO detectors and\npresent an effective attack strategy targeting the objectness aspect of visual\ndetection in autonomous vehicles. Furthermore, to address such vulnerability,\nwe propose a new objectness-aware adversarial training approach for visual\ndetection. Experiments show that the proposed attack targeting the objectness\naspect is 45.17% and 43.50% more effective than those generated from\nclassification and/or localization losses on the KITTI and COCO traffic\ndatasets, respectively. Also, the proposed adversarial defense approach can\nimprove the detectors' robustness against objectness-oriented attacks by up to\n21% and 12% mAP on KITTI and COCO traffic, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jung Im Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qing Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable COVID-19 Infections Identification and Delineation Using Calibrated Pseudo Labels. (arXiv:2202.07422v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.07422","description":"<p>The upheaval brought by the arrival of the COVID-19 pandemic has continued to\nbring fresh challenges over the past two years. During this COVID-19 pandemic,\nthere has been a need for rapid identification of infected patients and\nspecific delineation of infection areas in computed tomography (CT) images.\nAlthough deep supervised learning methods have been established quickly, the\nscarcity of both image-level and pixel-level labels as well as the lack of\nexplainable transparency still hinder the applicability of AI. Can we identify\ninfected patients and delineate the infections with extreme minimal\nsupervision? Semi-supervised learning has demonstrated promising performance\nunder limited labelled data and sufficient unlabelled data. Inspired by\nsemi-supervised learning, we propose a model-agnostic calibrated\npseudo-labelling strategy and apply it under a consistency regularization\nframework to generate explainable identification and delineation results. We\ndemonstrate the effectiveness of our model with the combination of limited\nlabelled data and sufficient unlabelled data or weakly-labelled data. Extensive\nexperiments have shown that our model can efficiently utilize limited labelled\ndata and provide explainable classification and segmentation results for\ndecision-making in clinical routine. The code is available at\nhttps://github.com/ayanglab/XAI COVID-19.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Ming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yingying Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zeyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Onuorah_C/0/1/0/all/0/1\">Chibudom Onuorah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1\">Simon Walsh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-World Blind Super-Resolution via Feature Matching with Implicit High-Resolution Priors. (arXiv:2202.13142v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13142","description":"<p>A key challenge of real-world image super-resolution (SR) is to recover the\nmissing details in low-resolution (LR) images with complex unknown degradations\n(e.g., downsampling, noise and compression). Most previous works restore such\nmissing details in the image space. To cope with the high diversity of natural\nimages, they either rely on the unstable GANs that are difficult to train and\nprone to artifacts, or resort to explicit references from high-resolution (HR)\nimages that are usually unavailable. In this work, we propose Feature Matching\nSR (FeMaSR), which restores realistic HR images in a much more compact feature\nspace. Unlike image-space methods, our FeMaSR restores HR images by matching\ndistorted LR image {\\it features} to their distortion-free HR counterparts in\nour pretrained HR priors, and decoding the matched features to obtain realistic\nHR images. Specifically, our HR priors contain a discrete feature codebook and\nits associated decoder, which are pretrained on HR images with a Vector\nQuantized Generative Adversarial Network (VQGAN). Notably, we incorporate a\nnovel semantic regularization in VQGAN to improve the quality of reconstructed\nimages. For the feature matching, we first extract LR features with an LR\nencoder consisting of several Swin Transformer blocks and then follow a simple\nnearest neighbour strategy to match them with the pretrained codebook. In\nparticular, we equip the LR encoder with residual shortcut connections to the\ndecoder, which is critical to the optimization of feature matching loss and\nalso helps to complement the possible feature matching errors. Experimental\nresults show that our approach produces more realistic HR images than previous\nmethods. Codes are released at \\url{https://github.com/chaofengc/FeMaSR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xinyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yipeng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shihui Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Scene Flow Estimation with 4-D Automotive Radar. (arXiv:2203.01137v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01137","description":"<p>Scene flow allows autonomous vehicles to reason about the arbitrary motion of\nmultiple independent objects which is the key to long-term mobile autonomy.\nWhile estimating the scene flow from LiDAR has progressed recently, it remains\nlargely unknown how to estimate the scene flow from a 4-D radar - an\nincreasingly popular automotive sensor for its robustness against adverse\nweather and lighting conditions. Compared with the LiDAR point clouds, radar\ndata are drastically sparser, noisier and in much lower resolution. Annotated\ndatasets for radar scene flow are also in absence and costly to acquire in the\nreal world. These factors jointly pose the radar scene flow estimation as a\nchallenging problem. This work aims to address the above challenges and\nestimate scene flow from 4-D radar point clouds by leveraging self-supervised\nlearning. A robust scene flow estimation architecture and three novel losses\nare bespoken designed to cope with intractable radar data. Real-world\nexperimental results validate that our method is able to robustly estimate the\nradar scene flow in the wild and effectively supports the downstream task of\nmotion segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1\">Fangqiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhijun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yimin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jianning Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks. (arXiv:2203.03844v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03844","description":"<p>Light-weight super-resolution (SR) models have received considerable\nattention for their serviceability in mobile devices. Many efforts employ\nnetwork quantization to compress SR models. However, these methods suffer from\nsevere performance degradation when quantizing the SR models to ultra-low\nprecision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In\nthis paper, we identify that the performance drop comes from the contradiction\nbetween the layer-wise symmetric quantizer and the highly asymmetric activation\ndistribution in SR models. This discrepancy leads to either a waste on the\nquantization levels or detail loss in reconstructed images. Therefore, we\npropose a novel activation quantizer, referred to as Dynamic Dual Trainable\nBounds (DDTB), to accommodate the asymmetry of the activations. Specifically,\nDDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower\nbounds to tackle the highly asymmetric activations. 2) A dynamic gate\ncontroller to adaptively adjust the upper and lower bounds at runtime to\novercome the drastically varying activation ranges over different samples.To\nreduce the extra overhead, the dynamic gate controller is quantized to 2-bit\nand applied to only part of the SR networks according to the introduced dynamic\nintensity. Extensive experiments demonstrate that our DDTB exhibits significant\nperformance improvements in ultra-low precision. For example, our DDTB achieves\na 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and\nscaling up output images to x4. Code is at\n\\url{https://github.com/zysxmu/DDTB}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xunchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Temporal Consistency for Source-Free Video Domain Adaptation. (arXiv:2203.04559v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04559","description":"<p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the\nrobustness of video models, enabling them to be applied to action recognition\ntasks across different environments. However, these methods require constant\naccess to source data during the adaptation process. Yet in many real-world\napplications, subjects and scenes in the source video domain should be\nirrelevant to those in the target video domain. With the increasing emphasis on\ndata privacy, such methods that require source data access would raise serious\nprivacy issues. Therefore, to cope with such concern, a more practical domain\nadaptation scenario is formulated as the Source-Free Video-based Domain\nAdaptation (SFVDA). Though there are a few methods for Source-Free Domain\nAdaptation (SFDA) on image data, these methods yield degenerating performance\nin SFVDA due to the multi-modality nature of videos, with the existence of\nadditional temporal features. In this paper, we propose a novel Attentive\nTemporal Consistent Network (ATCoN) to address SFVDA by learning temporal\nconsistency, guaranteed by two novel consistency objectives, namely feature\nconsistency and source prediction consistency, performed across local temporal\nfeatures. ATCoN further constructs effective overall temporal features by\nattending to local temporal features based on prediction confidence. Empirical\nresults demonstrate the state-of-the-art performance of ATCoN across various\ncross-domain action recognition benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Keyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Wu Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping global dynamics of benchmark creation and saturation in artificial intelligence. (arXiv:2203.04592v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.04592","description":"<p>Benchmarks are crucial to measuring and steering progress in artificial\nintelligence (AI). However, recent studies raised concerns over the state of AI\nbenchmarking, reporting issues such as benchmark overfitting, benchmark\nsaturation and increasing centralization of benchmark dataset creation. To\nfacilitate monitoring of the health of the AI benchmarking ecosystem, we\nintroduce methodologies for creating condensed maps of the global dynamics of\nbenchmark creation and saturation. We curated data for 1688 benchmarks covering\nthe entire domains of computer vision and natural language processing, and show\nthat a large fraction of benchmarks quickly trended towards near-saturation,\nthat many benchmarks fail to find widespread utilization, and that benchmark\nperformance gains for different AI tasks were prone to unforeseen bursts. We\nanalyze attributes associated with benchmark popularity, and conclude that\nfuture benchmarks should emphasize versatility, breadth and real-world utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_Silva_A/0/1/0/all/0/1\">Adriano Barbosa-Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1\">Simon Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities. (arXiv:2203.04959v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04959","description":"<p>Multiple Sclerosis (MS) is a chronic neuroinflammatory disease and\nmulti-modality MRIs are routinely used to monitor MS lesions. Many automatic MS\nlesion segmentation models have been developed and have reached human-level\nperformance. However, most established methods assume the MRI modalities used\nduring training are also available during testing, which is not guaranteed in\nclinical practice. Previously, a training strategy termed Modality Dropout\n(ModDrop) has been applied to MS lesion segmentation to achieve the\nstate-of-the-art performance with missing modality. In this paper, we present a\nnovel method dubbed ModDrop++ to train a unified network adaptive to an\narbitrary number of input MRI sequences. ModDrop++ upgrades the main idea of\nModDrop in two key ways. First, we devise a plug-and-play dynamic head and\nadopt a filter scaling strategy to improve the expressiveness of the network.\nSecond, we design a co-training strategy to leverage the intra-subject relation\nbetween full modality and missing modality. Specifically, the intra-subject\nco-training strategy aims to guide the dynamic head to generate similar feature\nrepresentations between the full- and missing-modality data from the same\nsubject. We use two public MS datasets to show the superiority of ModDrop++.\nSource code and trained models are available at\nhttps://github.com/han-liu/ModDropPlusPlus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiacheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dewei Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huahong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.05482","description":"<p>The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs -- we call\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on\nImageNet, achieved a new state of the art. Furthermore, we show that the model\nsoup approach extends to multiple image classification and natural language\nprocessing tasks, improves out-of-distribution performance, and improves\nzero-shot performance on new downstream tasks. Finally, we analytically relate\nthe performance similarity of weight-averaging and logit-ensembling to flatness\nof the loss and confidence of the predictions, and validate this relation\nempirically. Code is available at https://github.com/mlfoundations/model-soups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion. (arXiv:2203.09780v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09780","description":"<p>Current LiDAR-only 3D detection methods inevitably suffer from the sparsity\nof point clouds. Many multi-modal methods are proposed to alleviate this issue,\nwhile different representations of images and point clouds make it difficult to\nfuse them, resulting in suboptimal performance. In this paper, we present a\nnovel multi-modal framework SFD (Sparse Fuse Dense), which utilizes pseudo\npoint clouds generated from depth completion to tackle the issues mentioned\nabove. Different from prior works, we propose a new RoI fusion strategy 3D-GAF\n(3D Grid-wise Attentive Fusion) to make fuller use of information from\ndifferent types of point clouds. Specifically, 3D-GAF fuses 3D RoI features\nfrom the couple of point clouds in a grid-wise attentive way, which is more\nfine-grained and more precise. In addition, we propose a SynAugment\n(Synchronized Augmentation) to enable our multi-modal framework to utilize all\ndata augmentation approaches tailored to LiDAR-only methods. Lastly, we\ncustomize an effective and efficient feature extractor CPConv (Color Point\nConvolution) for pseudo point clouds. It can explore 2D image features and 3D\ngeometric features of pseudo point clouds simultaneously. Our method holds the\nhighest entry on the KITTI car 3D object detection leaderboard, demonstrating\nthe effectiveness of our SFD. Codes are available at\nhttps://github.com/LittlePey/SFD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaopei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Honghui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenxi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chengqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Point Cloud Representation Learning with Occlusion Auto-Encoder. (arXiv:2203.14084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14084","description":"<p>Learning representations for point clouds is an important task in 3D computer\nvision, especially without manually annotated supervision. Previous methods\nusually take the common aid from auto-encoders to establish the\nself-supervision by reconstructing the input itself. However, the existing\nself-reconstruction based auto-encoders merely focus on the global shapes, and\nignore the hierarchical context between the local and global geometries, which\nis a crucial supervision for 3D representation learning. To resolve this issue,\nwe present a novel self-supervised point cloud representation learning\nframework, named 3D Occlusion Auto-Encoder (3D-OAE). Our key idea is to\nrandomly occlude some local patches of the input point cloud and establish the\nsupervision via recovering the occluded patches using the remaining visible\nones. Specifically, we design an encoder for learning the features of visible\nlocal patches, and a decoder for leveraging these features to predict the\noccluded patches. In contrast with previous methods, our 3D-OAE can remove a\nlarge proportion of patches and predict them only with a small number of\nvisible patches, which enable us to significantly accelerate training and yield\na nontrivial self-supervisory performance. The trained encoder can be further\ntransferred to various downstream tasks. We demonstrate our superior\nperformances over the state-of-the-art methods in different discriminant and\ngenerative applications under widely used benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junsheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Baorui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Polyp Segmentation: A Deep Learning Perspective. (arXiv:2203.14291v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.14291","description":"<p>We present the first comprehensive video polyp segmentation (VPS) study in\nthe deep learning era. Over the years, developments in VPS are not moving\nforward with ease due to the lack of large-scale fine-grained segmentation\nannotations. To address this issue, we first introduce a high-quality\nframe-by-frame annotated VPS dataset, named SUN-SEG, which contains 158,690\nframes from the well-known SUN-database. We provide additional annotations with\ndiverse types, i.e., attribute, object mask, boundary, scribble, and polygon.\nSecond, we design a simple but efficient baseline, dubbed PNS+, consisting of a\nglobal encoder, a local encoder, and normalized self-attention (NS) blocks. The\nglobal and local encoders receive an anchor frame and multiple successive\nframes to extract long-term and short-term spatial-temporal representations,\nwhich are then progressively updated by two NS blocks. Extensive experiments\nshow that PNS+ achieves the best performance and real-time inference speed\n(170fps), making it a promising solution for the VPS task. Third, we\nextensively evaluate 13 representative polyp/object segmentation models on our\nSUN-SEG dataset and provide attribute-based comparisons. Finally, we discuss\nseveral open issues and suggest possible research directions for the VPS\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_G/0/1/0/all/0/1\">Guobao Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chou_Y/0/1/0/all/0/1\">Yu-Cheng Chou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Class Weights with Multi-Modal Information for Partial Video Domain Adaptation. (arXiv:2204.06187v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06187","description":"<p>Assuming the source label space subsumes the target one, Partial Video Domain\nAdaptation (PVDA) is a more general and practical scenario for cross-domain\nvideo classification problems. The key challenge of PVDA is to mitigate the\nnegative transfer caused by the source-only outlier classes. To tackle this\nchallenge, a crucial step is to aggregate target predictions to assign class\nweights by up-weighing target classes and down-weighing outlier classes.\nHowever, the incorrect predictions of class weights can mislead the network and\nlead to negative transfer. Previous works improve the class weight accuracy by\nutilizing temporal features and attention mechanisms, but these methods may\nfall short when trying to generate accurate class weight when domain shifts are\nsignificant, as in most real-world scenarios. To deal with these challenges, we\npropose the Multi-modality Cluster-calibrated partial Adversarial Network\n(MCAN). MCAN enhances video feature extraction with multi-modal features from\nmultiple temporal scales to form more robust overall features. It utilizes a\nnovel class weight calibration method to alleviate the negative transfer caused\nby incorrect class weights. The calibration method tries to identify and weigh\ncorrect and incorrect predictions using distributional information implied by\nunsupervised clustering. Extensive experiments are conducted on prevailing PVDA\nbenchmarks, and the proposed MCAN achieves significant improvements when\ncompared to state-of-the-art PVDA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_K/0/1/0/all/0/1\">Kezhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIFS: Neural Implicit Function for General Shape Representation. (arXiv:2204.07126v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07126","description":"<p>Recent development of neural implicit function has shown tremendous success\non high-quality 3D shape reconstruction. However, most works divide the space\ninto inside and outside of the shape, which limits their representing power to\nsingle-layer and watertight shapes. This limitation leads to tedious data\nprocessing (converting non-watertight raw data to watertight) as well as the\nincapability of representing general object shapes in the real world. In this\nwork, we propose a novel method to represent general shapes including\nnon-watertight shapes and shapes with multi-layer surfaces. We introduce\nGeneral Implicit Function for 3D Shape (GIFS), which models the relationships\nbetween every two points instead of the relationships between points and\nsurfaces. Instead of dividing 3D space into predefined inside-outside regions,\nGIFS encodes whether two points are separated by any surface. Experiments on\nShapeNet show that GIFS outperforms previous state-of-the-art methods in terms\nof reconstruction quality, rendering efficiency, and visual fidelity. Project\npage is available at https://jianglongye.com/gifs .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianglong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A case for using rotation invariant features in state of the art feature matchers. (arXiv:2204.10144v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10144","description":"<p>The aim of this paper is to demonstrate that a state of the art feature\nmatcher (LoFTR) can be made more robust to rotations by simply replacing the\nbackbone CNN with a steerable CNN which is equivariant to translations and\nimage rotations. It is experimentally shown that this boost is obtained without\nreducing performance on ordinary illumination and viewpoint matching sequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokman_G/0/1/0/all/0/1\">Georg B&#xf6;kman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1\">Fredrik Kahl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10965","description":"<p>In this paper, we propose CLIP-Dissect, a new technique to automatically\ndescribe the function of individual hidden neurons inside vision networks.\nCLIP-Dissect leverages recent advances in multimodal vision/language models to\nlabel internal neurons with open-ended concepts without the need for any\nlabeled data or human examples, which are required for existing tools to\nsucceed. We show that CLIP-Dissect provides more accurate descriptions than\nexisting methods for neurons where the ground-truth is available as well as\nqualitatively good descriptions for hidden layer neurons. In addition, our\nmethod is very flexible: it is model agnostic, can easily handle new concepts\nand can be extended to take advantage of better multimodal models in the\nfuture. Finally CLIP-Dissect is computationally efficient and labels all\nneurons of a layer in a large vision model in tens of minutes.\n</p>\n<p>In this paper, we propose CLIP-Dissect, a new technique to automatically\ndescribe the function of individual hidden neurons inside vision networks.\nCLIP-Dissect leverages recent advances in multimodal vision/language models to\nlabel internal neurons with open-ended concepts without the need for any\nlabeled data or human examples, which are required for existing tools to\nsucceed. We show that CLIP-Dissect provides more accurate descriptions than\nexisting methods for last layer neurons where the ground-truth is available as\nwell as qualitatively good descriptions for hidden layer neurons. In addition,\nour method is very flexible: it is model agnostic, can easily handle new\nconcepts and can be extended to take advantage of better multimodal models in\nthe future. Finally CLIP-Dissect is computationally efficient and can label all\nneurons from five layers of ResNet-50 in just four minutes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oikarinen_T/0/1/0/all/0/1\">Tuomas Oikarinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1\">Tsui-Wei Weng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Urban Change Detection Using a Dual-Task Siamese Network and Semi-Supervised Learning. (arXiv:2204.12202v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12202","description":"<p>In this study, a Semi-Supervised Learning (SSL) method for improving urban\nchange detection from bi-temporal image pairs was presented. The proposed\nmethod adapted a Dual-Task Siamese Difference network that not only predicts\nchanges with the difference decoder, but also segments buildings for both\nimages with a semantics decoder. First, the architecture was modified to\nproduce a second change prediction derived from the semantics predictions.\nSecond, SSL was adopted to improve supervised change detection. For unlabeled\ndata, we introduced a loss that encourages the network to predict consistent\nchanges across the two change outputs. The proposed method was tested on urban\nchange detection using the SpaceNet7 dataset. SSL achieved improved results\ncompared to three fully supervised benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hafner_S/0/1/0/all/0/1\">Sebastian Hafner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yifang Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascetti_A/0/1/0/all/0/1\">Andrea Nascetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving. (arXiv:2205.00376v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00376","description":"<p>Detection of rare objects (e.g., traffic cones, traffic barrels and traffic\nwarning triangles) is an important perception task to improve the safety of\nautonomous driving. Training of such models typically requires a large number\nof annotated data which is expensive and time consuming to obtain. To address\nthe above problem, an emerging approach is to apply data augmentation to\nautomatically generate cost-free training samples. In this work, we propose a\nsystematic study on simple Copy-Paste data augmentation for rare object\ndetection in autonomous driving. Specifically, local adaptive instance-level\nimage transformation is introduced to generate realistic rare object masks from\nsource domain to the target domain. Moreover, traffic scene context is utilized\nto guide the placement of masks of rare objects. To this end, our data\naugmentation generates training data with high quality and realistic\ncharacteristics by leveraging both local and global consistency. In addition,\nwe build a new dataset, Rare Object Dataset (ROD), consisting 10k training\nimages, 4k validation images and the corresponding labels with a diverse range\nof scenarios in autonomous driving. Experiments on ROD show that our method\nachieves promising results on rare object detection. We also present a thorough\nstudy to illustrate the effectiveness of our local-adaptive and global\nconstraints based Copy-Paste data augmentation for rare object detection. The\ndata, development kit and more information of ROD are available online at:\n\\url{https://nullmax-vision.github.io}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Naifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Fan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Pengpeng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_E/0/1/0/all/0/1\">Erkang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation. (arXiv:2205.01271v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01271","description":"<p>Pose estimation plays a critical role in human-centered vision applications.\nHowever, it is difficult to deploy state-of-the-art HRNet-based pose estimation\nmodels on resource-constrained edge devices due to the high computational cost\n(more than 150 GMACs per frame). In this paper, we study efficient architecture\ndesign for real-time multi-person pose estimation on edge. We reveal that\nHRNet's high-resolution branches are redundant for models at the\nlow-computation region via our gradual shrinking experiments. Removing them\nimproves both efficiency and performance. Inspired by this finding, we design\nLitePose, an efficient single-branch architecture for pose estimation, and\nintroduce two simple approaches to enhance the capacity of LitePose, including\nFusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the\nredundancy in high-resolution branches, allowing scale-aware feature fusion\nwith low overhead. Large Kernel Convs significantly improve the model's\ncapacity and receptive field while maintaining a low computational cost. With\nonly 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3\nkernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the\nlatency by up to 5.0x without sacrificing performance, compared with prior\nstate-of-the-art efficient pose estimation models, pushing the frontier of\nreal-time multi-person pose estimation on edge. Our code and pre-trained models\nare released at https://github.com/mit-han-lab/litepose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Muyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Han Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Ming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Detection of Unknown Objects on Roads for Autonomous Driving. (arXiv:2205.01414v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.01414","description":"<p>Tremendous progress in deep learning over the last years has led towards a\nfuture with autonomous vehicles on our roads. Nevertheless, the performance of\ntheir perception systems is strongly dependent on the quality of the utilized\ntraining data. As these usually only cover a fraction of all object classes an\nautonomous driving system will face, such systems struggle with handling the\nunexpected. In order to safely operate on public roads, the identification of\nobjects from unknown classes remains a crucial task. In this paper, we propose\na novel pipeline to detect unknown objects. Instead of focusing on a single\nsensor modality, we make use of lidar and camera data by combining state-of-the\nart detection models in a sequential manner. We evaluate our approach on the\nWaymo Open Perception Dataset and point out current research gaps in anomaly\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogdoll_D/0/1/0/all/0/1\">Daniel Bogdoll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisen_E/0/1/0/all/0/1\">Enrico Eisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitsche_M/0/1/0/all/0/1\">Maximilian Nitsche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheib_C/0/1/0/all/0/1\">Christin Scheib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Efficient Medical Imaging with Self-Supervision. (arXiv:2205.09723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09723","description":"<p>Recent progress in Medical Artificial Intelligence (AI) has delivered systems\nthat can reach clinical expert level performance. However, such systems tend to\ndemonstrate sub-optimal \"out-of-distribution\" performance when evaluated in\nclinical settings different from the training environment. A common mitigation\nstrategy is to develop separate systems for each clinical setting using\nsite-specific data [1]. However, this quickly becomes impractical as medical\ndata is time-consuming to acquire and expensive to annotate [2]. Thus, the\nproblem of \"data-efficient generalization\" presents an ongoing difficulty for\nMedical AI development. Although progress in representation learning shows\npromise, their benefits have not been rigorously studied, specifically for\nout-of-distribution settings. To meet these challenges, we present REMEDIS, a\nunified representation learning strategy to improve robustness and\ndata-efficiency of medical imaging AI. REMEDIS uses a generic combination of\nlarge-scale supervised transfer learning with self-supervised learning and\nrequires little task-specific customization. We study a diverse range of\nmedical imaging tasks and simulate three realistic application scenarios using\nretrospective data. REMEDIS exhibits significantly improved in-distribution\nperformance with up to 11.5% relative improvement in diagnostic accuracy over a\nstrong supervised baseline. More importantly, our strategy leads to strong\ndata-efficient generalization of medical imaging AI, matching strong supervised\nbaselines using between 1% to 33% of retraining data across tasks. These\nresults suggest that REMEDIS can significantly accelerate the life-cycle of\nmedical imaging AI development thereby presenting an important step forward for\nmedical imaging AI to deliver broad impact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1\">Shekoofeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1\">Laura Culp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freyberg_J/0/1/0/all/0/1\">Jan Freyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1\">Basil Mustafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baur_S/0/1/0/all/0/1\">Sebastien Baur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacWilliams_P/0/1/0/all/0/1\">Patricia MacWilliams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1\">S. Sara Mahdavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wulczyn_E/0/1/0/all/0/1\">Ellery Wulczyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_B/0/1/0/all/0/1\">Boris Babenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1\">Megan Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loh_A/0/1/0/all/0/1\">Aaron Loh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Po-Hsuan Cameron Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bavishi_P/0/1/0/all/0/1\">Pinal Bavishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinney_S/0/1/0/all/0/1\">Scott Mayer McKinney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winkens_J/0/1/0/all/0/1\">Jim Winkens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhijit Guha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_Z/0/1/0/all/0/1\">Zach Beaver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_F/0/1/0/all/0/1\">Fiona Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krogue_J/0/1/0/all/0/1\">Justin Krogue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemadi_M/0/1/0/all/0/1\">Mozziyar Etemadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telang_U/0/1/0/all/0/1\">Umesh Telang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lily Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1\">Greg S. Corrado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_D/0/1/0/all/0/1\">Dale R. Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1\">Vivek Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Preserving Image Registration. (arXiv:2205.10120v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10120","description":"<p>Image registration is a key task in medical imaging applications, allowing to\nrepresent medical images in a common spatial reference frame. Current\nliterature on image registration is generally based on the assumption that\nimages are usually accessible to the researcher, from which the spatial\ntransformation is subsequently estimated. This common assumption may not be met\nin current practical applications, since the sensitive nature of medical images\nmay ultimately require their analysis under privacy constraints, preventing to\nshare the image content in clear form. In this work, we formulate the problem\nof image registration under a privacy preserving regime, where images are\nassumed to be confidential and cannot be disclosed in clear. We derive our\nprivacy preserving image registration framework by extending classical\nregistration paradigms to account for advanced cryptographic tools, such as\nsecure multi-party computation and homomorphic encryption, that enable the\nexecution of operations without leaking the underlying data. To overcome the\nproblem of performance and scalability of cryptographic tools in high\ndimensions, we first propose to optimize the underlying image registration\noperations using gradient approximations. We further revisit the use of\nhomomorphic encryption and use a packing method to allow the encryption and\nmultiplication of large matrices more efficiently. We demonstrate our privacy\npreserving framework in linear and non-linear registration problems, evaluating\nits accuracy and scalability with respect to standard image registration. Our\nresults show that privacy preserving image registration is feasible and can be\nadopted in sensitive medical imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taiello_R/0/1/0/all/0/1\">Riccardo Taiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onen_M/0/1/0/all/0/1\">Melek &#xd6;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humbert_O/0/1/0/all/0/1\">Olivier Humbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenzi_M/0/1/0/all/0/1\">Marco Lorenzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Optimizing Color Rendition and In-Camera Backgrounds in an RGB Virtual Production Stage. (arXiv:2205.12403v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12403","description":"<p>While the LED panels used in virtual production systems can display vibrant\nimagery with a wide color gamut, they produce problematic color shifts when\nused as lighting due to their peaky spectral output from narrow-band red,\ngreen, and blue LEDs. In this work, we present an improved color calibration\nprocess for virtual production stages which ameliorates this color rendition\nproblem while also passing through accurate in-camera background colors. We do\nthis by optimizing linear color correction transformations for 1) the LED panel\npixels visible in the field of view of the camera, 2) the pixels outside the\nfield of view of the camera illuminating the subjects, and, as a post-process,\n3) the pixel values recorded by the camera. The result is that footage shot in\nan RGB LED panel virtual production stage can exhibit more accurate skin tones\nand costume colors while still reproducing the desired colors of the in-camera\nbackground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1\">Chloe LeGendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepicovsky_L/0/1/0/all/0/1\">Lukas Lepicovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debevec_P/0/1/0/all/0/1\">Paul Debevec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cervical Glandular Cell Detection from Whole Slide Image with Out-Of-Distribution Data. (arXiv:2205.14625v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14625","description":"<p>Cervical glandular cell (GC) detection is a key step in computer-aided\ndiagnosis for cervical adenocarcinomas screening. It is challenging to\naccurately recognize GCs in cervical smears in which squamous cells are the\nmajor. Widely existing Out-Of-Distribution (OOD) data in the entire smear leads\ndecreasing reliability of machine learning system for GC detection. Although,\nthe State-Of-The-Art (SOTA) deep learning model can outperform pathologists in\npreselected regions of interest, the mass False Positive (FP) prediction with\nhigh probability is still unsolved when facing such gigapixel whole slide\nimage. This paper proposed a novel PolarNet based on the morphological prior\nknowledge of GC trying to solve the FP problem via a self-attention mechanism\nin eight-neighbor. It estimates the polar orientation of nucleus of GC. As a\nplugin module, PolarNet can guide the deep feature and predicted confidence of\ngeneral object detection models. In experiments, we discovered that general\nmodels based on four different frameworks can reject FP in small image set and\nincrease the mean of average precision (mAP) by $\\text{0.007}\\sim\\text{0.015}$\nin average, where the highest exceeds the recent cervical cell detection model\n0.037. By plugging PolarNet, the deployed C++ program improved by 8.8\\% on\naccuracy of top-20 GC detection from external WSIs, while sacrificing 14.4 s of\ncomputational time. Code is available in\nhttps://github.com/Chrisa142857/PolarNet-GCdet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shenghua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jing Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehua Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Building Energy Efficiency From Street View Imagery, Aerial Imagery, and Land Surface Temperature Data. (arXiv:2206.02270v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02270","description":"<p>Decarbonizing the building sector by improving the energy efficiency of the\nexisting building stock through retrofits in a targeted and efficient way\nremains challenging. This is because, as of now, the energy efficiency of\nbuildings is generally determined by on-site visits of certified energy\nauditors which makes the process slow, costly, and geographically incomplete.\nIn order to accelerate the identification of promising retrofit targets on a\nlarge scale, we propose to estimate building energy efficiency from remotely\nsensed data sources only. To do so, we collect street view, aerial view,\nfootprint, and satellite-borne land surface temperature (LST) data for almost\n40,000 buildings across four diverse geographies in the United Kingdom. After\ntraining multiple end-to-end deep learning models on the fused input data in\norder to classify buildings as energy efficient (EU rating A-D) or inefficient\n(EU rating E-G), we analyze the best performing models quantitatively as well\nas qualitatively. Lastly, we extend our analysis by studying the predictive\npower of each data source in an ablation study. We find that the best\nend-to-end deep learning model achieves a macro-averaged F1-score of 62.06% and\noutperforms the k-NN and SVM-based baseline models by 5.62 to 11.47 percentage\npoints, respectively. As such, this work shows the potential and complementary\nnature of remotely sensed data in predicting energy efficiency and opens up new\nopportunities for future work to integrate additional data sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mayer_K/0/1/0/all/0/1\">Kevin Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1\">Lukas Haas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Layers are Equivariant to Discrete Shifts But Not Continuous Translations. (arXiv:2206.04979v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04979","description":"<p>The purpose of this short and simple note is to clarify a common\nmisconception about convolutional neural networks (CNNs). CNNs are made up of\nconvolutional layers which are shift equivariant due to weight sharing.\nHowever, convolutional layers are not translation equivariant, even when\nboundary effects are ignored and when pooling and subsampling are absent. This\nis because shift equivariance is a discrete symmetry while translation\nequivariance is a continuous symmetry. This fact is well known among\nresearchers in equivariant machine learning, but is usually overlooked among\nnon-experts. To minimize confusion, we suggest using the term `shift\nequivariance' to refer to discrete shifts in pixels and `translation\nequivariance' to refer to continuous translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McGreivy_N/0/1/0/all/0/1\">Nick McGreivy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakim_A/0/1/0/all/0/1\">Ammar Hakim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-purpose Real Haze Benchmark with Quantifiable Haze Levels and Ground Truth. (arXiv:2206.06427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.06427","description":"<p>Imagery collected from outdoor visual environments is often degraded due to\nthe presence of dense smoke or haze. A key challenge for research in scene\nunderstanding in these degraded visual environments (DVE) is the lack of\nrepresentative benchmark datasets. These datasets are required to evaluate\nstate-of-the-art object recognition and other computer vision algorithms in\ndegraded settings. In this paper, we address some of these limitations by\nintroducing the first paired real image benchmark dataset with hazy and\nhaze-free images, and in-situ haze density measurements. This dataset was\nproduced in a controlled environment with professional smoke generating\nmachines that covered the entire scene, and consists of images captured from\nthe perspective of both an unmanned aerial vehicle (UAV) and an unmanned ground\nvehicle (UGV). We also evaluate a set of representative state-of-the-art\ndehazing approaches as well as object detectors on the dataset. The full\ndataset presented in this paper, including the ground truth object\nclassification bounding boxes and haze density measurements, is provided for\nthe community to evaluate their algorithms at: https://a2i2-archangel.vision. A\nsubset of this dataset has been used for the Object Detection in Haze Track of\nCVPR UG2 2022 challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1\">Priya Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thielke_M/0/1/0/all/0/1\">Matthew D Thielke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_J/0/1/0/all/0/1\">John G Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_A/0/1/0/all/0/1\">Andre V Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAgostino_J/0/1/0/all/0/1\">John A D&#x27;Agostino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">James D Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quang_L/0/1/0/all/0/1\">Long P Quang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uplinger_J/0/1/0/all/0/1\">James R Uplinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heesung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantitative Imaging Principles Improves Medical Image Learning. (arXiv:2206.06663v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2206.06663","description":"<p>Fundamental differences between natural and medical images have recently\nfavored the use of self-supervised learning (SSL) over ImageNet transfer\nlearning for medical image applications. Differences between image types are\nprimarily due to the imaging modality and medical images utilize a wide range\nof physics based techniques while natural images are captured using only\nvisible light. While many have demonstrated that SSL on medical images has\nresulted in better downstream task performance, our work suggests that more\nperformance can be gained. The scientific principles which are used to acquire\nmedical images are not often considered when constructing learning problems.\nFor this reason, we propose incorporating quantitative imaging principles\nduring generative SSL to improve image quality and quantitative biological\naccuracy. We show that this training schema results in better starting states\nfor downstream supervised training on limited data. Our model also generates\nimages that validate on clinical quantitative analysis software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Leong_L/0/1/0/all/0/1\">Lambert T. Leong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wong_M/0/1/0/all/0/1\">Michael C. Wong</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Glaser_Y/0/1/0/all/0/1\">Yannik Glaser</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wolfgruber_T/0/1/0/all/0/1\">Thomas Wolfgruber</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Heymsfield_S/0/1/0/all/0/1\">Steven B. Heymsfield</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sadwoski_P/0/1/0/all/0/1\">Peter Sadwoski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shepherd_J/0/1/0/all/0/1\">John A. Shepherd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-Based Adversarial and Out-of-Distribution Detection. (arXiv:2206.08255v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.08255","description":"<p>We propose to utilize gradients for detecting adversarial and\nout-of-distribution samples. We introduce confounding labels -- labels that\ndiffer from normal labels seen during training -- in gradient generation to\nprobe the effective expressivity of neural networks. Gradients depict the\namount of change required for a model to properly represent given inputs,\nproviding insight into the representational power of the model established by\nnetwork architectural properties as well as training data. By introducing a\nlabel of different design, we remove the dependency on ground truth labels for\ngradient generation during inference. We show that our gradient-based approach\nallows for capturing the anomaly in inputs based on the effective expressivity\nof the models with no hyperparameter tuning or additional processing, and\noutperforms state-of-the-art methods for adversarial and out-of-distribution\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinsol Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhushankar_M/0/1/0/all/0/1\">Mohit Prabhushankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlRegib_G/0/1/0/all/0/1\">Ghassan AlRegib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Long-Tailed Bird Audio Recognition. (arXiv:2206.11260v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.11260","description":"<p>It is easier to hear birds than see them. However, they still play an\nessential role in nature and are excellent indicators of deteriorating\nenvironmental quality and pollution. Recent advances in Deep Neural Networks\nallow us to process audio data to detect and classify birds. This technology\ncan assist researchers in monitoring bird populations and biodiversity. We\npropose a sound detection and classification pipeline to analyze complex\nsoundscape recordings and identify birdcalls in the background. Our method\nlearns from weak labels and few data and acoustically recognizes the bird\nspecies. Our solution achieved 18th place of 807 teams at the BirdCLEF 2022\nChallenge hosted on Kaggle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_U/0/1/0/all/0/1\">Ui-Jin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CV 3315 Is All You Need : Semantic Segmentation Competition. (arXiv:2206.12571v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12571","description":"<p>This competition focus on Urban-Sense Segmentation based on the vehicle\ncamera view. Class highly unbalanced Urban-Sense images dataset challenge the\nexisting solutions and further studies. Deep Conventional neural network-based\nsemantic segmentation methods such as encoder-decoder architecture and\nmulti-scale and pyramid-based approaches become flexible solutions applicable\nto real-world applications. In this competition, we mainly review the\nliterature and conduct experiments on transformer-driven methods especially\nSegFormer, to achieve an optimal trade-off between performance and efficiency.\nFor example, SegFormer-B0 achieved 74.6% mIoU with the smallest FLOPS, 15.6G,\nand the largest model, SegFormer- B5 archived 80.2% mIoU. According to multiple\nfactors, including individual case failure analysis, individual class\nperformance, training pressure and efficiency estimation, the final candidate\nmodel for the competition is SegFormer- B2 with 50.6 GFLOPS and 78.5% mIoU\nevaluated on the testing set. Checkout our code implementation at\nhttps://vmv.re/cv3315.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Akide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view Feature Augmentation with Adaptive Class Activation Mapping. (arXiv:2206.12943v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12943","description":"<p>We propose an end-to-end-trainable feature augmentation module built for\nimage classification that extracts and exploits multi-view local features to\nboost model performance. Different from using global average pooling (GAP) to\nextract vectorized features from only the global view, we propose to sample and\nensemble diverse multi-view local features to improve model robustness. To\nsample class-representative local features, we incorporate a simple auxiliary\nclassifier head (comprising only one 1$\\times$1 convolutional layer) which\nefficiently and adaptively attends to class-discriminative local regions of\nfeature maps via our proposed AdaCAM (Adaptive Class Activation Mapping).\nExtensive experiments demonstrate consistent and noticeable performance gains\nachieved by our multi-view feature augmentation module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingjie Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Strategy Optimized Pix2pix Approach for SAR-to-Optical Image Translation Task. (arXiv:2206.13042v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13042","description":"<p>This technical report summarizes the analysis and approach on the\nimage-to-image translation task in the Multimodal Learning for Earth and\nEnvironment Challenge (MultiEarth 2022). In terms of strategy optimization,\ncloud classification is utilized to filter optical images with dense cloud\ncoverage to aid the supervised learning alike approach. The commonly used\npix2pix framework with a few optimizations is applied to build the model. A\nweighted combination of mean squared error and mean absolute error is\nincorporated in the loss function. As for evaluation, peak to signal ratio and\nstructural similarity were both considered in our preliminary analysis. Lastly,\nour method achieved the second place with a final error score of 0.0412. The\nresults indicate great potential towards SAR-to-optical translation in remote\nsensing tasks, specifically for the support of long-term environmental\nmonitoring and protection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fujian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yashu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kezhao Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SearchMorph:Multi-scale Correlation Iterative Network for Deformable Registration. (arXiv:2206.13076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13076","description":"<p>Deformable image registration provides dynamic information about the image\nand is essential in medical image analysis. However, due to the different\ncharacteristics of single-temporal brain MR images and multi-temporal\nechocardiograms, it is difficult to accurately register them using the same\nalgorithm or model. We propose an unsupervised multi-scale correlation\niterative registration network (SearchMorph), and the model has three\nhighlights. (1)We introduced cost volumes to strengthen feature correlations\nand constructed correlation pyramids to complement multi-scale correlation\ninformation. (2) We designed the search module to search for the registration\nof features in multi-scale pyramids. (3) We use the GRU module for iterative\nrefinement of the deformation field. The proposed network in this paper shows\nleadership in common single-temporal registration tasks and solves\nmulti-temporal motion estimation tasks. The experimental results show that our\nproposed method achieves higher registration accuracy and a lower folding point\nratio than the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xiao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shuxin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhemin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shunmin Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1\">Alex Noel Joseph Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yibiao Rong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Medical Image Fusion Method based on MDLatLRRv2. (arXiv:2206.15179v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.15179","description":"<p>Since MDLatLRR only considers detailed parts (salient features) of input\nimages extracted by latent low-rank representation (LatLRR), it doesn't use\nbase parts (principal features) extracted by LatLRR effectively. Therefore, we\nproposed an improved multi-level decomposition method called MDLatLRRv2 which\neffectively analyzes and utilizes all the image features obtained by LatLRR.\nThen we apply MDLatLRRv2 to medical image fusion. The base parts are fused by\naverage strategy and the detail parts are fused by nuclear-norm operation. The\ncomparison with the existing methods demonstrates that the proposed method can\nachieve state-of-the-art fusion performance in objective and subjective\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_X/0/1/0/all/0/1\">Xu Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class Impression for Data-free Incremental Learning. (arXiv:2207.00005v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00005","description":"<p>Standard deep learning-based classification approaches require collecting all\nsamples from all classes in advance and are trained offline. This paradigm may\nnot be practical in real-world clinical applications, where new classes are\nincrementally introduced through the addition of new data. Class incremental\nlearning is a strategy allowing learning from such data. However, a major\nchallenge is catastrophic forgetting, i.e., performance degradation on previous\nclasses when adapting a trained model to new data. Prior methodologies to\nalleviate this challenge save a portion of training data require perpetual\nstorage of such data that may introduce privacy issues. Here, we propose a\nnovel data-free class incremental learning framework that first synthesizes\ndata from the model trained on previous classes to generate a \\ours.\nSubsequently, it updates the model by combining the synthesized data with new\nclass data. Furthermore, we incorporate a cosine normalized Cross-entropy loss\nto mitigate the adverse effects of the imbalance, a margin loss to increase\nseparation among previous classes and new ones, and an intra-domain contrastive\nloss to generalize the model trained on the synthesized data to real data. We\ncompare our proposed framework with state-of-the-art methods in class\nincremental learning, where we demonstrate improvement in accuracy for the\nclassification of 11,062 echocardiography cine series of patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayromlou_S/0/1/0/all/0/1\">Sana Ayromlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abolmaesumi_P/0/1/0/all/0/1\">Purang Abolmaesumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_T/0/1/0/all/0/1\">Teresa Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Query-Key Pairwise Interactions in Vision Transformers. (arXiv:2207.00188v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00188","description":"<p>Vision Transformers have achieved state-of-the-art performance in many visual\ntasks. Due to the quadratic computational and memory complexities of\nself-attention, recent works either apply attention only to low-resolution\ninputs or restrict the receptive field to a small local region. To overcome\nthese limitations, we propose key-only attention, which excludes query-key\npairwise interactions and uses a compute-efficient saliency-gate to obtain\nattention weights, modeling local-global interactions in all stages. Key-only\nattention has linear computational and memory complexities w.r.t input size. We\nuse alternate layout to hybridize convolution and attention layers instead of\ngrafting which is suggested by previous works, so that all stages can benefit\nfrom both spatial attentions and convolutions. We leverage these improvements\nto develop a new self-attention model family, LinGlos, which reach\nstate-of-the-art accuracies on the parameter-limited setting of ImageNet\nclassification benchmark, and outperform baselines significantly in downstream\ntasks, e.g., COCO object detection and ADE20K semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yangxin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Covid-19 Detection Using transfer Learning Approach from Computed Temography Images. (arXiv:2207.00259v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.00259","description":"<p>Our main goal in this study is to propose a transfer learning based method\nfor COVID-19 detection from Computed Tomography (CT) images. The transfer\nlearning model used for the task is a pretrained Xception model. Both model\narchitecture and pre-trained weights on ImageNet were used. The resulting\nmodified model was trained with 128 batch size and 224x224, 3 channeled input\nimages, converted from original 512x512, grayscale images. The dataset used is\na the COV19-CT-DB. Labels in the dataset include COVID-19 cases and\nNon-COVID-19 cases for COVID-1919 detection. Firstly, a accuracy and loss on\nthe validation partition of the dataset as well as precision recall and macro\nF1 score were used to measure the performance of the proposed method. The\nresulting Macro F1 score on the validation set exceeded the baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1\">Kenan Morani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balikci_M/0/1/0/all/0/1\">Muhammet Fatih Balikci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Altuntas_T/0/1/0/all/0/1\">Tayfun Yigit Altuntas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unay_D/0/1/0/all/0/1\">Devrim Unay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. (arXiv:2207.00278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.00278","description":"<p>Due to its powerful feature learning capability and high efficiency, deep\nhashing has achieved great success in large-scale image retrieval. Meanwhile,\nextensive works have demonstrated that deep neural networks (DNNs) are\nsusceptible to adversarial examples, and exploring adversarial attack against\ndeep hashing has attracted many research efforts. Nevertheless, backdoor\nattack, another famous threat to DNNs, has not been studied for deep hashing\nyet. Although various backdoor attacks have been proposed in the field of image\nclassification, existing approaches failed to realize a truly imperceptive\nbackdoor attack that enjoys invisible triggers and clean label setting\nsimultaneously, and they also cannot meet the intrinsic demand of image\nretrieval backdoor.\n</p>\n<p>In this paper, we propose BadHash, the first generative-based imperceptible\nbackdoor attack against deep hashing, which can effectively generate invisible\nand input-specific poisoned images with clean label. Specifically, we first\npropose a new conditional generative adversarial network (cGAN) pipeline to\neffectively generate poisoned samples. For any given benign image, it seeks to\ngenerate a natural-looking poisoned counterpart with a unique invisible\ntrigger. In order to improve the attack effectiveness, we introduce a\nlabel-based contrastive learning network LabCLN to exploit the semantic\ncharacteristics of different labels, which are subsequently used for confusing\nand misleading the target model to learn the embedded trigger. We finally\nexplore the mechanism of backdoor attacks on image retrieval in the hash space.\nExtensive experiments on multiple benchmark datasets verify that BadHash can\ngenerate imperceptible poisoned samples with strong attack ability and\ntransferability over state-of-the-art deep hashing schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengshan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yechao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leo Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yifeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HE_Y/0/1/0/all/0/1\">Yuanyuan HE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hai Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt. (arXiv:2206.07137v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2206.07137","description":"<p>Training on web-scale data can take months. But most computation and time is\nwasted on redundant and noisy points that are already learnt or not learnable.\nTo accelerate training, we introduce Reducible Holdout Loss Selection\n(RHO-LOSS), a simple but principled technique which selects approximately those\npoints for training that most reduce the model's generalization loss. As a\nresult, RHO-LOSS mitigates the weaknesses of existing data selection methods:\ntechniques from the optimization literature typically select 'hard' (e.g. high\nloss) points, but such points are often noisy (not learnable) or less\ntask-relevant. Conversely, curriculum learning prioritizes 'easy' points, but\nsuch points need not be trained on once learned. In contrast, RHO-LOSS selects\npoints that are learnable, worth learning, and not yet learnt. RHO-LOSS trains\nin far fewer steps than prior art, improves accuracy, and speeds up training on\na wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and\nBERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in\n18x fewer steps and reaches 2% higher final accuracy than uniform data\nshuffling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1\">S&#xf6;ren Mindermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1\">Jan Brauner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_M/0/1/0/all/0/1\">Muhammed Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mrinank Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirsch_A/0/1/0/all/0/1\">Andreas Kirsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtgen_B/0/1/0/all/0/1\">Benedikt H&#xf6;ltgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Aidan N. Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1\">Sebastian Farquhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}