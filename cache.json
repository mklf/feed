{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Named Entity Recognition for Partially Annotated Datasets. (arXiv:2204.09081v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09081","description":"<p>The most common Named Entity Recognizers are usually sequence taggers trained\non fully annotated corpora, i.e. the class of all words for all entities is\nknown. Partially annotated corpora, i.e. some but not all entities of some\ntypes are annotated, are too noisy for training sequence taggers since the same\nentity may be annotated one time with its true type but not another time,\nmisleading the tagger. Therefore, we are comparing three training strategies\nfor partially annotated datasets and an approach to derive new datasets for new\nclasses of entities from Wikipedia without time-consuming manual data\nannotation. In order to properly verify that our data acquisition and training\napproaches are plausible, we manually annotated test datasets for two new\nclasses, namely food and drugs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strobl_M/0/1/0/all/0/1\">Michael Strobl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trabelsi_A/0/1/0/all/0/1\">Amine Trabelsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimize_Prime@DravidianLangTech-ACL2022: Emotion Analysis in Tamil. (arXiv:2204.09087v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09087","description":"<p>This paper aims to perform an emotion analysis of social media comments in\nTamil. Emotion analysis is the process of identifying the emotional context of\nthe text. In this paper, we present the findings obtained by Team\nOptimize_Prime in the ACL 2022 shared task \"Emotion Analysis in Tamil.\" The\ntask aimed to classify social media comments into categories of emotion like\nJoy, Anger, Trust, Disgust, etc. The task was further divided into two\nsubtasks, one with 11 broad categories of emotions and the other with 31\nspecific categories of emotion. We implemented three different approaches to\ntackle this problem: transformer-based models, Recurrent Neural Networks\n(RNNs), and Ensemble models. XLM-RoBERTa performed the best on the first task\nwith a macro-averaged f1 score of 0.27, while MuRIL provided the best results\non the second task with a macro-averaged f1 score of 0.13.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1\">Omkar Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1\">Shantanu Patankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1\">Onkar Litake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandke_A/0/1/0/all/0/1\">Aditya Mandke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadam_D/0/1/0/all/0/1\">Dipali Kadam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PICT@DravidianLangTech-ACL2022: Neural Machine Translation On Dravidian Languages. (arXiv:2204.09098v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09098","description":"<p>This paper presents a summary of the findings that we obtained based on the\nshared task on machine translation of Dravidian languages. We stood first in\nthree of the five sub-tasks which were assigned to us for the main shared task.\nWe carried out neural machine translation for the following five language\npairs: Kannada to Tamil, Kannada to Telugu, Kannada to Malayalam, Kannada to\nSanskrit, and Kannada to Tulu. The datasets for each of the five language pairs\nwere used to train various translation models, including Seq2Seq models such as\nLSTM, bidirectional LSTM, Conv2Seq, and training state-of-the-art as\ntransformers from scratch, and fine-tuning already pre-trained models. For some\nmodels involving monolingual corpora, we implemented backtranslation as well.\nThese models' accuracy was later tested with a part of the same dataset using\nBLEU score as an evaluation metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vyawahare_A/0/1/0/all/0/1\">Aditya Vyawahare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tangsali_R/0/1/0/all/0/1\">Rahul Tangsali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandke_A/0/1/0/all/0/1\">Aditya Mandke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1\">Onkar Litake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadam_D/0/1/0/all/0/1\">Dipali Kadam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Multi-hop Question Answering and Generation. (arXiv:2204.09140v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09140","description":"<p>The problem of Question Answering (QA) has attracted significant research\ninterest for long. Its relevance to language understanding and knowledge\nretrieval tasks, along with the simple setting makes the task of QA crucial for\nstrong AI systems. Recent success on simple QA tasks has shifted the focus to\nmore complex settings. Among these, Multi-Hop QA (MHQA) is one of the most\nresearched tasks over the recent years. The ability to answer multi-hop\nquestions and perform multi step reasoning can significantly improve the\nutility of NLP systems. Consequently, the field has seen a sudden surge with\nhigh quality datasets, models and evaluation strategies. The notion of\n`multiple hops' is somewhat abstract which results in a large variety of tasks\nthat require multi-hop reasoning. This implies that different datasets and\nmodels differ significantly which makes the field challenging to generalize and\nsurvey. This work aims to provide a general and formal definition of MHQA task,\nand organize and summarize existing MHQA frameworks. We also outline the best\nmethods to create MHQA datasets. The paper provides a systematic and thorough\nintroduction as well as the structuring of the existing attempts to this highly\ninteresting, yet quite challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mavi_V/0/1/0/all/0/1\">Vaibhav Mavi</a> (New York University, United States of America), <a href=\"http://arxiv.org/find/cs/1/au:+Jangra_A/0/1/0/all/0/1\">Anubhav Jangra</a> (Indian Institute of Technology, Patna, India), <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a> (University of Innsbruck, Austria)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALBETO and DistilBETO: Lightweight Spanish Language Models. (arXiv:2204.09145v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09145","description":"<p>In recent years there have been considerable advances in pre-trained language\nmodels, where non-English language versions have also been made available. Due\nto their increasing use, many lightweight versions of these models (with\nreduced parameters) have also been released to speed up training and inference\ntimes. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for\nlanguages other than English are still scarce. In this paper we present ALBETO\nand DistilBETO, which are versions of ALBERT and DistilBERT pre-trained\nexclusively on Spanish corpora. We train several versions of ALBETO ranging\nfrom 5M to 223M parameters and one of DistilBETO with 67M parameters. We\nevaluate our models in the GLUES benchmark that includes various natural\nlanguage understanding tasks in Spanish. The results show that our lightweight\nmodels achieve competitive results to those of BETO (Spanish-BERT) despite\nhaving fewer parameters. More specifically, our larger ALBETO model outperforms\nall other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets.\nHowever, BETO remains unbeaten for POS and NER. As a further contribution, all\nmodels are publicly available to the community for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Canete_J/0/1/0/all/0/1\">Jos&#xe9; Ca&#xf1;ete</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donoso_S/0/1/0/all/0/1\">Sebasti&#xe1;n Donoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bravo_Marquez_F/0/1/0/all/0/1\">Felipe Bravo-Marquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvallo_A/0/1/0/all/0/1\">Andr&#xe9;s Carvallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment. (arXiv:2204.09148v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09148","description":"<p>The instruction learning paradigm -- where a model learns to perform new\ntasks from task descriptions alone -- has become popular in general-purpose\nmodel research. The capabilities of large transformer models as instruction\nlearners, however, remain poorly understood. We use a controlled synthetic\nenvironment to characterize such capabilities. Specifically, we use the task of\ndeciding whether a given string matches a regular expression (viewed as an\ninstruction) to identify properties of tasks, instructions, and instances that\nmake instruction learning challenging. For instance, we find that our model, a\nfine-tuned T5-based text2text transformer, struggles with large regular\nlanguages, suggesting that less precise instructions are challenging for\nmodels. Additionally, instruction executions that require tracking longer\ncontexts of prior steps are also more difficult. We use our findings to\nsystematically construct a challenging instruction learning dataset, which we\ncall Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to\ncorrectly interpret only 65.6% of test instructions (with at least 90%\naccuracy), and 11%-24% of the instructions in out-of-distribution\ngeneralization settings. We propose Hard RegSet as a challenging instruction\nlearning task, and a controlled environment for studying instruction learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finlayson_M/0/1/0/all/0/1\">Matthew Finlayson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation. (arXiv:2204.09149v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09149","description":"<p>Task-oriented dialogue generation is challenging since the underlying\nknowledge is often dynamic and effectively incorporating knowledge into the\nlearning process is hard. It is particularly challenging to generate both\nhuman-like and informative responses in this setting. Recent research primarily\nfocused on various knowledge distillation methods where the underlying\nrelationship between the facts in a knowledge base is not effectively captured.\nIn this paper, we go one step further and demonstrate how the structural\ninformation of a knowledge graph can improve the system's inference\ncapabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue\nsystem that effectively incorporates knowledge into a language model. Our\nproposed system views relational knowledge as a knowledge graph and introduces\n(1) a structure-aware knowledge embedding technique, and (2) a knowledge\ngraph-weighted attention masking strategy to facilitate the system selecting\nrelevant information during the dialogue generation. An empirical evaluation\ndemonstrates the effectiveness of DialoKG over state-of-the-art methods on\nseveral standard benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rony_M/0/1/0/all/0/1\">Md Rashad Al Hasan Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Gender Representation in Multilingual Models. (arXiv:2204.09168v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09168","description":"<p>Multilingual language models were shown to allow for nontrivial transfer\nacross scripts and languages. In this work, we study the structure of the\ninternal representations that enable this transfer. We focus on the\nrepresentation of gender distinctions as a practical case study, and examine\nthe extent to which the gender concept is encoded in shared subspaces across\ndifferent languages. Our analysis shows that gender representations consist of\nseveral prominent components that are shared across languages, alongside\nlanguage-specific components. The existence of language-independent and\nlanguage-specific components provides an explanation for an intriguing\nempirical observation we make: while gender classification transfers well\nacross languages, interventions for gender removal, trained on a single\nlanguage, do not transfer easily to others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonen_H/0/1/0/all/0/1\">Hila Gonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Representation Collapse of Sparse Mixture of Experts. (arXiv:2204.09179v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09179","description":"<p>Sparse mixture of experts provides larger model capacity while requiring a\nconstant computational overhead. It employs the routing mechanism to distribute\ninput tokens to the best-matched experts according to their hidden\nrepresentations. However, learning such a routing mechanism encourages token\nclustering around expert centroids, implying a trend toward representation\ncollapse. In this work, we propose to estimate the routing scores between\ntokens and experts on a low-dimensional hypersphere. We conduct extensive\nexperiments on cross-lingual language model pre-training and fine-tuning on\ndownstream tasks. Experimental results across seven multilingual benchmarks\nshow that our method achieves consistent gains. We also present a comprehensive\nanalysis on the representation and routing behaviors of our models. Our method\nalleviates the representation collapse issue and achieves more consistent\nrouting than the baseline mixture-of-experts methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Barun Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who Is Missing? Characterizing the Participation of Different Demographic Groups in a Korean Nationwide Daily Conversation Corpus. (arXiv:2204.09209v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09209","description":"<p>A conversation corpus is essential to build interactive AI applications.\nHowever, the demographic information of the participants in such corpora is\nlargely underexplored mainly due to the lack of individual data in many\ncorpora. In this work, we analyze a Korean nationwide daily conversation corpus\nconstructed by the National Institute of Korean Language (NIKL) to characterize\nthe participation of different demographic (age and sex) groups in the corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jisun An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kunwoo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LingYi: Medical Conversational Question Answering System based on Multi-modal Knowledge Graphs. (arXiv:2204.09220v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09220","description":"<p>The medical conversational system can relieve the burden of doctors and\nimprove the efficiency of healthcare, especially during the pandemic. This\npaper presents a medical conversational question answering (CQA) system based\non the multi-modal knowledge graph, namely \"LingYi\", which is designed as a\npipeline framework to maintain high flexibility. Our system utilizes automated\nmedical procedures including medical triage, consultation, image-text drug\nrecommendation and record. To conduct knowledge-grounded dialogues with\npatients, we first construct a Chinese Medical Multi-Modal Knowledge Graph\n(CM3KG) and collect a large-scale Chinese Medical CQA (CMCQA) dataset. Compared\nwith the other existing medical question-answering systems, our system adopts\nseveral state-of-the-art technologies including medical entity disambiguation\nand medical dialogue generation, which is more friendly to provide medical\nservices to patients. In addition, we have open-sourced our codes which contain\nback-end models and front-end web pages at https://github.com/WENGSYX/LingYi.\nThe datasets including CM3KG at https://github.com/WENGSYX/CM3KG and CMCQA at\nhttps://github.com/WENGSYX/CMCQA are also released to further promote future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-LITE: Learning Transferable Visual Models with External Knowledge. (arXiv:2204.09222v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09222","description":"<p>Recent state-of-the-art computer vision systems are trained from natural\nlanguage supervision, ranging from simple object category names to descriptive\ncaptions. This free form of supervision ensures high generality and usability\nof the learned visual models, based on extensive heuristics on data collection\nto cover as many visual concepts as possible. Alternatively, learning with\nexternal knowledge about images is a promising way which leverages a much more\nstructured source of supervision. In this paper, we propose K-LITE\n(Knowledge-augmented Language-Image Training and Evaluation), a simple strategy\nto leverage external knowledge to build transferable visual systems: In\ntraining, it enriches entities in natural language with WordNet and Wiktionary\nknowledge, leading to an efficient and scalable approach to learning image\nrepresentations that can understand both visual concepts and their knowledge;\nIn evaluation, the natural language is also augmented with external knowledge\nand then used to reference learned visual concepts (or describe new ones) to\nenable zero-shot and few-shot transfer of the pre-trained models. We study the\nperformance of K-LITE on two important computer vision problems, image\nclassification and object detection, benchmarking on 20 and 13 different\nexisting datasets, respectively. The proposed knowledge-augmented models show\nsignificant improvement in transfer learning performance over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-stitched Multi-modal Encoders. (arXiv:2204.09227v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09227","description":"<p>In this paper, we propose a novel architecture for multi-modal speech and\ntext input. We combine pretrained speech and text encoders using multi-headed\ncross-modal attention and jointly fine-tune on the target problem. The\nresultant architecture can be used for continuous token-level classification or\nutterance-level prediction acting on simultaneous text and speech. The\nresultant encoder efficiently captures both acoustic-prosodic and lexical\ninformation. We compare the benefits of multi-headed attention-based fusion for\nmulti-modal utterance-level classification against a simple concatenation of\npre-pooled, modality-specific representations. Our model architecture is\ncompact, resource efficient, and can be trained on a single consumer GPU card.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_K/0/1/0/all/0/1\">Karan Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pressel_D/0/1/0/all/0/1\">Daniel Pressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1\">Ryan Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnari_B/0/1/0/all/0/1\">Bhargav Srinivas Chinnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeon-Jun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Semantics and Inference System for Temporal Order based on Japanese CCG. (arXiv:2204.09245v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09245","description":"<p>Natural Language Inference (NLI) is the task of determining whether a premise\nentails a hypothesis. NLI with temporal order is a challenging task because\ntense and aspect are complex linguistic phenomena involving interactions with\ntemporal adverbs and temporal connectives. To tackle this, temporal and\naspectual inference has been analyzed in various ways in the field of formal\nsemantics. However, a Japanese NLI system for temporal order based on the\nanalysis of formal semantics has not been sufficiently developed. We present a\nlogic-based NLI system that considers temporal order in Japanese based on\ncompositional semantics via Combinatory Categorial Grammar (CCG) syntactic\nanalysis. Our system performs inference involving temporal order by using\naxioms for temporal relations and automated theorem provers. We evaluate our\nsystem by experimenting with Japanese NLI datasets that involve temporal order.\nWe show that our system outperforms previous logic-based systems as well as\ncurrent deep learning-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_T/0/1/0/all/0/1\">Tomoki Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1\">Hitomi Yanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Target Domain Supervision for Open Retrieval QA. (arXiv:2204.09248v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09248","description":"<p>Neural passage retrieval is a new and promising approach in open retrieval\nquestion answering. In this work, we stress-test the Dense Passage Retriever\n(DPR) -- a state-of-the-art (SOTA) open domain neural retrieval model -- on\nclosed and specialized target domains such as COVID-19, and find that it lags\nbehind standard BM25 in this important real-world setting. To make DPR more\nrobust under domain shift, we explore its fine-tuning with synthetic training\nexamples, which we generate from unlabeled target domain text using a\ntext-to-text generator. In our experiments, this noisy but fully automated\ntarget domain supervision gives DPR a sizable advantage over BM25 in\nout-of-domain settings, making it a more viable model in practice. Finally, an\nensemble of BM25 and our improved DPR model yields the best results, further\npushing the SOTA for open retrieval QA on multiple out-of-domain test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Revanth Gangi Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_B/0/1/0/all/0/1\">Bhavani Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1\">Vittorio Castelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation. (arXiv:2204.09259v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09259","description":"<p>Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies\non a pre-trained general NMT model which is adapted to the new domain on a\nsample of in-domain parallel data. Without parallel data, there is no way to\nestimate the potential benefit of DA, nor the amount of parallel samples it\nwould require. It is however a desirable functionality that could help MT\npractitioners to make an informed decision before investing resources in\ndataset creation. We propose a Domain adaptation Learning Curve prediction\n(DaLC) model that predicts prospective DA performance based on in-domain\nmonolingual samples in the source language. Our model relies on the NMT encoder\nrepresentations combined with various instance and corpus-level features. We\ndemonstrate that instance-level is better able to distinguish between different\ndomains compared to corpus-level frameworks proposed in previous studies.\nFinally, we perform in-depth analyses of the results highlighting the\nlimitations of our approach, and provide directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Cheonbok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hantae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calapodescu_I/0/1/0/all/0/1\">Ioan Calapodescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyunchang Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-based Cross-Modal Retrieval with Probabilistic Representations. (arXiv:2204.09268v1 [cs.LG])","link":"http://arxiv.org/abs/2204.09268","description":"<p>Probabilistic embeddings have proven useful for capturing polysemous word\nmeanings, as well as ambiguity in image matching. In this paper, we study the\nadvantages of probabilistic embeddings in a cross-modal setting (i.e., text and\nimages), and propose a simple approach that replaces the standard vector point\nembeddings in extant image-text matching models with probabilistic\ndistributions that are parametrically learned. Our guiding hypothesis is that\nthe uncertainty encoded in the probabilistic embeddings captures the\ncross-modal ambiguity in the input instances, and that it is through capturing\nthis uncertainty that the probabilistic models can perform better at downstream\ntasks, such as image-to-text or text-to-image retrieval. Through extensive\nexperiments on standard and new benchmarks, we show a consistent advantage for\nprobabilistic representations in cross-modal retrieval, and validate the\nability of our embeddings to capture uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pishdad_L/0/1/0/all/0/1\">Leila Pishdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jepson_A/0/1/0/all/0/1\">Allan Jepson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazly_A/0/1/0/all/0/1\">Afsaneh Fazly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond. (arXiv:2204.09269v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09269","description":"<p>Non-autoregressive (NAR) generation, which is first proposed in neural\nmachine translation (NMT) to speed up inference, has attracted much attention\nin both machine learning and natural language processing communities. While NAR\ngeneration can significantly accelerate inference speed for machine\ntranslation, the speedup comes at the cost of sacrificed translation accuracy\ncompared to its counterpart, auto-regressive (AR) generation. In recent years,\nmany new models and algorithms have been designed/proposed to bridge the\naccuracy gap between NAR generation and AR generation. In this paper, we\nconduct a systematic survey with comparisons and discussions of various\nnon-autoregressive translation (NAT) models from different aspects.\nSpecifically, we categorize the efforts of NAT into several groups, including\ndata manipulation, modeling methods, training criterion, decoding algorithms,\nand the benefit from pre-trained models. Furthermore, we briefly review other\napplications of NAR models beyond machine translation, such as dialogue\ngeneration, text summarization, grammar error correction, semantic parsing,\nspeech synthesis, and automatic speech recognition. In addition, we also\ndiscuss potential directions for future exploration, including releasing the\ndependency of KD, dynamic length prediction, pre-training for NAR, and wider\napplications, etc. We hope this survey can help researchers capture the latest\nprogress in NAR generation, inspire the design of advanced NAR models and\nalgorithms, and enable industry practitioners to choose appropriate solutions\nfor their applications. The web page of this survey is at\n\\url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yisheng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situational Perception Guided Image Matting. (arXiv:2204.09276v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09276","description":"<p>Most automatic matting methods try to separate the salient foreground from\nthe background. However, the insufficient quantity and subjective bias of the\ncurrent existing matting datasets make it difficult to fully explore the\nsemantic association between object-to-object and object-to-environment in a\ngiven image. In this paper, we propose a Situational Perception Guided Image\nMatting (SPG-IM) method that mitigates subjective bias of matting annotations\nand captures sufficient situational perception information for better global\nsaliency distilled from the visual-to-textual task. SPG-IM can better associate\ninter-objects and object-to-environment saliency, and compensate the subjective\nnature of image matting and its expensive annotation. We also introduce a\ntextual Semantic Transformation (TST) module that can effectively transform and\nintegrate the semantic feature stream to guide the visual representations. In\naddition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed\nto adaptively switch multi-scale receptive fields and focal points to enhance\nboth global and local details. Extensive experiments demonstrate the\neffectiveness of situational perception guidance from the visual-to-textual\ntasks on image matting, and our model outperforms the state-of-the-art methods.\nWe also analyze the significance of different components in our model. The code\nwill be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Arabic Sentence Simplification via Classification and Generative Approaches. (arXiv:2204.09292v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09292","description":"<p>This paper presents an attempt to build a Modern Standard Arabic (MSA)\nsentence-level simplification system. We experimented with sentence\nsimplification using two approaches: (i) a classification approach leading to\nlexical simplification pipelines which use Arabic-BERT, a pre-trained\ncontextualised model, as well as a model of fastText word embeddings; and (ii)\na generative approach, a Seq2Seq technique by applying a multilingual\nText-to-Text Transfer Transformer mT5. We developed our training corpus by\naligning the original and simplified sentences from the internationally\nacclaimed Arabic novel \"Saaq al-Bambuu\". We evaluate effectiveness of these\nmethods by comparing the generated simple sentences to the target simple\nsentences using the BERTScore evaluation metric. The simple sentences produced\nby the mT5 model achieve P 0.72, R 0.68 and F-1 0.70 via BERTScore, while,\ncombining Arabic-BERT and fastText achieves P 0.97, R 0.97 and F-1 0.97. In\naddition, we report a manual error analysis for these experiments.\n\\url{https://github.com/Nouran-Khallaf/Lexical_Simplification}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khallaf_N/0/1/0/all/0/1\">Nouran Khallaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharoff_S/0/1/0/all/0/1\">Serge Sharoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Few-Shot Learning with FASL. (arXiv:2204.09347v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09347","description":"<p>Recent advances in natural language processing (NLP) have led to strong text\nclassification models for many tasks. However, still often thousands of\nexamples are needed to train models with good quality. This makes it\nchallenging to quickly develop and deploy new models for real world problems\nand business needs. Few-shot learning and active learning are two lines of\nresearch, aimed at tackling this problem. In this work, we combine both lines\ninto FASL, a platform that allows training text classification models using an\niterative and fast process. We investigate which active learning methods work\nbest in our few-shot setup. Additionally, we develop a model to predict when to\nstop annotating. This is relevant as in a few-shot setup we do not have access\nto a large validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Torro_G/0/1/0/all/0/1\">Guillermo P&#xe9;rez-Torr&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_A/0/1/0/all/0/1\">Angelo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative or Contrastive? Phrase Reconstruction for Better Sentence Representation Learning. (arXiv:2204.09358v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09358","description":"<p>Though offering amazing contextualized token-level representations, current\npre-trained language models actually take less attention on acquiring\nsentence-level representation during its self-supervised pre-training. If\nself-supervised learning can be distinguished into two subcategories,\ngenerative and contrastive, then most existing studies show that sentence\nrepresentation learning may more benefit from the contrastive methods but not\nthe generative methods. However, contrastive learning cannot be well compatible\nwith the common token-level generative self-supervised learning, and does not\nguarantee good performance on downstream semantic retrieval tasks. Thus, to\nalleviate such obvious inconveniences, we instead propose a novel generative\nself-supervised learning objective based on phrase reconstruction. Empirical\nstudies show that our generative learning may yield powerful enough sentence\nrepresentation and achieve performance in Sentence Textual Similarity (STS)\ntasks on par with contrastive learning. Further, in terms of unsupervised\nsetting, our generative method outperforms previous state-of-the-art SimCSE on\nthe benchmark of downstream semantic retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Intensity of Complaints on Social Media. (arXiv:2204.09366v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09366","description":"<p>Complaining is a speech act that expresses a negative inconsistency between\nreality and human expectations. While prior studies mostly focus on identifying\nthe existence or the type of complaints, in this work, we present the first\nstudy in computational linguistics of measuring the intensity of complaints\nfrom text. Analyzing complaints from such perspective is particularly useful,\nas complaints of certain degrees may cause severe consequences for companies or\norganizations. We create the first Chinese dataset containing 3,103 posts about\ncomplaints from Weibo, a popular Chinese social media platform. These posts are\nthen annotated with complaints intensity scores using Best-Worst Scaling (BWS)\nmethod. We show that complaints intensity can be accurately estimated by\ncomputational models with the best mean square error achieving 0.11.\nFurthermore, we conduct a comprehensive linguistic analysis around complaints,\nincluding the connections between complaints and sentiment, and a cross-lingual\ncomparison for complaints expressions used by Chinese and English speakers. We\nfinally show that our complaints intensity scores can be incorporated for\nbetter estimating the popularity of posts on social media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Ming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is BERT Robust to Label Noise? A Study on Learning with Noisy Labels in Text Classification. (arXiv:2204.09371v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09371","description":"<p>Incorrect labels in training data occur when human annotators make mistakes\nor when the data is generated via weak or distant supervision. It has been\nshown that complex noise-handling techniques - by modeling, cleaning or\nfiltering the noisy instances - are required to prevent models from fitting\nthis label noise. However, we show in this work that, for text classification\ntasks with modern NLP models like BERT, over a variety of noise types, existing\nnoisehandling methods do not always improve its performance, and may even\ndeteriorate it, suggesting the need for further investigation. We also back our\nobservations with a comprehensive analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael A. Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_F/0/1/0/all/0/1\">Fangzhou Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploration strategies for articulatory synthesis of complex syllable onsets. (arXiv:2204.09381v1 [cs.SD])","link":"http://arxiv.org/abs/2204.09381","description":"<p>High-quality articulatory speech synthesis has many potential applications in\nspeech science and technology. However, developing appropriate mappings from\nlinguistic specification to articulatory gestures is difficult and time\nconsuming. In this paper we construct an optimisation-based framework as a\nfirst step towards learning these mappings without manual intervention. We\ndemonstrate the production of syllables with complex onsets and discuss the\nquality of the articulatory gestures with reference to coarticulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_D/0/1/0/all/0/1\">Daniel R. van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Anqi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerazov_B/0/1/0/all/0/1\">Branislav Gerazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krug_P/0/1/0/all/0/1\">Paul K. Krug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birkholz_P/0/1/0/all/0/1\">Peter Birkholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Are What You Write: Preserving Privacy in the Era of Large Language Models. (arXiv:2204.09391v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09391","description":"<p>Large scale adoption of large language models has introduced a new era of\nconvenient knowledge transfer for a slew of natural language processing tasks.\nHowever, these models also run the risk of undermining user trust by exposing\nunwanted information about the data subjects, which may be extracted by a\nmalicious party, e.g. through adversarial attacks. We present an empirical\ninvestigation into the extent of the personal information encoded into\npre-trained representations by a range of popular models, and we show a\npositive correlation between the complexity of a model, the amount of data used\nin pre-training, and data leakage. In this paper, we present the first wide\ncoverage evaluation and comparison of some of the most popular\nprivacy-preserving algorithms, on a large, multi-lingual dataset on sentiment\nanalysis annotated with demographic information (location, age and gender). The\nresults show since larger and more complex models are more prone to leaking\nprivate information, use of privacy-preserving methods is highly desirable. We\nalso find that highly privacy-preserving technologies like differential privacy\n(DP) can have serious model utility effects, which can be ameliorated using\nhybrid or metric-DP techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plant_R/0/1/0/all/0/1\">Richard Plant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giuffrida_V/0/1/0/all/0/1\">Valerio Giuffrida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Corpus for Understanding and Generating Moral Stories. (arXiv:2204.09438v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09438","description":"<p>Teaching morals is one of the most important purposes of storytelling. An\nessential ability for understanding and writing moral stories is bridging story\nplots and implied morals. Its challenges mainly lie in: (1) grasping knowledge\nabout abstract concepts in morals, (2) capturing inter-event discourse\nrelations in stories, and (3) aligning value preferences of stories and morals\nconcerning good or bad behavior. In this paper, we propose two understanding\ntasks and two generation tasks to assess these abilities of machines. We\npresent STORAL, a new dataset of Chinese and English human-written moral\nstories. We show the difficulty of the proposed tasks by testing various models\nwith automatic and manual evaluation on STORAL. Furthermore, we present a\nretrieval-augmented algorithm that effectively exploits related concepts or\nevents in training sets as additional guidance to improve performance on these\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Transition Planning for Open-ended Text Generation. (arXiv:2204.09453v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09453","description":"<p>Open-ended text generation tasks, such as dialogue generation and story\ncompletion, require models to generate a coherent continuation given limited\npreceding context. The open-ended nature of these tasks brings new challenges\nto the neural auto-regressive text generators nowadays. Despite these neural\nmodels are good at producing human-like text, it is difficult for them to\narrange causalities and relations between given facts and possible ensuing\nevents. To bridge this gap, we propose a novel two-stage method which\nexplicitly arranges the ensuing events in open-ended text generation. Our\napproach can be understood as a specially-trained coarse-to-fine algorithm,\nwhere an event transition planner provides a \"coarse\" plot skeleton and a text\ngenerator in the second stage refines the skeleton. Experiments on two\nopen-ended text generation tasks demonstrate that our proposed method\neffectively improves the quality of the generated text, especially in coherence\nand diversity. The code is available at:\n\\url{https://github.com/qtli/EventPlanforTextGen}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qintong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Ranking and Aggregation of Label Descriptions for Zero-Shot Classifiers. (arXiv:2204.09481v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09481","description":"<p>Zero-shot text classifiers based on label descriptions embed an input text\nand a set of labels into the same space: measures such as cosine similarity can\nthen be used to select the most similar label description to the input text as\nthe predicted label. In a true zero-shot setup, designing good label\ndescriptions is challenging because no development set is available. Inspired\nby the literature on Learning with Disagreements, we look at how probabilistic\nmodels of repeated rating analysis can be used for selecting the best label\ndescriptions in an unsupervised fashion. We evaluate our method on a set of\ndiverse datasets and tasks (sentiment, topic and stance). Furthermore, we show\nthat multiple, noisy label descriptions can be aggregated to boost the\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basile_A/0/1/0/all/0/1\">Angelo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing to the Future: Mitigating Entity Bias in Fake News Detection. (arXiv:2204.09484v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09484","description":"<p>The wide dissemination of fake news is increasingly threatening both\nindividuals and society. Fake news detection aims to train a model on the past\nnews and detect fake news of the future. Though great efforts have been made,\nexisting fake news detection methods overlooked the unintended entity bias in\nthe real-world data, which seriously influences models' generalization ability\nto future data. For example, 97\\% of news pieces in 2010-2017 containing the\nentity `Donald Trump' are real in our data, but the percentage falls down to\nmerely 33\\% in 2018. This would lead the model trained on the former set to\nhardly generalize to the latter, as it tends to predict news pieces about\n`Donald Trump' as real for lower training loss. In this paper, we propose an\nentity debiasing framework (\\textbf{ENDEF}) which generalizes fake news\ndetection models to the future data by mitigating entity bias from a\ncause-effect perspective. Based on the causal graph among entities, news\ncontents, and news veracity, we separately model the contribution of each cause\n(entities and contents) during training. In the inference stage, we remove the\ndirect effect of the entities to mitigate entity bias. Extensive offline\nexperiments on the English and Chinese datasets demonstrate that the proposed\nframework can largely improve the performance of base fake news detectors, and\nonline tests verify its superiority in practice. To the best of our knowledge,\nthis is the first work to explicitly improve the generalization ability of fake\nnews detection models to the future data. The code has been released at\nhttps://github.com/ICTMCG/ENDEF-SIGIR2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yongchun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuokai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danding Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Neural Abstractive Summarization Methods and Factual Consistency of Summarization. (arXiv:2204.09519v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09519","description":"<p>Automatic summarization is the process of shortening a set of textual data\ncomputationally, to create a subset (a summary) that represents the most\nimportant pieces of information in the original text. Existing summarization\nmethods can be roughly divided into two types: extractive and abstractive. An\nextractive summarizer explicitly selects text snippets (words, phrases,\nsentences, etc.) from the source document, while an abstractive summarizer\ngenerates novel text snippets to convey the most salient concepts prevalent in\nthe source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Impact Model Narratives from Social Services' Text. (arXiv:2204.09557v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09557","description":"<p>Named entity recognition (NER) is an important task in narration extraction.\nNarration, as a system of stories, provides insights into how events and\ncharacters in the stories develop over time. This paper proposes an\narchitecture for NER on a corpus about social purpose organizations. This is\nthe first NER task specifically targeted at social service entities. We show\nhow this approach can be used for the sequencing of services and impacted\nclients with information extracted from unstructured text. The methodology\noutlines steps for extracting ontological representation of entities such as\nneeds and satisfiers and generating hypotheses to answer queries about impact\nmodels defined by social purpose organizations. We evaluate the model on a\ncorpus of social service descriptions with empirically calculated score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gajderowicz_B/0/1/0/all/0/1\">Bart Gajderowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosu_D/0/1/0/all/0/1\">Daniela Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_M/0/1/0/all/0/1\">Mark S Fox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-view Brain Decoding. (arXiv:2204.09564v1 [q-bio.NC])","link":"http://arxiv.org/abs/2204.09564","description":"<p>How the brain captures the meaning of linguistic stimuli across multiple\nviews is still a critical open question in neuroscience. Consider three\ndifferent views of the concept apartment: (1) picture (WP) presented with the\ntarget word label, (2) sentence (S) using the target word, and (3) word cloud\n(WC) containing the target word along with other semantically related words.\nUnlike previous efforts, which focus only on single view analysis, in this\npaper, we study the effectiveness of brain decoding in a zero-shot cross-view\nlearning setup. Further, we propose brain decoding in the novel context of\ncross-view-translation tasks like image captioning (IC), image tagging (IT),\nkeyword extraction (KE), and sentence formation (SF). Using extensive\nexperiments, we demonstrate that cross-view zero-shot brain decoding is\npractical leading to ~0.68 average pairwise accuracy across view pairs. Also,\nthe decoded representations are sufficiently detailed to enable high accuracy\nfor cross-view-translation tasks with following pairwise accuracy: IC (78.0),\nIT (83.0), KE (83.7) and SF (74.5). Analysis of the contribution of different\nbrain networks reveals exciting cognitive insights: (1) A high percentage of\nvisual voxels are involved in image captioning and image tagging tasks, and a\nhigh percentage of language voxels are involved in the sentence formation and\nkeyword extraction tasks. (2) Zero-shot accuracy of the model trained on S view\nand tested on WC view is better than same-view accuracy of the model trained\nand tested on WC view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arora_J/0/1/0/all/0/1\">Jashn Arora</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bapi_R/0/1/0/all/0/1\">Raju S. Bapi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Ethical Considerations of Text Simplification. (arXiv:2204.09565v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09565","description":"<p>This paper outlines the ethical implications of text simplification within\nthe framework of assistive systems. We argue that a distinction should be made\nbetween the technologies that perform text simplification and the realisation\nof these in assistive technologies. When using the latter as a motivation for\nresearch, it is important that the subsequent ethical implications be carefully\nconsidered. We provide guidelines for the framing of text simplification\nindependently of assistive systems, as well as suggesting directions for future\nresearch and discussion based on the concerns raised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gooding_S/0/1/0/all/0/1\">Sian Gooding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Descriptions of Movement Through Geovisual Analytics. (arXiv:2204.09588v1 [cs.HC])","link":"http://arxiv.org/abs/2204.09588","description":"<p>Sensemaking using automatically extracted information from text is a\nchallenging problem. In this paper, we address a specific type of information\nextraction, namely extracting information related to descriptions of movement.\nAggregating and understanding information related to descriptions of movement\nand lack of movement specified in text can lead to an improved understanding\nand sensemaking of movement phenomena of various types, e.g., migration of\npeople and animals, impediments to travel due to COVID-19, etc. We present\nGeoMovement, a system that is based on combining machine learning and\nrule-based extraction of movement-related information with state-of-the-art\nvisualization techniques. Along with the depiction of movement, our tool can\nextract and present a lack of movement. Very little prior work exists on\nautomatically extracting descriptions of movement, especially negation and\nmovement. Apart from addressing these, GeoMovement also provides a novel\nintegrated framework for combining these extraction modules with visualization.\nWe include two systematic case studies of GeoMovement that show how humans can\nderive meaningful geographic movement information. GeoMovement can complement\nprecise movement data, e.g., obtained using sensors, or be used by itself when\nprecise data is unavailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pezanowski_S/0/1/0/all/0/1\">Scott Pezanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Prasenjit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacEachren_A/0/1/0/all/0/1\">Alan M. MacEachren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning. (arXiv:2204.09589v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09589","description":"<p>In this paper, we study the named entity recognition (NER) problem under\ndistant supervision. Due to the incompleteness of the external dictionaries\nand/or knowledge bases, such distantly annotated training data usually suffer\nfrom a high false negative rate. To this end, we formulate the Distantly\nSupervised NER (DS-NER) problem via Multi-class Positive and Unlabeled (MPU)\nlearning and propose a theoretically and practically novel CONFidence-based MPU\n(Conf-MPU) approach. To handle the incomplete annotations, Conf-MPU consists of\ntwo steps. First, a confidence score is estimated for each token of being an\nentity token. Then, the proposed Conf-MPU risk estimation is applied to train a\nmulti-class classifier for the NER task. Thorough experiments on two benchmark\ndatasets labeled by various external knowledge demonstrate the superiority of\nthe proposed Conf-MPU over existing DS-NER methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuepei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Bias and Fairness in Natural Language Processing. (arXiv:2204.09591v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09591","description":"<p>As NLP models become more integrated with the everyday lives of people, it\nbecomes important to examine the social effect that the usage of these systems\nhas. While these models understand language and have increased accuracy on\ndifficult downstream tasks, there is evidence that these models amplify gender,\nracial and cultural stereotypes and lead to a vicious cycle in many settings.\nIn this survey, we analyze the origins of biases, the definitions of fairness,\nand how different subfields of NLP mitigate bias. We finally discuss how future\nstudies can work towards eradicating pernicious biases from NLP algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rajas Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09593","description":"<p>Vision outlookers improve the performance of vision transformers, which\nimplement a self-attention mechanism by adding outlook attention, a form of\nlocal attention.\n</p>\n<p>In natural language processing, as has been the case in computer vision and\nother domains, transformer-based models constitute the state-of-the-art for\nmost processing tasks. In this domain, too, many authors have argued and\ndemonstrated the importance of local context.\n</p>\n<p>We present and evaluate an outlook attention mechanism, COOL, for natural\nlanguage processing. COOL adds, on top of the self-attention layers of a\ntransformer-based model, outlook attention layers that encode local syntactic\ncontext considering word proximity and consider more pair-wise constraints than\ndynamic convolution operations used by existing approaches.\n</p>\n<p>A comparative empirical performance evaluation of an implementation of COOL\nwith different transformer-based approaches confirms the opportunity of\nimprovement over a baseline using the neural language models alone for various\nnatural language processing tasks, including question answering. The proposed\napproach is competitive with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bressan_S/0/1/0/all/0/1\">St&#xe9;phane Bressan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Clinical Intent from Free Text Electronic Health Records. (arXiv:2204.09594v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09594","description":"<p>After a patient consultation, a clinician determines the steps in the\nmanagement of the patient. A clinician may for example request to see the\npatient again or refer them to a specialist. Whilst most clinicians will record\ntheir intent as \"next steps\" in the patient's clinical notes, in some cases the\nclinician may forget to indicate their intent as an order or request, e.g.\nfailure to place the follow-up order. This consequently results in patients\nbecoming lost-to-follow up and may in some cases lead to adverse consequences.\nIn this paper we train a machine learning model to detect a clinician's intent\nto follow up with a patient from the patient's clinical notes. Annotators\nsystematically identified 22 possible types of clinical intent and annotated\n3000 Bariatric clinical notes. The annotation process revealed a class\nimbalance in the labeled data and we found that there was only sufficient\nlabeled data to train 11 out of the 22 intents. We used the data to train a\nBERT based multilabel classification model and reported the following average\naccuracy metrics for all intents: macro-precision: 0.91, macro-recall: 0.90,\nmacro-f1: 0.90.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noor_K/0/1/0/all/0/1\">Kawsar Noor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Katherine Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_J/0/1/0/all/0/1\">Julia Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnell_J/0/1/0/all/0/1\">Jade OConnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisk_J/0/1/0/all/0/1\">Jessica Fisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunt_M/0/1/0/all/0/1\">Monika Hunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philippo_G/0/1/0/all/0/1\">Gary Philippo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Teresa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_S/0/1/0/all/0/1\">Simon Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romao_L/0/1/0/all/0/1\">Luis Romao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1\">Richard JB Dobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Wai Keong Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Continuous Integrate-and-Fire for Efficient and Adaptive Simultaneous Speech Translation. (arXiv:2204.09595v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09595","description":"<p>Simultaneous speech translation (SimulST) is a challenging task that aims to\ndirectly translate streaming speech before the complete input is observed. A\nSimulST system generally includes two important components: the pre-decision\nthat aggregates the speech information, and the policy that decides read or\nwrite. While recent works had proposed a variety of strategies to improve the\npre-decision, they mostly adopt the fixed wait-k policy. The adaptive policies\nare rarely explored. We propose to model the adaptive policy using the\nContinuous Integrate-and-Fire (CIF). In our proposed model, the CIF is not only\nresponsible for aggregating speech information, but also deciding when to read\nor write. To adapt the CIF to SimulST task, we propose two modifications: a\ntoken-level quantity loss or an infinite lookback attention. We show that our\nmodel can learn an adaptive policy effectively, achieving comparable or\nsuperior performance to MMA at lower latency, while being more efficient to\ntrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1\">Chih-Chiang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceiving the World: Question-guided Reinforcement Learning for Text-based Games. (arXiv:2204.09597v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09597","description":"<p>Text-based games provide an interactive way to study natural language\nprocessing. While deep reinforcement learning has shown effectiveness in\ndeveloping the game playing agent, the low sample efficiency and the large\naction space remain to be the two major challenges that hinder the DRL from\nbeing applied in the real world. In this paper, we address the challenges by\nintroducing world-perceiving modules, which automatically decompose tasks and\nprune actions by answering questions about the environment. We then propose a\ntwo-phase training framework to decouple language learning from reinforcement\nlearning, which further improves the sample efficiency. The experimental\nresults show that the proposed method significantly improves the performance\nand sample efficiency. Besides, it shows robustness against compound error and\nlimited pre-training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunqiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Build a Robust QA System with Transformer-based Mixture of Experts. (arXiv:2204.09598v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09598","description":"<p>In this paper, we aim to build a robust question answering system that can\nadapt to out-of-domain datasets. A single network may overfit to the\nsuperficial correlation in the training distribution, but with a meaningful\nnumber of expert sub-networks, a gating network that selects a sparse\ncombination of experts for each input, and careful balance on the importance of\nexpert sub-networks, the Mixture-of-Experts (MoE) model allows us to train a\nmulti-task learner that can be generalized to out-of-domain datasets. We also\nexplore the possibility of bringing the MoE layers up to the middle of the\nDistilBERT and replacing the dense feed-forward network with a\nsparsely-activated switch FFN layers, similar to the Switch Transformer\narchitecture, which simplifies the MoE routing algorithm with reduced\ncommunication and computational costs. In addition to model architectures, we\nexplore techniques of data augmentation including Easy Data Augmentation (EDA)\nand back translation, to create more meaningful variance among the small\nout-of-domain training data, therefore boosting the performance and robustness\nof our models. In this paper, we show that our combination of best architecture\nand data augmentation techniques achieves a 53.477 F1 score in the\nout-of-domain evaluation, which is a 9.52% performance gain over the baseline.\nOn the final test set, we reported a higher 59.506 F1 and 41.651 EM. We\nsuccessfully demonstrate the effectiveness of Mixture-of-Expert architecture in\na Robust QA task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Qing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xixuan Julie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuanzhe Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Radiology Text Analysis System (RadText): Architecture and Evaluation. (arXiv:2204.09599v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09599","description":"<p>Analyzing radiology reports is a time-consuming and error-prone task, which\nraises the need for an efficient automated radiology report analysis system to\nalleviate the workloads of radiologists and encourage precise diagnosis. In\nthis work, we present RadText, an open-source radiology text analysis system\ndeveloped by Python. RadText offers an easy-to-use text analysis pipeline,\nincluding de-identification, section segmentation, sentence split and word\ntokenization, named entity recognition, parsing, and negation detection.\nRadText features a flexible modular design, provides a hybrid text processing\nschema, and supports raw text processing and local processing, which enables\nbetter usability and improved data privacy. RadText adopts BioC as the unified\ninterface, and also standardizes the input / output into a structured\nrepresentation compatible with Observational Medical Outcomes Partnership\n(OMOP) Common Data Model (CDM). This allows for a more systematic approach to\nobservational research across multiple, disparate data sources. We evaluated\nRadText on the MIMIC-CXR dataset, with five new disease labels we annotated for\nthis work. RadText demonstrates highly accurate classification performances,\nwith an average precision of, a recall of 0.94, and an F-1 score of 0.92. We\nhave made our code, documentation, examples, and the test set available at\nhttps://github.com/bionlplab/radtext .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingquan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Ying Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_G/0/1/0/all/0/1\">George Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical BERT for Medical Document Understanding. (arXiv:2204.09600v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09600","description":"<p>Medical document understanding has gained much attention recently. One\nrepresentative task is the International Classification of Disease (ICD)\ndiagnosis code assignment. Existing work adopts either RNN or CNN as the\nbackbone network because the vanilla BERT cannot handle well long documents\n(&gt;2000 to kens). One issue shared across all these approaches is that they are\nover specific to the ICD code assignment task, losing generality to give the\nwhole document-level and sentence-level embedding. As a result, it is not\nstraight-forward to direct them to other downstream NLU tasks. Motivated by\nthese observations, we propose Medical Document BERT (MDBERT) for long medical\ndocument understanding tasks. MDBERT is not only effective in learning\nrepresentations at different levels of semantics but efficient in encoding long\ndocuments by leveraging a bottom-up hierarchical architecture. Compared to\nvanilla BERT solutions: 1, MDBERT boosts the performance up to relatively 20%\non the MIMIC-III dataset, making it comparable to current SOTA solutions; 2, it\ncuts the computational complexity on self-attention modules to less than 1/100.\nOther than the ICD code assignment, we conduct a variety of other NLU tasks on\na large commercial dataset named as TrialTrove, to showcase MDBERT's strength\nin delivering different levels of semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1\">Maciej Jankowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extraction of Sleep Information from Clinical Notes of Alzheimer's Disease Patients Using Natural Language Processing. (arXiv:2204.09601v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09601","description":"<p>Alzheimer's Disease (AD) is the most common form of dementia in the United\nStates. Sleep is one of the lifestyle-related factors that has been shown\ncritical for optimal cognitive function in old age. . However, there is a lack\nof research studying the association between sleep and AD incidence. A major\nbottleneck for conducting such research is that the traditional way to acquire\nsleep information is time-consuming, inefficient, non-scalable, and limited to\npatients' subjective experience. In this study, we developed a rule-based NLP\nalgorithm and machine learning models to automate the extraction of\nsleep-related concepts, including snoring, napping, sleep problem, bad sleep\nquality, daytime sleepiness, night wakings, and sleep duration, from the\nclinical notes of patients diagnosed with AD. We trained and validated the\nproposed models on the clinical notes retrieved from the University of\nPittsburgh of Medical Center (UPMC). The results show that the rule-based NLP\nalgorithm consistently achieved the best performance for all sleep concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_H/0/1/0/all/0/1\">Haneef Ahamed Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivarajkumar_S/0/1/0/all/0/1\">Sonish Sivarajkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viggiano_S/0/1/0/all/0/1\">Samual Viggiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1\">David Oniani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visweswaran_S/0/1/0/all/0/1\">Shyam Visweswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Unintended Memorization in Language-Model-Fused ASR. (arXiv:2204.09606v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09606","description":"<p>End-to-end (E2E) models are often being accompanied by language models (LMs)\nvia shallow fusion for boosting their overall quality as well as recognition of\nrare words. At the same time, several prior works show that LMs are susceptible\nto unintentionally memorizing rare or unique sequences in the training data. In\nthis work, we design a framework for detecting memorization of random textual\nsequences (which we call canaries) in the LM training data when one has only\nblack-box (query) access to LM-fused speech recognizer, as opposed to direct\naccess to the LM. On a production-grade Conformer RNN-T E2E model fused with a\nTransformer LM, we show that detecting memorization of singly-occurring\ncanaries from the LM training data of 300M examples is possible. Motivated to\nprotect privacy, we also show that such memorization gets significantly reduced\nby per-example gradient-clipped LM training without compromising overall\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Steve Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_O/0/1/0/all/0/1\">Om Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The TalkMoves Dataset: K-12 Mathematics Lesson Transcripts Annotated for Teacher and Student Discursive Moves. (arXiv:2204.09652v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09652","description":"<p>Transcripts of teaching episodes can be effective tools to understand\ndiscourse patterns in classroom instruction. According to most educational\nexperts, sustained classroom discourse is a critical component of equitable,\nengaging, and rich learning environments for students. This paper describes the\nTalkMoves dataset, composed of 567 human-annotated K-12 mathematics lesson\ntranscripts (including entire lessons or portions of lessons) derived from\nvideo recordings. The set of transcripts primarily includes in-person lessons\nwith whole-class discussions and/or small group work, as well as some online\nlessons. All of the transcripts are human-transcribed, segmented by the speaker\n(teacher or student), and annotated at the sentence level for ten discursive\nmoves based on accountable talk theory. In addition, the transcripts include\nutterance-level information in the form of dialogue act labels based on the\nSwitchboard Dialog Act Corpus. The dataset can be used by educators,\npolicymakers, and researchers to understand the nature of teacher and student\ndiscourse in K-12 math classrooms. Portions of this dataset have been used to\ndevelop the TalkMoves application, which provides teachers with automated,\nimmediate, and actionable feedback about their mathematics instruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Abhijit Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_J/0/1/0/all/0/1\">Jennifer Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harty_C/0/1/0/all/0/1\">Charis Harty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perkoff_M/0/1/0/all/0/1\">Margaret Perkoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">James H. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumner_T/0/1/0/all/0/1\">Tamara Sumner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages. (arXiv:2204.09653v1 [cs.PL])","link":"http://arxiv.org/abs/2204.09653","description":"<p>A recent study by Ahmed and Devanbu reported that using a corpus of code\nwritten in multilingual datasets to fine-tune multilingual Pre-trained Language\nModels (PLMs) achieves higher performance as opposed to using a corpus of code\nwritten in just one programming language. However, no analysis was made with\nrespect to fine-tuning monolingual PLMs. Furthermore, some programming\nlanguages are inherently different and code written in one language usually\ncannot be interchanged with the others, i.e., Ruby and Java code possess very\ndifferent structure. To better understand how monolingual and multilingual PLMs\naffect different programming languages, we investigate 1) the performance of\nPLMs on Ruby for two popular Software Engineering tasks: Code Summarization and\nCode Search, 2) the strategy (to select programming languages) that works well\non fine-tuning multilingual PLMs for Ruby, and 3) the performance of the\nfine-tuned PLMs on Ruby given different code lengths.\n</p>\n<p>In this work, we analyze over a hundred of pre-trained and fine-tuned models.\nOur results show that 1) multilingual PLMs have a lower Performance-to-Time\nRatio (the BLEU, METEOR, or MRR scores over the fine-tuning duration) as\ncompared to monolingual PLMs, 2) our proposed strategy to select target\nprogramming languages to fine-tune multilingual PLMs is effective: it reduces\nthe time to fine-tune yet achieves higher performance in Code Summarization and\nCode Search tasks, and 3) our proposed strategy consistently shows good\nperformance on different code lengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fuxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1\">Fatemeh Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1\">David Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryksin_T/0/1/0/all/0/1\">Timofey Bryksin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAMNER: Code Comment Generation Using Character Language Model and Named Entity Recognition. (arXiv:2204.09654v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09654","description":"<p>Code comment generation is the task of generating a high-level natural\nlanguage description for a given code method or function. Although researchers\nhave been studying multiple ways to generate code comments automatically,\nprevious work mainly considers representing a code token in its entirety\nsemantics form only (e.g., a language model is used to learn the semantics of a\ncode token), and additional code properties such as the tree structure of a\ncode are included as an auxiliary input to the model. There are two\nlimitations: 1) Learning the code token in its entirety form may not be able to\ncapture information succinctly in source code, and 2) The code token does not\ncontain additional syntactic information, inherently important in programming\nlanguages.\n</p>\n<p>In this paper, we present LAnguage Model and Named Entity Recognition\n(LAMNER), a code comment generator capable of encoding code constructs\neffectively and capturing the structural property of a code token. A\ncharacter-level language model is used to learn the semantic representation to\nencode a code token. For the structural property of a token, a Named Entity\nRecognition model is trained to learn the different types of code tokens. These\nrepresentations are then fed into an encoder-decoder architecture to generate\ncode comments. We evaluate the generated comments from LAMNER and other\nbaselines on a popular Java dataset with four commonly used metrics. Our\nresults show that LAMNER is effective and improves over the best baseline model\nin BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR, and CIDEr by 14.34%,\n18.98%, 21.55%, 23.00%, 10.52%, 1.44%, and 25.86%, respectively. Additionally,\nwe fused LAMNER's code representation with the baseline models, and the fused\nmodels consistently showed improvement over the non-fused models. The human\nevaluation further shows that LAMNER produces high-quality code comments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishab Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fuxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1\">Fatemeh Fard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-informed Question Answering with Heterogeneous Graph Transformer. (arXiv:2204.09655v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09655","description":"<p>Large neural language models are steadily contributing state-of-the-art\nperformance to question answering and other natural language and information\nprocessing tasks. These models are expensive to train. We propose to evaluate\nwhether such pre-trained models can benefit from the addition of explicit\nlinguistics information without requiring retraining from scratch.\n</p>\n<p>We present a linguistics-informed question answering approach that extends\nand fine-tunes a pre-trained transformer-based neural language model with\nsymbolic knowledge encoded with a heterogeneous graph transformer. We\nillustrate the approach by the addition of syntactic information in the form of\ndependency and constituency graphic structures connecting tokens and virtual\nvertices.\n</p>\n<p>A comparative empirical performance evaluation with BERT as its baseline and\nwith Stanford Question Answering Dataset demonstrates the competitiveness of\nthe proposed approach. We argue, in conclusion and in the light of further\nresults of preliminary experiments, that the approach is extensible to further\nlinguistics information including semantics and pragmatics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangyi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Lok You Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bressan_S/0/1/0/all/0/1\">St&#xe9;phane Bressan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fast Post-Training Pruning Framework for Transformers. (arXiv:2204.09656v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09656","description":"<p>Pruning is an effective way to reduce the huge inference cost of large\nTransformer models. However, prior work on model pruning requires retraining\nthe model. This can add high cost and complexity to model deployment, making it\ndifficult to use in many practical situations. To address this, we propose a\nfast post-training pruning framework for Transformers that does not require any\nretraining. Given a resource constraint and a sample dataset, our framework\nautomatically prunes the Transformer model using structured sparsity methods.\nTo retain high accuracy without retraining, we introduce three novel\ntechniques: (i) a lightweight mask search algorithm that finds which heads and\nfilters to prune based on the Fisher information; (ii) mask rearrangement that\ncomplements the search algorithm; and (iii) mask tuning that reconstructs the\noutput activations for each layer. We apply our method to BERT-BASE and\nDistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our\nframework achieves up to 2.0x reduction in FLOPs and 1.56x speedup in inference\nlatency, while maintaining &lt; 1% loss in accuracy. Importantly, our framework\nprunes Transformers in less than 3 minutes on a single GPU, which is over two\norders of magnitude faster than existing pruning approaches that retrain. Our\ncode is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_W/0/1/0/all/0/1\">Woosuk Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_J/0/1/0/all/0/1\">Joseph Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MIT Voice Name System. (arXiv:2204.09657v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09657","description":"<p>This RFC white Paper summarizes our progress on the MIT Voice Name System\n(VNS) and Huey. The VNS, similar in name and function to the DNS, is a system\nto reserve and use \"wake words\" to activate Artificial Intelligence (AI)\ndevices. Just like you can say \"Hey Siri\" to activate Apple's personal\nassistant, we propose using the VNS in smart speakers and other devices to\nroute wake requests based on commands such as \"turn off\", \"open grocery\nshopping list\" or \"271, start flash card review of my computer vision class\".\nWe also introduce Huey, an unambiguous Natural Language to interact with AI\ndevices. We aim to standardize voice interactions to a universal reach similar\nto that of other systems such as phone numbering, with an agreed world-wide\napproach to assign and use numbers, or the Internet's DNS, with a standard\nnaming system, that has helped flourish popular services including the\nWorld-Wide-Web, FTP, and email. Just like these standards are \"neutral\", we\nalso aim to endow the VNS with \"wake neutrality\" so that each participant can\ndevelop its own digital voice. We focus on voice as a starting point to talk to\nany IoT object and explain briefly how the VNS may be expanded to other AI\ntechnologies enabling person-to-machine conversations (really\nmachine-to-machine), including computer vision or neural interfaces. We also\ndescribe briefly considerations for a broader set of standards, MIT Open AI\n(MOA), including a reference architecture to serve as a starting point for the\ndevelopment of a general conversational commerce infrastructure that has\nstandard \"Wake Words\", NLP commands such as \"Shopping Lists\" or \"Flash Card\nReviews\", and personalities such as Pi or 271. Privacy and security are key\nelements considered because of speech-to-text errors and the amount of personal\ninformation contained in a voice sample.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subirana_B/0/1/0/all/0/1\">Brian Subirana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levinson_H/0/1/0/all/0/1\">Harry Levinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hueto_F/0/1/0/all/0/1\">Ferran Hueto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_P/0/1/0/all/0/1\">Prithvi Rajasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidis_A/0/1/0/all/0/1\">Alexander Gaidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarrago_E/0/1/0/all/0/1\">Esteve Tarrag&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_Soens_P/0/1/0/all/0/1\">Peter Oliveira-Soens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Design Ideation: A Natural Language Generation Approach. (arXiv:2204.09658v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09658","description":"<p>This paper aims to explore a generative approach for knowledge-based design\nideation by applying the latest pre-trained language models in artificial\nintelligence (AI). Specifically, a method of fine-tuning the generative\npre-trained transformer using the USPTO patent database is proposed. The\nAI-generated ideas are not only in concise and understandable language but also\nable to synthesize the target design with external knowledge sources with\ncontrollable knowledge distance. The method is tested in a case study of\nrolling toy design and the results show good performance in generating ideas of\nvaried novelty with near-field and far-field source knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design and Development of Rule-based open-domain Question-Answering System on SQuAD v2.0 Dataset. (arXiv:2204.09659v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09659","description":"<p>Human mind is the palace of curious questions that seek answers.\nComputational resolution of this challenge is possible through Natural Language\nProcessing techniques. Statistical techniques like machine learning and deep\nlearning require a lot of data to train and despite that they fail to tap into\nthe nuances of language. Such systems usually perform best on close-domain\ndatasets. We have proposed development of a rule-based open-domain\nquestion-answering system which is capable of answering questions of any domain\nfrom a corresponding context passage. We have used 1000 questions from SQuAD\n2.0 dataset for testing the developed system and it gives satisfactory results.\nIn this paper, we have described the structure of the developed system and have\nanalyzed the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katyayan_P/0/1/0/all/0/1\">Pragya Katyayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nisheeth Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Dataset Classification for Kurdish Short Text over Social Media. (arXiv:2204.09660v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09660","description":"<p>The Facebook application is used as a resource for collecting the comments of\nthis dataset, The dataset consists of 6756 comments to create a Medical Kurdish\nDataset (MKD). The samples are comments of users, which are gathered from\ndifferent posts of pages (Medical, News, Economy, Education, and Sport). Six\nsteps as a preprocessing technique are performed on the raw dataset to clean\nand remove noise in the comments by replacing characters. The comments (short\ntext) are labeled for positive class (medical comment) and negative class\n(non-medical comment) as text classification. The percentage ratio of the\nnegative class is 55% while the positive class is 45%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Ari M. Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussein_S/0/1/0/all/0/1\">Shnya R. Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_C/0/1/0/all/0/1\">Chro M. Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments. (arXiv:2204.09667v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09667","description":"<p>Recent work in Vision-and-Language Navigation (VLN) has presented two\nenvironmental paradigms with differing realism -- the standard VLN setting\nbuilt on topological environments where navigation is abstracted away, and the\nVLN-CE setting where agents must navigate continuous 3D environments using\nlow-level actions. Despite sharing the high-level task and even the underlying\ninstruction-path data, performance on VLN-CE lags behind VLN significantly. In\nthis work, we explore this gap by transferring an agent from the abstract\nenvironment of VLN to the continuous environment of VLN-CE. We find that this\nsim-2-sim transfer is highly effective, improving over the prior state of the\nart in VLN-CE by +12% success rate. While this demonstrates the potential for\nthis direction, the transfer does not fully retain the original performance of\nthe agent in the abstract setting. We present a sequence of experiments to\nidentify what differences result in performance degradation, providing clear\ndirections for further improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards General Purpose Vision Systems. (arXiv:2104.00743v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00743","description":"<p>Computer vision systems today are primarily N-purpose systems, designed and\ntrained for a predefined set of tasks. Adapting such systems to new tasks is\nchallenging and often requires non-trivial modifications to the network\narchitecture (e.g. adding new output heads) or training process (e.g. adding\nnew losses). To reduce the time and expertise required to develop new\napplications, we would like to create general purpose vision systems that can\nlearn and perform a range of tasks without any modification to the architecture\nor learning process.\n</p>\n<p>In this paper, we propose GPV-1, a task-agnostic vision-language architecture\nthat can learn and perform tasks that involve receiving an image and producing\ntext and/or bounding boxes, including classification, localization, visual\nquestion answering, captioning, and more. We also propose evaluations of\ngenerality of architecture, skill-concept transfer, and learning efficiency\nthat may inform future work on general purpose vision. Our experiments indicate\nGPV-1 is effective at multiple tasks, reuses some concept knowledge across\ntasks, can perform the Referring Expressions task zero-shot, and further\nimproves upon the zero-shot performance using a few training samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanmay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Amita Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach. (arXiv:2104.04886v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.04886","description":"<p>Adversarial regularization has been shown to improve the generalization\nperformance of deep learning models in various natural language processing\ntasks. Existing works usually formulate the method as a zero-sum game, which is\nsolved by alternating gradient descent/ascent algorithms. Such a formulation\ntreats the adversarial and the defending players equally, which is undesirable\nbecause only the defending player contributes to the generalization\nperformance. To address this issue, we propose Stackelberg Adversarial\nRegularization (SALT), which formulates adversarial regularization as a\nStackelberg game. This formulation induces a competition between a leader and a\nfollower, where the follower generates perturbations, and the leader trains the\nmodel subject to the perturbations. Different from conventional approaches, in\nSALT, the leader is in an advantageous position. When the leader moves, it\nrecognizes the strategy of the follower and takes the anticipated follower's\noutcomes into consideration. Such a leader's advantage enables us to improve\nthe model fitting to the unperturbed data. The leader's strategic information\nis captured by the Stackelberg gradient, which is obtained using an unrolling\nalgorithm. Our experimental results on a set of machine translation and natural\nlanguage understanding tasks show that SALT outperforms existing adversarial\nregularization baselines across all tasks. Our code is available at\nhttps://github.com/SimiaoZuo/Stackelberg-Adv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trends, Limitations and Open Challenges in Automatic Readability Assessment Research. (arXiv:2105.00973v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00973","description":"<p>Readability assessment is the task of evaluating the reading difficulty of a\ngiven piece of text. Although research on computational approaches to\nreadability assessment is now two decades old, there is not much work on\nsynthesizing this research. This article is a brief survey of contemporary\nresearch on developing computational models for readability assessment. We\nidentify the common approaches, discuss their shortcomings, and identify some\nchallenges for the future. Where possible, we also connect computational\nresearch with insights from related work in other disciplines such as education\nand psychology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vajjala_S/0/1/0/all/0/1\">Sowmya Vajjala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization. (arXiv:2108.13684v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13684","description":"<p>Despite recent progress in abstractive summarization, systems still suffer\nfrom faithfulness errors. While prior work has proposed models that improve\nfaithfulness, it is unclear whether the improvement comes from an increased\nlevel of extractiveness of the model outputs as one naive way to improve\nfaithfulness is to make summarization models more extractive. In this work, we\npresent a framework for evaluating the effective faithfulness of summarization\nsystems, by generating a faithfulnessabstractiveness trade-off curve that\nserves as a control at different operating points on the abstractiveness\nspectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as\nwell as a recently proposed method for improving faithfulness, are both worse\nthan the control at the same level of abstractiveness. Finally, we learn a\nselector to identify the most faithful and abstractive summary for a given\ndocument, and show that this system can attain higher faithfulness scores in\nhuman evaluations while being more abstractive than the baseline system on two\ndatasets. Moreover, we show that our system is able to achieve a better\nfaithfulness-abstractiveness trade-off than the control at the same level of\nabstractiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARCH: Efficient Adversarial Regularized Training with Caching. (arXiv:2109.07048v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07048","description":"<p>Adversarial regularization can improve model generalization in many natural\nlanguage processing tasks. However, conventional approaches are computationally\nexpensive since they need to generate a perturbation for each sample in each\nepoch. We propose a new adversarial regularization method ARCH (adversarial\nregularization with caching), where perturbations are generated and cached once\nevery several epochs. As caching all the perturbations imposes memory usage\nconcerns, we adopt a K-nearest neighbors-based strategy to tackle this issue.\nThe strategy only requires caching a small amount of perturbations, without\nintroducing additional training time. We evaluate our proposed method on a set\nof neural machine translation and natural language understanding tasks. We\nobserve that ARCH significantly eases the computational burden (saves up to 70%\nof computational time in comparison with conventional approaches). More\nsurprisingly, by reducing the variance of stochastic gradients, ARCH produces a\nnotably better (in most of the tasks) or comparable model generalization. Our\ncode is available at https://github.com/SimiaoZuo/Caching-Adv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mention Memory: incorporating textual knowledge into Transformers through entity mention attention. (arXiv:2110.06176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06176","description":"<p>Natural language understanding tasks such as open-domain question answering\noften require retrieving and assimilating factual information from multiple\nsources. We propose to address this problem by integrating a semi-parametric\nrepresentation of a large text corpus into a Transformer model as a source of\nfactual knowledge. Specifically, our method represents knowledge with `mention\nmemory', a table of dense vector representations of every entity mention in a\ncorpus. The proposed model - TOME - is a Transformer that accesses the\ninformation through internal memory layers in which each entity mention in the\ninput passage attends to the mention memory. This approach enables synthesis of\nand reasoning over many disparate sources of information within a single\nTransformer model. In experiments using a memory of 150 million Wikipedia\nmentions, TOME achieves strong performance on several open-domain\nknowledge-intensive tasks, including the claim verification benchmarks HoVer\nand FEVER and several entity-based QA benchmarks. We also show that the model\nlearns to attend to informative mentions without any direct supervision.\nFinally we demonstrate that the model can generalize to new unseen entities by\nupdating the memory without retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_N/0/1/0/all/0/1\">Nicholas FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Dense Retrieval for Dialogue Response Selection. (arXiv:2110.06612v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06612","description":"<p>Recent progress in deep learning has continuously improved the accuracy of\ndialogue response selection. In particular, sophisticated neural network\narchitectures are leveraged to capture the rich interactions between dialogue\ncontext and response candidates. While remarkably effective, these models also\nbring in a steep increase in computational cost. Consequently, such models can\nonly be used as a re-rank module in practice. In this study, we present a\nsolution to directly select proper responses from a large corpus or even a\nnonparallel corpus that only consists of unpaired sentences, using a dense\nretrieval model. To push the limits of dense retrieval, we design an\ninteraction layer upon the dense retrieval models and apply a set of\ntailor-designed learning strategies. Our model shows superiority over strong\nbaselines on the conventional re-rank evaluation setting, which is remarkable\ngiven its efficiency. To verify the effectiveness of our approach in realistic\nscenarios, we also conduct full-rank evaluation, where the target is to select\nproper responses from a full candidate pool that may contain millions of\ncandidates and evaluate them fairly through human annotations. Our proposed\nmodel notably outperforms pipeline baselines that integrate fast recall and\nexpressive re-rank modules. Human evaluation results show that enlarging the\ncandidate pool with nonparallel corpora improves response quality further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Curriculum Learning for AMR Parsing. (arXiv:2110.07855v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07855","description":"<p>Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics. (arXiv:2201.05771v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2201.05771","description":"<p>We present an expanded version of our previously released Kazakh\ntext-to-speech (KazakhTTS) synthesis corpus. In the new KazakhTTS2 corpus, the\noverall size has increased from 93 hours to 271 hours, the number of speakers\nhas risen from two to five (three females and two males), and the topic\ncoverage has been diversified with the help of new sources, including a book\nand Wikipedia articles. This corpus is necessary for building high-quality TTS\nsystems for Kazakh, a Central Asian agglutinative language from the Turkic\nfamily, which presents several linguistic challenges. We describe the corpus\nconstruction process and provide the details of the training and evaluation\nprocedures for the TTS system. Our experimental results indicate that the\nconstructed corpus is sufficient to build robust TTS models for real-world\napplications, with a subjective mean opinion score ranging from 3.6 to 4.2 for\nall the five speakers. We believe that our corpus will facilitate speech and\nlanguage research for Kazakh and other Turkic languages, which are widely\nconsidered to be low-resource due to the limited availability of free\nlinguistic data. The constructed corpus, code, and pretrained models are\npublicly available in our GitHub repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1\">Saida Mussakhojayeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification. (arXiv:2202.09099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09099","description":"<p>Women are influential online, especially in image-based social media such as\nTwitter and Instagram. However, many in the network environment contain gender\ndiscrimination and aggressive information, which magnify gender stereotypes and\ngender inequality. Therefore, the filtering of illegal content such as gender\ndiscrimination is essential to maintain a healthy social network environment.\nIn this paper, we describe the system developed by our team for SemEval-2022\nTask 5: Multimedia Automatic Misogyny Identification. More specifically, we\nintroduce two novel system to analyze these posts: a multimodal multi-task\nlearning architecture that combines Bertweet for text encoding with ResNet-18\nfor image representation, and a single-flow transformer structure which\ncombines text embeddings from BERT-Embeddings and image embeddings from several\ndifferent modules such as EfficientNet and ResNet. In this manner, we show that\nthe information behind them can be properly revealed. Our approach achieves\ngood performance on each of the two subtasks of the current competition,\nranking 15th for Subtask A (0.746 macro F1-score), 11th for Subtask B (0.706\nmacro F1-score) while exceeding the official baseline results by high margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_M/0/1/0/all/0/1\">Ming Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yukai He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"splink\" is happy and \"phrouth\" is scary: Emotion Intensity Analysis for Nonsense Words. (arXiv:2202.12132v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12132","description":"<p>People associate affective meanings to words - \"death\" is scary and sad while\n\"party\" is connotated with surprise and joy. This raises the question if the\nassociation is purely a product of the learned affective imports inherent to\nsemantic meanings, or is also an effect of other features of words, e.g.,\nmorphological and phonological patterns. We approach this question with an\nannotation-based analysis leveraging nonsense words. Specifically, we conduct a\nbest-worst scaling crowdsourcing study in which participants assign intensity\nscores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense\nwords and, for comparison of the results to previous work, to 68 real words.\nBased on this resource, we develop character-level and phonology-based\nintensity regressors. We evaluate them on both nonsense words and real words\n(making use of the NRC emotion intensity lexicon of 7493 words), across six\nemotion categories. The analysis of our data reveals that some phonetic\npatterns show clear differences between emotion intensities. For instance, s as\na first phoneme contributes to joy, sh to surprise, p as last phoneme more to\ndisgust than to anger and fear. In the modelling experiments, a regressor\ntrained on real words from the NRC emotion intensity lexicon shows a higher\nperformance (r = 0.17) than regressors that aim at learning the emotion\nconnotation purely from nonsense words. We conclude that humans do associate\naffective meaning to words based on surface patterns, but also based on\nsimilarities to existing words (\"juy\" to \"joy\", or \"flike\" to \"like\").\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabbatino_V/0/1/0/all/0/1\">Valentino Sabbatino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweitzer_A/0/1/0/all/0/1\">Antje Schweitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiNER: Financial Numeric Entity Recognition for XBRL Tagging. (arXiv:2203.06482v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06482","description":"<p>Publicly traded companies are required to submit periodic reports with\neXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging\nthe reports is tedious and costly. We, therefore, introduce XBRL tagging as a\nnew entity extraction task for the financial domain and release FiNER-139, a\ndataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction\ndatasets, FiNER-139 uses a much larger label set of 139 entity types. Most\nannotated tokens are numeric, with the correct tag per token depending mostly\non context, rather than the token itself. We show that subword fragmentation of\nnumeric expressions harms BERT's performance, allowing word-level BILSTMs to\nperform better. To improve BERT's performance, we propose two simple and\neffective solutions that replace numeric expressions with pseudo-tokens\nreflecting original token shapes and numeric magnitudes. We also experiment\nwith FIN-BERT, an existing BERT model for the financial domain, and release our\nown BERT (SEC-BERT), pre-trained on financial filings, which performs best.\nThrough data and error analysis, we finally identify possible limitations to\ninspire future work on XBRL tagging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1\">Lefteris Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spyropoulou_E/0/1/0/all/0/1\">Eirini Spyropoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1\">Prodromos Malakasiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models. (arXiv:2203.07911v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.07911","description":"<p>Natural language processing models learn word representations based on the\ndistributional hypothesis, which asserts that word context (e.g.,\nco-occurrence) correlates with meaning. We propose that $n$-grams composed of\nrandom character sequences, or $garble$, provide a novel context for studying\nword meaning both within and beyond extant language. In particular, randomly\ngenerated character $n$-grams lack meaning but contain primitive information\nbased on the distribution of characters they contain. By studying the\nembeddings of a large corpus of garble, extant language, and pseudowords using\nCharacterBERT, we identify an axis in the model's high-dimensional embedding\nspace that separates these classes of $n$-grams. Furthermore, we show that this\naxis relates to structure within extant language, including word\npart-of-speech, morphology, and concept concreteness. Thus, in contrast to\nstudies that are mainly limited to extant language, our work reveals that\nmeaning and primitive information are intrinsically linked.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_M/0/1/0/all/0/1\">Mark Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desikan_B/0/1/0/all/0/1\">Bhargav Srinivasa Desikan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadler_E/0/1/0/all/0/1\">Ethan O. Nadler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardo_D/0/1/0/all/0/1\">D. Ruggiero Lo Sardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darragh_Ford_E/0/1/0/all/0/1\">Elise Darragh-Ford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guilbeault_D/0/1/0/all/0/1\">Douglas Guilbeault</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Roadmap for Big Model. (arXiv:2203.14101v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.14101","description":"<p>With the rapid development of deep learning, training Big Models (BMs) for\nmultiple downstream tasks becomes a popular paradigm. Researchers have achieved\nvarious outcomes in the construction of BMs and the BM application in many\nfields. At present, there is a lack of research work that sorts out the overall\nprogress of BMs and guides the follow-up research. In this paper, we cover not\nonly the BM technologies themselves but also the prerequisites for BM training\nand applications with BMs, dividing the BM review into four parts: Resource,\nModels, Key Technologies and Application. We introduce 16 specific BM-related\ntopics in those four parts, they are Data, Knowledge, Computing System,\nParallel Training System, Language Model, Vision Model, Multi-modal Model,\nTheory&amp;Interpretability, Commonsense Reasoning, Reliability&amp;Security,\nGovernance, Evaluation, Machine Translation, Text Generation, Dialogue and\nProtein Research. In each topic, we summarize clearly the current studies and\npropose some future research directions. At the end of this paper, we conclude\nthe further development of BMs in a more general view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yangxiao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhou Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiaao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yizhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lingxiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chence Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zuobai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaoyu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zijun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Weicheng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zheni Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weize Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenzhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, et al. (35 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Learning with Siamese Networks and Label Tuning. (arXiv:2203.14655v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14655","description":"<p>We study the problem of building text classifiers with little or no training\ndata, commonly known as zero and few-shot text classification. In recent years,\nan approach based on neural textual entailment models has been found to give\nstrong results on a diverse range of tasks. In this work, we show that with\nproper pre-training, Siamese Networks that embed texts and labels offer a\ncompetitive alternative. These models allow for a large reduction in inference\ncost: constant in the number of labels rather than linear. Furthermore, we\nintroduce label tuning, a simple and computationally efficient approach that\nallows to adapt the models in a few-shot setup by only changing the label\nembeddings. While giving lower performance than model fine-tuning, this\napproach has the architectural advantage that a single encoder can be shared by\nmany different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Torro_G/0/1/0/all/0/1\">Guillermo P&#xe9;rez-Torr&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. (arXiv:2203.14680v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14680","description":"<p>Transformer-based language models (LMs) are at the core of modern NLP, but\ntheir internal prediction construction process is opaque and largely not\nunderstood. In this work, we make a substantial step towards unveiling this\nunderlying prediction process, by reverse-engineering the operation of the\nfeed-forward network (FFN) layers, one of the building blocks of transformer\nmodels. We view the token representation as a changing distribution over the\nvocabulary, and the output from each FFN layer as an additive update to that\ndistribution. Then, we analyze the FFN updates in the vocabulary space, showing\nthat each update can be decomposed to sub-updates corresponding to single FFN\nparameter vectors, each promoting concepts that are often human-interpretable.\nWe then leverage these findings for controlling LM predictions, where we reduce\nthe toxicity of GPT2 by almost 50%, and for improving computation efficiency\nwith a simple early exit rule, saving 20% of computation on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kevin Ro Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics. (arXiv:2203.15858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.15858","description":"<p>Current practices in metric evaluation focus on one single dataset, e.g.,\nNewstest dataset in each year's WMT Metrics Shared Task. However, in this\npaper, we qualitatively and quantitatively show that the performances of\nmetrics are sensitive to data. The ranking of metrics varies when the\nevaluation is conducted on different datasets. Then this paper further\ninvestigates two potential hypotheses, i.e., insignificant data points and the\ndeviation of Independent and Identically Distributed (i.i.d) assumption, which\nmay take responsibility for the issue of data variance. In conclusion, our\nfindings suggest that when evaluating automatic translation metrics,\nresearchers should take data variance into account and be cautious to claim the\nresult on a single dataset, because it may leads to inconsistent results with\nmost of other datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jiannan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Effective Unsupervised Speech Synthesis. (arXiv:2204.02524v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.02524","description":"<p>We introduce the first unsupervised speech synthesis system based on a\nsimple, yet effective recipe. The framework leverages recent work in\nunsupervised speech recognition as well as existing neural-based speech\nsynthesis. Using only unlabeled speech audio and unlabeled text as well as a\nlexicon, our method enables speech synthesis without the need for a\nhuman-labeled corpus. Experiments demonstrate the unsupervised system can\nsynthesize speech similar to a supervised counterpart in terms of naturalness\nand intelligibility measured by human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Simultaneous Speech Translation need Simultaneous Models?. (arXiv:2204.03783v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03783","description":"<p>In simultaneous speech translation (SimulST), finding the best trade-off\nbetween high translation quality and low latency is a challenging task. To meet\nthe latency constraints posed by the different application scenarios, multiple\ndedicated SimulST models are usually trained and maintained, generating high\ncomputational costs. In this paper, motivated by the increased social and\nenvironmental impact caused by these costs, we investigate whether a single\nmodel trained offline can serve not only the offline but also the simultaneous\ntask without the need for any additional training or adaptation. Experiments on\nen-&gt;{de, es} indicate that, aside from facilitating the adoption of\nwell-established offline techniques and architectures without affecting\nlatency, the offline solution achieves similar or better translation quality\ncompared to the same model trained in simultaneous settings, as well as being\ncompetitive with the SimulST state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Biomedical Entity Linking via Knowledge Base-Guided Pre-training and Synonyms-Aware Fine-tuning. (arXiv:2204.05164v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05164","description":"<p>Entities lie in the heart of biomedical natural language understanding, and\nthe biomedical entity linking (EL) task remains challenging due to the\nfine-grained and diversiform concept names. Generative methods achieve\nremarkable performances in general domain EL with less memory usage while\nrequiring expensive pre-training. Previous biomedical EL methods leverage\nsynonyms from knowledge bases (KB) which is not trivial to inject into a\ngenerative method. In this work, we use a generative approach to model\nbiomedical EL and propose to inject synonyms knowledge in it. We propose\nKB-guided pre-training by constructing synthetic samples with synonyms and\ndefinitions from KB and require the model to recover concept names. We also\npropose synonyms-aware fine-tuning to select concept names for training, and\npropose decoder prompt and multi-synonyms constrained prefix tree for\ninference. Our method achieves state-of-the-art results on several biomedical\nEL tasks without candidate selection which displays the effectiveness of\nproposed pre-training and fine-tuning strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impossible Triangle: What's Next for Pre-trained Language Models?. (arXiv:2204.06130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06130","description":"<p>Recent development of large-scale pre-trained language models (PLM) have\nsignificantly improved the capability of models in various NLP tasks, in terms\nof performance after task-specific fine-tuning and zero-shot / few-shot\nlearning. However, many of such models come with a dauntingly huge size that\nfew institutions can afford to pre-train, fine-tune or even deploy, while\nmoderate-sized models usually lack strong generalized few-shot learning\ncapabilities. In this paper, we first elaborate the current obstacles of using\nPLM models in terms of the Impossible Triangle: 1) moderate model size, 2)\nstate-of-the-art few-shot learning capability, and 3) state-of-the-art\nfine-tuning capability. We argue that all existing PLM models lack one or more\nproperties from the Impossible Triangle. To remedy these missing properties of\nPLMs, various techniques have been proposed, such as knowledge distillation,\ndata augmentation and prompt learning, which inevitably brings additional work\nto the application of PLMs in real scenarios. We then offer insights into\nfuture research directions of PLMs to achieve the Impossible Triangle, and\nbreak down the task into several key phases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models. (arXiv:2204.07483v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07483","description":"<p>Text analysis of social media for sentiment, topic analysis, and other\nanalysis depends initially on the selection of keywords and phrases that will\nbe used to create the research corpora. However, keywords that researchers\nchoose may occur infrequently, leading to errors that arise from using small\nsamples. In this paper, we use the capacity for memorization, interpolation,\nand extrapolation of Transformer Language Models such as the GPT series to\nlearn the linguistic behaviors of a subgroup within larger corpora of Yelp\nreviews. We then use prompt-based queries to generate synthetic text that can\nbe analyzed to produce insights into specific opinions held by the populations\nthat the models were trained on. Once learned, more specific sentiment queries\ncan be made of the model with high levels of accuracy when compared to\ntraditional keyword searches. We show that even in cases where a specific\nkeyphrase is limited or not present at all in the training corpora, the GPT is\nable to accurately generate large volumes of text that have the correct\nsentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_P/0/1/0/all/0/1\">Philip Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dant_A/0/1/0/all/0/1\">Aaron Dant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foulds_J/0/1/0/all/0/1\">James R. Foulds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shemei Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Idiom Paraphrasing. (arXiv:2204.07555v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07555","description":"<p>Idioms, are a kind of idiomatic expression in Chinese, most of which consist\nof four Chinese characters. Due to the properties of non-compositionality and\nmetaphorical meaning, Chinese Idioms are hard to be understood by children and\nnon-native speakers. This study proposes a novel task, denoted as Chinese Idiom\nParaphrasing (CIP). CIP aims to rephrase idioms-included sentences to\nnon-idiomatic ones under the premise of preserving the original sentence's\nmeaning. Since the sentences without idioms are easier handled by Chinese NLP\nsystems, CIP can be used to pre-process Chinese datasets, thereby facilitating\nand improving the performance of Chinese NLP tasks, e.g., machine translation\nsystem, Chinese idiom cloze, and Chinese idiom embeddings. In this study, CIP\ntask is treated as a special paraphrase generation task. To circumvent\ndifficulties in acquiring annotations, we first establish a large-scale CIP\ndataset based on human and machine collaboration, which consists of 115,530\nsentence pairs. We further deploy three baselines and two novel CIP approaches\nto deal with CIP problems. The results show that the proposed methods have\nbetter performances than the baselines based on the established CIP dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xindong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Mixed-initiative Conversational Search Systems via User Simulation. (arXiv:2204.08046v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08046","description":"<p>Clarifying the underlying user information need by asking clarifying\nquestions is an important feature of modern conversational search system.\nHowever, evaluation of such systems through answering prompted clarifying\nquestions requires significant human effort, which can be time-consuming and\nexpensive. In this paper, we propose a conversational User Simulator, called\nUSi, for automatic evaluation of such conversational search systems. Given a\ndescription of an information need, USi is capable of automatically answering\nclarifying questions about the topic throughout the search session. Through a\nset of experiments, including automated natural language generation metrics and\ncrowdsourcing studies, we show that responses generated by USi are both inline\nwith the underlying information need and comparable to human-generated answers.\nMoreover, we make the first steps towards multi-turn interactions, where\nconversational search systems asks multiple questions to the (simulated) user\nwith a goal of clarifying the user need. To this end, we expand on currently\navailable datasets for studying clarifying questions, i.e., Qulac and ClariQ,\nby performing a crowdsourcing-based multi-turn data acquisition. We show that\nour generative, GPT2-based model, is capable of providing accurate and natural\nanswers to unseen clarifying questions in the single-turn setting and discuss\ncapabilities of our model in the multi-turn setting. We provide the code, data,\nand the pre-trained model to be used for further research on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekulic_I/0/1/0/all/0/1\">Ivan Sekuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crestani_F/0/1/0/all/0/1\">Fabio Crestani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08790","description":"<p>Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets/tasks. However, it remains a challenge to evaluate the\ntransferablity of these foundation models due to the lack of easy-to-use\ntoolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark to\ncompare and evaluate pre-trained language-augmented visual models. Several\nhighlights include: (i) Datasets. As downstream evaluation suites, it consists\nof 20 image classification datasets and 35 object detection datasets, each of\nwhich is augmented with external knowledge. (ii) Toolkit. An automatic\nhyper-parameter tuning toolkit is developed to ensure the fairness in model\nadaption. To leverage the full power of language-augmented visual models, novel\nlanguage-aware initialization methods are proposed to significantly improve the\nadaption performance. (iii) Metrics. A variety of evaluation metrics are used,\nincluding sample-efficiency (zero-shot and few-shot) and parameter-efficiency\n(linear probing and full model fine-tuning). We will release our toolkit and\nevaluation platforms for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Ping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs. (arXiv:2204.08875v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08875","description":"<p>As Abstract Meaning Representation (AMR) implicitly involves compound\nsemantic annotations, we hypothesize auxiliary tasks which are semantically or\nformally related can better enhance AMR parsing. We find that 1) Semantic role\nlabeling (SRL) and dependency parsing (DP), would bring more performance gain\nthan other tasks e.g. MT and summarization in the text-to-AMR transition even\nwith much less data. 2) To make a better fit for AMR, data from auxiliary tasks\nshould be properly \"AMRized\" to PseudoAMR before training. Knowledge from\nshallow level parsing tasks can be better transferred to AMR Parsing with\nstructure transform. 3) Intermediate-task learning is a better paradigm to\nintroduce auxiliary tasks to AMR parsing, compared to multitask learning. From\nan empirical perspective, we propose a principled method to involve auxiliary\ntasks to boost AMR parsing. Extensive experiments show that our method achieves\nnew state-of-the-art performance on different benchmarks especially in\ntopology-related scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PR-DAD: Phase Retrieval Using Deep Auto-Decoders. (arXiv:2204.09051v1 [eess.IV])","link":"http://arxiv.org/abs/2204.09051","description":"<p>Phase retrieval is a well known ill-posed inverse problem where one tries to\nrecover images given only the magnitude values of their Fourier transform as\ninput. In recent years, new algorithms based on deep learning have been\nproposed, providing breakthrough results that surpass the results of the\nclassical methods. In this work we provide a novel deep learning architecture\nPR-DAD (Phase Retrieval Using Deep Auto- Decoders), whose components are\ncarefully designed based on mathematical modeling of the phase retrieval\nproblem. The architecture provides experimental results that surpass all\ncurrent results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gugel_L/0/1/0/all/0/1\">Leon Gugel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dekel_S/0/1/0/all/0/1\">Shai Dekel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embodied Navigation at the Art Gallery. (arXiv:2204.09069v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09069","description":"<p>Embodied agents, trained to explore and navigate indoor photorealistic\nenvironments, have achieved impressive results on standard datasets and\nbenchmarks. So far, experiments and evaluations have involved domestic and\nworking scenes like offices, flats, and houses. In this paper, we build and\nrelease a new 3D space with unique characteristics: the one of a complete art\nmuseum. We name this environment ArtGallery3D (AG3D). Compared with existing 3D\nscenes, the collected space is ampler, richer in visual features, and provides\nvery sparse occupancy information. This feature is challenging for\noccupancy-based agents which are usually trained in crowded domestic\nenvironments with plenty of occupancy information. Additionally, we annotate\nthe coordinates of the main points of interest inside the museum, such as\npaintings, statues, and other items. Thanks to this manual process, we deliver\na new benchmark for PointGoal navigation inside this new space. Trajectories in\nthis dataset are far more complex and lengthy than existing ground-truth paths\nfor navigation in Gibson and Matterport3D. We carry on extensive experimental\nevaluation using our new space for evaluation and prove that existing methods\nhardly adapt to this scenario. As such, we believe that the availability of\nthis 3D model will foster future research and help improve existing solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bigazzi_R/0/1/0/all/0/1\">Roberto Bigazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Tool based Edited Images from Error Level Analysis and Convolutional Neural Network. (arXiv:2204.09075v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09075","description":"<p>Image Forgery is a problem of image forensics and its detection can be\nleveraged using Deep Learning. In this paper we present an approach for\nidentification of authentic and tampered images done using image editing tools\nwith Error Level Analysis and Convolutional Neural Network. The process is\nperformed on CASIA ITDE v2 dataset and trained for 50 and 100 epochs\nrespectively. The respective accuracies of the training and validation sets are\nrepresented using graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raunak Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laban_R/0/1/0/all/0/1\">Ronald Laban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Photometric single-view dense 3D reconstruction in endoscopy. (arXiv:2204.09083v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09083","description":"<p>Visual SLAM inside the human body will open the way to computer-assisted\nnavigation in endoscopy. However, due to space limitations, medical endoscopes\nonly provide monocular images, leading to systems lacking true scale. In this\npaper, we exploit the controlled lighting in colonoscopy to achieve the first\nin-vivo 3D reconstruction of the human colon using photometric stereo on a\ncalibrated monocular endoscope. Our method works in a real medical environment,\nproviding both a suitable in-place calibration procedure and a depth estimation\ntechnique adapted to the colon's tubular geometry. We validate our method on\nsimulated colonoscopies, obtaining a mean error of 7% on depth estimation,\nwhich is below 3 mm on average. Our qualitative results on the EndoMapper\ndataset show that the method is able to correctly estimate the colon shape in\nreal human colonoscopies, paving the ground for true-scale monocular SLAM in\nendoscopy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Batlle_V/0/1/0/all/0/1\">Victor M. Batlle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1\">J.M.M. Montiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tardos_J/0/1/0/all/0/1\">Juan D. Tardos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4D-MultispectralNet: Multispectral Stereoscopic Disparity Estimation using Human Masks. (arXiv:2204.09089v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09089","description":"<p>Multispectral stereoscopy is an emerging field. A lot of work has been done\nin classical stereoscopy, but multispectral stereoscopy is not studied as\nfrequently. This type of stereoscopy can be used in autonomous vehicles to\ncomplete the information given by RGB cameras. It helps to identify objects in\nthe surroundings when the conditions are more difficult, such as in night\nscenes. This paper focuses on the RGB-LWIR spectrum. RGB-LWIR stereoscopy has\nthe same challenges as classical stereoscopy, that is occlusions, textureless\nsurfaces and repetitive patterns, plus specific ones related to the different\nmodalities. Finding matches between two spectrums adds another layer of\ncomplexity. Color, texture and shapes are more likely to vary from a spectrum\nto another. To address this additional challenge, this paper focuses on\nestimating the disparity of people present in a scene. Given the fact that\npeople's shape is captured in both RGB and LWIR, we propose a novel method that\nuses segmentation masks of the human in both spectrum and than concatenate them\nto the original images before the first layer of a Siamese Network. This method\nhelps to improve the accuracy, particularly within the one pixel error range.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duplessis_Guindon_P/0/1/0/all/0/1\">Philippe Duplessis-Guindon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behind the Machine's Gaze: Biologically Constrained Neural Networks Exhibit Human-like Visual Attention. (arXiv:2204.09093v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09093","description":"<p>By and large, existing computational models of visual attention tacitly\nassume perfect vision and full access to the stimulus and thereby deviate from\nfoveated biological vision. Moreover, modelling top-down attention is generally\nreduced to the integration of semantic features without incorporating the\nsignal of a high-level visual tasks that have shown to partially guide human\nattention. We propose the Neural Visual Attention (NeVA) algorithm to generate\nvisual scanpaths in a top-down manner. With our method, we explore the ability\nof neural networks on which we impose the biological constraints of foveated\nvision to generate human-like scanpaths. Thereby, the scanpaths are generated\nto maximize the performance with respect to the underlying visual task (i.e.,\nclassification or reconstruction). Extensive experiments show that the proposed\nmethod outperforms state-of-the-art unsupervised human attention models in\nterms of similarity to human scanpaths. Additionally, the flexibility of the\nframework allows to quantitatively investigate the role of different tasks in\nthe generated visual behaviours. Finally, we demonstrate the superiority of the\napproach in a novel experiment that investigates the utility of scanpaths in\nreal-world applications, where imperfect viewing conditions are given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwinn_L/0/1/0/all/0/1\">Leo Schwinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskofier_B/0/1/0/all/0/1\">Bj&#xf6;rn Eskofier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1\">Dario Zanca</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Remote Sensing Image Understanding with Weak Supervision: Concepts, Methods, and Perspectives. (arXiv:2204.09120v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09120","description":"<p>In recent years, supervised learning has been widely used in various tasks of\noptical remote sensing image understanding, including remote sensing image\nclassification, pixel-wise segmentation, change detection, and object\ndetection. The methods based on supervised learning need a large amount of\nhigh-quality training data and their performance highly depends on the quality\nof the labels. However, in practical remote sensing applications, it is often\nexpensive and time-consuming to obtain large-scale data sets with high-quality\nlabels, which leads to a lack of sufficient supervised information. In some\ncases, only coarse-grained labels can be obtained, resulting in the lack of\nexact supervision. In addition, the supervised information obtained manually\nmay be wrong, resulting in a lack of accurate supervision. Therefore, remote\nsensing image understanding often faces the problems of incomplete, inexact,\nand inaccurate supervised information, which will affect the breadth and depth\nof remote sensing applications. In order to solve the above-mentioned problems,\nresearchers have explored various tasks in remote sensing image understanding\nunder weak supervision. This paper summarizes the research progress of weakly\nsupervised learning in the field of remote sensing, including three typical\nweakly supervised paradigms: 1) Incomplete supervision, where only a subset of\ntraining data is labeled; 2) Inexact supervision, where only coarse-grained\nlabels of training data are given; 3) Inaccurate supervision, where the labels\ngiven are not always true on the ground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jun Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Leyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weiying Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1\">Antonio J Plaza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Importance is in your attention: agent importance prediction for autonomous driving. (arXiv:2204.09121v1 [cs.RO])","link":"http://arxiv.org/abs/2204.09121","description":"<p>Trajectory prediction is an important task in autonomous driving.\nState-of-the-art trajectory prediction models often use attention mechanisms to\nmodel the interaction between agents. In this paper, we show that the attention\ninformation from such models can also be used to measure the importance of each\nagent with respect to the ego vehicle's future planned trajectory. Our\nexperiment results on the nuPlans dataset show that our method can effectively\nfind and rank surrounding agents by their impact on the ego's plan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazard_C/0/1/0/all/0/1\">Christopher Hazard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagat_A/0/1/0/all/0/1\">Akshay Bhagat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buddharaju_B/0/1/0/all/0/1\">Balarama Raju Buddharaju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunming Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1\">Sammy Omari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Henggang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Imagenet Models Transfer Better. (arXiv:2204.09134v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09134","description":"<p>A commonly accepted hypothesis is that models with higher accuracy on\nImagenet perform better on other downstream tasks, leading to much research\ndedicated to optimizing Imagenet accuracy. Recently this hypothesis has been\nchallenged by evidence showing that self-supervised models transfer better than\ntheir supervised counterparts, despite their inferior Imagenet accuracy. This\ncalls for identifying the additional factors, on top of Imagenet accuracy, that\nmake models transferable. In this work we show that high diversity of the\nfeatures learnt by the model promotes transferability jointly with Imagenet\naccuracy. Encouraged by the recent transferability results of self-supervised\nmodels, we propose a method that combines self-supervised and supervised\npretraining to generate models with both high diversity and high accuracy, and\nas a result high transferability. We demonstrate our results on several\narchitectures and multiple downstream tasks, including both single-label and\nmulti-label classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayman_N/0/1/0/all/0/1\">Niv Nayman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golbert_A/0/1/0/all/0/1\">Avram Golbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_T/0/1/0/all/0/1\">Tan Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RangeUDF: Semantic Surface Reconstruction from 3D Point Clouds. (arXiv:2204.09138v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09138","description":"<p>We present RangeUDF, a new implicit representation based framework to recover\nthe geometry and semantics of continuous 3D scene surfaces from point clouds.\nUnlike occupancy fields or signed distance fields which can only model closed\n3D surfaces, our approach is not restricted to any type of topology. Being\ndifferent from the existing unsigned distance fields, our framework does not\nsuffer from any surface ambiguity. In addition, our RangeUDF can jointly\nestimate precise semantics for continuous surfaces. The key to our approach is\na range-aware unsigned distance function together with a surface-oriented\nsemantic segmentation module. Extensive experiments show that RangeUDF clearly\nsurpasses state-of-the-art approaches for surface reconstruction on four point\ncloud datasets. Moreover, RangeUDF demonstrates superior generalization\ncapability across multiple unseen datasets, which is nearly impossible for all\nexisting approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengdi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby Breckon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Camera Multiple 3D Object Tracking on the Move for Autonomous Vehicles. (arXiv:2204.09151v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09151","description":"<p>The development of autonomous vehicles provides an opportunity to have a\ncomplete set of camera sensors capturing the environment around the car. Thus,\nit is important for object detection and tracking to address new challenges,\nsuch as achieving consistent results across views of cameras. To address these\nchallenges, this work presents a new Global Association Graph Model with Link\nPrediction approach to predict existing tracklets location and link detections\nwith tracklets via cross-attention motion modeling and appearance\nre-identification. This approach aims at solving issues caused by inconsistent\n3D object detection. Moreover, our model exploits to improve the detection\naccuracy of a standard 3D object detector in the nuScenes detection challenge.\nThe experimental results on the nuScenes dataset demonstrate the benefits of\nthe proposed method to produce SOTA performance on the existing vision-based\ntracking dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Pha Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1\">Kha Gia Quach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Bac Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Performance Evaluation of Action Recognition Models on Transcoded Low Quality Videos. (arXiv:2204.09166v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09166","description":"<p>In the design of action recognition models, the quality of videos in the\ndataset is an important issue, however the trade-off between the quality and\nperformance is often ignored. In general, action recognition models are trained\nand tested on high-quality videos, but in actual situations where action\nrecognition models are deployed, sometimes it might not be assumed that the\ninput videos are of high quality. In this study, we report qualitative\nevaluations of action recognition models for the quality degradation associated\nwith transcoding by JPEG and H.264/AVC. Experimental results are shown for\nevaluating the performance of pre-trained models on the transcoded validation\nvideos of Kinetics400. The models are also trained on the transcoded training\nvideos. From these results, we quantitatively show the degree of degradation of\nthe model performance with respect to the degradation of the video quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otani_A/0/1/0/all/0/1\">Aoi Otani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashiguchi_R/0/1/0/all/0/1\">Ryota Hashiguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omi_K/0/1/0/all/0/1\">Kazuki Omi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukushima_N/0/1/0/all/0/1\">Norishige Fukushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamaki_T/0/1/0/all/0/1\">Toru Tamaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Monocular Depth Priors in Visual-Inertial Initialization. (arXiv:2204.09171v1 [cs.RO])","link":"http://arxiv.org/abs/2204.09171","description":"<p>Visual-inertial odometry (VIO) is the pose estimation backbone for most AR/VR\nand autonomous robotic systems today, in both academia and industry. However,\nthese systems are highly sensitive to the initialization of key parameters such\nas sensor biases, gravity direction, and metric scale. In practical scenarios\nwhere high-parallax or variable acceleration assumptions are rarely met (e.g.\nhovering aerial robot, smartphone AR user not gesticulating with phone),\nclassical visual-inertial initialization formulations often become\nill-conditioned and/or fail to meaningfully converge. In this paper we target\nvisual-inertial initialization specifically for these low-excitation scenarios\ncritical to in-the-wild usage. We propose to circumvent the limitations of\nclassical visual-inertial structure-from-motion (SfM) initialization by\nincorporating a new learning-based measurement as a higher-level input. We\nleverage learned monocular depth images (mono-depth) to constrain the relative\ndepth of features, and upgrade the mono-depth to metric scale by jointly\noptimizing for its scale and shift. Our experiments show a significant\nimprovement in problem conditioning compared to a classical formulation for\nvisual-inertial initialization, and demonstrate significant accuracy and\nrobustness improvements relative to the state-of-the-art on public benchmarks,\nparticularly under motion-restricted scenarios. We further extend this\nimprovement to implementation within an existing odometry system to illustrate\nthe impact of our improved initialization method on resulting tracking\ntrajectories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1\">Abhishek Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_E/0/1/0/all/0/1\">Eric Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowdle_A/0/1/0/all/0/1\">Adarsh Kowdle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chao X. Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuToit_R/0/1/0/all/0/1\">Ryan C. DuToit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsotsos_K/0/1/0/all/0/1\">Konstantine Tsotsos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruction-Aware Prior Distillation for Semi-supervised Point Cloud Completion. (arXiv:2204.09186v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09186","description":"<p>Point clouds scanned by real-world sensors are always incomplete, irregular,\nand noisy, making the point cloud completion task become increasingly more\nimportant. Though many point cloud completion methods have been proposed, most\nof them require a large number of paired complete-incomplete point clouds for\ntraining, which is labor exhausted. In contrast, this paper proposes a novel\nReconstruction-Aware Prior Distillation semi-supervised point cloud completion\nmethod named RaPD, which takes advantage of a two-stage training scheme to\nreduce the dependence on a large-scale paired dataset. In training stage 1, the\nso-called deep semantic prior is learned from both unpaired complete and\nunpaired incomplete point clouds using a reconstruction-aware pretraining\nprocess. While in training stage 2, we introduce a semi-supervised prior\ndistillation process, where an encoder-decoder-based completion network is\ntrained by distilling the prior into the network utilizing only a small number\nof paired training samples. A self-supervised completion module is further\nintroduced, excavating the value of a large number of unpaired incomplete point\nclouds, leading to an increase in the network's performance. Extensive\nexperiments on several widely used datasets demonstrate that RaPD, the first\nsemi-supervised point cloud completion method, achieves superior performance to\nprevious methods on both homologous and heterologous scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhaoxin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kejian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTIRE 2022 Challenge on Stereo Image Super-Resolution: Methods and Results. (arXiv:2204.09197v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09197","description":"<p>In this paper, we summarize the 1st NTIRE challenge on stereo image\nsuper-resolution (restoration of rich details in a pair of low-resolution\nstereo images) with a focus on new solutions and results. This challenge has 1\ntrack aiming at the stereo image super-resolution problem under a standard\nbicubic degradation. In total, 238 participants were successfully registered,\nand 21 teams competed in the final testing phase. Among those participants, 20\nteams successfully submitted results with PSNR (RGB) scores better than the\nbaseline. This challenge establishes a new benchmark for stereo image SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction. (arXiv:2204.09204v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09204","description":"<p>When applying multi-instance learning (MIL) to make predictions for bags of\ninstances, the prediction accuracy of an instance often depends on not only the\ninstance itself but also its context in the corresponding bag. From the\nviewpoint of causal inference, such bag contextual prior works as a confounder\nand may result in model robustness and interpretability issues. Focusing on\nthis problem, we propose a novel interventional multi-instance learning (IMIL)\nframework to achieve deconfounded instance-level prediction. Unlike traditional\nlikelihood-based strategies, we design an Expectation-Maximization (EM)\nalgorithm based on causal intervention, providing a robust instance selection\nin the training phase and suppressing the bias caused by the bag contextual\nprior. Experiments on pathological image analysis demonstrate that our IMIL\nmethod substantially reduces false positives and outperforms state-of-the-art\nMIL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tiancheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongteng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Canqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Interference Exist When Training a Once-For-All Network?. (arXiv:2204.09210v1 [cs.LG])","link":"http://arxiv.org/abs/2204.09210","description":"<p>The Once-For-All (OFA) method offers an excellent pathway to deploy a trained\nneural network model into multiple target platforms by utilising the\nsupernet-subnet architecture. Once trained, a subnet can be derived from the\nsupernet (both architecture and trained weights) and deployed directly to the\ntarget platform with little to no retraining or fine-tuning. To train the\nsubnet population, OFA uses a novel training method called Progressive\nShrinking (PS) which is designed to limit the negative impact of interference\nduring training. It is believed that higher interference during training\nresults in lower subnet population accuracies. In this work we take a second\nlook at this interference effect. Surprisingly, we find that interference\nmitigation strategies do not have a large impact on the overall subnet\npopulation performance. Instead, we find the subnet architecture selection bias\nduring training to be a more important aspect. To show this, we propose a\nsimple-yet-effective method called Random Subnet Sampling (RSS), which does not\nhave mitigation on the interference effect. Despite no mitigation, RSS is able\nto produce a better performing subnet population than PS in four\nsmall-to-medium-sized datasets; suggesting that the interference effect does\nnot play a pivotal role in these datasets. Due to its simplicity, RSS provides\na $1.9\\times$ reduction in training times compared to PS. A $6.1\\times$\nreduction can also be achieved with a reasonable drop in performance when the\nnumber of RSS training epochs are reduced. Code available at\nhttps://github.com/Jordan-HS/RSS-Interference-CVPRW2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shipard_J/0/1/0/all/0/1\">Jordan Shipard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiliem_A/0/1/0/all/0/1\">Arnold Wiliem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Progressive High Dynamic Range Image Restoration via Attention and Alignment Network. (arXiv:2204.09213v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09213","description":"<p>HDR is an important part of computational photography technology. In this\npaper, we propose a lightweight neural network called Efficient\nAttention-and-alignment-guided Progressive Network (EAPNet) for the challenge\nNTIRE 2022 HDR Track 1 and Track 2. We introduce a multi-dimensional\nlightweight encoding module to extract features. Besides, we propose\nProgressive Dilated U-shape Block (PDUB) that can be a progressive\nplug-and-play module for dynamically tuning MAccs and PSNR. Finally, we use\nfast and low-power feature-align module to deal with misalignment problem in\nplace of the time-consuming Deformable Convolutional Network (DCN). The\nexperiments show that our method achieves about 20 times compression on MAccs\nwith better mu-PSNR and PSNR compared to the state-of-the-art method. We got\nthe second place of both two tracks during the testing phase. Figure1. shows\nthe visualized result of NTIRE 2022 HDR challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gaocheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision System of Curling Robots: Thrower and Skip. (arXiv:2204.09221v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09221","description":"<p>We built a vision system of curling robot which can be expected to play with\nhuman curling player. Basically, we built two types of vision systems for\nthrower and skip robots, respectively. First, the thrower robot drives towards\na given point of curling sheet to release a stone. Our vision system in the\nthrower robot initialize 3DoF pose on two dimensional curling sheet and updates\nthe pose to decide for the decision of stone release. Second, the skip robot\nstands at the opposite side of the thrower robot and monitors the state of the\ngame to make a strategic decision. Our vision system in the skip robot\nrecognize every stones on the curling sheet precisely. Since the viewpoint is\nquite perspective, many stones are occluded by each others so it is challenging\nto estimate the accurate position of stone. Thus, we recognize the ellipses of\nstone handles outline to find the exact midpoint of the stones using\nperspective Hough transform. Furthermore, we perform tracking of a thrown stone\nto produce a trajectory for ice condition analysis. Finally, we implemented our\nvision systems on two mobile robots and successfully perform a single turn and\neven careful gameplay. Specifically, our vision system includes three cameras\nwith different viewpoint for their respective purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seongwook Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gayoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1\">Myungpyo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sull_S/0/1/0/all/0/1\">Sanghoon Sull</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-LITE: Learning Transferable Visual Models with External Knowledge. (arXiv:2204.09222v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09222","description":"<p>Recent state-of-the-art computer vision systems are trained from natural\nlanguage supervision, ranging from simple object category names to descriptive\ncaptions. This free form of supervision ensures high generality and usability\nof the learned visual models, based on extensive heuristics on data collection\nto cover as many visual concepts as possible. Alternatively, learning with\nexternal knowledge about images is a promising way which leverages a much more\nstructured source of supervision. In this paper, we propose K-LITE\n(Knowledge-augmented Language-Image Training and Evaluation), a simple strategy\nto leverage external knowledge to build transferable visual systems: In\ntraining, it enriches entities in natural language with WordNet and Wiktionary\nknowledge, leading to an efficient and scalable approach to learning image\nrepresentations that can understand both visual concepts and their knowledge;\nIn evaluation, the natural language is also augmented with external knowledge\nand then used to reference learned visual concepts (or describe new ones) to\nenable zero-shot and few-shot transfer of the pre-trained models. We study the\nperformance of K-LITE on two important computer vision problems, image\nclassification and object detection, benchmarking on 20 and 13 different\nexisting datasets, respectively. The proposed knowledge-augmented models show\nsignificant improvement in transfer learning performance over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dark Spot Detection from SAR Images Based on Superpixel Deeper Graph Convolutional Network. (arXiv:2204.09230v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09230","description":"<p>Synthetic Aperture Radar (SAR) is the main instrument utilized for the\ndetection of oil slicks on the ocean surface. In SAR images, some areas\naffected by ocean phenomena, such as rain cells, upwellings, and internal\nwaves, or discharge from oil spills appear as dark spots on images. Dark spot\ndetection is the first step in the detection of oil spills, which then become\noil slick candidates. The accuracy of dark spot segmentation ultimately affects\nthe accuracy of oil slick identification. Although some advanced deep learning\nmethods that use pixels as processing units perform well in remote sensing\nimage semantic segmentation, detecting some dark spots with weak boundaries\nfrom noisy SAR images remains a huge challenge. We propose a dark spot\ndetection method based on superpixels deeper graph convolutional networks\n(SGDCN) in this paper, which takes the superpixels as the processing units and\nextracts features for each superpixel. The features calculated from superpixel\nregions are more robust than those from fixed pixel neighborhoods. To reduce\nthe difficulty of learning tasks, we discard irrelevant features and obtain an\noptimal subset of features. After superpixel segmentation, the images are\ntransformed into graphs with superpixels as nodes, which are fed into the\ndeeper graph convolutional neural network for node classification. This graph\nneural network uses a differentiable aggregation function to aggregate the\nfeatures of nodes and neighbors to form more advanced features. It is the first\ntime using it for dark spot detection. To validate our method, we mark all dark\nspots on six SAR images covering the Baltic Sea and construct a dark spots\ndetection dataset, which has been made publicly available\n(https://drive.google.com/drive/folders/12UavrntkDSPrItISQ8iGefXn2gIZHxJ6?usp=sharing).\nThe experimental results demonstrate that our proposed SGDCN is robust and\neffective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaojian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yansheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-based Positioning and Pose Estimation. (arXiv:2204.09232v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09232","description":"<p>Recent advances in deep learning and computer vision offer an excellent\nopportunity to investigate high-level visual analysis tasks such as human\nlocalization and human pose estimation. Although the performance of human\nlocalization and human pose estimation has significantly improved in recent\nreports, they are not perfect and erroneous localization and pose estimation\ncan be expected among video frames. Studies on the integration of these\ntechniques into a generic pipeline that is robust to noise introduced from\nthose errors are still lacking. This paper fills the missing study. We explored\nand developed two working pipelines that suited the visual-based positioning\nand pose estimation tasks. Analyses of the proposed pipelines were conducted on\na badminton game. We showed that the concept of tracking by detection could\nwork well, and errors in position and pose could be effectively handled by a\nlinear interpolation technique using information from nearby frames. The\nresults showed that the Visual-based Positioning and Pose Estimation could\ndeliver position and pose estimations with good spatial and temporal\nresolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phon_Amnuaisuk_S/0/1/0/all/0/1\">Somnuk Phon-Amnuaisuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_K/0/1/0/all/0/1\">Ken T. Murata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovavisaruch_L/0/1/0/all/0/1\">La-Or Kovavisaruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_T/0/1/0/all/0/1\">Tiong-Hoo Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavarangkoon_P/0/1/0/all/0/1\">Praphan Pavarangkoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizuhara_T/0/1/0/all/0/1\">Takamichi Mizuhara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving The Long-Tailed Problem via Intra- and Inter-Category Balance. (arXiv:2204.09234v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09234","description":"<p>Benchmark datasets for visual recognition assume that data is uniformly\ndistributed, while real-world datasets obey long-tailed distribution. Current\napproaches handle the long-tailed problem to transform the long-tailed dataset\nto uniform distribution by re-sampling or re-weighting strategies. These\napproaches emphasize the tail classes but ignore the hard examples in head\nclasses, which result in performance degradation. In this paper, we propose a\nnovel gradient harmonized mechanism with category-wise adaptive precision to\ndecouple the difficulty and sample size imbalance in the long-tailed problem,\nwhich are correspondingly solved via intra- and inter-category balance\nstrategies. Specifically, intra-category balance focuses on the hard examples\nin each category to optimize the decision boundary, while inter-category\nbalance aims to correct the shift of decision boundary by taking each category\nas a unit. Extensive experiments demonstrate that the proposed method\nconsistently outperforms other approaches on all the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tiancheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-based Cross-Modal Retrieval with Probabilistic Representations. (arXiv:2204.09268v1 [cs.LG])","link":"http://arxiv.org/abs/2204.09268","description":"<p>Probabilistic embeddings have proven useful for capturing polysemous word\nmeanings, as well as ambiguity in image matching. In this paper, we study the\nadvantages of probabilistic embeddings in a cross-modal setting (i.e., text and\nimages), and propose a simple approach that replaces the standard vector point\nembeddings in extant image-text matching models with probabilistic\ndistributions that are parametrically learned. Our guiding hypothesis is that\nthe uncertainty encoded in the probabilistic embeddings captures the\ncross-modal ambiguity in the input instances, and that it is through capturing\nthis uncertainty that the probabilistic models can perform better at downstream\ntasks, such as image-to-text or text-to-image retrieval. Through extensive\nexperiments on standard and new benchmarks, we show a consistent advantage for\nprobabilistic representations in cross-modal retrieval, and validate the\nability of our embeddings to capture uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pishdad_L/0/1/0/all/0/1\">Leila Pishdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1\">Konstantinos G. Derpanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jepson_A/0/1/0/all/0/1\">Allan Jepson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazly_A/0/1/0/all/0/1\">Afsaneh Fazly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Video-based Action Quality Assessment. (arXiv:2204.09271v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09271","description":"<p>Human action recognition and analysis have great demand and important\napplication significance in video surveillance, video retrieval, and\nhuman-computer interaction. The task of human action quality evaluation\nrequires the intelligent system to automatically and objectively evaluate the\naction completed by the human. The action quality assessment model can reduce\nthe human and material resources spent in action evaluation and reduce\nsubjectivity. In this paper, we provide a comprehensive survey of existing\npapers on video-based action quality assessment. Different from human action\nrecognition, the application scenario of action quality assessment is\nrelatively narrow. Most of the existing work focuses on sports and medical\ncare. We first introduce the definition and challenges of human action quality\nassessment. Then we present the existing datasets and evaluation metrics. In\naddition, we summarized the methods of sports and medical care according to the\nmodel categories and publishing institutions according to the characteristics\nof the two fields. At the end, combined with recent work, the promising\ndevelopment direction in action quality assessment is discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shunli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1\">Peng Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suo_T/0/1/0/all/0/1\">Tao Suo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ka Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sound-Guided Semantic Video Generation. (arXiv:2204.09273v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09273","description":"<p>The recent success in StyleGAN demonstrates that pre-trained StyleGAN latent\nspace is useful for realistic video generation. However, the generated motion\nin the video is usually not semantically meaningful due to the difficulty of\ndetermining the direction and magnitude in the StyleGAN latent space. In this\npaper, we propose a framework to generate realistic videos by leveraging\nmultimodal (sound-image-text) embedding space. As sound provides the temporal\ncontexts of the scene, our framework learns to generate a video that is\nsemantically consistent with sound. First, our sound inversion module maps the\naudio directly into the StyleGAN latent space. We then incorporate the\nCLIP-based multimodal embedding space to further provide the audio-visual\nrelationships. Finally, the proposed frame generator learns to find the\ntrajectory in the latent space which is coherent with the corresponding sound\nand generates a video in a hierarchical manner. We provide the new\nhigh-resolution landscape video dataset (audio-visual pair) for the\nsound-guided video generation task. The experiments show that our model\noutperforms the state-of-the-art methods in terms of video quality. We further\nshow several applications including image and video editing to verify the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_G/0/1/0/all/0/1\">Gyeongrok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jihyun Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_W/0/1/0/all/0/1\">Won Jeong Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sang Ho Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangpil Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situational Perception Guided Image Matting. (arXiv:2204.09276v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09276","description":"<p>Most automatic matting methods try to separate the salient foreground from\nthe background. However, the insufficient quantity and subjective bias of the\ncurrent existing matting datasets make it difficult to fully explore the\nsemantic association between object-to-object and object-to-environment in a\ngiven image. In this paper, we propose a Situational Perception Guided Image\nMatting (SPG-IM) method that mitigates subjective bias of matting annotations\nand captures sufficient situational perception information for better global\nsaliency distilled from the visual-to-textual task. SPG-IM can better associate\ninter-objects and object-to-environment saliency, and compensate the subjective\nnature of image matting and its expensive annotation. We also introduce a\ntextual Semantic Transformation (TST) module that can effectively transform and\nintegrate the semantic feature stream to guide the visual representations. In\naddition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed\nto adaptively switch multi-scale receptive fields and focal points to enhance\nboth global and local details. Extensive experiments demonstrate the\neffectiveness of situational perception guidance from the visual-to-textual\ntasks on image matting, and our model outperforms the state-of-the-art methods.\nWe also analyze the significance of different components in our model. The code\nwill be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Structured State-Evolution for Vision-Language Navigation. (arXiv:2204.09280v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09280","description":"<p>Vision-and-language Navigation (VLN) task requires an embodied agent to\nnavigate to a remote location following a natural language instruction.\nPrevious methods usually adopt a sequence model (e.g., Transformer and LSTM) as\nthe navigator. In such a paradigm, the sequence model predicts action at each\nstep through a maintained navigation state, which is generally represented as a\none-dimensional vector. However, the crucial navigation clues (i.e.,\nobject-level environment layout) for embodied navigation task is discarded\nsince the maintained vector is essentially unstructured. In this paper, we\npropose a novel Structured state-Evolution (SEvol) model to effectively\nmaintain the environment layout clues for VLN. Specifically, we utilise the\ngraph-based feature to represent the navigation state instead of the\nvector-based state. Accordingly, we devise a Reinforced Layout clues Miner\n(RLM) to mine and detect the most crucial layout graph for long-term navigation\nvia a customised reinforcement learning strategy. Moreover, the Structured\nEvolving Module (SEM) is proposed to maintain the structured graph-based state\nduring navigation, where the state is gradually evolved to learn the\nobject-level spatial-temporal relationship. The experiments on the R2R and R4R\ndatasets show that the proposed SEvol model improves VLN models' performance by\nlarge margins, e.g., +3% absolute SPL accuracy for NvEM and +8% for EnvDrop on\nthe R2R test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_E/0/1/0/all/0/1\">Erli Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Object Interaction Detection via Disentangled Transformer. (arXiv:2204.09290v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09290","description":"<p>Human-Object Interaction Detection tackles the problem of joint localization\nand classification of human object interactions. Existing HOI transformers\neither adopt a single decoder for triplet prediction, or utilize two parallel\ndecoders to detect individual objects and interactions separately, and compose\ntriplets by a matching process. In contrast, we decouple the triplet prediction\ninto human-object pair detection and interaction classification. Our main\nmotivation is that detecting the human-object instances and classifying\ninteractions accurately needs to learn representations that focus on different\nregions. To this end, we present Disentangled Transformer, where both encoder\nand decoder are disentangled to facilitate learning of two sub-tasks. To\nassociate the predictions of disentangled decoders, we first generate a unified\nrepresentation for HOI triplets with a base decoder, and then utilize it as\ninput feature of each disentangled decoder. Extensive experiments show that our\nmethod outperforms prior work on two public HOI benchmarks by a sizeable\nmargin. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhichao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A 3-stage Spectral-spatial Method for Hyperspectral Image Classification. (arXiv:2204.09294v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09294","description":"<p>Hyperspectral images often have hundreds of spectral bands of different\nwavelengths captured by aircraft or satellites that record land coverage.\nIdentifying detailed classes of pixels becomes feasible due to the enhancement\nin spectral and spatial resolution of hyperspectral images. In this work, we\npropose a novel framework that utilizes both spatial and spectral information\nfor classifying pixels in hyperspectral images. The method consists of three\nstages. In the first stage, the pre-processing stage, Nested Sliding Window\nalgorithm is used to reconstruct the original data by {enhancing the\nconsistency of neighboring pixels} and then Principal Component Analysis is\nused to reduce the dimension of data. In the second stage, Support Vector\nMachines are trained to estimate the pixel-wise probability map of each class\nusing the spectral information from the images. Finally, a smoothed total\nvariation model is applied to smooth the class probability vectors by {ensuring\nspatial connectivity} in the images. We demonstrate the superiority of our\nmethod against three state-of-the-art algorithms on six benchmark hyperspectral\ndata sets with 10 to 50 training labels for each class. The results show that\nour method gives the overall best performance in accuracy. Especially, our gain\nin accuracy increases when the number of labeled pixels decreases and therefore\nour method is more advantageous to be applied to problems with small training\nset. Hence it is of great practical significance since expert annotations are\noften expensive and difficult to collect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Raymond H. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruoning Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Restoration in Non-Linear Filtering Domain using MDB approach. (arXiv:2204.09296v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09296","description":"<p>This paper proposes a new technique based on a non-linear Minmax Detector\nBased (MDB) filter for image restoration. The aim of image enhancement is to\nreconstruct the true image from the corrupted image. The process of image\nacquisition frequently leads to degradation and the quality of the digitized\nimage becomes inferior to the original image. Image degradation can be due to\nthe addition of different types of noise in the original image. Image noise can\nbe modelled of many types and impulse noise is one of them. Impulse noise\ngenerates pixels with gray value not consistent with their local neighbourhood.\nIt appears as a sprinkle of both light and dark or only light spots in the\nimage. Filtering is a technique for enhancing the image. Linear filter is the\nfiltering in which the value of an output pixel is a linear combination of\nneighborhood values, which can produce blur in the image. Thus a variety of\nsmoothing techniques have been developed that are non linear. Median filter is\nthe one of the most popular non-linear filter. When considering a small\nneighborhood it is highly efficient but for large window and in case of high\nnoise it gives rise to more blurring to image. The Centre Weighted Mean (CWM)\nfilter has got a better average performance over the median filter. However the\noriginal pixel corrupted and noise reduction is substantial under high noise\ncondition. Hence this technique has also blurring affect on the image. To\nillustrate the superiority of the proposed approach, the proposed new scheme\nhas been simulated along with the standard ones and various restored\nperformance measures have been compared.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Satpathy_S/0/1/0/all/0/1\">S. K. Satpathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1\">S. Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagwanshi_K/0/1/0/all/0/1\">K. K. Nagwanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardil_C/0/1/0/all/0/1\">C. Ardil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Non-linear Filtering Technique for Image Restoration. (arXiv:2204.09302v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09302","description":"<p>Removing noise from the any processed images is very important. Noise should\nbe removed in such a way that important information of image should be\npreserved. A decisionbased nonlinear algorithm for elimination of band lines,\ndrop lines, mark, band lost and impulses in images is presented in this paper.\nThe algorithm performs two simultaneous operations, namely, detection of\ncorrupted pixels and evaluation of new pixels for replacing the corrupted\npixels. Removal of these artifacts is achieved without damaging edges and\ndetails. However, the restricted window size renders median operation less\neffective whenever noise is excessive in that case the proposed algorithm\nautomatically switches to mean filtering. The performance of the algorithm is\nanalyzed in terms of Mean Square Error [MSE], Peak-Signal-to-Noise Ratio\n[PSNR], Signal-to-Noise Ratio Improved [SNRI], Percentage Of Noise Attenuated\n[PONA], and Percentage Of Spoiled Pixels [POSP]. This is compared with standard\nalgorithms already in use and improved performance of the proposed algorithm is\npresented. The advantage of the proposed algorithm is that a single algorithm\ncan replace several independent algorithms which are required for removal of\ndifferent artifacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Satpathy_S/0/1/0/all/0/1\">S. K. Satpathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_S/0/1/0/all/0/1\">S. Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagwanshi_K/0/1/0/all/0/1\">K. K. Nagwanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">S. K. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardil_C/0/1/0/all/0/1\">C. Ardil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention in Attention: Modeling Context Correlation for Efficient Video Classification. (arXiv:2204.09303v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09303","description":"<p>Attention mechanisms have significantly boosted the performance of video\nclassification neural networks thanks to the utilization of perspective\ncontexts. However, the current research on video attention generally focuses on\nadopting a specific aspect of contexts (e.g., channel, spatial/temporal, or\nglobal context) to refine the features and neglects their underlying\ncorrelation when computing attentions. This leads to incomplete context\nutilization and hence bears the weakness of limited performance improvement. To\ntackle the problem, this paper proposes an efficient attention-in-attention\n(AIA) method for element-wise feature refinement, which investigates the\nfeasibility of inserting the channel context into the spatio-temporal attention\nlearning module, referred to as CinST, and also its reverse variant, referred\nto as STinC. Specifically, we instantiate the video feature contexts as\ndynamics aggregated along a specific axis with global average and max pooling\noperations. The workflow of an AIA module is that the first attention block\nuses one kind of context information to guide the gating weights calculation of\nthe second attention that targets at the other context. Moreover, all the\ncomputational operations in attention units act on the pooled dimension, which\nresults in quite few computational cost increase ($&lt;$0.02\\%). To verify our\nmethod, we densely integrate it into two classical video network backbones and\nconduct extensive experiments on several standard video classification\nbenchmarks. The source code of our AIA is available at\n\\url{https://github.com/haoyanbin918/Attention-in-Attention}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinjian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jinmeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deeper Look into Aleatoric and Epistemic Uncertainty Disentanglement. (arXiv:2204.09308v1 [cs.LG])","link":"http://arxiv.org/abs/2204.09308","description":"<p>Neural networks are ubiquitous in many tasks, but trusting their predictions\nis an open issue. Uncertainty quantification is required for many applications,\nand disentangled aleatoric and epistemic uncertainties are best. In this paper,\nwe generalize methods to produce disentangled uncertainties to work with\ndifferent uncertainty quantification methods, and evaluate their capability to\nproduce disentangled uncertainties. Our results show that: there is an\ninteraction between learning aleatoric and epistemic uncertainty, which is\nunexpected and violates assumptions on aleatoric uncertainty, some methods like\nFlipout produce zero epistemic uncertainty, aleatoric uncertainty is unreliable\nin the out-of-distribution setting, and Ensembles provide overall the best\ndisentangling quality. We also explore the error produced by the number of\nsamples hyper-parameter in the sampling softmax function, recommending N &gt; 100\nsamples. We expect that our formulation and results help practitioners and\nresearchers choose uncertainty methods and expand the use of disentangled\nuncertainties, as well as motivate additional research into this topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saromo_D/0/1/0/all/0/1\">Daniel Saromo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results. (arXiv:2204.09314v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09314","description":"<p>This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality\nEnhancement of Compressed Video. In this challenge, we proposed the LDV 2.0\ndataset, which includes the LDV dataset (240 videos) and 95 additional videos.\nThis challenge includes three tracks. Track 1 aims at enhancing the videos\ncompressed by HEVC at a fixed QP. Track 2 and Track 3 target both the\nsuper-resolution and quality enhancement of HEVC compressed video. They require\nx2 and x4 super-resolution, respectively. The three tracks totally attract more\nthan 600 registrations. In the test phase, 8 teams, 8 teams and 12 teams\nsubmitted the final results to Tracks 1, 2 and 3, respectively. The proposed\nmethods and solutions gauge the state-of-the-art of super-resolution and\nquality enhancement of compressed video. The proposed LDV 2.0 dataset is\navailable at https://github.com/RenYang-home/LDV_dataset. The homepage of this\nchallenge (including open-sourced codes) is at\nhttps://github.com/RenYang-home/NTIRE22_VEnh_SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meisong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Q/0/1/0/all/0/1\">Qunliang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Minglang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaida Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Y/0/1/0/all/0/1\">Youcheng Ben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Gang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Renlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhilu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_W/0/1/0/all/0/1\">Wei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhengyao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunjin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingcai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostyakov_P/0/1/0/all/0/1\">Pavel Ostyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dmitry_V/0/1/0/all/0/1\">Vyal Dmitry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanayev_S/0/1/0/all/0/1\">Shakarim Soltanayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sergey_C/0/1/0/all/0/1\">Chervontsev Sergey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magauiya_Z/0/1/0/all/0/1\">Zhussip Magauiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xueyi Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michelini_Y/0/1/0/all/0/1\">Youliang Yan Pablo Navarrete Michelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunhua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Diankai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Si Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Biao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chengjian Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kaidi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canh_T/0/1/0/all/0/1\">Thuong Nguyen Canh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_T/0/1/0/all/0/1\">Thong Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaopeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shijie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nian_C/0/1/0/all/0/1\">Chunmei Nian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Dong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jucai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhihuai Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dengyan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liuhan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shengjie Chen</a>, et al. (16 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logarithmic Morphological Neural Nets robust to lighting variations. (arXiv:2204.09319v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09319","description":"<p>Morphological neural networks allow to learn the weights of a structuring\nfunction knowing the desired output image. However, those networks are not\nintrinsically robust to lighting variations in images with an optical cause,\nsuch as a change of light intensity. In this paper, we introduce a\nmorphological neural network which possesses such a robustness to lighting\nvariations. It is based on the recent framework of Logarithmic Mathematical\nMorphology (LMM), i.e. Mathematical Morphology defined with the Logarithmic\nImage Processing (LIP) model. This model has a LIP additive law which simulates\nin images a variation of the light intensity. We especially learn the\nstructuring function of a LMM operator robust to those variations, namely : the\nmap of LIP-additive Asplund distances. Results in images show that our neural\nnetwork verifies the required property.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noyel_G/0/1/0/all/0/1\">Guillaume Noyel</a> (LHC), <a href=\"http://arxiv.org/find/cs/1/au:+Barbier__Renard_E/0/1/0/all/0/1\">Emile Barbier--Renard</a> (LHC), <a href=\"http://arxiv.org/find/cs/1/au:+Jourlin_M/0/1/0/all/0/1\">Michel Jourlin</a> (LHC), <a href=\"http://arxiv.org/find/cs/1/au:+Fournel_T/0/1/0/all/0/1\">Thierry Fournel</a> (LHC)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpiderNet: Hybrid Differentiable-Evolutionary Architecture Search via Train-Free Metrics. (arXiv:2204.09320v1 [cs.LG])","link":"http://arxiv.org/abs/2204.09320","description":"<p>Neural Architecture Search (NAS) algorithms are intended to remove the burden\nof manual neural network design, and have shown to be capable of designing\nexcellent models for a variety of well-known problems. However, these\nalgorithms require a variety of design parameters in the form of user\nconfiguration or hard-coded decisions which limit the variety of networks that\ncan be discovered. This means that NAS algorithms do not eliminate model design\ntuning, they instead merely shift the burden of where that tuning needs to be\napplied. In this paper, we present SpiderNet, a hybrid\ndifferentiable-evolutionary and hardware-aware algorithm that rapidly and\nefficiently produces state-of-the-art networks. More importantly, SpiderNet is\na proof-of-concept of a minimally-configured NAS algorithm; the majority of\ndesign choices seen in other algorithms are incorporated into SpiderNet's\ndynamically-evolving search space, minimizing the number of user choices to\njust two: reduction cell count and initial channel count. SpiderNet produces\nmodels highly-competitive with the state-of-the-art, and outperforms random\nsearch in accuracy, runtime, memory size, and parameter count.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geada_R/0/1/0/all/0/1\">Rob Geada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1\">Andrew Stephen McGough</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning for Sonar Image Classification. (arXiv:2204.09323v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09323","description":"<p>Self-supervised learning has proved to be a powerful approach to learn image\nrepresentations without the need of large labeled datasets. For underwater\nrobotics, it is of great interest to design computer vision algorithms to\nimprove perception capabilities such as sonar image classification. Due to the\nconfidential nature of sonar imaging and the difficulty to interpret sonar\nimages, it is challenging to create public large labeled sonar datasets to\ntrain supervised learning algorithms. In this work, we investigate the\npotential of three self-supervised learning methods (RotNet, Denoising\nAutoencoders, and Jigsaw) to learn high-quality sonar image representation\nwithout the need of human labels. We present pre-training and transfer learning\nresults on real-life sonar image datasets. Our results indicate that\nself-supervised pre-training yields classification performance comparable to\nsupervised pre-training in a few-shot transfer learning setup across all three\nmethods. Code and self-supervised pre-trained models are be available at\nhttps://github.com/agrija9/ssl-sonar-images\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Preciado_Grijalva_A/0/1/0/all/0/1\">Alan Preciado-Grijalva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1\">Bilal Wehbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firvida_M/0/1/0/all/0/1\">Miguel Bande Firvida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1\">Matias Valdenegro-Toro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NFormer: Robust Person Re-identification with Neighbor Transformer. (arXiv:2204.09331v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09331","description":"<p>Person re-identification aims to retrieve persons in highly varying settings\nacross different cameras and scenarios, in which robust and discriminative\nrepresentation learning is crucial. Most research considers learning\nrepresentations from single images, ignoring any potential interactions between\nthem. However, due to the high intra-identity variations, ignoring such\ninteractions typically leads to outlier features. To tackle this issue, we\npropose a Neighbor Transformer Network, or NFormer, which explicitly models\ninteractions across all input images, thus suppressing outlier features and\nleading to more robust representations overall. As modelling interactions\nbetween enormous amount of images is a massive task with lots of distractors,\nNFormer introduces two novel modules, the Landmark Agent Attention, and the\nReciprocal Neighbor Softmax. Specifically, the Landmark Agent Attention\nefficiently models the relation map between images by a low-rank factorization\nwith a few landmarks in feature space. Moreover, the Reciprocal Neighbor\nSoftmax achieves sparse attention to relevant -- rather than all -- neighbors\nonly, which alleviates interference of irrelevant representations and further\nrelieves the computational burden. In experiments on four large-scale datasets,\nNFormer achieves a new state-of-the-art. The code is released at\n\\url{https://github.com/haochenheheda/NFormer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiayi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Cardiac Segmentation: Towards Structure Mutual Information Maximization. (arXiv:2204.09334v1 [eess.IV])","link":"http://arxiv.org/abs/2204.09334","description":"<p>Unsupervised domain adaptation approaches have recently succeeded in various\nmedical image segmentation tasks. The reported works often tackle the domain\nshift problem by aligning the domain-invariant features and minimizing the\ndomain-specific discrepancies. That strategy works well when the difference\nbetween a specific domain and between different domains is slight. However, the\ngeneralization ability of these models on diverse imaging modalities remains a\nsignificant challenge. This paper introduces UDA-VAE++, an unsupervised domain\nadaptation framework for cardiac segmentation with a compact loss function\nlower bound. To estimate this new lower bound, we develop a novel Structure\nMutual Information Estimation (SMIE) block with a global estimator, a local\nestimator, and a prior information matching estimator to maximize the mutual\ninformation between the reconstruction and segmentation tasks. Specifically, we\ndesign a novel sequential reparameterization scheme that enables information\nflow and variance correction from the low-resolution latent space to the\nhigh-resolution latent space. Comprehensive experiments on benchmark cardiac\nsegmentation datasets demonstrate that our model outperforms previous\nstate-of-the-art qualitatively and quantitatively. The code is available at\nhttps://github.com/LOUEY233/Toward-Mutual-Information}{https://github.com/LOUEY233/Toward-Mutual-Information\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Changjie Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Shen Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_G/0/1/0/all/0/1\">Gaurav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Point Clouds: A Survey. (arXiv:2204.09337v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09337","description":"<p>Point cloud has drawn more and more research attention as well as real-world\napplications. However, many of these applications (e.g. autonomous driving and\nrobotic manipulation) are actually based on sequential point clouds (i.e. four\ndimensions) because the information of the static point cloud data could\nprovide is still limited. Recently, researchers put more and more effort into\nsequential point clouds. This paper presents an extensive review of the deep\nlearning-based methods for sequential point cloud research including dynamic\nflow estimation, object detection \\&amp; tracking, point cloud segmentation, and\npoint cloud forecasting. This paper further summarizes and compares the\nquantitative results of the reviewed methods over the public benchmark\ndatasets. Finally, this paper is concluded by discussing the challenges in the\ncurrent sequential point cloud research and pointing out insightful potential\nfuture research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OutCast: Outdoor Single-image Relighting with Cast Shadows. (arXiv:2204.09341v1 [cs.GR])","link":"http://arxiv.org/abs/2204.09341","description":"<p>We propose a relighting method for outdoor images. Our method mainly focuses\non predicting cast shadows in arbitrary novel lighting directions from a single\nimage while also accounting for shading and global effects such the sun light\ncolor and clouds. Previous solutions for this problem rely on reconstructing\noccluder geometry, e.g. using multi-view stereo, which requires many images of\nthe scene. Instead, in this work we make use of a noisy off-the-shelf\nsingle-image depth map estimation as a source of geometry. Whilst this can be a\ngood guide for some lighting effects, the resulting depth map quality is\ninsufficient for directly ray-tracing the shadows. Addressing this, we propose\na learned image space ray-marching layer that converts the approximate depth\nmap into a deep 3D representation that is fused into occlusion queries using a\nlearned traversal. Our proposed method achieves, for the first time,\nstate-of-the-art relighting results, with only a single image as input. For\nsupplementary material visit our project page at:\nhttps://dgriffiths.uk/outcast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_D/0/1/0/all/0/1\">David Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing unsupervised learning to improve sward content prediction and herbage mass estimation. (arXiv:2204.09343v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09343","description":"<p>Sward species composition estimation is a tedious one. Herbage must be\ncollected in the field, manually separated into components, dried and weighed\nto estimate species composition. Deep learning approaches using neural networks\nhave been used in previous work to propose faster and more cost efficient\nalternatives to this process by estimating the biomass information from a\npicture of an area of pasture alone. Deep learning approaches have, however,\nstruggled to generalize to distant geographical locations and necessitated\nfurther data collection to retrain and perform optimally in different climates.\nIn this work, we enhance the deep learning solution by reducing the need for\nground-truthed (GT) images when training the neural network. We demonstrate how\nunsupervised contrastive learning can be used in the sward composition\nprediction problem and compare with the state-of-the-art on the publicly\navailable GrassClover dataset collected in Denmark as well as a more recent\ndataset from Ireland where we tackle herbage mass and height estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albert_P/0/1/0/all/0/1\">Paul Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadeldin_M/0/1/0/all/0/1\">Mohamed Saadeldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_B/0/1/0/all/0/1\">Badri Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennessy_D/0/1/0/all/0/1\">Deirdre Hennessy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_A/0/1/0/all/0/1\">Aisling H. O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyber-Forensic Review of Human Footprint and Gait for Personal Identification. (arXiv:2204.09344v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09344","description":"<p>The human footprint is having a unique set of ridges unmatched by any other\nhuman being, and therefore it can be used in different identity documents for\nexample birth certificate, Indian biometric identification system AADHAR card,\ndriving license, PAN card, and passport. There are many instances of the crime\nscene where an accused must walk around and left the footwear impressions as\nwell as barefoot prints and therefore, it is very crucial to recovering the\nfootprints from identifying the criminals. Footprint-based biometric is a\nconsiderably newer technique for personal identification. Fingerprints, retina,\niris and face recognition are the methods most useful for attendance record of\nthe person. This time the world is facing the problem of global terrorism. It\nis challenging to identify the terrorist because they are living as regular as\nthe citizens do. Their soft target includes the industries of special interests\nsuch as defence, silicon and nanotechnology chip manufacturing units, pharmacy\nsectors. They pretend themselves as religious persons, so temples and other\nholy places, even in markets is in their targets. These are the places where\none can obtain their footprints quickly. The gait itself is sufficient to\npredict the behaviour of the suspects. The present research is driven to\nidentify the usefulness of footprint and gait as an alternative to personal\nidentification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagwanshi_K/0/1/0/all/0/1\">Kapil Kumar Nagwanshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Dual Stream Siamese U-net for Flood Detection on Multi-temporal Sentinel-1 Data. (arXiv:2204.09387v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09387","description":"<p>Due to climate and land-use change, natural disasters such as flooding have\nbeen increasing in recent years. Timely and reliable flood detection and\nmapping can help emergency response and disaster management. In this work, we\npropose a flood detection network using bi-temporal SAR acquisitions. The\nproposed segmentation network has an encoder-decoder architecture with two\nSiamese encoders for pre and post-flood images. The network's feature maps are\nfused and enhanced using attention blocks to achieve more accurate detection of\nthe flooded areas. Our proposed network is evaluated on publicly available\nSen1Flood11 benchmark dataset. The network outperformed the existing\nstate-of-the-art (uni-temporal) flood detection method by 6\\% IOU. The\nexperiments highlight that the combination of bi-temporal SAR data with an\neffective network architecture achieves more accurate flood detection than\nuni-temporal methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Ritu Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascetti_A/0/1/0/all/0/1\">Andrea Nascetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yifang Ban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Epistemic Uncertainty-Weighted Loss for Visual Bias Mitigation. (arXiv:2204.09389v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09389","description":"<p>Deep neural networks are highly susceptible to learning biases in visual\ndata. While various methods have been proposed to mitigate such bias, the\nmajority require explicit knowledge of the biases present in the training data\nin order to mitigate. We argue the relevance of exploring methods which are\ncompletely ignorant of the presence of any bias, but are capable of identifying\nand mitigating them. Furthermore, we propose using Bayesian neural networks\nwith an epistemic uncertainty-weighted loss function to dynamically identify\npotential bias in individual training samples and to weight them during\ntraining. We find a positive correlation between samples subject to bias and\nhigher epistemic uncertainties. Finally, we show the method has potential to\nmitigate visual bias on a bias benchmark dataset and on a real-world face\ndetection problem, and we consider the merits and weaknesses of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stone_R/0/1/0/all/0/1\">Rebecca S Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulpitt_A/0/1/0/all/0/1\">Andrew J Bulpitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogg_D/0/1/0/all/0/1\">David C Hogg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Scratches: Deployable Attacks to CNN Classifiers. (arXiv:2204.09397v1 [cs.LG])","link":"http://arxiv.org/abs/2204.09397","description":"<p>A growing body of work has shown that deep neural networks are susceptible to\nadversarial examples. These take the form of small perturbations applied to the\nmodel's input which lead to incorrect predictions. Unfortunately, most\nliterature focuses on visually imperceivable perturbations to be applied to\ndigital images that often are, by design, impossible to be deployed to physical\ntargets. We present Adversarial Scratches: a novel L0 black-box attack, which\ntakes the form of scratches in images, and which possesses much greater\ndeployability than other state-of-the-art attacks. Adversarial Scratches\nleverage B\\'ezier Curves to reduce the dimension of the search space and\npossibly constrain the attack to a specific location. We test Adversarial\nScratches in several scenarios, including a publicly available API and images\nof traffic signs. Results show that, often, our attack achieves higher fooling\nrate than other deployable state-of-the-art methods, while requiring\nsignificantly fewer queries and modifying very few pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giulivi_L/0/1/0/all/0/1\">Loris Giulivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jere_M/0/1/0/all/0/1\">Malhar Jere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossi_L/0/1/0/all/0/1\">Loris Rossi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciocarlie_G/0/1/0/all/0/1\">Gabriela Ciocarlie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hitaj_B/0/1/0/all/0/1\">Briland Hitaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boracchi_G/0/1/0/all/0/1\">Giacomo Boracchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Case-Aware Adversarial Training. (arXiv:2204.09398v1 [cs.LG])","link":"http://arxiv.org/abs/2204.09398","description":"<p>The neural network (NN) becomes one of the most heated type of models in\nvarious signal processing applications. However, NNs are extremely vulnerable\nto adversarial examples (AEs). To defend AEs, adversarial training (AT) is\nbelieved to be the most effective method while due to the intensive\ncomputation, AT is limited to be applied in most applications. In this paper,\nto resolve the problem, we design a generic and efficient AT improvement\nscheme, namely case-aware adversarial training (CAT). Specifically, the\nintuition stems from the fact that a very limited part of informative samples\ncan contribute to most of model performance. Alternatively, if only the most\ninformative AEs are used in AT, we can lower the computation complexity of AT\nsignificantly as maintaining the defense effect. To achieve this, CAT achieves\ntwo breakthroughs. First, a method to estimate the information degree of\nadversarial examples is proposed for AE filtering. Second, to further enrich\nthe information that the NN can obtain from AEs, CAT involves a weight\nestimation and class-level balancing based sampling strategy to increase the\ndiversity of AT at each iteration. Extensive experiments show that CAT is\nfaster than vanilla AT by up to 3x while achieving competitive defense effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Mingyuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenzhong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Ximeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianhua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Probabilistic Time-Evolving Approach to Scanpath Prediction. (arXiv:2204.09404v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09404","description":"<p>Human visual attention is a complex phenomenon that has been studied for\ndecades. Within it, the particular problem of scanpath prediction poses a\nchallenge, particularly due to the inter- and intra-observer variability, among\nother reasons. Besides, most existing approaches to scanpath prediction have\nfocused on optimizing the prediction of a gaze point given the previous ones.\nIn this work, we present a probabilistic time-evolving approach to scanpath\nprediction, based on Bayesian deep learning. We optimize our model using a\nnovel spatio-temporal loss function based on a combination of Kullback-Leibler\ndivergence and dynamic time warping, jointly considering the spatial and\ntemporal dimensions of scanpaths. Our scanpath prediction framework yields\nresults that outperform those of current state-of-the-art approaches, and are\nalmost on par with the human baseline, suggesting that our model is able to\ngenerate scanpaths whose behavior closely resembles those of the real ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_D/0/1/0/all/0/1\">Daniel Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1\">Diego Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masia_B/0/1/0/all/0/1\">Belen Masia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Moment Retrieval from Text Queries via Single Frame Annotation. (arXiv:2204.09409v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09409","description":"<p>Video moment retrieval aims at finding the start and end timestamps of a\nmoment (part of a video) described by a given natural language query. Fully\nsupervised methods need complete temporal boundary annotations to achieve\npromising results, which is costly since the annotator needs to watch the whole\nmoment. Weakly supervised methods only rely on the paired video and query, but\nthe performance is relatively poor. In this paper, we look closer into the\nannotation process and propose a new paradigm called \"glance annotation\". This\nparadigm requires the timestamp of only one single random frame, which we refer\nto as a \"glance\", within the temporal boundary of the fully supervised\ncounterpart. We argue this is beneficial because comparing to weak supervision,\ntrivial cost is added yet more potential in performance is provided. Under the\nglance annotation setting, we propose a method named as Video moment retrieval\nvia Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input\nvideo into clips and contrasts between clips and queries, in which glance\nguided Gaussian distributed weights are assigned to all clips. Our extensive\nexperiments indicate that ViGA achieves better results than the\nstate-of-the-art weakly supervised methods by a large margin, even comparable\nto fully supervised methods in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tianwen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Pai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daskalaki_E/0/1/0/all/0/1\">Elena Daskalaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRPose: Real-Time High-Resolution 6D Pose Estimation Network Using Knowledge Distillation. (arXiv:2204.09429v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09429","description":"<p>Real-time 6D object pose estimation is essential for many real-world\napplications, such as robotic grasping and augmented reality. To achieve an\naccurate object pose estimation from RGB images in real-time, we propose an\neffective and lightweight model, namely High-Resolution 6D Pose Estimation\nNetwork (HRPose). We adopt the efficient and small HRNetV2-W18 as a feature\nextractor to reduce computational burdens while generating accurate 6D poses.\nWith only 33\\% of the model size and lower computational costs, our HRPose\nachieves comparable performance compared with state-of-the-art models.\nMoreover, by transferring knowledge from a large model to our proposed HRPose\nthrough output and feature-similarity distillations, the performance of our\nHRPose is improved in effectiveness and efficiency. Numerical experiments on\nthe widely-used benchmark LINEMOD demonstrate the superiority of our proposed\nHRPose against state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1\">Qi Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zihao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1\">Shibei Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Mobile Food Recognition System for Dietary Assessment. (arXiv:2204.09432v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09432","description":"<p>Food recognition is an important task for a variety of applications,\nincluding managing health conditions and assisting visually impaired people.\nSeveral food recognition studies have focused on generic types of food or\nspecific cuisines, however, food recognition with respect to Middle Eastern\ncuisines has remained unexplored. Therefore, in this paper we focus on\ndeveloping a mobile friendly, Middle Eastern cuisine focused food recognition\napplication for assisted living purposes. In order to enable a low-latency,\nhigh-accuracy food classification system, we opted to utilize the Mobilenet-v2\ndeep learning model. As some of the foods are more popular than the others, the\nnumber of samples per class in the used Middle Eastern food dataset is\nrelatively imbalanced. To compensate for this problem, data augmentation\nmethods are applied on the underrepresented classes. Experimental results show\nthat using Mobilenet-v2 architecture for this task is beneficial in terms of\nboth accuracy and the memory usage. With the model achieving 94% accuracy on 23\nfood classes, the developed mobile application has potential to serve the\nvisually impaired in automatic food recognition via images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akti_S/0/1/0/all/0/1\">&#x15e;eymanur Akt&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qaraqe_M/0/1/0/all/0/1\">Marwa Qaraqe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PP-Matting: High-Accuracy Natural Image Matting. (arXiv:2204.09433v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09433","description":"<p>Natural image matting is a fundamental and challenging computer vision task.\nIt has many applications in image editing and composition. Recently, deep\nlearning-based approaches have achieved great improvements in image matting.\nHowever, most of them require a user-supplied trimap as an auxiliary input,\nwhich limits the matting applications in the real world. Although some\ntrimap-free approaches have been proposed, the matting quality is still\nunsatisfactory compared to trimap-based ones. Without the trimap guidance, the\nmatting models suffer from foreground-background ambiguity easily, and also\ngenerate blurry details in the transition area. In this work, we propose\nPP-Matting, a trimap-free architecture that can achieve high-accuracy natural\nimage matting. Our method applies a high-resolution detail branch (HRDB) that\nextracts fine-grained details of the foreground with keeping feature resolution\nunchanged. Also, we propose a semantic context branch (SCB) that adopts a\nsemantic segmentation subtask. It prevents the detail prediction from local\nambiguity caused by semantic context missing. In addition, we conduct extensive\nexperiments on two well-known benchmarks: Composition-1k and Distinctions-646.\nThe results demonstrate the superiority of PP-Matting over previous methods.\nFurthermore, we provide a qualitative evaluation of our method on human matting\nwhich shows its outstanding performance in the practical application. The code\nand pre-trained models will be available at PaddleSeg:\nhttps://github.com/PaddlePaddle/PaddleSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guowei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Juncai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yuying Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lutao Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shiyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zewu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_Q/0/1/0/all/0/1\">Qingqing Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FenceNet: Fine-grained Footwork Recognition in Fencing. (arXiv:2204.09434v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09434","description":"<p>Current data analysis for the Canadian Olympic fencing team is primarily done\nmanually by coaches and analysts. Due to the highly repetitive, yet dynamic and\nsubtle movements in fencing, manual data analysis can be inefficient and\ninaccurate. We propose FenceNet as a novel architecture to automate the\nclassification of fine-grained footwork techniques in fencing. FenceNet takes\n2D pose data as input and classifies actions using a skeleton-based action\nrecognition approach that incorporates temporal convolutional networks to\ncapture temporal information. We train and evaluate FenceNet on the Fencing\nFootwork Dataset (FFD), which contains 10 fencers performing 6 different\nfootwork actions for 10-11 repetitions each (652 total videos). FenceNet\nachieves 85.4% accuracy under 10-fold cross-validation, where each fencer is\nleft out as the test set. This accuracy is within 1% of the current\nstate-of-the-art method, JLJA (86.3%), which selects and fuses features\nengineered from skeleton data, depth videos, and inertial measurement units.\nBiFenceNet, a variant of FenceNet that captures the \"bidirectionality\" of human\nmovement through two separate networks, achieves 87.6% accuracy, outperforming\nJLJA. Since neither FenceNet nor BiFenceNet requires data from wearable\nsensors, unlike JLJA, they could be directly applied to most fencing videos,\nusing 2D pose data as input extracted from off-the-shelf 2D human pose\nestimators. In comparison to JLJA, our methods are also simpler as they do not\nrequire manual feature engineering, selection, or fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kevin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McPhee_J/0/1/0/all/0/1\">John McPhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hephaestus: A large scale multitask dataset towards InSAR understanding. (arXiv:2204.09435v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09435","description":"<p>Synthetic Aperture Radar (SAR) data and Interferometric SAR (InSAR) products\nin particular, are one of the largest sources of Earth Observation data. InSAR\nprovides unique information on diverse geophysical processes and geology, and\non the geotechnical properties of man-made structures. However, there are only\na limited number of applications that exploit the abundance of InSAR data and\ndeep learning methods to extract such knowledge. The main barrier has been the\nlack of a large curated and annotated InSAR dataset, which would be costly to\ncreate and would require an interdisciplinary team of experts experienced on\nInSAR data interpretation. In this work, we put the effort to create and make\navailable the first of its kind, manually annotated dataset that consists of\n19,919 individual Sentinel-1 interferograms acquired over 44 different\nvolcanoes globally, which are split into 216,106 InSAR patches. The annotated\ndataset is designed to address different computer vision problems, including\nvolcano state classification, semantic segmentation of ground deformation,\ndetection and classification of atmospheric signals in InSAR imagery,\ninterferogram captioning, text to InSAR generation, and InSAR image quality\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1\">Nikolaos Ioannis Bountos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1\">Ioannis Papoutsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1\">Dimitrios Michail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karavias_A/0/1/0/all/0/1\">Andreas Karavias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elias_P/0/1/0/all/0/1\">Panagiotis Elias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcharidis_I/0/1/0/all/0/1\">Isaak Parcharidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAM-GAN : Image Inpainting using Dynamic Attention Map based on Fake Texture Detection. (arXiv:2204.09442v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09442","description":"<p>Deep neural advancements have recently brought remarkable image synthesis\nperformance to the field of image inpainting. The adaptation of generative\nadversarial networks (GAN) in particular has accelerated significant progress\nin high-quality image reconstruction. However, although many notable GAN-based\nnetworks have been proposed for image inpainting, still pixel artifacts or\ncolor inconsistency occur in synthesized images during the generation process,\nwhich are usually called fake textures. To reduce pixel inconsistency disorder\nresulted from fake textures, we introduce a GAN-based model using dynamic\nattention map (DAM-GAN). Our proposed DAM-GAN concentrates on detecting fake\ntexture and products dynamic attention maps to diminish pixel inconsistency\nfrom the feature maps in the generator. Evaluation results on CelebA-HQ and\nPlaces2 datasets with other image inpainting approaches show the superiority of\nour network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_D/0/1/0/all/0/1\">Dongmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daijin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIMO: Gaze-Informed Human Motion Prediction in Context. (arXiv:2204.09443v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09443","description":"<p>Predicting human motion is critical for assistive robots and AR/VR\napplications, where the interaction with humans needs to be safe and\ncomfortable. Meanwhile, an accurate prediction depends on understanding both\nthe scene context and human intentions. Even though many works study\nscene-aware human motion prediction, the latter is largely underexplored due to\nthe lack of ego-centric views that disclose human intent and the limited\ndiversity in motion and scenes. To reduce the gap, we propose a large-scale\nhuman motion dataset that delivers high-quality body pose sequences, scene\nscans, as well as ego-centric views with eye gaze that serves as a surrogate\nfor inferring human intent. By employing inertial sensors for motion capture,\nour data collection is not tied to specific scenes, which further boosts the\nmotion dynamics observed from our subjects. We perform an extensive study of\nthe benefits of leveraging eye gaze for ego-centric human motion prediction\nwith various state-of-the-art architectures. Moreover, to realize the full\npotential of gaze, we propose a novel network architecture that enables\nbidirectional communication between the gaze and motion branches. Our network\nachieves the top performance in human motion prediction on the proposed\ndataset, thanks to the intent information from the gaze and the denoised gaze\nfeature modulated by the motion. The proposed dataset and our network\nimplementation will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaman Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STAU: A SpatioTemporal-Aware Unit for Video Prediction and Beyond. (arXiv:2204.09456v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09456","description":"<p>Video prediction aims to predict future frames by modeling the complex\nspatiotemporal dynamics in videos. However, most of the existing methods only\nmodel the temporal information and the spatial information for videos in an\nindependent manner but haven't fully explored the correlations between both\nterms. In this paper, we propose a SpatioTemporal-Aware Unit (STAU) for video\nprediction and beyond by exploring the significant spatiotemporal correlations\nin videos. On the one hand, the motion-aware attention weights are learned from\nthe spatial states to help aggregate the temporal states in the temporal\ndomain. On the other hand, the appearance-aware attention weights are learned\nfrom the temporal states to help aggregate the spatial states in the spatial\ndomain. In this way, the temporal information and the spatial information can\nbe greatly aware of each other in both domains, during which, the\nspatiotemporal receptive field can also be greatly broadened for more reliable\nspatiotemporal modeling. Experiments are not only conducted on traditional\nvideo prediction tasks but also other tasks beyond video prediction, including\nthe early action recognition and object detection tasks. Experimental results\nshow that our STAU can outperform other methods on all tasks in terms of\nperformance and computation efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1\">Zheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THORN: Temporal Human-Object Relation Network for Action Recognition. (arXiv:2204.09468v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09468","description":"<p>Most action recognition models treat human activities as unitary events.\nHowever, human activities often follow a certain hierarchy. In fact, many human\nactivities are compositional. Also, these actions are mostly human-object\ninteractions. In this paper we propose to recognize human action by leveraging\nthe set of interactions that define an action. In this work, we present an\nend-to-end network: THORN, that can leverage important human-object and\nobject-object interactions to predict actions. This model is built on top of a\n3D backbone network. The key components of our model are: 1) An object\nrepresentation filter for modeling object. 2) An object relation reasoning\nmodule to capture object relations. 3) A classification layer to predict the\naction labels. To show the robustness of THORN, we evaluate it on\nEPIC-Kitchen55 and EGTEA Gaze+, two of the largest and most challenging\nfirst-person and human-object interaction datasets. THORN achieves\nstate-of-the-art performance on both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guermal_M/0/1/0/all/0/1\">Mohammed Guermal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GazeOnce: Real-Time Multi-Person Gaze Estimation. (arXiv:2204.09480v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09480","description":"<p>Appearance-based gaze estimation aims to predict the 3D eye gaze direction\nfrom a single image. While recent deep learning-based approaches have\ndemonstrated excellent performance, they usually assume one calibrated face in\neach input image and cannot output multi-person gaze in real time. However,\nsimultaneous gaze estimation for multiple people in the wild is necessary for\nreal-world applications. In this paper, we propose the first one-stage\nend-to-end gaze estimation method, GazeOnce, which is capable of simultaneously\npredicting gaze directions for multiple faces (&gt;10) in an image. In addition,\nwe design a sophisticated data generation pipeline and propose a new dataset,\nMPSGaze, which contains full images of multiple people with 3D gaze ground\ntruth. Experimental results demonstrate that our unified framework not only\noffers a faster speed, but also provides a lower gaze estimation error compared\nwith state-of-the-art methods. This technique can be useful in real-time\napplications with multiple users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingfang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Special Session: Towards an Agile Design Methodology for Efficient, Reliable, and Secure ML Systems. (arXiv:2204.09514v1 [cs.AR])","link":"http://arxiv.org/abs/2204.09514","description":"<p>The real-world use cases of Machine Learning (ML) have exploded over the past\nfew years. However, the current computing infrastructure is insufficient to\nsupport all real-world applications and scenarios. Apart from high efficiency\nrequirements, modern ML systems are expected to be highly reliable against\nhardware failures as well as secure against adversarial and IP stealing\nattacks. Privacy concerns are also becoming a first-order issue. This article\nsummarizes the main challenges in agile development of efficient, reliable and\nsecure ML systems, and then presents an outline of an agile design methodology\nto generate efficient, reliable and secure ML systems based on user-defined\nconstraints and objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1\">Shail Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_A/0/1/0/all/0/1\">Alberto Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1\">Muhammad Abdullah Hanif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1\">Amira Guesmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Aviral Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1\">Ihsen Alouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"De-biasing facial detection system using VAE. (arXiv:2204.09556v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09556","description":"<p>Bias in AI/ML-based systems is a ubiquitous problem and bias in AI/ML systems\nmay negatively impact society. There are many reasons behind a system being\nbiased. The bias can be due to the algorithm we are using for our problem or\nmay be due to the dataset we are using, having some features over-represented\nin it. In the face detection system bias due to the dataset is majorly seen.\nSometimes models learn only features that are over-represented in data and\nignore rare features from data which results in being biased toward those\nover-represented features. In real life, these biased systems are dangerous to\nsociety. The proposed approach uses generative models which are best suited for\nlearning underlying features(latent variables) from the dataset and by using\nthese learned features models try to reduce the threats which are there due to\nbias in the system. With the help of an algorithm, the bias present in the\ndataset can be removed. And then we train models on two datasets and compare\nthe results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kandge_V/0/1/0/all/0/1\">Vedant V. Kandge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandge_S/0/1/0/all/0/1\">Siddhant V. Kandge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumbharkar_K/0/1/0/all/0/1\">Kajal Kumbharkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattanshetti_P/0/1/0/all/0/1\">Prof. Tanuja Pattanshetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-view Brain Decoding. (arXiv:2204.09564v1 [q-bio.NC])","link":"http://arxiv.org/abs/2204.09564","description":"<p>How the brain captures the meaning of linguistic stimuli across multiple\nviews is still a critical open question in neuroscience. Consider three\ndifferent views of the concept apartment: (1) picture (WP) presented with the\ntarget word label, (2) sentence (S) using the target word, and (3) word cloud\n(WC) containing the target word along with other semantically related words.\nUnlike previous efforts, which focus only on single view analysis, in this\npaper, we study the effectiveness of brain decoding in a zero-shot cross-view\nlearning setup. Further, we propose brain decoding in the novel context of\ncross-view-translation tasks like image captioning (IC), image tagging (IT),\nkeyword extraction (KE), and sentence formation (SF). Using extensive\nexperiments, we demonstrate that cross-view zero-shot brain decoding is\npractical leading to ~0.68 average pairwise accuracy across view pairs. Also,\nthe decoded representations are sufficiently detailed to enable high accuracy\nfor cross-view-translation tasks with following pairwise accuracy: IC (78.0),\nIT (83.0), KE (83.7) and SF (74.5). Analysis of the contribution of different\nbrain networks reveals exciting cognitive insights: (1) A high percentage of\nvisual voxels are involved in image captioning and image tagging tasks, and a\nhigh percentage of language voxels are involved in the sentence formation and\nkeyword extraction tasks. (2) Zero-shot accuracy of the model trained on S view\nand tested on WC view is better than same-view accuracy of the model trained\nand tested on WC view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Oota_S/0/1/0/all/0/1\">Subba Reddy Oota</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arora_J/0/1/0/all/0/1\">Jashn Arora</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bapi_R/0/1/0/all/0/1\">Raju S. Bapi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fetal Brain Tissue Annotation and Segmentation Challenge Results. (arXiv:2204.09573v1 [eess.IV])","link":"http://arxiv.org/abs/2204.09573","description":"<p>In-utero fetal MRI is emerging as an important tool in the diagnosis and\nanalysis of the developing human brain. Automatic segmentation of the\ndeveloping fetal brain is a vital step in the quantitative analysis of prenatal\nneurodevelopment both in the research and clinical context. However, manual\nsegmentation of cerebral structures is time-consuming and prone to error and\ninter-observer variability. Therefore, we organized the Fetal Tissue Annotation\n(FeTA) Challenge in 2021 in order to encourage the development of automatic\nsegmentation algorithms on an international level. The challenge utilized FeTA\nDataset, an open dataset of fetal brain MRI reconstructions segmented into\nseven different tissues (external cerebrospinal fluid, grey matter, white\nmatter, ventricles, cerebellum, brainstem, deep grey matter). 20 international\nteams participated in this challenge, submitting a total of 21 algorithms for\nevaluation. In this paper, we provide a detailed analysis of the results from\nboth a technical and clinical perspective. All participants relied on deep\nlearning methods, mainly U-Nets, with some variability present in the network\narchitecture, optimization, and image pre- and post-processing. The majority of\nteams used existing medical imaging deep learning frameworks. The main\ndifferences between the submissions were the fine tuning done during training,\nand the specific pre- and post-processing steps performed. The challenge\nresults showed that almost all submissions performed similarly. Four of the top\nfive teams used ensemble learning methods. However, one team's algorithm\nperformed significantly superior to the other submissions, and consisted of an\nasymmetrical U-Net network architecture. This paper provides a first of its\nkind benchmark for future automatic multi-tissue segmentation algorithms for\nthe developing human brain in utero.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Payette_K/0/1/0/all/0/1\">Kelly Payette</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dumast_P/0/1/0/all/0/1\">Priscille de Dumast</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Licandro_R/0/1/0/all/0/1\">Roxane Licandro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_H/0/1/0/all/0/1\">Hui Ji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddiquee_M/0/1/0/all/0/1\">Md Mahfuzur Rahman Siddiquee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1\">Andriy Myronenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pei_Y/0/1/0/all/0/1\">Yuchen Pei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lisheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Ying Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_J/0/1/0/all/0/1\">Juanying Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huiquan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_G/0/1/0/all/0/1\">Guiming Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Hao Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rieu_Z/0/1/0/all/0/1\">ZunHyan Rieu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1\">Donghyeon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hyun Gi Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1\">Davood Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholipour_A/0/1/0/all/0/1\">Ali Gholipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torres_H/0/1/0/all/0/1\">Helena R. Torres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_B/0/1/0/all/0/1\">Bruno Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vilaca_J/0/1/0/all/0/1\">Jo&#xe3;o L. Vila&#xe7;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Avisdris_N/0/1/0/all/0/1\">Netanell Avisdris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ben_Zvi_O/0/1/0/all/0/1\">Ori Ben-Zvi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bashat_D/0/1/0/all/0/1\">Dafna Ben Bashat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aertsen_M/0/1/0/all/0/1\">Michael Aertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sobotka_D/0/1/0/all/0/1\">Daniel Sobotka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Langs_G/0/1/0/all/0/1\">Georg Langs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alenya_M/0/1/0/all/0/1\">Mireia Aleny&#xe0;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Villanueva_M/0/1/0/all/0/1\">Maria Inmaculada Villanueva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Camara_O/0/1/0/all/0/1\">Oscar Camara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fadida_B/0/1/0/all/0/1\">Bella Specktor Fadida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joskowicz_L/0/1/0/all/0/1\">Leo Joskowicz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weibin_L/0/1/0/all/0/1\">Liao Weibin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yi_L/0/1/0/all/0/1\">Lv Yi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xuesong_L/0/1/0/all/0/1\">Li Xuesong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazher_M/0/1/0/all/0/1\">Moona Mazher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qayyum_A/0/1/0/all/0/1\">Abdul Qayyum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puig_D/0/1/0/all/0/1\">Domenec Puig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kebiri_H/0/1/0/all/0/1\">Hamza Kebiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zelin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xinyi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_D/0/1/0/all/0/1\">Dan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_K/0/1/0/all/0/1\">KuanLun Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">YiXuan Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">JinTai Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yunzhi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1\">Li Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasung_L/0/1/0/all/0/1\">Lana Vasung</a>, et al. (3 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Robust Femur Segmentation from Computed Tomography Images for Patient-Specific Hip Fracture Risk Screening. (arXiv:2204.09575v1 [eess.IV])","link":"http://arxiv.org/abs/2204.09575","description":"<p>Osteoporosis is a common bone disease that increases the risk of bone\nfracture. Hip-fracture risk screening methods based on finite element analysis\ndepend on segmented computed tomography (CT) images; however, current femur\nsegmentation methods require manual delineations of large data sets. Here we\npropose a deep neural network for fully automated, accurate, and fast\nsegmentation of the proximal femur from CT. Evaluation on a set of 1147\nproximal femurs with ground truth segmentations demonstrates that our method is\napt for hip-fracture risk screening, bringing us one step closer to a\nclinically viable option for screening at-risk patients for hip-fracture\nsusceptibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bjornsson_P/0/1/0/all/0/1\">Pall Asgeir Bjornsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baker_A/0/1/0/all/0/1\">Alexander Baker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fleps_I/0/1/0/all/0/1\">Ingmar Fleps</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pauchard_Y/0/1/0/all/0/1\">Yves Pauchard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palsson_H/0/1/0/all/0/1\">Halldor Palsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferguson_S/0/1/0/all/0/1\">Stephen J. Ferguson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sigurdsson_S/0/1/0/all/0/1\">Sigurdur Sigurdsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gudnason_V/0/1/0/all/0/1\">Vilmundur Gudnason</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helgason_B/0/1/0/all/0/1\">Benedikt Helgason</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ellingsen_L/0/1/0/all/0/1\">Lotta Maria Ellingsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical BERT for Medical Document Understanding. (arXiv:2204.09600v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09600","description":"<p>Medical document understanding has gained much attention recently. One\nrepresentative task is the International Classification of Disease (ICD)\ndiagnosis code assignment. Existing work adopts either RNN or CNN as the\nbackbone network because the vanilla BERT cannot handle well long documents\n(&gt;2000 to kens). One issue shared across all these approaches is that they are\nover specific to the ICD code assignment task, losing generality to give the\nwhole document-level and sentence-level embedding. As a result, it is not\nstraight-forward to direct them to other downstream NLU tasks. Motivated by\nthese observations, we propose Medical Document BERT (MDBERT) for long medical\ndocument understanding tasks. MDBERT is not only effective in learning\nrepresentations at different levels of semantics but efficient in encoding long\ndocuments by leveraging a bottom-up hierarchical architecture. Compared to\nvanilla BERT solutions: 1, MDBERT boosts the performance up to relatively 20%\non the MIMIC-III dataset, making it comparable to current SOTA solutions; 2, it\ncuts the computational complexity on self-attention modules to less than 1/100.\nOther than the ICD code assignment, we conduct a variety of other NLU tasks on\na large commercial dataset named as TrialTrove, to showcase MDBERT's strength\nin delivering different levels of semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankowski_M/0/1/0/all/0/1\">Maciej Jankowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assembly Planning from Observations under Physical Constraints. (arXiv:2204.09616v1 [cs.RO])","link":"http://arxiv.org/abs/2204.09616","description":"<p>This paper addresses the problem of copying an unknown assembly of primitives\nwith known shape and appearance using information extracted from a single\nphotograph by an off-the-shelf procedure for object detection and pose\nestimation. The proposed algorithm uses a simple combination of physical\nstability constraints, convex optimization and Monte Carlo tree search to plan\nassemblies as sequences of pick-and-place operations represented by STRIPS\noperators. It is efficient and, most importantly, robust to the errors in\nobject detection and pose estimation unavoidable in any real robotic system.\nThe proposed approach is demonstrated with thorough experiments on a UR5\nmanipulator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chabal_T/0/1/0/all/0/1\">Thomas Chabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strudel_R/0/1/0/all/0/1\">Robin Strudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arlaud_E/0/1/0/all/0/1\">Etienne Arlaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Mixture of Experts. (arXiv:2204.09636v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09636","description":"<p>Mixture of Experts (MoE) is able to scale up vision transformers effectively.\nHowever, it requires prohibiting computation resources to train a large MoE\ntransformer. In this paper, we propose Residual Mixture of Experts (RMoE), an\nefficient training pipeline for MoE vision transformers on downstream tasks,\nsuch as segmentation and detection. RMoE achieves comparable results with the\nupper-bound MoE training, while only introducing minor additional training cost\nthan the lower-bound non-MoE training pipelines. The efficiency is supported by\nour key observation: the weights of an MoE transformer can be factored into an\ninput-independent core and an input-dependent residual. Compared with the\nweight core, the weight residual can be efficiently trained with much less\ncomputation resource, e.g., finetuning on the downstream data. We show that,\ncompared with the current MoE training pipeline, we get comparable results\nwhile saving over 30% training cost. When compared with state-of-the-art non-\nMoE transformers, such as Swin-T / CvT-13 / Swin-L, we get +1.1 / 0.9 / 1.0\nmIoU gain on ADE20K segmentation and +1.4 / 1.6 / 0.6 AP gain on MS-COCO object\ndetection task with less than 3% additional training cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lemeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Class Model for Fabric Defect Detection. (arXiv:2204.09648v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09648","description":"<p>An automated and accurate fabric defect inspection system is in high demand\nas a replacement for slow, inconsistent, error-prone, and expensive human\noperators in the textile industry. Previous efforts focused on certain types of\nfabrics or defects, which is not an ideal solution. In this paper, we propose a\nnovel one-class model that is capable of detecting various defects on different\nfabric types. Our model takes advantage of a well-designed Gabor filter bank to\nanalyze fabric texture. We then leverage an advanced deep learning algorithm,\nautoencoder, to learn general feature representations from the outputs of the\nGabor filter bank. Lastly, we develop a nearest neighbor density estimator to\nlocate potential defects and draw them on the fabric images. We demonstrate the\neffectiveness and robustness of the proposed model by testing it on various\ntypes of fabrics such as plain, patterned, and rotated fabrics. Our model also\nachieves a true positive rate (a.k.a recall) value of 0.895 with no false\nalarms on our dataset based upon the Standard Fabric Defect Glossary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troendle_D/0/1/0/all/0/1\">David Troendle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_B/0/1/0/all/0/1\">Byunghyun Jang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments. (arXiv:2204.09667v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09667","description":"<p>Recent work in Vision-and-Language Navigation (VLN) has presented two\nenvironmental paradigms with differing realism -- the standard VLN setting\nbuilt on topological environments where navigation is abstracted away, and the\nVLN-CE setting where agents must navigate continuous 3D environments using\nlow-level actions. Despite sharing the high-level task and even the underlying\ninstruction-path data, performance on VLN-CE lags behind VLN significantly. In\nthis work, we explore this gap by transferring an agent from the abstract\nenvironment of VLN to the continuous environment of VLN-CE. We find that this\nsim-2-sim transfer is highly effective, improving over the prior state of the\nart in VLN-CE by +12% success rate. While this demonstrates the potential for\nthis direction, the transfer does not fully retain the original performance of\nthe agent in the abstract setting. We present a sequence of experiments to\nidentify what differences result in performance degradation, providing clear\ndirections for further improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Segmentation Networks should be Latency Aware. (arXiv:2004.02574v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.02574","description":"<p>As scene segmentation systems reach visually accurate results, many recent\npapers focus on making these network architectures faster, smaller and more\nefficient. In particular, studies often aim at designingreal-time'systems.\nAchieving this goal is particularly relevant in the context of real-time video\nunderstanding for autonomous vehicles, and robots. In this paper, we argue that\nthe commonly used performance metric of mean Intersection over Union (mIoU)\ndoes not fully capture the information required to estimate the true\nperformance of these networks when they operate inreal-time'. We propose a\nchange of objective in the segmentation task, and its associated metric that\nencapsulates this missing information in the following way: We propose to\npredict the future output segmentation map that will match the future input\nframe at the time when the network finishes the processing. We introduce the\nassociated latency-aware metric, from which we can determine a ranking. We\nperform latency timing experiments of some recent networks on different\nhardware and assess the performances of these networks on our proposed task. We\npropose improvements to scene segmentation networks to better perform on our\ntask by using multi-frames input and increasing capacity in the initial\nconvolutional layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Courdier_E/0/1/0/all/0/1\">Evann Courdier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Francois Fleuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Human Sex Be Learned Using Only 2D Body Keypoint Estimations?. (arXiv:2011.03104v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.03104","description":"<p>In this paper, we analyze human male and female sex recognition problem and\npresent a fully automated classification system using only 2D keypoints. The\nkeypoints represent human joints. A keypoint set consists of 15 joints and the\nkeypoint estimations are obtained using an OpenPose 2D keypoint detector. We\nlearn a deep learning model to distinguish males and females using the\nkeypoints as input and binary labels as output. We use two public datasets in\nthe experimental section - 3DPeople and PETA. On PETA dataset, we report a 77%\naccuracy. We provide model performance details on both PETA and 3DPeople. To\nmeasure the effect of noisy 2D keypoint detections on the performance, we run\nseparate experiments on 3DPeople ground truth and noisy keypoint data. Finally,\nwe extract a set of factors that affect the classification accuracy and propose\nfuture work. The advantage of the approach is that the input is small and the\narchitecture is simple, which enables us to run many experiments and keep the\nreal-time performance in inference. The source code, with the experiments and\ndata preparation scripts, are available on GitHub\n(https://github.com/kristijanbartol/human-sex-classifier).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartol_K/0/1/0/all/0/1\">Kristijan Bartol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pribanic_T/0/1/0/all/0/1\">Tomislav Pribanic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanic_D/0/1/0/all/0/1\">David Bojanic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_T/0/1/0/all/0/1\">Tomislav Petkovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Subsampling of Realistic Images From GANs Conditional on a Class or a Continuous Variable. (arXiv:2103.11166v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11166","description":"<p>Recently, subsampling or refining images generated from unconditional GANs\nhas been actively studied to improve the overall image quality. Unfortunately,\nthese methods are often observed less effective or inefficient in handling\nconditional GANs (cGANs) -- conditioning on a class (aka class-conditional\nGANs) or a continuous variable (aka continuous cGANs or CcGANs). In this work,\nwe introduce an effective and efficient subsampling scheme, named conditional\ndensity ratio-guided rejection sampling (cDR-RS), to sample high-quality images\nfrom cGANs. Specifically, we first develop a novel conditional density ratio\nestimation method, termed cDRE-F-cSP, by proposing the conditional Softplus\n(cSP) loss and an improved feature extraction mechanism. We then derive the\nerror bound of a density ratio model trained with the cSP loss. Finally, we\naccept or reject a fake image in terms of its estimated conditional density\nratio. A filtering scheme is also developed to increase fake images' label\nconsistency without losing diversity when sampling from CcGANs. We extensively\ntest the effectiveness and efficiency of cDR-RS in sampling from both\nclass-conditional GANs and CcGANs on five benchmark datasets. When sampling\nfrom class-conditional GANs, cDR-RS outperforms modern state-of-the-art methods\nby a large margin (except DRE-F-SP+RS) in terms of effectiveness. Although the\neffectiveness of cDR-RS is often comparable to that of DRE-F-SP+RS, cDR-RS is\nsubstantially more efficient. When sampling from CcGANs, the superiority of\ncDR-RS is even more noticeable in terms of both effectiveness and efficiency.\nNotably, with the consumption of reasonable computational resources, cDR-RS can\nsubstantially reduce Label Score without decreasing the diversity of\nCcGAN-generated images, while other methods often need to trade much diversity\nfor slightly improved Label Score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_W/0/1/0/all/0/1\">William J. Welch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards General Purpose Vision Systems. (arXiv:2104.00743v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00743","description":"<p>Computer vision systems today are primarily N-purpose systems, designed and\ntrained for a predefined set of tasks. Adapting such systems to new tasks is\nchallenging and often requires non-trivial modifications to the network\narchitecture (e.g. adding new output heads) or training process (e.g. adding\nnew losses). To reduce the time and expertise required to develop new\napplications, we would like to create general purpose vision systems that can\nlearn and perform a range of tasks without any modification to the architecture\nor learning process.\n</p>\n<p>In this paper, we propose GPV-1, a task-agnostic vision-language architecture\nthat can learn and perform tasks that involve receiving an image and producing\ntext and/or bounding boxes, including classification, localization, visual\nquestion answering, captioning, and more. We also propose evaluations of\ngenerality of architecture, skill-concept transfer, and learning efficiency\nthat may inform future work on general purpose vision. Our experiments indicate\nGPV-1 is effective at multiple tasks, reuses some concept knowledge across\ntasks, can perform the Referring Expressions task zero-shot, and further\nimproves upon the zero-shot performance using a few training samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanmay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Amita Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1\">Aniruddha Kembhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1\">Derek Hoiem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel three-stage training strategy for long-tailed classification. (arXiv:2104.09830v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09830","description":"<p>The long-tailed distribution datasets poses great challenges for deep\nlearning based classification models on how to handle the class imbalance\nproblem. Existing solutions usually involve class-balacing strategies or\ntransfer learing from head- to tail-classes or use two-stages learning strategy\nto re-train the classifier. However, the existing methods are difficult to\nsolve the low quality problem when images are obtained by SAR. To address this\nproblem, we establish a novel three-stages training strategy, which has\nexcellent results for processing SAR image datasets with long-tailed\ndistribution. Specifically, we divide training procedure into three stages. The\nfirst stage is to use all kinds of images for rough-training, so as to get the\nrough-training model with rich content. The second stage is to make the rough\nmodel learn the feature expression by using the residual dataset with the class\n0 removed. The third stage is to fine tune the model using class-balanced\ndatasets with all 10 classes (including the overall model fine tuning and\nclassifier re-optimization). Through this new training strategy, we only use\nthe information of SAR image dataset and the network model with very small\nparameters to achieve the top 1 accuracy of 22.34 in development phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Linpeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment. (arXiv:2105.06224v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06224","description":"<p>Table structure recognition is a challenging task due to the various\nstructures and complicated cell spanning relations. Previous methods handled\nthe problem starting from elements in different granularities (rows/columns,\ntext regions), which somehow fell into the issues like lossy heuristic rules or\nneglect of empty cell division. Based on table structure characteristics, we\nfind that obtaining the aligned bounding boxes of text region can effectively\nmaintain the entire relevant range of different cells. However, the aligned\nbounding boxes are hard to be accurately predicted due to the visual\nambiguities. In this paper, we aim to obtain more reliable aligned bounding\nboxes by fully utilizing the visual information from both text regions in\nproposed local features and cell relations in global features. Specifically, we\npropose the framework of Local and Global Pyramid Mask Alignment, which adopts\nthe soft pyramid mask learning mechanism in both the local and global feature\nmaps. It allows the predicted boundaries of bounding boxes to break through the\nlimitation of original proposals. A pyramid mask re-scoring module is then\nintegrated to compromise the local and global information and refine the\npredicted boundaries. Finally, we propose a robust table structure recovery\npipeline to obtain the final structure, in which we also effectively solve the\nproblems of empty cells locating and division. Experimental results show that\nthe proposed method achieves competitive and even new state-of-the-art\nperformance on several public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robotic Inspection of Underground Utilities for Construction Survey Using a Ground Penetrating Radar. (arXiv:2106.01907v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.01907","description":"<p>Ground Penetrating Radar (GPR) is a very useful non-destructive evaluation\n(NDE) device for locating and mapping underground assets prior to digging and\ntrenching efforts in construction. This paper presents a novel robotic system\nto automate the GPR data collection process, localize the underground\nutilities, interpret and reconstruct the underground objects for better\nvisualization allowing regular non-professional users to understand the survey\nresults. This system is composed of three modules: 1) an Omni-directional\nrobotic data collection platform, that carries an RGB-D camera with an Inertial\nMeasurement Unit (IMU) and a GPR antenna to perform automatic GPR data\ncollection, and tag each GPR measurement with visual positioning information at\nevery sampling step; 2) a learning-based migration module to interpret the raw\nGPR B-scan image into a 2D cross-section model of objects; 3) a 3D\nreconstruction module, i.e., GPRNet, to generate underground utility model\nrepresented as fine 3D point cloud. Comparative studies are performed on\nsynthetic data and field GPR raw data with various incompleteness and noise.\nExperimental results demonstrate that our proposed method achieves a $30.0\\%$\nhigher GPR imaging accuracy in mean Intersection Over Union (IoU) than the\nconventional back projection (BP) migration approach and $6.9\\%$-$7.2\\%$ less\nloss in Chamfer Distance (CD) than baseline methods regarding point cloud model\nreconstruction. The GPR-based robotic inspection provides an effective tool for\ncivil engineers to detect and survey underground utilities before construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1\">Jinglun Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Liang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoxha_E/0/1/0/all/0/1\">Ejup Hoxha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biao_J/0/1/0/all/0/1\">Jiang Biao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jizhong Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15754","description":"<p>Long-range contextual information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). However, image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range contexts in large RSIs. To overcome this limitation, we propose a\nWide-Context Network (WiCoNet) for the semantic segmentation of HR RSIs. Apart\nfrom extracting local features with a conventional CNN, the WiCoNet has an\nextra context branch to aggregate information from a larger image area.\nMoreover, we introduce a Context Transformer to embed contextual information\nfrom the context branch and selectively project it onto the local features. The\nContext Transformer extends the Vision Transformer, an emerging kind of neural\nnetwork, to model the dual-branch semantic correlations. It overcomes the\nlocality limitation of CNNs and enables the WiCoNet to see the bigger picture\nbefore segmenting the land-cover/land-use (LCLU) classes. Ablation studies and\ncomparative experiments conducted on several benchmark datasets demonstrate the\neffectiveness of the proposed method. In addition, we present a new Beijing\nLand-Use (BLU) dataset. This is a large-scale HR satellite dataset with\nhigh-quality and fine-grained reference labels, which can facilitate future\nstudies in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaofu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaojie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuebin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Parametric Wireframe Extraction Based on Distance Fields. (arXiv:2107.06165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06165","description":"<p>We present a pipeline for parametric wireframe extraction from densely\nsampled point clouds. Our approach processes a scalar distance field that\nrepresents proximity to the nearest sharp feature curve. In intermediate\nstages, it detects corners, constructs curve segmentation, and builds a\ntopological graph fitted to the wireframe. As an output, we produce parametric\nspline curves that can be edited and sampled arbitrarily. We evaluate our\nmethod on 50 complex 3D shapes and compare it to the novel deep learning-based\ntechnique, demonstrating superior quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matveev_A/0/1/0/all/0/1\">Albert Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00166","description":"<p>Micro-expressions are spontaneous, unconscious facial movements that show\npeople's true inner emotions and have great potential in related fields of\npsychological testing. Since the face is a 3D deformation object, the\noccurrence of an expression can arouse spatial deformation of the face, but\nlimited by the available databases are 2D videos, lacking the description of 3D\nspatial information of micro-expressions. Therefore, we proposed a new\nmicro-expression database containing 2D video sequences and 3D point clouds\nsequences. The database includes 373 micro-expressions sequences, and these\nsamples were classified using the objective method based on facial action\ncoding system, as well as the non-objective method that combines video contents\nand participants' self-reports. We extracted 2D and 3D features using the local\nbinary patterns on three orthogonal planes (LBP-TOP) and curvature algorithms,\nrespectively, and evaluated the classification accuracies of these two features\nand their fusion results with leave-one-subject-out (LOSO) and 10-fold\ncross-validation. Further, we performed various neural network algorithms for\ndatabase classification, the results show that classification accuracies are\nimproved by fusing 3D features than using only 2D features. The database offers\noriginal and cropped micro-expression samples, which will facilitate the\nexploration and research on 3D Spatio-temporal features of micro-expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1\">Danmin Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I3CL:Intra- and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection. (arXiv:2108.01343v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01343","description":"<p>Existing methods for arbitrary-shaped text detection in natural scenes face\ntwo critical issues, i.e., 1) fracture detections at the gaps in a text\ninstance; and 2) inaccurate detections of arbitrary-shaped text instances with\ndiverse background context. To address these issues, we propose a novel method\nnamed Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to\naddress the first issue, we design an effective convolutional module with\nmultiple receptive fields, which is able to collaboratively learn better\ncharacter and gap feature representations at local and long ranges inside a\ntext instance. To address the second issue, we devise an instance-based\ntransformer module to exploit the dependencies between different text instances\nand a global context module to exploit the semantic context from the shared\nbackground, which are able to collaboratively learn more discriminative text\nfeature representation. In this way, I3CL can effectively exploit the intra-\nand inter-instance dependencies together in a unified end-to-end trainable\nframework. Besides, to make full use of the unlabeled data, we design an\neffective semi-supervised learning method to leverage the pseudo labels via an\nensemble strategy. Without bells and whistles, experimental results show that\nthe proposed I3CL sets new state-of-the-art results on three challenging public\nbenchmarks, i.e., an F-measure of 77.5% on ICDAR2019-ArT, 86.9% on Total-Text,\nand 86.4% on CTW-1500. Notably, our I3CL with the ResNeSt-101 backbone ranked\n1st place on the ICDAR2019-ArT leaderboard. The source code will be available\nat https://github.com/ViTAE-Transformer/ViTAE-Transformer-Scene-Text-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Shape Modelling with Global Coherence: An Inverse Spectral Approach. (arXiv:2108.02161v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02161","description":"<p>Many natural shapes have most of their characterizing features concentrated\nover a few regions in space. For example, humans and animals have distinctive\nhead shapes, while inorganic objects like chairs and airplanes are made of\nwell-localized functional parts with specific geometric features. Often, these\nfeatures are strongly correlated -- a modification of facial traits in a\nquadruped should induce changes to the body structure. However, in shape\nmodelling applications, these types of edits are among the hardest ones; they\nrequire high precision, but also a global awareness of the entire shape. Even\nin the deep learning era, obtaining manipulable representations that satisfy\nsuch requirements is an open problem posing significant constraints. In this\nwork, we address this problem by defining a data-driven model upon a family of\nlinear operators (variants of the mesh Laplacian), whose spectra capture global\nand local geometric properties of the shape at hand. Modifications to these\nspectra are translated to semantically valid deformations of the corresponding\nsurface. By explicitly decoupling the global from the local surface features,\nour pipeline allows to perform local edits while simultaneously maintaining a\nglobal stylistic coherence. We empirically demonstrate how our learning-based\nmodel generalizes to shape representations not seen at training time, and we\nsystematically analyze different choices of local operators over diverse shape\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1\">Marco Pegoraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castellani_U/0/1/0/all/0/1\">Umberto Castellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1\">Riccardo Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1\">Emanuele Rodol&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tikhonov Regularization of Circle-Valued Signals. (arXiv:2108.02602v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2108.02602","description":"<p>It is common to have to process signals or images whose values are cyclic and\ncan be represented as points on the complex circle, like wrapped phases,\nangles, orientations, or color hues. We consider a Tikhonov-type regularization\nmodel to smoothen or interpolate circle-valued signals defined on arbitrary\ngraphs. We propose a convex relaxation of this nonconvex problem as a\nsemidefinite program, and an efficient algorithm to solve it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Condat_L/0/1/0/all/0/1\">Laurent Condat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DARTS for Inverse Problems: a Study on Hyperparameter Sensitivity. (arXiv:2108.05647v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.05647","description":"<p>Differentiable architecture search (DARTS) is a widely researched tool for\nthe discovery of novel architectures, due to its promising results for image\nclassification. The main benefit of DARTS is the effectiveness achieved through\nthe weight-sharing one-shot paradigm, which allows efficient architecture\nsearch. In this work, we investigate DARTS in a systematic case study of\ninverse problems, which allows us to analyze these potential benefits in a\ncontrolled manner. We demonstrate that the success of DARTS can be extended\nfrom image classification to signal reconstruction, in principle. However, our\nexperiments also expose three fundamental difficulties in the evaluation of\nDARTS-based methods in inverse problems: First, the results show a large\nvariance in all test cases. Second, the final performance is highly dependent\non the hyperparameters of the optimizer. And third, the performance of the\nweight-sharing architecture used during training does not reflect the final\nperformance of the found architecture well. Thus, we conclude the necessity to\n1) report the results of any DARTS-based methods from several runs along with\nits underlying performance statistics, 2) show the correlation of the training\nand final architecture performance, and 3) carefully consider if the\ncomputational efficiency of DARTS outweighs the costs of hyperparameter\noptimization and multiple runs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_J/0/1/0/all/0/1\">Jovita Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1\">Margret Keuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining chest X-rays and electronic health record (EHR) data using machine learning to diagnose acute respiratory failure. (arXiv:2108.12530v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.12530","description":"<p>Objective: When patients develop acute respiratory failure, accurately\nidentifying the underlying etiology is essential for determining the best\ntreatment. However, differentiating between common medical diagnoses can be\nchallenging in clinical practice. Machine learning models could improve medical\ndiagnosis by aiding in the diagnostic evaluation of these patients. Materials\nand Methods: Machine learning models were trained to predict the common causes\nof acute respiratory failure (pneumonia, heart failure, and/or COPD). Models\nwere trained using chest radiographs and clinical data from the electronic\nhealth record (EHR) and applied to an internal and external cohort. Results:\nThe internal cohort of 1,618 patients included 508 (31%) with pneumonia, 363\n(22%) with heart failure, and 137 (8%) with COPD based on physician chart\nreview. A model combining chest radiographs and EHR data outperformed models\nbased on each modality alone. Models had similar or better performance compared\nto a randomly selected physician reviewer. For pneumonia, the combined model\narea under the receiver operating characteristic curve (AUROC) was 0.79\n(0.77-0.79), image model AUROC was 0.74 (0.72-0.75), and EHR model AUROC was\n0.74 (0.70-0.76). For heart failure, combined: 0.83 (0.77-0.84), image: 0.80\n(0.71-0.81), and EHR: 0.79 (0.75-0.82). For COPD, combined: AUROC = 0.88\n(0.83-0.91), image: 0.83 (0.77-0.89), and EHR: 0.80 (0.76-0.84). In the\nexternal cohort, performance was consistent for heart failure and increased for\nCOPD, but declined slightly for pneumonia. Conclusions: Machine learning models\ncombining chest radiographs and EHR data can accurately differentiate between\ncommon causes of acute respiratory failure. Further work is needed to determine\nhow these models could act as a diagnostic aid to clinicians in clinical\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jabbour_S/0/1/0/all/0/1\">Sarah Jabbour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David Fouhey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazerooni_E/0/1/0/all/0/1\">Ella Kazerooni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1\">Jenna Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sjoding_M/0/1/0/all/0/1\">Michael W Sjoding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.09960v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09960","description":"<p>In this paper, we propose a novel mutual consistency network (MC-Net+) to\neffectively exploit the unlabeled data for semi-supervised medical image\nsegmentation. The MC-Net+ model is motivated by the observation that deep\nmodels trained with limited annotations are prone to output highly uncertain\nand easily mis-classified predictions in ambiguous regions (e.g., adhesive\nedges or thin branches) for medical image segmentation. Leveraging these\nregion-level challenging samples can make the semi-supervised segmentation\nmodel training more effective. Therefore, our proposed MC-Net+ model consists\nof two new designs. First, the model contains one shared encoder and multiple\nslightly different decoders (i.e., using different up-sampling strategies). The\nstatistical discrepancy of multiple decoders' outputs is computed to denote the\nmodel's uncertainty, which indicates the unlabeled hard regions. Second, we\napply a novel mutual consistency constraint between one decoder's probability\noutput and other decoders' soft pseudo labels. In this way, we minimize the\ndiscrepancy of multiple outputs (i.e., the model uncertainty) during training\nand force the model to generate invariant results in such challenging regions,\naiming at capturing more useful features. We compared the segmentation results\nof our MC-Net+ with five state-of-the-art semi-supervised approaches on three\npublic medical datasets. Extension experiments with two common semi-supervised\nsettings demonstrate the superior performance of our model over other existing\nmethods, which sets a new state of the art for semi-supervised medical image\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yicheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Human Pose Triangulation. (arXiv:2110.00280v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00280","description":"<p>We address the problem of generalizability for multi-view 3D human pose\nestimation. The standard approach is to first detect 2D keypoints in images and\nthen apply triangulation from multiple views. Even though the existing methods\nachieve remarkably accurate 3D pose estimation on public benchmarks, most of\nthem are limited to a single spatial camera arrangement and their number.\nSeveral methods address this limitation but demonstrate significantly degraded\nperformance on novel views. We propose a stochastic framework for human pose\ntriangulation and demonstrate a superior generalization across different camera\narrangements on two public datasets. In addition, we apply the same approach to\nthe fundamental matrix estimation problem, showing that the proposed method can\nsuccessfully apply to other computer vision problems. The stochastic framework\nachieves more than 8.8% improvement on the 3D pose estimation task, compared to\nthe state-of-the-art, and more than 30% improvement for fundamental matrix\nestimation, compared to a standard algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartol_K/0/1/0/all/0/1\">Kristijan Bartol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanic_D/0/1/0/all/0/1\">David Bojani&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_T/0/1/0/all/0/1\">Tomislav Petkovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pribanic_T/0/1/0/all/0/1\">Tomislav Pribani&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus on the Common Good: Group Distributional Robustness Follows. (arXiv:2110.02619v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.02619","description":"<p>We consider the problem of training a classification model with group\nannotated training data. Recent work has established that, if there is\ndistribution shift across different groups, models trained using the standard\nempirical risk minimization (ERM) objective suffer from poor performance on\nminority groups and that group distributionally robust optimization (Group-DRO)\nobjective is a better alternative. The starting point of this paper is the\nobservation that though Group-DRO performs better than ERM on minority groups\nfor some benchmark datasets, there are several other datasets where it performs\nmuch worse than ERM. Inspired by ideas from the closely related problem of\ndomain generalization, this paper proposes a new and simple algorithm that\nexplicitly encourages learning of features that are shared across various\ngroups. The key insight behind our proposed algorithm is that while Group-DRO\nfocuses on groups with worst regularized loss, focusing instead, on groups that\nenable better performance even on other groups, could lead to learning of\nshared/common features, thereby enhancing minority performance beyond what is\nachieved by Group-DRO. Empirically, we show that our proposed algorithm matches\nor achieves better performance compared to strong contemporary baselines\nincluding ERM and Group-DRO on standard benchmarks on both minority groups and\nacross all groups. Theoretically, we show that the proposed algorithm is a\ndescent method and finds first order stationary points of smooth nonconvex\nfunctions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piratla_V/0/1/0/all/0/1\">Vihari Piratla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1\">Praneeth Netrapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1\">Sunita Sarawagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FocusNet: Classifying Better by Focusing on Confusing Classes. (arXiv:2110.07307v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07307","description":"<p>Nowadays, most classification networks use one-hot encoding to represent\ncategorical data because of its simplicity. However, one-hot encoding may\naffect the generalization ability as it neglects inter-class correlations. We\nobserve that, even when a neural network trained with one-hot labels produces\nincorrect predictions, it still pays attention to the target image region and\nreveals which classes confuse the network. Inspired by this observation, we\npropose a confusion-focusing mechanism to address the class-confusion issue.\nOur confusion-focusing mechanism is implemented by a two-branch network\narchitecture. Its baseline branch generates confusing classes, and its FocusNet\nbranch, whose architecture is flexible, discriminates correct labels from these\nconfusing classes. We also introduce a novel focus-picking loss function to\nimprove classification accuracy by encouraging FocusNet to focus on the most\nconfusing classes. The experimental results validate that our FocusNet is\neffective for image classification on common datasets, and that our\nfocus-picking loss function can also benefit the current neural networks in\nimproving their classification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zehua Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hui-Liang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-OOCS: Learning Prostate Segmentation with Inductive Bias. (arXiv:2110.15664v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.15664","description":"<p>Despite the great success of convolutional neural networks (CNN) in 3D\nmedical image segmentation tasks, the methods currently in use are still not\nrobust enough to the different protocols utilized by different scanners, and to\nthe variety of image properties or artefacts they produce. To this end, we\nintroduce OOCS-enhanced networks, a novel architecture inspired by the innate\nnature of visual processing in the vertebrates. With different 3D U-Net\nvariants as the base, we add two 3D residual components to the second encoder\nblocks: on and off center-surround (OOCS). They generalise the ganglion\npathways in the retina to a 3D setting. The use of 2D-OOCS in any standard CNN\nnetwork complements the feedforward framework with sharp edge-detection\ninductive biases. The use of 3D-OOCS also helps 3D U-Nets to scrutinise and\ndelineate anatomical structures present in 3D images with increased accuracy.We\ncompared the state-of-the-art 3D U-Nets with their 3D-OOCS extensions and\nshowed the superior accuracy and robustness of the latter in automatic prostate\nsegmentation from 3D Magnetic Resonance Images (MRIs). For a fair comparison,\nwe trained and tested all the investigated 3D U-Nets with the same pipeline,\nincluding automatic hyperparameter optimisation and data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhandary_S/0/1/0/all/0/1\">Shrajan Bhandary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Babaiee_Z/0/1/0/all/0/1\">Zahra Babaiee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kostyszyn_D/0/1/0/all/0/1\">Dejan Kostyszyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fechter_T/0/1/0/all/0/1\">Tobias Fechter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamboglou_C/0/1/0/all/0/1\">Constantinos Zamboglou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grosu_A/0/1/0/all/0/1\">Anca-Ligia Grosu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auxiliary Loss Reweighting for Image Inpainting. (arXiv:2111.07279v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07279","description":"<p>Image Inpainting is a task that aims to fill in missing regions of corrupted\nimages with plausible contents. Recent inpainting methods have introduced\nperceptual and style losses as auxiliary losses to guide the learning of\ninpainting generators. Perceptual and style losses help improve the perceptual\nquality of inpainted results by supervising deep features of generated regions.\nHowever, two challenges have emerged with the usage of perceptual and style\nlosses: (i) the time-consuming grid search is required to decide weights for\nperceptual and style losses to properly perform, and (ii) loss terms with\ndifferent auxiliary abilities are equally weighted by perceptual and style\nlosses. To meet these two challenges, we propose a novel framework that\nindependently weights auxiliary loss terms and adaptively adjusts their weights\nwithin a single training process, without a time-consuming grid search.\nSpecifically, to release the auxiliary potential of perceptual and style\nlosses, we propose two auxiliary losses, Tunable Perceptual Loss (TPL) and\nTunable Style Loss (TSL) by using different tunable weights to consider the\ncontributions of different loss terms. TPL and TSL are supersets of perceptual\nand style losses and release the auxiliary potential of standard perceptual and\nstyle losses. We further propose the Adaptive Auxiliary Loss (AAL) algorithm,\nwhich efficiently reweights TPL and TSL in a single training process. AAL is\nbased on the principle that the best auxiliary weights would lead to the most\nimprovement in inpainting performance. We conduct experiments on publically\navailable datasets and find that our framework helps current SOTA methods\nachieve better results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1\">Siqi Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Ye Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenli Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinjun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Network Kalman filtering for 3D object tracking from linear array ultrasound data. (arXiv:2111.09631v2 [stat.AP] UPDATED)","link":"http://arxiv.org/abs/2111.09631","description":"<p>Many interventional surgical procedures rely on medical imaging to visualise\nand track instruments. Such imaging methods not only need to be real-time\ncapable, but also provide accurate and robust positional information. In\nultrasound applications, typically only two-dimensional data from a linear\narray are available, and as such obtaining accurate positional estimation in\nthree dimensions is non-trivial. In this work, we first train a neural network,\nusing realistic synthetic training data, to estimate the out-of-plane offset of\nan object with the associated axial aberration in the reconstructed ultrasound\nimage. The obtained estimate is then combined with a Kalman filtering approach\nthat utilises positioning estimates obtained in previous time-frames to improve\nlocalisation robustness and reduce the impact of measurement noise. The\naccuracy of the proposed method is evaluated using simulations, and its\npractical applicability is demonstrated on experimental data obtained using a\nnovel optical ultrasound imaging setup. Accurate and robust positional\ninformation is provided in real-time. Axial and lateral coordinates for\nout-of-plane objects are estimated with a mean error of 0.1mm for simulated\ndata and a mean error of 0.2mm for experimental data. Three-dimensional\nlocalisation is most accurate for elevational distances larger than 1mm, with a\nmaximum distance of 6mm considered for a 25mm aperture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Arjas_A/0/1/0/all/0/1\">Arttu Arjas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alles_E/0/1/0/all/0/1\">Erwin J. Alles</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maneas_E/0/1/0/all/0/1\">Efthymios Maneas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Arridge_S/0/1/0/all/0/1\">Simon Arridge</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Desjardins_A/0/1/0/all/0/1\">Adrien Desjardins</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sillanpaa_M/0/1/0/all/0/1\">Mikko J. Sillanp&#xe4;&#xe4;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hauptmann_A/0/1/0/all/0/1\">Andreas Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RADU: Ray-Aligned Depth Update Convolutions for ToF Data Denoising. (arXiv:2111.15513v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15513","description":"<p>Time-of-Flight (ToF) cameras are subject to high levels of noise and\ndistortions due to Multi-Path-Interference (MPI). While recent research showed\nthat 2D neural networks are able to outperform previous traditional\nState-of-the-Art (SOTA) methods on denoising ToF-Data, little research on\nlearning-based approaches has been done to make direct use of the 3D\ninformation present in depth images. In this paper, we propose an iterative\ndenoising approach operating in 3D space, that is designed to learn on 2.5D\ndata by enabling 3D point convolutions to correct the points' positions along\nthe view direction. As labeled real world data is scarce for this task, we\nfurther train our network with a self-training approach on unlabeled real world\ndata to account for real world statistics. We demonstrate that our method is\nable to outperform SOTA methods on several datasets, including two real world\ndatasets and a new large-scale synthetic data set introduced in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schelling_M/0/1/0/all/0/1\">Michael Schelling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermosilla_P/0/1/0/all/0/1\">Pedro Hermosilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separated Contrastive Learning for Organ-at-Risk and Gross-Tumor-Volume Segmentation with Limited Annotation. (arXiv:2112.02743v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.02743","description":"<p>Automatic delineation of organ-at-risk (OAR) and gross-tumor-volume (GTV) is\nof great significance for radiotherapy planning. However, it is a challenging\ntask to learn powerful representations for accurate delineation under limited\npixel (voxel)-wise annotations. Contrastive learning at pixel-level can\nalleviate the dependency on annotations by learning dense representations from\nunlabeled data. Recent studies in this direction design various contrastive\nlosses on the feature maps, to yield discriminative features for each pixel in\nthe map. However, pixels in the same map inevitably share semantics to be\ncloser than they actually are, which may affect the discrimination of pixels in\nthe same map and lead to the unfair comparison to pixels in other maps. To\naddress these issues, we propose a separated region-level contrastive learning\nscheme, namely SepaReg, the core of which is to separate each image into\nregions and encode each region separately. Specifically, SepaReg comprises two\ncomponents: a structure-aware image separation (SIS) module and an intra- and\ninter-organ distillation (IID) module. The SIS is proposed to operate on the\nimage set to rebuild a region set under the guidance of structural information.\nThe inter-organ representation will be learned from this set via typical\ncontrastive losses cross regions. On the other hand, the IID is proposed to\ntackle the quantity imbalance in the region set as tiny organs may produce\nfewer regions, by exploiting intra-organ representations. We conducted\nextensive experiments to evaluate the proposed model on a public dataset and\ntwo private datasets. The experimental results demonstrate the effectiveness of\nthe proposed model, consistently achieving better performance than\nstate-of-the-art approaches. Code is available at\nhttps://github.com/jcwang123/Separate_CL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiacheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_Y/0/1/0/all/0/1\">Yiming Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qichao_Z/0/1/0/all/0/1\">Zhou Qichao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SyntEO: Synthetic Data Set Generation for Earth Observation and Deep Learning -- Demonstrated for Offshore Wind Farm Detection. (arXiv:2112.02829v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02829","description":"<p>With the emergence of deep learning in the last years, new opportunities\narose in Earth observation research. Nevertheless, they also brought with them\nnew challenges. The data-hungry training processes of deep learning models\ndemand large, resource expensive, annotated data sets and partly replaced\nknowledge-driven approaches so that model behaviour and the final prediction\nprocess became a black box. The proposed SyntEO approach enables Earth\nobservation researchers to automatically generate large deep learning ready\ndata sets by merging existing and procedural data. SyntEO does this by\nincluding expert knowledge in the data generation process in a highly\nstructured manner to control the automatic image and label generation by\nemploying an ontology. In this way, fully controllable experiment environments\nare set up, which support insights in the model training on the synthetic data\nsets. Thus, SyntEO makes the learning process approachable, which is an\nimportant cornerstone for explainable machine learning. We demonstrate the\nSyntEO approach by predicting offshore wind farms in Sentinel-1 images on two\nof the worlds largest offshore wind energy production sites. The largest\ngenerated data set has 90,000 training examples. A basic convolutional neural\nnetwork for object detection, that is only trained on this synthetic data,\nconfidently detects offshore wind farms by minimising false detections in\nchallenging environments. In addition, four sequential data sets are generated,\ndemonstrating how the SyntEO approach can precisely define the data set\nstructure and influence the training process. SyntEO is thus a hybrid approach\nthat creates an interface between expert knowledge and data-driven image\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoeser_T/0/1/0/all/0/1\">Thorsten Hoeser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuenzer_C/0/1/0/all/0/1\">Claudia Kuenzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPTS: Single-Point Text Spotting. (arXiv:2112.07917v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07917","description":"<p>Existing scene text spotting (i.e., end-to-end text detection and\nrecognition) methods rely on costly bounding box annotations (e.g., text-line,\nword-level, or character-level bounding boxes). For the first time, we\ndemonstrate that training scene text spotting models can be achieved with an\nextremely low-cost annotation of a single-point for each instance. We propose\nan end-to-end scene text spotting method that tackles scene text spotting as a\nsequence prediction task. Given an image as input, we formulate the desired\ndetection and recognition results as a sequence of discrete tokens and use an\nauto-regressive Transformer to predict the sequence. The proposed method is\nsimple yet effective, which can achieve state-of-the-art results on widely used\nbenchmarks. Most significantly, we show that the performance is not very\nsensitive to the positions of the point annotation, meaning that it can be much\neasier to be annotated or even be automatically generated than the bounding box\nthat requires precise positions. We believe that such a pioneer attempt\nindicates a significant opportunity for scene text spotting applications of a\nmuch larger scale than previously possible. The code will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting People in their Place: Monocular Regression of 3D People in Depth. (arXiv:2112.08274v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08274","description":"<p>Given an image with multiple people, our goal is to directly regress the pose\nand shape of all the people as well as their relative depth. Inferring the\ndepth of a person in an image, however, is fundamentally ambiguous without\nknowing their height. This is particularly problematic when the scene contains\npeople of very different sizes, e.g. from infants to adults. To solve this, we\nneed several things. First, we develop a novel method to infer the poses and\ndepth of multiple people in a single image. While previous work that estimates\nmultiple people does so by reasoning in the image plane, our method, called\nBEV, adds an additional imaginary Bird's-Eye-View representation to explicitly\nreason about depth. BEV reasons simultaneously about body centers in the image\nand in depth and, by combing these, estimates 3D body position. Unlike prior\nwork, BEV is a single-shot method that is end-to-end differentiable. Second,\nheight varies with age, making it impossible to resolve depth without also\nestimating the age of people in the image. To do so, we exploit a 3D body model\nspace that lets BEV infer shapes from infants to adults. Third, to train BEV,\nwe need a new dataset. Specifically, we create a \"Relative Human\" (RH) dataset\nthat includes age labels and relative depth relationships between the people in\nthe images. Extensive experiments on RH and AGORA demonstrate the effectiveness\nof the model and training scheme. BEV outperforms existing methods on depth\nreasoning, child shape estimation, and robustness to occlusion. The code and\ndataset are released for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qian Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yili Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape from Polarization for Complex Scenes in the Wild. (arXiv:2112.11377v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11377","description":"<p>We present a new data-driven approach with physics-based priors to\nscene-level normal estimation from a single polarization image. Existing shape\nfrom polarization (SfP) works mainly focus on estimating the normal of a single\nobject rather than complex scenes in the wild. A key barrier to high-quality\nscene-level SfP is the lack of real-world SfP data in complex scenes. Hence, we\ncontribute the first real-world scene-level SfP dataset with paired input\npolarization images and ground-truth normal maps. Then we propose a\nlearning-based framework with a multi-head self-attention module and viewing\nencoding, which is designed to handle increasing polarization ambiguities\ncaused by complex materials and non-orthographic projection in scene-level SfP.\nOur trained model can be generalized to far-field outdoor scenes as the\nrelationship between polarized light and surface normals is not affected by\ndistance. Experimental results demonstrate that our approach significantly\noutperforms existing SfP models on two datasets. Our dataset and source code\nwill be publicly available at https://github.com/ChenyangLEI/sfp-wild\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chenyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflash Dropout in Image Super-Resolution. (arXiv:2112.12089v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12089","description":"<p>Dropout is designed to relieve the overfitting problem in high-level vision\ntasks but is rarely applied in low-level vision tasks, like image\nsuper-resolution (SR). As a classic regression problem, SR exhibits a different\nbehaviour as high-level tasks and is sensitive to the dropout operation.\nHowever, in this paper, we show that appropriate usage of dropout benefits SR\nnetworks and improves the generalization ability. Specifically, dropout is\nbetter embedded at the end of the network and is significantly helpful for the\nmulti-degradation settings. This discovery breaks our common sense and inspires\nus to explore its working mechanism. We further use two analysis tools -- one\nis from recent network interpretation works, and the other is specially\ndesigned for this task. The analysis results provide side proofs to our\nexperimental findings and show us a new perspective to understand SR networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangtao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xina Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Video Representation Learning with Cascade Positive Retrieval. (arXiv:2201.07989v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07989","description":"<p>Self-supervised video representation learning has been shown to effectively\nimprove downstream tasks such as video retrieval and action recognition. In\nthis paper, we present the Cascade Positive Retrieval (CPR) that successively\nmines positive examples w.r.t. the query for contrastive learning in a cascade\nof stages. Specifically, CPR exploits multiple views of a query example in\ndifferent modalities, where an alternative view may help find another positive\nexample dissimilar in the query view. We explore the effects of possible CPR\nconfigurations in ablations including the number of mining stages, the top\nsimilar example selection ratio in each stage, and progressive training with an\nincremental number of the final Top-k selection. The overall mining quality is\nmeasured to reflect the recall across training set classes. CPR reaches a\nmedian class mining recall of 83.3%, outperforming previous work by 5.5%.\nImplementation-wise, CPR is complementary to pretext tasks and can be easily\napplied to previous work. In the evaluation of pretraining on UCF101, CPR\nconsistently improves existing work and even achieves state-of-the-art R@1 of\n56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action\nrecognition on UCF101 and HMDB51. The code is available at https://github.\ncom/necla-ml/CPR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-En Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yu Hen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices. (arXiv:2201.12382v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2201.12382","description":"<p>Abstract visual reasoning (AVR) domain encompasses problems solving which\nrequires the ability to reason about relations among entities present in a\ngiven scene. While humans, generally, solve AVR tasks in a \"natural\" way, even\nwithout prior experience, this type of problems has proven difficult for\ncurrent machine learning systems. The paper summarises recent progress in\napplying deep learning methods to solving AVR problems, as a proxy for studying\nmachine intelligence. We focus on the most common type of AVR tasks -- the\nRaven's Progressive Matrices (RPMs) -- and provide a comprehensive review of\nthe learning methods and deep neural models applied to solve RPMs, as well as,\nthe RPM benchmark sets. Performance analysis of the state-of-the-art approaches\nto solving RPMs leads to formulation of certain insights and remarks on the\ncurrent and future trends in this area. We conclude the paper by demonstrating\nhow real-world problems can benefit from the discoveries of RPM studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malkinski_M/0/1/0/all/0/1\">Miko&#x142;aj Ma&#x142;ki&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review of Serial and Parallel Min-Cut/Max-Flow Algorithms for Computer Vision. (arXiv:2202.00418v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00418","description":"<p>Minimum cut/maximum flow (min-cut/max-flow) algorithms solve a variety of\nproblems in computer vision and thus significant effort has been put into\ndeveloping fast min-cut/max-flow algorithms. As a result, it is difficult to\nchoose an ideal algorithm for a given problem. Furthermore, parallel algorithms\nhave not been thoroughly compared. In this paper, we evaluate the\nstate-of-the-art serial and parallel min-cut/max-flow algorithms on the largest\nset of computer vision problems yet. We focus on generic algorithms, i.e., for\nunstructured graphs, but also compare with the specialized GridCut\nimplementation. When applicable, GridCut performs best. Otherwise, the two\npseudoflow algorithms, Hochbaum pseudoflow and excesses incremental breadth\nfirst search, achieves the overall best performance. The most memory efficient\nimplementation tested is the Boykov-Kolmogorov algorithm. Amongst generic\nparallel algorithms, we find the bottom-up merging approach by Liu and Sun to\nbe best, but no method is dominant. Of the generic parallel methods, only the\nparallel preflow push-relabel algorithm is able to efficiently scale with many\nprocessors across problem sizes, and no generic parallel method consistently\noutperforms serial algorithms. Finally, we provide and evaluate strategies for\nalgorithm selection to obtain good expected performance. We make our dataset\nand implementations publicly available for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jensen_P/0/1/0/all/0/1\">Patrick M. Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeppesen_N/0/1/0/all/0/1\">Niels Jeppesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahl_A/0/1/0/all/0/1\">Anders B. Dahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahl_V/0/1/0/all/0/1\">Vedrana A. Dahl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-friendly Synthetic Data for the Development of Face Morphing Attack Detectors. (arXiv:2203.06691v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06691","description":"<p>The main question this work aims at answering is: \"can morphing attack\ndetection (MAD) solutions be successfully developed based on synthetic data?\".\nTowards that, this work introduces the first synthetic-based MAD development\ndataset, namely the Synthetic Morphing Attack Detection Development dataset\n(SMDD). This dataset is utilized successfully to train three MAD backbones\nwhere it proved to lead to high MAD performance, even on completely unknown\nattack types. Additionally, an essential aspect of this work is the detailed\nlegal analyses of the challenges of using and sharing real biometric data,\nrendering our proposed SMDD dataset extremely essential. The SMDD dataset,\nconsisting of 30,000 attack and 50,000 bona fide samples, is publicly available\nfor research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">C&#xe9;sar Augusto Fontanillo L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meiling Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_N/0/1/0/all/0/1\">No&#xe9;mie Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh Vu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13052","description":"<p>Facial expression recognition plays an important role in human-computer\ninteraction. In this paper, we propose the Coarse-to-Fine Cascaded network with\nSmooth Predicting (CFC-SP) to improve the performance of facial expression\nrecognition. CFC-SP contains two core components, namely Coarse-to-Fine\nCascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups\nseveral similar emotions to form a rough category, and then employs a network\nto conduct a coarse but accurate classification. Later, an additional network\nfor these grouped emotions is further used to obtain fine-grained predictions.\nFor SP, it improves the recognition capability of the model by capturing both\nuniversal and unique expression features. To be specific, the universal\nfeatures denote the general characteristic of facial emotions within a period\nand the unique features denote the specific characteristic at this moment.\nExperiments on Aff-Wild2 show the effectiveness of the proposed CFSP. We\nachieved 3rd place in the Expression Classification Challenge of the 3rd\nCompetition on Affective Behavior Analysis in-the-wild. The code will be\nreleased at https://github.com/BR-IDL/PaddleViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fanglei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zichang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhongsong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition. (arXiv:2203.14779v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14779","description":"<p>Multimodal emotion recognition has recently gained much attention since it\ncan leverage diverse and complementary relationships over multiple modalities\n(e.g., audio, visual, biosignals, etc.), and can provide some robustness to\nnoisy modalities. Most state-of-the-art methods for audio-visual (A-V) fusion\nrely on recurrent networks or conventional attention mechanisms that do not\neffectively leverage the complementary nature of A-V modalities. In this paper,\nwe focus on dimensional emotion recognition based on the fusion of facial and\nvocal modalities extracted from videos. Specifically, we propose a joint\ncross-attention model that relies on the complementary relationships to extract\nthe salient features across A-V modalities, allowing for accurate prediction of\ncontinuous values of valence and arousal. The proposed fusion model efficiently\nleverages the inter-modal relationships, while reducing the heterogeneity\nbetween the features. In particular, it computes the cross-attention weights\nbased on correlation between the combined feature representation and individual\nmodalities. By deploying the combined A-V feature representation into the\ncross-attention module, the performance of our fusion module improves\nsignificantly over the vanilla cross-attention module. Experimental results on\nvalidation-set videos from the AffWild2 dataset indicate that our proposed A-V\nfusion model provides a cost-effective solution that can outperform\nstate-of-the-art approaches. The code is available on GitHub:\nhttps://github.com/praveena2j/JointCrossAttentional-AV-Fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajasekar_G/0/1/0/all/0/1\">Gnana Praveen Rajasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_W/0/1/0/all/0/1\">Wheidima Carneiro de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullah_N/0/1/0/all/0/1\">Nasib Ullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslam_H/0/1/0/all/0/1\">Haseeb Aslam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeeshan_O/0/1/0/all/0/1\">Osama Zeeshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denorme_T/0/1/0/all/0/1\">Th&#xe9;o Denorme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1\">Marco Pedersoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro Koerich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacon_S/0/1/0/all/0/1\">Simon Bacon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardinal_P/0/1/0/all/0/1\">Patrick Cardinal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Agnostic Prior for Transfer Semantic Segmentation. (arXiv:2204.02684v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02684","description":"<p>Unsupervised domain adaptation (UDA) is an important topic in the computer\nvision community. The key difficulty lies in defining a common property between\nthe source and target domains so that the source-domain features can align with\nthe target-domain semantics. In this paper, we present a simple and effective\nmechanism that regularizes cross-domain representation learning with a\ndomain-agnostic prior (DAP) that constrains the features extracted from source\nand target domains to align with a domain-agnostic space. In practice, this is\neasily implemented as an extra loss term that requires a little extra costs. In\nthe standard evaluation protocol of transferring synthesized data to real data,\nwe validate the effectiveness of different types of DAP, especially that\nborrowed from a text embedding model that shows favorable performance beyond\nthe state-of-the-art UDA approaches in terms of segmentation accuracy. Our\nresearch reveals that UDA benefits much from better proxies, possibly from\nother data modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_X/0/1/0/all/0/1\">Xinyue Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengtong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Trajectory-Aware Transformer for Video Super-Resolution. (arXiv:2204.04216v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.04216","description":"<p>Video super-resolution (VSR) aims to restore a sequence of high-resolution\n(HR) frames from their low-resolution (LR) counterparts. Although some progress\nhas been made, there are grand challenges to effectively utilize temporal\ndependency in entire video sequences. Existing approaches usually align and\naggregate video frames from limited adjacent frames (e.g., 5 or 7 frames),\nwhich prevents these approaches from satisfactory results. In this paper, we\ntake one step further to enable effective spatio-temporal learning in videos.\nWe propose a novel Trajectory-aware Transformer for Video Super-Resolution\n(TTVSR). In particular, we formulate video frames into several pre-aligned\ntrajectories which consist of continuous visual tokens. For a query token,\nself-attention is only learned on relevant visual tokens along spatio-temporal\ntrajectories. Compared with vanilla vision Transformers, such a design\nsignificantly reduces the computational cost and enables Transformers to model\nlong-range features. We further propose a cross-scale feature tokenization\nmodule to overcome scale-changing problems that often occur in long-range\nvideos. Experimental results demonstrate the superiority of the proposed TTVSR\nover state-of-the-art models, by extensive quantitative and qualitative\nevaluations in four widely-used video super-resolution benchmarks. Both code\nand pre-trained models can be downloaded at\nhttps://github.com/researchmm/TTVSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chengxu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transparent Shape from Single Polarization Images. (arXiv:2204.06331v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06331","description":"<p>This paper presents a data-driven approach for transparent shape from\npolarization. Due to the inherent high transmittance, the previous shape from\npolarization(SfP) methods based on specular reflection model have difficulty in\nestimating transparent shape, and the lack of datasets for transparent SfP also\nlimits the application of the data-driven approach. Hence, we construct the\ntransparent SfP dataset which consists of both synthetic and real-world\ndatasets. To determine the reliability of the physics-based reflection model,\nwe define the physics-based prior confidence by exploiting the inherent fault\nof polarization information, then we propose a multi-branch fusion network to\nembed the confidence. Experimental results show that our approach outperforms\nother SfP methods. Compared with the previous method, the mean and median\nangular error of our approach are reduced from $19.00^\\circ$ and $14.91^\\circ$\nto $16.72^\\circ$ and $13.36^\\circ$, and the accuracy $11.25^\\circ, 22.5^\\circ,\n30^\\circ$ are improved from $38.36\\%, 77.36\\%, 87.48\\%$ to $45.51\\%, 78.86\\%,\n89.98\\%$, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Chongkun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junnan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueqian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HASA: Hybrid Architecture Search with Aggregation Strategy for Echinococcosis Classification and Ovary Segmentation in Ultrasound Images. (arXiv:2204.06697v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06697","description":"<p>Different from handcrafted features, deep neural networks can automatically\nlearn task-specific features from data. Due to this data-driven nature, they\nhave achieved remarkable success in various areas. However, manual design and\nselection of suitable network architectures are time-consuming and require\nsubstantial effort of human experts. To address this problem, researchers have\nproposed neural architecture search (NAS) algorithms which can automatically\ngenerate network architectures but suffer from heavy computational cost and\ninstability if searching from scratch. In this paper, we propose a hybrid NAS\nframework for ultrasound (US) image classification and segmentation. The hybrid\nframework consists of a pre-trained backbone and several searched cells (i.e.,\nnetwork building blocks), which takes advantage of the strengths of both NAS\nand the expert knowledge from existing convolutional neural networks.\nSpecifically, two effective and lightweight operations, a mixed depth-wise\nconvolution operator and a squeeze-and-excitation block, are introduced into\nthe candidate operations to enhance the variety and capacity of the searched\ncells. These two operations not only decrease model parameters but also boost\nnetwork performance. Moreover, we propose a re-aggregation strategy for the\nsearched cells, aiming to further improve the performance for different vision\ntasks. We tested our method on two large US image datasets, including a 9-class\nechinococcosis dataset containing 9566 images for classification and an ovary\ndataset containing 3204 images for segmentation. Ablation experiments and\ncomparison with other handcrafted or automatically searched architectures\ndemonstrate that our method can generate more powerful and lightweight models\nfor the above US image classification and segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jikuan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenhui Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruobing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haining Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos. (arXiv:2204.06918v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06918","description":"<p>Tracking objects in soccer videos is extremely important to gather both\nplayer and team statistics, whether it is to estimate the total distance run,\nthe ball possession or the team formation. Video processing can help automating\nthe extraction of those information, without the need of any invasive sensor,\nhence applicable to any team on any stadium. Yet, the availability of datasets\nto train learnable models and benchmarks to evaluate methods on a common\ntestbed is very limited. In this work, we propose a novel dataset for multiple\nobject tracking composed of 200 sequences of 30s each, representative of\nchallenging soccer scenarios, and a complete 45-minutes half-time for long-term\ntracking. The dataset is fully annotated with bounding boxes and tracklet IDs,\nenabling the training of MOT baselines in the soccer domain and a full\nbenchmarking of those methods on our segregated challenge sets. Our analysis\nshows that multiple player, referee and ball tracking in soccer videos is far\nfrom being solved, with several improvement required in case of fast motion or\nin scenarios of severe occlusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cioppa_A/0/1/0/all/0/1\">Anthony Cioppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deliege_A/0/1/0/all/0/1\">Adrien Deliege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Deep Learning to Identify the Critical 3D Structural Features of the Optic Nerve Head for Glaucoma Diagnosis. (arXiv:2204.06931v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.06931","description":"<p>Purpose: The optic nerve head (ONH) undergoes complex and deep 3D\nmorphological changes during the development and progression of glaucoma.\nOptical coherence tomography (OCT) is the current gold standard to visualize\nand quantify these changes, however the resulting 3D deep-tissue information\nhas not yet been fully exploited for the diagnosis and prognosis of glaucoma.\nTo this end, we aimed: (1) To compare the performance of two relatively recent\ngeometric deep learning techniques in diagnosing glaucoma from a single OCT\nscan of the ONH; and (2) To identify the 3D structural features of the ONH that\nare critical for the diagnosis of glaucoma.\n</p>\n<p>Methods: In this study, we included a total of 2,247 non-glaucoma and 2,259\nglaucoma scans from 1,725 subjects. All subjects had their ONHs imaged in 3D\nwith Spectralis OCT. All OCT scans were automatically segmented using deep\nlearning to identify major neural and connective tissues. Each ONH was then\nrepresented as a 3D point cloud. We used PointNet and dynamic graph\nconvolutional neural network (DGCNN) to diagnose glaucoma from such 3D ONH\npoint clouds and to identify the critical 3D structural features of the ONH for\nglaucoma diagnosis.\n</p>\n<p>Results: Both the DGCNN (AUC: 0.97$\\pm$0.01) and PointNet (AUC:\n0.95$\\pm$0.02) were able to accurately detect glaucoma from 3D ONH point\nclouds. The critical points formed an hourglass pattern with most of them\nlocated in the inferior and superior quadrant of the ONH.\n</p>\n<p>Discussion: The diagnostic accuracy of both geometric deep learning\napproaches was excellent. Moreover, we were able to identify the critical 3D\nstructural features of the ONH for glaucoma diagnosis that tremendously\nimproved the transparency and interpretability of our method. Consequently, our\napproach may have strong potential to be used in clinical applications for the\ndiagnosis and prognosis of a wide range of ophthalmic disorders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Braeu_F/0/1/0/all/0/1\">Fabian A. Braeu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiery_A/0/1/0/all/0/1\">Alexandre H. Thi&#xe9;ry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tun_T/0/1/0/all/0/1\">Tin A. Tun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kadziauskiene_A/0/1/0/all/0/1\">Aiste Kadziauskiene</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barbastathis_G/0/1/0/all/0/1\">George Barbastathis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aung_T/0/1/0/all/0/1\">Tin Aung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Girard_M/0/1/0/all/0/1\">Micha&#xeb;l J.A. Girard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic spinal curvature measurement on ultrasound spine images using Faster R-CNN. (arXiv:2204.07988v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.07988","description":"<p>Ultrasound spine imaging technique has been applied to the assessment of\nspine deformity. However, manual measurements of scoliotic angles on ultrasound\nimages are time-consuming and heavily rely on raters experience. The objectives\nof this study are to construct a fully automatic framework based on Faster\nR-CNN for detecting vertebral lamina and to measure the fitting spinal curves\nfrom the detected lamina pairs. The framework consisted of two closely linked\nmodules: 1) the lamina detector for identifying and locating each lamina pairs\non ultrasound coronal images, and 2) the spinal curvature estimator for\ncalculating the scoliotic angles based on the chain of detected lamina. Two\nhundred ultrasound images obtained from AIS patients were identified and used\nfor the training and evaluation of the proposed method. The experimental\nresults showed the 0.76 AP on the test set, and the Mean Absolute Difference\n(MAD) between automatic and manual measurement which was within the clinical\nacceptance error. Meanwhile the correlation between automatic measurement and\nCobb angle from radiographs was 0.79. The results revealed that our proposed\ntechnique could provide accurate and reliable automatic curvature measurements\non ultrasound spine images for spine deformities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhichao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1\">Liyue Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jing_W/0/1/0/all/0/1\">Wenke Jing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lou_E/0/1/0/all/0/1\">Edmond Lou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_R/0/1/0/all/0/1\">Rui Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning 3D Semantics from Pose-Noisy 2D Images with Hierarchical Full Attention Network. (arXiv:2204.08084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08084","description":"<p>We propose a novel framework to learn 3D point cloud semantics from 2D\nmulti-view image observations containing pose error. On the one hand, directly\nlearning from the massive, unstructured and unordered 3D point cloud is\ncomputationally and algorithmically more difficult than learning from\ncompactly-organized and context-rich 2D RGB images. On the other hand, both\nLiDAR point cloud and RGB images are captured in standard automated-driving\ndatasets. This motivates us to conduct a \"task transfer\" paradigm so that 3D\nsemantic segmentation benefits from aggregating 2D semantic cues, albeit pose\nnoises are contained in 2D image observations. Among all difficulties, pose\nnoise and erroneous prediction from 2D semantic segmentation approaches are the\nmain challenges for the task transfer. To alleviate the influence of those\nfactor, we perceive each 3D point using multi-view images and for each single\nimage a patch observation is associated. Moreover, the semantic labels of a\nblock of neighboring 3D points are predicted simultaneously, enabling us to\nexploit the point structure prior to further improve the performance. A\nhierarchical full attention network~(HiFANet) is designed to sequentially\naggregates patch, bag-of-frames and inter-point semantic cues, with\nhierarchical attention mechanism tailored for different level of semantic cues.\nAlso, each preceding attention block largely reduces the feature size before\nfeeding to the next attention block, making our framework slim. Experiment\nresults on Semantic-KITTI show that the proposed framework outperforms existing\n3D point cloud based methods significantly, it requires much less training data\nand exhibits tolerance to pose noise. The code is available at\nhttps://github.com/yuhanghe01/HiFANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Junkun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Distracted Driving (SynDD1) dataset for analyzing distracted behaviors and various gaze zones of a driver. (arXiv:2204.08096v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08096","description":"<p>This article presents a synthetic distracted driving (SynDD1) dataset for\nmachine learning models to detect and analyze drivers' various distracted\nbehavior and different gaze zones. We collected the data in a stationary\nvehicle using three in-vehicle cameras positioned at locations: on the\ndashboard, near the rearview mirror, and on the top right-side window corner.\nThe dataset contains two activity types: distracted activities, and gaze zones\nfor each participant and each activity type has two sets: without appearance\nblocks and with appearance blocks such as wearing a hat or sunglasses. The\norder and duration of each activity for each participant are random. In\naddition, the dataset contains manual annotations for each activity, having its\nstart and end time annotated. Researchers could use this dataset to evaluate\nthe performance of machine learning algorithms for the classification of\nvarious distracting activities and gaze zones of drivers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammed Shaiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatachalapathy_A/0/1/0/all/0/1\">Archana Venkatachalapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gursoy_S/0/1/0/all/0/1\">Senem Velipasalar Gursoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1\">David Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Optimal Transport for Comparing Histopathology Datasets. (arXiv:2204.08324v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08324","description":"<p>Scarcity of labeled histopathology data limits the applicability of deep\nlearning methods to under-profiled cancer types and labels. Transfer learning\nallows researchers to overcome the limitations of small datasets by\npre-training machine learning models on larger datasets similar to the small\ntarget dataset. However, similarity between datasets is often determined\nheuristically. In this paper, we propose a principled notion of distance\nbetween histopathology datasets based on a hierarchical generalization of\noptimal transport distances. Our method does not require any training, is\nagnostic to model type, and preserves much of the hierarchical structure in\nhistopathology datasets imposed by tiling. We apply our method to H&amp;E stained\nslides from The Cancer Genome Atlas from six different cancer types. We show\nthat our method outperforms a baseline distance in a cancer-type prediction\ntask. Our results also show that our optimal transport distance predicts\ndifficulty of transferability in a tumor vs.normal prediction setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1\">Anna Yeaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_R/0/1/0/all/0/1\">Rahul G. Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mieloszyk_R/0/1/0/all/0/1\">Rebecca Mieloszyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Melis_D/0/1/0/all/0/1\">David Alvarez-Melis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_G/0/1/0/all/0/1\">Grace Huynh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. (arXiv:2204.08680v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08680","description":"<p>Vision transformers have achieved great successes in many computer vision\ntasks. Most methods generate vision tokens by splitting an image into a regular\nand fixed grid and treating each cell as a token. However, not all regions are\nequally important in human-centric vision tasks, e.g., the human body needs a\nfine representation with many tokens, while the image background can be modeled\nby a few tokens. To address this problem, we propose a novel Vision\nTransformer, called Token Clustering Transformer (TCFormer), which merges\ntokens by progressive clustering, where the tokens can be merged from different\nlocations with flexible shapes and sizes. The tokens in TCFormer can not only\nfocus on important areas but also adjust the token shapes to fit the semantic\nconcept and adopt a fine resolution for regions containing critical details,\nwhich is beneficial to capturing detailed information. Extensive experiments\nshow that TCFormer consistently outperforms its counterparts on different\nchallenging human-centric tasks and datasets, including whole-body pose\nestimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is\navailable at https://github.com/ zengwang430521/TCFormer.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning. (arXiv:2204.08770v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08770","description":"<p>Demystifying the interactions among multiple agents from their past\ntrajectories is fundamental to precise and interpretable trajectory prediction.\nHowever, previous works only consider pair-wise interactions with limited\nrelational reasoning. To promote more comprehensive interaction modeling for\nrelational reasoning, we propose GroupNet, a multiscale hypergraph neural\nnetwork, which is novel in terms of both interaction capturing and\nrepresentation learning. From the aspect of interaction capturing, we propose a\ntrainable multiscale hypergraph to capture both pair-wise and group-wise\ninteractions at multiple group sizes. From the aspect of interaction\nrepresentation learning, we propose a three-element format that can be learnt\nend-to-end and explicitly reason some relational factors including the\ninteraction strength and category. We apply GroupNet into both CVAE-based\nprediction system and previous state-of-the-art prediction systems for\npredicting socially plausible trajectories with relational reasoning. To\nvalidate the ability of relational reasoning, we experiment with synthetic\nphysics simulations to reflect the ability to capture group behaviors, reason\ninteraction strength and interaction category. To validate the effectiveness of\nprediction, we conduct extensive experiments on three real-world trajectory\nprediction datasets, including NBA, SDD and ETH-UCY; and we show that with\nGroupNet, the CVAE-based prediction system outperforms state-of-the-art\nmethods. We also show that adding GroupNet will further improve the performance\nof previous state-of-the-art prediction systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zhenyang Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08790","description":"<p>Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets/tasks. However, it remains a challenge to evaluate the\ntransferablity of these foundation models due to the lack of easy-to-use\ntoolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark to\ncompare and evaluate pre-trained language-augmented visual models. Several\nhighlights include: (i) Datasets. As downstream evaluation suites, it consists\nof 20 image classification datasets and 35 object detection datasets, each of\nwhich is augmented with external knowledge. (ii) Toolkit. An automatic\nhyper-parameter tuning toolkit is developed to ensure the fairness in model\nadaption. To leverage the full power of language-augmented visual models, novel\nlanguage-aware initialization methods are proposed to significantly improve the\nadaption performance. (iii) Metrics. A variety of evaluation metrics are used,\nincluding sample-efficiency (zero-shot and few-shot) and parameter-efficiency\n(linear probing and full model fine-tuning). We will release our toolkit and\nevaluation platforms for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Ping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised 3D shape segmentation with multilevel consistency and part substitution. (arXiv:2204.08824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08824","description":"<p>The lack of fine-grained 3D shape segmentation data is the main obstacle to\ndeveloping learning-based 3D segmentation techniques. We propose an effective\nsemi-supervised method for learning 3D segmentations from a few labeled 3D\nshapes and a large amount of unlabeled 3D data. For the unlabeled data, we\npresent a novel multilevel consistency loss to enforce consistency of network\npredictions between perturbed copies of a 3D shape at multiple levels:\npoint-level, part-level, and hierarchical level. For the labeled data, we\ndevelop a simple yet effective part substitution scheme to augment the labeled\n3D shapes with more structural variations to enhance training. Our method has\nbeen extensively validated on the task of 3D object semantic segmentation on\nPartNet and ShapeNetPart, and indoor scene semantic segmentation on ScanNet. It\nexhibits superior performance to existing semi-supervised and unsupervised\npre-training 3D approaches. Our code and trained models are publicly available\nat https://github.com/isunchy/semi_supervised_3d_segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chun-Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao-Xiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng-Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Box Image Recognition and its Improvement with a New Augmentation Technique. (arXiv:2204.08853v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08853","description":"<p>Most methods for automated full-bore rock core image analysis (description,\ncolour, properties distribution, etc.) are based on separate core column\nanalyses. The core is usually imaged in a box because of the significant amount\nof time taken to get an image for each core column. The work presents an\ninnovative method and algorithm for core columns extraction from core boxes.\nThe conditions for core boxes imaging may differ tremendously. Such differences\nare disastrous for machine learning algorithms which need a large dataset\ndescribing all possible data variations. Still, such images have some standard\nfeatures - a box and core. Thus, we can emulate different environments with a\nunique augmentation described in this work. It is called template-like\naugmentation (TLA). The method is described and tested on various environments,\nand results are compared on an algorithm trained on both 'traditional' data and\na mix of traditional and TLA data. The algorithm trained with TLA data provides\nbetter metrics and can detect core on most new images, unlike the algorithm\ntrained on data without TLA. The algorithm for core column extraction\nimplemented in an automated core description system speeds up the core box\nprocessing by a factor of 20.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baraboshkin_E/0/1/0/all/0/1\">E.E. Baraboshkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidov_A/0/1/0/all/0/1\">A.E. Demidov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1\">D.M. Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1\">D.A. Koroteev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Invariant Representation Learning from EEG with Private Encoders. (arXiv:2201.11613v1 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2201.11613","description":"<p>Deep learning based electroencephalography (EEG) signal processing methods\nare known to suffer from poor test-time generalization due to the changes in\ndata distribution. This becomes a more challenging problem when\nprivacy-preserving representation learning is of interest such as in clinical\nsettings. To that end, we propose a multi-source learning architecture where we\nextract domain-invariant representations from dataset-specific private\nencoders. Our model utilizes a maximum-mean-discrepancy (MMD) based domain\nalignment approach to impose domain-invariance for encoded representations,\nwhich outperforms state-of-the-art approaches in EEG-based emotion\nclassification. Furthermore, representations learned in our pipeline preserve\ndomain privacy as dataset-specific private encoding alleviates the need for\nconventional, centralized EEG-based deep neural network training approaches\nwith shared parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bethge_D/0/1/0/all/0/1\">David Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallgarten_P/0/1/0/all/0/1\">Philipp Hallgarten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosse_Puppendahl_T/0/1/0/all/0/1\">Tobias Grosse-Puppendahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kari_M/0/1/0/all/0/1\">Mohamed Kari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1\">Ralf Mikut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_A/0/1/0/all/0/1\">Albrecht Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozdenizci_O/0/1/0/all/0/1\">Ozan &#xd6;zdenizci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}