{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-12-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09153","description":"<p>The lifelong learning paradigm in machine learning is an attractive\nalternative to the more prominent isolated learning scheme not only due to its\nresemblance to biological learning, but also its potential to reduce energy\nwaste by obviating excessive model re-training. A key challenge to this\nparadigm is the phenomenon of catastrophic forgetting. With the increasing\npopularity and success of pre-trained models in machine learning, we pose the\nquestion: What role does pre-training play in lifelong learning, specifically\nwith respect to catastrophic forgetting? We investigate existing methods in the\ncontext of large, pre-trained models and evaluate their performance on a\nvariety of text and image classification tasks, including a large-scale study\nusing a novel dataset of 15 diverse NLP tasks. Across all settings, we observe\nthat generic pre-training implicitly alleviates the effects of catastrophic\nforgetting when learning multiple tasks sequentially compared to randomly\ninitialized models. We then further investigate why pre-training alleviates\nforgetting in this setting. We study this phenomenon by analyzing the loss\nlandscape, finding that pre-trained weights appear to ease forgetting by\nleading to wider minima. Based on this insight, we propose jointly optimizing\nfor current task loss and loss basin sharpness in order to explicitly encourage\nwider basins during sequential fine-tuning. We show that this optimization\napproach leads to performance comparable to the state-of-the-art in\ntask-sequential continual learning across multiple settings, without retaining\na memory that scales in size with the number of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sanket Vaibhav Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_D/0/1/0/all/0/1\">Darshan Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Bounded Context-Free-Grammar via LSTM and the Transformer:Difference and Explanations. (arXiv:2112.09174v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09174","description":"<p>Long Short-Term Memory (LSTM) and Transformers are two popular neural\narchitectures used for natural language processing tasks. Theoretical results\nshow that both are Turing-complete and can represent any context-free language\n(CFL).In practice, it is often observed that Transformer models have better\nrepresentation power than LSTM. But the reason is barely understood. We study\nsuch practical differences between LSTM and Transformer and propose an\nexplanation based on their latent space decomposition patterns. To achieve this\ngoal, we introduce an oracle training paradigm, which forces the decomposition\nof the latent representation of LSTM and the Transformer and supervises with\nthe transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With\nthe forced decomposition, we show that the performance upper bounds of LSTM and\nTransformer in learning CFL are close: both of them can simulate a stack and\nperform stack operation along with state transitions. However, the absence of\nforced decomposition leads to the failure of LSTM models to capture the stack\nand stack operations, while having a marginal impact on the Transformer model.\nLastly, we connect the experiment on the prototypical PDA to a real-world\nparsing task to re-verify the conclusions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hui Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sicun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jishen Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperbolic Disentangled Representation for Fine-Grained Aspect Extraction. (arXiv:2112.09215v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09215","description":"<p>Automatic identification of salient aspects from user reviews is especially\nuseful for opinion analysis. There has been significant progress in utilizing\nweakly supervised approaches, which require only a small set of seed words for\ntraining aspect classifiers. However, there is always room for improvement.\nFirst, no weakly supervised approaches fully utilize latent hierarchies between\nwords. Second, each seed words representation should have different latent\nsemantics and be distinct when it represents a different aspect. In this paper,\nwe propose HDAE, a hyperbolic disentangled aspect extractor in which a\nhyperbolic aspect classifier captures words latent hierarchies, and\naspect-disentangled representation models the distinct latent semantics of each\nseed word. Compared to previous baselines, HDAE achieves average F1 performance\ngains of 18.2% and 24.1% on Amazon product review and restaurant review\ndatasets, respectively. In addition, the em-bedding visualization experience\ndemonstrates that HDAE is a more effective approach to leveraging seed words.\nAn ablation study and a case study further attest to the effectiveness of the\nproposed components\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chang-You Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Ming-Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ku_L/0/1/0/all/0/1\">Lun-Wei Ku</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09231","description":"<p>In this paper, we introduce a novel GNN-based knowledge graph embedding\nmodel, named WGE, to capture entity-focused graph structure and\nrelation-focused graph structure. In particular, given the knowledge graph, WGE\nbuilds a single undirected entity-focused graph that views entities as nodes.\nIn addition, WGE also constructs another single undirected graph from\nrelation-focused constraints, which views entities and relations as nodes. WGE\nthen proposes a new architecture of utilizing two vanilla GNNs directly on\nthese two single graphs to better update vector representations of entities and\nrelations, followed by a weighted score function to return the triple scores.\nExperimental results show that WGE obtains state-of-the-art performances on\nthree new and challenging benchmark datasets CoDEx for knowledge graph\ncompletion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Identifying Semantic Bias in Crowdsourced Natural Language Inference Datasets. (arXiv:2112.09237v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09237","description":"<p>Natural language inference (NLI) is an important task for producing useful\nmodels of human language. Unfortunately large-scale NLI dataset production\nrelies on crowdworkers who are prone to introduce biases in the sentences they\nwrite. In particular, without quality control they produce hypotheses from\nwhich the relational label can be predicted, without the premise, better than\nchance. We introduce a model-driven, unsupervised technique to find \"bias\nclusters\" in a learned embedding space of the hypotheses in NLI datasets, from\nwhich interventions and additional rounds of labeling can be performed to\nameliorate the semantic bias of the hypothesis distribution of a dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logically at the Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09253","description":"<p>This paper describes our participant system for the multi-modal fact\nverification (Factify) challenge at AAAI 2022. Despite the recent advance in\ntext based verification techniques and large pre-trained multimodal models\ncross vision and language, very limited work has been done in applying\nmultimodal techniques to automate fact checking process, particularly\nconsidering the increasing prevalence of claims and fake news about images and\nvideos on social media. In our work, the challenge is treated as multimodal\nentailment task and framed as multi-class classification. Two baseline\napproaches are proposed and explored including an ensemble model (combining two\nuni-modal models) and a multi-modal attention network (modeling the interaction\nbetween image and text pair from claim and evidence document). We conduct\nseveral experiments investigating and benchmarking different SoTA pre-trained\ntransformers and vision models in this work. Our best model is ranked first in\nleaderboard which obtains a weighted average F-measure of 0.77 on both\nvalidation and test set. Exploratory analysis of dataset is also carried out on\nthe Factify data set and uncovers salient patterns and issues (e.g., word\noverlapping, visual entailment correlation, source bias) that motivates our\nhypothesis. Finally, we highlight challenges of the task and multimodal dataset\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_H/0/1/0/all/0/1\">Hella-Franziska Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_S/0/1/0/all/0/1\">Stylianos Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiskovski_D/0/1/0/all/0/1\">David Kiskovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandhakavi_A/0/1/0/all/0/1\">Anil Bandhakavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Link-Intensive Alignment for Incomplete Knowledge Graphs. (arXiv:2112.09266v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09266","description":"<p>Knowledge graph (KG) alignment - the task of recognizing entities referring\nto the same thing in different KGs - is recognized as one of the most important\noperations in the field of KG construction and completion. However, existing\nalignment techniques often assume that the input KGs are complete and\nisomorphic, which is not true due to the real-world heterogeneity in the\ndomain, size, and sparsity. In this work, we address the problem of aligning\nincomplete KGs with representation learning. Our KG embedding framework\nexploits two feature channels: transitivity-based and proximity-based. The\nformer captures the consistency constraints between entities via translation\npaths, while the latter captures the neighbourhood structure of KGs via\nattention guided relation-aware graph neural network. The two feature channels\nare jointly learned to exchange important features between the input KGs while\nenforcing the output representations of the input KGs in the same embedding\nspace. Also, we develop a missing links detector that discovers and recovers\nthe missing links in the input KGs during the training process, which helps\nmitigate the incompleteness issue and thus improve the compatibility of the\nlearned representations. The embeddings then are fused to generate the\nalignment result, and the high-confidence matched node pairs are updated to the\npre-aligned supervision data to improve the embeddings gradually. Empirical\nresults show that our model is up to 15.2\\% more accurate than the SOTA and is\nrobust against different levels of incompleteness. We also demonstrate that the\nknowledge exchanging between the KGs helps reveal the unseen facts from\nknowledge graphs (a.k.a. knowledge completion), with the result being 3.5\\%\nhigher than the SOTA knowledge graph completion techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Van Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Thanh Trung Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Tam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1\">Quoc Viet Hung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_Q/0/1/0/all/0/1\">Quyet Thang Huynh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Architectures for Biological Inter-Sentence Relation Extraction. (arXiv:2112.09288v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09288","description":"<p>We introduce a family of deep-learning architectures for inter-sentence\nrelation extraction, i.e., relations where the participants are not necessarily\nin the same sentence. We apply these architectures to an important use case in\nthe biomedical domain: assigning biological context to biochemical events. In\nthis work, biological context is defined as the type of biological system\nwithin which the biochemical event is observed. The neural architectures encode\nand aggregate multiple occurrences of the same candidate context mentions to\ndetermine whether it is the correct context for a particular event mention. We\npropose two broad types of architectures: the first type aggregates multiple\ninstances that correspond to the same candidate context with respect to event\nmention before emitting a classification; the second type independently\nclassifies each instance and uses the results to vote for the final class, akin\nto an ensemble approach. Our experiments show that the proposed neural\nclassifiers are competitive and some achieve better performance than previous\nstate of the art traditional machine learning methods without the need for\nfeature engineering. Our analysis shows that the neural methods particularly\nimprove precision compared to traditional machine learning classifiers and also\ndemonstrates how the difficulty of inter-sentence relation extraction increases\nas the distance between the event and context mentions increase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noriega_Atala_E/0/1/0/all/0/1\">Enrique Noriega-Atala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovett_P/0/1/0/all/0/1\">Peter M. Lovett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_C/0/1/0/all/0/1\">Clayton T. Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the HASOC Subtrack at FIRE 2021: Hate Speech and Offensive Content Identification in English and Indo-Aryan Languages. (arXiv:2112.09301v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09301","description":"<p>The widespread of offensive content online such as hate speech poses a\ngrowing societal problem. AI tools are necessary for supporting the moderation\nprocess at online platforms. For the evaluation of these identification tools,\ncontinuous experimentation with data sets in different languages are necessary.\nThe HASOC track (Hate Speech and Offensive Content Identification) is dedicated\nto develop benchmark data for this purpose. This paper presents the HASOC\nsubtrack for English, Hindi, and Marathi. The data set was assembled from\nTwitter. This subtrack has two sub-tasks. Task A is a binary classification\nproblem (Hate and Not Offensive) offered for all three languages. Task B is a\nfine-grained classification problem for three classes (HATE) Hate speech,\nOFFENSIVE and PROFANITY offered for English and Hindi. Overall, 652 runs were\nsubmitted by 65 teams. The performance of the best classification algorithms\nfor task A are F1 measures 0.91, 0.78 and 0.83 for Marathi, Hindi and English,\nrespectively. This overview presents the tasks and the data development as well\nas the detailed results. The systems submitted to the competition applied a\nvariety of technologies. The best performing algorithms were mainly variants of\ntransformer architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandl_T/0/1/0/all/0/1\">Thomas Mandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modha_S/0/1/0/all/0/1\">Sandip Modha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhu_H/0/1/0/all/0/1\">Hiren Madhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satapara_S/0/1/0/all/0/1\">Shrey Satapara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_J/0/1/0/all/0/1\">Johannes Schaefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandini_D/0/1/0/all/0/1\">Durgesh Nandini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Amit Kumar Jaiswal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebGPT: Browser-assisted question-answering with human feedback. (arXiv:2112.09332v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09332","description":"<p>We fine-tune GPT-3 to answer long-form questions using a text-based\nweb-browsing environment, which allows the model to search and navigate the\nweb. By setting up the task so that it can be performed by humans, we are able\nto train models on the task using imitation learning, and then optimize answer\nquality with human feedback. To make human evaluation of factual accuracy\neasier, models must collect references while browsing in support of their\nanswers. We train and evaluate our models on ELI5, a dataset of questions asked\nby Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior\ncloning, and then performing rejection sampling against a reward model trained\nto predict human preferences. This model's answers are preferred by humans 56%\nof the time to those of our human demonstrators, and 69% of the time to the\nhighest-voted answer from Reddit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakano_R/0/1/0/all/0/1\">Reiichiro Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1\">Suchir Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Christina Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesse_C/0/1/0/all/0/1\">Christopher Hesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shantanu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosaraju_V/0/1/0/all/0/1\">Vineet Kosaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobbe_K/0/1/0/all/0/1\">Karl Cobbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eloundou_T/0/1/0/all/0/1\">Tyna Eloundou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Button_K/0/1/0/all/0/1\">Kevin Button</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1\">Matthew Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chess_B/0/1/0/all/0/1\">Benjamin Chess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KGBoost: A Classification-based Knowledge Base Completion Method with Negative Sampling. (arXiv:2112.09340v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09340","description":"<p>Knowledge base completion is formulated as a binary classification problem in\nthis work, where an XGBoost binary classifier is trained for each relation\nusing relevant links in knowledge graphs (KGs). The new method, named KGBoost,\nadopts a modularized design and attempts to find hard negative samples so as to\ntrain a powerful classifier for missing link prediction. We conduct experiments\non multiple benchmark datasets, and demonstrate that KGBoost outperforms\nstate-of-the-art methods across most datasets. Furthermore, as compared with\nmodels trained by end-to-end optimization, KGBoost works well under the\nlow-dimensional setting so as to allow a smaller model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yun-Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1\">Xiou Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expedition: A System for the Unsupervised Learning of a Hierarchy of Concepts. (arXiv:2112.09348v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09348","description":"<p>We present a system for bottom-up cumulative learning of myriad concepts\ncorresponding to meaningful character strings, and their part-related and\nprediction edges. The learning is self-supervised in that the concepts\ndiscovered are used as predictors as well as targets of prediction. We devise\nan objective for segmenting with the learned concepts, derived from comparing\nto a baseline prediction system, that promotes making and using larger\nconcepts, which in turn allows for predicting larger spans of text, and we\ndescribe a simple technique to promote exploration, i.e. trying out newly\ngenerated concepts in the segmentation process. We motivate and explain a\nlayering of the concepts, to help separate the (conditional) distributions\nlearnt among concepts. The layering of the concepts roughly corresponds to a\npart-whole concept hierarchy. With rudimentary segmentation and learning\nalgorithms, the system is promising in that it acquires many concepts (tens of\nthousands in our small-scale experiments), and it learns to segment text well:\nwhen fed with English text with spaces removed, starting at the character\nlevel, much of what is learned respects word or phrase boundaries, and over\ntime the average number of \"bad\" splits within segmentations, i.e. splits\ninside words, decreases as larger concepts are discovered and the system learns\nwhen to use them during segmentation. We report on promising experiments when\nthe input text is converted to binary and the system begins with only two\nconcepts, \"0\" and \"1\". The system is transparent, in the sense that it is easy\nto tell what the concepts learned correspond to, and which ones are active in a\nsegmentation, or how the system \"sees\" its input. We expect this framework to\nbe extensible and we discuss the current limitations and a number of directions\nfor enhancing the learning and inference capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madani_O/0/1/0/all/0/1\">Omid Madani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Monolingual End-to-End Automatic Speech Recognition. (arXiv:2112.09427v1 [eess.AS])","link":"http://arxiv.org/abs/2112.09427","description":"<p>Adapting Automatic Speech Recognition (ASR) models to new domains leads to a\ndeterioration of performance on the original domain(s), a phenomenon called\nCatastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to\nnew accents, dialects, topics, etc. without suffering from CF, making them\nunable to be continually enhanced without storing all past data. Fortunately,\nContinual Learning (CL) methods, which aim to enable continual adaptation while\novercoming CF, can be used. In this paper, we implement an extensive number of\nCL methods for End-to-End ASR and test and compare their ability to extend a\nmonolingual Hybrid CTC-Transformer model across four new tasks. We find that\nthe best performing CL method closes the gap between the fine-tuned model\n(lower bound) and the model trained jointly on all tasks (upper bound) by more\nthan 40%, while requiring access to only 0.6% of the original data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eeckt_S/0/1/0/all/0/1\">Steven Vander Eeckt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+hamme_H/0/1/0/all/0/1\">Hugo Van hamme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal Approach for Automatic Mania Assessment in Bipolar Disorder. (arXiv:2112.09467v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09467","description":"<p>Bipolar disorder is a mental health disorder that causes mood swings that\nrange from depression to mania. Diagnosis of bipolar disorder is usually done\nbased on patient interviews, and reports obtained from the caregivers of the\npatients. Subsequently, the diagnosis depends on the experience of the expert,\nand it is possible to have confusions of the disorder with other mental\ndisorders. Automated processes in the diagnosis of bipolar disorder can help\nproviding quantitative indicators, and allow easier observations of the\npatients for longer periods. Furthermore, the need for remote treatment and\ndiagnosis became especially important during the COVID-19 pandemic. In this\nthesis, we create a multimodal decision system based on recordings of the\npatient in acoustic, linguistic, and visual modalities. The system is trained\non the Bipolar Disorder corpus. Comprehensive analysis of unimodal and\nmultimodal systems, as well as various fusion techniques are performed. Besides\nprocessing entire patient sessions using unimodal features, a task-level\ninvestigation of the clips is studied. Using acoustic, linguistic, and visual\nfeatures in a multimodal fusion system, we achieved a 64.8% unweighted average\nrecall score, which improves the state-of-the-art performance achieved on this\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baki_P/0/1/0/all/0/1\">P&#x131;nar Baki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-stage Span Labeling. (arXiv:2112.09488v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09488","description":"<p>Chinese word segmentation and part-of-speech tagging are necessary tasks in\nterms of computational linguistics and application of natural language\nprocessing. Many re-searchers still debate the demand for Chinese word\nsegmentation and part-of-speech tagging in the deep learning era. Nevertheless,\nresolving ambiguities and detecting unknown words are challenging problems in\nthis field. Previous studies on joint Chinese word segmentation and\npart-of-speech tagging mainly follow the character-based tagging model focusing\non modeling n-gram features. Unlike previous works, we propose a neural model\nnamed SpanSegTag for joint Chinese word segmentation and part-of-speech tagging\nfollowing the span labeling in which the probability of each n-gram being the\nword and the part-of-speech tag is the main problem. We use the biaffine\noperation over the left and right boundary representations of consecutive\ncharacters to model the n-grams. Our experiments show that our BERT-based model\nSpanSegTag achieved competitive performances on the CTB5, CTB6, and UD, or\nsignificant improvements on CTB7 and CTB9 benchmark datasets compared with the\ncurrent state-of-the-art method using BERT or ZEN encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_L/0/1/0/all/0/1\">Linh-Bao Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Ngoc-Linh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenge Dataset of Cognates and False Friend Pairs from Indian Languages. (arXiv:2112.09526v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09526","description":"<p>Cognates are present in multiple variants of the same text across different\nlanguages (e.g., \"hund\" in German and \"hound\" in English language mean \"dog\").\nThey pose a challenge to various Natural Language Processing (NLP) applications\nsuch as Machine Translation, Cross-lingual Sense Disambiguation, Computational\nPhylogenetics, and Information Retrieval. A possible solution to address this\nchallenge is to identify cognates across language pairs. In this paper, we\ndescribe the creation of two cognate datasets for twelve Indian languages,\nnamely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu,\nPunjabi, Bengali, Marathi, and Malayalam. We digitize the cognate data from an\nIndian language cognate dictionary and utilize linked Indian language Wordnets\nto generate cognate sets. Additionally, we use the Wordnet data to create a\nFalse Friends' dataset for eleven language pairs. We also evaluate the efficacy\nof our dataset using previously available baseline cognate detection\napproaches. We also perform a manual evaluation with the help of lexicographers\nand release the curated gold-standard dataset with this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_M/0/1/0/all/0/1\">Malhar Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Aware Encoding for Extractive Summarization. (arXiv:2112.09572v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09572","description":"<p>Document summarization provides an instrument for faster understanding the\ncollection of text documents and has several real-life applications. With the\ngrowth of online text data, numerous summarization models have been proposed\nrecently. The Sequence-to-Sequence (Seq2Seq) based neural summarization model\nis the most widely used in the summarization field due to its high performance.\nThis is because semantic information and structure information in the text is\nadequately considered when encoding. However, the existing extractive\nsummarization models pay little attention to and use the central topic\ninformation to assist the generation of summaries, which leads to models not\nensuring the generated summary under the primary topic. A lengthy document can\nspan several topics, and a single summary cannot do justice to all the topics.\nTherefore, the key to generating a high-quality summary is determining the\ncentral topic and building a summary based on it, especially for a long\ndocument. We propose a topic-aware encoding for document summarization to deal\nwith this issue. This model effectively combines syntactic-level and\ntopic-level information to build a comprehensive sentence representation.\nSpecifically, a neural topic model is added in the neural-based sentence-level\nrepresentation learning to adequately consider the central topic information\nfor capturing the critical content in the original document. The experimental\nresults on three public datasets show that our model outperforms the\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcribing Natural Languages for The Deaf via Neural Editing Programs. (arXiv:2112.09600v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09600","description":"<p>This work studies the task of glossification, of which the aim is to em\ntranscribe natural spoken language sentences for the Deaf (hard-of-hearing)\ncommunity to ordered sign language glosses. Previous sequence-to-sequence\nlanguage models trained with paired sentence-gloss data often fail to capture\nthe rich connections between the two distinct languages, leading to\nunsatisfactory transcriptions. We observe that despite different grammars,\nglosses effectively simplify sentences for the ease of deaf communication,\nwhile sharing a large portion of vocabulary with sentences. This has motivated\nus to implement glossification by executing a collection of editing actions,\ne.g. word addition, deletion, and copying, called editing programs, on their\nnatural spoken language counterparts. Specifically, we design a new neural\nagent that learns to synthesize and execute editing programs, conditioned on\nsentence contexts and partial editing results. The agent is trained to imitate\nminimal editing programs, while exploring more widely the program space via\npolicy gradients to optimize sequence-wise transcription quality. Results show\nthat our approach outperforms previous glossification models by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsifying Sparse Representations for Passage Retrieval by Top-$k$ Masking. (arXiv:2112.09628v1 [cs.IR])","link":"http://arxiv.org/abs/2112.09628","description":"<p>Sparse lexical representation learning has demonstrated much progress in\nimproving passage retrieval effectiveness in recent models such as DeepImpact,\nuniCOIL, and SPLADE. This paper describes a straightforward yet effective\napproach for sparsifying lexical representations for passage retrieval,\nbuilding on SPLADE by introducing a top-$k$ masking scheme to control sparsity\nand a self-learning method to coax masked representations to mimic unmasked\nrepresentations. A basic implementation of our model is competitive with more\nsophisticated approaches and achieves a good balance between effectiveness and\nefficiency. The simplicity of our methods opens the door for future\nexplorations in lexical representation learning for passage retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jheng-Hong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sublinear Time Approximation of Text Similarity Matrices. (arXiv:2112.09631v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09631","description":"<p>We study algorithms for approximating pairwise similarity matrices that arise\nin natural language processing. Generally, computing a similarity matrix for\n$n$ data points requires $\\Omega(n^2)$ similarity computations. This quadratic\nscaling is a significant bottleneck, especially when similarities are computed\nvia expensive functions, e.g., via transformer models. Approximation methods\nreduce this quadratic complexity, often by using a small subset of exactly\ncomputed similarities to approximate the remainder of the complete pairwise\nsimilarity matrix.\n</p>\n<p>Significant work focuses on the efficient approximation of positive\nsemidefinite (PSD) similarity matrices, which arise e.g., in kernel methods.\nHowever, much less is understood about indefinite (non-PSD) similarity\nmatrices, which often arise in NLP. Motivated by the observation that many of\nthese matrices are still somewhat close to PSD, we introduce a generalization\nof the popular Nystr\\\"{o}m method to the indefinite setting. Our algorithm can\nbe applied to any similarity matrix and runs in sublinear time in the size of\nthe matrix, producing a rank-$s$ approximation with just $O(ns)$ similarity\ncomputations.\n</p>\n<p>We show that our method, along with a simple variant of CUR decomposition,\nperforms very well in approximating a variety of similarity matrices arising in\nNLP tasks. We demonstrate high accuracy of the approximated similarity matrices\nin the downstream tasks of document classification, sentence similarity, and\ncross-document coreference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Archan Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering. (arXiv:2112.09658v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09658","description":"<p>Recent years have witnessed impressive advances in challenging multi-hop QA\ntasks. However, these QA models may fail when faced with some disturbance in\nthe input text and their interpretability for conducting multi-hop reasoning\nremains uncertain. Previous adversarial attack works usually edit the whole\nquestion sentence, which has limited effect on testing the entity-based\nmulti-hop inference ability. In this paper, we propose a multi-hop reasoning\nchain based adversarial attack method. We formulate the multi-hop reasoning\nchains starting from the query entity to the answer entity in the constructed\ngraph, which allows us to align the question to each reasoning hop and thus\nattack any hop. We categorize the questions into different reasoning types and\nadversarially modify part of the question corresponding to the selected\nreasoning hop to generate the distracting sentence. We test our adversarial\nscheme on three QA models on HotpotQA dataset. The results demonstrate\nsignificant performance reduction on both answer and supporting facts\nprediction, verifying the effectiveness of our reasoning chain based attack\nmethod for multi-hop reasoning models and the vulnerability of them. Our\nadversarial re-training further improves the performance and robustness of\nthese models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiayu Ding</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a> (1) ((1) Fudan University, (2) East China Normal University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations. (arXiv:2112.09669v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09669","description":"<p>In attempts to \"explain\" predictions of machine learning models, researchers\nhave proposed hundreds of techniques for attributing predictions to features\nthat are deemed important. While these attributions are often claimed to hold\nthe potential to improve human \"understanding\" of the models, surprisingly\nlittle work explicitly evaluates progress towards this aspiration. In this\npaper, we conduct a crowdsourcing study, where participants interact with\ndeception detection models that have been trained to distinguish between\ngenuine and fake hotel reviews. They are challenged both to simulate the model\non fresh reviews, and to edit reviews with the goal of lowering the probability\nof the originally predicted class. Successful manipulations would lead to an\nadversarial example. During the training (but not the test) phase, input spans\nare highlighted to communicate salience. Through our evaluation, we observe\nthat for a linear bag-of-words model, participants with access to the feature\ncoefficients during training are able to cause a larger reduction in model\nconfidence in the testing phase when compared to the no-explanation control.\nFor the BERT-based classifier, popular local explanations do not improve their\nability to reduce the model confidence over the no-explanation case.\nRemarkably, when the explanation for the BERT model is given by the (global)\nattributions of a linear model trained to imitate the BERT model, people can\neffectively manipulate the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeh_N/0/1/0/all/0/1\">Norman Sadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Suicidal Self-Injury Online Posts: Implications for Mental Health Professionals. (arXiv:1902.06689v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/1902.06689","description":"<p>While non-suicidal self-injury (NSSI) is not a new phenomenon, there is still\na limited yet little is still known about understanding of the behavior, the\nintent behind the behavior and what the individuals themselves say about their\nbehavior. This study collected pro-NSSI public blog posts from Reddit on\npro-NSSI and analyzed the content linguistically using LIWC software, in order\nto examine the use of NSSI specific words, linguistic properties and the\npsychological linguistic properties. were examined. The results inform current\ncounseling practices by dispelling myths and providing insight into the inner\nworld of people who engage in use NSSII to cope. The most frequently appearing\ncategory of For NSSI specific words categories, in the Reddit blogs was the\nreasons in which one engagesfor engaging in NSSI was the most frequently used\nin the Reddit blogs. The linguistic properties found in the analysis reflected\nthe predicted results; authors of pro-NSSI posts used demonstrated expected\nresults of first-person singular pronouns extensively, which indicatesing high\nlevels of mental health distress and isolation. The psychological linguistic\nproperties that could be observed of in these public Reddit posts were\ndominantly in a negative emotional tone which demonstrates youth and\nimpulsivity. The linguistic properties found when these posts were analyzed\nsupports the work of earlier studies that dispelled common myths about NSSI\nthat were circulating in the mental health community. These findings suggest\nthat the language of people who engage in NSSI supports research findings in\ndispelling common myths about NSSI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Greaves_M/0/1/0/all/0/1\">Mandy M. Greaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dykeman_C/0/1/0/all/0/1\">Cass Dykeman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Language Model with Entanglement Embedding for Question Answering. (arXiv:2008.09943v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.09943","description":"<p>Quantum Language Models (QLMs) in which words are modelled as quantum\nsuperposition of sememes have demonstrated a high level of model transparency\nand good post-hoc interpretability. Nevertheless, in the current literature\nword sequences are basically modelled as a classical mixture of word states,\nwhich cannot fully exploit the potential of a quantum probabilistic\ndescription. A full quantum model is yet to be developed to explicitly capture\nthe non-classical correlations within the word sequences. We propose a neural\nnetwork model with a novel Entanglement Embedding (EE) module, whose function\nis to transform the word sequences into entangled pure states of many-body\nquantum systems. Strong quantum entanglement, which is the central concept of\nquantum information and an indication of parallelized correlations among the\nwords, is observed within the word sequences. Numerical experiments show that\nthe proposed QLM with EE (QLM-EE) achieves superior performance compared with\nthe classical deep neural network models and other QLMs on Question Answering\n(QA) datasets. In addition, the post-hoc interpretability of the model can be\nimproved by quantizing the degree of entanglement among the words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Daoyi Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce. (arXiv:2009.02835v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.02835","description":"<p>Pre-trained language models such as BERT have achieved great success in a\nbroad range of natural language processing tasks. However, BERT cannot well\nsupport E-commerce related tasks due to the lack of two levels of domain\nknowledge, i.e., phrase-level and product-level. On one hand, many E-commerce\ntasks require an accurate understanding of domain phrases, whereas such\nfine-grained phrase-level knowledge is not explicitly modeled by BERT's\ntraining objective. On the other hand, product-level knowledge like product\nassociations can enhance the language modeling of E-commerce, but they are not\nfactual knowledge thus using them indiscriminately may introduce noise. To\ntackle the problem, we propose a unified pre-training framework, namely,\nE-BERT. Specifically, to preserve phrase-level knowledge, we introduce Adaptive\nHybrid Masking, which allows the model to adaptively switch from learning\npreliminary word knowledge to learning complex phrases, based on the fitting\nprogress of two modes. To utilize product-level knowledge, we introduce\nNeighbor Product Reconstruction, which trains E-BERT to predict a product's\nassociated neighbors with a denoising cross attention layer. Our investigation\nreveals promising results in four downstream tasks, i.e., review-based question\nanswering, aspect extraction, aspect sentiment classification, and product\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Denghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zixuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperText: Endowing FastText with Hyperbolic Geometry. (arXiv:2010.16143v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.16143","description":"<p>Natural language data exhibit tree-like hierarchical structures such as the\nhypernym-hyponym relations in WordNet. FastText, as the state-of-the-art text\nclassifier based on shallow neural network in Euclidean space, may not model\nsuch hierarchies precisely with limited representation capacity. Considering\nthat hyperbolic space is naturally suitable for modeling tree-like hierarchical\ndata, we propose a new model named HyperText for efficient text classification\nby endowing FastText with hyperbolic geometry. Empirically, we show that\nHyperText outperforms FastText on a range of text classification tasks with\nmuch reduced parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yudong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Di Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Text Style Transfer: A Survey. (arXiv:2011.00416v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.00416","description":"<p>Text style transfer is an important task in natural language generation,\nwhich aims to control certain attributes in the generated text, such as\npoliteness, emotion, humor, and many others. It has a long history in the field\nof natural language processing, and recently has re-gained significant\nattention thanks to the promising performance brought by deep neural models. In\nthis paper, we present a systematic survey of the research on neural text style\ntransfer, spanning over 100 representative articles since the first neural text\nstyle transfer work in 2017. We discuss the task formulation, existing datasets\nand subtasks, evaluation, as well as the rich methodologies in the presence of\nparallel and non-parallel data. We also provide discussions on a variety of\nimportant topics regarding the future development of this task. Our curated\npaper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1\">Olga Vechtomova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Explanations: How much do explanations from the teacher aid students?. (arXiv:2012.00893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.00893","description":"<p>While many methods purport to explain predictions by highlighting salient\nfeatures, what aims these explanations serve and how they ought to be evaluated\noften go unstated. In this work, we introduce a framework to quantify the value\nof explanations via the accuracy gains that they confer on a student model\ntrained to simulate a teacher model. Crucially, the explanations are available\nto the student during training, but are not available at test time. Compared to\nprior proposals, our approach is less easily gamed, enabling principled,\nautomatic, model-agnostic evaluation of attributions. Using our framework, we\ncompare numerous attribution methods for text classification and question\nanswering, and observe quantitative differences that are consistent (to a\nmoderate to high degree) across different student model architectures and\nlearning strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rachit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_L/0/1/0/all/0/1\">Livio Baldini Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VeeAlign: Multifaceted Context Representation using Dual Attention for Ontology Alignment. (arXiv:2102.04081v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.04081","description":"<p>Ontology Alignment is an important research problem applied to various fields\nsuch as data integration, data transfer, data preparation, etc.\nState-of-the-art (SOTA) Ontology Alignment systems typically use naive\ndomain-dependent approaches with handcrafted rules or domain-specific\narchitectures, making them unscalable and inefficient. In this work, we propose\nVeeAlign, a Deep Learning based model that uses a novel dual-attention\nmechanism to compute the contextualized representation of a concept which, in\nturn, is used to discover alignments. By doing this, not only is our approach\nable to exploit both syntactic and semantic information encoded in ontologies,\nit is also, by design, flexible and scalable to different domains with minimal\neffort. We evaluate our model on four different datasets from different domains\nand languages, and establish its superiority through these results as well as\ndetailed ablation studies. The code and datasets used are available at\nhttps://github.com/Remorax/VeeAlign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1\">Vivek Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Arvind Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_H/0/1/0/all/0/1\">Harshit Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three-level Hierarchical Transformer Networks for Long-sequence and Multiple Clinical Documents Classification. (arXiv:2104.08444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08444","description":"<p>We present a Three-level Hierarchical Transformer Network (3-level-HTN) for\nmodeling long-term dependencies across clinical notes for the purpose of\npatient-level prediction. The network is equipped with three levels of\nTransformer-based encoders to learn progressively from words to sentences,\nsentences to notes, and finally notes to patients. The first level from word to\nsentence directly applies a pre-trained BERT model as a fully trainable\ncomponent. While the second and third levels both implement a stack of\ntransformer-based encoders, before the final patient representation is fed into\na classification layer for clinical predictions. Compared to conventional BERT\nmodels, our model increases the maximum input length from 512 tokens to much\nlonger sequences that are appropriate for modeling large numbers of clinical\nnotes. We empirically examine different hyper-parameters to identify an optimal\ntrade-off given computational resource limits. Our experiment results on the\nMIMIC-III dataset for different prediction tasks demonstrate that the proposed\nHierarchical Transformer Network outperforms previous state-of-the-art models,\nincluding but not limited to BigBird.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_Y/0/1/0/all/0/1\">Yuqi Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_K/0/1/0/all/0/1\">Kirk Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07340","description":"<p>Since the introduction of the original BERT (i.e., BASE BERT), researchers\nhave developed various customized BERT models with improved performance for\nspecific domains and tasks by exploiting the benefits of transfer learning. Due\nto the nature of mathematical texts, which often use domain specific vocabulary\nalong with equations and math symbols, we posit that the development of a new\nBERT model for mathematics would be useful for many mathematical downstream\ntasks. In this resource paper, we introduce our multi-institutional effort\n(i.e., two learning platforms and three academic institutions in the US) toward\nthis need: MathBERT, a model created by pre-training the BASE BERT model on a\nlarge mathematical corpus ranging from pre-kindergarten (pre-k), to\nhigh-school, to college graduate level mathematical content. In addition, we\nselect three general NLP tasks that are often used in mathematics education:\nprediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge\ntracing, to demonstrate the superiority of MathBERT over BASE BERT. Our\nexperiments show that MathBERT outperforms prior best methods by 1.2-22% and\nBASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific\nvocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT\npre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT\nvocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the\nparticipated leaning platforms: Stride, Inc, a commercial educational resource\nprovider, and ASSISTments.org, a free online educational platform. We release\nMathBERT for public usage at: https://github.com/tbs17/MathBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jia Tracy Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_M/0/1/0/all/0/1\">Michiharu Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prihar_E/0/1/0/all/0/1\">Ethan Prihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heffernan_N/0/1/0/all/0/1\">Neil Heffernan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xintao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_B/0/1/0/all/0/1\">Ben Graff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N24News: A New Dataset for Multimodal News Classification. (arXiv:2108.13327v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13327","description":"<p>Current news datasets merely focus on text features on the news and rarely\nleverage the feature of images, excluding numerous essential features for news\nclassification. In this paper, we propose a new dataset, N24News, which is\ngenerated from New York Times with 24 categories and contains both text and\nimage information in each news. We use a multitask multimodal method and the\nexperimental results show multimodal news classification performs better than\ntext-only news classification. Depending on the length of the text, the\nclassification accuracy can be increased by up to 8.11%. Our research reveals\nthe relationship between the performance of a multimodal classifier and its\nsub-classifiers, and also the possible improvements when applying multimodal in\nnews classification. N24News is shown to have great potential to prompt the\nmultimodal news studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_X/0/1/0/all/0/1\">Xu Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangxie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08256","description":"<p>The importance and pervasiveness of emotions in our lives makes affective\ncomputing a tremendously important and vibrant line of work. Systems for\nautomatic emotion recognition (AER) and sentiment analysis can be facilitators\nof enormous progress (e.g., in improving public health and commerce) but also\nenablers of great harm (e.g., for suppressing dissidents and manipulating\nvoters). Thus, it is imperative that the affective computing community actively\nengage with the ethical ramifications of their creations. In this paper, I have\nsynthesized and organized information from AI Ethics and Emotion Recognition\nliterature to present fifty ethical considerations relevant to AER. Notably,\nthe sheet fleshes out assumptions hidden in how AER is commonly framed, and in\nthe choices often made regarding the data, method, and evaluation. Special\nattention is paid to the implications of AER on privacy and social groups.\nAlong the way, key recommendations are made for responsible AER. The objective\nof the sheet is to facilitate and encourage more thoughtfulness on why to\nautomate, how to automate, and how to judge success well before the building of\nAER systems. Additionally, the sheet acts as a useful introductory document on\nemotion recognition (complementing survey articles).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural News Recommendation with Event Extraction. (arXiv:2111.05068v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2111.05068","description":"<p>A key challenge of online news recommendation is to help users find articles\nthey are interested in. Traditional news recommendation methods usually use\nsingle news information, which is insufficient to encode news and user\nrepresentation. Recent research uses multiple channel news information, e.g.,\ntitle, category, and body, to enhance news and user representation. However,\nthese methods only use various attention mechanisms to fuse multi-view\nembeddings without considering deep digging higher-level information contained\nin the context. These methods encode news content on the word level and jointly\ntrain the attention parameters in the recommendation network, leading to more\ncorpora being required to train the model. We propose an Event Extraction-based\nNews Recommendation (EENR) framework to overcome these shortcomings, utilizing\nevent extraction to abstract higher-level information. EENR also uses a\ntwo-stage strategy to reduce parameters in subsequent parts of the\nrecommendation network. We train the Event Extraction module by external\ncorpora in the first stage and apply the trained model to the news\nrecommendation dataset to predict event-level information, including event\ntypes, roles, and arguments, in the second stage. Then we fuse multiple channel\ninformation, including event information, news title, and category, to encode\nnews and users. Extensive experiments on a real-world dataset show that our\nEENR method can effectively improve the performance of news recommendations.\nFinally, we also explore the reasonability of utilizing higher abstract level\ninformation to substitute news body content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Songqiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel knowledge graph development for industry design: A case study on indirect coal liquefaction process. (arXiv:2111.13854v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.13854","description":"<p>Hazard and operability analysis (HAZOP) is a remarkable representative in\nindustrial safety engineering. However, a great storehouse of industrial safety\nknowledge (ISK) in HAZOP reports has not been thoroughly exploited. In order to\nreuse and unlock the value of ISK and optimize HAZOP, we have developed a novel\nknowledge graph for industrial safety (ISKG) with HAZOP as the carrier through\nbridging data science (DS) and engineering design (ED). Specifically, firstly,\nconsidering that the knowledge contained in HAZOP reports of different\nprocesses in industry is not the same, we have creatively developed a general\nISK standardization framework (ISKSF), ISKSF provides a practical scheme for\nthe standardization of HAZOP reports in various processes and the unified\nrepresentation of different types of ISK, which realizes the integration and\ncirculation of ISK. Secondly, we conceive a novel and reliable information\nextraction model (HAINEX) based on deep learning combined with DS. HAINEX can\neffectively mine ISK from HAZOP reports, which alleviates the obstacle of ISK\nextraction caused by the particularity of HAZOP text. Finally, we build ISK\ntriples based on ISKSF and HAINEX and store them in the Neo4j graph database.\nWe take indirect coal liquefaction process as a case study to develop ISKG, and\nits oriented applications can optimize HAZOP and mine the potential of ISK,\nwhich is of great significance to improve the security of the system and\nenhance prevention awareness for people. ISKG containing ISKSF and HAINEX sets\nan example of the interaction between DS and ED for industrial safety, which\ncan enlighten other researchers committed to DS for ED and extend the\nperspectives of industrial safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Dong Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06223","description":"<p>Code-switching is a speech phenomenon when a speaker switches language during\na conversation. Despite the spontaneous nature of code-switching in\nconversational spoken language, most existing works collect code-switching data\nthrough read speech instead of spontaneous speech. ASCEND (A Spontaneous\nChinese-English Dataset) introduces a high-quality resource of spontaneous\nmulti-turn conversational dialogue Chinese-English code-switching corpus\ncollected in Hong Kong. We report ASCEND's design and procedure of collecting\nthe speech data, including the annotations in this work. ASCEND includes 23\nbilinguals that are fluent in both Chinese and English and consists of 10.62\nhours clean speech corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Scene Imagination for Generative Commonsense Reasoning. (arXiv:2112.06318v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06318","description":"<p>Humans use natural language to compose common concepts from their environment\ninto plausible, day-to-day scene descriptions. However, such generative\ncommonsense reasoning (GCSR) skills are lacking in state-of-the-art text\ngeneration methods. Descriptive sentences about arbitrary concepts generated by\nneural text generation models (e.g., pre-trained text-to-text Transformers) are\noften grammatically fluent but may not correspond to human common sense,\nlargely due to their lack of mechanisms to capture concept relations, to\nidentify implicit concepts, and to perform generalizable reasoning about unseen\nconcept compositions. In this paper, we propose an Imagine-and-Verbalize (I&amp;V)\nmethod, which learns to imagine a relational scene knowledge graph (SKG) with\nrelations between the input concepts, and leverage the SKG as a constraint when\ngenerating a plausible scene description. We collect and harmonize a set of\nknowledge resources from different domains and modalities, providing a rich\nauxiliary supervision signal for I&amp;V. The experiments demonstrate the\neffectiveness of I&amp;V in improving language models on both concept-to-sentence\nand concept-to-story generation tasks, while enabling the model to learn well\nfrom fewer task examples and generate SKGs that make common sense to human\nannotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">PeiFeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamora_J/0/1/0/all/0/1\">Jonathan Zamora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TASSY -- A Text Annotation Survey System. (arXiv:2112.07391v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07391","description":"<p>We present a free and open-source tool for creating web-based surveys that\ninclude text annotation tasks. Existing tools offer either text annotation or\nsurvey functionality but not both. Combining the two input types is\nparticularly relevant for investigating a reader's perception of a text which\nalso depends on the reader's background, such as age, gender, and education.\nOur tool caters primarily to the needs of researchers in the Library and\nInformation Sciences, the Social Sciences, and the Humanities who apply Content\nAnalysis to investigate, e.g., media bias, political communication, or fake\nnews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1\">Timo Spinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Kanishka Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do You Think It's Biased? How To Ask For The Perception Of Media Bias. (arXiv:2112.07392v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07392","description":"<p>Media coverage possesses a substantial effect on the public perception of\nevents. The way media frames events can significantly alter the beliefs and\nperceptions of our society. Nevertheless, nearly all media outlets are known to\nreport news in a biased way. While such bias can be introduced by altering the\nword choice or omitting information, the perception of bias also varies largely\ndepending on a reader's personal background. Therefore, media bias is a very\ncomplex construct to identify and analyze. Even though media bias has been the\nsubject of many studies, previous assessment strategies are oversimplified,\nlack overlap and empirical evaluation. Thus, this study aims to develop a scale\nthat can be used as a reliable standard to evaluate article bias. To name an\nexample: Intending to measure bias in a news article, should we ask, \"How\nbiased is the article?\" or should we instead ask, \"How did the article treat\nthe American president?\". We conducted a literature search to find 824 relevant\nquestions about text perception in previous research on the topic. In a\nmulti-iterative process, we summarized and condensed these questions\nsemantically to conclude a complete and representative set of possible question\ntypes about bias. The final set consisted of 25 questions with varying\nanswering formats, 17 questions using semantic differentials, and six ratings\nof feelings. We tested each of the questions on 190 articles with overall 663\nparticipants to identify how well the questions measure an article's perceived\nbias. Our results show that 21 final items are suitable and reliable for\nmeasuring the perception of media bias. We publish the final set of questions\non <a href=\"http://bias-question-tree.gipplab.org/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1\">Timo Spinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreuter_C/0/1/0/all/0/1\">Christina Kreuter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaissmaier_W/0/1/0/all/0/1\">Wolfgang Gaissmaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giese_H/0/1/0/all/0/1\">Helge Giese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards A Reliable Ground-Truth For Biased Language Detection. (arXiv:2112.07421v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07421","description":"<p>Reference texts such as encyclopedias and news articles can manifest biased\nlanguage when objective reporting is substituted by subjective writing.\nExisting methods to detect bias mostly rely on annotated data to train machine\nlearning models. However, low annotator agreement and comparability is a\nsubstantial drawback in available media bias corpora. To evaluate data\ncollection options, we collect and compare labels obtained from two popular\ncrowdsourcing platforms. Our results demonstrate the existing crowdsourcing\napproaches' lack of data quality, underlining the need for a trained expert\nframework to gather a more reliable dataset. By creating such a framework and\ngathering a first dataset, we are able to improve Krippendorff's $\\alpha$ =\n0.144 (crowdsourcing labels) to $\\alpha$ = 0.419 (expert labels). We conclude\nthat detailed annotator training increases data quality, improving the\nperformance of existing bias detection systems. We will continue to extend our\ndataset in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1\">Timo Spinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krieger_D/0/1/0/all/0/1\">David Krieger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_M/0/1/0/all/0/1\">Manuel Plank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Abstractive Summarization with Adaptive Length Controlling. (arXiv:2112.07534v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07534","description":"<p>Document summarization, as a fundamental task in natural language generation,\naims to generate a short and coherent summary for a given document.\nControllable summarization, especially of the length, is an important issue for\nsome practical applications, especially how to trade-off the length constraint\nand information integrity. In this paper, we propose an \\textbf{A}daptive\n\\textbf{L}ength \\textbf{C}ontrolling \\textbf{O}ptimization (\\textbf{ALCO})\nmethod to leverage two-stage abstractive summarization model via reinforcement\nlearning. ALCO incorporates length constraint into the stage of sentence\nextraction to penalize the overlength extracted sentences. Meanwhile, a\nsaliency estimation mechanism is designed to preserve the salient information\nin the generated sentences. A series of experiments have been conducted on a\nwildly-used benchmark dataset \\textit{CNN/Daily Mail}. The results have shown\nthat ALCO performs better than the popular baselines in terms of length\ncontrollability and content preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingyang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liping Jing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-combine Decoding and Classification Approach for Punctuation Prediction with real-time Inference Constraints. (arXiv:2112.08098v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08098","description":"<p>In this work, we unify several existing decoding strategies for punctuation\nprediction in one framework and introduce a novel strategy which utilises\nmultiple predictions at each word across different windows. We show that\nsignificant improvements can be achieved by optimising these strategies after\ntraining a model, only leading to a potential increase in inference time, with\nno requirement for retraining. We further use our decoding strategy framework\nfor the first comparison of tagging and classification approaches for\npunctuation prediction in a real-time setting. Our results show that a\nclassification approach for punctuation prediction can be beneficial when\nlittle or no right-side context is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minixhofer_C/0/1/0/all/0/1\">Christoph Minixhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klejch_O/0/1/0/all/0/1\">Ond&#x159;ej Klejch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIN-X: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain. (arXiv:2112.08754v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08754","description":"<p>The field of natural language processing (NLP) has recently seen a large\nchange towards using pre-trained language models for solving almost any task.\nDespite showing great improvements in benchmark datasets for various tasks,\nthese models often perform sub-optimal in non-standard domains like the\nclinical domain where a large gap between pre-training documents and target\ndocuments is observed. In this paper, we aim at closing this gap with\ndomain-specific training of the language model and we investigate its effect on\na diverse set of downstream tasks and settings. We introduce the pre-trained\nCLIN-X (Clinical XLM-R) language models and show how CLIN-X outperforms other\npre-trained transformer models by a large margin for ten clinical concept\nextraction tasks from two languages. In addition, we demonstrate how the\ntransformer model can be further improved with our proposed task- and\nlanguage-agnostic model architecture based on ensembles over random splits and\ncross-sentence context. Our studies in low-resource and transfer settings\nreveal stable model performance despite a lack of annotated data with\nimprovements of up to 47 F1 points when only 250 labeled sentences are\navailable. Our results highlight the importance of specialized language models\nas CLIN-X for concept extraction in non-standard domains, but also show that\nour task-agnostic model architecture is robust across the tested tasks and\nlanguages so that domain- or task-specific adaptations are not required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ASC-Net: Unsupervised Medical Anomaly Segmentation Using an Adversarial-based Selective Cutting Network. (arXiv:2112.09135v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09135","description":"<p>In this paper we consider the problem of unsupervised anomaly segmentation in\nmedical images, which has attracted increasing attention in recent years due to\nthe expensive pixel-level annotations from experts and the existence of a large\namount of unannotated normal and abnormal image scans. We introduce a\nsegmentation network that utilizes adversarial learning to partition an image\ninto two cuts, with one of them falling into a reference distribution provided\nby the user. This Adversarial-based Selective Cutting network (ASC-Net) bridges\nthe two domains of cluster-based deep segmentation and adversarial-based\nanomaly/novelty detection algorithms. Our ASC-Net learns from normal and\nabnormal medical scans to segment anomalies in medical scans without any masks\nfor supervision. We evaluate this unsupervised anomly segmentation model on\nthree public datasets, i.e., BraTS 2019 for brain tumor segmentation, LiTS for\nliver lesion segmentation, and MS-SEG 2015 for brain lesion segmentation, and\nalso on a private dataset for brain tumor segmentation. Compared to existing\nmethods, our model demonstrates tremendous performance gains in unsupervised\nanomaly segmentation tasks. Although there is still room to further improve\nperformance compared to supervised learning algorithms, the promising\nexperimental results and interesting observations shed light on building an\nunsupervised learning algorithm for medical anomaly identification using\nuser-defined knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dey_R/0/1/0/all/0/1\">Raunak Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_W/0/1/0/all/0/1\">Wenbo Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1\">Haibo Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1\">Yi Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAFIM: Targeted Adversarial Attacks against Facial Image Manipulations. (arXiv:2112.09151v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09151","description":"<p>Face image manipulation methods, despite having many beneficial applications\nin computer graphics, can also raise concerns by affecting an individual's\nprivacy or spreading disinformation. In this work, we propose a proactive\ndefense to prevent face manipulation from happening in the first place. To this\nend, we introduce a novel data-driven approach that produces image-specific\nperturbations which are embedded in the original images. The key idea is that\nthese protected images prevent face manipulation by causing the manipulation\nmodel to produce a predefined manipulation target (uniformly colored output\nimage in our case) instead of the actual manipulation. Compared to traditional\nadversarial attacks that optimize noise patterns for each image individually,\nour generalized model only needs a single forward pass, thus running orders of\nmagnitude faster and allowing for easy integration in image processing stacks,\neven on resource-constrained devices like smartphones. In addition, we propose\nto leverage a differentiable compression approximation, hence making generated\nperturbations robust to common image compression. We further show that a\ngenerated perturbation can simultaneously prevent against multiple manipulation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1\">Shivangi Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markhasin_L/0/1/0/all/0/1\">Lev Markhasin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Niessner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09153","description":"<p>The lifelong learning paradigm in machine learning is an attractive\nalternative to the more prominent isolated learning scheme not only due to its\nresemblance to biological learning, but also its potential to reduce energy\nwaste by obviating excessive model re-training. A key challenge to this\nparadigm is the phenomenon of catastrophic forgetting. With the increasing\npopularity and success of pre-trained models in machine learning, we pose the\nquestion: What role does pre-training play in lifelong learning, specifically\nwith respect to catastrophic forgetting? We investigate existing methods in the\ncontext of large, pre-trained models and evaluate their performance on a\nvariety of text and image classification tasks, including a large-scale study\nusing a novel dataset of 15 diverse NLP tasks. Across all settings, we observe\nthat generic pre-training implicitly alleviates the effects of catastrophic\nforgetting when learning multiple tasks sequentially compared to randomly\ninitialized models. We then further investigate why pre-training alleviates\nforgetting in this setting. We study this phenomenon by analyzing the loss\nlandscape, finding that pre-trained weights appear to ease forgetting by\nleading to wider minima. Based on this insight, we propose jointly optimizing\nfor current task loss and loss basin sharpness in order to explicitly encourage\nwider basins during sequential fine-tuning. We show that this optimization\napproach leads to performance comparable to the state-of-the-art in\ntask-sequential continual learning across multiple settings, without retaining\na memory that scales in size with the number of tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sanket Vaibhav Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_D/0/1/0/all/0/1\">Darshan Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1\">Emma Strubell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALEBk: Feasibility Study of Attention Level Estimation via Blink Detection applied to e-Learning. (arXiv:2112.09165v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09165","description":"<p>This work presents a feasibility study of remote attention level estimation\nbased on eye blink frequency. We first propose an eye blink detection system\nbased on Convolutional Neural Networks (CNNs), very competitive with respect to\nrelated works. Using this detector, we experimentally evaluate the relationship\nbetween the eye blink rate and the attention level of students captured during\nonline sessions. The experimental framework is carried out using a public\nmultimodal database for eye blink detection and attention level estimation\ncalled mEBAL, which comprises data from 38 students and multiples acquisition\nsensors, in particular, i) an electroencephalogram (EEG) band which provides\nthe time signals coming from the student's cognitive information, and ii) RGB\nand NIR cameras to capture the students face gestures. The results achieved\nsuggest an inverse correlation between the eye blink frequency and the\nattention level. This relation is used in our proposed method called ALEBk for\nestimating the attention level as the inverse of the eye blink frequency. Our\nresults open a new research line to introduce this technology for attention\nlevel estimation on future e-learning platforms, among other applications of\nthis kind of behavioral biometrics based on face analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Daza_R/0/1/0/all/0/1\">Roberto Daza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeAlcala_D/0/1/0/all/0/1\">Daniel DeAlcala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_R/0/1/0/all/0/1\">Ruth Cobos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Audio-Visual Dataset and Deep Learning Frameworks for Crowded Scene Classification. (arXiv:2112.09172v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09172","description":"<p>This paper presents a task of audio-visual scene classification (SC) where\ninput videos are classified into one of five real-life crowded scenes: 'Riot',\n'Noise-Street', 'Firework-Event', 'Music-Event', and 'Sport-Atmosphere'. To\nthis end, we firstly collect an audio-visual dataset (videos) of these five\ncrowded contexts from Youtube (in-the-wild scenes). Then, a wide range of deep\nlearning frameworks are proposed to deploy either audio or visual input data\nindependently. Finally, results obtained from high-performed deep learning\nframeworks are fused to achieve the best accuracy score. Our experimental\nresults indicate that audio and visual input factors independently contribute\nto the SC task's performance. Significantly, an ensemble of deep learning\nframeworks exploring either audio or visual input data can achieve the best\naccuracy of 95.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pham_L/0/1/0/all/0/1\">Lam Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_D/0/1/0/all/0/1\">Dat Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phu X. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Truong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_A/0/1/0/all/0/1\">Alexander Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coherence Learning using Keypoint-based Pooling Network for Accurately Assessing Radiographic Knee Osteoarthritis. (arXiv:2112.09177v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09177","description":"<p>Knee osteoarthritis (OA) is a common degenerate joint disorder that affects a\nlarge population of elderly people worldwide. Accurate radiographic assessment\nof knee OA severity plays a critical role in chronic patient management.\nCurrent clinically-adopted knee OA grading systems are observer subjective and\nsuffer from inter-rater disagreements. In this work, we propose a\ncomputer-aided diagnosis approach to provide more accurate and consistent\nassessments of both composite and fine-grained OA grades simultaneously. A\nnovel semi-supervised learning method is presented to exploit the underlying\ncoherence in the composite and fine-grained OA grades by learning from\nunlabeled data. By representing the grade coherence using the log-probability\nof a pre-trained Gaussian Mixture Model, we formulate an incoherence loss to\nincorporate unlabeled data in training. The proposed method also describes a\nkeypoint-based pooling network, where deep image features are pooled from the\ndisease-targeted keypoints (extracted along the knee joint) to provide more\naligned and pathologically informative feature representations, for accurate OA\ngrade assessments. The proposed method is comprehensively evaluated on the\npublic Osteoarthritis Initiative (OAI) data, a multi-center ten-year\nobservational study on 4,796 subjects. Experimental results demonstrate that\nour method leads to significant improvements over previous strong whole\nimage-based deep classification network baselines (like ResNet-50).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_K/0/1/0/all/0/1\">Kang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsieh_C/0/1/0/all/0/1\">Chen-I Hsieh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuo_C/0/1/0/all/0/1\">Chang-Fu Kuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_S/0/1/0/all/0/1\">Shun Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring crop phenology with street-level imagery using computer vision. (arXiv:2112.09190v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09190","description":"<p>Street-level imagery holds a significant potential to scale-up in-situ data\ncollection. This is enabled by combining the use of cheap high quality cameras\nwith recent advances in deep learning compute solutions to derive relevant\nthematic information. We present a framework to collect and extract crop type\nand phenological information from street level imagery using computer vision.\nDuring the 2018 growing season, high definition pictures were captured with\nside-looking action cameras in the Flevoland province of the Netherlands. Each\nmonth from March to October, a fixed 200-km route was surveyed collecting one\npicture per second resulting in a total of 400,000 geo-tagged pictures. At 220\nspecific parcel locations detailed on the spot crop phenology observations were\nrecorded for 17 crop types. Furthermore, the time span included specific\npre-emergence parcel stages, such as differently cultivated bare soil for\nspring and summer crops as well as post-harvest cultivation practices, e.g.\ngreen manuring and catch crops. Classification was done using TensorFlow with a\nwell-known image recognition model, based on transfer learning with\nconvolutional neural networks (MobileNet). A hypertuning methodology was\ndeveloped to obtain the best performing model among 160 models. This best model\nwas applied on an independent inference set discriminating crop type with a\nMacro F1 score of 88.1% and main phenological stage at 86.9% at the parcel\nlevel. Potential and caveats of the approach along with practical\nconsiderations for implementation and improvement are discussed. The proposed\nframework speeds up high quality in-situ data collection and suggests avenues\nfor massive data collection via automated classification using computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+dAndrimont_R/0/1/0/all/0/1\">Rapha&#xeb;l d&#x27;Andrimont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yordanov_M/0/1/0/all/0/1\">Momchil Yordanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Sanchez_L/0/1/0/all/0/1\">Laura Martinez-Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velde_M/0/1/0/all/0/1\">Marijn van der Velde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating the Bias of Centered Objects in Common Datasets. (arXiv:2112.09195v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09195","description":"<p>Convolutional networks are considered shift invariant, but it was\ndemonstrated that their response may vary according to the exact location of\nthe objects. In this paper we will demonstrate that most commonly investigated\ndatasets have a bias, where objects are over-represented at the center of the\nimage during training. This bias and the boundary condition of these networks\ncan have a significant effect on the performance of these architectures and\ntheir accuracy drops significantly as an object approaches the boundary. We\nwill also demonstrate how this effect can be mitigated with data augmentation\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1\">Gergely Szabo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvath_A/0/1/0/all/0/1\">Andras Horvath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Based Few-Shot Learning by Interactive Psychometric Testing. (arXiv:2112.09201v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09201","description":"<p>Few-shot classification tasks aim to classify images in query sets based on\nonly a few labeled examples in support sets. Most studies usually assume that\neach image in a task has a single and unique class association. Under these\nassumptions, these algorithms may not be able to identify the proper class\nassignment when there is no exact matching between support and query classes.\nFor example, given a few images of lions, bikes, and apples to classify a\ntiger. However, in a more general setting, we could consider the higher-level\nconcept of large carnivores to match the tiger to the lion for semantic\nclassification. Existing studies rarely considered this situation due to the\nincompatibility of label-based supervision with complex conception\nrelationships. In this work, we advanced the few-shot learning towards this\nmore challenging scenario, the semantic-based few-shot learning, and proposed a\nmethod to address the paradigm by capturing the inner semantic relationships\nusing interactive psychometric learning. We evaluate our method on the\nCIFAR-100 dataset. The results show the merits of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1\">Yulong Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds. (arXiv:2112.09205v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09205","description":"<p>There have been two streams in the 3D detection from point clouds:\nsingle-stage methods and two-stage methods. While the former is more\ncomputationally efficient, the latter usually provides better detection\naccuracy. By carefully examining the two-stage approaches, we have found that\nif appropriately designed, the first stage can produce accurate box regression.\nIn this scenario, the second stage mainly rescores the boxes such that the\nboxes with better localization get selected. From this observation, we have\ndevised a single-stage anchor-free network that can fulfill these requirements.\nThis network, named AFDetV2, extends the previous work by incorporating a\nself-calibrated convolution block in the backbone, a keypoint auxiliary\nsupervision, and an IoU prediction branch in the multi-task head. As a result,\nthe detection accuracy is drastically boosted in the single-stage. To evaluate\nour approach, we have conducted extensive experiments on the Waymo Open Dataset\nand the nuScenes Dataset. We have observed that our AFDetV2 achieves the\nstate-of-the-art results on these two datasets, superior to all the prior arts,\nincluding both the single-stage and the two-stage se3D detectors. AFDetV2 won\nthe 1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge\n2021. In addition, a variant of our model AFDetV2-Base was entitled the \"Most\nEfficient Model\" by the Challenge Sponsor, showing a superior computational\nefficiency. To demonstrate the generality of this single-stage method, we have\nalso applied it to the first stage of the two-stage networks. Without\nexception, the results show that with the strengthened backbone and the\nrescoring approach, the second stage refinement is no longer needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yihan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuangzhuang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1\">Runzhou Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenxin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Li Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Coding with Multi-Layer Decoders using Variance Regularization. (arXiv:2112.09214v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09214","description":"<p>Sparse coding with an $l_1$ penalty and a learned linear dictionary requires\nregularization of the dictionary to prevent a collapse in the $l_1$ norms of\nthe codes. Typically, this regularization entails bounding the Euclidean norms\nof the dictionary's elements. In this work, we propose a novel sparse coding\nprotocol which prevents a collapse in the codes without the need to regularize\nthe decoder. Our method regularizes the codes directly so that each latent code\ncomponent has variance greater than a fixed threshold over a set of sparse\nrepresentations for a given set of inputs. Furthermore, we explore ways to\neffectively train sparse coding systems with multi-layer decoders since they\ncan model more complex relationships than linear dictionaries. In our\nexperiments with MNIST and natural image patches, we show that decoders learned\nwith our approach have interpretable features both in the linear and\nmulti-layer case. Moreover, we show that sparse autoencoders with multi-layer\ndecoders trained using our variance regularization method produce higher\nquality reconstructions with sparser representations when compared to\nautoencoders with linear dictionaries. Additionally, sparse representations\nobtained with our variance regularization approach are useful in the downstream\ntasks of denoising and classification in the low-data regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evtimova_K/0/1/0/all/0/1\">Katrina Evtimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep-Learning Framework for Improving COVID-19 CT Image Quality and Diagnostic Accuracy. (arXiv:2112.09216v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09216","description":"<p>We present a deep-learning based computing framework for fast-and-accurate CT\n(DL-FACT) testing of COVID-19. Our CT-based DL framework was developed to\nimprove the testing speed and accuracy of COVID-19 (plus its variants) via a\nDL-based approach for CT image enhancement and classification. The image\nenhancement network is adapted from DDnet, short for DenseNet and Deconvolution\nbased network. To demonstrate its speed and accuracy, we evaluated DL-FACT\nacross several sources of COVID-19 CT images. Our results show that DL-FACT can\nsignificantly shorten the turnaround time from days to minutes and improve the\nCOVID-19 testing accuracy up to 91%. DL-FACT could be used as a software tool\nfor medical professionals in diagnosing and monitoring COVID-19.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goel_G/0/1/0/all/0/1\">Garvit Goel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_J/0/1/0/all/0/1\">Jingyuan Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1\">Wu-chun Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_G/0/1/0/all/0/1\">Guohua Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need is RAW: Defending Against Adversarial Attacks with Camera Image Pipelines. (arXiv:2112.09219v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09219","description":"<p>Existing neural networks for computer vision tasks are vulnerable to\nadversarial attacks: adding imperceptible perturbations to the input images can\nfool these methods to make a false prediction on an image that was correctly\npredicted without the perturbation. Various defense methods have proposed\nimage-to-image mapping methods, either including these perturbations in the\ntraining process or removing them in a preprocessing denoising step. In doing\nso, existing methods often ignore that the natural RGB images in today's\ndatasets are not captured but, in fact, recovered from RAW color filter array\ncaptures that are subject to various degradations in the capture. In this work,\nwe exploit this RAW data distribution as an empirical prior for adversarial\ndefense. Specifically, we proposed a model-agnostic adversarial defensive\nmethod, which maps the input RGB images to Bayer RAW space and back to output\nRGB using a learned camera image signal processing (ISP) pipeline to eliminate\npotential adversarial patterns. The proposed method acts as an off-the-shelf\npreprocessing module and, unlike model-specific adversarial training methods,\ndoes not require adversarial images to train. As a result, the method\ngeneralizes to unseen tasks without additional retraining. Experiments on\nlarge-scale datasets (e.g., ImageNet, COCO) for different vision tasks (e.g.,\nclassification, semantic segmentation, object detection) validate that the\nmethod significantly outperforms existing methods across task domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim2Real Docs: Domain Randomization for Documents in Natural Scenes using Ray-traced Rendering. (arXiv:2112.09220v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09220","description":"<p>In the past, computer vision systems for digitized documents could rely on\nsystematically captured, high-quality scans. Today, transactions involving\ndigital documents are more likely to start as mobile phone photo uploads taken\nby non-professionals. As such, computer vision for document automation must now\naccount for documents captured in natural scene contexts. An additional\nchallenge is that task objectives for document processing can be highly\nuse-case specific, which makes publicly-available datasets limited in their\nutility, while manual data labeling is also costly and poorly translates\nbetween use cases.\n</p>\n<p>To address these issues we created Sim2Real Docs - a framework for\nsynthesizing datasets and performing domain randomization of documents in\nnatural scenes. Sim2Real Docs enables programmatic 3D rendering of documents\nusing Blender, an open source tool for 3D modeling and ray-traced rendering. By\nusing rendering that simulates physical interactions of light, geometry,\ncamera, and background, we synthesize datasets of documents in a natural scene\ncontext. Each render is paired with use-case specific ground truth data\nspecifying latent characteristics of interest, producing unlimited fit-for-task\ntraining data. The role of machine learning models is then to solve the inverse\nproblem posed by the rendering pipeline. Such models can be further iterated\nupon with real-world data by either fine tuning or making adjustments to domain\nrandomization parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maddikunta_N/0/1/0/all/0/1\">Nikhil Maddikunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huijun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keswani_S/0/1/0/all/0/1\">Sumit Keswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_A/0/1/0/all/0/1\">Alfy Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fu-Ming Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srishankar_N/0/1/0/all/0/1\">Nishan Srishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardeshi_V/0/1/0/all/0/1\">Vishwa Pardeshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Austin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Wanderings of Odysseus in 3D Scenes. (arXiv:2112.09251v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09251","description":"<p>Our goal is to populate digital environments, in which the digital humans\nhave diverse body shapes, move perpetually, and have plausible body-scene\ncontact. The core challenge is to generate realistic, controllable, and\ninfinitely long motions for diverse 3D bodies. To this end, we propose\ngenerative motion primitives via body surface markers, shortened as GAMMA. In\nour solution, we decompose the long-term motion into a time sequence of motion\nprimitives. We exploit body surface markers and conditional variational\nautoencoder to model each motion primitive, and generate long-term motion by\nimplementing the generative model recursively. To control the motion to reach a\ngoal, we apply a policy network to explore the model latent space, and use a\ntree-based search to preserve the motion quality during testing. Experiments\nshow that our method can produce more realistic and controllable motion than\nstate-of-the-art data-driven method. With conventional path-finding algorithms,\nthe generated human bodies can realistically move long distances for a long\nperiod of time in the scene. Code will be released for research purposes at:\n\\url{https://yz-cnsdqz.github.io/eigenmotion/GAMMA/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logically at the Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09253","description":"<p>This paper describes our participant system for the multi-modal fact\nverification (Factify) challenge at AAAI 2022. Despite the recent advance in\ntext based verification techniques and large pre-trained multimodal models\ncross vision and language, very limited work has been done in applying\nmultimodal techniques to automate fact checking process, particularly\nconsidering the increasing prevalence of claims and fake news about images and\nvideos on social media. In our work, the challenge is treated as multimodal\nentailment task and framed as multi-class classification. Two baseline\napproaches are proposed and explored including an ensemble model (combining two\nuni-modal models) and a multi-modal attention network (modeling the interaction\nbetween image and text pair from claim and evidence document). We conduct\nseveral experiments investigating and benchmarking different SoTA pre-trained\ntransformers and vision models in this work. Our best model is ranked first in\nleaderboard which obtains a weighted average F-measure of 0.77 on both\nvalidation and test set. Exploratory analysis of dataset is also carried out on\nthe Factify data set and uncovers salient patterns and issues (e.g., word\noverlapping, visual entailment correlation, source bias) that motivates our\nhypothesis. Finally, we highlight challenges of the task and multimodal dataset\nfor future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_H/0/1/0/all/0/1\">Hella-Franziska Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_S/0/1/0/all/0/1\">Stylianos Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiskovski_D/0/1/0/all/0/1\">David Kiskovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandhakavi_A/0/1/0/all/0/1\">Anil Bandhakavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Image Denoising Algorithm Using Concepts of Quantum Many-Body Theory. (arXiv:2112.09254v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09254","description":"<p>Sparse representation of real-life images is a very effective approach in\nimaging applications, such as denoising. In recent years, with the growth of\ncomputing power, data-driven strategies exploiting the redundancy within\npatches extracted from one or several images to increase sparsity have become\nmore prominent. This paper presents a novel image denoising algorithm\nexploiting such an image-dependent basis inspired by the quantum many-body\ntheory. Based on patch analysis, the similarity measures in a local image\nneighborhood are formalized through a term akin to interaction in quantum\nmechanics that can efficiently preserve the local structures of real images.\nThe versatile nature of this adaptive basis extends the scope of its\napplication to image-independent or image-dependent noise scenarios without any\nadjustment. We carry out a rigorous comparison with contemporary methods to\ndemonstrate the denoising capability of the proposed algorithm regardless of\nthe image characteristics, noise statistics and intensity. We illustrate the\nproperties of the hyperparameters and their respective effects on the denoising\nperformance, together with automated rules of selecting their values close to\nthe optimal one in experimental setups with ground truth not available.\nFinally, we show the ability of our approach to deal with practical images\ndenoising problems such as medical ultrasound image despeckling applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dutta_S/0/1/0/all/0/1\">Sayantan Dutta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Basarab_A/0/1/0/all/0/1\">Adrian Basarab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Georgeot_B/0/1/0/all/0/1\">Bertrand Georgeot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kouame_D/0/1/0/all/0/1\">Denis Kouam&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to augment your ViTs? Consistency loss and StyleAug, a random style transfer augmentation. (arXiv:2112.09260v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09260","description":"<p>The Vision Transformer (ViT) architecture has recently achieved competitive\nperformance across a variety of computer vision tasks. One of the motivations\nbehind ViTs is weaker inductive biases, when compared to convolutional neural\nnetworks (CNNs). However this also makes ViTs more difficult to train. They\nrequire very large training datasets, heavy regularization, and strong data\naugmentations. The data augmentation strategies used to train ViTs have largely\nbeen inherited from CNN training, despite the significant differences between\nthe two architectures. In this work, we empirical evaluated how different data\naugmentation strategies performed on CNN (e.g., ResNet) versus ViT\narchitectures for image classification. We introduced a style transfer data\naugmentation, termed StyleAug, which worked best for training ViTs, while\nRandAugment and Augmix typically worked best for training CNNs. We also found\nthat, in addition to a classification loss, using a consistency loss between\nmultiple augmentations of the same image was especially helpful when training\nViTs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Umakantha_A/0/1/0/all/0/1\">Akash Umakantha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semedo_J/0/1/0/all/0/1\">Joao D. Semedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golestaneh_S/0/1/0/all/0/1\">S. Alireza Golestaneh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wan-Yi S. Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Inpainting Using AutoEncoder and Guided Selection of Predicted Pixels. (arXiv:2112.09262v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09262","description":"<p>Image inpainting is an effective method to enhance distorted digital images.\nDifferent inpainting methods use the information of neighboring pixels to\npredict the value of missing pixels. Recently deep neural networks have been\nused to learn structural and semantic details of images for inpainting\npurposes. In this paper, we propose a network for image inpainting. This\nnetwork, similar to U-Net, extracts various features from images, leading to\nbetter results. We improved the final results by replacing the damaged pixels\nwith the recovered pixels of the output images. Our experimental results show\nthat this method produces high-quality results compare to the traditional\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Givkashi_M/0/1/0/all/0/1\">Mohammad H. Givkashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadipour_M/0/1/0/all/0/1\">Mahshid Hadipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PariZanganeh_A/0/1/0/all/0/1\">Arezoo PariZanganeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabizadeh_Z/0/1/0/all/0/1\">Zahra Nabizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_N/0/1/0/all/0/1\">Nader Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All-photon Polarimetric Time-of-Flight Imaging. (arXiv:2112.09278v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09278","description":"<p>Time-of-flight (ToF) sensors provide an imaging modality fueling diverse\napplications, including LiDAR in autonomous driving, robotics, and augmented\nreality. Conventional ToF imaging methods estimate the depth by sending pulses\nof light into a scene and measuring the ToF of the first-arriving photons\ndirectly reflected from a scene surface without any temporal delay. As such,\nall photons following this first response are typically considered as unwanted\nnoise. In this paper, we depart from the principle of using first-arriving\nphotons and propose an all-photon ToF imaging method by incorporating the\ntemporal-polarimetric analysis of first- and late-arriving photons, which\npossess rich scene information about its geometry and material. To this end, we\npropose a novel temporal-polarimetric reflectance model, an efficient capture\nmethod, and a reconstruction method that exploits the temporal-polarimetric\nchanges of light reflected by the surface and sub-surface reflection. The\nproposed all-photon polarimetric ToF imaging method allows for acquiring depth,\nsurface normals, and material parameters of a scene by utilizing all photons\ncaptured by the system, whereas conventional ToF imaging only obtains coarse\ndepth from the first-arriving photons. We validate our method in simulation and\nexperimentally with a prototype.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Seung-Hwan Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision. (arXiv:2112.09290v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09290","description":"<p>In recent years, person detection and human pose estimation have made great\nstrides, helped by large-scale labeled datasets. However, these datasets had no\nguarantees or analysis of human activities, poses, or context diversity.\nAdditionally, privacy, legal, safety, and ethical concerns may limit the\nability to collect more human data. An emerging alternative to real-world data\nthat alleviates some of these issues is synthetic data. However, creation of\nsynthetic data generators is incredibly challenging and prevents researchers\nfrom exploring their usefulness. Therefore, we release a human-centric\nsynthetic data generator PeopleSansPeople which contains simulation-ready 3D\nhuman assets, a parameterized lighting and camera system, and generates 2D and\n3D bounding box, instance and semantic segmentation, and COCO pose labels.\nUsing PeopleSansPeople, we performed benchmark synthetic data training using a\nDetectron2 Keypoint R-CNN variant [1]. We found that pre-training a network\nusing synthetic data and fine-tuning on target real-world data (few-shot\ntransfer to limited subsets of COCO-person train [2]) resulted in a keypoint AP\nof $60.37 \\pm 0.48$ (COCO test-dev2017) outperforming models trained with the\nsame real data alone (keypoint AP of $55.80$) and pre-trained with ImageNet\n(keypoint AP of $57.50$). This freely-available data generator should enable a\nwide range of research into the emerging field of simulation to real transfer\nlearning in the critical area of human-centric computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebadi_S/0/1/0/all/0/1\">Salehe Erfanian Ebadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jhang_Y/0/1/0/all/0/1\">You-Cyuan Jhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zook_A/0/1/0/all/0/1\">Alex Zook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhakad_S/0/1/0/all/0/1\">Saurav Dhakad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1\">Adam Crespi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parisi_P/0/1/0/all/0/1\">Pete Parisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkman_S/0/1/0/all/0/1\">Steven Borkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1\">Jonathan Hogins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1\">Sujoy Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Vehicle Cooperative Visual Perception for Shared Autonomous Driving. (arXiv:2112.09298v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09298","description":"<p>With the development of key technologies like environment perception, the\nautomation level of autonomous vehicles has been increasing. However, before\nreaching highly autonomous driving, manual driving still needs to participate\nin the driving process to ensure the safety of human-vehicle shared driving.\nThe existing human-vehicle cooperative driving focuses on auto engineering and\ndrivers' behaviors, with few research studies in the field of visual\nperception. Due to the bad performance in the complex road traffic conflict\nscenarios, cooperative visual perception needs to be studied further. In\naddition, the autonomous driving perception system cannot correctly understand\nthe characteristics of manual driving. Based on the background above, this\npaper directly proposes a human-vehicle cooperative visual perception method to\nenhance the visual perception ability of shared autonomous driving based on the\ntransfer learning method and the image fusion algorithm for the complex road\ntraffic scenarios. Based on transfer learning, the mAP of object detection\nreaches 75.52% and lays a solid foundation for visual fusion. And the fusion\nexperiment further reveals that human-vehicle cooperative visual perception\nreflects the riskiest zone and predicts the conflict object's trajectory more\nprecisely. This study pioneers a cooperative visual perception solution for\nshared autonomous driving and experiments in real-world complex traffic\nconflict scenarios, which can better support the following planning and\ncontrolling and improve the safety of autonomous vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiyue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Cailin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuchuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-End Image Compression and Analysis with Transformers. (arXiv:2112.09300v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09300","description":"<p>We propose an end-to-end image compression and analysis model with\nTransformers, targeting to the cloud-based image classification application.\nInstead of placing an existing Transformer-based image classification model\ndirectly after an image codec, we aim to redesign the Vision Transformer (ViT)\nmodel to perform image classification from the compressed features and\nfacilitate image compression with the long-term information from the\nTransformer. Specifically, we first replace the patchify stem (i.e., image\nsplitting and embedding) of the ViT model with a lightweight image encoder\nmodelled by a convolutional neural network. The compressed features generated\nby the image encoder are injected convolutional inductive bias and are fed to\nthe Transformer for image classification bypassing image reconstruction.\nMeanwhile, we propose a feature aggregation module to fuse the compressed\nfeatures with the selected intermediate features of the Transformer, and feed\nthe aggregated features to a deconvolutional neural network for image\nreconstruction. The aggregated features can obtain the long-term information\nfrom the self-attention mechanism of the Transformer and improve the\ncompression performance. The rate-distortion-accuracy optimization problem is\nfinally solved by a two-step training strategy. Experimental results\ndemonstrate the effectiveness of the proposed model in both the image\ncompression and the classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yuanchao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Procedural Kernel Networks. (arXiv:2112.09318v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09318","description":"<p>In the last decade Convolutional Neural Networks (CNNs) have defined the\nstate of the art for many low level image processing and restoration tasks such\nas denoising, demosaicking, upscaling, or inpainting. However, on-device mobile\nphotography is still dominated by traditional image processing techniques, and\nuses mostly simple machine learning techniques or limits the neural network\nprocessing to producing low resolution masks. High computational and memory\nrequirements of CNNs, limited processing power and thermal constraints of\nmobile devices, combined with large output image resolutions (typically 8--12\nMPix) prevent their wider application. In this work, we introduce Procedural\nKernel Networks (PKNs), a family of machine learning models which generate\nparameters of image filter kernels or other traditional algorithms. A\nlightweight CNN processes the input image at a lower resolution, which yields a\nsignificant speedup compared to other kernel-based machine learning methods and\nallows for new applications. The architecture is learned end-to-end and is\nespecially well suited for a wide range of low-level image processing tasks,\nwhere it improves the performance of many traditional algorithms. We also\ndescribe how this framework unifies some previous work applying machine\nlearning for common image restoration tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wronski_B/0/1/0/all/0/1\">Bartlomiej Wronski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cinderella's shoe won't fit Soundarya: An audit of facial processing tools on Indian faces. (arXiv:2112.09326v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09326","description":"<p>The increasing adoption of facial processing systems in India is fraught with\nconcerns of privacy, transparency, accountability, and missing procedural\nsafeguards. At the same time, we also know very little about how these\ntechnologies perform on the diverse features, characteristics, and skin tones\nof India's 1.34 billion-plus population. In this paper, we test the face\ndetection and facial analysis functions of four commercial facial processing\ntools on a dataset of Indian faces. The tools display varying error rates in\nthe face detection and gender and age classification functions. The gender\nclassification error rate for Indian female faces is consistently higher\ncompared to that of males -- the highest female error rate being 14.68%. In\nsome cases, this error rate is much higher than that shown by previous studies\nfor females of other nationalities. Age classification errors are also high.\nDespite taking into account an acceptable error margin of plus or minus 10\nyears from a person's actual age, age prediction failures are in the range of\n14.3% to 42.2%. These findings point to the limited accuracy of facial\nprocessing tools, particularly for certain demographic groups, and the need for\nmore critical thinking before adopting such systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_G/0/1/0/all/0/1\">Gaurav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsheera_S/0/1/0/all/0/1\">Smriti Parsheera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point2Cyl: Reverse Engineering 3D Objects from Point Clouds to Extrusion Cylinders. (arXiv:2112.09329v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09329","description":"<p>We propose Point2Cyl, a supervised network transforming a raw 3D point cloud\nto a set of extrusion cylinders. Reverse engineering from a raw geometry to a\nCAD model is an essential task to enable manipulation of the 3D data in shape\nediting software and thus expand their usages in many downstream applications.\nParticularly, the form of CAD models having a sequence of extrusion cylinders\n-- a 2D sketch plus an extrusion axis and range -- and their boolean\ncombinations is not only widely used in the CAD community/software but also has\ngreat expressivity of shapes, compared to having limited types of primitives\n(e.g., planes, spheres, and cylinders). In this work, we introduce a neural\nnetwork that solves the extrusion cylinder decomposition problem in a\ngeometry-grounded way by first learning underlying geometric proxies.\nPrecisely, our approach first predicts per-point segmentation, base/barrel\nlabels and normals, then estimates for the underlying extrusion parameters in\ndifferentiable and closed-form formulations. Our experiments show that our\napproach demonstrates the best performance on two recent CAD datasets, Fusion\nGallery and DeepCAD, and we further showcase our approach on reverse\nengineering and editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uy_M/0/1/0/all/0/1\">Mikaela Angelina Uy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-yu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Purvi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1\">Joseph Lambourne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroVL: A Strong Baseline for Aligning Vision-Language Representations with Limited Resources. (arXiv:2112.09331v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09331","description":"<p>Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have\nrevealed the potential of aligning multi-modal representations with contrastive\nlearning. However, these works require a tremendous amount of data and\ncomputational resources (e.g., billion-level web data and hundreds of GPUs),\nwhich prevent researchers with limited resources from reproduction and further\nexploration. To this end, we explore a stack of simple but effective\nheuristics, and provide a comprehensive training guidance, which allows us to\nconduct dual-encoder multi-modal representation alignment with limited\nresources. We provide a reproducible strong baseline of competitive results,\nnamely ZeroVL, with only 14M publicly accessible academic datasets and 8 V100\nGPUs. Additionally, we collect 100M web data for pre-training, and achieve\ncomparable or superior results than state-of-the-art methods, further proving\nthe effectiveness of our method on large-scale data. We hope that this work\nwill provide useful data points and experience for future research in\nmulti-modal pre-training. Our code and pre-trained models will be released to\nfacilitate the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Boyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Weidong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshie_O/0/1/0/all/0/1\">Osamu Yoshie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation on Point Clouds via Geometry-Aware Implicits. (arXiv:2112.09343v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09343","description":"<p>As a popular geometric representation, point clouds have attracted much\nattention in 3D vision, leading to many applications in autonomous driving and\nrobotics. One important yet unsolved issue for learning on point cloud is that\npoint clouds of the same object can have significant geometric variations if\ngenerated using different procedures or captured using different sensors. These\ninconsistencies induce domain gaps such that neural networks trained on one\ndomain may fail to generalize on others. A typical technique to reduce the\ndomain gap is to perform adversarial training so that point clouds in the\nfeature space can align. However, adversarial training is easy to fall into\ndegenerated local minima, resulting in negative adaptation gains. Here we\npropose a simple yet effective method for unsupervised domain adaptation on\npoint clouds by employing a self-supervised task of learning geometry-aware\nimplicits, which plays two critical roles in one shot. First, the geometric\ninformation in the point clouds is preserved through the implicit\nrepresentations for downstream tasks. More importantly, the domain-specific\nvariations can be effectively learned away in the implicit space. We also\npropose an adaptive strategy to compute unsigned distance fields for arbitrary\npoint clouds due to the lack of shape models in practice. When combined with a\ntask loss, the proposed outperforms state-of-the-art unsupervised domain\nadaptation methods that rely on adversarial domain alignment and more\ncomplicated self-supervised tasks. Our method is evaluated on both PointDA-10\nand GraspNet datasets. The code and trained models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuefan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Mi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified 2D and 3D Pre-training for Medical Image classification and Segmentation. (arXiv:2112.09356v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09356","description":"<p>Self-supervised learning (SSL) opens up huge opportunities for better\nutilizing unlabeled data. It is essential for medical image analysis that is\ngenerally known for its lack of annotations. However, when we attempt to use as\nmany as possible unlabeled medical images in SSL, breaking the dimension\nbarrier (\\ie, making it possible to jointly use both 2D and 3D images) becomes\na must. In this paper, we propose a Universal Self-Supervised Transformer\n(USST) framework based on the student-teacher paradigm, aiming to leverage a\nhuge of unlabeled medical data with multiple dimensions to learn rich\nrepresentations. To achieve this, we design a Pyramid Transformer U-Net (PTU)\nas the backbone, which is composed of switchable patch embedding (SPE) layers\nand Transformer layers. The SPE layer switches to either 2D or 3D patch\nembedding depending on the input dimension. After that, the images are\nconverted to a sequence regardless of their original dimensions. The\nTransformer layer then models the long-term dependencies in a\nsequence-to-sequence manner, thus enabling USST to learn representations from\nboth 2D and 3D images. USST has two obvious merits compared to current\ndimension-specific SSL: (1) \\textbf{more effective} - can learn representations\nfrom more and diverse data; and (2) \\textbf{more versatile} - can be\ntransferred to various downstream tasks. The results show that USST provides\npromising results on six 2D/3D medical image classification and segmentation\ntasks, outperforming the supervised ImageNet pre-training and advanced SSL\ncounterparts substantially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yutong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Audiograms with Multi-stage Neural Networks. (arXiv:2112.09357v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09357","description":"<p>Audiograms are a particular type of line charts representing individuals'\nhearing level at various frequencies. They are used by audiologists to diagnose\nhearing loss, and further select and tune appropriate hearing aids for\ncustomers. There have been several projects such as Autoaudio that aim to\naccelerate this process through means of machine learning. But all existing\nmodels at their best can only detect audiograms in images and classify them\ninto general categories. They are unable to extract hearing level information\nfrom detected audiograms by interpreting the marks, axis, and lines. To address\nthis issue, we propose a Multi-stage Audiogram Interpretation Network (MAIN)\nthat directly reads hearing level data from photos of audiograms. We also\nestablished Open Audiogram, an open dataset of audiogram images with\nannotations of marks and axes on which we trained and evaluated our proposed\nmodel. Experiments show that our model is feasible and reliable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shufan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Congxi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linkai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jirong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xinping Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoshuai Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colloquium: Advances in automation of quantum dot devices control. (arXiv:2112.09362v1 [quant-ph])","link":"http://arxiv.org/abs/2112.09362","description":"<p>Arrays of quantum dots (QDs) are a promising candidate system to realize\nscalable, coupled qubits systems and serve as a fundamental building block for\nquantum computers. In such semiconductor quantum systems, devices now have tens\nof individual electrostatic and dynamical voltages that must be carefully set\nto localize the system into the single-electron regime and to realize good\nqubit operational performance. The mapping of requisite dot locations and\ncharges to gate voltages presents a challenging classical control problem. With\nan increasing number of QD qubits, the relevant parameter space grows\nsufficiently to make heuristic control unfeasible. In recent years, there has\nbeen a considerable effort to automate device control that combines\nscript-based algorithms with machine learning (ML) techniques. In this\nColloquium, we present a comprehensive overview of the recent progress in the\nautomation of QD device control, with a particular emphasis on silicon- and\nGaAs-based QDs formed in two-dimensional electron gases. Combining\nphysics-based modeling with modern numerical optimization and ML has proven\nquite effective in yielding efficient, scalable control. Further integration of\ntheoretical, computational, and experimental efforts with computer science and\nML holds tremendous potential in advancing semiconductor and other platforms\nfor quantum computing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1\">Jacob M. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperStyleNet: Deep Image Synthesis with Superpixel Based Style Encoder. (arXiv:2112.09367v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09367","description":"<p>Existing methods for image synthesis utilized a style encoder based on stacks\nof convolutions and pooling layers to generate style codes from input images.\nHowever, the encoded vectors do not necessarily contain local information of\nthe corresponding images since small-scale objects are tended to \"wash away\"\nthrough such downscaling procedures. In this paper, we propose deep image\nsynthesis with superpixel based style encoder, named as SuperStyleNet. First,\nwe directly extract the style codes from the original image based on\nsuperpixels to consider local objects. Second, we recover spatial relationships\nin vectorized style codes based on graphical analysis. Thus, the proposed\nnetwork achieves high-quality image synthesis by mapping the style codes into\nsemantic labels. Experimental results show that the proposed method outperforms\nstate-of-the-art ones in terms of visual quality and quantitative measurements.\nFurthermore, we achieve elaborate spatial style editing by adjusting style\ncodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jonghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1\">Cheolkon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongkyu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Frame and Event-Based Simulator and Event-Based Video Interpolation Network. (arXiv:2112.09379v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09379","description":"<p>Fast neuromorphic event-based vision sensors (Dynamic Vision Sensor, DVS) can\nbe combined with slower conventional frame-based sensors to enable\nhigher-quality inter-frame interpolation than traditional methods relying on\nfixed motion approximations using e.g. optical flow. In this work we present a\nnew, advanced event simulator that can produce realistic scenes recorded by a\ncamera rig with an arbitrary number of sensors located at fixed offsets. It\nincludes a new configurable frame-based image sensor model with realistic image\nquality reduction effects, and an extended DVS model with more accurate\ncharacteristics. We use our simulator to train a novel reconstruction model\ndesigned for end-to-end reconstruction of high-fps video. Unlike previously\npublished methods, our method does not require the frame and DVS cameras to\nhave the same optics, positions, or camera resolutions. It is also not limited\nto objects a fixed distance from the sensor. We show that data generated by our\nsimulator can be used to train our new model, leading to reconstructed images\non public datasets of equivalent or better quality than the state of the art.\nWe also show our sensor generalizing to data recorded by real sensors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radomski_A/0/1/0/all/0/1\">Adam Radomski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiou_A/0/1/0/all/0/1\">Andreas Georgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debrunner_T/0/1/0/all/0/1\">Thomas Debrunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenghan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longinotti_L/0/1/0/all/0/1\">Luca Longinotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minwon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_M/0/1/0/all/0/1\">Moosung Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1\">Chang-Woo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_P/0/1/0/all/0/1\">Paul K. J. Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1\">Hyunsurk Eric Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eng_K/0/1/0/all/0/1\">Kynan Eng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full Transformer Framework for Robust Point Cloud Registration with Deep Information Interaction. (arXiv:2112.09385v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09385","description":"<p>Recent Transformer-based methods have achieved advanced performance in point\ncloud registration by utilizing advantages of the Transformer in\norder-invariance and modeling dependency to aggregate information. However,\nthey still suffer from indistinct feature extraction, sensitivity to noise, and\noutliers. The reasons are: (1) the adoption of CNNs fails to model global\nrelations due to their local receptive fields, resulting in extracted features\nsusceptible to noise; (2) the shallow-wide architecture of Transformers and\nlack of positional encoding lead to indistinct feature extraction due to\ninefficient information interaction; (3) the omission of geometrical\ncompatibility leads to inaccurate classification between inliers and outliers.\nTo address above limitations, a novel full Transformer network for point cloud\nregistration is proposed, named the Deep Interaction Transformer (DIT), which\nincorporates: (1) a Point Cloud Structure Extractor (PSE) to model global\nrelations and retrieve structural information with Transformer encoders; (2) a\ndeep-narrow Point Feature Transformer (PFT) to facilitate deep information\ninteraction across two point clouds with positional encoding, such that\nTransformers can establish comprehensive associations and directly learn\nrelative position between points; (3) a Geometric Matching-based Correspondence\nConfidence Evaluation (GMCCE) method to measure spatial consistency and\nestimate inlier confidence by designing the triangulated descriptor. Extensive\nexperiments on clean, noisy, partially overlapping point cloud registration\ndemonstrate that our method outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meiling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yufeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-attention based anchor proposal for skeleton-based action recognition. (arXiv:2112.09413v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09413","description":"<p>Skeleton sequences are widely used for action recognition task due to its\nlightweight and compact characteristics. Recent graph convolutional network\n(GCN) approaches have achieved great success for skeleton-based action\nrecognition since its grateful modeling ability of non-Euclidean data. GCN is\nable to utilize the short-range joint dependencies while lack to directly model\nthe distant joints relations that are vital to distinguishing various actions.\nThus, many GCN approaches try to employ hierarchical mechanism to aggregate\nwider-range neighborhood information. We propose a novel self-attention based\nskeleton-anchor proposal (SAP) module to comprehensively model the internal\nrelations of a human body for motion feature learning. The proposed SAP module\naims to explore inherent relationship within human body using a triplet\nrepresentation via encoding high order angle information rather than the fixed\npair-wise bone connection used in the existing hierarchical GCN approaches. A\nSelf-attention based anchor selection method is designed in the proposed SAP\nmodule for extracting the root point of encoding angular information. By\ncoupling proposed SAP module with popular spatial-temporal graph neural\nnetworks, e.g. MSG3D, it achieves new state-of-the-art accuracy on challenging\nbenchmark datasets. Further ablation study have shown the effectiveness of our\nproposed SAP module, which is able to obviously improve the performance of many\npopular skeleton-based action recognition methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Ruijie Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled representations: towards interpretation of sex determination from hip bone. (arXiv:2112.09414v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09414","description":"<p>By highlighting the regions of the input image that contribute the most to\nthe decision, saliency maps have become a popular method to make neural\nnetworks interpretable. In medical imaging, they are particularly well-suited\nto explain neural networks in the context of abnormality localization. However,\nfrom our experiments, they are less suited to classification problems where the\nfeatures that allow to distinguish between the different classes are spatially\ncorrelated, scattered and definitely non-trivial. In this paper we thus propose\na new paradigm for better interpretability. To this end we provide the user\nwith relevant and easily interpretable information so that he can form his own\nopinion. We use Disentangled Variational Auto-Encoders which latent\nrepresentation is divided into two components: the non-interpretable part and\nthe disentangled part. The latter accounts for the categorical variables\nexplicitly representing the different classes of interest. In addition to\nproviding the class of a given input sample, such a model offers the\npossibility to transform the sample from a given class to a sample of another\nclass, by modifying the value of the categorical variables in the latent\nrepresentation. This paves the way to easier interpretation of class\ndifferences. We illustrate the relevance of this approach in the context of\nautomatic sex determination from hip bones in forensic medicine. The features\nencoded by the model, that distinguish the different classes were found to be\nconsistent with expert knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1\">Kaifeng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisan_S/0/1/0/all/0/1\">Sylvain Faisan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heitz_F/0/1/0/all/0/1\">Fabrice Heitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epain_M/0/1/0/all/0/1\">Marie Epain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croisille_P/0/1/0/all/0/1\">Pierre Croisille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanton_L/0/1/0/all/0/1\">Laurent Fanton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valette_S/0/1/0/all/0/1\">S&#xe9;bastien Valette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review on Visual Privacy Preservation Techniques for Active and Assisted Living. (arXiv:2112.09422v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09422","description":"<p>This paper reviews the state of the art in visual privacy protection\ntechniques, with particular attention paid to techniques applicable to the\nfield of active and assisted living (AAL). A novel taxonomy with which\nstate-of-the-art visual privacy protection methods can be classified is\nintroduced. Perceptual obfuscation methods, a category in the taxonomy, is\nhighlighted. These are a category of visual privacy preservation techniques\nparticularly relevant when considering scenarios that come under video-based\nAAL monitoring. Obfuscation against machine learning models is also explored. A\nhigh-level classification scheme of the different levels of privacy by design\nis connected to the proposed taxonomy of visual privacy preservation\ntechniques. Finally, we note open questions that exist in the field and\nintroduce the reader to some exciting avenues for future research in the area\nof visual privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Siddharth Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Climent_Perez_P/0/1/0/all/0/1\">Pau Climent-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florez_Revuelta_F/0/1/0/all/0/1\">Francisco Florez-Revuelta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-Trained Siamese Transformers. (arXiv:2112.09426v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09426","description":"<p>We propose a novel zero-shot multi-frame image restoration method for\nremoving unwanted obstruction elements (such as rains, snow, and moire\npatterns) that vary in successive frames. It has three stages: transformer\npre-training, zero-shot restoration, and hard patch refinement. Using the\npre-trained transformers, our model is able to tell the motion difference\nbetween the true image information and the obstructing elements. For zero-shot\nimage restoration, we design a novel model, termed SiamTrans, which is\nconstructed by Siamese transformers, encoders, and decoders. Each transformer\nhas a temporal attention layer and several self-attention layers, to capture\nboth temporal and spatial information of multiple frames. Only pre-trained\n(self-supervised) on the denoising task, SiamTrans is tested on three different\nlow-level vision tasks (deraining, demoireing, and desnowing). Compared with\nrelated methods, ours achieves the best performances, even outperforming those\nwith supervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shanxin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Youliang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamics-aware Adversarial Attack of 3D Sparse Convolution Network. (arXiv:2112.09428v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09428","description":"<p>In this paper, we investigate the dynamics-aware adversarial attack problem\nin deep neural networks. Most existing adversarial attack algorithms are\ndesigned under a basic assumption -- the network architecture is fixed\nthroughout the attack process. However, this assumption does not hold for many\nrecently proposed networks, e.g. 3D sparse convolution network, which contains\ninput-dependent execution to improve computational efficiency. It results in a\nserious issue of lagged gradient, making the learned attack at the current step\nineffective due to the architecture changes afterward. To address this issue,\nwe propose a Leaded Gradient Method (LGM) and show the significant effects of\nthe lagged gradient. More specifically, we re-formulate the gradients to be\naware of the potential dynamic changes of network architectures, so that the\nlearned attack better \"leads\" the next step than the dynamics-unaware methods\nwhen network architecture changes dynamically. Extensive experiments on various\ndatasets show that our LGM achieves impressive performance on semantic\nsegmentation and classification. Compared with the dynamic-unaware methods, LGM\nachieves about 20% lower mIoU averagely on the ScanNet and S3DIS datasets. LGM\nalso outperforms the recent point cloud attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1\">An Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pengliang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haowen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptively Customizing Activation Functions for Various Layers. (arXiv:2112.09442v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09442","description":"<p>To enhance the nonlinearity of neural networks and increase their mapping\nabilities between the inputs and response variables, activation functions play\na crucial role to model more complex relationships and patterns in the data. In\nthis work, a novel methodology is proposed to adaptively customize activation\nfunctions only by adding very few parameters to the traditional activation\nfunctions such as Sigmoid, Tanh, and ReLU. To verify the effectiveness of the\nproposed methodology, some theoretical and experimental analysis on\naccelerating the convergence and improving the performance is presented, and a\nseries of experiments are conducted based on various network models (such as\nAlexNet, VGGNet, GoogLeNet, ResNet and DenseNet), and various datasets (such as\nCIFAR10, CIFAR100, miniImageNet, PASCAL VOC and COCO) . To further verify the\nvalidity and suitability in various optimization strategies and usage\nscenarios, some comparison experiments are also implemented among different\noptimization strategies (such as SGD, Momentum, AdaGrad, AdaDelta and ADAM) and\ndifferent recognition tasks like classification and detection. The results show\nthat the proposed methodology is very simple but with significant performance\nin convergence speed, precision and generalization, and it can surpass other\npopular methods like ReLU and adaptive functions like Swish in almost all\nexperiments in terms of overall performance.The code is publicly available at\nhttps://github.com/HuHaigen/Adaptively-Customizing-Activation-Functions. The\npackage includes the proposed three adaptive activation functions for\nreproducibility purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haigen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aizhu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1\">Qiu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shengyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianwei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Efficient Language-supervised Zero-shot Recognition with Optimal Transport Distillation. (arXiv:2112.09445v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09445","description":"<p>Traditional computer vision models are trained to predict a fixed set of\npredefined categories. Recently, natural language has been shown to be a\nbroader and richer source of supervision that provides finer descriptions to\nvisual concepts than supervised \"gold\" labels. Previous works, such as CLIP,\nuse InfoNCE loss to train a model to predict the pairing between images and\ntext captions. CLIP, however, is data hungry and requires more than 400M\nimage-text pairs for training. The inefficiency can be partially attributed to\nthe fact that the image-text pairs are noisy. To address this, we propose OTTER\n(Optimal TransporT distillation for Efficient zero-shot Recognition), which\nuses online entropic optimal transport to find a soft image-text match as\nlabels for contrastive learning. Based on pretrained image and text encoders,\nmodels trained with OTTER achieve strong performance with only 3M image text\npairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation,\nOTTER consistently outperforms these baselines in zero shot evaluation on\nGoogle Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032\nclasses) from Tencent ML-Images. Over 42 evaluations on 7 different\ndataset/architecture settings x 6 metrics, OTTER outperforms (32) or ties (2)\nall baselines in 34 of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bichen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ruizhe Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1\">Peter Vajda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distillation of Human-Object Interaction Contexts for Action Recognition. (arXiv:2112.09448v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09448","description":"<p>Modeling spatial-temporal relations is imperative for recognizing human\nactions, especially when a human is interacting with objects, while multiple\nobjects appear around the human differently over time. Most existing action\nrecognition models focus on learning overall visual cues of a scene but\ndisregard informative fine-grained features, which can be captured by learning\nhuman-object relationships and interactions. In this paper, we learn\nhuman-object relationships by exploiting the interaction of their local and\nglobal contexts. We hence propose the Global-Local Interaction Distillation\nNetwork (GLIDN), learning human and object interactions through space and time\nvia knowledge distillation for fine-grained scene understanding. GLIDN encodes\nhumans and objects into graph nodes and learns local and global relations via\ngraph attention network. The local context graphs learn the relation between\nhumans and objects at a frame level by capturing their co-occurrence at a\nspecific time step. The global relation graph is constructed based on the\nvideo-level of human and object interactions, identifying their long-term\nrelations throughout a video sequence. More importantly, we investigate how\nknowledge from these graphs can be distilled to their counterparts for\nimproving human-object interaction (HOI) recognition. We evaluate our model by\nconducting comprehensive experiments on two datasets including Charades and\nCAD-120 datasets. We have achieved better results than the baselines and\ncounterpart approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almushyti_M/0/1/0/all/0/1\">Muna Almushyti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Frederick W. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Semantic Segmentation via Alternative Self-Dual Teaching. (arXiv:2112.09459v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09459","description":"<p>Current weakly supervised semantic segmentation (WSSS) frameworks usually\ncontain the separated mask-refinement model and the main semantic region mining\nmodel. These approaches would contain redundant feature extraction backbones\nand biased learning objectives, making them computational complex yet\nsub-optimal to addressing the WSSS task. To solve this problem, this paper\nestablishes a compact learning framework that embeds the classification and\nmask-refinement components into a unified deep model. With the shared feature\nextraction backbone, our model is able to facilitate knowledge sharing between\nthe two components while preserving a low computational complexity. To\nencourage high-quality knowledge interaction, we propose a novel alternative\nself-dual teaching (ASDT) mechanism. Unlike the conventional distillation\nstrategy, the knowledge of the two teacher branches in our model is\nalternatively distilled to the student branch by a Pulse Width Modulation\n(PWM), which generates PW wave-like selection signal to guide the knowledge\ndistillation process. In this way, the student branch can help prevent the\nmodel from falling into local minimum solutions caused by the imperfect\nknowledge provided of either teacher branch. Comprehensive experiments on the\nPASCAL VOC 2012 and COCO-Stuff 10K demonstrate the effectiveness of the\nproposed alternative self-dual teaching mechanism as well as the new\nstate-of-the-art performance of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guangyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Microfossil Identificationvia Deep Metric Learning. (arXiv:2112.09490v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09490","description":"<p>We apply deep metric learning for the first time to the prob-lem of\nclassifying planktic foraminifer shells on microscopic images. This species\nrecognition task is an important information source and scientific pillar for\nreconstructing past climates. All foraminifer CNN recognition pipelines in the\nliterature produce black-box classifiers that lack visualisation options for\nhuman experts and cannot be applied to open set problems. Here, we benchmark\nmetric learning against these pipelines, produce the first scientific\nvisualisation of the phenotypic planktic foraminifer morphology space, and\ndemonstrate that metric learning can be used to cluster species unseen during\ntraining. We show that metric learning out-performs all published CNN-based\nstate-of-the-art benchmarks in this domain. We evaluate our approach on the\n34,640 expert-annotated images of the Endless Forams public library of 35\nmodern planktic foraminifera species. Our results on this data show leading 92%\naccuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,\nand 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered\nin training. We conclude that metric learning is highly effective for this\ndomain and serves as an important tool towards expert-in-the-loop automation of\nmicrofossil identification. Key code, network weights, and data splits are\npublished with this paper for full reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karaderi_T/0/1/0/all/0/1\">Tayfun Karaderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiang_A/0/1/0/all/0/1\">Allison Y. Hsiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaer_J/0/1/0/all/0/1\">Jacob Ramaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Daniela N. Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Methods for segmenting cracks in 3d images of concrete: A comparison based on semi-synthetic images. (arXiv:2112.09493v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09493","description":"<p>Concrete is the standard construction material for buildings, bridges, and\nroads. As safety plays a central role in the design, monitoring, and\nmaintenance of such constructions, it is important to understand the cracking\nbehavior of concrete. Computed tomography captures the microstructure of\nbuilding materials and allows to study crack initiation and propagation. Manual\nsegmentation of crack surfaces in large 3d images is not feasible. In this\npaper, automatic crack segmentation methods for 3d images are reviewed and\ncompared. Classical image processing methods (edge detection filters, template\nmatching, minimal path and region growing algorithms) and learning methods\n(convolutional neural networks, random forests) are considered and tested on\nsemi-synthetic 3d images. Their performance strongly depends on parameter\nselection which should be adapted to the grayvalue distribution of the images\nand the geometric properties of the concrete. In general, the learning methods\nperform best, in particular for thin cracks and low grayvalue contrast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barisin_T/0/1/0/all/0/1\">Tin Barisin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1\">Christian Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musebeck_F/0/1/0/all/0/1\">Franziska M&#xfc;sebeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redenbach_C/0/1/0/all/0/1\">Claudia Redenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schladitz_K/0/1/0/all/0/1\">Katja Schladitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Launching AI Algorithms for Cellular Pathology into Clinical & Pharmaceutical Orbits. (arXiv:2112.09496v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09496","description":"<p>Computational Pathology (CPath) is an emerging field concerned with the study\nof tissue pathology via computational algorithms for the processing and\nanalysis of digitized high-resolution images of tissue slides. Recent deep\nlearning based developments in CPath have successfully leveraged sheer volume\nof raw pixel data in histology images for predicting target parameters in the\ndomains of diagnostics, prognostics, treatment sensitivity and patient\nstratification -- heralding the promise of a new data-driven AI era for both\nhistopathology and oncology. With data serving as the fuel and AI as the\nengine, CPath algorithms are poised to be ready for takeoff and eventual launch\ninto clinical and pharmaceutical orbits. In this paper, we discuss CPath\nlimitations and associated challenges to enable the readers distinguish hope\nfrom hype and provide directions for future research to overcome some of the\nmajor challenges faced by this budding field to enable its launch into the two\norbits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asif_A/0/1/0/all/0/1\">Amina Asif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_K/0/1/0/all/0/1\">Kashif Rajpoot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symmetry-aware Neural Architecture for Embodied Visual Navigation. (arXiv:2112.09515v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09515","description":"<p>Visual exploration is a task that seeks to visit all the navigable areas of\nan environment as quickly as possible. The existing methods employ deep\nreinforcement learning (RL) as the standard tool for the task. However, they\ntend to be vulnerable to statistical shifts between the training and test data,\nresulting in poor generalization over novel environments that are\nout-of-distribution (OOD) from the training data. In this paper, we attempt to\nimprove the generalization ability by utilizing the inductive biases available\nfor the task. Employing the active neural SLAM (ANS) that learns exploration\npolicies with the advantage actor-critic (A2C) method as the base framework, we\nfirst point out that the mappings represented by the actor and the critic\nshould satisfy specific symmetries. We then propose a network design for the\nactor and the critic to inherently attain these symmetries. Specifically, we\nuse $G$-convolution instead of the standard convolution and insert the\nsemi-global polar pooling (SGPP) layer, which we newly design in this study, in\nthe last section of the critic network. Experimental results show that our\nmethod increases area coverage by $8.1 m^2$ when trained on the Gibson dataset\nand tested on the MP3D dataset, establishing the new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Rate-Distortion Optimized Learned Hierarchical Bi-Directional Video Compression. (arXiv:2112.09529v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09529","description":"<p>Conventional video compression (VC) methods are based on motion compensated\ntransform coding, and the steps of motion estimation, mode and quantization\nparameter selection, and entropy coding are optimized individually due to the\ncombinatorial nature of the end-to-end optimization problem. Learned VC allows\nend-to-end rate-distortion (R-D) optimized training of nonlinear transform,\nmotion and entropy model simultaneously. Most works on learned VC consider\nend-to-end optimization of a sequential video codec based on R-D loss averaged\nover pairs of successive frames. It is well-known in conventional VC that\nhierarchical, bi-directional coding outperforms sequential compression because\nof its ability to use both past and future reference frames. This paper\nproposes a learned hierarchical bi-directional video codec (LHBDC) that\ncombines the benefits of hierarchical motion-compensated prediction and\nend-to-end optimization. Experimental results show that we achieve the best R-D\nresults that are reported for learned VC schemes to date in both PSNR and\nMS-SSIM. Compared to conventional video codecs, the R-D performance of our\nend-to-end optimized codec outperforms those of both x265 and SVT-HEVC encoders\n(\"veryslow\" preset) in PSNR and MS-SSIM as well as HM 16.23 reference software\nin MS-SSIM. We present ablation studies showing performance gains due to\nproposed novel tools such as learned masking, flow-field subsampling, and\ntemporal flow vector prediction. The models and instructions to reproduce our\nresults can be found in https://github.com/makinyilmaz/LHBDC/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_M/0/1/0/all/0/1\">M.Ak&#x131;n Y&#x131;lmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tekalp_A/0/1/0/all/0/1\">A.Murat Tekalp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel Distillation: A New Knowledge Distillation Scheme for Low-Resolution Image Recognition. (arXiv:2112.09532v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09532","description":"<p>The great success of deep learning is mainly due to the large-scale network\narchitecture and the high-quality training data. However, it is still\nchallenging to deploy recent deep models on portable devices with limited\nmemory and imaging ability. Some existing works have engaged to compress the\nmodel via knowledge distillation. Unfortunately, these methods cannot deal with\nimages with reduced image quality, such as the low-resolution (LR) images. To\nthis end, we make a pioneering effort to distill helpful knowledge from a heavy\nnetwork model learned from high-resolution (HR) images to a compact network\nmodel that will handle LR images, thus advancing the current knowledge\ndistillation technique with the novel pixel distillation. To achieve this goal,\nwe propose a Teacher-Assistant-Student (TAS) framework, which disentangles\nknowledge distillation into the model compression stage and the high resolution\nrepresentation transfer stage. By equipping a novel Feature Super Resolution\n(FSR) module, our approach can learn lightweight network model that can achieve\nsimilar accuracy as the heavy teacher model but with much fewer parameters,\nfaster inference speed, and lower-resolution inputs. Comprehensive experiments\non three widely-used benchmarks, \\ie, CUB-200-2011, PASCAL VOC 2007, and\nImageNetSub, demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guangyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Longfei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Functional Maps : a Conformal Link Between Tangent Bundles. (arXiv:2112.09546v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09546","description":"<p>In this paper, we introduce complex functional maps, which extend the\nfunctional map framework to conformal maps between tangent vector fields on\nsurfaces. A key property of these maps is their orientation awareness. More\nspecifically, we demonstrate that unlike regular functional maps that link\nfunctional spaces of two manifolds, our complex functional maps establish a\nlink between oriented tangent bundles, thus permitting robust and efficient\ntransfer of tangent vector fields. By first endowing and then exploiting the\ntangent bundle of each shape with a complex structure, the resulting operations\nbecome naturally orientationaware, thus favoring orientation and angle\npreserving correspondence across shapes, without relying on descriptors or\nextra regularization. Finally, and perhaps more importantly, we demonstrate how\nthese objects enable several practical applications within the functional map\nframework. We show that functional maps and their complex counterparts can be\nestimated jointly to promote orientation preservation, regularizing pipelines\nthat previously suffered from orientation-reversing symmetry errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Donati_N/0/1/0/all/0/1\">Nicolas Donati</a> (LIX), <a href=\"http://arxiv.org/find/cs/1/au:+Corman_E/0/1/0/all/0/1\">Etienne Corman</a> (LORIA, CNRS, PIXEL), <a href=\"http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1\">Simone Melzi</a> (Sapienza University of Rome), <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a> (LIX)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LTB curves with Lipschitz turn are par-regular. (arXiv:2112.09567v1 [cs.CG])","link":"http://arxiv.org/abs/2112.09567","description":"<p>Preserving the topology during a digitization process is a requirement of\nfirst importance. To this end, it is classical in Digital Geometry to assume\nthe shape borders to be par-regular. Par-regularity was proved to be equivalent\nto having positive reach or to belong to the class C 1,1 of curves with\nLipschitz derivative. Recently, we proposed to use a larger class that\nencompasses polygons with obtuse angles, the locally turn-bounded curves. The\naim of this technical report is to define the class of par-regular curves\ninside the class of locally turn-bounded curves using only the notion of turn,\nthat is of integral curvature. To be more precise, in a previous article, we\nhave already proved that par-regular curves are locally turn-bounded.\nIncidentally this proof lead us to show that the turn of par-regular curves is\na Lipschitz function of their length. We call the class of curves verifying\nthis latter property the curves with Lipschitz turn. In this technical report,\nwe prove the converse assertion : locally turn-bounded curves with Lipschitz\nturn are par-regular. The equivalence is stated in Theorem 3.1 and the converse\nassertion is proved in Lemma 3.2. In section 1, we recall the definition of\npar-regularity and equivalently of sets with positive reach. In section 2, we\npresent the notions of curves locally turn-bounded and of curves with Lipschitz\nturn. Throughout this latter section, some of intermediate steps (Lemmas 2.3\nand 2.11) are proved just after the introduction of their related notions. The\nlast section (section 3) is dedicated to the proof of the equivalence of the\nnotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quentrec_E/0/1/0/all/0/1\">Etienne Le Quentrec</a> (AMU), <a href=\"http://arxiv.org/find/cs/1/au:+Mazo_L/0/1/0/all/0/1\">Lo&#xef;c Mazo</a> (UNISTRA), <a href=\"http://arxiv.org/find/cs/1/au:+Baudrier_E/0/1/0/all/0/1\">&#xc9;tienne Baudrier</a> (UNISTRA), <a href=\"http://arxiv.org/find/cs/1/au:+Tajine_M/0/1/0/all/0/1\">Mohamed Tajine</a> (UNISTRA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nearest neighbor search with compact codes: A decoder perspective. (arXiv:2112.09568v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09568","description":"<p>Modern approaches for fast retrieval of similar vectors on billion-scaled\ndatasets rely on compressed-domain approaches such as binary sketches or\nproduct quantization. These methods minimize a certain loss, typically the mean\nsquared error or other objective functions tailored to the retrieval problem.\nIn this paper, we re-interpret popular methods such as binary hashing or\nproduct quantizers as auto-encoders, and point out that they implicitly make\nsuboptimal assumptions on the form of the decoder. We design\nbackward-compatible decoders that improve the reconstruction of the vectors\nfrom the same codes, which translates to a better performance in nearest\nneighbor search. Our method significantly improves over binary hashing methods\nor product quantization on popular benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amara_K/0/1/0/all/0/1\">Kenza Amara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sablayrolles_A/0/1/0/all/0/1\">Alexandre Sablayrolles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPPE-5: Medical Personal Protective Equipment Dataset. (arXiv:2112.09569v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09569","description":"<p>We present a new challenging dataset, CPPE - 5 (Medical Personal Protective\nEquipment), with the goal to allow the study of subordinate categorization of\nmedical personal protective equipments, which is not possible with other\npopular data sets that focus on broad level categories (such as PASCAL VOC,\nImageNet, Microsoft COCO, OpenImages, etc). To make it easy for models trained\non this dataset to be used in practical scenarios in complex scenes, our\ndataset mainly contains images that show complex scenes with several objects in\neach scene in their natural context. The image collection for this dataset\nfocusing on: obtaining as many non-iconic images as possible and making sure\nall the images are real-life images unlike other existing datasets in this\narea. Our dataset includes 5 object categories (coveralls, face shield, gloves,\nmask, and goggles) and each image is annotated with a set of bounding boxes and\npositive labels. We present a detailed analysis of the dataset in comparison to\nother popular broad category datasets as well as datasets focusing on personal\nprotective equipments, we also find that at present there exist no such\npublicly available datasets. Finally we also analyze performance and compare\nmodel complexities on baseline and state-of-the-art models for bounding box\nresults. Our code, data, and trained models are available at\nhttps://git.io/cppe5-dataset .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dagli_R/0/1/0/all/0/1\">Rishit Dagli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1\">Ali Mustufa Shaikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-resolution reconstruction of cytoskeleton image based on A-net deep learning network. (arXiv:2112.09574v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09574","description":"<p>To date, live-cell imaging at the nanometer scale remains challenging. Even\nthough super-resolution microscopy methods have enabled visualization of\nsubcellular structures below the optical resolution limit, the spatial\nresolution is still far from enough for the structural reconstruction of\nbiomolecules in vivo (i.e. ~24 nm thickness of microtubule fiber). In this\nstudy, we proposed an A-net network and showed that the resolution of\ncytoskeleton images captured by a confocal microscope can be significantly\nimproved by combining the A-net deep learning network with the DWDC algorithm\nbased on degradation model. Utilizing the DWDC algorithm to construct new\ndatasets and taking advantage of A-net neural network's features (i.e.,\nconsiderably fewer layers), we successfully removed the noise and flocculent\nstructures, which originally interfere with the cellular structure in the raw\nimage, and improved the spatial resolution by 10 times using relatively small\ndataset. We, therefore, conclude that the proposed algorithm that combines\nA-net neural network with the DWDC method is a suitable and universal approach\nfor exacting structural details of biomolecules, cells and organs from\nlow-resolution images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_H/0/1/0/all/0/1\">Haoxin Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Che_B/0/1/0/all/0/1\">Bingchen Che</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyun Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1\">Kaige Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_J/0/1/0/all/0/1\">Jintao Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watermarking Images in Self-Supervised Latent Spaces. (arXiv:2112.09581v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09581","description":"<p>We revisit watermarking techniques based on pre-trained deep networks, in the\nlight of self-supervised approaches. We present a way to embed both marks and\nbinary messages into their latent spaces, leveraging data augmentation at\nmarking time. Our method can operate at any resolution and creates watermarks\nrobust to a broad range of transformations (rotations, crops, JPEG, contrast,\netc). It significantly outperforms the previous zero-bit methods, and its\nperformance on multi-bit watermarking is on par with state-of-the-art\nencoder-decoder architectures trained end-to-end for watermarking. Our\nimplementation and models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1\">Pierre Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sablayrolles_A/0/1/0/all/0/1\">Alexandre Sablayrolles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1\">Teddy Furon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align and Prompt: Video-and-Language Pre-training with Entity Prompts. (arXiv:2112.09583v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09583","description":"<p>Video-and-language pre-training has shown promising improvements on various\ndownstream tasks. Most previous methods capture cross-modal interactions with a\ntransformer-based multimodal encoder, not fully addressing the misalignment\nbetween unimodal video and text features. Besides, learning fine-grained\nvisual-language alignment usually requires off-the-shelf object detectors to\nprovide object information, which is bottlenecked by the detector's limited\nvocabulary and expensive computation cost.\n</p>\n<p>We propose Align and Prompt: an efficient and effective video-and-language\npre-training framework with better cross-modal alignment. First, we introduce a\nvideo-text contrastive (VTC) loss to align unimodal video-text features at the\ninstance level, which eases the modeling of cross-modal interactions. Then, we\npropose a new visually-grounded pre-training task, prompting entity modeling\n(PEM), which aims to learn fine-grained region-entity alignment. To achieve\nthis, we first introduce an entity prompter module, which is trained with VTC\nto produce the similarity between a video crop and text prompts instantiated\nwith entity names. The PEM task then asks the model to predict the entity\npseudo-labels (i.e~normalized similarity scores) for randomly-selected video\ncrops. The resulting pre-trained model achieves state-of-the-art performance on\nboth text-video retrieval and videoQA, outperforming prior work by a\nsubstantial margin. Our code and pre-trained models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1\">Juan Carlos Niebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global explainability in aligned image modalities. (arXiv:2112.09591v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09591","description":"<p>Deep learning (DL) models are very effective on many computer vision problems\nand increasingly used in critical applications. They are also inherently black\nbox. A number of methods exist to generate image-wise explanations that allow\npractitioners to understand and verify model predictions for a given image.\nBeyond that, it would be desirable to validate that a DL model\n\\textit{generally} works in a sensible way, i.e. consistent with domain\nknowledge and not relying on undesirable data artefacts. For this purpose, the\nmodel needs to be explained globally. In this work, we focus on image\nmodalities that are naturally aligned such that each pixel position represents\na similar relative position on the imaged object, as is common in medical\nimaging. We propose the pixel-wise aggregation of image-wise explanations as a\nsimple method to obtain label-wise and overall global explanations. These can\nthen be used for model validation, knowledge discovery, and as an efficient way\nto communicate qualitative conclusions drawn from inspecting image-wise\nexplanations. We further propose Progressive Erasing Plus Progressive\nRestoration (PEPPR) as a method to quantitatively validate that these global\nexplanations are faithful to how the model makes its predictions. We then apply\nthese methods to ultra-widefield retinal images, a naturally aligned modality.\nWe find that the global explanations are consistent with domain knowledge and\nfaithfully reflect the model's workings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_J/0/1/0/all/0/1\">Justin Engelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernabeu_M/0/1/0/all/0/1\">Miguel O. Bernabeu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Deep Learning-based 6D Bin Pose Estimation in 3D Scans. (arXiv:2112.09598v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09598","description":"<p>An automated robotic system needs to be as robust as possible and fail-safe\nin general while having relatively high precision and repeatability. Although\ndeep learning-based methods are becoming research standard on how to approach\n3D scan and image processing tasks, the industry standard for processing this\ndata is still analytically-based. Our paper claims that analytical methods are\nless robust and harder for testing, updating, and maintaining. This paper\nfocuses on a specific task of 6D pose estimation of a bin in 3D scans.\nTherefore, we present a high-quality dataset composed of synthetic data and\nreal scans captured by a structured-light scanner with precise annotations.\nAdditionally, we propose two different methods for 6D bin pose estimation, an\nanalytical method as the industrial standard and a baseline data-driven method.\nBoth approaches are cross-evaluated, and our experiments show that augmenting\nthe training on real scans with synthetic data improves our proposed\ndata-driven neural model. This position paper is preliminary, as proposed\nmethods are trained and evaluated on a relatively small initial dataset which\nwe plan to extend in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gajdosech_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Gajdo&#x161;ech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocur_V/0/1/0/all/0/1\">Viktor Kocur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuchlik_M/0/1/0/all/0/1\">Martin Stuchl&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudec_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Hudec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaras_M/0/1/0/all/0/1\">Martin Madaras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local contrastive loss with pseudo-label based self-training for semi-supervised medical image segmentation. (arXiv:2112.09645v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09645","description":"<p>Supervised deep learning-based methods yield accurate results for medical\nimage segmentation. However, they require large labeled datasets for this, and\nobtaining them is a laborious task that requires clinical expertise.\nSemi/self-supervised learning-based approaches address this limitation by\nexploiting unlabeled data along with limited annotated data. Recent\nself-supervised learning methods use contrastive loss to learn good global\nlevel representations from unlabeled images and achieve high performance in\nclassification tasks on popular natural image datasets like ImageNet. In\npixel-level prediction tasks such as segmentation, it is crucial to also learn\ngood local level representations along with global representations to achieve\nbetter accuracy. However, the impact of the existing local contrastive\nloss-based methods remains limited for learning good local representations\nbecause similar and dissimilar local regions are defined based on random\naugmentations and spatial proximity; not based on the semantic label of local\nregions due to lack of large-scale expert annotations in the\nsemi/self-supervised setting. In this paper, we propose a local contrastive\nloss to learn good pixel level features useful for segmentation by exploiting\nsemantic label information obtained from pseudo-labels of unlabeled images\nalongside limited annotated images. In particular, we define the proposed loss\nto encourage similar representations for the pixels that have the same\npseudo-label/ label while being dissimilar to the representation of pixels with\ndifferent pseudo-label/label in the dataset. We perform pseudo-label based\nself-training and train the network by jointly optimizing the proposed\ncontrastive loss on both labeled and unlabeled sets and segmentation loss on\nonly the limited labeled set. We evaluated on three public cardiac and prostate\ndatasets, and obtain high segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaitanya_K/0/1/0/all/0/1\">Krishna Chaitanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1\">Ertunc Erdil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karani_N/0/1/0/all/0/1\">Neerav Karani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-Based Reconstruction of the Trajectories Performed by Skiers. (arXiv:2112.09647v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09647","description":"<p>Trajectories are fundamental in different skiing disciplines. Tools enabling\nthe analysis of such curves can enhance the training activity and enrich the\nbroadcasting contents. However, the solutions currently available are based on\ngeo-localized sensors and surface models. In this short paper, we propose a\nvideo-based approach to reconstruct the sequence of points traversed by an\nathlete during its performance. Our prototype is constituted by a pipeline of\ndeep learning-based algorithms to reconstruct the athlete's motion and to\nvisualize it according to the camera perspective. This is achieved for\ndifferent skiing disciplines in the wild without any camera calibration. We\ntested our solution on broadcast and smartphone-captured videos of alpine\nskiing and ski jumping professional competitions. The qualitative results\nachieved show the potential of our solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunnhofer_M/0/1/0/all/0/1\">Matteo Dunnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zurini_A/0/1/0/all/0/1\">Alberto Zurini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunnhofer_M/0/1/0/all/0/1\">Maurizio Dunnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micheloni_C/0/1/0/all/0/1\">Christian Micheloni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving neural implicit surfaces geometry with patch warping. (arXiv:2112.09648v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09648","description":"<p>Neural implicit surfaces have become an important technique for multi-view 3D\nreconstruction but their accuracy remains limited. In this paper, we argue that\nthis comes from the difficulty to learn and render high frequency textures with\nneural networks. We thus propose to add to the standard neural rendering\noptimization a direct photo-consistency term across the different views.\nIntuitively, we optimize the implicit geometry so that it warps views on each\nother in a consistent way. We demonstrate that two elements are key to the\nsuccess of such an approach: (i) warping entire patches, using the predicted\noccupancy and normals of the 3D points along each ray, and measuring their\nsimilarity with a robust structural similarity (SSIM); (ii) handling visibility\nand occlusion in such a way that incorrect warps are not given too much\nimportance while encouraging a reconstruction as complete as possible. We\nevaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL\nbenchmarks and show it outperforms state of the art unsupervised implicit\nsurfaces reconstructions by over 20% on both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darmon_F/0/1/0/all/0/1\">Fran&#xe7;ois Darmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bascle_B/0/1/0/all/0/1\">B&#xe9;n&#xe9;dicte Bascle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devaux_J/0/1/0/all/0/1\">Jean-Cl&#xe9;ment Devaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monasse_P/0/1/0/all/0/1\">Pascal Monasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information-theoretic stochastic contrastive conditional GAN: InfoSCC-GAN. (arXiv:2112.09653v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09653","description":"<p>Conditional generation is a subclass of generative problems where the output\nof the generation is conditioned by the attribute information. In this paper,\nwe present a stochastic contrastive conditional generative adversarial network\n(InfoSCC-GAN) with an explorable latent space. The InfoSCC-GAN architecture is\nbased on an unsupervised contrastive encoder built on the InfoNCE paradigm, an\nattribute classifier and an EigenGAN generator. We propose a novel training\nmethod, based on generator regularization using external or internal attributes\nevery $n$-th iteration, using a pre-trained contrastive encoder and a\npre-trained classifier. The proposed InfoSCC-GAN is derived based on an\ninformation-theoretic formulation of mutual information maximization between\ninput data and latent space representation as well as latent space and\ngenerated data. Thus, we demonstrate a link between the training objective\nfunctions and the above information-theoretic formulation. The experimental\nresults show that InfoSCC-GAN outperforms the \"vanilla\" EigenGAN in the image\ngeneration on AFHQ and CelebA datasets. In addition, we investigate the impact\nof discriminator architectures and loss functions by performing ablation\nstudies. Finally, we demonstrate that thanks to the EigenGAN generator, the\nproposed framework enjoys a stochastic generation in contrast to vanilla\ndeterministic GANs yet with the independent training of encoder, classifier,\nand generator in contrast to existing frameworks. Code, experimental results,\nand demos are available online at https://github.com/vkinakh/InfoSCC-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kinakh_V/0/1/0/all/0/1\">Vitaliy Kinakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdova_M/0/1/0/all/0/1\">Mariia Drozdova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quetant_G/0/1/0/all/0/1\">Guillaume Qu&#xe9;tant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golling_T/0/1/0/all/0/1\">Tobias Golling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1\">Slava Voloshynovskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastSurferVINN: Building Resolution-Independence into Deep Learning Segmentation Methods -- A Solution for HighRes Brain MRI. (arXiv:2112.09654v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09654","description":"<p>Leading neuroimaging studies have pushed 3T MRI acquisition resolutions below\n1.0 mm for improved structure definition and morphometry. Yet, only few,\ntime-intensive automated image analysis pipelines have been validated for\nhigh-resolution (HiRes) settings. Efficient deep learning approaches, on the\nother hand, rarely support more than one fixed resolution (usually 1.0 mm).\nFurthermore, the lack of a standard submillimeter resolution as well as limited\navailability of diverse HiRes data with sufficient coverage of scanner, age,\ndiseases, or genetic variance poses additional, unsolved challenges for\ntraining HiRes networks. Incorporating resolution-independence into deep\nlearning-based segmentation, i.e., the ability to segment images at their\nnative resolution across a range of different voxel sizes, promises to overcome\nthese challenges, yet no such approach currently exists. We now fill this gap\nby introducing a Voxelsize Independent Neural Network (VINN) for\nresolution-independent segmentation tasks and present FastSurferVINN, which (i)\nestablishes and implements resolution-independence for deep learning as the\nfirst method simultaneously supporting 0.7-1.0 mm whole brain segmentation,\n(ii) significantly outperforms state-of-the-art methods across resolutions, and\n(iii) mitigates the data imbalance problem present in HiRes datasets. Overall,\ninternal resolution-independence mutually benefits both HiRes and 1.0 mm MRI\nsegmentation. With our rigorously validated FastSurferVINN we distribute a\nrapid tool for morphometric neuroimage analysis. The VINN architecture,\nfurthermore, represents an efficient resolution-independent segmentation method\nfor wider application\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Henschel_L/0/1/0/all/0/1\">Leonie Henschel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kugler_D/0/1/0/all/0/1\">David K&#xfc;gler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-Assisted Verification of Biometric Data Collection. (arXiv:2112.09660v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09660","description":"<p>Recognizing actions from a video feed is a challenging task to automate,\nespecially so on older hardware. There are two aims for this project: one is to\nrecognize an action from the front-facing camera on an Android phone, the other\nis to support as many phones and Android versions as possible. This limits us\nto using models that are small enough to run on mobile phones with and without\nGPUs, and only using the camera feed to recognize the action. In this paper we\ncompare performance of the YOLO architecture across devices (with and without\ndedicated GPUs) using models trained on a custom dataset. We also discuss\nlimitations in recognizing faces and actions from video on limited hardware.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lindsey_R/0/1/0/all/0/1\">Ryan Lindsey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards More Effective PRM-based Crowd Counting via A Multi-resolution Fusion and Attention Network. (arXiv:2112.09664v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09664","description":"<p>The paper focuses on improving the recent plug-and-play patch rescaling\nmodule (PRM) based approaches for crowd counting. In order to make full use of\nthe PRM potential and obtain more reliable and accurate results for challenging\nimages with crowd-variation, large perspective, extreme occlusions, and\ncluttered background regions, we propose a new PRM based multi-resolution and\nmulti-task crowd counting network by exploiting the PRM module with more\neffectiveness and potency. The proposed model consists of three deep-layered\nbranches with each branch generating feature maps of different resolutions.\nThese branches perform a feature-level fusion across each other to build the\nvital collective knowledge to be used for the final crowd estimate.\nAdditionally, early-stage feature maps undergo visual attention to strengthen\nthe later-stage channels understanding of the foreground regions. The\nintegration of these deep branches with the PRM module and the early-attended\nblocks proves to be more effective than the original PRM based schemes through\nextensive numerical and visual evaluations on four benchmark datasets. The\nproposed approach yields a significant improvement by a margin of 12.6% in\nterms of the RMSE evaluation criterion. It also outperforms state-of-the-art\nmethods in cross-dataset evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajid_U/0/1/0/all/0/1\">Usman Sajid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Spatiotemporal Modeling of Urbanization. (arXiv:2112.09668v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09668","description":"<p>Urbanization has a strong impact on the health and wellbeing of populations\nacross the world. Predictive spatial modeling of urbanization therefore can be\na useful tool for effective public health planning. Many spatial urbanization\nmodels have been developed using classic machine learning and numerical\nmodeling techniques. However, deep learning with its proven capacity to capture\ncomplex spatiotemporal phenomena has not been applied to urbanization modeling.\nHere we explore the capacity of deep spatial learning for the predictive\nmodeling of urbanization. We treat numerical geospatial data as images with\npixels and channels, and enrich the dataset by augmentation, in order to\nleverage the high capacity of deep learning. Our resulting model can generate\nend-to-end multi-variable urbanization predictions, and outperforms a\nstate-of-the-art classic machine learning urbanization model in preliminary\ncomparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuromorphic Camera Denoising using Graph Neural Network-driven Transformers. (arXiv:2112.09685v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09685","description":"<p>Neuromorphic vision is a bio-inspired technology that has triggered a\nparadigm shift in the computer-vision community and is serving as a key-enabler\nfor a multitude of applications. This technology has offered significant\nadvantages including reduced power consumption, reduced processing needs, and\ncommunication speed-ups. However, neuromorphic cameras suffer from significant\namounts of measurement noise. This noise deteriorates the performance of\nneuromorphic event-based perception and navigation algorithms. In this paper,\nwe propose a novel noise filtration algorithm to eliminate events which do not\nrepresent real log-intensity variations in the observed scene. We employ a\nGraph Neural Network (GNN)-driven transformer algorithm, called\nGNN-Transformer, to classify every active event pixel in the raw stream into\nreal-log intensity variation or noise. Within the GNN, a message-passing\nframework, called EventConv, is carried out to reflect the spatiotemporal\ncorrelation among the events, while preserving their asynchronous nature. We\nalso introduce the Known-object Ground-Truth Labeling (KoGTL) approach for\ngenerating approximate ground truth labels of event streams under various\nillumination conditions. KoGTL is used to generate labeled datasets, from\nexperiments recorded in challenging lighting conditions. These datasets are\nused to train and extensively test our proposed algorithm. When tested on\nunseen datasets, the proposed algorithm outperforms existing methods by 12% in\nterms of filtration accuracy. Additional tests are also conducted on publicly\navailable datasets to demonstrate the generalization capabilities of the\nproposed algorithm in the presence of illumination variations and different\nmotion dynamics. Compared to existing solutions, qualitative results verified\nthe superior capability of the proposed algorithm to eliminate noise while\npreserving meaningful scene events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkendi_Y/0/1/0/all/0/1\">Yusra Alkendi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azzam_R/0/1/0/all/0/1\">Rana Azzam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayyad_A/0/1/0/all/0/1\">Abdulla Ayyad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Sajid Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_L/0/1/0/all/0/1\">Lakmal Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zweiri_Y/0/1/0/all/0/1\">Yahya Zweiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Visual Tracking with Exemplar Transformers. (arXiv:2112.09686v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09686","description":"<p>The design of more complex and powerful neural network models has\nsignificantly advanced the state-of-the-art in visual object tracking. These\nadvances can be attributed to deeper networks, or to the introduction of new\nbuilding blocks, such as transformers. However, in the pursuit of increased\ntracking performance, efficient tracking architectures have received\nsurprisingly little attention. In this paper, we introduce the Exemplar\nTransformer, an efficient transformer for real-time visual object tracking.\nE.T.Track, our visual tracker that incorporates Exemplar Transformer layers,\nruns at 47 fps on a CPU. This is up to 8 times faster than other\ntransformer-based models, making it the only real-time transformer-based\ntracker. When compared to lightweight trackers that can operate in real-time on\nstandard CPUs, E.T.Track consistently outperforms all other methods on the\nLaSOT, OTB-100, NFS, TrackingNet and VOT-ST2020 datasets. The code will soon be\nreleased on https://github.com/visionml/pytracking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blatter_P/0/1/0/all/0/1\">Philippe Blatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakis_M/0/1/0/all/0/1\">Menelaos Kanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Field Neural Rendering. (arXiv:2112.09687v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09687","description":"<p>Classical light field rendering for novel view synthesis can accurately\nreproduce view-dependent effects such as reflection, refraction, and\ntranslucency, but requires a dense view sampling of the scene. Methods based on\ngeometric reconstruction need only sparse views, but cannot accurately model\nnon-Lambertian effects. We introduce a model that combines the strengths and\nmitigates the limitations of these two directions. By operating on a\nfour-dimensional representation of the light field, our model learns to\nrepresent view-dependent effects accurately. By enforcing geometric constraints\nduring training and inference, the scene geometry is implicitly learned from a\nsparse set of views. Concretely, we introduce a two-stage transformer-based\nmodel that first aggregates features along epipolar lines, then aggregates\nfeatures along reference views to produce the color of a target ray. Our model\noutperforms the state-of-the-art on multiple forward-facing and 360{\\deg}\ndatasets, with larger margins on scenes with severe view-dependent variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suhail_M/0/1/0/all/0/1\">Mohammed Suhail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1\">Carlos Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1\">Ameesh Makadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Model Pseudo-Labeling for Semi-Supervised Action Recognition. (arXiv:2112.09690v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09690","description":"<p>Semi-supervised action recognition is a challenging but important task due to\nthe high cost of data annotation. A common approach to this problem is to\nassign unlabeled data with pseudo-labels, which are then used as additional\nsupervision in training. Typically in recent work, the pseudo-labels are\nobtained by training a model on the labeled data, and then using confident\npredictions from the model to teach itself. In this work, we propose a more\neffective pseudo-labeling scheme, called Cross-Model Pseudo-Labeling (CMPL).\nConcretely, we introduce a lightweight auxiliary network in addition to the\nprimary backbone, and ask them to predict pseudo-labels for each other. We\nobserve that, due to their different structural biases, these two models tend\nto learn complementary representations from the same video clips. Each model\ncan thus benefit from its counterpart by utilizing cross-model predictions as\nsupervision. Experiments on different data partition protocols demonstrate the\nsignificant improvement of our framework over existing alternatives. For\nexample, CMPL achieves $17.6\\%$ and $25.1\\%$ Top-1 accuracy on Kinetics-400 and\nUCF-101 using only the RGB modality and $1\\%$ labeled data, outperforming our\nbaseline model, FixMatch, by $9.0\\%$ and $10.3\\%$, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Ceyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Un-Mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning. (arXiv:2003.05438v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.05438","description":"<p>The recently advanced unsupervised learning approaches use the siamese-like\nframework to compare two \"views\" from the same image for learning\nrepresentations. Making the two views distinctive is a core to guarantee that\nunsupervised methods can learn meaningful information. However, such frameworks\nare sometimes fragile on overfitting if the augmentations used for generating\ntwo views are not strong enough, causing the over-confident issue on the\ntraining data. This drawback hinders the model from learning subtle variance\nand fine-grained information. To address this, in this work we aim to involve\nthe distance concept on label space in the unsupervised learning and let the\nmodel be aware of the soft degree of similarity between positive or negative\npairs through mixing the input data space, to further work collaboratively for\nthe input and loss spaces. Despite its conceptual simplicity, we show\nempirically that with the solution -- Unsupervised image mixtures (Un-Mix), we\ncan learn subtler, more robust and generalized representations from the\ntransformed input and corresponding new label space. Extensive experiments are\nconducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet\nwith popular unsupervised methods SimCLR, BYOL, MoCo V1&amp;V2, SwAV, etc. Our\nproposed image mixture and label assignment strategy can obtain consistent\nimprovement by 1~3% following exactly the same hyperparameters and training\nprocedures of the base methods. Code is publicly available at\nhttps://github.com/szq0214/Un-Mix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DTVNet+: A High-Resolution Scenic Dataset for Dynamic Time-lapse Video Generation. (arXiv:2008.04776v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.04776","description":"<p>This paper presents a novel end-to-end dynamic time-lapse video generation\nframework, named DTVNet, to generate diversified time-lapse videos from a\nsingle landscape image conditioned on normalized motion vectors. The proposed\nDTVNet consists of two submodules: \\emph{Optical Flow Encoder} (OFE) and\n\\emph{Dynamic Video Generator} (DVG). The OFE maps a sequence of optical flow\nmaps to a \\emph{normalized motion vector} that encodes the motion information\nof the generated video. The DVG contains motion and content streams to learn\nfrom the motion vector and the single landscape image. Besides, it contains an\nencoder to learn shared content features and a decoder to construct video\nframes with corresponding motion. Specifically, the \\emph{motion stream}\nintroduces multiple \\emph{adaptive instance normalization} (AdaIN) layers to\nintegrate multi-level motion information for controlling the object motion. In\nthe testing stage, videos with the same content but various motion information\ncan be generated by different \\emph{normalized motion vectors} based on only\none input image. Also, we propose a high-resolution scenic time-lapse video\ndataset, named Quick-Sky-Time, to evaluate different approaches, which can be\nviewed as a new benchmark for high-quality scenic image and video generation\ntasks. We further conduct experiments on Sky Time-lapse, Beach, and\nQuick-Sky-Time datasets. The results demonstrate the superiority of our\napproach over state-of-the-art methods for generating high-quality and various\ndynamic videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yunliang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04408","description":"<p>Different environments pose a great challenge to the outdoor robust visual\nperception for long-term autonomous driving and the generalization of\nlearning-based algorithms on different environmental effects is still an open\nproblem. Although monocular depth prediction has been well studied recently,\nthere is few work focusing on the robust learning-based depth prediction across\ndifferent environments, e.g. changing illumination and seasons, owing to the\nlack of such a multi-environment real-world dataset and benchmark. To this end,\nthe first cross-season monocular depth prediction dataset and benchmark\nSeasonDepth is built based on CMU Visual Localization dataset. To benchmark the\ndepth estimation performance under different environments, we investigate\nrepresentative and recent state-of-the-art open-source supervised,\nself-supervised and domain adaptation depth prediction methods from KITTI\nbenchmark using several newly-formulated metrics. Through extensive\nexperimental evaluation on the proposed dataset, the influence of multiple\nenvironments on performance and robustness is analyzed qualitatively and\nquantitatively, showing that the long-term monocular depth prediction is still\nchallenging even with fine-tuning. We further give promising avenues that\nself-supervised training and stereo geometry constraint help to enhance the\nrobustness to changing environments. The dataset is available on\nhttps://seasondepth.github.io, and benchmark toolkit is available on\nhttps://github.com/SeasonDepth/SeasonDepth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Baoquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhijian Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Ding Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobileSal: Extremely Efficient RGB-D Salient Object Detection. (arXiv:2012.13095v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13095","description":"<p>The high computational cost of neural networks has prevented recent successes\nin RGB-D salient object detection (SOD) from benefiting real-world\napplications. Hence, this paper introduces a novel network, MobileSal, which\nfocuses on efficient RGB-D SOD using mobile networks for deep feature\nextraction. However, mobile networks are less powerful in feature\nrepresentation than cumbersome networks. To this end, we observe that the depth\ninformation of color images can strengthen the feature representation related\nto SOD if leveraged properly. Therefore, we propose an implicit depth\nrestoration (IDR) technique to strengthen the mobile networks' feature\nrepresentation capability for RGB-D SOD. IDR is only adopted in the training\nphase and is omitted during testing, so it is computationally free. Besides, we\npropose compact pyramid refinement (CPR) for efficient multi-level feature\naggregation to derive salient objects with clear boundaries. With IDR and CPR\nincorporated, MobileSal performs favorably against state-of-the-art methods on\nsix challenging RGB-D SOD datasets with much faster speed (450fps for the input\nsize of 320 $\\times$ 320) and fewer parameters (6.5M). The code is released at\nhttps://mmcheng.net/mobilesal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jia-Wang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu-Chao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JigsawGAN: Auxiliary Learning for Solving Jigsaw Puzzles with Generative Adversarial Networks. (arXiv:2101.07555v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07555","description":"<p>The paper proposes a solution based on Generative Adversarial Network (GAN)\nfor solving jigsaw puzzles. The problem assumes that an image is divided into\nequal square pieces, and asks to recover the image according to information\nprovided by the pieces. Conventional jigsaw puzzle solvers often determine the\nrelationships based on the boundaries of pieces, which ignore the important\nsemantic information. In this paper, we propose JigsawGAN, a GAN-based\nauxiliary learning method for solving jigsaw puzzles with unpaired images (with\nno prior knowledge of the initial images). We design a multi-task pipeline that\nincludes, (1) a classification branch to classify jigsaw permutations, and (2)\na GAN branch to recover features to images in correct orders. The\nclassification branch is constrained by the pseudo-labels generated according\nto the shuffled pieces. The GAN branch concentrates on the image semantic\ninformation, where the generator produces the natural images to fool the\ndiscriminator, while the discriminator distinguishes whether a given image\nbelongs to the synthesized or the real target domain. These two branches are\nconnected by a flow-based warp module that is applied to warp features to\ncorrect the order according to the classification results. The proposed method\ncan solve jigsaw puzzles more efficiently by utilizing both semantic\ninformation and boundary information simultaneously. Qualitative and\nquantitative comparisons against several representative jigsaw puzzle solvers\ndemonstrate the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral decoupling allows training transferable neural networks in medical imaging. (arXiv:2103.17171v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.17171","description":"<p>Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n</p>\n<p>To address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n</p>\n<p>We show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations and increases networks' robustness for data\ndistribution shifts. To validate our findings, we train networks with and\nwithout spectral decoupling to detect prostate cancer tissue slides and\nCOVID-19 in chest radiographs. Networks trained with spectral decoupling\nachieve up to 9.5 percent point higher performance on external datasets.\n</p>\n<p>Our results show that spectral decoupling helps with generalisation issues\nassociated with neural networks, and can be used to complement or replace\ncomputationally expensive explicit bias mitigation methods, such as stain\nnormalization in histological images. We recommend using spectral decoupling as\nan implicit bias mitigation method in any neural network intended for clinical\nuse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pohjonen_J/0/1/0/all/0/1\">Joona Pohjonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sturenberg_C/0/1/0/all/0/1\">Carolin St&#xfc;renberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rannikko_A/0/1/0/all/0/1\">Antti Rannikko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirtti_T/0/1/0/all/0/1\">Tuomas Mirtti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pitkanen_E/0/1/0/all/0/1\">Esa Pitk&#xe4;nen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Against Image Corruptions Through Adversarial Augmentations. (arXiv:2104.01086v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01086","description":"<p>Modern neural networks excel at image classification, yet they remain\nvulnerable to common image corruptions such as blur, speckle noise or fog.\nRecent methods that focus on this problem, such as AugMix and DeepAugment,\nintroduce defenses that operate in expectation over a distribution of image\ncorruptions. In contrast, the literature on $\\ell_p$-norm bounded perturbations\nfocuses on defenses against worst-case corruptions. In this work, we reconcile\nboth approaches by proposing AdversarialAugment, a technique which optimizes\nthe parameters of image-to-image models to generate adversarially corrupted\naugmented images. We theoretically motivate our method and give sufficient\nconditions for the consistency of its idealized version as well as that of\nDeepAugment. Our classifiers improve upon the state-of-the-art on common image\ncorruption benchmarks conducted in expectation on CIFAR-10-C and improve\nworst-case performance against $\\ell_p$-norm bounded perturbations on both\nCIFAR-10 and ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calian_D/0/1/0/all/0/1\">Dan A. Calian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stimberg_F/0/1/0/all/0/1\">Florian Stimberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiles_O/0/1/0/all/0/1\">Olivia Wiles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebuffi_S/0/1/0/all/0/1\">Sylvestre-Alvise Rebuffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1\">Andras Gyorgy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mann_T/0/1/0/all/0/1\">Timothy Mann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1\">Sven Gowal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.03736","description":"<p>Meta-learning can extract an inductive bias from previous learning experience\nand assist the training of new tasks. It is often realized through optimizing a\nmeta-model with the evaluation loss of task-specific solvers. Most existing\nalgorithms sample non-overlapping $\\mathit{support}$ sets and $\\mathit{query}$\nsets to train and evaluate the solvers respectively due to simplicity\n($\\mathcal{S}$/$\\mathcal{Q}$ protocol). Different from\n$\\mathcal{S}$/$\\mathcal{Q}$ protocol, we can also evaluate a task-specific\nsolver by comparing it to a target model $\\mathcal{T}$, which is the optimal\nmodel for this task or a model that behaves well enough on this task\n($\\mathcal{S}$/$\\mathcal{T}$ protocol). Although being short of research,\n$\\mathcal{S}$/$\\mathcal{T}$ protocol has unique advantages such as offering\nmore informative supervision, but it is computationally expensive. This paper\nlooks into this special evaluation method and takes a step towards putting it\ninto practice. We find that with a small ratio of tasks armed with target\nmodels, classic meta-learning algorithms can be improved a lot without\nconsuming many resources. We empirically verify the effectiveness of\n$\\mathcal{S}$/$\\mathcal{T}$ protocol in a typical application of meta-learning,\n$\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models\nby fine-tuning the pre-trained network on those hard tasks, we match the\ntask-specific solvers and target models via knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Su Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Le Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07719","description":"<p>Few-shot object detection (FSOD) aims to detect objects using only a few\nexamples. How to adapt state-of-the-art object detectors to the few-shot domain\nremains challenging. Object proposal is a key ingredient in modern object\ndetectors. However, the quality of proposals generated for few-shot classes\nusing existing methods is far worse than that of many-shot classes, e.g.,\nmissing boxes for few-shot classes due to misclassification or inaccurate\nspatial locations with respect to true objects. To address the noisy proposal\nproblem, we propose a novel meta-learning based FSOD model by jointly\noptimizing the few-shot proposal generation and fine-grained few-shot proposal\nclassification. To improve proposal generation for few-shot classes, we propose\nto learn a lightweight metric-learning based prototype matching network,\ninstead of the conventional simple linear object/nonobject classifier, e.g.,\nused in RPN. Our non-linear classifier with the feature fusion network could\nimprove the discriminative prototype matching and the proposal recall for\nfew-shot classes. To improve the fine-grained few-shot proposal classification,\nwe propose a novel attentive feature alignment method to address the spatial\nmisalignment between the noisy proposals and few-shot classes, thus improving\nthe performance of few-shot object detection. Meanwhile we learn a separate\nFaster R-CNN detection head for many-shot base classes and show strong\nperformance of maintaining base-classes knowledge. Our model achieves\nstate-of-the-art performance on multiple FSOD benchmarks over most of the shots\nand metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention and Prediction Guided Motion Detection for Low-Contrast Small Moving Targets. (arXiv:2104.13018v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13018","description":"<p>Small target motion detection within complex natural environments is an\nextremely challenging task for autonomous robots. Surprisingly, the visual\nsystems of insects have evolved to be highly efficient in detecting mates and\ntracking prey, even though targets occupy as small as a few degrees of their\nvisual fields. The excellent sensitivity to small target motion relies on a\nclass of specialized neurons called small target motion detectors (STMDs).\nHowever, existing STMD-based models are heavily dependent on visual contrast\nand perform poorly in complex natural environments where small targets\ngenerally exhibit extremely low contrast against neighbouring backgrounds. In\nthis paper, we develop an attention and prediction guided visual system to\novercome this limitation. The developed visual system comprises three main\nsubsystems, namely, an attention module, an STMD-based neural network, and a\nprediction module. The attention module searches for potential small targets in\nthe predicted areas of the input image and enhances their contrast against\ncomplex background. The STMD-based neural network receives the\ncontrast-enhanced image and discriminates small moving targets from background\nfalse positives. The prediction module foresees future positions of the\ndetected targets and generates a prediction map for the attention module. The\nthree subsystems are connected in a recurrent architecture allowing information\nto be processed sequentially to activate specific areas for small target\ndetection. Extensive experiments on synthetic and real-world datasets\ndemonstrate the effectiveness and superiority of the proposed visual system for\ndetecting small, low-contrast moving targets against complex natural\nenvironments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiannan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huatian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jigen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shigang Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation. (arXiv:2105.07059v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07059","description":"<p>Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, manually annotating\nmedical data is often laborious, and most existing learning-based approaches\nfail to accurately delineate object boundaries without effective geometric\nconstraints. Contrastive learning, a sub-area of self-supervised learning, has\nrecently been noted as a promising direction in multiple application fields. In\nthis work, we present a novel Contrastive Voxel-wise Representation\nDistillation (CVRD) method with geometric constraints to learn global-local\nvisual representations for volumetric medical image segmentation with limited\nannotations. Our framework can effectively learn global and local features by\ncapturing 3D spatial context and rich anatomical information. Specifically, we\nintroduce a voxel-to-volume contrastive algorithm to learn global information\nfrom 3D images, and propose to perform local voxel-to-voxel distillation to\nexplicitly make use of local cues in the embedding space. Moreover, we\nintegrate an elastic interaction-based active contour model as a geometric\nregularization term to enable fast and reliable object delineations in an\nend-to-end learning manner. Results on the Atrial Segmentation Challenge\ndataset demonstrate superiority of our proposed scheme, especially in a setting\nwith a very limited number of annotated data. The code will be available at\nhttps://github.com/charlesyou999648/CVRD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACNet: Mask-Aware Attention with Dynamic Context Enhancement for Robust Acne Detection. (arXiv:2105.14891v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14891","description":"<p>Computer-aided diagnosis has recently received attention for its advantage of\nlow cost and time efficiency. Although deep learning played a major role in the\nrecent success of acne detection, there are still several challenges such as\ncolor shift by inconsistent illumination, variation in scales, and high density\ndistribution. To address these problems, we propose an acne detection network\nwhich consists of three components, specifically: Composite Feature Refinement,\nDynamic Context Enhancement, and Mask-Aware Multi-Attention. First, Composite\nFeature Refinement integrates semantic information and fine details to enrich\nfeature representation, which mitigates the adverse impact of imbalanced\nillumination. Then, Dynamic Context Enhancement controls different receptive\nfields of multi-scale features for context enhancement to handle scale\nvariation. Finally, Mask-Aware Multi-Attention detects densely arranged and\nsmall acne by suppressing uninformative regions and highlighting probable acne\nregions. Experiments are performed on acne image dataset ACNE04 and natural\nimage dataset PASCAL VOC 2007. We demonstrate how our method achieves the\nstate-of-the-art result on ACNE04 and competitive performance with previous\nstate-of-the-art methods on the PASCAL VOC 2007.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_K/0/1/0/all/0/1\">Kyungseo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gun-Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model. (arXiv:2105.15089v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.15089","description":"<p>Inspired by biological evolution, we explain the rationality of Vision\nTransformer by analogy with the proven practical Evolutionary Algorithm (EA)\nand derive that both of them have consistent mathematical representation.\nAnalogous to the dynamic local population in EA, we improve the existing\ntransformer structure and propose a more efficient EAT model, and design\ntask-related heads to deal with different tasks more flexibly. Moreover, we\nintroduce the spatial-filling curve into the current vision transformer to\nsequence image data into a uniform sequential format. Thus we can design a\nunified EAT framework to address multi-modal tasks, separating the network\narchitecture from the data format adaptation. Our approach achieves\nstate-of-the-art results on the ImageNet classification task compared with\nrecent vision transformer works while having smaller parameters and greater\nthroughput. We further conduct multi-modal tasks to demonstrate the superiority\nof the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves\nthe rank-1 by +3.7 points over the baseline on the CSS dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSI: Temporal Saliency Integration for Video Action Recognition. (arXiv:2106.01088v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01088","description":"<p>Efficient spatiotemporal modeling is an important yet challenging problem for\nvideo action recognition. Existing state-of-the-art methods exploit neighboring\nfeature differences to obtain motion clues for short-term temporal modeling\nwith a simple convolution. However, only one local convolution is incapable of\nhandling various kinds of actions because of the limited receptive field.\nBesides, action-irrelated noises brought by camera movement will also harm the\nquality of extracted motion features. In this paper, we propose a Temporal\nSaliency Integration (TSI) block, which mainly contains a Salient Motion\nExcitation (SME) module and a Cross-perception Temporal Integration (CTI)\nmodule. Specifically, SME aims to highlight the motion-sensitive area through\nspatial-level local-global motion modeling, where the saliency alignment and\npyramidal motion modeling are conducted successively between adjacent frames to\ncapture motion dynamics with fewer noises caused by misaligned background. CTI\nis designed to perform multi-perception temporal modeling through a group of\nseparate 1D convolutions respectively. Meanwhile, temporal interactions across\ndifferent perceptions are integrated with the attention mechanism. Through\nthese two modules, long short-term temporal relationships can be encoded\nefficiently by introducing limited additional parameters. Extensive experiments\nare conducted on several popular benchmarks (i.e., Something-Something V1 &amp; V2,\nKinetics-400, UCF-101, and HMDB-51), which demonstrate the effectiveness of our\nproposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Haisheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jinyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RegionViT: Regional-to-Local Attention for Vision Transformers. (arXiv:2106.02689v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02689","description":"<p>Vision transformer (ViT) has recently shown its strong capability in\nachieving comparable results to convolutional neural networks (CNNs) on image\nclassification. However, vanilla ViT simply inherits the same architecture from\nthe natural language processing directly, which is often not optimized for\nvision applications. Motivated by this, in this paper, we propose a new\narchitecture that adopts the pyramid structure and employ a novel\nregional-to-local attention rather than global self-attention in vision\ntransformers. More specifically, our model first generates regional tokens and\nlocal tokens from an image with different patch sizes, where each regional\ntoken is associated with a set of local tokens based on the spatial location.\nThe regional-to-local attention includes two steps: first, the regional\nself-attention extract global information among all regional tokens and then\nthe local self-attention exchanges the information among one regional token and\nthe associated local tokens via self-attention. Therefore, even though local\nself-attention confines the scope in a local region but it can still receive\nglobal information. Extensive experiments on four vision tasks, including image\nclassification, object and keypoint detection, semantics segmentation and\naction recognition, show that our approach outperforms or is on par with\nstate-of-the-art ViT variants including many concurrent works. Our source codes\nand models are available at https://github.com/ibm/regionvit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Quanfu Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v7 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2106.08208","description":"<p>Adaptive gradient methods have shown excellent performances for solving many\nmachine learning problems. Although multiple adaptive gradient methods were\nrecently studied, they mainly focus on either empirical or theoretical aspects\nand also only work for specific problems by using some specific adaptive\nlearning rates. Thus, it is desired to design a universal framework for\npractical algorithms of adaptive gradients with theoretical guarantee to solve\ngeneral problems. To fill this gap, we propose a faster and universal framework\nof adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive\nmatrix that includes most existing adaptive gradient forms. Moreover, our\nframework can flexibly integrate the momentum and variance reduced techniques.\nIn particular, our novel framework provides the convergence analysis support\nfor adaptive gradient methods under the nonconvex setting. In theoretical\nanalysis, we prove that our SUPER-ADAM algorithm can achieve the best known\ngradient (i.e., stochastic first-order oracle (SFO)) complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms. Code is available at\nhttps://github.com/LIJUNYI95/SuperAdam\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15282","description":"<p>We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation benchmark, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and\nclassification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,\noutperforming VQ-VAE-2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Semantic Segmentation using Psychometric Learning. (arXiv:2107.03212v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03212","description":"<p>Assigning meaning to parts of image data is the goal of semantic image\nsegmentation. Machine learning methods, specifically supervised learning is\ncommonly used in a variety of tasks formulated as semantic segmentation. One of\nthe major challenges in the supervised learning approaches is expressing and\ncollecting the rich knowledge that experts have with respect to the meaning\npresent in the image data. Towards this, typically a fixed set of labels is\nspecified and experts are tasked with annotating the pixels, patches or\nsegments in the images with the given labels. In general, however, the set of\nclasses does not fully capture the rich semantic information present in the\nimages. For example, in medical imaging such as histology images, the different\nparts of cells could be grouped and sub-grouped based on the expertise of the\npathologist.\n</p>\n<p>To achieve such a precise semantic representation of the concepts in the\nimage, we need access to the full depth of knowledge of the annotator. In this\nwork, we develop a novel approach to collect segmentation annotations from\nexperts based on psychometric testing. Our method consists of the psychometric\ntesting procedure, active query selection, query enhancement, and a deep metric\nlearning model to achieve a patch-level image embedding that allows for\nsemantic segmentation of images. We show the merits of our method with\nevaluation on the synthetically generated image, aerial image and histology\nimage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise-Resistant Deep Metric Learning with Probabilistic Instance Filtering. (arXiv:2108.01431v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.01431","description":"<p>Noisy labels are commonly found in real-world data, which cause performance\ndegradation of deep neural networks. Cleaning data manually is labour-intensive\nand time-consuming. Previous research mostly focuses on enhancing\nclassification models against noisy labels, while the robustness of deep metric\nlearning (DML) against noisy labels remains less well-explored. In this paper,\nwe bridge this important gap by proposing Probabilistic Ranking-based Instance\nSelection with Memory (PRISM) approach for DML. PRISM calculates the\nprobability of a label being clean, and filters out potentially noisy samples.\nSpecifically, we propose a novel method, namely the von Mises-Fisher\nDistribution Similarity (vMF-Sim), to calculate this probability by estimating\na von Mises-Fisher (vMF) distribution for each data class. Compared with the\nexisting average similarity method (AvgSim), vMF-Sim considers the variance of\neach class in addition to the average similarity. With such a design, the\nproposed approach can deal with challenging DML situations in which the\nmajority of the samples are noisy. Extensive experiments on both synthetic and\nreal-world noisy dataset show that the proposed approach achieves up to 8.37%\nhigher Precision@1 compared with the best performing state-of-the-art baseline\napproaches, within reasonable training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhanning Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lizhen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PnP-3D: A Plug-and-Play for 3D Point Clouds. (arXiv:2108.07378v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07378","description":"<p>With the help of the deep learning paradigm, many point cloud networks have\nbeen invented for visual analysis. However, there is great potential for\ndevelopment of these networks since the given information of point cloud data\nhas not been fully exploited. To improve the effectiveness of existing networks\nin analyzing point cloud data, we propose a plug-and-play module, PnP-3D,\naiming to refine the fundamental point cloud feature representations by\ninvolving more local context and global bilinear response from explicit 3D\nspace and implicit feature space. To thoroughly evaluate our approach, we\nconduct experiments on three standard point cloud analysis tasks, including\nclassification, semantic segmentation, and object detection, where we select\nthree state-of-the-art networks from each task for evaluation. Serving as a\nplug-and-play module, PnP-3D can significantly boost the performances of\nestablished networks. In addition to achieving state-of-the-art results on four\nwidely used point cloud benchmarks, we present comprehensive ablation studies\nand visualizations to demonstrate our approach's advantages. The code will be\navailable at https://github.com/ShiQiu0419/pnp-3d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1\">Shi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representing Shape Collections with Alignment-Aware Linear Models. (arXiv:2109.01605v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01605","description":"<p>In this paper, we revisit the classical representation of 3D point clouds as\nlinear shape models. Our key insight is to leverage deep learning to represent\na collection of shapes as affine transformations of low-dimensional linear\nshape models. Each linear model is characterized by a shape prototype, a\nlow-dimensional shape basis and two neural networks. The networks take as input\na point cloud and predict the coordinates of a shape in the linear basis and\nthe affine transformation which best approximate the input. Both linear models\nand neural networks are learned end-to-end using a single reconstruction loss.\nThe main advantage of our approach is that, in contrast to many recent deep\napproaches which learn feature-based complex shape representations, our model\nis explicit and every operation occurs in 3D space. As a result, our linear\nshape models can be easily visualized and annotated, and failure cases can be\nvisually understood. While our main goal is to introduce a compact and\ninterpretable representation of shape collections, we show it leads to state of\nthe art results for few-shot segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loiseau_R/0/1/0/all/0/1\">Romain Loiseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monnier_T/0/1/0/all/0/1\">Tom Monnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Lo&#xef;c Landrieu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wasserstein Patch Prior for Image Superresolution. (arXiv:2109.12880v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12880","description":"<p>In this paper, we introduce a Wasserstein patch prior for superresolution of\ntwo- and three-dimensional images. Here, we assume that we have given\n(additionally to the low resolution observation) a reference image which has a\nsimilar patch distribution as the ground truth of the reconstruction. This\nassumption is e.g. fulfilled when working with texture images or material data.\nThen, the proposed regularizer penalizes the $W_2$-distance of the patch\ndistribution of the reconstruction to the patch distribution of some reference\nimage at different scales. We demonstrate the performance of the proposed\nregularizer by two- and three-dimensional numerical examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hertrich_J/0/1/0/all/0/1\">Johannes Hertrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houdard_A/0/1/0/all/0/1\">Antoine Houdard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redenbach_C/0/1/0/all/0/1\">Claudia Redenbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Fractal Pre-training. (arXiv:2110.03091v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03091","description":"<p>The deep neural networks used in modern computer vision systems require\nenormous image datasets to train them. These carefully-curated datasets\ntypically have a million or more images, across a thousand or more distinct\ncategories. The process of creating and curating such a dataset is a monumental\nundertaking, demanding extensive effort and labelling expense and necessitating\ncareful navigation of technical and social issues such as label accuracy,\ncopyright ownership, and content bias.\n</p>\n<p>What if we had a way to harness the power of large image datasets but with\nfew or none of the major issues and concerns currently faced? This paper\nextends the recent work of Kataoka et. al. (2020), proposing an improved\npre-training dataset based on dynamically-generated fractal images. Challenging\nissues with large-scale image datasets become points of elegance for fractal\npre-training: perfect label accuracy at zero cost; no need to store/transmit\nlarge image archives; no privacy/demographic bias/concerns of inappropriate\ncontent, as no humans are pictured; limitless supply and diversity of images;\nand the images are free/open-source. Perhaps surprisingly, avoiding these\ndifficulties imposes only a small penalty in performance. Leveraging a\nnewly-proposed pre-training task -- multi-instance prediction -- our\nexperiments demonstrate that fine-tuning a network pre-trained using fractals\nattains 92.7-98.1% of the accuracy of an ImageNet pre-trained network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1\">Connor Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrell_R/0/1/0/all/0/1\">Ryan Farrell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Flow Estimation for Spiking Camera. (arXiv:2110.03916v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03916","description":"<p>As a bio-inspired sensor with high temporal resolution, the spiking camera\nhas an enormous potential in real applications, especially for motion\nestimation in high-speed scenes. However, frame-based and event-based methods\nare not well suited to spike streams from the spiking camera due to the\ndifferent data modalities. To this end, we present, SCFlow, a tailored deep\nlearning pipeline to estimate optical flow in high-speed scenes from spike\nstreams. Importantly, a novel input representation is introduced which can\nadaptively remove the motion blur in spike streams according to the prior\nmotion. Further, for training SCFlow, we synthesize two sets of optical flow\ndata for the spiking camera, SPIkingly Flying Things and Photo-realistic\nHigh-speed Motion, denoted as SPIFT and PHM respectively, corresponding to\nrandom high-speed and well-designed scenes. Experimental results show that the\nSCFlow can predict optical flow from spike streams in different high-speed\nscenes. Moreover, SCFlow shows promising generalization on \\real spike streams.\nAll codes and constructed datasets will be released after publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Ziluo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parametric Variational Linear Units (PVLUs) in Deep Convolutional Networks. (arXiv:2110.12246v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12246","description":"<p>The Rectified Linear Unit is currently a state-of-the-art activation function\nin deep convolutional neural networks. To combat ReLU's dying neuron problem,\nwe propose the Parametric Variational Linear Unit (PVLU), which adds a\nsinusoidal function with trainable coefficients to ReLU. Along with introducing\nnonlinearity and non-zero gradients across the entire real domain, PVLU acts as\na mechanism of fine-tuning when implemented in the context of transfer\nlearning. On a simple, non-transfer sequential CNN, PVLU substitution allowed\nfor relative error decreases of 16.3% and 11.3% (without and with data\naugmentation) on CIFAR-100. PVLU is also tested on transfer learning models.\nThe VGG-16 and VGG-19 models experience relative error reductions of 9.5% and\n10.7% on CIFAR-10, respectively, after the substitution of ReLU with PVLU. When\ntraining on Gaussian-filtered CIFAR-10 images, similar improvements are noted\nfor the VGG models. Most notably, fine-tuning using PVLU allows for relative\nerror reductions up to and exceeding 10% for near state-of-the-art residual\nneural network architectures on the CIFAR datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aarush Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1\">Shikhar Ahuja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13981","description":"<p>Filter pruning has been widely used for neural network compression because of\nits enabled practical acceleration. To date, most of the existing filter\npruning works explore the importance of filters via using intra-channel\ninformation. In this paper, starting from an inter-channel perspective, we\npropose to perform efficient filter pruning using Channel Independence, a\nmetric that measures the correlations among different feature maps. The less\nindependent feature map is interpreted as containing less useful\ninformation$/$knowledge, and hence its corresponding filter can be pruned\nwithout affecting model capacity. We systematically investigate the\nquantification metric, measuring scheme and sensitiveness$/$reliability of\nchannel independence in the context of filter pruning. Our evaluation results\nfor different models on various datasets show the superior performance of our\napproach. Notably, on CIFAR-10 dataset our solution can bring $0.75\\%$ and\n$0.94\\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models,\nrespectively, and meanwhile the model size and FLOPs are reduced by $42.8\\%$\nand $47.4\\%$ (for ResNet-56) and $48.3\\%$ and $52.1\\%$ (for ResNet-110),\nrespectively. On ImageNet dataset, our approach can achieve $40.8\\%$ and\n$44.8\\%$ storage and computation reductions, respectively, with $0.15\\%$\naccuracy increase over the baseline ResNet-50 model. The code is available at\nhttps://github.com/Eclipsess/CHIP_NeurIPS2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Miao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Huy Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonouz_S/0/1/0/all/0/1\">Saman Zonouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Object Detectors with Feature Richness. (arXiv:2111.00674v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00674","description":"<p>In recent years, large-scale deep models have achieved great success, but the\nhuge computational complexity and massive storage requirements make it a great\nchallenge to deploy them in resource-limited devices. As a model compression\nand acceleration method, knowledge distillation effectively improves the\nperformance of small models by transferring the dark knowledge from the teacher\ndetector. However, most of the existing distillation-based detection methods\nmainly imitating features near bounding boxes, which suffer from two\nlimitations. First, they ignore the beneficial features outside the bounding\nboxes. Second, these methods imitate some features which are mistakenly\nregarded as the background by the teacher detector. To address the above\nissues, we propose a novel Feature-Richness Score (FRS) method to choose\nimportant features that improve generalized detectability during distilling.\nThe proposed method effectively retrieves the important features outside the\nbounding boxes and removes the detrimental features within the bounding boxes.\nExtensive experiments show that our methods achieve excellent performance on\nboth anchor-based and anchor-free detectors. For example, RetinaNet with\nResNet-50 achieves 39.7% in mAP on the COCO2017 dataset, which even surpasses\nthe ResNet-101 based teacher detector 38.9% by 0.8%. Our implementation is\navailable at https://github.com/duzhixing/FRS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhixing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Pre-Training in High-Resolution Remote Sensing Scene Classification. (arXiv:2111.03690v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.03690","description":"<p>Due to the scarcity of labeled data, using models pre-trained on ImageNet is\na de facto standard in remote sensing scene classification. Although, recently,\nseveral larger high resolution remote sensing (HRRS) datasets have appeared\nwith a goal of establishing new benchmarks, attempts at training models from\nscratch on these datasets are sporadic. In this paper, we show that training\nmodels from scratch on several newer datasets yields comparable results to\nfine-tuning the models pre-trained on ImageNet. Furthermore, the\nrepresentations learned on HRRS datasets transfer to other HRRS scene\nclassification tasks better or at least similarly as those learned on ImageNet.\nFinally, we show that in many cases the best representations are obtained by\nusing a second round of pre-training using in-domain data, i.e. domain-adaptive\npre-training. The source code and pre-trained models are available at\n\\url{https://github.com/risojevicv/RSSC-transfer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Risojevic_V/0/1/0/all/0/1\">Vladimir Risojevi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stojnic_V/0/1/0/all/0/1\">Vladan Stojni&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Point Light Fields. (arXiv:2112.01473v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01473","description":"<p>We introduce Neural Point Light Fields that represent scenes implicitly with\na light field living on a sparse point cloud. Combining differentiable volume\nrendering with learned implicit density representations has made it possible to\nsynthesize photo-realistic images for novel views of small scenes. As neural\nvolumetric rendering methods require dense sampling of the underlying\nfunctional scene representation, at hundreds of samples along a ray cast\nthrough the volume, they are fundamentally limited to small scenes with the\nsame objects projected to hundreds of training views. Promoting sparse point\nclouds to neural implicit light fields allows us to represent large scenes\neffectively with only a single implicit sampling operation per ray. These point\nlight fields are as a function of the ray direction, and local point feature\nneighborhood, allowing us to interpolate the light field conditioned training\nimages without dense object coverage and parallax. We assess the proposed\nmethod for novel view synthesis on large driving scenarios, where we synthesize\nrealistic unseen views that existing implicit approaches fail to represent. We\nvalidate that Neural Point Light Fields make it possible to predict videos\nalong unseen trajectories previously only feasible to generate by explicitly\nmodeling the scene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ost_J/0/1/0/all/0/1\">Julian Ost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newell_A/0/1/0/all/0/1\">Alejandro Newell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahat_Y/0/1/0/all/0/1\">Yuval Bahat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Medical Point Transformer: Introducing Convolution to Attention Networks for Medical Point Cloud Analysis. (arXiv:2112.04863v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.04863","description":"<p>General point clouds have been increasingly investigated for different tasks,\nand recently Transformer-based networks are proposed for point cloud analysis.\nHowever, there are barely related works for medical point clouds, which are\nimportant for disease detection and treatment. In this work, we propose an\nattention-based model specifically for medical point clouds, namely 3D medical\npoint Transformer (3DMedPT), to examine the complex biological structures. By\naugmenting contextual information and summarizing local responses at query, our\nattention module can capture both local context and global content feature\ninteractions. However, the insufficient training samples of medical data may\nlead to poor feature learning, so we apply position embeddings to learn\naccurate local geometry and Multi-Graph Reasoning (MGR) to examine global\nknowledge propagation over channel graphs to enrich feature representations.\nExperiments conducted on IntrA dataset proves the superiority of 3DMedPT, where\nwe achieve the best classification and segmentation results. Furthermore, the\npromising generalization ability of our method is validated on general 3D point\ncloud benchmarks: ModelNet40 and ShapeNetPart. Code is released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jianhui Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dingxin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CityNeRF: Building NeRF at City Scale. (arXiv:2112.05504v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05504","description":"<p>Neural Radiance Field (NeRF) has achieved outstanding performance in modeling\n3D objects and controlled scenes, usually under a single scale. In this work,\nwe make the first attempt to bring NeRF to city-scale, with views ranging from\nsatellite-level that captures the overview of a city, to ground-level imagery\nshowing complex details of an architecture. The wide span of camera distance to\nthe scene yields multi-scale data with different levels of detail and spatial\ncoverage, which casts great challenges to vanilla NeRF and biases it towards\ncompromised results. To address these issues, we introduce CityNeRF, a\nprogressive learning paradigm that grows the NeRF model and training set\nsynchronously. Starting from fitting distant views with a shallow base block,\nas training progresses, new blocks are appended to accommodate the emerging\ndetails in the increasingly closer views. The strategy effectively activates\nhigh-frequency channels in the positional encoding and unfolds more complex\ndetails as the training proceeds. We demonstrate the superiority of CityNeRF in\nmodeling diverse city-scale scenes with drastically varying views, and its\nsupport for rendering views in different levels of detail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiangli_Y/0/1/0/all/0/1\">Yuanbo Xiangli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Nanxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anyi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking and Long-Term Identification Using Non-Visual Markers. (arXiv:2112.06809v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06809","description":"<p>Our objective is to track and identify mice in a cluttered home-cage\nenvironment, as a precursor to automated behaviour recognition for biological\nresearch. This is a very challenging problem due to (i) the lack of\ndistinguishing visual features for each mouse, and (ii) the close confines of\nthe scene with constant occlusion, making standard visual tracking approaches\nunusable. However, a coarse estimate of each mouse's location is available from\na unique RFID implant, so there is the potential to optimally combine\ninformation from (weak) tracking with coarse information on identity. To\nachieve our objective, we make the following key contributions: (a) the\nformulation of the identification problem as an assignment problem (solved\nusing Integer Linear Programming), and (b) a novel probabilistic model of the\naffinity between tracklets and RFID data. The latter is a crucial part of the\nmodel, as it provides a principled probabilistic treatment of object detections\ngiven coarse localisation. Our approach achieves 77% accuracy on this\nidentification problem, and is able to reject spurious detections when the\nanimals are hidden.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camilleri_M/0/1/0/all/0/1\">Michael P. J. Camilleri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bains_R/0/1/0/all/0/1\">Rasneer S. Bains</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher K. I. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-guided Deblurring of Unknown Exposure Time Videos. (arXiv:2112.06988v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06988","description":"<p>Video deblurring is a highly ill-posed problem due to the loss of motion\ninformation in the blur degradation process. Since event cameras can capture\napparent motion with a high temporal resolution, several attempts have explored\nthe potential of events for guiding video deblurring. These methods generally\nassume that the exposure time is the same as the reciprocal of the video frame\nrate. However,this is not true in real situations, and the exposure time might\nbe unknown and dynamically varies depending on the video shooting\nenvironment(e.g., illumination condition). In this paper, we address the\nevent-guided video deblurring assuming dynamically variable unknown exposure\ntime of the frame-based camera. To this end, we first derive a new formulation\nfor event-guided video deblurring by considering the exposure and readout time\nin the video frame acquisition process. We then propose a novel end-toend\nlearning framework for event-guided video deblurring. In particular, we design\na novel Exposure Time-based Event Selection(ETES) module to selectively use\nevent features by estimating the cross-modal correlation between the features\nfrom blurred frames and the events. Moreover, we propose a feature fusion\nmodule to effectively fuse the selected features from events and blur frames.\nWe conduct extensive experiments on various datasets and demonstrate that our\nmethod achieves state-of-the-art performance. Our project code and pretrained\nmodels will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jeongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Body-Aware 3D Shape Generative Models. (arXiv:2112.07022v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2112.07022","description":"<p>The shape of many objects in the built environment is dictated by their\nrelationships to the human body: how will a person interact with this object?\nExisting data-driven generative models of 3D shapes produce plausible objects\nbut do not reason about the relationship of those objects to the human body. In\nthis paper, we learn body-aware generative models of 3D shapes. Specifically,\nwe train generative models of chairs, an ubiquitous shape category, which can\nbe conditioned on a given body shape or sitting pose. The\nbody-shape-conditioned models produce chairs which will be comfortable for a\nperson with the given body shape; the pose-conditioned models produce chairs\nwhich accommodate the given sitting pose. To train these models, we define a\n\"sitting pose matching\" metric and a novel \"sitting comfort\" metric.\nCalculating these metrics requires an expensive optimization to sit the body\ninto the chair, which is too slow to be used as a loss function for training a\ngenerative model. Thus, we train neural networks to efficiently approximate\nthese metrics. We use our approach to train three body-aware generative shape\nmodels: a structured part-based generator, a point cloud generator, and an\nimplicit surface generator. In all cases, our approach produces models which\nadapt their output chair shapes to input human body specifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blinn_B/0/1/0/all/0/1\">Bryce Blinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_A/0/1/0/all/0/1\">Alexander Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Transformer Networks with Self-Supervision for Action Recognition. (arXiv:2112.07338v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07338","description":"<p>In recent years, 2D Convolutional Networks-based video action recognition has\nencouragingly gained wide popularity; However, constrained by the lack of\nlong-range non-linear temporal relation modeling and reverse motion information\nmodeling, the performance of existing models is, therefore, undercut seriously.\nTo address this urgent problem, we introduce a startling Temporal Transformer\nNetwork with Self-supervision (TTSN). Our high-performance TTSN mainly consists\nof a temporal transformer module and a temporal sequence self-supervision\nmodule. Concisely speaking, we utilize the efficient temporal transformer\nmodule to model the non-linear temporal dependencies among non-local frames,\nwhich significantly enhances complex motion feature representations. The\ntemporal sequence self-supervision module we employ unprecedentedly adopts the\nstreamlined strategy of \"random batch random channel\" to reverse the sequence\nof video frames, allowing robust extractions of motion information\nrepresentation from inversed temporal dimensions and improving the\ngeneralization capability of the model. Extensive experiments on three widely\nused datasets (HMDB51, UCF101, and Something-something V1) have conclusively\ndemonstrated that our proposed TTSN is promising as it successfully achieves\nstate-of-the-art performance for action recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongkang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhiping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoxun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homography Decomposition Networks for Planar Object Tracking. (arXiv:2112.07909v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07909","description":"<p>Planar object tracking plays an important role in AI applications, such as\nrobotics, visual servoing, and visual SLAM. Although the previous planar\ntrackers work well in most scenarios, it is still a challenging task due to the\nrapid motion and large transformation between two consecutive frames. The\nessential reason behind this problem is that the condition number of such a\nnon-linear system changes unstably when the searching range of the homography\nparameter space becomes larger. To this end, we propose a novel Homography\nDecomposition Networks~(HDN) approach that drastically reduces and stabilizes\nthe condition number by decomposing the homography transformation into two\ngroups. Specifically, a similarity transformation estimator is designed to\npredict the first group robustly by a deep convolution equivariant network. By\ntaking advantage of the scale and rotation estimation with high confidence, a\nresidual transformation is estimated by a simple regression model. Furthermore,\nthe proposed end-to-end network is trained in a semi-supervised fashion.\nExtensive experiments show that our proposed approach outperforms the\nstate-of-the-art planar tracking methods at a large margin on the challenging\nPOT, UCSB and POIC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinrui Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation. (arXiv:2112.08867v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08867","description":"<p>3D-aware image generative modeling aims to generate 3D-consistent images with\nexplicitly controllable camera poses. Recent works have shown promising results\nby training neural radiance field (NeRF) generators on unstructured 2D images,\nbut still can not generate highly-realistic images with fine details. A\ncritical reason is that the high memory and computation cost of volumetric\nrepresentation learning greatly restricts the number of point samples for\nradiance integration during training. Deficient sampling not only limits the\nexpressive power of the generator to handle fine details but also impedes\neffective GAN training due to the noise caused by unstable Monte Carlo\nsampling. We propose a novel approach that regulates point sampling and\nradiance field learning on 2D manifolds, embodied as a set of learned implicit\nsurfaces in the 3D volume. For each viewing ray, we calculate ray-surface\nintersections and accumulate their radiance generated by the network. By\ntraining and rendering such radiance manifolds, our generator can produce high\nquality images with realistic fine details and strong visual 3D consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yu Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jianfeng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}}]}]}