{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Dark Side of the Language: Pre-trained Transformers in the DarkNet. (arXiv:2201.05613v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05613","description":"<p>Pre-trained Transformers are challenging human performances in many natural\nlanguage processing tasks. The gigantic datasets used for pre-training seem to\nbe the key for their success on existing tasks. In this paper, we explore how a\nrange of pre-trained natural language understanding models perform on truly\nnovel and unexplored data, provided by classification tasks over a DarkNet\ncorpus. Surprisingly, results show that syntactic and lexical neural networks\nlargely outperform pre-trained Transformers. This seems to suggest that\npre-trained Transformers have serious difficulties in adapting to radically\nnovel texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1\">Leonardo Ranaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nourbakhsh_A/0/1/0/all/0/1\">Aria Nourbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patrizi_A/0/1/0/all/0/1\">Arianna Patrizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzzetti_E/0/1/0/all/0/1\">Elena Sofia Ruzzetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onorati_D/0/1/0/all/0/1\">Dario Onorati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1\">Francesca Fallucchi Fabio Massimo Zanzotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Reducing Manual Workload in Technology-Assisted Reviews: Estimating Ranking Performance. (arXiv:2201.05648v1 [cs.IR])","link":"http://arxiv.org/abs/2201.05648","description":"<p>Conducting a systematic review (SR) is comprised of multiple tasks: (i)\ncollect documents (studies) that are likely to be relevant from digital\nlibraries (eg., PubMed), (ii) manually read and label the documents as relevant\nor irrelevant, (iii) extract information from the relevant studies, and (iv)\nanalyze and synthesize the information and derive a conclusion of SR. When\nresearchers label studies, they can screen ranked documents where relevant\ndocuments are higher than irrelevant ones. This practice, known as screening\nprioritization (ie., document ranking approach), speeds up the process of\nconducting a SR as the documents labelled as relevant can move to the next\ntasks earlier. However, the approach is limited in reducing the manual workload\nbecause the total number of documents to screen remains the same. Towards\nreducing the manual workload in the screening process, we investigate the\nquality of document ranking of SR. This can signal researchers whereabouts in\nthe ranking relevant studies are located and let them decide where to stop the\nscreening. After extensive analysis on SR document rankings from different\nranking models, we hypothesize 'topic broadness' as a factor that affects the\nranking quality of SR. Finally, we propose a measure that estimates the topic\nbroadness and demonstrate that the proposed measure is a simple yet effective\nmethod to predict the qualities of document rankings for SRs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Grace E. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Models for Extracting Information from Registration and Legal Documents. (arXiv:2201.05658v1 [cs.AI])","link":"http://arxiv.org/abs/2201.05658","description":"<p>A typical information extraction pipeline consists of token- or span-level\nclassification models coupled with a series of pre- and post-processing\nscripts. In a production pipeline, requirements often change, with classes\nbeing added and removed, which leads to nontrivial modifications to the source\ncode and the possible introduction of bugs. In this work, we evaluate\nsequence-to-sequence models as an alternative to token-level classification\nmethods for information extraction of legal and registration documents. We\nfinetune models that jointly extract the information and generate the output\nalready in a structured format. Post-processing steps are learned during\ntraining, thus eliminating the need for rule-based methods and simplifying the\npipeline. Furthermore, we propose a novel method to align the output with the\ninput text, thus facilitating system inspection and auditing. Our experiments\non four real-world datasets show that the proposed method is an alternative to\nclassical pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1\">Ramon Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_F/0/1/0/all/0/1\">F&#xe1;bio C. de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Guilherme Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto A. Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Stability with Continuous Data Updates. (arXiv:2201.05692v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05692","description":"<p>In this paper, we study the \"stability\" of machine learning (ML) models\nwithin the context of larger, complex NLP systems with continuous training data\nupdates. For this study, we propose a methodology for the assessment of model\nstability (which we refer to as jitter under various experimental conditions.\nWe find that model design choices, including network architecture and input\nrepresentation, have a critical impact on stability through experiments on four\ntext classification tasks and two sequence labeling tasks. In classification\ntasks, non-RNN-based models are observed to be more stable than RNN-based ones,\nwhile the encoder-decoder model is less stable in sequence labeling tasks.\nMoreover, input representations based on pre-trained fastText embeddings\ncontribute to more stability than other choices. We also show that two learning\nstrategies -- ensemble models and incremental training -- have a significant\ninfluence on stability. We recommend ML model designers account for trade-offs\nin accuracy and jitter when making modeling choices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huiting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S%2E_A/0/1/0/all/0/1\">Avinesh P.V.S.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwardhan_S/0/1/0/all/0/1\">Siddharth Patwardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1\">Peter Grasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sachin Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cost-Effective Training in Low-Resource Neural Machine Translation. (arXiv:2201.05700v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05700","description":"<p>While Active Learning (AL) techniques are explored in Neural Machine\nTranslation (NMT), only a few works focus on tackling low annotation budgets\nwhere a limited number of sentences can get translated. Such situations are\nespecially challenging and can occur for endangered languages with few human\nannotators or having cost constraints to label large amounts of data. Although\nAL is shown to be helpful with large budgets, it is not enough to build\nhigh-quality translation systems in these low-resource conditions. In this\nwork, we propose a cost-effective training procedure to increase the\nperformance of NMT models utilizing a small number of annotated sentences and\ndictionary entries. Our method leverages monolingual data with self-supervised\nobjectives and a small-scale, inexpensive dictionary for additional supervision\nto initialize the NMT model before applying AL. We show that improving the\nmodel using a combination of these knowledge sources is essential to exploit AL\nstrategies and increase gains in low-resource conditions. We also present a\nnovel AL strategy inspired by domain adaptation for NMT and show that it is\neffective for low budgets. We propose a new hybrid data-driven approach, which\nsamples sentences that are diverse from the labelled data and also most similar\nto unlabelled data. Finally, we show that initializing the NMT model and\nfurther using our AL strategy can achieve gains of up to $13$ BLEU compared to\nconventional AL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koneru_S/0/1/0/all/0/1\">Sai Koneru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Space Situational Awareness Events from News Text. (arXiv:2201.05721v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05721","description":"<p>Space situational awareness typically makes use of physical measurements from\nradar, telescopes, and other assets to monitor satellites and other spacecraft\nfor operational, navigational, and defense purposes. In this work we explore\nusing textual input for the space situational awareness task. We construct a\ncorpus of 48.5k news articles spanning all known active satellites between 2009\nand 2020. Using a dependency-rule-based extraction system designed to target\nthree high-impact events -- spacecraft launches, failures, and\ndecommissionings, we identify 1,787 space-event sentences that are then\nannotated by humans with 15.9k labels for event slots. We empirically\ndemonstrate a state-of-the-art neural extraction system achieves an overall F1\nbetween 53 and 91 per slot for event extraction in this low-resource,\nhigh-impact domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhengnan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_A/0/1/0/all/0/1\">Alice Saebom Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_E/0/1/0/all/0/1\">Enfa George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozal_L/0/1/0/all/0/1\">Laura W. Dozal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hoang Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jah_M/0/1/0/all/0/1\">Moriba Jah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furfaro_R/0/1/0/all/0/1\">Roberto Furfaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05729","description":"<p>Contrastive language-image pretraining (CLIP) links vision and language\nmodalities into a unified embedding space, yielding the tremendous potential\nfor vision-language (VL) tasks. While early concurrent works have begun to\nstudy this potential on a subset of tasks, important questions remain: 1) What\nis the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in\nlow-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches\nwithout impacting inference or pretraining complexity? In this work, we seek to\nanswer these questions through two key contributions. First, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata availability constraints and conditions of domain shift. Second, we\npropose an approach, named CLIP Targeted Distillation (CLIP-TD), to\nintelligently distill knowledge from CLIP into existing architectures using a\ndynamically weighted objective applied to adaptively selected tokens per\ninstance. Experiments demonstrate that our proposed CLIP-TD leads to\nexceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to\n71.3%) conditions of VCR, while simultaneously improving performance under\nstandard fully-supervised conditions (up to 2%), achieving state-of-art\nperformance on VCR compared to other single models that are pretrained with\nimage-text data only. On SNLI-VE, CLIP-TD produces significant gains in\nlow-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On\nVQA, CLIP-TD provides improvement in low-shot (up to 9%), and in\nfully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works\nutilizing CLIP for finetuning, as well as baseline naive distillation\napproaches. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kformer: Knowledge Injection in Transformer Feed-Forward Layers. (arXiv:2201.05742v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05742","description":"<p>Knowledge-Enhanced Model have developed a diverse set of techniques for\nknowledge integration on different knowledge sources. However, most previous\nwork neglect the language model's own ability and simply concatenate external\nknowledge at the input. Recent work proposed that Feed Forward Network (FFN) in\npre-trained language model can be seen as an memory that stored factual\nknowledge. In this work, we explore the FFN in Transformer and propose a novel\nknowledge fusion model, namely Kformer, which incorporates external knowledge\nthrough the feed-forward layer in Transformer. We empirically find that simply\ninjecting knowledge into FFN can enhance the pre-trained language model's\nability and facilitate current knowledge fusion methods. Our results on two\nbenchmarks in the commonsense reasoning (i.e., SocialIQA) and medical question\nanswering (i.e., MedQA-USMLE) domains demonstrate that Kformer can utilize\nexternal knowledge deeply and achieves absolute improvements in these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems. (arXiv:2201.05767v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05767","description":"<p>Large transformer models can highly improve Answer Sentence Selection (AS2)\ntask, but their high computational costs prevent their use in many real world\napplications. In this paper, we explore the following research question: How\ncan we make the AS2models more accurate without significantly increasing their\nmodel complexity? To address the question, we propose a Multiple Heads Student\narchitecture (MHS), an efficient neural network designed to distill an ensemble\nof large transformers into a single smaller model. An MHS model consists of two\ncomponents: a stack of transformer layers that is used to encode inputs, and a\nset of ranking heads; each of them is trained by distilling a different large\ntransformer architecture. Unlike traditional distillation techniques, our\napproach leverages individual models in ensemble as teachers in a way that\npreserves the diversity of the ensemble members. The resulting model captures\nthe knowledge of different types of transformer models by using just a few\nextra parameters. We show the effectiveness of MHS on three English datasets\nfor AS2; our proposed approach outperforms all single-model distillations we\nconsider, rivaling the state-of-the-art large AS2 models that have 2.7x more\nparameters and run 2.5x slower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lind_E/0/1/0/all/0/1\">Eric Lind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics. (arXiv:2201.05771v1 [eess.AS])","link":"http://arxiv.org/abs/2201.05771","description":"<p>We present an expanded version of our previously released Kazakh\ntext-to-speech (KazakhTTS) synthesis corpus. In the new KazakhTTS2 corpus, the\noverall size is increased from 93 hours to 271 hours, the number of speakers\nhas risen from two to five (three females and two males), and the topic\ncoverage is diversified with the help of new sources, including a book and\nWikipedia articles. This corpus is necessary for building high-quality TTS\nsystems for Kazakh, a Central Asian agglutinative language from the Turkic\nfamily, which presents several linguistic challenges. We describe the corpus\nconstruction process and provide the details of the training and evaluation\nprocedures for the TTS system. Our experimental results indicate that the\nconstructed corpus is sufficient to build robust TTS models for real-world\napplications, with a subjective mean opinion score of above 4.0 for all the\nfive speakers. We believe that our corpus will facilitate speech and language\nresearch for Kazakh and other Turkic languages, which are widely considered to\nbe low-resource due to the limited availability of free linguistic data. The\nconstructed corpus, code, and pretrained models are publicly available in our\nGitHub repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1\">Saida Mussakhojayeva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt Learning for Few-Shot Dialogue State Tracking. (arXiv:2201.05780v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05780","description":"<p>Collecting dialogue state labels, slots and values, for learning dialogue\nstate tracking (DST) models can be costly, especially with the wide application\nof dialogue systems in new-rising domains. In this paper, we focus on how to\nlearn a DST model efficiently with limited labeled data. We design a prompt\nlearning framework for few-shot DST, which consists of two main components:\nvalue-based prompt and inverse prompt mechanism. This framework aims to utilize\nthe language understanding and generation ability of pre-trained language\nmodels (PLM). First, we design value-based prompt functions to probe the\nDST-related knowledge from PLM, which do not rely on the known ontology of\nslots. Further, an inverse prompt mechanism is utilized to self-check the\n\"prompted\" knowledge and help the PLM understand the essence of DST task\nfurther. Experiments show that our model can generate unseen slots and\noutperforms existing state-of-the-art few-shot methods. It indicates that\nDST-related knowledge can be probed from PLM and utilized to address\nlow-resource DST efficiently with the help of prompt learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark for Generalizable and Interpretable Temporal Question Answering over Knowledge Bases. (arXiv:2201.05793v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05793","description":"<p>Knowledge Base Question Answering (KBQA) tasks that involve complex reasoning\nare emerging as an important research direction. However, most existing KBQA\ndatasets focus primarily on generic multi-hop reasoning over explicit facts,\nlargely ignoring other reasoning types such as temporal, spatial, and taxonomic\nreasoning. In this paper, we present a benchmark dataset for temporal\nreasoning, TempQA-WD, to encourage research in extending the present approaches\nto target a more challenging set of complex reasoning tasks. Specifically, our\nbenchmark is a temporal question answering dataset with the following\nadvantages: (a) it is based on Wikidata, which is the most frequently curated,\nopenly available knowledge base, (b) it includes intermediate sparql queries to\nfacilitate the evaluation of semantic parsing based approaches for KBQA, and\n(c) it generalizes to multiple knowledge bases: Freebase and Wikidata. The\nTempQA-WD dataset is available at https://github.com/IBM/tempqa-wd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neelam_S/0/1/0/all/0/1\">Sumit Neelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_U/0/1/0/all/0/1\">Udit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_H/0/1/0/all/0/1\">Hima Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikbal_S/0/1/0/all/0/1\">Shajith Ikbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Santosh Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pendus_C/0/1/0/all/0/1\">Cezar Pendus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_S/0/1/0/all/0/1\">Saswati Dana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Dinesh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokoue_A/0/1/0/all/0/1\">Achille Fokoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargav_G/0/1/0/all/0/1\">G P Shrivatsa Bhargav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_D/0/1/0/all/0/1\">Dinesh Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1\">Srinivas Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1\">Sairam Gurajada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Maria Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uceda_Sosa_R/0/1/0/all/0/1\">Rosario Uceda-Sosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1\">Alexander Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lima_G/0/1/0/all/0/1\">Guilherme Lima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegel_R/0/1/0/all/0/1\">Ryan Riegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luus_F/0/1/0/all/0/1\">Francois Luus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramaniam_L/0/1/0/all/0/1\">L Venkata Subramaniam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Lexical Simplification for Turkish. (arXiv:2201.05878v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05878","description":"<p>In this paper, we present the first automatic lexical simplification system\nfor the Turkish language. Recent text simplification efforts rely on manually\ncrafted simplified corpora and comprehensive NLP tools that can analyse the\ntarget text both in word and sentence levels. Turkish is a morphologically rich\nagglutinative language that requires unique considerations such as the proper\nhandling of inflectional cases. Being a low-resource language in terms of\navailable resources and industrial-strength tools, it makes the text\nsimplification task harder to approach. We present a new text simplification\npipeline based on pretrained representation model BERT together with\nmorphological features to generate grammatically correct and semantically\nappropriate word-level simplifications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning over Hybrid Chain for Table-and-Text Open Domain QA. (arXiv:2201.05880v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05880","description":"<p>Tabular and textual question answering requires systems to perform reasoning\nover heterogeneous information, considering table structure, and the\nconnections among table and text. In this paper, we propose a ChAin-centric\nReasoning and Pre-training framework (CARP). CARP utilizes hybrid chain to\nmodel the explicit intermediate reasoning process across table and text for\nquestion answering. We also propose a novel chain-centric pre-training method,\nto enhance the pre-trained model in identifying the cross-modality reasoning\nprocess and alleviating the data sparsity problem. This method constructs the\nlarge-scale reasoning corpus by synthesizing pseudo heterogeneous reasoning\npaths from Wikipedia and generating corresponding questions. We evaluate our\nsystem on OTT-QA, a large-scale table-and-text open-domain question answering\nbenchmark, and our system achieves the state-of-the-art performance. Further\nanalyses illustrate that the explicit hybrid chain offers substantial\nperformance improvement and interpretablity of the intermediate reasoning\nprocess, and the chain-centric pre-training boosts the performance on the chain\nextraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Correction of Syntactic Dependency Annotation Differences. (arXiv:2201.05891v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05891","description":"<p>Annotation inconsistencies between data sets can cause problems for\nlow-resource NLP, where noisy or inconsistent data cannot be as easily replaced\ncompared with resource-rich languages. In this paper, we propose a method for\nautomatically detecting annotation mismatches between dependency parsing\ncorpora, as well as three related methods for automatically converting the\nmismatches. All three methods rely on comparing an unseen example in a new\ncorpus with similar examples in an existing corpus. These three methods include\na simple lexical replacement using the most frequent tag of the example in the\nexisting corpus, a GloVe embedding-based replacement that considers a wider\npool of examples, and a BERT embedding-based replacement that uses\ncontextualized embeddings to provide examples fine-tuned to our specific data.\nWe then evaluate these conversions by retraining two dependency parsers --\nStanza (Qi et al. 2020) and Parsing as Tagging (PaT) (Vacareanu et al. 2020) --\non the converted and unconverted data. We find that applying our conversions\nyields significantly better performance in many cases. Some differences\nobserved between the two parsers are observed. Stanza has a more complex\narchitecture with a quadratic algorithm, so it takes longer to train, but it\ncan generalize better with less data. The PaT parser has a simpler architecture\nwith a linear algorithm, speeding up training time but requiring more training\ndata to reach comparable or better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zupon_A/0/1/0/all/0/1\">Andrew Zupon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carnie_A/0/1/0/all/0/1\">Andrew Carnie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammond_M/0/1/0/all/0/1\">Michael Hammond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unobserved Local Structures Make Compositional Generalization Hard. (arXiv:2201.05899v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05899","description":"<p>While recent work has convincingly showed that sequence-to-sequence models\nstruggle to generalize to new compositions (termed compositional\ngeneralization), little is known on what makes compositional generalization\nhard on a particular test instance. In this work, we investigate what are the\nfactors that make generalization to certain test instances challenging. We\nfirst substantiate that indeed some examples are more difficult than others by\nshowing that different models consistently fail or succeed on the same test\ninstances. Then, we propose a criterion for the difficulty of an example: a\ntest instance is hard if it contains a local structure that was not observed at\ntraining time. We formulate a simple decision rule based on this criterion and\nempirically show it predicts instance-level generalization well across 5\ndifferent semantic parsing datasets, substantially better than alternative\ndecision rules. Last, we show local structures can be leveraged for creating\ndifficult adversarial compositional splits and also to improve compositional\ngeneralization under limited training budgets by strategically selecting\nexamples for the training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing the Challenges of Cross-Lingual Hate Speech Detection. (arXiv:2201.05922v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05922","description":"<p>The goal of hate speech detection is to filter negative online content aiming\nat certain groups of people. Due to the easy accessibility of social media\nplatforms it is crucial to protect everyone which requires building hate speech\ndetection systems for a wide range of languages. However, the available labeled\nhate speech datasets are limited making it problematic to build systems for\nmany languages. In this paper we focus on cross-lingual transfer learning to\nsupport hate speech detection in low-resource languages. We leverage\ncross-lingual word embeddings to train our neural network systems on the source\nlanguage and apply it to the target language, which lacks labeled examples, and\nshow that good performance can be achieved. We then incorporate unlabeled\ntarget language data for further model improvements by bootstrapping labels\nusing an ensemble of different model architectures. Furthermore, we investigate\nthe issue of label imbalance of hate speech datasets, since the high ratio of\nnon-hate examples compared to hate examples often leads to low model\nperformance. We test simple data undersampling and oversampling techniques and\nshow their effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bigoulaeva_I/0/1/0/all/0/1\">Irina Bigoulaeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hangya_V/0/1/0/all/0/1\">Viktor Hangya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05955","description":"<p>A recurring challenge of crowdsourcing NLP datasets at scale is that human\nwriters often rely on repetitive patterns when crafting examples, leading to a\nlack of linguistic diversity. We introduce a novel paradigm for dataset\ncreation based on human and machine collaboration, which brings together the\ngenerative strength of language models and the evaluative strength of humans.\nStarting with an existing dataset, MultiNLI, our approach uses dataset\ncartography to automatically identify examples that demonstrate challenging\nreasoning patterns, and instructs GPT-3 to compose new examples with similar\npatterns. Machine generated examples are then automatically filtered, and\nfinally revised and labeled by human crowdworkers to ensure quality. The\nresulting dataset, WANLI, consists of 108,357 natural language inference (NLI)\nexamples that present unique empirical strengths over existing NLI datasets.\nRemarkably, training a model on WANLI instead of MNLI (which is 4 times larger)\nimproves performance on seven out-of-domain test sets we consider, including by\n11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is\nmore effective than combining with other augmentation sets that have been\nintroduced. Our results demonstrate the potential of natural language\ngeneration techniques to curate NLP datasets of enhanced quality and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alisa Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. (arXiv:2201.05966v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05966","description":"<p>Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the SKG framework, which unifies 21 SKG\ntasks into a text-to-text format, aiming to promote systematic SKG research,\ninstead of being exclusive to a single task, domain, or dataset. We use\nUnifiedSKG to benchmark T5 with different sizes and show that T5, with simple\nmodifications when necessary, achieves state-of-the-art performance on almost\nall of the 21 tasks. We further demonstrate that multi-task prefix-tuning\nimproves the performance on most tasks, largely improving the overall\nperformance. UnifiedSKG also facilitates the investigation of zero-shot and\nfew-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot\nand few-shot learning for SKG. We also use UnifiedSKG to conduct a series of\ncontrolled experiments on structured knowledge encoding variants across SKG\ntasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at\nhttps://github.com/hkunlp/unifiedskg; latest collections at\nhttps://unifiedskg.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1\">Ming Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida I. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1\">Victor Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengzu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyle_C/0/1/0/all/0/1\">Connor Boyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Ziyu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples. (arXiv:2201.05979v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05979","description":"<p>Unsupervised sentence embedding aims to obtain the most appropriate embedding\nfor a sentence to reflect its semantic. Contrastive learning has been\nattracting developing attention. For a sentence, current models utilize diverse\ndata augmentation methods to generate positive samples, while consider other\nindependent sentences as negative samples. Then they adopt InfoNCE loss to pull\nthe embeddings of positive pairs gathered, and push those of negative pairs\nscattered. Although these models have made great progress on sentence\nembedding, we argue that they may suffer from feature suppression. The models\nfail to distinguish and decouple textual similarity and semantic similarity.\nAnd they may overestimate the semantic similarity of any pairs with similar\ntextual regardless of the actual semantic difference between them. This is\nbecause positive pairs in unsupervised contrastive learning come with similar\nand even the same textual through data augmentation. To alleviate feature\nsuppression, we propose contrastive learning for unsupervised sentence\nembedding with soft negative samples (SNCSE). Soft negative samples share\nhighly similar textual but have surely and apparently different semantic with\nthe original samples. Specifically, we take the negation of original sentences\nas soft negative samples, and propose Bidirectional Margin Loss (BML) to\nintroduce them into traditional contrastive learning framework, which merely\ninvolves positive and negative samples. Our experimental results show that\nSNCSE can obtain state-of-the-art performance on semantic textual similarity\n(STS) task with average Spearman's correlation coefficient of 78.97% on\nBERTbase and 79.23% on RoBERTabase. Besides, we adopt rank-based error analysis\nmethod to detect the weakness of SNCSE for future study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yong Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Double Retrieval and Ranking for Accurate Question Answering. (arXiv:2201.05981v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05981","description":"<p>Recent work has shown that an answer verification step introduced in\nTransformer-based answer selection models can significantly improve the state\nof the art in Question Answering. This step is performed by aggregating the\nembeddings of top $k$ answer candidates to support the verification of a target\nanswer. Although the approach is intuitive and sound still shows two\nlimitations: (i) the supporting candidates are ranked only according to the\nrelevancy with the question and not with the answer, and (ii) the support\nprovided by the other answer candidates is suboptimal as these are retrieved\nindependently of the target answer. In this paper, we address both drawbacks by\nproposing (i) a double reranking model, which, for each target answer, selects\nthe best support; and (ii) a second neural retrieval stage designed to encode\nquestion and answer pair as the query, which finds more specific verification\ninformation. The results on three well-known datasets for AS2 show consistent\nand significant improvement of the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thuy Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Situ Answer Sentence Selection at Web-scale. (arXiv:2201.05984v1 [cs.CL])","link":"http://arxiv.org/abs/2201.05984","description":"<p>Current answer sentence selection (AS2) applied in open-domain question\nanswering (ODQA) selects answers by ranking a large set of possible candidates,\ni.e., sentences, extracted from the retrieved text. In this paper, we present\nPassage-based Extracting Answer Sentence In-place (PEASI), a novel design for\nAS2 optimized for Web-scale setting, that, instead, computes such answer\nwithout processing each candidate individually. Specifically, we design a\nTransformer-based framework that jointly (i) reranks passages retrieved for a\nquestion and (ii) identifies a probable answer from the top passages in place.\nWe train PEASI in a multi-task learning framework that encourages feature\nsharing between the components: passage reranker and passage-based answer\nsentence extractor. To facilitate our development, we construct a new\nWeb-sourced large-scale QA dataset consisting of 800,000+ labeled\npassages/sentences for 60,000+ questions. The experiments show that our\nproposed design effectively outperforms the current state-of-the-art setting\nfor AS2, i.e., a point-wise model for ranking sentences independently, by 6.51%\nin accuracy, from 48.86% to 55.37%. In addition, PEASI is exceptionally\nefficient in computing answer sentences, requiring only ~20% inferences\ncompared to the standard setting, i.e., reranking all possible candidates. We\nbelieve the release of PEASI, both the dataset and our proposed design, can\ncontribute to advancing the research and development in deploying question\nanswering services at Web scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thuy Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v1 [cs.CL])","link":"http://arxiv.org/abs/2201.06009","description":"<p>Large LMs such as GPT-3, while powerful, are not immune to mistakes, but are\nprohibitively costly to retrain. One failure mode is misinterpreting a user's\ninstruction (e.g., GPT-3 interpreting \"What word is similar to good?\" to mean a\nhomonym, while the user intended a synonym). Our goal is to allow users to\ncorrect such errors directly through interaction -- without retraining. Our\napproach pairs GPT-3 with a growing memory of cases where the model\nmisunderstood the user's intent and was provided with feedback, clarifying the\ninstruction. Given a new query, our memory-enhanced GPT-3 uses feedback from\nsimilar, prior queries to enrich the prompt. Through simple proof-of-concept\nexperiments, we show how a (simulated) user can interactively teach a deployed\nGPT-3, doubling its accuracy on basic lexical tasks (e.g., generate a synonym)\nwhere users query in different, novel (often misunderstood) ways. In such\nscenarios, memory helps avoid repeating similar past mistakes. Our simple idea\nis a first step towards strengthening deployed models, potentially broadening\ntheir utility. All the code and data is available at\nhttps://github.com/madaan/memprompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLD: A Benchmark for Chinese Offensive Language Detection. (arXiv:2201.06025v1 [cs.CL])","link":"http://arxiv.org/abs/2201.06025","description":"<p>Offensive language detection and prevention becomes increasing critical for\nmaintaining a healthy social platform and the safe deployment of language\nmodels. Despite plentiful researches on toxic and offensive language problem in\nNLP, existing studies mainly focus on English, while few researches involve\nChinese due to the limitation of resources. To facilitate Chinese offensive\nlanguage detection and model evaluation, we collect COLDataset, a Chinese\noffensive language dataset containing 37k annotated sentences. With this\nhigh-quality dataset, we provide a strong baseline classifier, COLDetector,\nwith 81% accuracy for offensive language detection. Furthermore, we also\nutilize the proposed \\textsc{COLDetector} to study output offensiveness of\npopular Chinese language models (CDialGPT and CPM). We find that (1) CPM tends\nto generate more offensive output than CDialGPT, and (2) certain type of\nprompts, like anti-bias sentences, can trigger offensive outputs more\neasily.Altogether, our resources and analyses are intended to help detoxify the\nChinese online communities and evaluate the safety performance of generative\nlanguage models. Disclaimer: The paper contains example data that may be\nconsidered profane, vulgar, or offensive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiawen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Deduction through Search over Statement Compositions. (arXiv:2201.06028v1 [cs.CL])","link":"http://arxiv.org/abs/2201.06028","description":"<p>In settings from fact-checking to question answering, we frequently want to\nknow whether a collection of evidence entails a hypothesis. Existing methods\nprimarily focus on end-to-end discriminative versions of this task, but less\nwork has treated the generative version in which a model searches over the\nspace of entailed statements to derive the hypothesis. We propose a system for\nnatural language deduction that decomposes the task into separate steps\ncoordinated by best-first search, producing a tree of intermediate conclusions\nthat faithfully reflects the system's reasoning process. Our experiments\ndemonstrate that the proposed system can better distinguish verifiable\nhypotheses from unverifiable ones and produce natural language explanations\nthat are more internally consistent than those produced by an end-to-end T5\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bostrom_K/0/1/0/all/0/1\">Kaj Bostrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sprague_Z/0/1/0/all/0/1\">Zayne Sprague</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inspecting state of the art performance and NLP metrics in image-based medical report generation. (arXiv:2011.09257v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.09257","description":"<p>Several deep learning architectures have been proposed over the last years to\ndeal with the problem of generating a written report given an imaging exam as\ninput. Most works evaluate the generated reports using standard Natural\nLanguage Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant\nprogress. In this article, we contrast this progress by comparing state of the\nart (SOTA) models against weak baselines. We show that simple and even naive\napproaches yield near SOTA performance on most traditional NLP metrics. We\nconclude that evaluation methods in this task should be further studied towards\ncorrectly measuring clinical accuracy, ideally involving physicians to\ncontribute to this end.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pino_P/0/1/0/all/0/1\">Pablo Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_P/0/1/0/all/0/1\">Pablo Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besa_C/0/1/0/all/0/1\">Cecilia Besa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uribe_S/0/1/0/all/0/1\">Sergio Uribe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASER: Towards Large-scale Commonsense Knowledge Acquisition via Higher-order Selectional Preference over Eventualities. (arXiv:2104.02137v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.02137","description":"<p>Commonsense knowledge acquisition and reasoning have long been a core\nartificial intelligence problem. However, in the past, there has been a lack of\nscalable methods to collect commonsense knowledge. In this paper, we propose to\ndevelop principles for collecting commonsense knowledge based on selectional\npreference. We generalize the definition of selectional preference from one-hop\nlinguistic syntactic relations to higher-order relations over linguistic\ngraphs. Unlike previous commonsense knowledge definition (e.g., ConceptNet),\nselectional preference (SP) knowledge only relies on statistical distribution\nover linguistic graphs, which can be efficiently and accurately acquired from\nthe unlabeled corpus with modern tools. Following this principle, we develop a\nlarge-scale eventuality (a linguistic term covering activity, state, and\nevent)-based knowledge graph ASER, where each eventuality is represented as a\ndependency graph, and the relation between them is a discourse relation defined\nin shallow discourse parsing. The higher-order selectional preference over\ncollected linguistic graphs reflects various kinds of commonsense knowledge.\nMoreover, motivated by the observation that humans understand events by\nabstracting the observed events to a higher level and can thus transfer their\nknowledge to new events, we propose a conceptualization module to significantly\nboost the coverage of ASER. In total, ASER contains 648 million edges between\n438 million eventualities. After conceptualization with Probase, a selectional\npreference based concept-instance relational knowledge base, our concept graph\ncontains 15 million conceptualized eventualities and 224 million edges between\nthem. Detailed analysis is provided to demonstrate its quality. All the\ncollected data, APIs, and tools are available at\nhttps://github.com/HKUST-KnowComp/ASER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Haojie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_H/0/1/0/all/0/1\">Haowen Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiefu Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tianqing Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Network Pruning -- under the Pre-train and Fine-tune Paradigm. (arXiv:2104.08682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08682","description":"<p>Transformer-based pre-trained language models have significantly improved the\nperformance of various natural language processing (NLP) tasks in the recent\nyears. While effective and prevalent, these models are usually prohibitively\nlarge for resource-limited deployment scenarios. A thread of research has thus\nbeen working on applying network pruning techniques under the\npretrain-then-finetune paradigm widely adopted in NLP. However, the existing\npruning results on benchmark transformers, such as BERT, are not as remarkable\nas the pruning results in the literature of convolutional neural networks\n(CNNs). In particular, common wisdom in pruning CNN states that sparse pruning\ntechnique compresses a model more than that obtained by reducing number of\nchannels and layers (Elsen et al., 2020; Zhu and Gupta, 2017), while existing\nworks on sparse pruning of BERT yields inferior results than its small-dense\ncounterparts such as TinyBERT (Jiao et al., 2020). In this work, we aim to fill\nthis gap by studying how knowledge are transferred and lost during the\npre-train, fine-tune, and pruning process, and proposing a knowledge-aware\nsparse pruning process that achieves significantly superior results than\nexisting literature. We show for the first time that sparse pruning compresses\na BERT model significantly more than reducing its number of channels and\nlayers. Experiments on multiple data sets of GLUE benchmark show that our\nmethod outperforms the leading competitors with a 20-times weight/FLOPs\ncompression and neglectable loss in prediction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1\">Ian E.H. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinxi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhibin Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08793","description":"<p>Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, to\nmotivate saliency-based supervision, we analyze oracle KG-augmented models\nwhich directly use saliency explanations as extra inputs for guiding their\nattention. Third, we propose SalKG, a framework for KG-augmented models to\nlearn from coarse and/or fine saliency explanations. Given saliency\nexplanations created from a task's training set, SalKG jointly trains the model\nto predict the explanations, then solve the task by attending to KG features\nhighlighted by the predicted explanations. On three commonsense QA benchmarks\n(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can\nyield considerable performance gains -- up to 2.76% absolute improvement on\nCSQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aaron Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Boyuan Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition. (arXiv:2105.12708v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.12708","description":"<p>Anglicisms are a challenge in German speech recognition. Due to their\nirregular pronunciation compared to native German words, automatically\ngenerated pronunciation dictionaries often include faulty phoneme sequences for\nAnglicisms. In this work, we propose a multitask sequence-to-sequence approach\nfor grapheme-to-phoneme conversion to improve the phonetization of Anglicisms.\nWe extended a grapheme-to-phoneme model with a classifier to distinguish\nAnglicisms from native German words. With this approach, the model learns to\ngenerate pronunciations differently depending on the classification result. We\nused our model to create supplementary Anglicism pronunciation dictionaries\nthat are added to an existing German speech recognition model. Tested on a\ndedicated Anglicism evaluation set, we improved the recognition of Anglicisms\ncompared to a baseline model, reducing the word error rate by 1 % and the\nAnglicism error rate by 3 %. We show that multitask learning can help solving\nthe challenge of Anglicisms in German speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pritzen_J/0/1/0/all/0/1\">Julia Pritzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gref_M/0/1/0/all/0/1\">Michael Gref</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuhlke_D/0/1/0/all/0/1\">Dietlind Z&#xfc;hlke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1\">Christoph Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CatVRNN: Generating Category Texts via Multi-task Learning. (arXiv:2107.05219v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05219","description":"<p>Controlling the model to generate texts of different categories is a\nchallenging task that is receiving increasing attention. Recently, generative\nadversarial networks (GANs) have shown promising results for category text\ngeneration. However, the texts generated by GANs usually suffer from problems\nof mode collapse and training instability. To avoid the above problems, in this\nstudy, inspired by multi-task learning, a novel model called category-aware\nvariational recurrent neural network (CatVRNN) is proposed. In this model,\ngeneration and classification tasks are trained simultaneously to generate\ntexts of different categories. The use of multi-task learning can improve the\nquality of the generated texts, when the classification task is appropriate. In\naddition, a function is proposed to initialize the hidden state of the CatVRNN\nto force the model to generate texts of a specific category. Experimental\nresults on three datasets demonstrate that the model can outperform\nstate-of-the-art text generation methods based on GAN in terms of diversity of\ngenerated texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengsen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jinqiao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation. (arXiv:2108.12960v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12960","description":"<p>Standard multi-task benchmarks are essential for developing pretraining\nmodels that can generalize to various downstream tasks. Existing benchmarks for\nnatural language processing (NLP) usually focus only on understanding or\ngenerating short texts. However, long text modeling requires many distinct\nabilities in contrast to short texts, such as the modeling of long-range\ndiscourse and commonsense relations, and the coherence and controllability of\ngeneration. The lack of standardized benchmarks makes it difficult to assess\nthese abilities of a model and fairly compare different models, especially\nChinese models. Therefore, we propose a story-centric benchmark named LOT for\nevaluating Chinese long text modeling, which aggregates two understanding tasks\nand two generation tasks. We construct new datasets for these tasks based on\nhuman-written Chinese stories with hundreds of words. Furthermore, we release\nan encoder-decoder-based Chinese long text pretraining model named LongLM with\nup to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two\ngenerative tasks including text infilling and conditional continuation.\nExtensive experiments show that LongLM outperforms similar-sized pretraining\nmodels substantially on both the understanding and generation tasks in LOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhuoer Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yamei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruilin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL. (arXiv:2111.00653v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00653","description":"<p>The Text-to-SQL task, aiming to translate the natural language of the\nquestions into SQL queries, has drawn much attention recently. One of the most\nchallenging problems of Text-to-SQL is how to generalize the trained model to\nthe unseen database schemas, also known as the cross-domain Text-to-SQL task.\nThe key lies in the generalizability of (i) the encoding method to model the\nquestion and the database schema and (ii) the question-schema linking method to\nlearn the mapping between words in the question and tables/columns in the\ndatabase schema. Focusing on the above two key issues, we propose a\nStructure-Aware Dual Graph Aggregation Network (SADGA) for cross-domain\nText-to-SQL. In SADGA, we adopt the graph structure to provide a unified\nencoding model for both the natural language question and database schema.\nBased on the proposed unified modeling, we further devise a structure-aware\naggregation method to learn the mapping between the question-graph and\nschema-graph. The structure-aware aggregation method is featured with Global\nGraph Linking, Local Graph Linking, and Dual-Graph Aggregation Mechanism. We\nnot only study the performance of our proposal empirically but also achieved\n3rd place on the challenging Text-to-SQL benchmark Spider at the time of\nwriting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruichu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jinjie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Boyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhifeng Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding. (arXiv:2111.09098v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.09098","description":"<p>EHR systems lack a unified code system forrepresenting medical concepts,\nwhich acts asa barrier for the deployment of deep learningmodels in large scale\nto multiple clinics and hos-pitals. To overcome this problem, we\nintroduceDescription-based Embedding,DescEmb, a code-agnostic representation\nlearning framework forEHR. DescEmb takes advantage of the flexibil-ity of\nneural language understanding models toembed clinical events using their\ntextual descrip-tions rather than directly mapping each event toa dedicated\nembedding. DescEmb outperformedtraditional code-based embedding in\nextensiveexperiments, especially in a zero-shot transfertask (one hospital to\nanother), and was able totrain a single unified model for heterogeneousEHR\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hur_K/0/1/0/all/0/1\">Kyunghoon Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jungwoo Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Wesley Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoBERTuito: a pre-trained language model for social media text in Spanish. (arXiv:2111.09453v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.09453","description":"<p>Since BERT appeared, Transformer language models and transfer learning have\nbecome state-of-the-art for Natural Language Understanding tasks. Recently,\nsome works geared towards pre-training specially-crafted models for particular\ndomains, such as scientific papers, medical documents, user-generated texts,\namong others. These domain-specific models have been shown to improve\nperformance significantly in most tasks. However, for languages other than\nEnglish such models are not widely available.\n</p>\n<p>In this work, we present RoBERTuito, a pre-trained language model for\nuser-generated text in Spanish, trained on over 500 million tweets. Experiments\non a benchmark of tasks involving user-generated text showed that RoBERTuito\noutperformed other pre-trained language models in Spanish. In addition to this,\nour model achieves top results for some English-Spanish tasks of the Linguistic\nCode-Switching Evaluation benchmark (LinCE) and has also competitive\nperformance against monolingual models in English tasks. To facilitate further\nresearch, we make RoBERTuito publicly available at the HuggingFace model hub\ntogether with the dataset used to pre-train it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1\">Juan Manuel P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furman_D/0/1/0/all/0/1\">Dami&#xe1;n A. Furman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemany_L/0/1/0/all/0/1\">Laura Alonso Alemany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_F/0/1/0/all/0/1\">Franco Luque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00656","description":"<p>Recently, by introducing large-scale dataset and strong transformer network,\nvideo-language pre-training has shown great success especially for retrieval.\nYet, existing video-language transformer models do not explicitly fine-grained\nsemantic align. In this work, we present Object-aware Transformers, an\nobject-centric approach that extends video-language transformer to incorporate\nobject representations. The key idea is to leverage the bounding boxes and\nobject tags to guide the training process. We evaluate our model on three\nstandard sub-tasks of video-text matching on four widely used benchmarks. We\nalso provide deep analysis and detailed ablation about the proposed method. We\nshow clear improvement in performance across all tasks and datasets considered,\ndemonstrating the value of a model that incorporates object representations\ninto a video-language architecture. The code will be released at\n\\url{https://github.com/FingerRec/OA-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTMap: A BERT-based Ontology Alignment System. (arXiv:2112.02682v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2112.02682","description":"<p>Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in\nknowledge integration. Owing to the success of machine learning in many\ndomains, it has been applied in OM. However, the existing methods, which often\nadopt ad-hoc feature engineering or non-contextual word embeddings, have not\nyet outperformed rule-based systems especially in an unsupervised setting. In\nthis paper, we propose a novel OM system named BERTMap which can support both\nunsupervised and semi-supervised settings. It first predicts mappings using a\nclassifier based on fine-tuning the contextual embedding model BERT on text\nsemantics corpora extracted from ontologies, and then refines the mappings\nthrough extension and repair by utilizing the ontology structure and logic. Our\nevaluation with three alignment tasks on biomedical ontologies demonstrates\nthat BERTMap can often perform better than the leading OM systems LogMap and\nAML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonyrajah_D/0/1/0/all/0/1\">Denvar Antonyrajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06223","description":"<p>Code-switching is a speech phenomenon occurring when a speaker switches\nlanguage during a conversation. Despite the spontaneous nature of\ncode-switching in conversational spoken language, most existing works collect\ncode-switching data from read speech instead of spontaneous speech. ASCEND (A\nSpontaneous Chinese-English Dataset) is a high-quality Mandarin Chinese-English\ncode-switching corpus built on spontaneous multi-turn conversational dialogue\nsources collected in Hong Kong. We report ASCEND's design and procedure for\ncollecting the speech data, including annotations. ASCEND consists of 10.62\nhours of clean speech, collected from 23 bilingual speakers of Chinese and\nEnglish. Furthermore, we conduct baseline experiments using pre-trained wav2vec\n2.0 models, achieving a best performance of 22.69\\% character error rate and\n27.05% mixed error rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation. (arXiv:2112.07194v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07194","description":"<p>Chatbots are designed to carry out human-like conversations across different\ndomains, such as general chit-chat, knowledge exchange, and persona-grounded\nconversations. To measure the quality of such conversational agents, a dialogue\nevaluator is expected to conduct assessment across domains as well. However,\nmost of the state-of-the-art automatic dialogue evaluation metrics (ADMs) are\nnot designed for multi-domain evaluation. We are motivated to design a general\nand robust framework, MDD-Eval, to address the problem. Specifically, we first\ntrain a teacher evaluator with human-annotated data to acquire a rating skill\nto tell good dialogue responses from bad ones in a particular domain and then,\nadopt a self-training strategy to train a new evaluator with teacher-annotated\nmulti-domain data, that helps the new evaluator to generalize across multiple\ndomains. MDD-Eval is extensively assessed on six dialogue evaluation\nbenchmarks. Empirical results show that the MDD-Eval framework achieves a\nstrong performance with an absolute improvement of 7% over the state-of-the-art\nADMs in terms of mean Spearman correlation scores across all the evaluation\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrichs_T/0/1/0/all/0/1\">Thomas Friedrichs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Defeat of the Winograd Schema Challenge. (arXiv:2201.02387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02387","description":"<p>The Winograd Schema Challenge -- a set of twin sentences involving pronoun\nreference disambiguation that seem to require the use of commonsense knowledge\n-- was proposed by Hector Levesque in 2011. By 2019, a number of AI systems,\nbased on large pre-trained transformer-based language models and fine-tuned on\nthese kinds of problems, achieved better than 90% accuracy. In this paper, we\nreview the history of the Winograd Schema Challenge and assess its\nsignificance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocijan_V/0/1/0/all/0/1\">Vid Kocijan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Ernest Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_G/0/1/0/all/0/1\">Gary Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgenstern_L/0/1/0/all/0/1\">Leora Morgenstern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset. (arXiv:2201.02419v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02419","description":"<p>Automatic speech recognition (ASR) on low resource languages improves the\naccess of linguistic minorities to technological advantages provided by\nartificial intelligence (AI). In this paper, we address the problem of data\nscarcity for the Hong Kong Cantonese language by creating a new Cantonese\ndataset. Our dataset, Multi-Domain Cantonese Corpus (MDCC), consists of 73.6\nhours of clean read speech paired with transcripts, collected from Cantonese\naudiobooks from Hong Kong. It comprises philosophy, politics, education,\nculture, lifestyle and family domains, covering a wide range of topics. We also\nreview all existing Cantonese datasets and analyze them according to their\nspeech type, data source, total size and availability. We further conduct\nexperiments with Fairseq S2T Transformer, a state-of-the-art ASR model, on the\nbiggest existing dataset, Common Voice zh-HK, and our proposed MDCC, and the\nresults show the effectiveness of our dataset. In addition, we create a\npowerful and robust Cantonese ASR model by applying multi-dataset learning on\nMDCC and Common Voice zh-HK.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yiu_C/0/1/0/all/0/1\">Cheuk Tung Shadow Yiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1\">Holy Lovenia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J. Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bertram E. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. (arXiv:2201.03713v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03713","description":"<p>We introduce CVSS, a massively multilingual-to-English speech-to-speech\ntranslation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21\nlanguages into English. CVSS is derived from the Common Voice speech corpus and\nthe CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the\ntranslation text from CoVoST 2 into speech using state-of-the-art TTS systems.\nTwo versions of translation speeches are provided: 1) CVSS-C: All the\ntranslation speeches are in a single high-quality canonical voice; 2) CVSS-T:\nThe translation speeches are in voices transferred from the corresponding\nsource speeches. In addition, CVSS provides normalized translation text which\nmatches the pronunciation in the translation speech. On each version of CVSS,\nwe built baseline multilingual direct S2ST models and cascade S2ST models,\nverifying the effectiveness of the corpus. To build strong cascade S2ST\nbaselines, we trained an ST model on CoVoST 2, which outperforms the previous\nstate-of-the-art trained on the corpus without extra data by 5.8 BLEU.\nNevertheless, the performance of the direct S2ST models approaches the strong\ncascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU\ndifference on ASR transcribed translation when initialized from matching ST\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1\">Heiga Zen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Warm Start and a Clean Crawled Corpus -- A Recipe for Good Language Models. (arXiv:2201.05601v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05601","description":"<p>We train several language models for Icelandic, including IceBERT, that\nachieve state-of-the-art performance in a variety of downstream tasks,\nincluding part-of-speech tagging, named entity recognition, grammatical error\ndetection and constituency parsing. To train the models we introduce a new\ncorpus of Icelandic text, the Icelandic Common Crawl Corpus (IC3), a collection\nof high quality texts found online by targeting the Icelandic top-level-domain\n(TLD). Several other public data sources are also collected for a total of 16GB\nof Icelandic text. To enhance the evaluation of model performance and to raise\nthe bar in baselines for Icelandic, we translate and adapt the WinoGrande\ndataset for co-reference resolution. Through these efforts we demonstrate that\na properly cleaned crawled corpus is sufficient to achieve state-of-the-art\nresults in NLP applications for low to medium resource languages, by comparison\nwith models trained on a curated corpus. We further show that initializing\nmodels using existing multilingual models can lead to state-of-the-art results\nfor some downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1\">V&#xe9;steinn Sn&#xe6;bjarnarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonarson_H/0/1/0/all/0/1\">Haukur Barri S&#xed;monarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragnarsson_P/0/1/0/all/0/1\">P&#xe9;tur Orri Ragnarsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingolfsdottir_S/0/1/0/all/0/1\">Svanhv&#xed;t Lilja Ing&#xf3;lfsd&#xf3;ttir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_H/0/1/0/all/0/1\">Haukur P&#xe1;ll J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7B%5CTH%7Dorsteinsson_V/0/1/0/all/0/1\">Vilhj&#xe1;lmur &#xde;orsteinsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Einarsson_H/0/1/0/all/0/1\">Hafsteinn Einarsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Staged Cross-Lingual Acoustic Model Adaption for Robust Speech Recognition in Real-World Applications -- A Case Study on German Oral History Interviews. (arXiv:2005.12562v1 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2005.12562","description":"<p>While recent automatic speech recognition systems achieve remarkable\nperformance when large amounts of adequate, high quality annotated speech data\nis used for training, the same systems often only achieve an unsatisfactory\nresult for tasks in domains that greatly deviate from the conditions\nrepresented by the training data. For many real-world applications, there is a\nlack of sufficient data that can be directly used for training robust speech\nrecognition systems. To address this issue, we propose and investigate an\napproach that performs a robust acoustic model adaption to a target domain in a\ncross-lingual, multi-staged manner. Our approach enables the exploitation of\nlarge-scale training data from other domains in both the same and other\nlanguages. We evaluate our approach using the challenging task of German oral\nhistory interviews, where we achieve a relative reduction of the word error\nrate by more than 30% compared to a model trained from scratch only on the\ntarget domain, and 6-7% relative compared to a model trained robustly on 1000\nhours of same-language out-of-domain training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gref_M/0/1/0/all/0/1\">Michael Gref</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walter_O/0/1/0/all/0/1\">Oliver Walter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_C/0/1/0/all/0/1\">Christoph Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohler_J/0/1/0/all/0/1\">Joachim K&#xf6;hler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Disentanglement enables cross-domain Hippocampus Segmentation. (arXiv:2201.05650v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05650","description":"<p>Limited amount of labelled training data are a common problem in medical\nimaging. This makes it difficult to train a well-generalised model and\ntherefore often leads to failure in unknown domains. Hippocampus segmentation\nfrom magnetic resonance imaging (MRI) scans is critical for the diagnosis and\ntreatment of neuropsychatric disorders. Domain differences in contrast or shape\ncan significantly affect segmentation. We address this issue by disentangling a\nT1-weighted MRI image into its content and domain. This separation enables us\nto perform a domain transfer and thus convert data from new sources into the\ntraining domain. This step thus simplifies the segmentation problem, resulting\nin higher quality segmentations. We achieve the disentanglement with the\nproposed novel methodology 'Content Domain Disentanglement GAN', and we propose\nto retrain the UNet on the transformed outputs to deal with GAN-specific\nartefacts. With these changes, we are able to improve performance on unseen\ndomains by 6-13% and outperform state-of-the-art domain transfer methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kalkhof_J/0/1/0/all/0/1\">John Kalkhof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in Action:Weakly Supervised Action Segmentation. (arXiv:2201.05675v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05675","description":"<p>The video action segmentation task is regularly explored under weaker forms\nof supervision, such as transcript supervision, where a list of actions is\neasier to obtain than dense frame-wise labels. In this formulation, the task\npresents various challenges for sequence modeling approaches due to the\nemphasis on action transition points, long sequence lengths, and frame\ncontextualization, making the task well-posed for transformers. Given\ndevelopments enabling transformers to scale linearly, we demonstrate through\nour architecture how they can be applied to improve action alignment accuracy\nover the equivalent RNN-based models with the attention mechanism focusing\naround salient action transition regions. Additionally, given the recent focus\non inference-time transcript selection, we propose a supplemental transcript\nembedding approach to select transcripts more quickly at inference-time.\nFurthermore, we subsequently demonstrate how this approach can also improve the\noverall segmentation performance. Finally, we evaluate our proposed methods\nacross the benchmark datasets to better understand the applicability of\ntransformers and the importance of transcript selection on this video-driven\nweakly-supervised task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ridley_J/0/1/0/all/0/1\">John Ridley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coskun_H/0/1/0/all/0/1\">Huseyin Coskun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1\">David Joseph Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective Transformation Layer. (arXiv:2201.05706v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05706","description":"<p>Incorporating geometric transformations that reflect the relative position\nchanges between an observer and an object into computer vision and deep\nlearning models has attracted much attention in recent years. However, the\nexisting proposals mainly focus on affine transformations that cannot fully\nshow viewpoint changes. Furthermore, current solutions often apply a neural\nnetwork module to learn a single transformation matrix, which ignores the\npossibility for various viewpoints and creates extra to-be-trained module\nparameters. In this paper, a layer (PT layer) is proposed to learn the\nperspective transformations that not only model the geometries in affine\ntransformation but also reflect the viewpoint changes. In addition, being able\nto be directly trained with gradient descent like traditional layers such as\nconvolutional layers, a single proposed PT layer can learn an adjustable number\nof multiple viewpoints without training extra module parameters. The\nexperiments and evaluations confirm the superiority of the proposed PT layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khatri_N/0/1/0/all/0/1\">Nishan Khatri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1\">Agnibh Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yucong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_F/0/1/0/all/0/1\">Frank Shih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-free Online Test-time Adaptation. (arXiv:2201.05718v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05718","description":"<p>Training state-of-the-art vision models has become prohibitively expensive\nfor researchers and practitioners. For the sake of accessibility and resource\nreuse, it is important to focus on adapting these models to a variety of\ndownstream scenarios. An interesting and practical paradigm is online test-time\nadaptation, according to which training data is inaccessible, no labelled data\nfrom the test distribution is available, and adaptation can only happen at test\ntime and on a handful of samples. In this paper, we investigate how test-time\nadaptation methods fare for a number of pre-trained models on a variety of\nreal-world scenarios, significantly extending the way they have been originally\nevaluated. We show that they perform well only in narrowly-defined experimental\nsetups and sometimes fail catastrophically when their hyperparameters are not\nselected for the same scenario in which they are being tested. Motivated by the\ninherent uncertainty around the conditions that will ultimately be encountered\nat test time, we propose a particularly \"conservative\" approach, which\naddresses the problem with a Laplacian Adjusted Maximum-likelihood Estimation\n(LAME) objective. By adapting the model's output (not its parameters), and\nsolving our objective with an efficient concave-convex procedure, our approach\nexhibits a much higher average accuracy across scenarios than existing methods,\nwhile being notably faster and have a much lower memory footprint. Code\navailable at https://github.com/fiveai/LAME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1\">Romain Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertinetto_L/0/1/0/all/0/1\">Luca Bertinetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Temporally and Semantically Consistent Unpaired Video-to-video Translation Through Pseudo-Supervision From Synthetic Optical Flow. (arXiv:2201.05723v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05723","description":"<p>Unpaired video-to-video translation aims to translate videos between a source\nand a target domain without the need of paired training data, making it more\nfeasible for real applications. Unfortunately, the translated videos generally\nsuffer from temporal and semantic inconsistency. To address this, many existing\nworks adopt spatiotemporal consistency constraints incorporating temporal\ninformation based on motion estimation. However, the inaccuracies in the\nestimation of motion deteriorate the quality of the guidance towards\nspatiotemporal consistency, which leads to unstable translation. In this work,\nwe propose a novel paradigm that regularizes the spatiotemporal consistency by\nsynthesizing motions in input videos with the generated optical flow instead of\nestimating them. Therefore, the synthetic motion can be applied in the\nregularization paradigm to keep motions consistent across domains without the\nrisk of errors in motion estimation. Thereafter, we utilize our unsupervised\nrecycle and unsupervised spatial loss, guided by the pseudo-supervision\nprovided by the synthetic optical flow, to accurately enforce spatiotemporal\nconsistency in both domains. Experiments show that our method is versatile in\nvarious scenarios and achieves state-of-the-art performance in generating\ntemporally and semantically consistent videos. Code is available at:\nhttps://github.com/wangkaihong/Unsup_Recycle_GAN/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akash_K/0/1/0/all/0/1\">Kumar Akash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misu_T/0/1/0/all/0/1\">Teruhisa Misu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05729","description":"<p>Contrastive language-image pretraining (CLIP) links vision and language\nmodalities into a unified embedding space, yielding the tremendous potential\nfor vision-language (VL) tasks. While early concurrent works have begun to\nstudy this potential on a subset of tasks, important questions remain: 1) What\nis the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in\nlow-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches\nwithout impacting inference or pretraining complexity? In this work, we seek to\nanswer these questions through two key contributions. First, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata availability constraints and conditions of domain shift. Second, we\npropose an approach, named CLIP Targeted Distillation (CLIP-TD), to\nintelligently distill knowledge from CLIP into existing architectures using a\ndynamically weighted objective applied to adaptively selected tokens per\ninstance. Experiments demonstrate that our proposed CLIP-TD leads to\nexceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to\n71.3%) conditions of VCR, while simultaneously improving performance under\nstandard fully-supervised conditions (up to 2%), achieving state-of-art\nperformance on VCR compared to other single models that are pretrained with\nimage-text data only. On SNLI-VE, CLIP-TD produces significant gains in\nlow-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On\nVQA, CLIP-TD provides improvement in low-shot (up to 9%), and in\nfully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works\nutilizing CLIP for finetuning, as well as baseline naive distillation\napproaches. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchical Graph Representation for Image Manipulation Detection. (arXiv:2201.05730v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05730","description":"<p>The objective of image manipulation detection is to identify and locate the\nmanipulated regions in the images. Recent approaches mostly adopt the\nsophisticated Convolutional Neural Networks (CNNs) to capture the tampering\nartifacts left in the images to locate the manipulated regions. However, these\napproaches ignore the feature correlations, i.e., feature inconsistencies,\nbetween manipulated regions and non-manipulated regions, leading to inferior\ndetection performance. To address this issue, we propose a hierarchical Graph\nConvolutional Network (HGCN-Net), which consists of two parallel branches: the\nbackbone network branch and the hierarchical graph representation learning\n(HGRL) branch for image manipulation detection. Specifically, the feature maps\nof a given image are extracted by the backbone network branch, and then the\nfeature correlations within the feature maps are modeled as a set of\nfully-connected graphs for learning the hierarchical graph representation by\nthe HGRL branch. The learned hierarchical graph representation can sufficiently\ncapture the feature correlations across different scales, and thus it provides\nhigh discriminability for distinguishing manipulated and non-manipulated\nregions. Extensive experiments on four public datasets demonstrate that the\nproposed HGCN-Net not only provides promising detection accuracy, but also\nachieves strong robustness under a variety of common image attacks in the task\nof image manipulation detection, compared to the state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wenyan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_M/0/1/0/all/0/1\">Miaogen Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xin Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Q. M. Jonathan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance. (arXiv:2201.05739v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05739","description":"<p>Action recognition is a key algorithmic part of emerging on-the-edge smart\nvideo surveillance and security systems. Skeleton-based action recognition is\nan attractive approach which, instead of using RGB pixel data, relies on human\npose information to classify appropriate actions. However, existing algorithms\noften assume ideal conditions that are not representative of real-world\nlimitations, such as noisy input, latency requirements, and edge resource\nconstraints.\n</p>\n<p>To address the limitations of existing approaches, this paper presents\nReal-World Graph Convolution Networks (RW-GCNs), an architecture-level solution\nfor meeting the domain constraints of Real World Skeleton-based Action\nRecognition. Inspired by the presence of feedback connections in the human\nvisual cortex, RW-GCNs leverage attentive feedback augmentation on existing\nnear state-of-the-art (SotA) Spatial-Temporal Graph Convolution Networks\n(ST-GCNs). The ST-GCNs' design choices are derived from information\ntheory-centric principles to address both the spatial and temporal noise\ntypically encountered in end-to-end real-time and on-the-edge smart video\nsystems. Our results demonstrate RW-GCNs' ability to serve these applications\nby achieving a new SotA accuracy on the NTU-RGB-D-120 dataset at 94.1%, and\nachieving 32X less latency than baseline ST-GCN applications while still\nachieving 90.4% accuracy on the Northwestern UCLA dataset in the presence of\nspatial keypoint noise. RW-GCNs further show system scalability by running on\nthe 10X cost effective NVIDIA Jetson Nano (as opposed to NVIDIA Xavier NX),\nwhile still maintaining a respectful range of throughput (15.6 to 5.5 Actions\nper Second) on the resource constrained device. The code is available here:\nhttps://github.com/TeCSAR-UNCC/RW-GCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_J/0/1/0/all/0/1\">Justin Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_C/0/1/0/all/0/1\">Christopher Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on RGB-D Datasets. (arXiv:2201.05761v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05761","description":"<p>RGB-D data is essential for solving many problems in computer vision.\nHundreds of public RGB-D datasets containing various scenes, such as indoor,\noutdoor, aerial, driving, and medical, have been proposed. These datasets are\nuseful for different applications and are fundamental for addressing classic\ncomputer vision tasks, such as monocular depth estimation. This paper reviewed\nand categorized image datasets that include depth information. We gathered 203\ndatasets that contain accessible data and grouped them into three categories:\nscene/objects, body, and medical. We also provided an overview of the different\ntypes of sensors, depth applications, and we examined trends and future\ndirections of the usage and creation of datasets containing depth data, and how\nthey can be applied to investigate the development of generalizable machine\nlearning models in the monocular depth estimation field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopes_A/0/1/0/all/0/1\">Alexandre Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1\">Roberto Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrini_H/0/1/0/all/0/1\">Helio Pedrini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Compressive Imaging Reconstruction Using Convolution and Spectral Contextual Transformer. (arXiv:2201.05768v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05768","description":"<p>Spectral compressive imaging (SCI) is able to encode the high-dimensional\nhyperspectral image to a 2D measurement, and then uses algorithms to\nreconstruct the spatio-spectral data-cube. At present, the main bottleneck of\nSCI is the reconstruction algorithm, and the state-of-the-art (SOTA)\nreconstruction methods generally face the problem of long reconstruction time\nand/or poor detail recovery. In this paper, we propose a novel hybrid network\nmodule, namely CSCoT (Convolution and Spectral Contextual Transformer) block,\nwhich can acquire the local perception of convolution and the global perception\nof transformer simultaneously, and is conducive to improving the quality of\nreconstruction to restore fine details. We integrate the proposed CSCoT block\ninto deep unfolding framework based on the generalized alternating projection\nalgorithm, and further propose the GAP-CSCoT network. Finally, we apply the\nGAP-CSCoT algorithm to SCI reconstruction. Through the experiments of extensive\nsynthetic and real data, our proposed model achieves higher reconstruction\nquality ($&gt;$2dB in PSNR on simulated benchmark datasets) and shorter running\ntime than existing SOTA algorithms by a large margin. The code and models will\nbe released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lishun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zongliang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1\">Yong Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asymmetric Hash Code Learning for Remote Sensing Image Retrieval. (arXiv:2201.05772v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05772","description":"<p>Remote sensing image retrieval (RSIR), aiming at searching for a set of\nsimilar items to a given query image, is a very important task in remote\nsensing applications. Deep hashing learning as the current mainstream method\nhas achieved satisfactory retrieval performance. On one hand, various deep\nneural networks are used to extract semantic features of remote sensing images.\nOn the other hand, the hashing techniques are subsequently adopted to map the\nhigh-dimensional deep features to the low-dimensional binary codes. This kind\nof methods attempts to learn one hash function for both the query and database\nsamples in a symmetric way. However, with the number of database samples\nincreasing, it is typically time-consuming to generate the hash codes of\nlarge-scale database images. In this paper, we propose a novel deep hashing\nmethod, named asymmetric hash code learning (AHCL), for RSIR. The proposed AHCL\ngenerates the hash codes of query and database images in an asymmetric way. In\nmore detail, the hash codes of query images are obtained by binarizing the\noutput of the network, while the hash codes of database images are directly\nlearned by solving the designed objective function. In addition, we combine the\nsemantic information of each image and the similarity information of pairs of\nimages as supervised information to train a deep hashing network, which\nimproves the representation ability of deep features and hash codes. The\nexperimental results on three public datasets demonstrate that the proposed\nmethod outperforms symmetric methods in terms of retrieval accuracy and\nefficiency. The source code is available at\nhttps://github.com/weiweisong415/Demo AHCL for TGRS2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dian_R/0/1/0/all/0/1\">Renwei Dian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benediktsson_J/0/1/0/all/0/1\">J&#xf3;n Atli Benediktsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainability Tools Enabling Deep Learning in Future In-Situ Real-Time Planetary Explorations. (arXiv:2201.05775v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05775","description":"<p>Deep learning (DL) has proven to be an effective machine learning and\ncomputer vision technique. DL-based image segmentation, object recognition and\nclassification will aid many in-situ Mars rover tasks such as path planning and\nartifact recognition/extraction. However, most of the Deep Neural Network (DNN)\narchitectures are so complex that they are considered a 'black box'. In this\npaper, we used integrated gradients to describe the attributions of each neuron\nto the output classes. It provides a set of explainability tools (ET) that\nopens the black box of a DNN so that the individual contribution of neurons to\ncategory classification can be ranked and visualized. The neurons in each dense\nlayer are mapped and ranked by measuring expected contribution of a neuron to a\nclass vote given a true image label. The importance of neurons is prioritized\naccording to their correct or incorrect contribution to the output classes and\nsuppression or bolstering of incorrect classes, weighted by the size of each\nclass. ET provides an interface to prune the network to enhance high-rank\nneurons and remove low-performing neurons. ET technology will make DNNs smaller\nand more efficient for implementation in small embedded systems. It also leads\nto more explainable and testable DNNs that can make systems easier for\nValidation \\&amp; Verification. The goal of ET technology is to enable the adoption\nof DL in future in-situ planetary exploration missions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lundstrom_D/0/1/0/all/0/1\">Daniel Lundstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huyen_A/0/1/0/all/0/1\">Alexander Huyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mevada_A/0/1/0/all/0/1\">Arya Mevada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kyongsik Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Thomas Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Multi-View Representation Learning. (arXiv:2201.05776v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05776","description":"<p>Learning from different data views by exploring the underlying complementary\ninformation among them can endow the representation with stronger expressive\nability. However, high-dimensional features tend to contain noise, and\nfurthermore, the quality of data usually varies for different samples (even for\ndifferent views), i.e., one view may be informative for one sample but not the\ncase for another. Therefore, it is quite challenging to integrate multi-view\nnoisy data under unsupervised setting. Traditional multi-view methods either\nsimply treat each view with equal importance or tune the weights of different\nviews to fixed values, which are insufficient to capture the dynamic noise in\nmulti-view data. In this work, we devise a novel unsupervised multi-view\nlearning approach, termed as Dynamic Uncertainty-Aware Networks (DUA-Nets).\nGuided by the uncertainty of data estimated from the generation perspective,\nintrinsic information from multiple views is integrated to obtain noise-free\nrepresentations. Under the help of uncertainty, DUA-Nets weigh each view of\nindividual sample according to data quality so that the high-quality samples\n(or views) can be fully exploited while the effects from the noisy samples (or\nviews) will be alleviated. Our model achieves superior performance in extensive\nexperiments and shows the robustness to noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yu Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zongbo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghua Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic decoupled representation learning for remote sensing image change detection. (arXiv:2201.05778v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05778","description":"<p>Contemporary transfer learning-based methods to alleviate the data\ninsufficiency in change detection (CD) are mainly based on ImageNet\npre-training. Self-supervised learning (SSL) has recently been introduced to\nremote sensing (RS) for learning in-domain representations. Here, we propose a\nsemantic decoupled representation learning for RS image CD. Typically, the\nobject of interest (e.g., building) is relatively small compared to the vast\nbackground. Different from existing methods expressing an image into one\nrepresentation vector that may be dominated by irrelevant land-covers, we\ndisentangle representations of different semantic regions by leveraging the\nsemantic mask. We additionally force the model to distinguish different\nsemantic representations, which benefits the recognition of objects of interest\nin the downstream CD task. We construct a dataset of bitemporal images with\nsemantic masks in an effortless manner for pre-training. Experiments on two CD\ndatasets show our model outperforms ImageNet pre-training, in-domain supervised\npre-training, and several recent SSL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zao_Y/0/1/0/all/0/1\">Yifan Zao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liqin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OneDConv: Generalized Convolution For Transform-Invariant Representation. (arXiv:2201.05781v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05781","description":"<p>Convolutional Neural Networks (CNNs) have exhibited their great power in a\nvariety of vision tasks. However, the lack of transform-invariant property\nlimits their further applications in complicated real-world scenarios. In this\nwork, we proposed a novel generalized one dimension convolutional operator\n(OneDConv), which dynamically transforms the convolution kernels based on the\ninput features in a computationally and parametrically efficient manner. The\nproposed operator can extract the transform-invariant features naturally. It\nimproves the robustness and generalization of convolution without sacrificing\nthe performance on common images. The proposed OneDConv operator can substitute\nthe vanilla convolution, thus it can be incorporated into current popular\nconvolutional architectures and trained end-to-end readily. On several popular\nbenchmarks, OneDConv outperforms the original convolution operation and other\nproposed models both in canonical and distorted images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_H/0/1/0/all/0/1\">Haohan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Ke Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">C. L. Philip Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification. (arXiv:2201.05809v1 [cs.LG])","link":"http://arxiv.org/abs/2201.05809","description":"<p>In this paper, we first introduce batch normalization to the edRVFL network.\nThis re-normalization method can help the network avoid divergence of the\nhidden features. Then we propose novel variants of Ensemble Deep Random Vector\nFunctional Link (edRVFL). Weighted edRVFL (WedRVFL) uses weighting methods to\ngive training samples different weights in different layers according to how\nthe samples were classified confidently in the previous layer thereby\nincreasing the ensemble's diversity and accuracy. Furthermore, a pruning-based\nedRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based\non their importance for classification before generating the next hidden layer.\nThrough this method, we ensure that the randomly generated inferior features\nwill not propagate to deeper layers. Subsequently, the combination of weighting\nand pruning, called Weighting and Pruning based Ensemble Deep Random Vector\nFunctional Link Network (WPedRVFL), is proposed. We compare their performances\nwith other state-of-the-art deep feedforward neural networks (FNNs) on 24\ntabular UCI classification datasets. The experimental results illustrate the\nsuperior performance of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qiushi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganthan_P/0/1/0/all/0/1\">Ponnuthurai Nagaratnam Suganthan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katuwal_R/0/1/0/all/0/1\">Rakesh Katuwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage is Enough: A Concise Deep Unfolding Reconstruction Network for Flexible Video Compressive Sensing. (arXiv:2201.05810v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05810","description":"<p>We consider the reconstruction problem of video compressive sensing (VCS)\nunder the deep unfolding/rolling structure. Yet, we aim to build a flexible and\nconcise model using minimum stages. Different from existing deep unfolding\nnetworks used for inverse problems, where more stages are used for higher\nperformance but without flexibility to different masks and scales, hereby we\nshow that a 2-stage deep unfolding network can lead to the state-of-the-art\n(SOTA) results (with a 1.7dB gain in PSNR over the single stage model, RevSCI)\nin VCS. The proposed method possesses the properties of adaptation to new masks\nand ready to scale to large data without any additional training thanks to the\nadvantages of deep unfolding. Furthermore, we extend the proposed model for\ncolor VCS to perform joint reconstruction and demosaicing. Experimental results\ndemonstrate that our 2-stage model has also achieved SOTA on color VCS\nreconstruction, leading to a &gt;2.3dB gain in PSNR over the previous SOTA\nalgorithm based on plug-and-play framework, meanwhile speeds up the\nreconstruction by &gt;17 times. In addition, we have found that our network is\nalso flexible to the mask modulation and scale size for color VCS\nreconstruction so that a single trained network can be applied to different\nhardware systems. The code and models will be released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_S/0/1/0/all/0/1\">Siming Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Critical Analysis of Image-based Camera Pose Estimation Techniques. (arXiv:2201.05816v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05816","description":"<p>Camera, and associated with its objects within the field of view,\nlocalization could benefit many computer vision fields, such as autonomous\ndriving, robot navigation, and augmented reality (AR). In this survey, we first\nintroduce specific application areas and the evaluation metrics for camera\nlocalization pose according to different sub-tasks (learning-based 2D-2D task,\nfeature-based 2D-3D task, and 3D-3D task). Then, we review common methods for\nstructure-based camera pose estimation approaches, absolute pose regression and\nrelative pose regression approaches by critically modelling the methods to\ninspire further improvements in their algorithms such as loss functions, neural\nnetwork structures. Furthermore, we summarise what are the popular datasets\nused for camera localization and compare the quantitative and qualitative\nresults of these methods with detailed performance metrics. Finally, we discuss\nfuture research possibilities and applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Meng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Youchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poslad_S/0/1/0/all/0/1\">Stefan Poslad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offline-Online Associated Camera-Aware Proxies for Unsupervised Person Re-identification. (arXiv:2201.05820v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05820","description":"<p>Recently, unsupervised person re-identification (Re-ID) has received\nincreasing research attention due to its potential for label-free applications.\nA promising way to address unsupervised Re-ID is clustering-based, which\ngenerates pseudo labels by clustering and uses the pseudo labels to train a\nRe-ID model iteratively. However, most clustering-based methods take each\ncluster as a pseudo identity class, neglecting the intra-cluster variance\nmainly caused by the change of cameras. To address this issue, we propose to\nsplit each single cluster into multiple proxies according to camera views. The\ncamera-aware proxies explicitly capture local structures within clusters, by\nwhich the intra-ID variance and inter-ID similarity can be better tackled.\nAssisted with the camera-aware proxies, we design two proxy-level contrastive\nlearning losses that are, respectively, based on offline and online association\nresults. The offline association directly associates proxies according to the\nclustering and splitting results, while the online strategy dynamically\nassociates proxies in terms of up-to-date features to reduce the noise caused\nby the delayed update of pseudo labels. The combination of two losses enable us\nto train a desirable Re-ID model. Extensive experiments on three person Re-ID\ndatasets and one vehicle Re-ID dataset show that our proposed approach\ndemonstrates competitive performance with state-of-the-art methods. Code will\nbe available at: https://github.com/Terminator8758/O2CAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Menglin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_B/0/1/0/all/0/1\">Baisheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xiaojin Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View representation learning in Multi-Task Scene. (arXiv:2201.05829v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05829","description":"<p>Over recent decades have witnessed considerable progress in whether\nmulti-task learning or multi-view learning, but the situation that consider\nboth learning scenes simultaneously has received not too much attention. How to\nutilize multiple views latent representation of each single task to improve\neach learning task performance is a challenge problem. Based on this, we\nproposed a novel semi-supervised algorithm, termed as Multi-Task Multi-View\nlearning based on Common and Special Features (MTMVCSF). In general,\nmulti-views are the different aspects of an object and every view includes the\nunderlying common or special information of this object. As a consequence, we\nwill mine multiple views jointly latent factor of each learning task which\nconsists of each view special feature and the common feature of all views. By\nthis way, the original multi-task multi-view data has degenerated into\nmulti-task data, and exploring the correlations among multiple tasks enables to\nmake an improvement on the performance of learning algorithm. Another obvious\nadvantage of this approach is that we get latent representation of the set of\nunlabeled instances by the constraint of regression task with labeled\ninstances. The performance of classification and semi-supervised clustering\ntask in these latent representations perform obviously better than it in raw\ndata. Furthermore, an anti-noise multi-task multi-view algorithm called\nAN-MTMVCSF is proposed, which has a strong adaptability to noise labels. The\neffectiveness of these algorithms is proved by a series of well-designed\nexperiments on both real world and synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Run-kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_S/0/1/0/all/0/1\">Si-ming Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xin Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tailor Versatile Multi-modal Learning for Multi-label Emotion Recognition. (arXiv:2201.05834v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05834","description":"<p>Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various\nhuman emotions from heterogeneous visual, audio and text modalities. Previous\nmethods mainly focus on projecting multiple modalities into a common latent\nspace and learning an identical representation for all labels, which neglects\nthe diversity of each modality and fails to capture richer semantic information\nfor each label from different perspectives. Besides, associated relationships\nof modalities and labels have not been fully exploited. In this paper, we\npropose versaTile multi-modAl learning for multI-labeL emOtion Recognition\n(TAILOR), aiming to refine multi-modal representations and enhance\ndiscriminative capacity of each label. Specifically, we design an adversarial\nmulti-modal refinement module to sufficiently explore the commonality among\ndifferent modalities and strengthen the diversity of each modality. To further\nexploit label-modal dependence, we devise a BERT-like cross-modal encoder to\ngradually fuse private and common modality representations in a granularity\ndescent way, as well as a label-guided decoder to adaptively generate a\ntailored representation for each label with the guidance of label semantics. In\naddition, we conduct experiments on the benchmark MMER dataset CMU-MOSEI in\nboth aligned and unaligned settings, which demonstrate the superiority of\nTAILOR over the state-of-the-arts. Code is available at\nhttps://github.com/kniter1/TAILOR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jundong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Parking Space Detection under Hazy conditions using Convolutional Neural Networks: A Novel Approach. (arXiv:2201.05858v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05858","description":"<p>Limited urban parking space combined with urbanization has necessitated the\ndevelopment of smart parking systems that can communicate the availability of\nparking slots to the end users. Towards this, various deep learning based\nsolutions using convolutional neural networks have been proposed for parking\nspace occupation detection. Though these approaches are robust to partial\nobstructions and lighting conditions, their performance is found to degrade in\nthe presence of haze conditions. Looking in this direction, this paper\ninvestigates the use of dehazing networks that improves the performance of\nparking space occupancy classifier under hazy conditions. Additionally,\ntraining procedures are proposed for dehazing networks to maximize the\nperformance of the system on both hazy and non-hazy conditions. The proposed\nsystem is deployable as part of existing smart parking systems where limited\nnumber of cameras are used to monitor hundreds of parking spaces. To validate\nour approach, we have developed a custom hazy parking system dataset from\nreal-world task-driven test set of RESIDE-\\b{eta} dataset. The proposed\napproach is tested against existing state-of-the-art parking space detectors on\nCNRPark-EXT and hazy parking system datasets. Experimental results indicate\nthat there is a significant accuracy improvement of the proposed approach on\nthe hazy parking system dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Satyanath_G/0/1/0/all/0/1\">Gaurav Satyanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_J/0/1/0/all/0/1\">Jajati Keshari Sahoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roul_R/0/1/0/all/0/1\">Rajendra Kumar Roul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDT-DCSCN for Simultaneous Super-Resolution and Deblurring of Text Images. (arXiv:2201.05865v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05865","description":"<p>Deep convolutional neural networks (Deep CNN) have achieved hopeful\nperformance for single image super-resolution. In particular, the Deep CNN skip\nConnection and Network in Network (DCSCN) architecture has been successfully\napplied to natural images super-resolution. In this work we propose an approach\ncalled SDT-DCSCN that jointly performs super-resolution and deblurring of\nlow-resolution blurry text images based on DCSCN. Our approach uses subsampled\nblurry images in the input and original sharp images as ground truth. The used\narchitecture is consists of a higher number of filters in the input CNN layer\nto a better analysis of the text details. The quantitative and qualitative\nevaluation on different datasets prove the high performance of our model to\nreconstruct high-resolution and sharp text images. In addition, in terms of\ncomputational time, our proposed method gives competitive performance compared\nto state of the art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Neji_H/0/1/0/all/0/1\">Hala Neji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halima_M/0/1/0/all/0/1\">Mohamed Ben Halima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nogueras_Iso_J/0/1/0/all/0/1\">Javier Nogueras-Iso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamdani_T/0/1/0/all/0/1\">Tarek. M. Hamdani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qahtani_A/0/1/0/all/0/1\">Abdulrahman M. Qahtani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Almutiry_O/0/1/0/all/0/1\">Omar Almutiry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhahri_H/0/1/0/all/0/1\">Habib Dhahri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alimi_A/0/1/0/all/0/1\">Adel M. Alimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype Guided Network for Anomaly Segmentation. (arXiv:2201.05869v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05869","description":"<p>Semantic segmentation methods can not directly identify abnormal objects in\nimages. Anomaly Segmentation algorithm from this realistic setting can\ndistinguish between in-distribution objects and Out-Of-Distribution (OOD)\nobjects and output the anomaly probability for pixels. In this paper, a\nPrototype Guided Anomaly segmentation Network (PGAN) is proposed to extract\nsemantic prototypes for in-distribution training data from limited annotated\nimages. In the model, prototypes are used to model the hierarchical category\nsemantic information and distinguish OOD pixels. The proposed PGAN model\nincludes a semantic segmentation network and a prototype extraction network.\nSimilarity measures are adopted to optimize the prototypes. The learned\nsemantic prototypes are used as category semantics to compare the similarity\nwith features extracted from test images and then to generate semantic\nsegmentation prediction. The proposed prototype extraction network can also be\nintegrated into most semantic segmentation networks and recognize OOD pixels.\nOn the StreetHazards dataset, the proposed PGAN model produced mIoU of 53.4%\nfor anomaly segmentation. The experimental results demonstrate PGAN may achieve\nthe SOTA performance in the anomaly segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yiqing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_G/0/1/0/all/0/1\">Gaoyun An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation via Bidirectional Cross-Attention Transformer. (arXiv:2201.05887v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05887","description":"<p>Domain Adaptation (DA) aims to leverage the knowledge learned from a source\ndomain with ample labeled data to a target domain with unlabeled data only.\nMost existing studies on DA contribute to learning domain-invariant feature\nrepresentations for both domains by minimizing the domain gap based on\nconvolution-based neural networks. Recently, vision transformers significantly\nimproved performance in multiple vision tasks. Built on vision transformers, in\nthis paper we propose a Bidirectional Cross-Attention Transformer (BCAT) for DA\nwith the aim to improve the performance. In the proposed BCAT, the attention\nmechanism can extract implicit source and target mix-up feature representations\nto narrow the domain discrepancy. Specifically, in BCAT, we design a\nweight-sharing quadruple-branch transformer with a bidirectional\ncross-attention mechanism to learn domain-invariant feature representations.\nExtensive experiments demonstrate that the proposed BCAT model achieves\nsuperior performance on four benchmark datasets over existing state-of-the-art\nDA methods that are based on convolutions or transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data. (arXiv:2201.05905v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05905","description":"<p>Capsule network is a recent new deep network architecture that has been\napplied successfully for medical image segmentation tasks. This work extends\ncapsule networks for volumetric medical image segmentation with self-supervised\nlearning. To improve on the problem of weight initialization compared to\nprevious capsule networks, we leverage self-supervised learning for capsule\nnetworks pre-training, where our pretext-task is optimized by\nself-reconstruction. Our capsule network, SS-3DCapsNet, has a UNet-based\narchitecture with a 3D Capsule encoder and 3D CNNs decoder. Our experiments on\nmultiple datasets including iSeg-2017, Hippocampus, and Cardiac demonstrate\nthat our 3D capsule network with self-supervised pre-training considerably\noutperforms previous capsule networks and 3D-UNets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ly_L/0/1/0/all/0/1\">Loi Ly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-shot Sign Language Recognition. (arXiv:2201.05914v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05914","description":"<p>This paper tackles the problem of zero-shot sign language recognition\n(ZSSLR), where the goal is to leverage models learned over the seen sign\nclasses to recognize the instances of unseen sign classes. In this context,\nreadily available textual sign descriptions and attributes collected from sign\nlanguage dictionaries are utilized as semantic class representations for\nknowledge transfer. For this novel problem setup, we introduce three benchmark\ndatasets with their accompanying textual and attribute descriptions to analyze\nthe problem in detail. Our proposed approach builds spatiotemporal models of\nbody and hand regions. By leveraging the descriptive text and attribute\nembeddings along with these visual representations within a zero-shot learning\nframework, we show that textual and attribute based class definitions can\nprovide effective knowledge for the recognition of previously unseen sign\nclasses. We additionally introduce techniques to analyze the influence of\nbinary attributes in correct and incorrect zero-shot predictions. We anticipate\nthat the introduced approaches and the accompanying datasets will provide a\nbasis for further exploration of zero-shot learning in sign language\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bilge_Y/0/1/0/all/0/1\">Yunus Can Bilge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikizler_Cinbis_N/0/1/0/all/0/1\">Nazli Ikizler-Cinbis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-level Second-order Few-shot Learning. (arXiv:2201.05916v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05916","description":"<p>We propose a Multi-level Second-order (MlSo) few-shot learning network for\nsupervised or unsupervised few-shot image classification and few-shot action\nrecognition. We leverage so-called power-normalized second-order base learner\nstreams combined with features that express multiple levels of visual\nabstraction, and we use self-supervised discriminating mechanisms. As\nSecond-order Pooling (SoP) is popular in image recognition, we employ its basic\nelement-wise variant in our pipeline. The goal of multi-level feature design is\nto extract feature representations at different layer-wise levels of CNN,\nrealizing several levels of visual abstraction to achieve robust few-shot\nlearning. As SoP can handle convolutional feature maps of varying spatial\nsizes, we also introduce image inputs at multiple spatial scales into MlSo. To\nexploit the discriminative information from multi-level and multi-scale\nfeatures, we develop a Feature Matching (FM) module that reweights their\nrespective branches. We also introduce a self-supervised step, which is a\ndiscriminator of the spatial level and the scale of abstraction. Our pipeline\nis trained in an end-to-end manner. With a simple architecture, we demonstrate\nrespectable results on standard datasets such as Omniglot, mini-ImageNet,\ntiered-ImageNet, Open MIC, fine-grained datasets such as CUB Birds, Stanford\nDogs and Cars, and action recognition datasets such as HMDB51, UCF101, and\nmini-MIT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongguang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViTBIS: Vision Transformer for Biomedical Image Segmentation. (arXiv:2201.05920v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05920","description":"<p>In this paper, we propose a novel network named Vision Transformer for\nBiomedical Image Segmentation (ViTBIS). Our network splits the input feature\nmaps into three parts with $1\\times 1$, $3\\times 3$ and $5\\times 5$\nconvolutions in both encoder and decoder. Concat operator is used to merge the\nfeatures before being fed to three consecutive transformer blocks with\nattention mechanism embedded inside it. Skip connections are used to connect\nencoder and decoder transformer blocks. Similarly, transformer blocks and multi\nscale architecture is used in decoder before being linearly projected to\nproduce the output segmentation map. We test the performance of our network\nusing Synapse multi-organ segmentation dataset, Automated cardiac diagnosis\nchallenge dataset, Brain tumour MRI segmentation dataset and Spleen CT\nsegmentation dataset. Without bells and whistles, our network outperforms most\nof the previous state of the art CNN and transformer based models using Dice\nscore and the Hausdorff distance as the evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradTail: Learning Long-Tailed Data Using Gradient-based Sample Weighting. (arXiv:2201.05938v1 [cs.LG])","link":"http://arxiv.org/abs/2201.05938","description":"<p>We propose GradTail, an algorithm that uses gradients to improve model\nperformance on the fly in the face of long-tailed training data distributions.\nUnlike conventional long-tail classifiers which operate on converged - and\npossibly overfit - models, we demonstrate that an approach based on gradient\ndot product agreement can isolate long-tailed data early on during model\ntraining and improve performance by dynamically picking higher sample weights\nfor that data. We show that such upweighting leads to model improvements for\nboth classification and regression models, the latter of which are relatively\nunexplored in the long-tail literature, and that the long-tail examples found\nby gradient alignment are consistent with our semantic expectations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casser_V/0/1/0/all/0/1\">Vincent Casser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kretzschmar_H/0/1/0/all/0/1\">Henrik Kretzschmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Regular Network for Writer Identification. (arXiv:2201.05951v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05951","description":"<p>Writer identification has practical applications for forgery detection and\nforensic science. Most models based on deep neural networks extract features\nfrom character image or sub-regions in character image, which ignoring features\ncontained in page-region image. Our proposed global regular network (GRN) pays\nattention to these features. GRN network consists of two branches: one branch\ntakes page handwriting as input to extract global features, and the other takes\nword handwriting as input to extract local features. Global features and local\nfeatures merge in a global residual way to form overall features of the\nhandwriting. The proposed GRN has two attributions: one is adding a branch to\nextract features contained in page; the other is using residual attention\nnetwork to extract local feature. Experiments demonstrate the effectiveness of\nboth strategies. On CVL dataset, our models achieve impressive 99.98% top-1\naccuracy and 100% top-5 accuracy with shorter training time and fewer network\nparameters, which exceeded the state-of-the-art structure. The experiment shows\nthe powerful ability of the network in the field of writer identification. The\nsource code is available at https://github.com/wangshiyu001/GRN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Centroid Ripple Pattern for Facial Expression Recognition. (arXiv:2201.05958v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05958","description":"<p>In this paper, we propose a new feature descriptor Cross-Centroid Ripple\nPattern (CRIP) for facial expression recognition. CRIP encodes the transitional\npattern of a facial expression by incorporating cross-centroid relationship\nbetween two ripples located at radius r1 and r2 respectively. These ripples are\ngenerated by dividing the local neighborhood region into subregions. Thus, CRIP\nhas ability to preserve macro and micro structural variations in an extensive\nregion, which enables it to deal with side views and spontaneous expressions.\nFurthermore, gradient information between cross centroid ripples provides\nstrenght to captures prominent edge features in active patches: eyes, nose and\nmouth, that define the disparities between different facial expressions. Cross\ncentroid information also provides robustness to irregular illumination.\nMoreover, CRIP utilizes the averaging behavior of pixels at subregions that\nyields robustness to deal with noisy conditions. The performance of proposed\ndescriptor is evaluated on seven comprehensive expression datasets consisting\nof challenging conditions such as age, pose, ethnicity and illumination\nvariations. The experimental results show that our descriptor consistently\nachieved better accuracy rate as compared to existing state-of-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Monu Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_P/0/1/0/all/0/1\">Prafulla Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vipparthi_S/0/1/0/all/0/1\">Santosh Kumar Vipparthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Girdhari Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Residual Encoder-Decoder Network for Segmentation of Retinal Image-Based Exudates in Diabetic Retinopathy Screening. (arXiv:2201.05963v1 [eess.IV])","link":"http://arxiv.org/abs/2201.05963","description":"<p>Diabetic retinopathy refers to the pathology of the retina induced by\ndiabetes and is one of the leading causes of preventable blindness in the\nworld. Early detection of diabetic retinopathy is critical to avoid vision\nproblem through continuous screening and treatment. In traditional clinical\npractice, the involved lesions are manually detected using photographs of the\nfundus. However, this task is cumbersome and time-consuming and requires\nintense effort due to the small size of lesion and low contrast of the images.\nThus, computer-assisted diagnosis of diabetic retinopathy based on the\ndetection of red lesions is actively being explored recently. In this paper, we\npresent a convolutional neural network with residual skip connection for the\nsegmentation of exudates in retinal images. To improve the performance of\nnetwork architecture, a suitable image augmentation technique is used. The\nproposed network can robustly segment exudates with high accuracy, which makes\nit suitable for diabetic retinopathy screening. Comparative performance\nanalysis of three benchmark databases: HEI-MED, E-ophtha, and DiaretDB1 is\npresented. It is shown that the proposed method achieves accuracy (0.98, 0.99,\n0.98) and sensitivity (0.97, 0.92, and 0.95) on E-ophtha, HEI-MED, and\nDiaReTDB1, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Manan_M/0/1/0/all/0/1\">Malik A. Manan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_T/0/1/0/all/0/1\">Tariq M. Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saadat_A/0/1/0/all/0/1\">Ahsan Saadat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arsalan_M/0/1/0/all/0/1\">Muhammad Arsalan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naqvi_S/0/1/0/all/0/1\">Syed S. Naqvi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Cross-scale Attention Network for Efficient LiDAR Panoptic Segmentation. (arXiv:2201.05972v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05972","description":"<p>Two major challenges of 3D LiDAR Panoptic Segmentation (PS) are that point\nclouds of an object are surface-aggregated and thus hard to model the\nlong-range dependency especially for large instances, and that objects are too\nclose to separate each other. Recent literature addresses these problems by\ntime-consuming grouping processes such as dual-clustering, mean-shift offsets,\netc., or by bird-eye-view (BEV) dense centroid representation that downplays\ngeometry. However, the long-range geometry relationship has not been\nsufficiently modeled by local feature learning from the above methods. To this\nend, we present SCAN, a novel sparse cross-scale attention network to first\nalign multi-scale sparse features with global voxel-encoded attention to\ncapture the long-range relationship of instance context, which can boost the\nregression accuracy of the over-segmented large objects. For the\nsurface-aggregated points, SCAN adopts a novel sparse class-agnostic\nrepresentation of instance centroids, which can not only maintain the sparsity\nof aligned features to solve the under-segmentation on small objects, but also\nreduce the computation amount of the network through sparse convolution. Our\nmethod outperforms previous methods by a large margin in the SemanticKITTI\ndataset for the challenging 3D PS task, achieving 1st place with a real-time\ninference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1\">Rui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Maosheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xiaoyi Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tongyi Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Object-level Topological Semantic Mapping and Long-term Global Localization based on Graph Matching. (arXiv:2201.05977v1 [cs.RO])","link":"http://arxiv.org/abs/2201.05977","description":"<p>Mapping and localization are two essential tasks for mobile robots in\nreal-world applications. However, largescale and dynamic scenes challenge the\naccuracy and robustness of most current mature solutions. This situation\nbecomes even worse when computational resources are limited. In this paper, we\npresent a novel lightweight object-level mapping and localization method with\nhigh accuracy and robustness. Different from previous methods, our method does\nnot need a prior constructed precise geometric map, which greatly releases the\nstorage burden, especially for large-scale navigation. We use object-level\nfeatures with both semantic and geometric information to model landmarks in the\nenvironment. Particularly, a learning topological primitive is first proposed\nto efficiently obtain and organize the object-level landmarks. On the basis of\nthis, we use a robot-centric mapping framework to represent the environment as\na semantic topology graph and relax the burden of maintaining global\nconsistency at the same time. Besides, a hierarchical memory management\nmechanism is introduced to improve the efficiency of online mapping with\nlimited computational resources. Based on the proposed map, the robust\nlocalization is achieved by constructing a novel local semantic scene graph\ndescriptor, and performing multi-constraint graph matching to compare scene\nsimilarity. Finally, we test our method on a low-cost embedded platform to\ndemonstrate its advantages. Experimental results on a large scale and\nmulti-session real-world environment show that the proposed method outperforms\nthe state of arts in terms of lightweight and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaofan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fulin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hongkui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels. (arXiv:2201.05986v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05986","description":"<p>In this paper, we present a dynamic convolution kernel (DCK) strategy for\nconvolutional neural networks. Using a fully convolutional network with the\nproposed DCKs, high-quality talking-face video can be generated from\nmulti-modal sources (i.e., unmatched audio and video) in real time, and our\ntrained model is robust to different identities, head postures, and input\naudios. Our proposed DCKs are specially designed for audio-driven talking face\nvideo generation, leading to a simple yet effective end-to-end system. We also\nprovide a theoretical analysis to interpret why DCKs work. Experimental results\nshow that our method can generate high-quality talking-face video with\nbackground at 60 fps. Comparison and evaluation between our method and the\nstate-of-the-art methods demonstrate the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zipeng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1\">Mengfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1\">Ran Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. (arXiv:2201.05989v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05989","description":"<p>Neural graphics primitives, parameterized by fully connected neural networks,\ncan be costly to train and evaluate. We reduce this cost with a versatile new\ninput encoding that permits the use of a smaller network without sacrificing\nquality, thus significantly reducing the number of floating point and memory\naccess operations: a small neural network is augmented by a multiresolution\nhash table of trainable feature vectors whose values are optimized through\nstochastic gradient descent. The multiresolution structure allows the network\nto disambiguate hash collisions, making for a simple architecture that is\ntrivial to parallelize on modern GPUs. We leverage this parallelism by\nimplementing the whole system using fully-fused CUDA kernels with a focus on\nminimizing wasted bandwidth and compute operations. We achieve a combined\nspeedup of several orders of magnitude, enabling training of high-quality\nneural graphics primitives in a matter of seconds, and rendering in tens of\nmilliseconds at a resolution of ${1920\\!\\times\\!1080}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_A/0/1/0/all/0/1\">Alex Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schied_C/0/1/0/all/0/1\">Christoph Schied</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1\">Alexander Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Transformers: A Survey. (arXiv:2201.05991v1 [cs.CV])","link":"http://arxiv.org/abs/2201.05991","description":"<p>Transformer models have shown great success modeling long-range interactions.\nNevertheless, they scale quadratically with input length and lack inductive\nbiases. These limitations can be further exacerbated when dealing with the high\ndimensionality of video. Proper modeling of video, which can span from seconds\nto hours, requires handling long-range interactions. This makes Transformers a\npromising tool for solving video related tasks, but some adaptations are\nrequired. While there are previous works that study the advances of\nTransformers for vision tasks, there is none that focus on in-depth analysis of\nvideo-specific designs. In this survey we analyse and summarize the main\ncontributions and trends for adapting Transformers to model video data.\nSpecifically, we delve into how videos are embedded and tokenized, finding a\nvery widspread use of large CNN backbones to reduce dimensionality and a\npredominance of patches and frames as tokens. Furthermore, we study how the\nTransformer layer has been tweaked to handle longer sequences, generally by\nreducing the number of tokens in single attention operation. Also, we analyse\nthe self-supervised losses used to train Video Transformers, which to date are\nmostly constrained to contrastive approaches. Finally, we explore how other\nmodalities are integrated with video and conduct a performance comparison on\nthe most common benchmark for Video Transformers (i.e., action classification),\nfinding them to outperform 3D CNN counterparts with equivalent FLOPs and no\nsignificant parameter increase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selva_J/0/1/0/all/0/1\">Javier Selva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_A/0/1/0/all/0/1\">Anders S. Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1\">Kamal Nasrollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clapes_A/0/1/0/all/0/1\">Albert Clap&#xe9;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hardware Implementation of Multimodal Biometric using Fingerprint and Iris. (arXiv:2201.05996v1 [cs.CR])","link":"http://arxiv.org/abs/2201.05996","description":"<p>In this paper, a hardware architecture of a multimodal biometric system is\npresented that massively exploits the inherent parallelism. The proposed system\nis based on multiple biometric fusion that uses two biometric traits,\nfingerprint and iris. Each biometric trait is first optimised at the software\nlevel, by addressing some of the issues that directly affect the FAR and FRR.\nThen the hardware architectures for both biometric traits are presented,\nfollowed by a final multimodal hardware architecture. To the best of the\nauthor's knowledge, no other FPGA-based design exits that used these two\ntraits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1\">Tariq M Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines. (arXiv:1804.06579v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/1804.06579","description":"<p>We present a semi-supervised co-analysis method for learning 3D shape styles\nfrom projected feature lines, achieving style patch localization with only weak\nsupervision. Given a collection of 3D shapes spanning multiple object\ncategories and styles, we perform style co-analysis over projected feature\nlines of each 3D shape and then backproject the learned style features onto the\n3D shapes. Our core analysis pipeline starts with mid-level patch sampling and\npre-selection of candidate style patches. Projective features are then encoded\nvia patch convolution. Multi-view feature integration and style clustering are\ncarried out under the framework of partially shared latent factor (PSLF)\nlearning, a multi-view feature learning scheme. PSLF achieves effective\nmulti-view feature fusion by distilling and exploiting consistent and\ncomplementary feature information from multiple views, while also selecting\nstyle patches from the candidates. Our style analysis approach supports both\nunsupervised and semi-supervised analysis. For the latter, our method accepts\nboth user-specified shape labels and style-ranked triplets as clustering\nconstraints.We demonstrate results from 3D shape style analysis and patch\nlocalization as well as improvements over state-of-the-art methods. We also\npresent several applications enabled by our style analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fenggen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Generalization Ability of Convolutional Neural Networks and Capsule Networks for Image Classification via Top-2 Classification. (arXiv:1901.10112v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1901.10112","description":"<p>Image classification is a challenging problem which aims to identify the\ncategory of object in the image. In recent years, deep Convolutional Neural\nNetworks (CNNs) have been applied to handle this task, and impressive\nimprovement has been achieved. However, some research showed the output of CNNs\ncan be easily altered by adding relatively small perturbations to the input\nimage, such as modifying few pixels. Recently, Capsule Networks (CapsNets) are\nproposed, which can help eliminating this limitation. Experiments on MNIST\ndataset revealed that capsules can better characterize the features of object\nthan CNNs. But it's hard to find a suitable quantitative method to compare the\ngeneralization ability of CNNs and CapsNets. In this paper, we propose a new\nimage classification task called Top-2 classification to evaluate the\ngeneralization ability of CNNs and CapsNets. The models are trained on single\nlabel image samples same as the traditional image classification task. But in\nthe test stage, we randomly concatenate two test image samples which contain\ndifferent labels, and then use the trained models to predict the top-2 labels\non the unseen newly-created two label image samples. This task can provide us\nprecise quantitative results to compare the generalization ability of CNNs and\nCapsNets. Back to the CapsNet, because it uses Full Connectivity (FC) mechanism\namong all capsules, it requires many parameters. To reduce the number of\nparameters, we introduce the Parameter-Sharing (PS) mechanism between capsules.\nExperiments on five widely used benchmark image datasets demonstrate the method\nsignificantly reduces the number of parameters, without losing the\neffectiveness of extracting features. Further, on the Top-2 classification\ntask, the proposed PS CapsNets obtain impressive higher accuracy compared to\nthe traditional CNNs and FC CapsNets by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hao Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation. (arXiv:1903.00709v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1903.00709","description":"<p>Deep learning approaches to 3D shape segmentation are typically formulated as\na multi-class labeling problem. Existing models are trained for a fixed set of\nlabels, which greatly limits their flexibility and adaptivity. We opt for\ntop-down recursive decomposition and develop the first deep learning model for\nhierarchical segmentation of 3D shapes, based on recursive neural networks.\nStarting from a full shape represented as a point cloud, our model performs\nrecursive binary decomposition, where the decomposition network at all nodes in\nthe hierarchy share weights. At each node, a node classifier is trained to\ndetermine the type (adjacency or symmetry) and stopping criteria of its\ndecomposition. The features extracted in higher level nodes are recursively\npropagated to lower level ones. Thus, the meaningful decompositions in higher\nlevels provide strong contextual cues constraining the segmentations in lower\nlevels. Meanwhile, to increase the segmentation accuracy at each node, we\nenhance the recursive contextual feature with the shape feature extracted for\nthe corresponding part. Our method segments a 3D shape in point cloud into an\nunfixed number of parts, depending on the shape complexity, showing strong\ngenerality and flexibility. It achieves the state-of-the-art performance, both\nfor fine-grained and semantic segmentation, on the public benchmark and a new\nbenchmark of fine-grained segmentation proposed in this work. We also\ndemonstrate its application for fine-grained part refinements in image-to-shape\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fenggen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Discourse Parsing. (arXiv:1903.02252v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1903.02252","description":"<p>Text-level discourse parsing aims to unmask how two segments (or sentences)\nin the text are related to each other. We propose the task of Visual Discourse\nParsing, which requires understanding discourse relations among scenes in a\nvideo. Here we use the term scene to refer to a subset of video frames that can\nbetter summarize the video. In order to collect a dataset for learning\ndiscourse cues from videos, one needs to manually identify the scenes from a\nlarge pool of video frames and then annotate the discourse relations between\nthem. This is clearly a time consuming, expensive and tedious task. In this\nwork, we propose an approach to identify discourse cues from the videos without\nthe need to explicitly identify and annotate the scenes. We also present a\nnovel dataset containing 310 videos and the corresponding discourse cues to\nevaluate our approach. We believe that many of the multi-discipline Artificial\nIntelligence problems such as Visual Dialog and Visual Storytelling would\ngreatly benefit from the use of visual discourse cues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun R Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAMMA: A General Agent Motion Model for Autonomous Driving. (arXiv:1906.01566v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1906.01566","description":"<p>This paper presents GAMMA, a general motion prediction model that enables\nlarge-scale real-time simulation and planning for autonomous driving. GAMMA\nmodels heterogeneous, interactive traffic agents. They operate under diverse\nroad conditions, with various geometric and kinematic constraints. GAMMA treats\nthe prediction task as constrained optimization in traffic agents' velocity\nspace. The objective is to optimize an agent's driving performance, while\nobeying all the constraints resulting from the agent's kinematics, collision\navoidance with other agents, and the environmental context. Further, GAMMA\nexplicitly conditions the prediction on human behavioral states as parameters\nof the optimization model, in order to account for versatile human behaviors.\nWe evaluated GAMMA on a set of real-world benchmark datasets. The results show\nthat GAMMA achieves high prediction accuracy on both homogeneous and\nheterogeneous traffic datasets, with sub-millisecond execution time. Further,\nthe computational efficiency and the flexibility of GAMMA enable (i) simulation\nof mixed urban traffic at many locations worldwide and (ii) planning for\nautonomous driving in dense traffic with uncertain driver behaviors, both in\nreal-time. The open-source code of GAMMA is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuanfu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Panpan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yiyuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Guided Single Image Reflection Removal. (arXiv:1907.11912v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.11912","description":"<p>Reflection is common in images capturing scenes behind a glass window, which\nis not only a disturbance visually but also influence the performance of other\ncomputer vision algorithms. Single image reflection removal is an ill-posed\nproblem because the color at each pixel needs to be separated into two values,\ni.e., the desired clear background and the reflection. To solve it, existing\nmethods propose priors such as smoothness, color consistency. However, the\nlow-level priors are not reliable in complex scenes, for instance, when\ncapturing a real outdoor scene through a window, both the foreground and\nbackground contain both smooth and sharp area and a variety of color. In this\npaper, inspired by the fact that human can separate the two layers easily by\nrecognizing the objects, we use the object semantic as guidance to force the\nsame semantic object belong to the same layer. Extensive experiments on\ndifferent datasets show that adding the semantic information offers a\nsignificant improvement to reflection separation. We also demonstrate the\napplications of the proposed method to other computer vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shaodi You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Scale Open-Set Deep Logo Detection. (arXiv:1911.07440v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.07440","description":"<p>We present an open-set logo detection (OSLD) system, which can detect\n(localize and recognize) any number of unseen logo classes without re-training;\nit only requires a small set of canonical logo images for each logo class. We\nachieve this using a two-stage approach: (1) Generic logo detection to detect\ncandidate logo regions in an image. (2) Logo matching for matching the detected\nlogo regions to a set of canonical logo images to recognize them.\n</p>\n<p>We constructed an open-set logo detection dataset with 12.1k logo classes and\nreleased it for research purposes.We demonstrate the effectiveness of OSLD on\nour dataset and on the standard Flickr-32 logo dataset, outperforming the\nstate-of-the-art open-set and closed-set logo detection methods by a large\nmargin. OSLD is scalable to millions of logo classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastan_M/0/1/0/all/0/1\">Muhammet Bastan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao-Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tian Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kota_B/0/1/0/all/0/1\">Bhargava Kota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tek_M/0/1/0/all/0/1\">Mehmet Tek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Posterior Adaptation With New Priors. (arXiv:2007.01386v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.01386","description":"<p>Classification approaches based on the direct estimation and analysis of\nposterior probabilities will degrade if the original class priors begin to\nchange. We prove that a unique (up to scale) solution is possible to recover\nthe data likelihoods for a test example from its original class posteriors and\ndataset priors. Given the recovered likelihoods and a set of new priors, the\nposteriors can be re-computed using Bayes' Rule to reflect the influence of the\nnew priors. The method is simple to compute and allows a dynamic update of the\noriginal posteriors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">Jim Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight-dependent Gates for Network Pruning. (arXiv:2007.02066v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.02066","description":"<p>In this paper, a simple yet effective network pruning framework is proposed\nto simultaneously address the problems of pruning indicator, pruning ratio, and\nefficiency constraint. This paper argues that the pruning decision should\ndepend on the convolutional weights, and thus proposes novel weight-dependent\ngates (W-Gates) to learn the information from filter weights and obtain binary\ngates to prune or keep the filters automatically. To prune the network under\nefficiency constraints, a switchable Efficiency Module is constructed to\npredict the hardware latency or FLOPs of candidate pruned networks. Combined\nwith the proposed Efficiency Module, W-Gates can perform filter pruning in an\nefficiency-aware manner and achieve a compact network with a better\naccuracy-efficiency trade-off. We have demonstrated the effectiveness of the\nproposed method on ResNet34, ResNet50, and MobileNet V2, respectively achieving\nup to 1.33/1.28/1.1 higher Top-1 accuracy with lower hardware latency on\nImageNet. Compared with state-of-the-art methods, W-Gates also achieves\nsuperior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiqun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Haotian Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baoqun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Contrastive Motion Learning for Video Action Recognition. (arXiv:2007.10321v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.10321","description":"<p>One central question for video action recognition is how to model motion. In\nthis paper, we present hierarchical contrastive motion learning, a new\nself-supervised learning framework to extract effective motion representations\nfrom raw video frames. Our approach progressively learns a hierarchy of motion\nfeatures that correspond to different abstraction levels in a network. This\nhierarchical design bridges the semantic gap between low-level motion cues and\nhigh-level recognition tasks, and promotes the fusion of appearance and motion\ninformation at multiple levels. At each level, an explicit motion\nself-supervision is provided via contrastive learning to enforce the motion\nfeatures at the current level to predict the future ones at the previous level.\nThus, the motion features at higher levels are trained to gradually capture\nsemantic dynamics and evolve more discriminative for action recognition. Our\nmotion learning module is lightweight and flexible to be embedded into various\nbackbone networks. Extensive experiments on four benchmarks show that the\nproposed approach consistently achieves superior results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xitong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaodong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v7 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2008.08170","description":"<p>In the paper, we propose a class of accelerated zeroth-order and first-order\nmomentum methods for both nonconvex mini-optimization and minimax-optimization.\nSpecifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)\nmethod for black-box mini-optimization where only function values can be\nobtained. Moreover, we prove that our Acc-ZOM method achieves a lower query\ncomplexity of $\\tilde{O}(d^{3/4}\\epsilon^{-3})$ for finding an\n$\\epsilon$-stationary point, which improves the best known result by a factor\nof $O(d^{1/4})$ where $d$ denotes the variable dimension. In particular, our\nAcc-ZOM does not need large batches required in the existing zeroth-order\nstochastic algorithms. Meanwhile, we propose an accelerated zeroth-order\nmomentum descent ascent (Acc-ZOMDA) method for black-box minimax optimization,\nwhere only function values can be obtained. Our Acc-ZOMDA obtains a low query\ncomplexity of $\\tilde{O}((d_1+d_2)^{3/4}\\kappa_y^{4.5}\\epsilon^{-3})$ without\nrequiring large batches for finding an $\\epsilon$-stationary point, where $d_1$\nand $d_2$ denote variable dimensions and $\\kappa_y$ is condition number.\nMoreover, we propose an accelerated first-order momentum descent ascent\n(Acc-MDA) method for minimax optimization, whose explicit gradients are\naccessible. Our Acc-MDA achieves a low gradient complexity of\n$\\tilde{O}(\\kappa_y^{4.5}\\epsilon^{-3})$ without requiring large batches for\nfinding an $\\epsilon$-stationary point. In particular, our Acc-MDA can obtain a\nlower gradient complexity of $\\tilde{O}(\\kappa_y^{2.5}\\epsilon^{-3})$ with a\nbatch size $O(\\kappa_y^4)$, which improves the best known result by a factor of\n$O(\\kappa_y^{1/2})$. Extensive experimental results on black-box adversarial\nattack to deep neural networks and poisoning attack to logistic regression\ndemonstrate efficiency of our algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gao_S/0/1/0/all/0/1\">Shangqian Gao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inspecting state of the art performance and NLP metrics in image-based medical report generation. (arXiv:2011.09257v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.09257","description":"<p>Several deep learning architectures have been proposed over the last years to\ndeal with the problem of generating a written report given an imaging exam as\ninput. Most works evaluate the generated reports using standard Natural\nLanguage Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant\nprogress. In this article, we contrast this progress by comparing state of the\nart (SOTA) models against weak baselines. We show that simple and even naive\napproaches yield near SOTA performance on most traditional NLP metrics. We\nconclude that evaluation methods in this task should be further studied towards\ncorrectly measuring clinical accuracy, ideally involving physicians to\ncontribute to this end.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pino_P/0/1/0/all/0/1\">Pablo Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_P/0/1/0/all/0/1\">Pablo Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besa_C/0/1/0/all/0/1\">Cecilia Besa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uribe_S/0/1/0/all/0/1\">Sergio Uribe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical VAEs Know What They Don't Know. (arXiv:2102.08248v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.08248","description":"<p>Deep generative models have been demonstrated as state-of-the-art density\nestimators. Yet, recent work has found that they often assign a higher\nlikelihood to data from outside the training distribution. This seemingly\nparadoxical behavior has caused concerns over the quality of the attained\ndensity estimates. In the context of hierarchical variational autoencoders, we\nprovide evidence to explain this behavior by out-of-distribution data having\nin-distribution low-level features. We argue that this is both expected and\ndesirable behavior. With this insight in hand, we develop a fast, scalable and\nfully unsupervised likelihood-ratio score for OOD detection that requires data\nto be in-distribution across all feature-levels. We benchmark the method on a\nvast set of data and model combinations and achieve state-of-the-art results on\nout-of-distribution detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Havtorn_J/0/1/0/all/0/1\">Jakob D. Havtorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frellsen_J/0/1/0/all/0/1\">Jes Frellsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1\">S&#xf8;ren Hauberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maaloe_L/0/1/0/all/0/1\">Lars Maal&#xf8;e</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Margin Preserving Self-paced Contrastive Learning Towards Domain Adaptation for Medical Image Segmentation. (arXiv:2103.08454v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08454","description":"<p>To bridge the gap between the source and target domains in unsupervised\ndomain adaptation (UDA), the most common strategy puts focus on matching the\nmarginal distributions in the feature space through adversarial learning.\nHowever, such category-agnostic global alignment lacks of exploiting the\nclass-level joint distributions, causing the aligned distribution less\ndiscriminative. To address this issue, we propose in this paper a novel margin\npreserving self-paced contrastive Learning (MPSCL) model for cross-modal\nmedical image segmentation. Unlike the conventional construction of contrastive\npairs in contrastive learning, the domain-adaptive category prototypes are\nutilized to constitute the positive and negative sample pairs. With the\nguidance of progressively refined semantic prototypes, a novel margin\npreserving contrastive loss is proposed to boost the discriminability of\nembedded representation space. To enhance the supervision for contrastive\nlearning, more informative pseudo-labels are generated in target domain in a\nself-paced way, thus benefiting the category-aware distribution alignment for\nUDA. Furthermore, the domain-invariant representations are learned through\njoint contrastive learning between the two domains. Extensive experiments on\ncross-modal cardiac segmentation tasks demonstrate that MPSCL significantly\nimproves semantic segmentation performance, and outperforms a wide variety of\nstate-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiayu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stepwise Goal-Driven Networks for Trajectory Prediction. (arXiv:2103.14107v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14107","description":"<p>We propose to predict the future trajectories of observed agents (e.g.,\npedestrians or vehicles) by estimating and using their goals at multiple time\nscales. We argue that the goal of a moving agent may change over time, and\nmodeling goals continuously provides more accurate and detailed information for\nfuture trajectory estimation. In this paper, we present a novel recurrent\nnetwork for trajectory prediction, called Stepwise Goal-Driven Network (SGNet).\nUnlike prior work that models only a single, long-term goal, SGNet estimates\nand uses goals at multiple temporal scales. In particular, the framework\nincorporates an encoder module that captures historical information, a stepwise\ngoal estimator that predicts successive goals into the future, and a decoder\nmodule that predicts future trajectory. We evaluate our model on three\nfirst-person traffic datasets (HEV-I, JAAD, and PIE) as well as on two bird's\neye view datasets (ETH and UCY), and show that our model outperforms the\nstate-of-the-art methods in terms of both average and final displacement errors\non all datasets. Code has been made available at:\nhttps://github.com/ChuhuaW/SGNet.pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuchen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingze Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David J. Crandall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Generalization in Deep Neural Networks via Sparsity. (arXiv:2104.00851v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00851","description":"<p>Generalization is the key capability for deep neural networks (DNNs).\nHowever, it is challenging to give a reliable measure of the generalization\nability of a DNN via only its nature. In this paper, we propose a novel method\nfor estimating the generalization gap based on network sparsity. In our method,\ntwo key quantities are proposed first. They have close relationship with the\ngeneralization ability and can be calculated directly from the training results\nalone. Then a simple linear model involving two key quantities are constructed\nto give accurate estimation of the generalization gap. By training DNNs with a\nwide range of generalization gap on popular datasets, we show that our key\nquantities and linear model could be efficient tools for estimating the\ngeneralization gap of DNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations. (arXiv:2104.03926v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03926","description":"<p>Although single-image super-resolution (SISR) methods have achieved great\nsuccess on single degradation, they still suffer performance drop with multiple\ndegrading effects in real scenarios. Recently, some blind and non-blind models\nfor multiple degradations have been explored. However, those methods usually\ndegrade significantly for distribution shifts between the training and test\ndata. Towards this end, we propose a conditional meta-network framework (named\nCMDSR) for the first time, which helps SR framework learn how to adapt to\nchanges in input distribution. We extract degradation prior at task-level with\nthe proposed ConditionNet, which will be used to adapt the parameters of the\nbasic SR network (BaseNet). Specifically, the ConditionNet of our framework\nfirst learns the degradation prior from a support set, which is composed of a\nseries of degraded image patches from the same task. Then the adaptive BaseNet\nrapidly shifts its parameters according to the conditional features. Moreover,\nin order to better extract degradation prior, we propose a task contrastive\nloss to decrease the inner-task distance and increase the cross-task distance\nbetween task-level features. Without predefining degradation maps, our blind\nframework can conduct one single parameter update to yield considerable SR\nresults. Extensive experiments demonstrate the effectiveness of CMDSR over\nvarious blind, even non-blind methods. The flexible BaseNet structure also\nreveals that CMDSR can be a general framework for large series of SISR models.\nOur code is available at \\url{https://github.com/guanghaoyin/CMDSR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1\">Guanghao Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shouqian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mutual Contrastive Learning for Visual Representation Learning. (arXiv:2104.12565v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12565","description":"<p>We present a collaborative learning method called Mutual Contrastive Learning\n(MCL) for general visual representation learning. The core idea of MCL is to\nperform mutual interaction and transfer of contrastive distributions among a\ncohort of networks. A crucial component of MCL is Interactive Contrastive\nLearning (ICL). Compared with vanilla contrastive learning, ICL can aggregate\ncross-network embedding information and maximize the lower bound to the mutual\ninformation between two networks. This enables each network to learn extra\ncontrastive knowledge from others, leading to better feature representations\nfor visual recognition tasks. We emphasize that the resulting MCL is\nconceptually simple yet empirically powerful. It is a generic framework that\ncan be applied to both supervised and self-supervised representation learning.\nExperimental results on image classification and transfer learning to object\ndetection show that MCL can lead to consistent performance gains, demonstrating\nthat MCL can guide the network to generate better feature representations. Code\nis available at https://github.com/winycg/MCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers. (arXiv:2105.08059v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.08059","description":"<p>Supervised reconstruction models are characteristically trained on matched\npairs of undersampled and fully-sampled data to capture an MRI prior, along\nwith supervision regarding the imaging operator to enforce data consistency. To\nreduce supervision requirements, the recent deep image prior framework instead\nconjoins untrained MRI priors with the imaging operator during inference. Yet,\ncanonical convolutional architectures are suboptimal in capturing long-range\nrelationships, and priors based on randomly initialized networks may yield\nsuboptimal performance. To address these limitations, here we introduce a novel\nunsupervised MRI reconstruction method based on zero-Shot Learned Adversarial\nTransformERs (SLATER). SLATER embodies a deep adversarial network with\ncross-attention transformers to map noise and latent variables onto\ncoil-combined MR images. During pre-training, this unconditional network learns\na high-quality MRI prior in an unsupervised generative modeling task. During\ninference, a zero-shot reconstruction is then performed by incorporating the\nimaging operator and optimizing the prior to maximize consistency to\nundersampled data. Comprehensive experiments on brain MRI datasets clearly\ndemonstrate the superior performance of SLATER against state-of-the-art\nunsupervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Korkmaz_Y/0/1/0/all/0/1\">Yilmaz Korkmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dar_S/0/1/0/all/0/1\">Salman UH Dar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks on Self-Supervised Learning. (arXiv:2105.10123v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10123","description":"<p>Large-scale unlabeled data has spurred recent progress in self-supervised\nlearning methods that learn rich visual representations. State-of-the-art\nself-supervised methods for learning representations from images (e.g., MoCo,\nBYOL, MSF) use an inductive bias that random augmentations (e.g., random crops)\nof an image should produce similar embeddings. We show that such methods are\nvulnerable to backdoor attacks - where an attacker poisons a small part of the\nunlabeled data by adding a trigger (image patch chosen by the attacker) to the\nimages. The model performance is good on clean test images, but the attacker\ncan manipulate the decision of the model by showing the trigger at test time.\nBackdoor attacks have been studied extensively in supervised learning and to\nthe best of our knowledge, we are the first to study them for self-supervised\nlearning. Backdoor attacks are more practical in self-supervised learning,\nsince the use of large unlabeled data makes data inspection to remove poisons\nprohibitive. We show that in our targeted attack, the attacker can produce many\nfalse positives for the target category by using the trigger at test time. We\nalso propose a knowledge distillation based defense algorithm that succeeds in\nneutralizing the attack. Our code is available here:\nhttps://github.com/UMBCvision/SSL-Backdoor .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aniruddha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training. (arXiv:2105.11333v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11333","description":"<p>Recently a number of studies demonstrated impressive performance on diverse\nvision-language multimodal tasks such as image captioning and visual question\nanswering by extending the self-attention based Transformer architecture with\nmultimodal pre-training objectives. Despite its huge potential, vision-language\nmultimodal pre-training in the medical domain has only recently received\nattention, and only demonstrated improved diagnosis accuracy of vision-language\npre-trained models. In this work we explore a broad set of multimodal\nrepresentation learning tasks in the medical domain, specifically using\nradiology images and the unstructured report. We propose a new model which\nadopts a Transformer based architecture combined with a novel multimodal\nattention masking scheme to maximize generalization performance for both\nvision-language understanding task (e.g., diagnosis classification) and\nvision-language generation task (e.g., radiology report generation). By\nrigorously evaluating the proposed model on four downstream tasks with three\nradiographic image-text datasets (MIMIC-CXR, Open-I, and VQA-RAD), we\nempirically demonstrate the superior downstream task performance and generality\nof our model against various baselines including task specific architectures.\nIn addition, we qualitatively analyze our model by showing the results of\nretrieved image-report pairs, the attention map visualization, and generated\nreports. Our proposed multimodal pre-training model could flexibly adapt to\nmultiple downstream tasks of vision-language understanding and generation with\na novel self-attention scheme. We believe that our approach can provide the\nbasis for a wide range of interpretations of vision-language multimodal in the\nmedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jong Hak Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyungyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Woncheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stylizing 3D Scene via Implicit Representation and HyperNetwork. (arXiv:2105.13016v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13016","description":"<p>In this work, we aim to address the 3D scene stylization problem - generating\nstylized images of the scene at arbitrary novel view angles. A straightforward\nsolution is to combine existing novel view synthesis and image/video style\ntransfer approaches, which often leads to blurry results or inconsistent\nappearance. Inspired by the high-quality results of the neural radiance fields\n(NeRF) method, we propose a joint framework to directly render novel views with\nthe desired style. Our framework consists of two components: an implicit\nrepresentation of the 3D scene with the neural radiance fields model, and a\nhypernetwork to transfer the style information into the scene representation.\nIn particular, our implicit representation model disentangles the scene into\nthe geometry and appearance branches, and the hypernetwork learns to predict\nthe parameters of the appearance branch from the reference style image. To\nalleviate the training difficulties and memory burden, we propose a two-stage\ntraining procedure and a patch sub-sampling approach to optimize the style and\ncontent losses with the neural radiance fields model. After optimization, our\nmodel is able to render consistent novel views at arbitrary view angles with\narbitrary style. Both quantitative evaluation and human subject study have\ndemonstrated that the proposed method generates faithful stylization results\nwith consistent appearance across different views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1\">Pei-Ze Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1\">Meng-Shiun Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02634","description":"<p>Inferring representations of 3D scenes from 2D observations is a fundamental\nproblem of computer graphics, computer vision, and artificial intelligence.\nEmerging 3D-structured neural scene representations are a promising approach to\n3D scene understanding. In this work, we propose a novel neural scene\nrepresentation, Light Field Networks or LFNs, which represent both geometry and\nappearance of the underlying 3D scene in a 360-degree, four-dimensional light\nfield parameterized via a neural implicit representation. Rendering a ray from\nan LFN requires only a single network evaluation, as opposed to hundreds of\nevaluations per ray for ray-marching or volumetric based renderers in\n3D-structured neural scene representations. In the setting of simple scenes, we\nleverage meta-learning to learn a prior over LFNs that enables multi-view\nconsistent light field reconstruction from as little as a single image\nobservation. This results in dramatic reductions in time and memory complexity,\nand enables real-time rendering. The cost of storing a 360-degree light field\nvia an LFN is two orders of magnitude lower than conventional methods such as\nthe Lumigraph. Utilizing the analytical differentiability of neural implicit\nrepresentations and a novel parameterization of light space, we further\ndemonstrate the extraction of sparse depth maps from LFNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1\">Vincent Sitzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1\">Semon Rezchikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fredo Durand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02701","description":"<p>Recent advances in brain clearing and imaging have made it possible to image\nentire mammalian brains at sub-micron resolution. These images offer the\npotential to assemble brain-wide atlases of neuron morphology, but manual\nneuron reconstruction remains a bottleneck. Several automatic reconstruction\nalgorithms exist, but most focus on single neuron images. In this paper, we\npresent a probabilistic reconstruction method, ViterBrain, which combines a\nhidden Markov state process that encodes neuron geometry with a random field\nappearance model of neuron fluorescence. Our method utilizes dynamic\nprogramming to compute the global maximizers of what we call the \"most\nprobable\" neuron path. Our most probable estimation method models the task of\nreconstructing neuronal processes in the presence of other neurons, and thus is\napplicable in images with several neurons. Our method operates on image\nsegmentations in order to leverage cutting edge computer vision technology. We\napplied our algorithm to imperfect image segmentations where false negatives\nsevered neuronal processes, and showed that it can follow axons in the presence\nof noise or nearby neurons. Additionally, it creates a framework where users\ncan intervene to, for example, fit start and endpoints. The code used in this\nwork is available in our open-source Python package brainlit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1\">Thomas L. Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1\">Daniel J. Tward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1\">Ulrich Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1\">Michael I. Miller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03743","description":"<p>We investigate the reasons for the performance degradation incurred with\nbatch-independent normalization. We find that the prototypical techniques of\nlayer normalization and instance normalization both induce the appearance of\nfailure modes in the neural network's pre-activations: (i) layer normalization\ninduces a collapse towards channel-wise constant functions; (ii) instance\nnormalization induces a lack of variability in instance statistics, symptomatic\nof an alteration of the expressivity. To alleviate failure mode (i) without\naggravating failure mode (ii), we introduce the technique \"Proxy Normalization\"\nthat normalizes post-activations using a proxy distribution. When combined with\nlayer normalization or group normalization, this batch-independent\nnormalization emulates batch normalization's behavior and consistently matches\nor exceeds its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Labatie_A/0/1/0/all/0/1\">Antoine Labatie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masters_D/0/1/0/all/0/1\">Dominic Masters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1\">Zach Eaton-Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06927","description":"<p>Despite unconditional feature inverters being the foundation of many\nsynthesis tasks, training them requires a large computational overhead,\ndecoding capacity or additional autoregressive priors. We propose to train an\nadversarially robust encoder to learn disentangled and perceptually-aligned\nbottleneck features, making them easily invertible. Then, by training a simple\ngenerator with the mirror architecture of the encoder, we achieve superior\nreconstructions and generalization over standard approaches. We exploit such\nproperties using an encoding-decoding network based on AR features and\ndemonstrate its oustanding performance on three applications: anomaly\ndetection, style transfer and image denoising. Comparisons against alternative\nlearn-based methods show that our model attains improved performance with\nsignificantly less training parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_Gomez_R/0/1/0/all/0/1\">Renan A. Rojas-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_R/0/1/0/all/0/1\">Raymond A. Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_M/0/1/0/all/0/1\">Minh N. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Modular Should Neural Module Networks Be for Systematic Generalization?. (arXiv:2106.08170v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.08170","description":"<p>Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via\ncomposition of modules that tackle a sub-task. NMNs are a promising strategy to\nachieve systematic generalization, i.e., overcoming biasing factors in the\ntraining distribution. However, the aspects of NMNs that facilitate systematic\ngeneralization are not fully understood. In this paper, we demonstrate that the\ndegree of modularity of the NMN have large influence on systematic\ngeneralization. In a series of experiments on three VQA datasets (VQA-MNIST,\nSQOOP, and CLEVR-CoGenT), our results reveal that tuning the degree of\nmodularity, especially at the image encoder stage, reaches substantially higher\nsystematic generalization. These findings lead to new NMN architectures that\noutperform previous ones in terms of systematic generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1\">Vanessa D&#x27;Amario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised GANs with Label Augmentation. (arXiv:2106.08601v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.08601","description":"<p>Recently, transformation-based self-supervised learning has been applied to\ngenerative adversarial networks (GANs) to mitigate catastrophic forgetting in\nthe discriminator by introducing a stationary learning environment. However,\nthe separate self-supervised tasks in existing self-supervised GANs cause a\ngoal inconsistent with generative modeling due to the fact that their\nself-supervised classifiers are agnostic to the generator distribution. To\naddress this problem, we propose a novel self-supervised GAN that unifies the\nGAN task with the self-supervised task by augmenting the GAN labels (real or\nfake) via self-supervision of data transformation. Specifically, the original\ndiscriminator and self-supervised classifier are unified into a label-augmented\ndiscriminator that predicts the augmented labels to be aware of both the\ngenerator distribution and the data distribution under every transformation,\nand then provide the discrepancy between them to optimize the generator.\nTheoretically, we prove that the optimal generator could converge to replicate\nthe real data distribution. Empirically, we show that the proposed method\nsignificantly outperforms previous self-supervised and data augmentation GANs\non both generative modeling and representation learning across benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Liang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00710","description":"<p>Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cote_Allard_U/0/1/0/all/0/1\">Ulysse C&#xf4;t&#xe9;-Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_P/0/1/0/all/0/1\">Petter Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stautland_A/0/1/0/all/0/1\">Andrea Stautland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordgreen_T/0/1/0/all/0/1\">Tine Nordgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasmer_O/0/1/0/all/0/1\">Ole Bernt Fasmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oedegaard_K/0/1/0/all/0/1\">Ketil Joachim Oedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1\">Jim Torresen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05445","description":"<p>Humans are incredibly good at transferring knowledge from one domain to\nanother, enabling rapid learning of new tasks. Likewise, transfer learning has\nenabled enormous success in many computer vision problems using pretraining.\nHowever, the benefits of transfer in multi-domain learning, where a network\nlearns multiple tasks defined by different datasets, has not been adequately\nstudied. Learning multiple domains could be beneficial, or these domains could\ninterfere with each other given limited network capacity. Understanding how\ndeep neural networks of varied capacity facilitate transfer across inputs from\ndifferent distributions is a critical step towards open world learning. In this\nwork, we decipher the conditions where interference and knowledge transfer\noccur in multi-domain learning. We propose new metrics disentangling\ninterference and transfer, set up experimental protocols, and examine the roles\nof network capacity, task grouping, and dynamic loss weighting in reducing\ninterference and facilitating transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CentripetalText: An Efficient Text Instance Representation for Scene Text Detection. (arXiv:2107.05945v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05945","description":"<p>Scene text detection remains a grand challenge due to the variation in text\ncurvatures, orientations, and aspect ratios. One of the hardest problems in\nthis task is how to represent text instances of arbitrary shapes. Although many\nmethods have been proposed to model irregular texts in a flexible manner, most\nof them lose simplicity and robustness. Their complicated post-processings and\nthe regression under Dirac delta distribution undermine the detection\nperformance and the generalization ability. In this paper, we propose an\nefficient text instance representation named CentripetalText (CT), which\ndecomposes text instances into the combination of text kernels and centripetal\nshifts. Specifically, we utilize the centripetal shifts to implement pixel\naggregation, guiding the external text pixels to the internal text kernels. The\nrelaxation operation is integrated into the dense regression for centripetal\nshifts, allowing the correct prediction in a range instead of a specific value.\nThe convenient reconstruction of text contours and the tolerance of prediction\nerrors in our method guarantee the high detection accuracy and the fast\ninference speed, respectively. Besides, we shrink our text detector into a\nproposal generation module, namely CentripetalText Proposal Network, replacing\nSegmentation Proposal Network in Mask TextSpotter v3 and producing more\naccurate proposals. To validate the effectiveness of our method, we conduct\nexperiments on several commonly used scene text benchmarks, including both\ncurved and multi-oriented text datasets. For the task of scene text detection,\nour approach achieves superior or competitive performance compared to other\nexisting methods, e.g., F-measure of 86.3% at 40.0 FPS on Total-Text, F-measure\nof 86.1% at 34.8 FPS on MSRA-TD500, etc. For the task of end-to-end scene text\nrecognition, our method outperforms Mask TextSpotter v3 by 1.1% on Total-Text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_T/0/1/0/all/0/1\">Tao Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Designing Good Representation Learning Models. (arXiv:2107.05948v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05948","description":"<p>The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets. Code available at\nhttps://github.com/qlilx/odgrlm.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Guided NIR Image Colorization via Adaptive Fusion of Semantic and Texture Clues. (arXiv:2107.09237v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09237","description":"<p>Near infrared (NIR) imaging has been widely applied in low-light imaging\nscenarios; however, it is difficult for human and algorithms to perceive the\nreal scene in the colorless NIR domain. While Generative Adversarial Network\n(GAN) has been widely employed in various image colorization tasks, it is\nchallenging for a direct mapping mechanism, such as a conventional GAN, to\ntransform an image from the NIR to the RGB domain with correct semantic\nreasoning, well-preserved textures, and vivid color combinations concurrently.\nIn this work, we propose a novel Attention-based NIR image colorization\nframework via Adaptive Fusion of Semantic and Texture clues, aiming at\nachieving these goals within the same framework. The tasks of texture transfer\nand semantic reasoning are carried out in two separate network blocks.\nSpecifically, the Texture Transfer Block (TTB) aims at extracting texture\nfeatures from the NIR image's Laplacian component and transferring them for\nsubsequent color fusion. The Semantic Reasoning Block (SRB) extracts semantic\nclues and maps the NIR pixel values to the RGB domain. Finally, a Fusion\nAttention Block (FAB) is proposed to adaptively fuse the features from the two\nbranches and generate an optimized colorization result. In order to enhance the\nnetwork's learning capacity in semantic reasoning as well as mapping precision\nin texture transfer, we have proposed the Residual Coordinate Attention Block\n(RCAB), which incorporates coordinate attention into a residual learning\nframework, enabling the network to capture long-range dependencies along the\nchannel direction and meanwhile precise positional information can be preserved\nalong spatial directions. RCAB is also incorporated into FAB to facilitate\naccurate texture alignment during fusion. Both quantitative and qualitative\nevaluations show that the proposed method outperforms state-of-the-art NIR\nimage colorization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xingxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zaifeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.11191","description":"<p>Deep neural network approaches to inverse imaging problems have produced\nimpressive results in the last few years. In this paper, we consider the use of\ngenerative models in a variational regularisation approach to inverse problems.\nThe considered regularisers penalise images that are far from the range of a\ngenerative model that has learned to produce images similar to a training\ndataset. We name this family \\textit{generative regularisers}. The success of\ngenerative regularisers depends on the quality of the generative model and so\nwe propose a set of desired criteria to assess models and guide future\nresearch. In our numerical experiments, we evaluate three common generative\nmodels, autoencoders, variational autoencoders and generative adversarial\nnetworks, against our desired criteria. We also test three different generative\nregularisers on the inverse problems of deblurring, deconvolution, and\ntomography. We show that the success of solutions restricted to lie exactly in\nthe range of the generator is highly dependent on the ability of the generative\nmodel but that allowing small deviations from the range of the generator\nproduces more consistent results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duff_M/0/1/0/all/0/1\">Margaret Duff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campbell_N/0/1/0/all/0/1\">Neill D. F. Campbell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehrhardt_M/0/1/0/all/0/1\">Matthias J. Ehrhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00166","description":"<p>Micro-expressions are spontaneous, unconscious facial movements that show\npeople's true inner emotions and have great potential in related fields of\npsychological testing. Since the face is a 3D deformation object, the\noccurrence of an expression can arouse spatial deformation of the face, but\nlimited by the available databases are 2D videos, lacking the description of 3D\nspatial information of micro-expressions. Therefore, we proposed a new\nmicro-expression database containing 2D video sequences and 3D point clouds\nsequences. The database includes 373 micro-expressions sequences, and these\nsamples were classified using the objective method based on facial action\ncoding system, as well as the non-objective method that combines video contents\nand participants' self-reports. We extracted 2D and 3D features using the local\nbinary patterns on three orthogonal planes (LBP-TOP) and curvature algorithms,\nrespectively, and evaluated the classification accuracies of these two features\nand their fusion results with leave-one-subject-out (LOSO) and 10-fold\ncross-validation. Further, we performed various neural network algorithms for\ndatabase classification, the results show that classification accuracies are\nimproved by fusing 3D features than using only 2D features. The database offers\noriginal and cropped micro-expression samples, which will facilitate the\nexploration and research on 3D Spatio-temporal features of micro-expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1\">Danmin Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inference via Sparse Coding in a Hierarchical Vision Model. (arXiv:2108.01548v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01548","description":"<p>Sparse coding has been incorporated in models of the visual cortex for its\ncomputational advantages and connection to biology. But how the level of\nsparsity contributes to performance on visual tasks is not well understood. In\nthis work, sparse coding has been integrated into an existing hierarchical V2\nmodel (Hosoya and Hyv\\\"arinen, 2015), but replacing its independent component\nanalysis (ICA) with an explicit sparse coding in which the degree of sparsity\ncan be controlled. After training, the sparse coding basis functions with a\nhigher degree of sparsity resembled qualitatively different structures, such as\ncurves and corners. The contributions of the models were assessed with image\nclassification tasks, specifically tasks associated with mid-level vision\nincluding figure-ground classification, texture classification, and angle\nprediction between two line stimuli. In addition, the models were assessed in\ncomparison to a texture sensitivity measure that has been reported in V2\n(Freeman et al., 2013), and a deleted-region inference task. The results from\nthe experiments show that while sparse coding performed worse than ICA at\nclassifying images, only sparse coding was able to better match the texture\nsensitivity level of V2 and infer deleted image regions, both by increasing the\ndegree of sparsity in sparse coding. Higher degrees of sparsity allowed for\ninference over larger deleted image regions. The mechanism that allows for this\ninference capability in sparse coding is described here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bowren_J/0/1/0/all/0/1\">Joshua Bowren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Giraldo_L/0/1/0/all/0/1\">Luis Sanchez-Giraldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_O/0/1/0/all/0/1\">Odelia Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uniform Sampling over Episode Difficulty. (arXiv:2108.01662v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.01662","description":"<p>Episodic training is a core ingredient of few-shot learning to train models\non tasks with limited labelled data. Despite its success, episodic training\nremains largely understudied, prompting us to ask the question: what is the\nbest way to sample episodes? In this paper, we first propose a method to\napproximate episode sampling distributions based on their difficulty. Building\non this method, we perform an extensive analysis and find that sampling\nuniformly over episode difficulty outperforms other sampling schemes, including\ncurriculum and easy-/hard-mining. As the proposed sampling method is algorithm\nagnostic, we can leverage these insights to improve few-shot learning\naccuracies across many episodic training algorithms. We demonstrate the\nefficacy of our method across popular few-shot learning datasets, algorithms,\nnetwork architectures, and protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1\">S&#xe9;bastien M. R. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_G/0/1/0/all/0/1\">Guneet S. Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1\">Avinash Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-repository of screening mammography classifiers. (arXiv:2108.04800v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.04800","description":"<p>Artificial intelligence (AI) is showing promise in improving clinical\ndiagnosis. In breast cancer screening, recent studies show that AI has the\npotential to improve early cancer diagnosis and reduce unnecessary workup. As\nthe number of proposed models and their complexity grows, it is becoming\nincreasingly difficult to re-implement them. To enable reproducibility of\nresearch and to enable comparison between different methods, we release a\nmeta-repository containing models for classification of screening mammograms.\nThis meta-repository creates a framework that enables the evaluation of AI\nmodels on any screening mammography data set. At its inception, our\nmeta-repository contains five state-of-the-art models with open-source\nimplementations and cross-platform compatibility. We compare their performance\non seven international data sets. Our framework has a flexible design that can\nbe generalized to other medical image analysis tasks. The meta-repository is\navailable at https://www.github.com/nyukat/mammography_metarepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1\">Benjamin Stadnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1\">Jan Witowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1\">Vishwaesh Rajiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices. (arXiv:2108.06128v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2108.06128","description":"<p>Deep learning has celebrated resounding successes in many application areas\nof relevance to the Internet of Things (IoT), such as computer vision and\nmachine listening. These technologies must ultimately be brought directly to\nthe edge to fully harness the power of deep learning for the IoT. The obvious\nchallenge is that deep learning techniques can only be implemented on strictly\nresource-constrained edge devices if the models are radically downsized. This\ntask relies on different model compression techniques, such as network pruning,\nquantization, and the recent advancement of XNOR-Net. This study examines the\nsuitability of these techniques for audio classification on microcontrollers.\nWe present an application of XNOR-Net for end-to-end raw audio classification\nand a comprehensive empirical study comparing this approach with\npruning-and-quantization methods. We show that raw audio classification with\nXNOR yields comparable performance to regular full precision networks for small\nnumbers of classes while reducing memory requirements 32-fold and computation\nrequirements 58-fold. However, as the number of classes increases\nsignificantly, performance degrades, and pruning-and-quantization based\ncompression techniques take over as the preferred technique being able to\nsatisfy the same space constraints but requiring approximately 8x more\ncomputation. We show that these insights are consistent between raw audio\nclassification and image classification using standard benchmark sets. To the\nbest of our knowledge, this is the first study to apply XNOR to end-to-end\naudio classification and evaluate it in the context of alternative techniques.\nAll codes are publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohaimenuzzaman_M/0/1/0/all/0/1\">Md Mohaimenuzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1\">Christoph Bergmeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_B/0/1/0/all/0/1\">Bernd Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06227","description":"<p>Automated segmentation in medical image analysis is a challenging task that\nrequires a large amount of manually labeled data. However, most existing\nlearning-based approaches usually suffer from limited manually annotated\nmedical data, which poses a major practical problem for accurate and robust\nmedical image segmentation. In addition, most existing semi-supervised\napproaches are usually not robust compared with the supervised counterparts,\nand also lack explicit modeling of geometric structure and semantic\ninformation, both of which limit the segmentation accuracy. In this work, we\npresent SimCVD, a simple contrastive distillation framework that significantly\nadvances state-of-the-art voxel-wise representation learning. We first describe\nan unsupervised training strategy, which takes two views of an input volume and\npredicts their signed distance maps of object boundaries in a contrastive\nobjective, with only two independent dropout as mask. This simple approach\nworks surprisingly well, performing on the same level as previous fully\nsupervised methods with much less labeled data. We hypothesize that dropout can\nbe viewed as a minimal form of data augmentation and makes the network robust\nto representation collapse. Then, we propose to perform structural distillation\nby distilling pair-wise similarities. We evaluate SimCVD on two popular\ndatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT\ndataset. The results on the LA dataset demonstrate that, in two types of\nlabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of\n90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to\nprevious best results. Our method can be trained in an end-to-end fashion,\nshowing the promise of utilizing SimCVD as a general framework for downstream\ntasks, such as medical image synthesis, enhancement, and registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-level Active Detector Learning. (arXiv:2108.09186v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09186","description":"<p>Active learning for object detection is conventionally achieved by applying\ntechniques developed for classification in a way that aggregates individual\ndetections into image-level selection criteria. This is typically coupled with\nthe costly assumption that every image selected for labelling must be\nexhaustively annotated. This yields incremental improvements on well-curated\nvision datasets and struggles in the presence of data imbalance and visual\nclutter that occurs in real-world imagery. Alternatives to the image-level\napproach are surprisingly under-explored in the literature. In this work, we\nintroduce a new strategy that subsumes previous Image-level and Object-level\napproaches into a generalized, Region-level approach that promotes\nspatial-diversity by avoiding nearby redundant queries from the same image and\nminimizes context-switching for the labeler. We show that this approach\nsignificantly decreases labeling effort and improves rare object search on\nrealistic data with inherent class-imbalance and cluttered scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laielli_M/0/1/0/all/0/1\">Michael Laielli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biamby_G/0/1/0/all/0/1\">Giscard Biamby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Ritwik Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loeffler_A/0/1/0/all/0/1\">Adam Loeffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phat Dat Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ross Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Self-Attention Network for Video Saliency Prediction. (arXiv:2108.10696v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10696","description":"<p>3D convolutional neural networks have achieved promising results for video\ntasks in computer vision, including video saliency prediction that is explored\nin this paper. However, 3D convolution encodes visual representation merely on\nfixed local spacetime according to its kernel size, while human attention is\nalways attracted by relational visual features at different time. To overcome\nthis limitation, we propose a novel Spatio-Temporal Self-Attention 3D Network\n(STSANet) for video saliency prediction, in which multiple Spatio-Temporal\nSelf-Attention (STSA) modules are employed at different levels of 3D\nconvolutional backbone to directly capture long-range relations between\nspatio-temporal features of different time steps. Besides, we propose an\nAttentional Multi-Scale Fusion (AMSF) module to integrate multi-level features\nwith the perception of context in semantic and spatio-temporal subspaces.\nExtensive experiments demonstrate the contributions of key components of our\nmethod, and the results on DHF1K, Hollywood-2, UCF, and DIEM benchmark datasets\nclearly prove the superiority of the proposed model compared with all\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lihua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jijun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BVMatch: Lidar-based Place Recognition Using Bird's-eye View Images. (arXiv:2109.00317v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00317","description":"<p>Recognizing places using Lidar in large-scale environments is challenging due\nto the sparse nature of point cloud data. In this paper we present BVMatch, a\nLidar-based frame-to-frame place recognition framework, that is capable of\nestimating 2D relative poses. Based on the assumption that the ground area can\nbe approximated as a plane, we uniformly discretize the ground area into grids\nand project 3D Lidar scans to bird's-eye view (BV) images. We further use a\nbank of Log-Gabor filters to build a maximum index map (MIM) that encodes the\norientation information of the structures in the images. We analyze the\norientation characteristics of MIM theoretically and introduce a novel\ndescriptor called bird's-eye view feature transform (BVFT). The proposed BVFT\nis insensitive to rotation and intensity variations of BV images. Leveraging\nthe BVFT descriptors, we unify the Lidar place recognition and pose estimation\ntasks into the BVMatch framework. The experiments conducted on three\nlarge-scale datasets show that BVMatch outperforms the state-of-the-art methods\nin terms of both recall rate of place recognition and pose estimation accuracy.\nThe source code of our method is publicly available at\nhttps://github.com/zjuluolun/BVMatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Si-Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hui-Liang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing domain adaptation techniques for mitosis detection in multi-scanner breast cancer histopathology images. (arXiv:2109.00869v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.00869","description":"<p>Breast cancer is the most commonly diagnosed cancer worldwide, with over two\nmillion new cases each year. During diagnostic tumour grading, pathologists\nmanually count the number of dividing cells (mitotic figures) in biopsy or\ntumour resection specimens. Since the process is subjective and time-consuming,\ndata-driven artificial intelligence (AI) methods have been developed to\nautomatically detect mitotic figures. However, these methods often generalise\npoorly, with performance reduced by variations in tissue types, staining\nprotocols, or the scanners used to digitise whole-slide images. Domain\nadaptation approaches have been adopted in various applications to mitigate\nthis issue of domain shift. We evaluate two unsupervised domain adaptation\nmethods, CycleGAN and Neural Style Transfer, using the MIDOG 2021 Challenge\ndataset. This challenge focuses on detecting mitotic figures in whole-slide\nimages digitised using different scanners. Two baseline mitosis detection\nmodels based on U-Net and RetinaNet were investigated in combination with the\naforementioned domain adaptation methods. Both baseline models achieved human\nexpert level performance, but had reduced performance when evaluated on images\nwhich had been digitised using a different scanner. The domain adaptation\ntechniques were each found to be beneficial for detection with data from some\nscanners but not for others, with the only average increase across all scanners\nbeing achieved by CycleGAN on the RetinaNet detector. These techniques require\nfurther refinement to ensure consistency in mitosis detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Breen_J/0/1/0/all/0/1\">Jack Breen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zucker_K/0/1/0/all/0/1\">Kieran Zucker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orsi_N/0/1/0/all/0/1\">Nicolas M. Orsi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual 3D Scene Flow Learning with Context-Aware Feature Extraction. (arXiv:2109.04685v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04685","description":"<p>Scene flow estimation is the task to predict the point-wise or pixel-wise 3D\ndisplacement vector between two consecutive frames of point clouds or images,\nwhich has important application in fields such as service robots and autonomous\ndriving. Although many previous works have explored greatly on scene flow\nestimation based on point clouds, there are two problems that have not been\nnoticed or well solved before: 1) Points of adjacent frames in repetitive\npatterns may be wrongly associated due to similar spatial structure in their\nneighbourhoods; 2) Scene flow between adjacent frames of point clouds with\nlong-distance movement may be inaccurately estimated. To solve the first\nproblem, a novel context-aware set convolution layer is proposed in this paper\nto exploit contextual structure information of Euclidean space and learn soft\naggregation weights for local point features. This design is inspired by human\nperception of contextual structure information during scene understanding with\nrepetitive patterns. The context-aware set convolution layer is incorporated in\na context-aware point feature pyramid module of 3D point clouds for scene flow\nestimation. For the second problem, an explicit residual flow learning\nstructure is proposed in the residual flow refinement layer to cope with\nlong-distance movement. The experiments and ablation study on FlyingThings3D\nand KITTI scene flow datasets demonstrate the effectiveness of each proposed\ncomponent. The qualitative results show that the problems of ambiguous\ninter-frame association and long-distance movement estimation are well handled.\nQuantitative results on both FlyingThings3D and KITTI scene flow datasets show\nthat the proposed method achieves state-of-the-art performance, surpassing all\nother previous works to the best of our knowledge by at least 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunzhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinrui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid detection and recognition of whole brain activity in a freely behaving Caenorhabditis elegans. (arXiv:2109.10474v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2109.10474","description":"<p>Advanced volumetric imaging methods and genetically encoded activity\nindicators have permitted a comprehensive characterization of whole brain\nactivity at single neuron resolution in \\textit{Caenorhabditis elegans}. The\nconstant motion and deformation of the mollusc nervous system, however, impose\na great challenge for consistent identification of densely packed neurons in a\nbehaving animal. Here, we propose a cascade solution for long-term and rapid\nrecognition of head ganglion neurons in a freely moving \\textit{C. elegans}.\nFirst, potential neuronal regions from a stack of fluorescence images are\ndetected by a deep learning algorithm. Second, 2-dimensional neuronal regions\nare fused into 3-dimensional neuron entities. Third, by exploiting the neuronal\ndensity distribution surrounding a neuron and relative positional information\nbetween neurons, a multi-class artificial neural network transforms engineered\nneuronal feature vectors into digital neuronal identities. With a small number\nof training samples, our bottom-up approach is able to process each volume -\n$1024 \\times 1024 \\times 18$ in voxels - in less than 1 second and achieves an\naccuracy of $91\\%$ in neuronal detection and $80\\%$ in neuronal recognition.\nOur work represents a step towards rapid and fully automated algorithms for\ndecoding whole brain activity underlying naturalistic behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_S/0/1/0/all/0/1\">Shang Wu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lang_C/0/1/0/all/0/1\">Chengtian Lang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wen_Q/0/1/0/all/0/1\">Quan Wen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_T/0/1/0/all/0/1\">Tianqi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Competence-Aware Path Planning via Introspective Perception. (arXiv:2109.13974v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.13974","description":"<p>Robots deployed in the real world over extended periods of time need to\nreason about unexpected failures, learn to predict them, and to proactively\ntake actions to avoid future failures. Existing approaches for competence-aware\nplanning are either model-based, requiring explicit enumeration of known\nfailure modes, or purely statistical, using state- and location-specific\nfailure statistics to infer competence. We instead propose a structured\nmodel-free approach to competence-aware planning by reasoning about plan\nexecution failures due to errors in perception, without requiring a priori\nenumeration of failure sources or requiring location-specific failure\nstatistics. We introduce competence-aware path planning via introspective\nperception (CPIP), a Bayesian framework to iteratively learn and exploit\ntask-level competence in novel deployment environments. CPIP factorizes the\ncompetence-aware planning problem into two components. First, perception errors\nare learned in a model-free and location-agnostic setting via introspective\nperception prior to deployment in novel environments. Second, during actual\ndeployments, the prediction of task-level failures is learned in a\ncontext-aware setting. Experiments in a simulation show that the proposed CPIP\napproach outperforms the frequentist baseline in multiple mobile robot tasks,\nand is further validated via real robot experiments in an environment with\nperceptually challenging obstacles and terrain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabiee_S/0/1/0/all/0/1\">Sadegh Rabiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basich_C/0/1/0/all/0/1\">Connor Basich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wray_K/0/1/0/all/0/1\">Kyle Hollins Wray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilberstein_S/0/1/0/all/0/1\">Shlomo Zilberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_J/0/1/0/all/0/1\">Joydeep Biswas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Communication-Efficient and Privacy-Preserving Federated Representation Learning. (arXiv:2109.14611v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.14611","description":"<p>This paper investigates the feasibility of federated representation learning\nunder the constraints of communication cost and privacy protection. Existing\nworks either conduct annotation-guided local training which requires frequent\ncommunication or aggregates the client models via weight averaging which has\npotential risks of privacy exposure. To tackle the above problems, we first\nidentify that self-supervised contrastive local training is robust against the\nnon-identically distributed data, which provides the feasibility of longer\nlocal training and thus reduces the communication cost. Then based on the\naforementioned robustness, we propose a novel Federated representation Learning\nframework with Ensemble Similarity Distillation~(FLESD) that utilizes this\nrobustness. At each round of communication, the server first gathers a fraction\nof the clients' inferred similarity matrices on a public dataset. Then it\nensembles the similarity matrices and train the global model via similarity\ndistillation. We verify the effectiveness of FLESD by a series of empirical\nexperiments and show that, despite stricter constraints, it achieves comparable\nresults under multiple settings on multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haizhou Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youcai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zijin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranSalNet: Towards perceptually relevant visual saliency prediction. (arXiv:2110.03593v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2110.03593","description":"<p>Convolutional neural networks (CNNs) have significantly advanced\ncomputational modelling for saliency prediction. However, accurately simulating\nthe mechanisms of visual attention in the human cortex remains an academic\nchallenge. It is critical to integrate properties of human vision into the\ndesign of CNN architectures, leading to perceptually more relevant saliency\nprediction. Due to the inherent inductive biases of CNN architectures, there is\na lack of sufficient long-range contextual encoding capacity. This hinders\nCNN-based saliency models from capturing properties that emulate viewing\nbehaviour of humans. Transformers have shown great potential in encoding\nlong-range information by leveraging the self-attention mechanism. In this\npaper, we propose a novel saliency model that integrates transformer components\nto CNNs to capture the long-range contextual visual information. Experimental\nresults show that the transformers provide added value to saliency prediction,\nenhancing its perceptual relevance in the performance. Our proposed saliency\nmodel using transformers has achieved superior results on public benchmarks and\ncompetitions for saliency prediction models.\n</p>\n<p>The source code of our proposed saliency model TranSalNet is available at:\nhttps://github.com/LJOVO/TranSalNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jianxun Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hanhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_D/0/1/0/all/0/1\">David Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saupe_D/0/1/0/all/0/1\">Dietmar Saupe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hantao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Semantic Aggregation and Calibration for Separated Domain Generalization. (arXiv:2110.06736v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06736","description":"<p>Domain generalization (DG) aims to learn from multiple known source domains a\nmodel that can generalize well to unknown target domains. The existing DG\nmethods usually exploit the fusion of shared multi-source data for capturing\ndomain invariance and training a generalizable model. However, tremendous data\nis distributed across lots of places nowadays that can not be shared due to\nstrict privacy policies. A dilemma is thus raised between the generalization\nlearning with shared multi-source data and the privacy protection of real-world\nsensitive data. In this paper, we introduce a separated domain generalization\ntask with separated source datasets that can only be accessed locally for data\nprivacy protection. We propose a novel solution called Collaborative Semantic\nAggregation and Calibration (CSAC) to enable this challenging task. To fully\nabsorb multi-source semantic information while avoiding unsafe data fusion, we\nconduct data-free semantic aggregation by fusing the models trained on the\nseparated domains layer-by-layer. To address the semantic dislocation problem\ncaused by domain shift, we further design cross-layer semantic calibration with\nan elaborate attention mechanism to align each semantic level and enhance\ndomain invariance. We unify multi-source semantic learning and alignment in a\ncollaborative way by repeating the semantic aggregation and calibration\nalternately, keeping each dataset localized, and the data privacy is thus\ncarefully protected. Extensive experiments show the significant performance of\nour method in addressing this challenging task, which is even comparable to the\nprevious DG methods with shared source data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junkun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling. (arXiv:2110.08263v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08263","description":"<p>The recently proposed FixMatch achieved state-of-the-art results on most\nsemi-supervised learning (SSL) benchmarks. However, like other modern SSL\nalgorithms, FixMatch uses a pre-defined constant threshold for all classes to\nselect unlabeled data that contribute to the training, thus failing to consider\ndifferent learning status and learning difficulties of different classes. To\naddress this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum\nlearning approach to leverage unlabeled data according to the model's learning\nstatus. The core of CPL is to flexibly adjust thresholds for different classes\nat each time step to let pass informative unlabeled data and their pseudo\nlabels. CPL does not introduce additional parameters or computations (forward\nor backward propagation). We apply CPL to FixMatch and call our improved\nalgorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a\nvariety of SSL benchmarks, with especially strong performances when the labeled\ndata are extremely limited or when the task is challenging. For example,\nFlexMatch achieves 13.96% and 18.96% error rate reduction over FixMatch on\nCIFAR-100 and STL-10 datasets respectively, when there are only 4 labels per\nclass. CPL also significantly boosts the convergence speed, e.g., FlexMatch can\nuse only 1/5 training time of FixMatch to achieve even better performance.\nFurthermore, we show that CPL can be easily adapted to other SSL algorithms and\nremarkably improve their performances. We open-source our code at\nhttps://github.com/TorchSSL/TorchSSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okumura_M/0/1/0/all/0/1\">Manabu Okumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithmic encoding of protected characteristics in image-based models for disease detection. (arXiv:2110.14755v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.14755","description":"<p>It has been rightfully emphasized that the use of AI for clinical decision\nmaking could amplify health disparities. A machine learning model may pick up\nundesirable correlations, for example, between a patient's racial identity and\nclinical outcome. Such correlations are often present in (historical) data used\nfor model development. There has been an increase in studies reporting biases\nin image-based disease detection models. Besides the scarcity of data from\nunderserved populations, very little is known about how these biases are\nencoded and how one may reduce or even remove disparate performance. There are\nconcerns that an algorithm may recognize patient characteristics such as\nbiological sex or racial identity, and then directly or indirectly use this\ninformation when making predictions. But it remains unclear how we can\nestablish whether such information is actually used. This article aims to shed\nsome light on these issues by exploring different methodology for assessing the\ninner working of disease detection models. We explore multitask learning and\nmodel inspection to assess the relationship between protected characteristics\nand prediction of disease. We believe our analysis framework could provide\nvaluable insights in future studies in medical imaging AI. Our findings also\ncall for further research to better understand the underlying causes of\nperformance disparities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1\">Charles Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernhardt_M/0/1/0/all/0/1\">Melanie Bernhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winzeck_S/0/1/0/all/0/1\">Stefan Winzeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Distilled Collaboration Graph for Multi-Agent Perception. (arXiv:2111.00643v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00643","description":"<p>To promote better performance-bandwidth trade-off for multi-agent perception,\nwe propose a novel distilled collaboration graph (DiscoGraph) to model\ntrainable, pose-aware, and adaptive collaboration among agents. Our key\nnovelties lie in two aspects. First, we propose a teacher-student framework to\ntrain DiscoGraph via knowledge distillation. The teacher model employs an early\ncollaboration with holistic-view inputs; the student model is based on\nintermediate collaboration with single-view inputs. Our framework trains\nDiscoGraph by constraining post-collaboration feature maps in the student model\nto match the correspondences in the teacher model. Second, we propose a\nmatrix-valued edge weight in DiscoGraph. In such a matrix, each element\nreflects the inter-agent attention at a specific spatial region, allowing an\nagent to adaptively highlight the informative regions. During inference, we\nonly need to use the student model named as the distilled collaboration network\n(DiscoNet). Attributed to the teacher-student framework, multiple agents with\nthe shared DiscoNet could collaboratively approach the performance of a\nhypothetical teacher model with a holistic view. Our approach is validated on\nV2X-Sim 1.0, a large-scale multi-agent perception dataset that we synthesized\nusing CARLA and SUMO co-simulation. Our quantitative and qualitative\nexperiments in multi-agent 3D object detection show that DiscoNet could not\nonly achieve a better performance-bandwidth trade-off than the state-of-the-art\ncollaborative perception methods, but also bring more straightforward design\nrationale. Our code is available on https://github.com/ai4ce/DiscoNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shunli Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pengxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"First steps on Gamification of Lung Fluid Cells Annotations in the Flower Domain. (arXiv:2111.03663v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.03663","description":"<p>Annotating data, especially in the medical domain, requires expert knowledge\nand a lot of effort. This limits the amount and/or usefulness of available\nmedical data sets for experimentation. Therefore, developing strategies to\nincrease the number of annotations while lowering the needed domain knowledge\nis of interest. A possible strategy is the use of gamification, i.e.\ntransforming the annotation task into a game. We propose an approach to gamify\nthe task of annotating lung fluid cells from pathological whole slide images\n(WSIs). As the domain is unknown to non-expert annotators, we transform images\nof cells to the domain of flower images using a CycleGAN architecture. In this\nmore assessable domain, non-expert annotators can be (t)asked to annotate\ndifferent kinds of flowers in a playful setting. In order to provide a proof of\nconcept, this work shows that the domain transfer is possible by evaluating an\nimage classification network trained on real cell images and tested on the cell\nimages generated by the CycleGAN network (reconstructed cell images) as well as\nreal cell images. The classification network reaches an average accuracy of\n94.73 % on the original lung fluid cells and 95.25 % on the transformed lung\nfluid cells, respectively. Our study lays the foundation for future research on\ngamification using CycleGANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kunzmann_S/0/1/0/all/0/1\">Sonja Kunzmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marzahl_C/0/1/0/all/0/1\">Christian Marzahl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1\">Felix Denzinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertram_C/0/1/0/all/0/1\">Christof A. Bertram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klopfleisch_R/0/1/0/all/0/1\">Robert Klopfleisch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1\">Katharina Breininger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christlein_V/0/1/0/all/0/1\">Vincent Christlein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks. (arXiv:2111.07492v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07492","description":"<p>One major problem in black-box adversarial attacks is the high query\ncomplexity in the hard-label attack setting, where only the top-1 predicted\nlabel is available. In this paper, we propose a novel geometric-based approach\ncalled Tangent Attack (TA), which identifies an optimal tangent point of a\nvirtual hemisphere located on the decision boundary to reduce the distortion of\nthe attack. Assuming the decision boundary is locally flat, we theoretically\nprove that the minimum $\\ell_2$ distortion can be obtained by reaching the\ndecision boundary along the tangent line passing through such tangent point in\neach iteration. To improve the robustness of our method, we further propose a\ngeneralized method which replaces the hemisphere with a semi-ellipsoid to adapt\nto curved decision boundaries. Our approach is free of pre-training. Extensive\nexperiments conducted on the ImageNet and CIFAR-10 datasets demonstrate that\nour approach can consume only a small number of queries to achieve the\nlow-magnitude distortion. The implementation source code is released online at\nhttps://github.com/machanic/TangentAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chen Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiangyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1\">Jun-Hai Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00656","description":"<p>Recently, by introducing large-scale dataset and strong transformer network,\nvideo-language pre-training has shown great success especially for retrieval.\nYet, existing video-language transformer models do not explicitly fine-grained\nsemantic align. In this work, we present Object-aware Transformers, an\nobject-centric approach that extends video-language transformer to incorporate\nobject representations. The key idea is to leverage the bounding boxes and\nobject tags to guide the training process. We evaluate our model on three\nstandard sub-tasks of video-text matching on four widely used benchmarks. We\nalso provide deep analysis and detailed ablation about the proposed method. We\nshow clear improvement in performance across all tasks and datasets considered,\ndemonstrating the value of a model that incorporates object representations\ninto a video-language architecture. The code will be released at\n\\url{https://github.com/FingerRec/OA-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to automate cryo-electron microscopy data collection with Ptolemy. (arXiv:2112.01534v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.01534","description":"<p>Over the past decade, cryogenic electron microscopy (cryo-EM) has emerged as\na primary method for determining near-native, near-atomic resolution 3D\nstructures of biological macromolecules. In order to meet increasing demand for\ncryo-EM, automated methods to improve throughput and efficiency while lowering\ncosts are needed. Currently, all high-magnification cryo-EM data collection\nsoftwares require human input and manual tuning of parameters. Expert operators\nmust navigate low- and medium-magnification images to find good\nhigh-magnification collection locations. Automating this is non-trivial: the\nimages suffer from low signal-to-noise ratio and are affected by a range of\nexperimental parameters that can differ for each collection session. Here, we\nuse various computer vision algorithms, including mixture models, convolutional\nneural networks, and U-Nets to develop the first pipeline to automate low- and\nmedium-magnification targeting. Learned models in this pipeline are trained on\na large internal dataset of images from real world cryo-EM data collection\nsessions, labeled with locations that were selected by operators. Using these\nmodels, we show that we can effectively detect and classify regions of interest\nin low- and medium-magnification images, and can generalize to unseen sessions,\nas well as to images captured using different microscopes from external\nfacilities. We expect our open-source pipeline, Ptolemy, will be both\nimmediately useful as a tool for automation of cryo-EM data collection, and\nserve as a foundation for future advanced methods for efficient and automated\ncryo-EM microscopy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_P/0/1/0/all/0/1\">Paul T. Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noble_A/0/1/0/all/0/1\">Alex J. Noble</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_A/0/1/0/all/0/1\">Anchi Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bepler_T/0/1/0/all/0/1\">Tristan Bepler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action Units That Constitute Trainable Micro-expressions (and A Large-scale Synthetic Dataset). (arXiv:2112.01730v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01730","description":"<p>Because of the expensive data collection process, micro-expression (MiE)\ndatasets are generally much smaller in scale than those in other computer\nvision fields, rendering large-scale training less feasible. This paper\ndevelops a protocol to automatically synthesize MiE training data that 1) are\nof a large scale and 2) allow us to train accurate recognition models for\nreal-world test data. Specifically, we discover three types of Action Units\n(AUs) that can constitute trainable MiEs. These AUs come from real-world MiEs,\nearly frames of macro-expression videos, and the relationship between AUs and\nexpression categories defined by human expert knowledge. With these AUs, our\nprotocol then employs large numbers of face images of various identities and an\noff-the-shelf face generator for MiE synthesis, yielding the MiE-X dataset. MiE\nrecognition models are trained or pre-trained on MiE-X and evaluated on\nreal-world test sets, where competitive accuracy is obtained. Experimental\nresults not only validate the effectiveness of these AUs and our MiE-X dataset\nbut also reveal some critical properties of MiEs: they generalize across faces,\nare close to early-stage macro-expressions, and can be manually defined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongdao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DualFormer: Local-Global Stratified Transformer for Efficient Video Recognition. (arXiv:2112.04674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04674","description":"<p>While transformers have shown great potential on video recognition tasks with\ntheir strong capability of capturing long-range dependencies, they often suffer\nhigh computational costs induced by self-attention operation on the huge number\nof 3D tokens in a video. In this paper, we propose a new transformer\narchitecture, termed DualFormer, which can effectively and efficiently perform\nspace-time attention for video recognition. Specifically, our DualFormer\nstratifies the full space-time attention into dual cascaded levels, i.e., to\nfirst learn fine-grained local space-time interactions among nearby 3D tokens,\nfollowed by the capture of coarse-grained global dependencies between the query\ntoken and the coarse-grained global pyramid contexts. Different from existing\nmethods that apply space-time factorization or restrict attention computations\nwithin local windows for improving efficiency, our local-global stratified\nstrategy can well capture both short- and long-range spatiotemporal\ndependencies, and meanwhile greatly reduces the number of keys and values in\nattention computation to boost efficiency. Experimental results show the\nsuperiority of DualFormer on five video benchmarks against existing methods. In\nparticular, DualFormer sets new state-of-the-art 82.9%/85.2% top-1 accuracy on\nKinetics-400/600 with around 1000G inference FLOPs which is at least 3.2 times\nfewer than existing methods with similar performances. We have released our\ncode at https://github.com/sail-sg/dualformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1\">Roger Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-Training for Spatial-Aware Visual Representations. (arXiv:2112.04680v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04680","description":"<p>Pre-training has become a standard paradigm in many computer vision tasks.\nHowever, most of the methods are generally designed on the RGB image domain.\nDue to the discrepancy between the two-dimensional image plane and the\nthree-dimensional space, such pre-trained models fail to perceive spatial\ninformation and serve as sub-optimal solutions for 3D-related tasks. To bridge\nthis gap, we aim to learn a spatial-aware visual representation that can\ndescribe the three-dimensional space and is more suitable and effective for\nthese tasks. To leverage point clouds, which are much more superior in\nproviding spatial information compared to images, we propose a simple yet\neffective 2D Image and 3D Point cloud Unsupervised pre-training strategy,\ncalled SimIPU. Specifically, we develop a multi-modal contrastive learning\nframework that consists of an intra-modal spatial perception module to learn a\nspatial-aware representation from point clouds and an inter-modal feature\ninteraction module to transfer the capability of perceiving spatial information\nfrom the point cloud encoder to the image encoder, respectively. Positive pairs\nfor contrastive losses are established by the matching algorithm and the\nprojection matrix. The whole framework is trained in an unsupervised end-to-end\nfashion. To the best of our knowledge, this is the first study to explore\ncontrastive learning pre-training strategies for outdoor multi-modal datasets,\ncontaining paired camera images and LIDAR point clouds. Codes and models are\navailable at https://github.com/zhyever/SimIPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Translation Prior: Test-time Training for Photorealistic Style Transfer. (arXiv:2112.06150v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06150","description":"<p>Recent techniques to solve photorealistic style transfer within deep\nconvolutional neural networks (CNNs) generally require intensive training from\nlarge-scale datasets, thus having limited applicability and poor generalization\nability to unseen images or styles. To overcome this, we propose a novel\nframework, dubbed Deep Translation Prior (DTP), to accomplish photorealistic\nstyle transfer through test-time training on given input image pair with\nuntrained networks, which learns an image pair-specific translation prior and\nthus yields better performance and generalization. Tailored for such test-time\ntraining for style transfer, we present novel network architectures, with two\nsub-modules of correspondence and generation modules, and loss functions\nconsisting of contrastive content, style, and cycle consistency losses. Our\nframework does not require offline training phase for style transfer, which has\nbeen one of the main challenges in existing methods, but the networks are to be\nsolely learned during test-time. Experimental results prove that our framework\nhas a better generalization ability to unseen image pairs and even outperforms\nthe state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soohyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Human-Object Interaction Detection via Phrase Learning and Label Composition. (arXiv:2112.07383v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07383","description":"<p>Human-Object Interaction (HOI) detection is a fundamental task in high-level\nhuman-centric scene understanding. We propose PhraseHOI, containing a HOI\nbranch and a novel phrase branch, to leverage language prior and improve\nrelation expression. Specifically, the phrase branch is supervised by semantic\nembeddings, whose ground truths are automatically converted from the original\nHOI annotations without extra human efforts. Meanwhile, a novel label\ncomposition method is proposed to deal with the long-tailed problem in HOI,\nwhich composites novel phrase labels by semantic neighbors. Further, to\noptimize the phrase branch, a loss composed of a distilling loss and a balanced\ntriplet loss is proposed. Extensive experiments are conducted to prove the\neffectiveness of the proposed PhraseHOI, which achieves significant improvement\nover the baseline and surpasses previous state-of-the-art methods on Full and\nNonRare on the challenging HICO-DET benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1\">Cheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boxun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensembling Off-the-shelf Models for GAN Training. (arXiv:2112.09130v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09130","description":"<p>The advent of large-scale training has produced a cornucopia of powerful\nvisual recognition models. However, generative models, such as GANs, have\ntraditionally been trained from scratch in an unsupervised manner. Can the\ncollective \"knowledge\" from a large bank of pretrained vision models be\nleveraged to improve GAN training? If so, with so many models to choose from,\nwhich one(s) should be selected, and in what manner are they most effective? We\nfind that pretrained computer vision models can significantly improve\nperformance when used in an ensemble of discriminators. Notably, the particular\nsubset of selected models greatly affects performance. We propose an effective\nselection mechanism, by probing the linear separability between real and fake\nsamples in pretrained model embeddings, choosing the most accurate model, and\nprogressively adding it to the discriminator ensemble. Interestingly, our\nmethod can improve GAN training in both limited data and large-scale settings.\nGiven only 10k training samples, our FID on LSUN Cat matches the StyleGAN2\ntrained on 1.6M images. On the full dataset, our method improves FID by 1.5x to\n2x on cat, church, and horse categories of LSUN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumari_N/0/1/0/all/0/1\">Nupur Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroVL: A Strong Baseline for Aligning Vision-Language Representations with Limited Resources. (arXiv:2112.09331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09331","description":"<p>Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have\nrevealed the potential of aligning multi-modal representations with contrastive\nlearning. However, these works require a tremendous amount of data and\ncomputational resources (e.g., billion-level web data and hundreds of GPUs),\nwhich prevent researchers with limited resources from reproduction and further\nexploration. To this end, we explore a stack of simple but effective\nheuristics, and provide a comprehensive training guidance, which allows us to\nconduct dual-encoder multi-modal representation alignment with limited\nresources. We provide a reproducible strong baseline of competitive results,\nnamely ZeroVL, with only 14M publicly accessible academic datasets and 8 V100\nGPUs. Additionally, we collect 100M web data for pre-training, and achieve\ncomparable or superior results than state-of-the-art methods, further proving\nthe effectiveness of our method on large-scale data. We hope that this work\nwill provide useful data points and experience for future research in\nmulti-modal pre-training. Our code is available at\nhttps://github.com/zerovl/ZeroVL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1\">Quan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Boyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Weidong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshie_O/0/1/0/all/0/1\">Osamu Yoshie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Subsampling for ROI-based Visual Tracking: Algorithms and FPGA Implementation. (arXiv:2112.09775v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09775","description":"<p>There is tremendous scope for improving the energy efficiency of embedded\nvision systems by incorporating programmable region-of-interest (ROI) readout\nin the image sensor design. In this work, we study how ROI programmability can\nbe leveraged for tracking applications by anticipating where the ROI will be\nlocated in future frames and switching pixels off outside of this region. We\nrefer to this process of ROI prediction and corresponding sensor configuration\nas adaptive subsampling. Our adaptive subsampling algorithms comprise an object\ndetector and an ROI predictor (Kalman filter) which operate in conjunction to\noptimize the energy efficiency of the vision pipeline with the end task being\nobject tracking. To further facilitate the implementation of our adaptive\nalgorithms in real life, we select a candidate algorithm and map it onto an\nFPGA. Leveraging Xilinx Vitis AI tools, we designed and accelerated a YOLO\nobject detector-based adaptive subsampling algorithm. In order to further\nimprove the algorithm post-deployment, we evaluated several competing baselines\non the OTB100 and LaSOT datasets. We found that coupling the ECO tracker with\nthe Kalman filter has a competitive AUC score of 0.4568 and 0.3471 on the\nOTB100 and LaSOT datasets respectively. Further, the power efficiency of this\nalgorithm is on par with, and in a couple of instances superior to, the other\nbaselines. The ECO-based algorithm incurs a power consumption of approximately\n4 W averaged across both datasets while the YOLO-based approach requires power\nconsumption of approximately 6 W (as per our power consumption model). In terms\nof accuracy-latency tradeoff, the ECO-based algorithm provides near-real-time\nperformance (19.23 FPS) while managing to attain competitive tracking\nprecision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_O/0/1/0/all/0/1\">Odrika Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muro_V/0/1/0/all/0/1\">Victor Isaac Torres Muro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katoch_S/0/1/0/all/0/1\">Sameeksha Katoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanias_A/0/1/0/all/0/1\">Andreas Spanias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasuriya_S/0/1/0/all/0/1\">Suren Jayasuriya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Implicit Body Representations from Double Diffusion Based Neural Radiance Fields. (arXiv:2112.12390v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12390","description":"<p>In this paper, we present a novel double diffusion based neural radiance\nfield, dubbed DD-NeRF, to reconstruct human body geometry and render the human\nbody appearance in novel views from a sparse set of images. We first propose a\ndouble diffusion mechanism to achieve expressive representations of input\nimages by fully exploiting human body priors and image appearance details at\ntwo levels. At the coarse level, we first model the coarse human body poses and\nshapes via an unclothed 3D deformable vertex model as guidance. At the fine\nlevel, we present a multi-view sampling network to capture subtle geometric\ndeformations and image detailed appearances, such as clothing and hair, from\nmultiple input views. Considering the sparsity of the two level features, we\ndiffuse them into feature volumes in the canonical space to construct neural\nradiance fields. Then, we present a signed distance function (SDF) regression\nnetwork to construct body surfaces from the diffused features. Thanks to our\ndouble diffused representations, our method can even synthesize novel views of\nunseen subjects. Experiments on various datasets demonstrate that our approach\noutperforms the state-of-the-art in both geometric reconstruction and novel\nview synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_G/0/1/0/all/0/1\">Guangming Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01293","description":"<p>This paper presents a transformer-based Siamese network architecture\n(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of\nco-registered remote sensing images. Different from recent CD frameworks, which\nare based on fully convolutional networks (ConvNets), the proposed method\nunifies hierarchically structured transformer encoder with Multi-Layer\nPerception (MLP) decoder in a Siamese network architecture to efficiently\nrender multi-scale long-range details required for accurate CD. Experiments on\ntwo CD datasets show that the proposed end-to-end trainable ChangeFormer\narchitecture achieves better CD performance than previous counterparts. Our\ncode is available at https://github.com/wgcban/ChangeFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Uniform Point Distribution in Feature-preserving Point Cloud Filtering. (arXiv:2201.01503v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01503","description":"<p>As a popular representation of 3D data, point cloud may contain noise and\nneed to be filtered before use. Existing point cloud filtering methods either\ncannot preserve sharp features or result in uneven point distribution in the\nfiltered output. To address this problem, this paper introduces a point cloud\nfiltering method that considers both point distribution and feature\npreservation during filtering. The key idea is to incorporate a repulsion term\nwith a data term in energy minimization. The repulsion term is responsible for\nthe point distribution, while the data term is to approximate the noisy\nsurfaces while preserving the geometric features. This method is capable of\nhandling models with fine-scale features and sharp features. Extensive\nexperiments show that our method yields better results with a more uniform\npoint distribution ($5.8\\times10^{-5}$ Chamfer Distance on average) in seconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Intracranial Aneurysm Classification and Segmentation via Unsupervised Dual-branch Learning. (arXiv:2201.02198v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02198","description":"<p>Intracranial aneurysms are common nowadays and how to detect them\nintelligently is of great significance in digital health. While most existing\ndeep learning research focused on medical images in a supervised way, we\nintroduce an unsupervised method for the detection of intracranial aneurysms\nbased on 3D point cloud data. In particular, our method consists of two stages:\nunsupervised pre-training and downstream tasks. As for the former, the main\nidea is to pair each point cloud with its jittered counterpart and maximise\ntheir correspondence. Then we design a dual-branch contrastive network with an\nencoder for each branch and a subsequent common projection head. As for the\nlatter, we design simple networks for supervised classification and\nsegmentation training. Experiments on the public dataset (IntrA) show that our\nunsupervised method achieves comparable or even better performance than some\nstate-of-the-art supervised techniques, and it is most prominent in the\ndetection of aneurysmal vessels. Experiments on the ModelNet40 also show that\nour method achieves the accuracy of 90.79\\% which outperforms existing\nstate-of-the-art unsupervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shao_D/0/1/0/all/0/1\">Di Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Scale-Invariance and Uncertainty for Self-Supervised Domain Adaptation of Foggy Scenes Segmentation. (arXiv:2201.02588v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02588","description":"<p>This paper presents FogAdapt, a novel approach for domain adaptation of\nsemantic segmentation for dense foggy scenes. Although significant research has\nbeen directed to reduce the domain shift in semantic segmentation, adaptation\nto scenes with adverse weather conditions remains an open question. Large\nvariations in the visibility of the scene due to weather conditions, such as\nfog, smog, and haze, exacerbate the domain shift, thus making unsupervised\nadaptation in such scenarios challenging. We propose a self-entropy and\nmulti-scale information augmented self-supervised domain adaptation method\n(FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported\nby the empirical evidence that an increase in fog density results in high\nself-entropy for segmentation probabilities, we introduce a self-entropy based\nloss function to guide the adaptation method. Furthermore, inferences obtained\nat different image scales are combined and weighted by the uncertainty to\ngenerate scale-invariant pseudo-labels for the target domain. These\nscale-invariant pseudo-labels are robust to visibility and scale variations. We\nevaluate the proposed model on real clear-weather scenes to real foggy scenes\nadaptation and synthetic non-foggy images to real foggy scenes adaptation\nscenarios. Our experiments demonstrate that FogAdapt significantly outperforms\nthe current state-of-the-art in semantic segmentation of foggy images.\nSpecifically, by considering the standard settings compared to state-of-the-art\n(SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy\nDriving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes\nto Foggy Zurich.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_J/0/1/0/all/0/1\">Javed Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafiz_R/0/1/0/all/0/1\">Rehan Hafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counteracting Dark Web Text-Based CAPTCHA with Generative Adversarial Learning for Proactive Cyber Threat Intelligence. (arXiv:2201.02799v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02799","description":"<p>Automated monitoring of dark web (DW) platforms on a large scale is the first\nstep toward developing proactive Cyber Threat Intelligence (CTI). While there\nare efficient methods for collecting data from the surface web, large-scale\ndark web data collection is often hindered by anti-crawling measures. In\nparticular, text-based CAPTCHA serves as the most prevalent and prohibiting\ntype of these measures in the dark web. Text-based CAPTCHA identifies and\nblocks automated crawlers by forcing the user to enter a combination of\nhard-to-recognize alphanumeric characters. In the dark web, CAPTCHA images are\nmeticulously designed with additional background noise and variable character\nlength to prevent automated CAPTCHA breaking. Existing automated CAPTCHA\nbreaking methods have difficulties in overcoming these dark web challenges. As\nsuch, solving dark web text-based CAPTCHA has been relying heavily on human\ninvolvement, which is labor-intensive and time-consuming. In this study, we\npropose a novel framework for automated breaking of dark web CAPTCHA to\nfacilitate dark web data collection. This framework encompasses a novel\ngenerative method to recognize dark web text-based CAPTCHA with noisy\nbackground and variable character length. To eliminate the need for human\ninvolvement, the proposed framework utilizes Generative Adversarial Network\n(GAN) to counteract dark web background noise and leverages an enhanced\ncharacter segmentation algorithm to handle CAPTCHA images with variable\ncharacter length. Our proposed framework, DW-GAN, was systematically evaluated\non multiple dark web CAPTCHA testbeds. DW-GAN significantly outperformed the\nstate-of-the-art benchmark methods on all datasets, achieving over 94.4%\nsuccess rate on a carefully collected real-world dark web dataset...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mohammadreza Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsinchun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving Camera Position for a Practical Application of Gaze Estimation on Edge Devices. (arXiv:2201.02946v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02946","description":"<p>Most Gaze estimation research only works on a setup condition that a camera\nperfectly captures eyes gaze. They have not literarily specified how to set up\na camera correctly for a given position of a person. In this paper, we carry\nout a study on gaze estimation with a logical camera setup position. We further\nbring our research in a practical application by using inexpensive edge devices\nwith a realistic scenario. That is, we first set up a shopping environment\nwhere we want to grasp customers gazing behaviors. This setup needs an optimal\ncamera position in order to maintain estimation accuracy from existing gaze\nestimation research. We then apply the state-of-the-art of few-shot learning\ngaze estimation to reduce training sampling in the inference phase. In the\nexperiment, we perform our implemented research on NVIDIA Jetson TX2 and\nachieve a reasonable speed, 12 FPS which is faster compared with our reference\nwork, without much degradation of gaze estimation accuracy. The source code is\nreleased at https://github.com/linh-gist/GazeEstimationTX2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linh Van Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tin Trung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducing BowNet: Learning Representations by Predicting Bags of Visual Words. (arXiv:2201.03556v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03556","description":"<p>This work aims to reproduce results from the CVPR 2020 paper by Gidaris et\nal. Self-supervised learning (SSL) is used to learn feature representations of\nan image using an unlabeled dataset. This work proposes to use bag-of-words\n(BoW) deep feature descriptors as a self-supervised learning target to learn\nrobust, deep representations. BowNet is trained to reconstruct the histogram of\nvisual words (ie. the deep BoW descriptor) of a reference image when presented\na perturbed version of the image as input. Thus, this method aims to learn\nperturbation-invariant and context-aware image features that can be useful for\nfew-shot tasks or supervised downstream tasks. In the paper, the author\ndescribes BowNet as a network consisting of a convolutional feature extractor\n$\\Phi(\\cdot)$ and a Dense-softmax layer $\\Omega(\\cdot)$ trained to predict BoW\nfeatures from images. After BoW training, the features of $\\Phi$ are used in\ndownstream tasks. For this challenge we were trying to build and train a\nnetwork that could reproduce the CIFAR-100 accuracy improvements reported in\nthe original paper. However, we were unsuccessful in reproducing an accuracy\nimprovement comparable to what the authors mentioned. This could be for a\nvariety of factors and we believe that time constraints were the primary\nbottleneck.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Harry Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Stone Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_H/0/1/0/all/0/1\">Hisham Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrintsGAN: Synthetic Fingerprint Generator. (arXiv:2201.03674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03674","description":"<p>A major impediment to researchers working in the area of fingerprint\nrecognition is the lack of publicly available, large-scale, fingerprint\ndatasets. The publicly available datasets that do exist contain very few\nidentities and impressions per finger. This limits research on a number of\ntopics, including e.g., using deep networks to learn fixed length fingerprint\nembeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator\ncapable of generating unique fingerprints along with multiple impressions for a\ngiven fingerprint. Using PrintsGAN, we synthesize a database of 525k\nfingerprints (35K distinct fingers, each with 15 impressions). Next, we show\nthe utility of the PrintsGAN generated dataset by training a deep network to\nextract a fixed-length embedding from a fingerprint. In particular, an\nembedding model trained on our synthetic fingerprints and fine-tuned on a small\nnumber of publicly available real fingerprints (25K prints from NIST SD302)\nobtains a TAR of 87.03% @ FAR=0.01% on the NIST SD4 database (a boost from\nTAR=73.37% when only trained on NIST SD302). Prevailing synthetic fingerprint\ngeneration methods do not enable such performance gains due to i) lack of\nrealism or ii) inability to generate multiple impressions per finger. We plan\nto release our database of synthetic fingerprints to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Engelsma_J/0/1/0/all/0/1\">Joshua J. Engelsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosz_S/0/1/0/all/0/1\">Steven A. Grosz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anil K. Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning. (arXiv:2201.04182v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.04182","description":"<p>In this work we propose a HyperTransformer, a transformer-based model for\nfew-shot learning that generates weights of a convolutional neural network\n(CNN) directly from support samples. Since the dependence of a small generated\nCNN model on a specific task is encoded by a high-capacity transformer model,\nwe effectively decouple the complexity of the large task space from the\ncomplexity of individual tasks. Our method is particularly effective for small\ntarget CNN architectures where learning a fixed universal task-independent\nembedding is not optimal and better performance is attained when the\ninformation about the task can modulate all model parameters. For larger models\nwe discover that generating the last layer alone allows us to produce\ncompetitive or better results than those obtained with state-of-the-art methods\nwhile being end-to-end differentiable. Finally, we extend our approach to a\nsemi-supervised regime utilizing unlabeled samples in the support set and\nfurther improving few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Capacitance: A New Perspective of Neural Network Selection via Edge Dynamics. (arXiv:2201.04194v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.04194","description":"<p>Efficient model selection for identifying a suitable pre-trained neural\nnetwork to a downstream task is a fundamental yet challenging task in deep\nlearning. Current practice requires expensive computational costs in model\ntraining for performance prediction. In this paper, we propose a novel\nframework for neural network selection by analyzing the governing dynamics over\nsynaptic connections (edges) during training. Our framework is built on the\nfact that back-propagation during neural network training is equivalent to the\ndynamical evolution of synaptic connections. Therefore, a converged neural\nnetwork is associated with an equilibrium state of a networked system composed\nof those edges. To this end, we construct a network mapping $\\phi$, converting\na neural network $G_A$ to a directed line graph $G_B$ that is defined on those\nedges in $G_A$. Next, we derive a neural capacitance metric $\\beta_{\\rm eff}$\nas a predictive measure universally capturing the generalization capability of\n$G_A$ on the downstream task using only a handful of early training results. We\ncarried out extensive experiments using 17 popular pre-trained ImageNet models\nand five benchmark datasets, including CIFAR10, CIFAR100, SVHN, Fashion MNIST\nand Birds, to evaluate the fine-tuning performance of our framework. Our neural\ncapacitance metric is shown to be a powerful indicator for model selection\nbased only on early training results and is more efficient than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chunheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedapati_T/0/1/0/all/0/1\">Tejaswini Pedapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianxi Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04756","description":"<p>In this paper, we present the solution of roadside LiDAR object detection\nusing a combination of two unsupervised learning algorithms. The 3D point\nclouds data are firstly converted into spherical coordinates and filled into\nthe azimuth grid matrix using a hash function. After that, the raw LiDAR data\nwere rearranged into spatial-temporal data structures to store the information\nof range, azimuth, and intensity. Dynamic Mode Decomposition method is applied\nfor decomposing the point cloud data into low-rank backgrounds and sparse\nforegrounds based on intensity channel pattern recognition. The Triangle\nAlgorithm automatically finds the dividing value to separate the moving targets\nfrom static background according to range information. After intensity and\nrange background subtraction, the foreground moving objects will be detected\nusing a density-based detector and encoded into the state-space model for\ntracking. The output of the proposed model includes vehicle trajectories that\ncan enable many mobility and safety applications. The method was validated\nagainst a commercial traffic data collection platform and demonstrated to be an\nefficient and reliable solution for infrastructure LiDAR object detection. In\ncontrast to the previous methods that process directly on the scattered and\ndiscrete point clouds, the proposed method can establish the less sophisticated\nlinear relationship of the 3D measurement data, which captures the\nspatial-temporal structure that we often desire.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peter J. Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Cross-Modality Retinal Vessel Segmentation via Disentangling Representation Style Transfer and Collaborative Consistency Learning. (arXiv:2201.04812v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.04812","description":"<p>Various deep learning models have been developed to segment anatomical\nstructures from medical images, but they typically have poor performance when\ntested on another target domain with different data distribution. Recently,\nunsupervised domain adaptation methods have been proposed to alleviate this\nso-called domain shift issue, but most of them are designed for scenarios with\nrelatively small domain shifts and are likely to fail when encountering a large\ndomain gap. In this paper, we propose DCDA, a novel cross-modality unsupervised\ndomain adaptation framework for tasks with large domain shifts, e.g.,\nsegmenting retinal vessels from OCTA and OCT images. DCDA mainly consists of a\ndisentangling representation style transfer (DRST) module and a collaborative\nconsistency learning (CCL) module. DRST decomposes images into content\ncomponents and style codes and performs style transfer and image\nreconstruction. CCL contains two segmentation models, one for source domain and\nthe other for target domain. The two models use labeled data (together with the\ncorresponding transferred images) for supervised learning and perform\ncollaborative consistency learning on unlabeled data. Each model focuses on the\ncorresponding single domain and aims to yield an expertized domain-specific\nsegmentation model. Through extensive experiments on retinal vessel\nsegmentation, our framework achieves Dice scores close to target-trained oracle\nboth from OCTA to OCT and from OCT to OCTA, significantly outperforming other\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1\">Linkai Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Ziqi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Style Image Super-Resolution using Conditional Objective. (arXiv:2201.04898v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04898","description":"<p>Recent studies have significantly enhanced the performance of single-image\nsuper-resolution (SR) using convolutional neural networks (CNNs). While there\ncan be many high-resolution (HR) solutions for a given input, most existing\nCNN-based methods do not explore alternative solutions during the inference. A\ntypical approach to obtaining alternative SR results is to train multiple SR\nmodels with different loss weightings and exploit the combination of these\nmodels. Instead of using multiple models, we present a more efficient method to\ntrain a single adjustable SR model on various combinations of losses by taking\nadvantage of multi-task learning. Specifically, we optimize an SR model with a\nconditional objective during training, where the objective is a weighted sum of\nmultiple perceptual losses at different feature levels. The weights vary\naccording to given conditions, and the set of weights is defined as a style\ncontroller. Also, we present an architecture appropriate for this training\nscheme, which is the Residual-in-Residual Dense Block equipped with spatial\nfeature transformation layers. At the inference phase, our trained model can\ngenerate locally different outputs conditioned on the style control map.\nExtensive experiments show that the proposed SR model produces various\ndesirable reconstructions without artifacts and yields comparable quantitative\nperformance to state-of-the-art SR methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seung Ho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_Y/0/1/0/all/0/1\">Young Su Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers. (arXiv:2201.05047v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05047","description":"<p>Detection Transformer (DETR) and Deformable DETR have been proposed to\neliminate the need for many hand-designed components in object detection while\ndemonstrating good performance as previous complex hand-crafted detectors.\nHowever, their performance on Video Object Detection (VOD) has not been well\nexplored. In this paper, we present TransVOD, the first end-to-end video object\ndetection system based on spatial-temporal Transformer architectures. The first\ngoal of this paper is to streamline the pipeline of VOD, effectively removing\nthe need for many hand-crafted components for feature aggregation, e.g.,\noptical flow model, relation networks. Besides, benefited from the object query\ndesign in DETR, our method does not need complicated post-processing methods\nsuch as Seq-NMS. In particular, we present a temporal Transformer to aggregate\nboth the spatial object queries and the feature memories of each frame. Our\ntemporal transformer consists of two components: Temporal Query Encoder (TQE)\nto fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to\nobtain current frame detection results. These designs boost the strong baseline\ndeformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID\ndataset. Then, we present two improved versions of TransVOD including\nTransVOD++ and TransVOD Lite. The former fuses object-level information into\nobject query via dynamic convolution while the latter models the entire video\nclips as the output to speed up the inference time. We give detailed analysis\nof all three models in the experiment part. In particular, our proposed\nTransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet\nVID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and\naccuracy trade-off with 83.7% mAP while running at around 30 FPS on a single\nV100 GPU device. Code and models will be available for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiangtai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}