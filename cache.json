{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Extracting linguistic speech patterns of Japanese fictional characters using subword units. (arXiv:2203.02632v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02632","description":"<p>This study extracted and analyzed the linguistic speech patterns that\ncharacterize Japanese anime or game characters. Conventional morphological\nanalyzers, such as MeCab, segment words with high performance, but they are\nunable to segment broken expressions or utterance endings that are not listed\nin the dictionary, which often appears in lines of anime or game characters. To\novercome this challenge, we propose segmenting lines of Japanese anime or game\ncharacters using subword units that were proposed mainly for deep learning, and\nextracting frequently occurring strings to obtain expressions that characterize\ntheir utterances. We analyzed the subword units weighted by TF/IDF according to\ngender, age, and each anime character and show that they are linguistic speech\npatterns that are specific for each feature. Additionally, a classification\nexperiment shows that the model with subword units outperformed that with the\nconventional method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kishino_M/0/1/0/all/0/1\">Mika Kishino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komiya_K/0/1/0/all/0/1\">Kanako Komiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unfreeze with Care: Space-Efficient Fine-Tuning of Semantic Parsing Models. (arXiv:2203.02652v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02652","description":"<p>Semantic parsing is a key NLP task that maps natural language to structured\nmeaning representations. As in many other NLP tasks, SOTA performance in\nsemantic parsing is now attained by fine-tuning a large pretrained language\nmodel (PLM). While effective, this approach is inefficient in the presence of\nmultiple downstream tasks, as a new set of values for all parameters of the PLM\nneeds to be stored for each task separately. Recent work has explored methods\nfor adapting PLMs to downstream tasks while keeping most (or all) of their\nparameters frozen. We examine two such promising techniques, prefix tuning and\nbias-term tuning, specifically on semantic parsing. We compare them against\neach other on two different semantic parsing datasets, and we also compare them\nagainst full and partial fine-tuning, both in few-shot and conventional data\nsettings. While prefix tuning is shown to do poorly for semantic parsing tasks\noff the shelf, we modify it by adding special token embeddings, which results\nin very strong performance without compromising parameter savings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Haidar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesnards_N/0/1/0/all/0/1\">Nicolas Guenon des Mesnards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubino_M/0/1/0/all/0/1\">Melanie Rubino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02668","description":"<p>It has been widely known that CAM (Class Activation Map) usually only\nactivates discriminative object regions and falsely includes lots of\nobject-related backgrounds. As only a fixed set of image-level object labels\nare available to the WSSS (weakly supervised semantic segmentation) model, it\ncould be very difficult to suppress those diverse background regions consisting\nof open set objects. In this paper, we propose a novel Cross Language Image\nMatching (CLIMS) framework, based on the recently introduced Contrastive\nLanguage-Image Pre-training (CLIP) model, for WSSS. The core idea of our\nframework is to introduce natural language supervision to activate more\ncomplete object regions and suppress closely-related open background regions.\nIn particular, we design object, background region and text label matching\nlosses to guide the model to excite more reasonable object regions for CAM of\neach category. In addition, we design a co-occurring background suppression\nloss to prevent the model from activating closely-related background regions,\nwith a predefined set of class-related background text descriptions. These\ndesigns enable the proposed CLIMS to generate a more complete and compact\nactivation map for the target objects. Extensive experiments on PASCAL VOC2012\ndataset show that our CLIMS significantly outperforms the previous\nstate-of-the-art methods. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xianxu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralDPS: Neural Deterministic Plus Stochastic Model with Multiband Excitation for Noise-Controllable Waveform Generation. (arXiv:2203.02678v1 [cs.SD])","link":"http://arxiv.org/abs/2203.02678","description":"<p>The traditional vocoders have the advantages of high synthesis efficiency,\nstrong interpretability, and speech editability, while the neural vocoders have\nthe advantage of high synthesis quality. To combine the advantages of two\nvocoders, inspired by the traditional deterministic plus stochastic model, this\npaper proposes a novel neural vocoder named NeuralDPS which can retain high\nspeech quality and acquire high synthesis efficiency and noise controllability.\nFirstly, this framework contains four modules: a deterministic source module, a\nstochastic source module, a neural V/UV decision module and a neural filter\nmodule. The input required by the vocoder is just the spectral parameter, which\navoids the error caused by estimating additional parameters, such as F0.\nSecondly, to solve the problem that different frequency bands may have\ndifferent proportions of deterministic components and stochastic components, a\nmultiband excitation strategy is used to generate a more accurate excitation\nsignal and reduce the neural filter's burden. Thirdly, a method to control\nnoise components of speech is proposed. In this way, the signal-to-noise ratio\n(SNR) of speech can be adjusted easily. Objective and subjective experimental\nresults show that our proposed NeuralDPS vocoder can obtain similar performance\nwith the WaveNet and it generates waveforms at least 280 times faster than the\nWaveNet vocoder. It is also 28% faster than WaveGAN's synthesis efficiency on a\nsingle CPU core. We have also verified through experiments that this method can\neffectively control the noise components in the predicted speech and adjust the\nSNR of speech. Examples of generated speech can be found at\nhttps://hairuo55.github.io/NeuralDPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Ruibo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhengqi Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Rank: Rethinking Evaluation with Word and Sentence Similarities. (arXiv:2203.02679v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02679","description":"<p>Word and sentence embeddings are useful feature representations in natural\nlanguage processing. However, intrinsic evaluation for embeddings lags far\nbehind, and there has been no significant update since the past decade. Word\nand sentence similarity tasks have become the de facto evaluation method. It\nleads models to overfit to such evaluations, negatively impacting embedding\nmodels' development. This paper first points out the problems using semantic\nsimilarity as the gold standard for word and sentence embedding evaluations.\nFurther, we propose a new intrinsic evaluation method called EvalRank, which\nshows a much stronger correlation with downstream tasks. Extensive experiments\nare conducted based on 60+ models and popular datasets to certify our\njudgments. Finally, the practical evaluation toolkit is released for future\nbenchmarking purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Proof is in the Pudding: Using Automated Theorem Proving to Generate Cooking Recipes. (arXiv:2203.02683v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02683","description":"<p>This paper presents FASTFOOD, a rule-based Natural Language Generation\nProgram for cooking recipes. Recipes are generated by using an Automated\nTheorem Proving procedure to select the ingredients and instructions, with\ningredients corresponding to axioms and instructions to implications. FASTFOOD\nalso contains a temporal optimization module which can rearrange the recipe to\nmake it more time-efficient for the user, e.g. the recipe specifies to chop the\nvegetables while the rice is boiling. The system is described in detail, using\na framework which divides Natural Language Generation into 4 phases: content\nproduction, content selection, content organisation and content realisation. A\ncomparison is then made with similar existing systems and techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahon_L/0/1/0/all/0/1\">Louis Mahon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1\">Carl Vogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Representation Learning for Continual Relation Extraction. (arXiv:2203.02721v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02721","description":"<p>Continual relation extraction (CRE) aims to continuously train a model on\ndata with new relations while avoiding forgetting old ones. Some previous work\nhas proved that storing a few typical samples of old relations and replaying\nthem when learning new relations can effectively avoid forgetting. However,\nthese memory-based methods tend to overfit the memory samples and perform\npoorly on imbalanced datasets. To solve these challenges, a consistent\nrepresentation learning method is proposed, which maintains the stability of\nthe relation embedding by adopting contrastive learning and knowledge\ndistillation when replaying memory. Specifically, supervised contrastive\nlearning based on a memory bank is first used to train each new task so that\nthe model can effectively learn the relation representation. Then, contrastive\nreplay is conducted of the samples in memory and makes the model retain the\nknowledge of historical relations through memory knowledge distillation to\nprevent the catastrophic forgetting of the old task. The proposed method can\nbetter learn consistent representations to alleviate forgetting effectively.\nExtensive experiments on FewRel and TACRED datasets show that our method\nsignificantly outperforms state-of-the-art baselines and yield strong\nrobustness on the imbalanced dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiangong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kai Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Differential Privacy on Group Disparity Mitigation. (arXiv:2203.02745v1 [cs.CR])","link":"http://arxiv.org/abs/2203.02745","description":"<p>The performance cost of differential privacy has, for some applications, been\nshown to be higher for minority groups; fairness, conversely, has been shown to\ndisproportionally compromise the privacy of members of such groups. Most work\nin this area has been restricted to computer vision and risk assessment. In\nthis paper, we evaluate the impact of differential privacy on fairness across\nfour tasks, focusing on how attempts to mitigate privacy violations and\nbetween-group performance differences interact: Does privacy inhibit attempts\nto ensure fairness? To this end, we train $(\\varepsilon,\\delta)$-differentially\nprivate models with empirical risk minimization and group distributionally\nrobust training objectives. Consistent with previous findings, we find that\ndifferential privacy increases between-group performance differences in the\nbaseline setting; but more interestingly, differential privacy reduces\nbetween-group performance differences in the robust setting. We explain this by\nreinterpreting differential privacy as regularization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_V/0/1/0/all/0/1\">Victor Petr&#xe9;n Bach Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neerkaje_A/0/1/0/all/0/1\">Atula Tejaswi Neerkaje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawhney_R/0/1/0/all/0/1\">Ramit Sawhney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feeding What You Need by Understanding What You Learned. (arXiv:2203.02753v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02753","description":"<p>Machine Reading Comprehension (MRC) reveals the ability to understand a given\ntext passage and answer questions based on it. Existing research works in MRC\nrely heavily on large-size models and corpus to improve the performance\nevaluated by metrics such as Exact Match ($EM$) and $F_1$. However, such a\nparadigm lacks sufficient interpretation to model capability and can not\nefficiently train a model with a large corpus. In this paper, we argue that a\ndeep understanding of model capabilities and data properties can help us feed a\nmodel with appropriate training data based on its learning status.\nSpecifically, we design an MRC capability assessment framework that assesses\nmodel capabilities in an explainable and multi-dimensional manner. Based on it,\nwe further uncover and disentangle the connections between various data\nproperties and model performance. Finally, to verify the effectiveness of the\nproposed MRC capability assessment framework, we incorporate it into a\ncurriculum learning pipeline and devise a Capability Boundary Breakthrough\nCurriculum (CBBC) strategy, which performs a model capability-based training to\nmaximize the data value and improve training efficiency. Extensive experiments\ndemonstrate that our approach significantly improves performance, achieving up\nto an 11.22% / 8.71% improvement of $EM$ / $F_1$ on MRC tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation. (arXiv:2203.02764v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02764","description":"<p>Most existing works in vision-and-language navigation (VLN) focus on either\ndiscrete or continuous environments, training agents that cannot generalize\nacross the two. The fundamental difference between the two setups is that\ndiscrete navigation assumes prior knowledge of the connectivity graph of the\nenvironment, so that the agent can effectively transfer the problem of\nnavigation with low-level controls to jumping from node to node with high-level\nactions by grounding to an image of a navigable direction. To bridge the\ndiscrete-to-continuous gap, we propose a predictor to generate a set of\ncandidate waypoints during navigation, so that agents designed with high-level\nactions can be transferred to and trained in continuous environments. We refine\nthe connectivity graph of Matterport3D to fit the continuous\nHabitat-Matterport3D, and train the waypoints predictor with the refined graphs\nto produce accessible waypoints at each time step. Moreover, we demonstrate\nthat the predicted waypoints can be augmented during training to diversify the\nviews and paths, and therefore enhance agent's generalization ability. Through\nextensive experiments we show that agents navigating in continuous environments\nwith predicted waypoints perform significantly better than agents using\nlow-level actions, which reduces the absolute discrete-to-continuous gap by\n11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent\nand 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple\nimitation learning objective, outperform previous methods by a large margin,\nachieving new state-of-the-art results on the testing environments of the\nR2R-CE and the RxR-CE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CptGraphSum: Let key clues guide the cross-lingual abstractive summarization. (arXiv:2203.02797v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02797","description":"<p>Cross-Lingual Summarization (CLS) is the task to generate a summary in one\nlanguage for an article in a different language. Previous studies on CLS mainly\ntake pipeline methods or train the end-to-end model using the translated\nparallel data. However, the quality of generated cross-lingual summaries needs\nmore further efforts to improve, and the model performance has never been\nevaluated on the hand-written CLS dataset. Therefore, we first propose a\nclue-guided cross-lingual abstractive summarization method to improve the\nquality of cross-lingual summaries, and then construct a novel hand-written CLS\ndataset for evaluation. Specifically, we extract keywords, named entities, etc.\nof the input article as key clues for summarization and then design a\nclue-guided algorithm to transform an article into a graph with less noisy\nsentences. One Graph encoder is built to learn sentence semantics and article\nstructures and one Clue encoder is built to encode and translate key clues,\nensuring the information of important parts are reserved in the generated\nsummary. These two encoders are connected by one decoder to directly learn\ncross-lingual semantics. Experimental results show that our method has stronger\nrobustness for longer inputs and substantially improves the performance over\nthe strong baseline, achieving an improvement of 8.55 ROUGE-1\n(English-to-Chinese summarization) and 2.13 MoverScore (Chinese-to-English\nsummarization) scores over the existing SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Dengbiao Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haizhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation. (arXiv:2203.02889v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02889","description":"<p>Label smoothing and vocabulary sharing are two widely used techniques in\nneural machine translation models. However, we argue that simply applying both\ntechniques can be conflicting and even leads to sub-optimal performance. When\nallocating smoothed probability, original label smoothing treats the\nsource-side words that would never appear in the target language equally to the\nreal target-side words, which could bias the translation model. To address this\nissue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the\nsoft label probability of source-side words to zero. Simple yet effective, MLS\nmanages to better integrate label smoothing with vocabulary sharing. Our\nextensive experiments show that MLS consistently yields improvement over\noriginal label smoothing on different datasets, including bilingual and\nmultilingual translation from both translation quality and model's calibration.\nOur code is released at https://github.com/PKUnlp-icler/MLS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization. (arXiv:2203.02894v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02894","description":"<p>Multi-document summarization (MDS) has made significant progress in recent\nyears, in part facilitated by the availability of new, dedicated datasets and\ncapacious language models. However, a standing limitation of these models is\nthat they are trained against limited references and with plain\nmaximum-likelihood objectives. As for many other generative tasks,\nreinforcement learning (RL) offers the potential to improve the training of MDS\nmodels; yet, it requires a carefully-designed reward that can ensure\nappropriate leverage of both the reference summaries and the input documents.\nFor this reason, in this paper we propose fine-tuning an MDS baseline with a\nreward that balances a reference-based metric such as ROUGE with coverage of\nthe input documents. To implement the approach, we utilize RELAX (Grathwohl et\nal., 2018), a contemporary gradient estimator which is both low-variance and\nunbiased, and we fine-tune the baseline in a few-shot style for both stability\nand computational efficiency. Experimental results over the Multi-News and WCEP\nMDS datasets show significant improvements of up to +0.95 pp average ROUGE\nscore and +3.17 pp METEOR score over the baseline, and competitive results with\nthe literature. In addition, they show that the coverage of the input documents\nis increased, and evenly across all documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parnell_J/0/1/0/all/0/1\">Jacob Parnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unanue_I/0/1/0/all/0/1\">Inigo Jauregi Unanue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccardi_M/0/1/0/all/0/1\">Massimo Piccardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents. (arXiv:2203.02898v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02898","description":"<p>Text semantic matching is a fundamental task that has been widely used in\nvarious scenarios, such as community question answering, information retrieval,\nand recommendation. Most state-of-the-art matching models, e.g., BERT, directly\nperform text comparison by processing each word uniformly. However, a query\nsentence generally comprises content that calls for different levels of\nmatching granularity. Specifically, keywords represent factual information such\nas action, entity, and event that should be strictly matched, while intents\nconvey abstract concepts and ideas that can be paraphrased into various\nexpressions. In this work, we propose a simple yet effective training strategy\nfor text semantic matching in a divide-and-conquer manner by disentangling\nkeywords from intents. Our approach can be easily combined with pre-trained\nlanguage models (PLM) without influencing their inference efficiency, achieving\nstable performance improvements against a wide range of PLMs on three\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Meng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haixiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daniel Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Network Enhanced Language Models for Efficient Multilingual Text Classification. (arXiv:2203.02912v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02912","description":"<p>Online social media works as a source of various valuable and actionable\ninformation during disasters. These information might be available in multiple\nlanguages due to the nature of user generated content. An effective system to\nautomatically identify and categorize these actionable information should be\ncapable to handle multiple languages and under limited supervision. However,\nexisting works mostly focus on English language only with the assumption that\nsufficient labeled data is available. To overcome these challenges, we propose\na multilingual disaster related text classification system which is capable to\nwork under \\{mono, cross and multi\\} lingual scenarios and under limited\nsupervision. Our end-to-end trainable framework combines the versatility of\ngraph neural networks, by applying over the corpus, with the power of\ntransformer based large language models, over examples, with the help of\ncross-attention between the two. We evaluate our framework over total nine\nEnglish, Non-English and monolingual datasets in \\{mono, cross and multi\\}\nlingual classification scenarios. Our framework outperforms state-of-the-art\nmodels in disaster domain and multilingual BERT baseline in terms of Weighted\nF$_1$ score. We also show the generalizability of the proposed model under\nlimited supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Samujjwal Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhadeep Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doctor Recommendation in Online Health Forums via Expertise Learning. (arXiv:2203.02932v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02932","description":"<p>Huge volumes of patient queries are daily generated on online health forums,\nrendering manual doctor allocation a labor-intensive task. To better help\npatients, this paper studies a novel task of doctor recommendation to enable\nautomatic pairing of a patient to a doctor with relevant expertise. While most\nprior work in recommendation focuses on modeling target users from their past\nbehavior, we can only rely on the limited words in a query to infer a patient's\nneeds for privacy reasons. For doctor modeling, we study the joint effects of\ntheir profiles and previous dialogues with other patients and explore their\ninteractions via self-learning. The learned doctor embeddings are further\nemployed to estimate their capabilities of handling a patient query with a\nmulti-head attention mechanism. For experiments, a large-scale dataset is\ncollected from Chunyu Yisheng, a Chinese online health forum, where our model\nexhibits the state-of-the-art results, outperforming baselines only consider\nprofiles and past dialogues to characterize a doctor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoxin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation. (arXiv:2203.02951v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02951","description":"<p>Token-level adaptive training approaches can alleviate the token imbalance\nproblem and thus improve neural machine translation, through re-weighting the\nlosses of different target tokens based on specific statistical metrics (e.g.,\ntoken frequency or mutual information). Given that standard translation models\nmake predictions on the condition of previous target contexts, we argue that\nthe above statistical metrics ignore target context information and may assign\ninappropriate weights to target tokens. While one possible solution is to\ndirectly take target contexts into these statistical metrics, the\ntarget-context-aware statistical computing is extremely expensive, and the\ncorresponding storage overhead is unrealistic. To solve the above issues, we\npropose a target-context-aware metric, named conditional bilingual mutual\ninformation (CBMI), which makes it feasible to supplement target context\ninformation for statistical metrics. Particularly, our CBMI can be formalized\nas the log quotient of the translation model probability and language model\nprobability by decomposing the conditional joint distribution. Thus CBMI can be\nefficiently calculated during model training without any pre-specific\nstatistical calculations and large storage overhead. Furthermore, we propose an\neffective adaptive training approach based on both the token- and\nsentence-level CBMI. Experimental results on WMT14 English-German and WMT19\nChinese-English tasks show our approach can significantly outperform the\nTransformer baseline and other related methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter Dataset for 2022 Russo-Ukrainian Crisis. (arXiv:2203.02955v1 [cs.SI])","link":"http://arxiv.org/abs/2203.02955","description":"<p>Online Social Networks (OSNs) play a significant role in information sharing\nduring a crisis. The data collected during such a crisis can reflect the large\nscale public opinions and sentiment. In addition, OSN data can also be used to\nstudy different campaigns that are employed by various entities to engineer\npublic opinions. Such information sharing campaigns can range from spreading\nfactual information to propaganda and misinformation. We provide a Twitter\ndataset of the 2022 Russo-Ukrainian conflict. In the first release, we share\nover 1.6 million tweets shared during the 1st week of the crisis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haq_E/0/1/0/all/0/1\">Ehsan-Ul Haq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyson_G/0/1/0/all/0/1\">Gareth Tyson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lik-Hang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braud_T/0/1/0/all/0/1\">Tristan Braud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1\">Pan Hui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for Temporal Sentence Grounding. (arXiv:2203.02966v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02966","description":"<p>Temporal sentence grounding aims to localize a target segment in an untrimmed\nvideo semantically according to a given sentence query. Most previous works\nfocus on learning frame-level features of each whole frame in the entire video,\nand directly match them with the textual information. Such frame-level feature\nextraction leads to the obstacles of these methods in distinguishing ambiguous\nvideo frames with complicated contents and subtle appearance differences, thus\nlimiting their performance. In order to differentiate fine-grained appearance\nsimilarities among consecutive frames, some state-of-the-art methods\nadditionally employ a detection model like Faster R-CNN to obtain detailed\nobject-level features in each frame for filtering out the redundant background\ncontents. However, these methods suffer from missing motion analysis since the\nobject detection module in Faster R-CNN lacks temporal modeling. To alleviate\nthe above limitations, in this paper, we propose a novel Motion- and\nAppearance-guided 3D Semantic Reasoning Network (MA3SRN), which incorporates\noptical-flow-guided motion-aware, detection-based appearance-aware, and\n3D-aware object-level features to better reason the spatial-temporal object\nrelations for accurately modelling the activity among consecutive frames.\nSpecifically, we first develop three individual branches for motion,\nappearance, and 3D encoding separately to learn fine-grained motion-guided,\nappearance-guided, and 3D-aware object features, respectively. Then, both\nmotion and appearance information from corresponding branches are associated to\nenhance the 3D-aware features for the final precise grounding. Extensive\nexperiments on three challenging datasets (ActivityNet Caption, Charades-STA\nand TACoS) demonstrate that the proposed MA3SRN model achieves a new\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Implicit Discourse Relation Recognition. (arXiv:2203.02982v1 [cs.CL])","link":"http://arxiv.org/abs/2203.02982","description":"<p>A discourse containing one or more sentences describes daily issues and\nevents for people to communicate their thoughts and opinions. As sentences are\nnormally consist of multiple text segments, correct understanding of the theme\nof a discourse should take into consideration of the relations in between text\nsegments. Although sometimes a connective exists in raw texts for conveying\nrelations, it is more often the cases that no connective exists in between two\ntext segments but some implicit relation does exist in between them. The task\nof implicit discourse relation recognition (IDRR) is to detect implicit\nrelation and classify its sense between two text segments without a connective.\nIndeed, the IDRR task is important to diverse downstream natural language\nprocessing tasks, such as text summarization, machine translation and so on.\nThis article provides a comprehensive and up-to-date survey for the IDRR task.\nWe first summarize the task definition and data sources widely used in the\nfield. We categorize the main solution approaches for the IDRR task from the\nviewpoint of its development history. In each solution category, we present and\nanalyze the most representative methods, including their origins, ideas,\nstrengths and weaknesses. We also present performance comparisons for those\nsolutions experimented on a public corpus with standard data processing\nprocedures. Finally, we discuss future research directions for discourse\nrelation analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Key-value Memory Enhanced Multi-step Graph Reasoning for Knowledge-based Visual Question Answering. (arXiv:2203.02985v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02985","description":"<p>Knowledge-based visual question answering (VQA) is a vision-language task\nthat requires an agent to correctly answer image-related questions using\nknowledge that is not presented in the given image. It is not only a more\nchallenging task than regular VQA but also a vital step towards building a\ngeneral VQA system. Most existing knowledge-based VQA systems process knowledge\nand image information similarly and ignore the fact that the knowledge base\n(KB) contains complete information about a triplet, while the extracted image\ninformation might be incomplete as the relations between two objects are\nmissing or wrongly detected. In this paper, we propose a novel model named\ndynamic knowledge memory enhanced multi-step graph reasoning (DMMGR), which\nperforms explicit and implicit reasoning over a key-value knowledge memory\nmodule and a spatial-aware image graph, respectively. Specifically, the memory\nmodule learns a dynamic knowledge representation and generates a\nknowledge-aware question representation at each reasoning step. Then, this\nrepresentation is used to guide a graph attention operator over the\nspatial-aware image graph. Our model achieves new state-of-the-art accuracy on\nthe KRVQR and FVQA datasets. We also conduct ablation experiments to prove the\neffectiveness of each component of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Coreference Relations in Visual Dialog. (arXiv:2203.02986v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02986","description":"<p>Visual dialog is a vision-language task where an agent needs to answer a\nseries of questions grounded in an image based on the understanding of the\ndialog history and the image. The occurrences of coreference relations in the\ndialog makes it a more challenging task than visual question-answering. Most\nprevious works have focused on learning better multi-modal representations or\non exploring different ways of fusing visual and language features, while the\ncoreferences in the dialog are mainly ignored. In this paper, based on\nlinguistic knowledge and discourse features of human dialog we propose two soft\nconstraints that can improve the model's ability of resolving coreferences in\ndialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset\nshows that our model, which integrates two novel and linguistically inspired\nsoft constraints in a deep transformer neural architecture, obtains new\nstate-of-the-art performance in terms of recall at 1 and other evaluation\nmetrics compared to current existing models and this without pretraining on\nother vision-language datasets. Our qualitative results also demonstrate the\neffectiveness of the method that we propose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in Neural Text Generation: A Task-Agnostic Survey. (arXiv:2203.03047v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03047","description":"<p>In recent years much effort has been devoted to applying neural models to the\ntask of natural language generation. The challenge is to generate natural\nhuman-like text, and to control the generation process. This paper presents a\ntask-agnostic survey of recent advances in neural text generation. These\nadvances have been achieved by numerous developments, which we group under the\nfollowing four headings: data construction, neural frameworks, training and\ninference strategies, and evaluation metrics. Finally we discuss the future\ndirections for the development of neural text generation including neural\npipelines and exploiting back-ground knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_F/0/1/0/all/0/1\">Frank Guerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yucheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leashing the Inner Demons: Self-Detoxification for Language Models. (arXiv:2203.03072v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03072","description":"<p>Language models (LMs) can reproduce (or amplify) toxic language seen during\ntraining, which poses a risk to their practical application. In this paper, we\nconduct extensive experiments to study this phenomenon. We analyze the impact\nof prompts, decoding strategies and training corpora on the output toxicity.\nBased on our findings, we propose a simple yet effective method for language\nmodels to \"detoxify\" themselves without an additional large corpus or external\ndiscriminator. Compared to a supervised baseline, our proposed method shows\nbetter toxicity reduction with good generation quality in the generated content\nunder multiple settings. Warning: some examples shown in the paper may contain\nuncensored offensive content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhankui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ILDAE: Instance-Level Difficulty Analysis of Evaluation Data. (arXiv:2203.03073v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03073","description":"<p>Knowledge of questions' difficulty level helps a teacher in several ways,\nsuch as estimating students' potential quickly by asking carefully selected\nquestions and improving quality of examination by modifying trivial and hard\nquestions. Can we extract such benefits of instance difficulty in NLP? To this\nend, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE)\nin a large-scale setup of 23 datasets and demonstrate its five novel\napplications: 1) conducting efficient-yet-accurate evaluations with fewer\ninstances saving computational cost and time, 2) improving quality of existing\nevaluation datasets by repairing erroneous and trivial instances, 3) selecting\nthe best model based on application requirements, 4) analyzing dataset\ncharacteristics for guiding future data creation, 5) estimating Out-of-Domain\nperformance reliably. Comprehensive experiments for these applications result\nin several interesting findings, such as evaluation using just 5% instances\n(selected via ILDAE) achieves as high as 0.93 Kendall correlation with\nevaluation using complete dataset and computing weighted accuracy using\ndifficulty scores leads to 5.2% higher correlation with Out-of-Domain\nperformance. We release the difficulty scores and hope our analyses and\nfindings will bring more attention to this important yet understudied field of\nleveraging instance difficulty in evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking. (arXiv:2203.03123v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03123","description":"<p>Dialogue state tracking (DST) aims to extract essential information from\nmulti-turn dialogue situations and take appropriate actions. A belief state,\none of the core pieces of information, refers to the subject and its specific\ncontent, and appears in the form of \\texttt{domain-slot-value}. The trained\nmodel predicts \"accumulated\" belief states in every turn, and joint goal\naccuracy and slot accuracy are mainly used to evaluate the prediction; however,\nwe specify that the current evaluation metrics have a critical limitation when\nevaluating belief states accumulated as the dialogue proceeds, especially in\nthe most used MultiWOZ dataset. Additionally, we propose \\textbf{relative slot\naccuracy} to complement existing metrics. Relative slot accuracy does not\ndepend on the number of predefined slots, and allows intuitive evaluation by\nassigning relative scores according to the turn of each dialogue. This study\nalso encourages not solely the reporting of joint goal accuracy, but also\nvarious complementary metrics in DST tasks for the sake of a realistic\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Takyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hoonsang Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yukyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_P/0/1/0/all/0/1\">Pilsung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Misuk Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models. (arXiv:2203.03131v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03131","description":"<p>Recently the prompt-tuning paradigm has attracted significant attention. By\nonly tuning continuous prompts with a frozen pre-trained language model (PLM),\nprompt-tuning takes a step towards deploying a shared frozen PLM to serve\nnumerous downstream tasks. Although prompt-tuning shows good performance on\ncertain natural language understanding (NLU) tasks, its effectiveness on\nnatural language generation (NLG) tasks is still under-explored. In this paper,\nwe argue that one of the factors hindering the development of prompt-tuning on\nNLG tasks is the unfamiliar inputs (i.e., inputs are linguistically different\nfrom the pretraining corpus). For example, our preliminary exploration reveals\na large performance gap between prompt-tuning and fine-tuning when unfamiliar\ninputs occur frequently in NLG tasks. This motivates us to propose\ninput-tuning, which fine-tunes both the continuous prompts and the input\nrepresentations, leading to a more effective way to adapt unfamiliar inputs to\nfrozen PLMs. Our proposed input-tuning is conceptually simple and empirically\npowerful. Experimental results on seven NLG tasks demonstrate that input-tuning\nis significantly and consistently better than prompt-tuning. Furthermore, on\nthree of these tasks, input-tuning can achieve a comparable or even better\nperformance than fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shengnan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zeqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Online Dialogue Response Generation. (arXiv:2203.03168v1 [cs.CL])","link":"http://arxiv.org/abs/2203.03168","description":"<p>Although pre-trained sequence-to-sequence models have achieved great success\nin dialogue response generation, chatbots still suffer from generating\ninconsistent responses in real-world practice, especially in multi-turn\nsettings. We argue that this can be caused by a discrepancy between training\nand real-world testing. At training time, chatbot generates the response with\nthe golden context, while it has to generate based on the context consisting of\nboth user utterances and the model predicted utterances during real-world\ntesting. With the growth of the number of utterances, this discrepancy becomes\nmore serious in the multi-turn settings. In this paper, we propose a\nhierarchical sampling-based method consisting of both utterance-level sampling\nand semi-utterance-level sampling, to alleviate the discrepancy, which\nimplicitly increases the dialogue coherence. We further adopt reinforcement\nlearning and re-ranking methods to explicitly optimize the dialogue coherence\nduring training and inference, respectively. Empirical experiments show the\neffectiveness of the proposed methods for improving the robustness of chatbots\nin real practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A theory of interaction semantics. (arXiv:2007.06258v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2007.06258","description":"<p>The aim of this article is to delineate a theory of interaction semantics and\nthereby provide a proper understanding of the \"meaning\" of the exchanged\ncharacters within an interaction. The key idea is to approach the semantics of\nan interaction as we do for a formal language. This approach consists of two\nsteps: first to assign values to variables and second to provide meaning by an\ninterpretation function. A natural choice for the variables are the state\nfunctions of the interacting systems, assigning values at each time step.\nThereby the description of a system's behaviour with an input/output-transition\nsystem (I/O-TS) becomes a representation of the variable-to-value\nassignments.To identify the interpretation function I propose to model the\ninteraction of systems based on Shannon's theory of information with the\nprotocol concept, complemented by decisions to form a \"game in interactive form\n(GIF)\". Decisions in this sense determine the transition relation and thereby\ncreate a transition function. Then the natural choice for the interpretation\nfunction is the transition function of the GIF. In this sense, the\ninterpretation of the interaction becomes its execution. Now we can say that\nthe interpretation of the characters during the GIF's execution results in\ntheir meaning, the result of the mapping. Equivalent meaning is based on\nresulting equivalent states of the GIF. Based on the decisions we can partition\nany GIF into a deterministic traditional automaton where the states represent\nequivalance classes of GIF-states related to a single decision. Except for the\nutility function, this automaton is equivalent to a traditional game in\nextensive form. Thus, traditional game theory actually abstracts from\ninteractions and deals with the meaning of decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reich_J/0/1/0/all/0/1\">Johannes Reich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsifying Transformer Models with Trainable Representation Pooling. (arXiv:2009.05169v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.05169","description":"<p>We propose a novel method to sparsify attention in the Transformer model by\nlearning to select the most-informative token representations during the\ntraining process, thus focusing on the task-specific parts of an input. A\nreduction of quadratic time and memory complexity to sublinear was achieved due\nto a robust trainable top-$k$ operator. Our experiments on a challenging long\ndocument summarization task show that even our simple baseline performs\ncomparably to the current SOTA, and with trainable pooling, we can retain its\ntop quality, while being $1.8\\times$ faster during training, $4.5\\times$ faster\nduring inference, and up to $13\\times$ more computationally efficient in the\ndecoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pietruszka_M/0/1/0/all/0/1\">Micha&#x142; Pietruszka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchmann_L/0/1/0/all/0/1\">&#x141;ukasz Borchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garncarek_L/0/1/0/all/0/1\">&#x141;ukasz Garncarek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Cross-lingual Semantic Parsing. (arXiv:2104.07554v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07554","description":"<p>Recent work in cross-lingual semantic parsing has successfully applied\nmachine translation to localize parsers to new languages. However, these\nadvances assume access to high-quality machine translation systems and word\nalignment tools. We remove these assumptions and study cross-lingual semantic\nparsing as a zero-shot problem, without parallel data (i.e., utterance-logical\nform pairs) for new languages. We propose a multi-task encoder-decoder model to\ntransfer parsing knowledge to additional languages using only English-logical\nform paired data and in-domain natural language corpora in each new language.\nOur model encourages language-agnostic encodings by jointly optimizing for\nlogical-form generation with auxiliary objectives designed for cross-lingual\nlatent representation alignment. Our parser performs significantly above\ntranslation-based baselines and, in some cases, competes with the supervised\nupper-bound.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sherborne_T/0/1/0/all/0/1\">Tom Sherborne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Parsing as MRC-based Span-Span Prediction. (arXiv:2105.07654v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07654","description":"<p>Higher-order methods for dependency parsing can partially but not fully\naddress the issue that edges in dependency trees should be constructed at the\ntext span/subtree level rather than word level. In this paper, we propose a new\nmethod for dependency parsing to address this issue. The proposed method\nconstructs dependency trees by directly modeling span-span (in other words,\nsubtree-subtree) relations. It consists of two modules: the {\\it text span\nproposal module} which proposes candidate text spans, each of which represents\na subtree in the dependency tree denoted by (root, start, end); and the {\\it\nspan linking module}, which constructs links between proposed spans. We use the\nmachine reading comprehension (MRC) framework as the backbone to formalize the\nspan linking module, where one span is used as a query to extract the text\nspan/subtree it should be linked to. The proposed method has the following\nmerits: (1) it addresses the fundamental problem that edges in a dependency\ntree should be constructed between subtrees; (2) the MRC framework allows the\nmethod to retrieve missing spans in the span proposal stage, which leads to\nhigher recall for eligible spans. Extensive experiments on the PTB, CTB and\nUniversal Dependencies (UD) benchmarks demonstrate the effectiveness of the\nproposed method. The code is available at\n\\url{https://github.com/ShannonAI/mrc-for-dependency-parsing}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Leilei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ByT5: Towards a token-free future with pre-trained byte-to-byte models. (arXiv:2105.13626v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.13626","description":"<p>Most widely-used pre-trained language models operate on sequences of tokens\ncorresponding to word or subword units. By comparison, token-free models that\noperate directly on raw text (bytes or characters) have many benefits: they can\nprocess text in any language out of the box, they are more robust to noise, and\nthey minimize technical debt by removing complex and error-prone text\npreprocessing pipelines. Since byte or character sequences are longer than\ntoken sequences, past work on token-free models has often introduced new model\narchitectures designed to amortize the cost of operating directly on raw text.\nIn this paper, we show that a standard Transformer architecture can be used\nwith minimal modifications to process byte sequences. We characterize the\ntrade-offs in terms of parameter count, training FLOPs, and inference speed,\nand show that byte-level models are competitive with their token-level\ncounterparts. We also demonstrate that byte-level models are significantly more\nrobust to noise and perform better on tasks that are sensitive to spelling and\npronunciation. As part of our contribution, we release a new set of pre-trained\nbyte-level Transformer models based on the T5 architecture, as well as all code\nand data used in our experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Linting Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barua_A/0/1/0/all/0/1\">Aditya Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1\">Mihir Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Nearest Neighbor Machine Translation. (arXiv:2105.14528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14528","description":"<p>Though nearest neighbor Machine Translation ($k$NN-MT)\n\\citep{khandelwal2020nearest} has proved to introduce significant performance\nboosts over standard neural MT systems, it is prohibitively slow since it uses\nthe entire reference corpus as the datastore for the nearest neighbor search.\nThis means each step for each beam in the beam search has to search over the\nentire reference corpus. $k$NN-MT is thus two-orders slower than vanilla MT\nmodels, making it hard to be applied to real-world applications, especially\nonline services. In this work, we propose Fast $k$NN-MT to address this issue.\nFast $k$NN-MT constructs a significantly smaller datastore for the nearest\nneighbor search: for each word in a source sentence, Fast $k$NN-MT first\nselects its nearest token-level neighbors, which is limited to tokens that are\nthe same as the query token. Then at each decoding step, in contrast to using\nthe entire corpus as the datastore, the search space is limited to target\ntokens corresponding to the previously selected reference source tokens. This\nstrategy avoids search through the whole datastore for nearest neighbors and\ndrastically improves decoding efficiency. Without loss of performance, Fast\n$k$NN-MT is two-orders faster than $k$NN-MT, and is only two times slower than\nthe standard NMT model. Fast $k$NN-MT enables the practical use of $k$NN-MT\nsystems in real-world MT applications. The code is available at\n\\url{https://github.com/ShannonAI/fast-knn-nmt}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiayu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08087","description":"<p>Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&amp;lang=en-us}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1\">Hui Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion. (arXiv:2106.08657v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08657","description":"<p>Document-level relation extraction (DocRE) aims to extract semantic relations\namong entity pairs in a document. Typical DocRE methods blindly take the full\ndocument as input, while a subset of the sentences in the document, noted as\nthe evidence, are often sufficient for humans to predict the relation of an\nentity pair. In this paper, we propose an evidence-enhanced framework, Eider,\nthat empowers DocRE by efficiently extracting evidence and effectively fusing\nthe extracted evidence in inference. We first jointly train an RE model with a\nlightweight evidence extraction model, which is efficient in both memory and\nruntime. Empirically, even training the evidence model on silver labels\nconstructed by our heuristic rules can lead to better RE performance. We\nfurther design a simple yet effective inference process that makes RE\npredictions on both extracted evidence and the full document, then fuses the\npredictions through a blending layer. This allows Eider to focus on important\nsentences while still having access to the complete information in the\ndocument. Extensive experiments show that Eider outperforms state-of-the-art\nmethods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning. (arXiv:2108.00578v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00578","description":"<p>Neural models command state-of-the-art performance across NLP tasks,\nincluding ones involving \"reasoning\". Models claiming to reason about the\nevidence presented to them should attend to the correct parts of the input\navoiding spurious patterns therein, be self-consistent in their predictions\nacross inputs, and be immune to biases derived from their pre-training in a\nnuanced, context-sensitive fashion. {\\em Do the prevalent *BERT-family of\nmodels do so?} In this paper, we study this question using the problem of\nreasoning on tabular data. Tabular inputs are especially well-suited for the\nstudy -- they admit systematic probes targeting the properties listed above.\nOur experiments demonstrate that a RoBERTa-based model, representative of the\ncurrent state-of-the-art, fails at reasoning on the following counts: it (a)\nignores relevant parts of the evidence, (b) is over-sensitive to annotation\nartifacts, and (c) relies on the knowledge encoded in the pre-trained language\nmodel rather than the evidence presented in its tabular inputs. Finally,\nthrough inoculation experiments, we show that fine-tuning the model on\nperturbed data does not help it overcome the above challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Riyaz A. Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1\">Atreya Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1\">Manish Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuning Pretrained Language Models With Label Attention for Biomedical Text Classification. (arXiv:2108.11809v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11809","description":"<p>The massive scale and growth of textual biomedical data have made its\nindexing and classification increasingly important. However, existing research\non this topic mainly utilized convolutional and recurrent neural networks,\nwhich generally achieve inferior performance than the novel transformers. On\nthe other hand, systems that apply transformers only focus on the target\ndocuments, overlooking the rich semantic information that label descriptions\ncontain. To address this gap, we develop a transformer-based biomedical text\nclassifier that considers label information. The system achieves this with a\nlabel attention module incorporated into the fine-tuning process of pretrained\nlanguage models (PTMs). Our results on two public medical datasets show that\nthe proposed fine-tuning scheme outperforms the vanilla PTMs and\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bruce Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Grammar-Learning Trajectories of Neural Language Models. (arXiv:2109.06096v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06096","description":"<p>The learning trajectories of linguistic phenomena in humans provide insight\ninto linguistic representation, beyond what can be gleaned from inspecting the\nbehavior of an adult speaker. To apply a similar approach to analyze neural\nlanguage models (NLM), it is first necessary to establish that different models\nare similar enough in the generalizations they make. In this paper, we show\nthat NLMs with different initialization, architecture, and training data\nacquire linguistic phenomena in a similar order, despite their different end\nperformance. These findings suggest that there is some mutual inductive bias\nthat underlies these models' learning of linguistic phenomena. Taking\ninspiration from psycholinguistics, we argue that studying this inductive bias\nis an opportunity to study the linguistic representation implicit in NLMs.\n</p>\n<p>Leveraging these findings, we compare the relative performance on different\nphenomena at varying learning stages with simpler reference models. Results\nsuggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find\nthe learning trajectory to be approximately one-dimensional: given an NLM with\na certain overall performance, it is possible to predict what linguistic\ngeneralizations it has already acquired. Initial analysis of these stages\npresents phenomena clusters (notably morphological ones), whose performance\nprogresses in unison, suggesting a potential link between the generalizations\nbehind them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hacohen_G/0/1/0/all/0/1\">Guy Hacohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1\">Daphna Weinshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSTL -- From English Requirements to Signal Temporal Logic. (arXiv:2109.10294v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10294","description":"<p>Formal methods provide very powerful tools and techniques for the design and\nanalysis of complex systems. Their practical application remains however\nlimited, due to the widely accepted belief that formal methods require\nextensive expertise and a steep learning curve. Writing correct formal\nspecifications in form of logical formulas is still considered to be a\ndifficult and error prone task.\n</p>\n<p>In this paper we propose DeepSTL, a tool and technique for the translation of\ninformal requirements, given as free English sentences, into Signal Temporal\nLogic (STL), a formal specification language for cyber-physical systems, used\nboth by academia and advanced research labs in industry. A major challenge to\ndevise such a translator is the lack of publicly available informal\nrequirements and formal specifications. We propose a two-step workflow to\naddress this challenge. We first design a grammar-based generation technique of\nsynthetic data, where each output is a random STL formula and its associated\nset of possible English translations. In the second step, we use a\nstate-of-the-art transformer-based neural translation technique, to train an\naccurate attentional translator of English to STL. The experimental results\nshow high translation quality for patterns of English requirements that have\nbeen well trained, making this workflow promising to be extended for processing\nmore complex translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1\">Ezio Bartocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickovic_D/0/1/0/all/0/1\">Dejan Ni&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isakovic_H/0/1/0/all/0/1\">Haris Isakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval. (arXiv:2110.07524v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07524","description":"<p>Training dense passage representations via contrastive learning has been\nshown effective for Open-Domain Passage Retrieval (ODPR). Existing studies\nfocus on further optimizing by improving negative sampling strategy or extra\npretraining. However, these studies keep unknown in capturing passage with\ninternal representation conflicts from improper modeling granularity. This work\nthus presents a refined model on the basis of a smaller granularity, contextual\nsentences, to alleviate the concerned conflicts. In detail, we introduce an\nin-passage negative sampling strategy to encourage a diverse generation of\nsentence representations within the same passage. Experiments on three\nbenchmark datasets verify the efficacy of our method, especially on datasets\nwhere conflicts are severe. Extensive experiments further present good\ntransferability of our method across datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-Grained Reasoning for Fake News Detection. (arXiv:2110.15064v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15064","description":"<p>The detection of fake news often requires sophisticated reasoning skills,\nsuch as logically combining information by considering word-level subtle clues.\nIn this paper, we move towards fine-grained reasoning for fake news detection\nby better reflecting the logical processes of human thinking and enabling the\nmodeling of subtle clues. In particular, we propose a fine-grained reasoning\nframework by following the human information-processing model, introduce a\nmutual-reinforcement-based method for incorporating human knowledge about which\nevidence is more important, and design a prior-aware bi-channel kernel graph\nnetwork to model subtle differences between pieces of evidence. Extensive\nexperiments show that our model outperforms the state-of-the-art methods and\ndemonstrate the explainability of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-Labeling for Massively Multilingual Speech Recognition. (arXiv:2111.00161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00161","description":"<p>Semi-supervised learning through pseudo-labeling has become a staple of\nstate-of-the-art monolingual speech recognition systems. In this work, we\nextend pseudo-labeling to massively multilingual speech recognition with 60\nlanguages. We propose a simple pseudo-labeling recipe that works well even with\nlow-resource languages: train a supervised multilingual model, fine-tune it\nwith semi-supervised learning on a target language, generate pseudo-labels for\nthat language, and train a final model using pseudo-labels for all languages,\neither from scratch or by fine-tuning. Experiments on the labeled Common Voice\nand unlabeled VoxPopuli datasets show that our recipe can yield a model with\nbetter performance for many languages that also transfers well to LibriSpeech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugosch_L/0/1/0/all/0/1\">Loren Lugosch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalabilty. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for text-to-image and image-to-text\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation tasks without any finetuning or extra\nobject detection frameworks. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial results of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNMT: Neural Machine Translation Toolkit. (arXiv:2112.15272v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15272","description":"<p>We present an open-source toolkit for neural machine translation (NMT). The\nnew toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along\nwith many other improvements detailed below, in order to create a\nself-contained, simple to use, consistent and comprehensive framework for\nMachine Translation tasks of various domains. It is tooled to support both\nbilingual and multilingual translation tasks, starting from building the model\nfrom respective corpora, to inferring new predictions or packaging the model to\nserving-capable JIT format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_N/0/1/0/all/0/1\">Nguyen Hoang Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dat_N/0/1/0/all/0/1\">Nguyen Thanh Dat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_N/0/1/0/all/0/1\">Nguyen Hoang Minh Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Nguyen Van Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Ngo Thi Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_N/0/1/0/all/0/1\">Nguyen Phuong Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_T/0/1/0/all/0/1\">Tran Hong Viet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training. (arXiv:2201.10207v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2201.10207","description":"<p>We introduce a new approach for speech pre-training named SPIRAL which works\nby learning denoising representation of perturbed data in a teacher-student\nframework. Specifically, given a speech utterance, we first feed the utterance\nto a teacher network to obtain corresponding representation. Then the same\nutterance is perturbed and fed to a student network. The student network is\ntrained to output representation resembling that of the teacher. At the same\ntime, the teacher network is updated as moving average of student's weights\nover training steps. In order to prevent representation collapse, we apply an\nin-utterance contrastive loss as pre-training objective and impose position\nrandomization on the input to the teacher. SPIRAL achieves competitive or\nbetter results compared to state-of-the-art speech pre-training method wav2vec\n2.0, with significant reduction of training cost (80% for BASE model, 65% for\nLARGE model). Furthermore, we address the problem of noise-robustness that is\ncritical to real-world speech applications. We propose multi-condition\npre-training by perturbing the student's input with various types of additive\nnoise. We demonstrate that multi-condition pre-trained SPIRAL models are more\nrobust to noisy speech (9.0% - 13.3% relative word error rate reduction on real\nnoisy test data), compared to applying multi-condition training solely in the\nfine-tuning stage. Source code is available at\nhttps://github.com/huawei-noah/Speech-Backbones/tree/main/SPIRAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenyong Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenhe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02113","description":"<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKGC/tree/main/GenKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04003","description":"<p>ROUGE is a standard automatic evaluation metric based on n-grams for\nsequence-to-sequence tasks, while cross-entropy loss is an essential objective\nof neural network language model that optimizes at a unigram level. We present\ndifferentiable n-gram objectives, attempting to alleviate the discrepancy\nbetween training criterion and evaluating criterion. The objective maximizes\nthe probabilistic weight of matched sub-sequences, and the novelty of our work\nis the objective weights the matched sub-sequences equally and does not ceil\nthe number of matched sub-sequences by the ground truth count of n-grams in\nreference sequence. We jointly optimize cross-entropy loss and the proposed\nobjective, providing decent ROUGE score enhancement over abstractive\nsummarization dataset CNN/DM and XSum, outperforming alternative n-gram\nobjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wensheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingjin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenderedNews: Une approche computationnelle des \\'ecarts de repr\\'esentation des genres dans la presse fran\\c{c}aise. (arXiv:2202.05682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.05682","description":"<p>In this article, we present {\\it GenderedNews}\n(\\url{https://gendered-news.imag.fr}), an online dashboard which gives weekly\nmeasures of gender imbalance in French online press. We use Natural Language\nProcessing (NLP) methods to quantify gender inequalities in the media, in the\nwake of global projects like the Global Media Monitoring Project. Such projects\nare instrumental in highlighting gender imbalance in the media and its very\nslow evolution. However, their generalisation is limited by their sampling and\ncost in terms of time, data and staff. Automation allows us to offer\ncomplementary measures to quantify inequalities in gender representation. We\nunderstand representation as the presence and distribution of men and women\nmentioned and quoted in the news -- as opposed to representation as\nstereotypification. In this paper, we first review different means adopted by\nprevious studies on gender inequality in the media : qualitative content\nanalysis, quantitative content analysis and computational methods. We then\ndetail the methods adopted by {\\it GenderedNews} and the two metrics\nimplemented: the masculinity rate of mentions and the proportion of men quoted\nin online news. We describe the data collected daily (seven main titles of\nFrench online news media) and the methodology behind our metrics, as well as a\nfew visualisations. We finally propose to illustrate possible analysis of our\ndata by conducting an in-depth observation of a sample of two months of our\ndatabase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richard_A/0/1/0/all/0/1\">Ange Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastin_G/0/1/0/all/0/1\">Gilles Bastin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1\">Fran&#xe7;ois Portet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation. (arXiv:2202.12350v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12350","description":"<p>Natural language processing (NLP) algorithms have become very successful, but\nthey still struggle when applied to out-of-distribution examples. In this paper\nwe propose a controllable generation approach in order to deal with this domain\nadaptation (DA) challenge. Given an input text example, our DoCoGen algorithm\ngenerates a domain-counterfactual textual example (D-con) - that is similar to\nthe original in all aspects, including the task label, but its domain is\nchanged to a desired one. Importantly, DoCoGen is trained using only unlabeled\nexamples from multiple domains - no NLP task labels or parallel pairs of\ntextual examples and their domain-counterfactuals are required. We show that\nDoCoGen can generate coherent counterfactuals consisting of multiple sentences.\nWe use the D-cons generated by DoCoGen to augment a sentiment classifier and a\nmulti-label intent classifier in 20 and 78 DA setups, respectively, where\nsource-domain labeled data is scarce. Our model outperforms strong baselines\nand improves the accuracy of a state-of-the-art unsupervised DA algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_N/0/1/0/all/0/1\">Nitay Calderon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning, Natural Language Processing, and Explainable Artificial Intelligence in the Biomedical Domain. (arXiv:2202.12678v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.12678","description":"<p>In this article, we first give an introduction to artificial intelligence and\nits applications in biology and medicine in Section 1. Deep learning methods\nare then described in Section 2. We narrow down the focus of the study on\ntextual data in Section 3, where natural language processing and its\napplications in the biomedical domain are described. In Section 4, we give an\nintroduction to explainable artificial intelligence and discuss the importance\nof explainability of artificial intelligence systems, especially in the\nbiomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Performance of Automated Essay Scoring by using back-translation essays and adjusted scores. (arXiv:2203.00354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.00354","description":"<p>Automated essay scoring plays an important role in judging students' language\nabilities in education. Traditional approaches use handcrafted features to\nscore and are time-consuming and complicated. Recently, neural network\napproaches have improved performance without any feature engineering. Unlike\nother natural language processing tasks, only a small number of datasets are\npublicly available for automated essay scoring, and the size of the dataset is\nnot sufficiently large. Considering that the performance of a neural network is\nclosely related to the size of the dataset, the lack of data limits the\nperformance improvement of the automated essay scoring model. In this paper, we\nproposed a method to increase the number of essay-score pairs using\nback-translation and score adjustment and applied it to the Automated Student\nAssessment Prize dataset for augmentation. We evaluated the effectiveness of\nthe augmented data using models from prior work. In addition, performance was\nevaluated in a model using long short-term memory, which is widely used for\nautomated essay scoring. The performance of the models was improved by using\naugmented data to train the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_Y/0/1/0/all/0/1\">You-Jin Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ri_O/0/1/0/all/0/1\">Ok-Chol Ri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crossed-Time Delay Neural Network for Speaker Recognition. (arXiv:2006.00452v3 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2006.00452","description":"<p>Time Delay Neural Network (TDNN) is a well-performing structure for DNN-based\nspeaker recognition systems. In this paper we introduce a novel structure\nCrossed-Time Delay Neural Network (CTDNN) to enhance the performance of current\nTDNN. Inspired by the multi-filters setting of convolution layer from\nconvolution neural network, we set multiple time delay units each with\ndifferent context size at the bottom layer and construct a multilayer parallel\nnetwork. The proposed CTDNN gives significant improvements over original TDNN\non both speaker verification and identification tasks. It outperforms in\nVoxCeleb1 dataset in verification experiment with a 2.6% absolute Equal Error\nRate improvement. In few shots condition CTDNN reaches 90.4% identification\naccuracy, which doubles the identification accuracy of original TDNN. We also\ncompare the proposed CTDNN with another new variant of TDNN, FTDNN, which shows\nthat our model has a 36% absolute identification accuracy improvement under few\nshots condition and can better handle training of a larger batch in a shorter\ntraining time, which better utilize the calculation resources. The code of the\nnew model is released at https://github.com/chenllliang/CTDNN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yanchun Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xiaohu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunguo Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ARM 4-BIT PQ: SIMD-based Acceleration for Approximate Nearest Neighbor Search on ARM. (arXiv:2203.02505v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02505","description":"<p>We accelerate the 4-bit product quantization (PQ) on the ARM architecture.\nNotably, the drastic performance of the conventional 4-bit PQ strongly relies\non x64-specific SIMD register, such as AVX2; hence, we cannot yet achieve such\ngood performance on ARM. To fill this gap, we first bundle two 128-bit\nregisters as one 256-bit component. We then apply shuffle operations for each\nusing the ARM-specific NEON instruction. By making this simple but critical\nmodification, we achieve a dramatic speedup for the 4-bit PQ on an ARM\narchitecture. Experiments show that the proposed method consistently achieves a\n10x improvement over the naive PQ with the same accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsui_Y/0/1/0/all/0/1\">Yusuke Matsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imaizumi_Y/0/1/0/all/0/1\">Yoshiki Imaizumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyamoto_N/0/1/0/all/0/1\">Naoya Miyamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshifuji_N/0/1/0/all/0/1\">Naoki Yoshifuji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cellular Segmentation and Composition in Routine Histology Images using Deep Learning. (arXiv:2203.02510v1 [q-bio.QM])","link":"http://arxiv.org/abs/2203.02510","description":"<p>Identification and quantification of nuclei in colorectal cancer haematoxylin\n\\&amp; eosin (H\\&amp;E) stained histology images is crucial to prognosis and patient\nmanagement. In computational pathology these tasks are referred to as nuclear\nsegmentation, classification and composition and are used to extract meaningful\ninterpretable cytological and architectural features for downstream analysis.\nThe CoNIC challenge poses the task of automated nuclei segmentation,\nclassification and composition into six different types of nuclei from the\nlargest publicly known nuclei dataset - Lizard. In this regard, we have\ndeveloped pipelines for the prediction of nuclei segmentation using HoVer-Net\nand ALBRT for cellular composition. On testing on the preliminary test set,\nHoVer-Net achieved a PQ of 0.58, a PQ+ of 0.58 and finally a mPQ+ of 0.35. For\nthe prediction of cellular composition with ALBRT on the preliminary test set,\nwe achieved an overall $R^2$ score of 0.53, consisting of 0.84 for lymphocytes,\n0.70 for epithelial cells, 0.70 for plasma and .060 for eosinophils.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Dawood_M/0/1/0/all/0/1\">Muhammad Dawood</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bashir_R/0/1/0/all/0/1\">Raja Muhammad Saad Bashir</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deshpande_S/0/1/0/all/0/1\">Srijay Deshpande</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Raza_M/0/1/0/all/0/1\">Manahil Raza</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shephard_A/0/1/0/all/0/1\">Adam Shephard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation. (arXiv:2203.02533v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02533","description":"<p>In this paper, we propose a novel semi-supervised learning (SSL) framework\nnamed BoostMIS that combines adaptive pseudo labeling and informative active\nannotation to unleash the potential of medical image SSL models: (1) BoostMIS\ncan adaptively leverage the cluster assumption and consistency regularization\nof the unlabeled data according to the current learning status. This strategy\ncan adaptively generate one-hot ``hard'' labels converted from task model\npredictions for better task model training. (2) For the unselected unlabeled\nimages with low confidence, we introduce an Active learning (AL) algorithm to\nfind the informative samples as the annotation candidates by exploiting virtual\nadversarial perturbation and model's density-aware entropy. These informative\ncandidates are subsequently fed into the next training cycle for better SSL\nlabel propagation. Notably, the adaptive pseudo-labeling and informative active\nannotation form a learning closed-loop that are mutually collaborative to boost\nmedical image SSL. To verify the effectiveness of the proposed method, we\ncollected a metastatic epidural spinal cord compression (MESCC) dataset that\naims to optimize MESCC diagnosis and classification for improved specialist\nreferral and treatment. We conducted an extensive experimental study of\nBoostMIS on MESCC and another public dataset COVIDx. The experimental results\nverify our framework's effectiveness and generalisability for different medical\nimage datasets with a significant improvement over various state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hallinan_J/0/1/0/all/0/1\">James Hallinan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makmur_A/0/1/0/all/0/1\">Andrew Makmur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_Q/0/1/0/all/0/1\">Qingpeng Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ooi_B/0/1/0/all/0/1\">Beng Chin Ooi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Pruning is All You Need for Pruning CNNs at Initialization. (arXiv:2203.02549v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02549","description":"<p>Pruning is a popular technique for reducing the model size and computational\ncost of convolutional neural networks (CNNs). However, a slow retraining or\nfine-tuning procedure is often required to recover the accuracy loss caused by\npruning. Recently, a new research direction on weight pruning,\npruning-at-initialization (PAI), is proposed to directly prune CNNs before\ntraining so that fine-tuning or retraining can be avoided. While PAI has shown\npromising results in reducing the model size, existing approaches rely on\nfine-grained weight pruning which requires unstructured sparse matrix\ncomputation, making it difficult to achieve real speedup in practice unless the\nsparsity is very high.\n</p>\n<p>This work is the first to show that fine-grained weight pruning is in fact\nnot necessary for PAI. Instead, the layerwise compression ratio is the main\ncritical factor to determine the accuracy of a CNN model pruned at\ninitialization. Based on this key observation, we propose PreCropping, a\nstructured hardware-efficient model compression scheme. PreCropping directly\ncompresses the model at the channel level following the layerwise compression\nratio. Compared to weight pruning, the proposed scheme is regular and dense in\nboth storage and computation without sacrificing accuracy. In addition, since\nPreCropping compresses CNNs at initialization, the computational and memory\ncosts of CNNs are reduced for both training and inference on commodity\nhardware. We empirically demonstrate our approaches on several modern CNN\narchitectures, including ResNet, ShuffleNet, and MobileNet for both CIFAR-10\nand ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yaohui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Weizhe Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongzheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1\">G. Edward Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiru Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building 3D Generative Models from Minimal Data. (arXiv:2203.02554v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02554","description":"<p>We propose a method for constructing generative models of 3D objects from a\nsingle 3D mesh and improving them through unsupervised low-shot learning from\n2D images. Our method produces a 3D morphable model that represents shape and\nalbedo in terms of Gaussian processes. Whereas previous approaches have\ntypically built 3D morphable models from multiple high-quality 3D scans through\nprincipal component analysis, we build 3D morphable models from a single scan\nor template. As we demonstrate in the face domain, these models can be used to\ninfer 3D reconstructions from 2D data (inverse graphics) or 3D data\n(registration). Specifically, we show that our approach can be used to perform\nface recognition using only a single 3D template (one scan total, not one per\nperson). We extend our model to a preliminary unsupervised learning framework\nthat enables the learning of the distribution of 3D faces using one 3D template\nand a small number of 2D images. This approach could also provide a model for\nthe origins of face perception in human infants, who appear to start with an\ninnate face template and subsequently develop a flexible system for perceiving\nthe 3D structure of any novel face from experience with only 2D images of a\nrelatively small number of familiar faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sutherland_S/0/1/0/all/0/1\">Skylar Sutherland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation. (arXiv:2203.02557v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02557","description":"<p>Image-to-image translation has broad applications in art, design, and\nscientific simulations. The original CycleGAN model emphasizes one-to-one\nmapping via a cycle-consistent loss, while more recent works promote\none-to-many mapping to boost the diversity of the translated images. With\nscientific simulation and one-to-one needs in mind, this work examines if\nequipping CycleGAN with a vision transformer (ViT) and employing advanced\ngenerative adversarial network (GAN) training techniques can achieve better\nperformance. The resulting UNet ViT Cycle-consistent GAN (UVCGAN) model is\ncompared with previous best-performing models on open benchmark image-to-image\ntranslation datasets, Selfie2Anime and CelebA. UVCGAN performs better and\nretains a strong correlation between the original and translated images. An\naccompanying ablation study shows that the gradient penalty and BERT-like\npre-training also contribute to the improvement.~To promote reproducibility and\nopen science, the source code, hyperparameter configurations, and pre-trained\nmodel will be made available at: https://github.com/LS4GAN/uvcga.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torbunov_D/0/1/0/all/0/1\">Dmitrii Torbunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiwang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1\">Shinjae Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meifeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viren_B/0/1/0/all/0/1\">Brett Viren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yihui Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Energy Efficiency and Robustness of tinyML Computer Vision using Log-Gradient Input Images. (arXiv:2203.02571v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02571","description":"<p>This paper studies the merits of applying log-gradient input images to\nconvolutional neural networks (CNNs) for tinyML computer vision (CV). We show\nthat log gradients enable: (i) aggressive 1.5-bit quantization of first-layer\ninputs, (ii) potential CNN resource reductions, and (iii) inherent robustness\nto illumination changes (1.7% accuracy loss across 1/32...8 brightness\nvariation vs. up to 10% for JPEG). We establish these results using the PASCAL\nRAW image data set and through a combination of experiments using neural\narchitecture search and a fixed three-layer network. The latter reveal that\ntraining on log-gradient images leads to higher filter similarity, making the\nCNN more prunable. The combined benefits of aggressive first-layer\nquantization, CNN resource reductions, and operation without tight exposure\ncontrol and image signal processing (ISP) are helpful for pushing tinyML CV\ntoward its ultimate efficiency limits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Q/0/1/0/all/0/1\">Qianyun Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murmann_B/0/1/0/all/0/1\">Boris Murmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning. (arXiv:2203.02573v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02573","description":"<p>Most methods for conditional video synthesis use a single modality as the\ncondition. This comes with major limitations. For example, it is problematic\nfor a model conditioned on an image to generate a specific motion trajectory\ndesired by the user since there is no means to provide motion information.\nConversely, language information can describe the desired motion, while not\nprecisely defining the content of the video. This work presents a multimodal\nvideo generation framework that benefits from text and images provided jointly\nor separately. We leverage the recent progress in quantized representations for\nvideos and apply a bidirectional transformer with multiple modalities as inputs\nto predict a discrete video representation. To improve video quality and\nconsistency, we propose a new video token trained with self-learning and an\nimproved mask-prediction algorithm for sampling video tokens. We introduce text\naugmentation to improve the robustness of the textual representation and\ndiversity of generated videos. Our framework can incorporate various visual\nmodalities, such as segmentation masks, drawings, and partially occluded\nimages. It can generate much longer sequences than the one used for training.\nIn addition, our model can extract visual information as suggested by the text\nprompt, e.g., \"an object in image one is moving northeast\", and generate\ncorresponding videos. We run evaluations on three public datasets and a newly\ncollected dataset labeled with facial attributes, achieving state-of-the-art\ngeneration results on all four.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Ligong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbieri_F/0/1/0/all/0/1\">Francesco Barbieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minaee_S/0/1/0/all/0/1\">Shervin Minaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-ERD: Responsive and Coherent Online Motion Style Transfer. (arXiv:2203.02574v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02574","description":"<p>Motion style transfer is a common method for enriching character animation.\nMotion style transfer algorithms are often designed for offline settings where\nmotions are processed in segments. However, for online animation applications,\nsuch as realtime avatar animation from motion capture, motions need to be\nprocessed as a stream with minimal latency. In this work, we realize a\nflexible, high-quality motion style transfer method for this setting. We\npropose a novel style transfer model, Style-ERD, to stylize motions in an\nonline manner with an Encoder-Recurrent-Decoder structure, along with a novel\ndiscriminator that combines feature attention and temporal attention. Our\nmethod stylizes motions into multiple target styles with a unified model.\nAlthough our method targets online settings, it outperforms previous offline\nmethods in motion realism and style expressiveness and provides significant\ngains in runtime efficiency\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianxin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xiaohang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1\">Michiel van de Panne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Learning of Reusable Abstract Models for Object Goal Navigation. (arXiv:2203.02583v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02583","description":"<p>In this paper, we present a novel approach to incrementally learn an Abstract\nModel of an unknown environment, and show how an agent can reuse the learned\nmodel for tackling the Object Goal Navigation task. The Abstract Model is a\nfinite state machine in which each state is an abstraction of a state of the\nenvironment, as perceived by the agent in a certain position and orientation.\nThe perceptions are high-dimensional sensory data (e.g., RGB-D images), and the\nabstraction is reached by exploiting image segmentation and the Taskonomy model\nbank. The learning of the Abstract Model is accomplished by executing actions,\nobserving the reached state, and updating the Abstract Model with the acquired\ninformation. The learned models are memorized by the agent, and they are reused\nwhenever it recognizes to be in an environment that corresponds to the stored\nmodel. We investigate the effectiveness of the proposed approach for the Object\nGoal Navigation task, relying on public benchmarks. Our results show that the\nreuse of learned Abstract Models can boost performance on Object Goal\nNavigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Campari_T/0/1/0/all/0/1\">Tommaso Campari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamanna_L/0/1/0/all/0/1\">Leonardo Lamanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traverso_P/0/1/0/all/0/1\">Paolo Traverso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serafini_L/0/1/0/all/0/1\">Luciano Serafini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballan_L/0/1/0/all/0/1\">Lamberto Ballan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept-based Explanations for Out-Of-Distribution Detectors. (arXiv:2203.02586v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02586","description":"<p>Out-of-distribution (OOD) detection plays a crucial role in ensuring the safe\ndeployment of deep neural network (DNN) classifiers. While a myriad of methods\nhave focused on improving the performance of OOD detectors, a critical gap\nremains in interpreting their decisions. We help bridge this gap by providing\nexplanations for OOD detectors based on learned high-level concepts. We first\npropose two new metrics for assessing the effectiveness of a particular set of\nconcepts for explaining OOD detectors: 1) detection completeness, which\nquantifies the sufficiency of concepts for explaining an OOD-detector's\ndecisions, and 2) concept separability, which captures the distributional\nseparation between in-distribution and OOD data in the concept space. Based on\nthese metrics, we propose a framework for learning a set of concepts that\nsatisfy the desired properties of detection completeness and concept\nseparability and demonstrate the framework's effectiveness in providing\nconcept-based explanations for diverse OOD techniques. We also show how to\nidentify prominent concepts that contribute to the detection results via a\nmodified Shapley value-based importance score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jihye Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghuram_J/0/1/0/all/0/1\">Jayaram Raghuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Quality Index Metric and Method for Online Self-Assessment of Autonomous Vehicles Sensory Perception. (arXiv:2203.02588v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02588","description":"<p>Perception is critical to autonomous driving safety. Camera-based object\ndetection is one of the most important methods for autonomous vehicle\nperception. Current camera-based object detection solutions for autonomous\ndriving cannot provide feedback on the detection performance for each frame. We\npropose an evaluation metric, namely the perception quality index (PQI), to\nassess the camera-based object detection algorithm performance and provide the\nperception quality feedback frame by frame. The method of the PQI generation is\nby combining the fine-grained saliency map intensity with the object detection\nalgorithm's output results. Furthermore, we developed a superpixel-based\nattention network (SPA-NET) to predict the proposed PQI evaluation metric by\nusing raw image pixels and superpixels as input. The proposed evaluation metric\nand prediction network are tested on three open-source datasets. The proposed\nevaluation metric can correctly assess the camera-based perception quality\nunder the autonomous driving environment according to the experiment results.\nThe network regression R-square values determine the comparison among models.\nIt is shown that a Perception Quality Index is useful in self-evaluating a\ncameras visual scene perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskandarian_A/0/1/0/all/0/1\">Azim Eskandarian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geodesic Gramian Denoising Applied to the Images Contaminated With Noise Sampled From Diverse Probability Distributions. (arXiv:2203.02600v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02600","description":"<p>As quotidian use of sophisticated cameras surges, people in modern society\nare more interested in capturing fine-quality images. However, the quality of\nthe images might be inferior to people's expectations due to the noise\ncontamination in the images. Thus, filtering out the noise while preserving\nvital image features is an essential requirement. Current existing denoising\nmethods have their own assumptions on the probability distribution in which the\ncontaminated noise is sampled for the method to attain its expected denoising\nperformance. In this paper, we utilize our recent Gramian-based filtering\nscheme to remove noise sampled from five prominent probability distributions\nfrom selected images. This method preserves image smoothness by adopting\npatches partitioned from the image, rather than pixels, and retains vital image\nfeatures by performing denoising on the manifold underlying the patch space\nrather than in the image domain. We validate its denoising performance, using\nthree benchmark computer vision test images applied to two state-of-the-art\ndenoising methods, namely BM3D and K-SVD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_Y/0/1/0/all/0/1\">Yonggi Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gajamannage_K/0/1/0/all/0/1\">Kelum Gajamannage</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sadovski_A/0/1/0/all/0/1\">Alexey Sadovski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plant Species Recognition with Optimized 3D Polynomial Neural Networks and Variably Overlapping Time-Coherent Sliding Window. (arXiv:2203.02611v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02611","description":"<p>Recently, the EAGL-I system was developed to rapidly create massive labeled\ndatasets of plants intended to be commonly used by farmers and researchers to\ncreate AI-driven solutions in agriculture. As a result, a publicly available\nplant species recognition dataset composed of 40,000 images with different\nsizes consisting of 8 plant species was created with the system in order to\ndemonstrate its capabilities. This paper proposes a novel method, called\nVariably Overlapping Time-Coherent Sliding Window (VOTCSW), that transforms a\ndataset composed of images with variable size to a 3D representation with fixed\nsize that is suitable for convolutional neural networks, and demonstrates that\nthis representation is more informative than resizing the images of the dataset\nto a given size. We theoretically formalized the use cases of the method as\nwell as its inherent properties and we proved that it has an oversampling and a\nregularization effect on the data. By combining the VOTCSW method with the 3D\nextension of a recently proposed machine learning model called 1-Dimensional\nPolynomial Neural Networks, we were able to create a model that achieved a\nstate-of-the-art accuracy of 99.9% on the dataset created by the EAGL-I system,\nsurpassing well-known architectures such as ResNet and Inception. In addition,\nwe created a heuristic algorithm that enables the degree reduction of any\npre-trained N-Dimensional Polynomial Neural Network and which compresses it\nwithout altering its performance, thus making the model faster and lighter.\nFurthermore, we established that the currently available dataset could not be\nused for machine learning in its present form, due to a substantial class\nimbalance between the training set and the test set. Hence, we created a\nspecific preprocessing and a model development framework that enabled us to\nimprove the accuracy from 49.23% to 99.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_H/0/1/0/all/0/1\">Habib Ben Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_C/0/1/0/all/0/1\">Christopher J. Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanna_S/0/1/0/all/0/1\">Sheela Ramanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Train Unstable Looped Tensor Network. (arXiv:2203.02617v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02617","description":"<p>A rising problem in the compression of Deep Neural Networks is how to reduce\nthe number of parameters in convolutional kernels and the complexity of these\nlayers by low-rank tensor approximation. Canonical polyadic tensor\ndecomposition (CPD) and Tucker tensor decomposition (TKD) are two solutions to\nthis problem and provide promising results. However, CPD often fails due to\ndegeneracy, making the networks unstable and hard to fine-tune. TKD does not\nprovide much compression if the core tensor is big. This motivates using a\nhybrid model of CPD and TKD, a decomposition with multiple Tucker models with\nsmall core tensor, known as block term decomposition (BTD). This paper proposes\na more compact model that further compresses the BTD by enforcing core tensors\nin BTD identical. We establish a link between the BTD with shared parameters\nand a looped chain tensor network (TC). Unfortunately, such strongly\nconstrained tensor networks (with loop) encounter severe numerical instability,\nas proved by y (Landsberg, 2012) and (Handschuh, 2015a). We study perturbation\nof chain tensor networks, provide interpretation of instability in TC,\ndemonstrate the problem. We propose novel methods to gain the stability of the\ndecomposition results, keep the network robust and attain better approximation.\nExperimental results will confirm the superiority of the proposed methods in\ncompression of well-known CNNs, and TC decomposition under challenging\nscenarios\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phan_A/0/1/0/all/0/1\">Anh-Huy Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sobolev_K/0/1/0/all/0/1\">Konstantin Sobolev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermilov_D/0/1/0/all/0/1\">Dmitry Ermilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorona_I/0/1/0/all/0/1\">Igor Vorona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozyrskiy_N/0/1/0/all/0/1\">Nikolay Kozyrskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tichavsky_P/0/1/0/all/0/1\">Petr Tichavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cichocki_A/0/1/0/all/0/1\">Andrzej Cichocki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Important Object Identification with Semi-Supervised Learning for Autonomous Driving. (arXiv:2203.02634v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02634","description":"<p>Accurate identification of important objects in the scene is a prerequisite\nfor safe and high-quality decision making and motion planning of intelligent\nagents (e.g., autonomous vehicles) that navigate in complex and dynamic\nenvironments. Most existing approaches attempt to employ attention mechanisms\nto learn importance weights associated with each object indirectly via various\ntasks (e.g., trajectory prediction), which do not enforce direct supervision on\nthe importance estimation. In contrast, we tackle this task in an explicit way\nand formulate it as a binary classification (\"important\" or \"unimportant\")\nproblem. We propose a novel approach for important object identification in\negocentric driving scenarios with relational reasoning on the objects in the\nscene. Besides, since human annotations are limited and expensive to obtain, we\npresent a semi-supervised learning pipeline to enable the model to learn from\nunlimited unlabeled data. Moreover, we propose to leverage the auxiliary tasks\nof ego vehicle behavior prediction to further improve the accuracy of\nimportance estimation. The proposed approach is evaluated on a public\negocentric driving dataset (H3D) collected in complex traffic scenarios. A\ndetailed ablative study is conducted to demonstrate the effectiveness of each\nmodel component and the training strategy. Our approach also outperforms\nrule-based baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1\">Haiming Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Chiho Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training privacy-preserving video analytics pipelines by suppressing features that reveal information about private attributes. (arXiv:2203.02635v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02635","description":"<p>Deep neural networks are increasingly deployed for scene analytics, including\nto evaluate the attention and reaction of people exposed to out-of-home\nadvertisements. However, the features extracted by a deep neural network that\nwas trained to predict a specific, consensual attribute (e.g. emotion) may also\nencode and thus reveal information about private, protected attributes (e.g.\nage or gender). In this work, we focus on such leakage of private information\nat inference time. We consider an adversary with access to the features\nextracted by the layers of a deployed neural network and use these features to\npredict private attributes. To prevent the success of such an attack, we modify\nthe training of the network using a confusion loss that encourages the\nextraction of features that make it difficult for the adversary to accurately\npredict private attributes. We validate this training approach on image-based\ntasks using a publicly available dataset. Results show that, compared to the\noriginal network, the proposed PrivateNet can reduce the leakage of private\ninformation of a state-of-the-art emotion recognition classifier by 2.88% for\ngender and by 13.06% for age group, with a minimal effect on task accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chau Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Crowd Counting via Multifaceted Attention. (arXiv:2203.02636v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02636","description":"<p>This paper focuses on the challenging crowd counting task. As large-scale\nvariations often exist within crowd images, neither fixed-size convolution\nkernel of CNN nor fixed-size attention of recent vision transformers can well\nhandle this kind of variation. To address this problem, we propose a\nMultifaceted Attention Network (MAN) to improve transformer models in local\nspatial relation encoding. MAN incorporates global attention from a vanilla\ntransformer, learnable local attention, and instance attention into a counting\nmodel. Firstly, the local Learnable Region Attention (LRA) is proposed to\nassign attention exclusively for each feature location dynamically. Secondly,\nwe design the Local Attention Regularization to supervise the training of LRA\nby minimizing the deviation among the attention for different feature\nlocations. Finally, we provide an Instance Attention mechanism to focus on the\nmost important instances dynamically during training. Extensive experiments on\nfour challenging crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++,\nand NWPU have validated the proposed method. Codes:\nhttps://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhiheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaopeng Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-based Contrastive Disentangling for Generalized Zero-Shot Learning. (arXiv:2203.02648v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02648","description":"<p>Generalized Zero-Shot Learning (GZSL) aims to recognize both seen and unseen\nclasses by training only the seen classes, in which the instances of unseen\nclasses tend to be biased towards the seen class. In this paper, we propose a\nCluster-based Contrastive Disentangling (CCD) method to improve GZSL by\nalleviating the semantic gap and domain shift problems. Specifically, we first\ncluster the batch data to form several sets containing similar classes. Then,\nwe disentangle the visual features into semantic-unspecific and\nsemantic-matched variables, and further disentangle the semantic-matched\nvariables into class-shared and class-unique variables according to the\nclustering results. The disentangled learning module with random swapping and\nsemantic-visual alignment bridges the semantic gap. Moreover, we introduce\ncontrastive learning on semantic-matched and class-unique variables to learn\nhigh intra-set and intra-class similarity, as well as inter-set and inter-class\ndiscriminability. Then, the generated visual features conform to the underlying\ncharacteristics of general images and have strong discriminative information,\nwhich alleviates the domain shift problem well. We evaluate our proposed method\non four datasets and achieve state-of-the-art results in both conventional and\ngeneralized settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chenwei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02651","description":"<p>Conventional NAS-based pruning algorithms aim to find the sub-network with\nthe best validation performance. However, validation performance does not\nsuccessfully represent test performance, i.e., potential performance. Also,\nalthough fine-tuning the pruned network to restore the performance drop is an\ninevitable process, few studies have handled this issue. This paper proposes a\nnovel sub-network search and fine-tuning method that is named Ensemble\nKnowledge Guidance (EKG). First, we experimentally prove that the fluctuation\nof the loss landscape is an effective metric to evaluate the potential\nperformance. In order to search a sub-network with the smoothest loss landscape\nat a low cost, we propose a pseudo-supernet built by an ensemble sub-network\nknowledge distillation. Next, we propose a novel fine-tuning that re-uses the\ninformation of the search phase. We store the interim sub-networks, that is,\nthe by-products of the search phase, and transfer their knowledge into the\npruned network. Note that EKG is easy to be plugged-in and computationally\nefficient. For example, in the case of ResNet-50, about 45% of FLOPS is removed\nwithout any performance drop in only 315 GPU hours. The implemented code is\navailable at https://github.com/sseung0703/EKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Byung Cheol Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-scale Comprehensive Dataset and Copy-overlap Aware Evaluation Protocol for Segment-level Video Copy Detection. (arXiv:2203.02654v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02654","description":"<p>In this paper, we introduce VCSL (Video Copy Segment Localization), a new\ncomprehensive segment-level annotated video copy dataset. Compared with\nexisting copy detection datasets restricted by either video-level annotation or\nsmall-scale, VCSL not only has two orders of magnitude more segment-level\nlabelled data, with 160k realistic video copy pairs containing more than 280k\nlocalized copied segment pairs, but also covers a variety of video categories\nand a wide range of video duration. All the copied segments inside each\ncollected video pair are manually extracted and accompanied by precisely\nannotated starting and ending timestamps. Alongside the dataset, we also\npropose a novel evaluation protocol that better measures the prediction\naccuracy of copy overlapping segments between a video pair and shows improved\nadaptability in different scenarios. By benchmarking several baseline and\nstate-of-the-art segment-level video copy detection methods with the proposed\ndataset and evaluation metric, we provide a comprehensive analysis that\nuncovers the strengths and weaknesses of current approaches, hoping to open up\npromising directions for future works. The VCSL dataset, metric and benchmark\ncodes are all publicly available at https://github.com/alipay/VCSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sifeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xudong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Furong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingxiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaiming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_F/0/1/0/all/0/1\">Feng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaobo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-visual speech separation based on joint feature representation with cross-modal attention. (arXiv:2203.02655v1 [cs.SD])","link":"http://arxiv.org/abs/2203.02655","description":"<p>Multi-modal based speech separation has exhibited a specific advantage on\nisolating the target character in multi-talker noisy environments.\nUnfortunately, most of current separation strategies prefer a straightforward\nfusion based on feature learning of each single modality, which is far from\nsufficient consideration of inter-relationships between modalites. Inspired by\nlearning joint feature representations from audio and visual streams with\nattention mechanism, in this study, a novel cross-modal fusion strategy is\nproposed to benefit the whole framework with semantic correlations between\ndifferent modalities. To further improve audio-visual speech separation, the\ndense optical flow of lip motion is incorporated to strengthen the robustness\nof visual representation. The evaluation of the proposed work is performed on\ntwo public audio-visual speech separation benchmark datasets. The overall\nimprovement of the performance has demonstrated that the additional motion\nnetwork effectively enhances the visual representation of the combined lip\nimages and audio signal, as well as outperforming the baseline in terms of all\nmetrics with the proposed cross-modal fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Junwen Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yufei Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers. (arXiv:2203.02664v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02664","description":"<p>Weakly-supervised semantic segmentation (WSSS) with image-level labels is an\nimportant and challenging task. Due to the high training efficiency, end-to-end\nsolutions for WSSS have received increasing attention from the community.\nHowever, current methods are mainly based on convolutional neural networks and\nfail to explore the global information properly, thus usually resulting in\nincomplete object regions. In this paper, to address the aforementioned\nproblem, we introduce Transformers, which naturally integrate global\ninformation, to generate more integral initial pseudo labels for end-to-end\nWSSS. Motivated by the inherent consistency between the self-attention in\nTransformers and the semantic affinity, we propose an Affinity from Attention\n(AFA) module to learn semantic affinity from the multi-head self-attention\n(MHSA) in Transformers. The learned affinity is then leveraged to refine the\ninitial pseudo labels for segmentation. In addition, to efficiently derive\nreliable affinity labels for supervising AFA and ensure the local consistency\nof pseudo labels, we devise a Pixel-Adaptive Refinement module that\nincorporates low-level image appearance information to refine the pseudo\nlabels. We perform extensive experiments and our method achieves 66.0% and\n38.9% mIoU on the PASCAL VOC 2012 and MS COCO 2014 datasets, respectively,\nsignificantly outperforming recent end-to-end methods and several multi-stage\ncompetitors. Code is available at https://github.com/rulixiang/afa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ru_L/0/1/0/all/0/1\">Lixiang Ru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Language Image Matching for Weakly Supervised Semantic Segmentation. (arXiv:2203.02668v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02668","description":"<p>It has been widely known that CAM (Class Activation Map) usually only\nactivates discriminative object regions and falsely includes lots of\nobject-related backgrounds. As only a fixed set of image-level object labels\nare available to the WSSS (weakly supervised semantic segmentation) model, it\ncould be very difficult to suppress those diverse background regions consisting\nof open set objects. In this paper, we propose a novel Cross Language Image\nMatching (CLIMS) framework, based on the recently introduced Contrastive\nLanguage-Image Pre-training (CLIP) model, for WSSS. The core idea of our\nframework is to introduce natural language supervision to activate more\ncomplete object regions and suppress closely-related open background regions.\nIn particular, we design object, background region and text label matching\nlosses to guide the model to excite more reasonable object regions for CAM of\neach category. In addition, we design a co-occurring background suppression\nloss to prevent the model from activating closely-related background regions,\nwith a predefined set of class-related background text descriptions. These\ndesigns enable the proposed CLIMS to generate a more complete and compact\nactivation map for the target objects. Extensive experiments on PASCAL VOC2012\ndataset show that our CLIMS significantly outperforms the previous\nstate-of-the-art methods. Code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xianxu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1\">Kai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Newton-PnP: Real-time Visual Navigation for Autonomous Toy-Drones. (arXiv:2203.02686v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02686","description":"<p>The Perspective-n-Point problem aims to estimate the relative pose between a\ncalibrated monocular camera and a known 3D model, by aligning pairs of 2D\ncaptured image points to their corresponding 3D points in the model. We suggest\nan algorithm that runs on weak IoT devices in real-time but still provides\nprovable theoretical guarantees for both running time and correctness. Existing\nsolvers provide only one of these requirements. Our main motivation was to turn\nthe popular DJI's Tello Drone (&lt;90gr, &lt;\\$100) into an autonomous drone that\nnavigates in an indoor environment with no external human/laptop/sensor, by\nsimply attaching a Raspberry PI Zero (&lt;9gr, &lt;\\$25) to it. This tiny\nmicro-processor takes as input a real-time video from a tiny RGB camera, and\nruns our PnP solver on-board. Extensive experimental results, open source code,\nand a demonstration video are included.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jubran_I/0/1/0/all/0/1\">Ibrahim Jubran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fares_F/0/1/0/all/0/1\">Fares Fares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfassi_Y/0/1/0/all/0/1\">Yuval Alfassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_F/0/1/0/all/0/1\">Firas Ayoub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1\">Dan Feldman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection. (arXiv:2203.02688v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02688","description":"<p>The recently proposed camouflaged object detection (COD) attempts to segment\nobjects that are visually blended into their surroundings, which is extremely\ncomplex and difficult in real-world scenarios. Apart from high intrinsic\nsimilarity between the camouflaged objects and their background, the objects\nare usually diverse in scale, fuzzy in appearance, and even severely occluded.\nTo deal with these problems, we propose a mixed-scale triplet network,\n\\textbf{ZoomNet}, which mimics the behavior of humans when observing vague\nimages, i.e., zooming in and out. Specifically, our ZoomNet employs the zoom\nstrategy to learn the discriminative mixed-scale semantics by the designed\nscale integration unit and hierarchical mixed-scale unit, which fully explores\nimperceptible clues between the candidate objects and background surroundings.\nMoreover, considering the uncertainty and ambiguity derived from\nindistinguishable textures, we construct a simple yet effective regularization\nconstraint, uncertainty-aware loss, to promote the model to accurately produce\npredictions with higher confidence in candidate regions. Without bells and\nwhistles, our proposed highly task-friendly model consistently surpasses the\nexisting 23 state-of-the-art methods on four public datasets. Besides, the\nsuperior performance over the recent cutting-edge models on the SOD task also\nverifies the effectiveness and generality of our model. The code will be\navailable at \\url{https://github.com/lartpang/ZoomNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Youwei_P/0/1/0/all/0/1\">Pang Youwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaoqi_Z/0/1/0/all/0/1\">Zhao Xiaoqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Zhu_X/0/1/0/all/0/1\">Xiang Tian-Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lihe_Z/0/1/0/all/0/1\">Zhang Lihe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huchuan_L/0/1/0/all/0/1\">Lu Huchuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated and Generalized Person Re-identification through Domain and Feature Hallucinating. (arXiv:2203.02689v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02689","description":"<p>In this paper, we study the problem of federated domain generalization\n(FedDG) for person re-identification (re-ID), which aims to learn a generalized\nmodel with multiple decentralized labeled source domains. An empirical method\n(FedAvg) trains local models individually and averages them to obtain the\nglobal model for further local fine-tuning or deploying in unseen target\ndomains. One drawback of FedAvg is neglecting the data distributions of other\nclients during local training, making the local model overfit local data and\nproducing a poorly-generalized global model. To solve this problem, we propose\na novel method, called \"Domain and Feature Hallucinating (DFH)\", to produce\ndiverse features for learning generalized local and global models.\nSpecifically, after each model aggregation process, we share the Domain-level\nFeature Statistics (DFS) among different clients without violating data\nprivacy. During local training, the DFS are used to synthesize novel domain\nstatistics with the proposed domain hallucinating, which is achieved by\nre-weighting DFS with random weights. Then, we propose feature hallucinating to\ndiversify local features by scaling and shifting them to the distribution of\nthe obtained novel domain. The synthesized novel features retain the original\npair-wise similarities, enabling us to utilize them to optimize the model in a\nsupervised manner. Extensive experiments verify that the proposed DFH can\neffectively improve the generalization ability of the global model. Our method\nachieves the state-of-the-art performance for FedDG on four large-scale re-ID\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fengxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhiming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaozi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IDmUNet: A new image decomposition induced network for sparse feature segmentation. (arXiv:2203.02690v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02690","description":"<p>UNet and its variants are among the most popular methods for medical image\nsegmentation. Despite their successes in task generality, most of them consider\nlittle mathematical modeling behind specific applications. In this paper, we\nfocus on the sparse feature segmentation task and make a task-oriented network\ndesign, in which the target objects are sparsely distributed and the background\nis hard to be mathematically modeled. We start from an image decomposition\nmodel with sparsity regularization, and propose a deep unfolding network,\nnamely IDNet, based on an iterative solver, scaled alternating direction method\nof multipliers (scaled-ADMM). The IDNet splits raw inputs into double feature\nlayers. Then a new task-oriented segmentation network is constructed, dubbed as\nIDmUNet, based on the proposed IDNets and a mini-UNet. Because of the sparsity\nprior and deep unfolding method in the structure design, this IDmUNet combines\nthe advantages of mathematical modeling and data-driven approaches. Firstly,\nour approach has mathematical interpretability and can achieve favorable\nperformance with far fewer learnable parameters. Secondly, our IDmUNet is\nrobust in a simple end-to-end training with explainable behaviors. In the\nexperiments of retinal vessel segmentation (RVS), IDmUNet produces the\nstate-of-the-art results with only 0.07m parameters, whereas SA-UNet, one of\nthe latest variants of UNet, contains 0.54m and the original UNet 31.04m.\nMoreover, the training procedure of our network converges faster without\noverfitting phenomenon. This decomposition-based network construction strategy\ncan be generalized to other problems with mathematically clear targets and\ncomplicated unclear backgrounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yumeng Ren</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yiming Gao</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1\">Chunlin Wu</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Tai_X/0/1/0/all/0/1\">Xue-cheng Tai</a> (3) ((1) School of Mathematical Sciences, Nankai University, Tianjin, China (2) College of Science, Nanjing University of Aeronautics and Astronautics, Nanjing, China (3) Department of Mathematics, Hong Kong Baptist University, China )"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-resolution Coastline Extraction in SAR Images via MISP-GGD Superpixel Segmentation. (arXiv:2203.02708v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02708","description":"<p>High accuracy coastline/shoreline extraction from SAR imagery is a crucial\nstep in a number of maritime and coastal monitoring applications. We present a\nmethod based on image segmentation using the Generalised Gamma Mixture Model\nsuperpixel algorithm (MISP-GGD). MISP-GGD produces superpixels adhering with\ngreat accuracy to object edges in the image, such as the coastline.\nUnsupervised clustering of the generated superpixels according to textural and\nradiometric features allows for generation of a land/water mask from which a\nhighly accurate coastline can be extracted. We present results of our proposed\nmethod on a number of SAR images of varying characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pappas_O/0/1/0/all/0/1\">Odysseas Pappas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adams_B/0/1/0/all/0/1\">Byron Adams</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Achim_A/0/1/0/all/0/1\">Alin Achim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient and Scalable Sharpness-Aware Minimization. (arXiv:2203.02714v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02714","description":"<p>Recently, Sharpness-Aware Minimization (SAM), which connects the geometry of\nthe loss landscape and generalization, has demonstrated significant performance\nboosts on training large-scale models such as vision transformers. However, the\nupdate rule of SAM requires two sequential (non-parallelizable) gradient\ncomputations at each step, which can double the computational overhead. In this\npaper, we propose a novel algorithm LookSAM - that only periodically calculates\nthe inner gradient ascent, to significantly reduce the additional training cost\nof SAM. The empirical results illustrate that LookSAM achieves similar accuracy\ngains to SAM while being tremendously faster - it enjoys comparable\ncomputational complexity with first-order optimizers such as SGD or Adam. To\nfurther evaluate the performance and scalability of LookSAM, we incorporate a\nlayer-wise modification and perform experiments in the large-batch training\nscenario, which is more prone to converge to sharp local minima. We are the\nfirst to successfully scale up the batch size when training Vision Transformers\n(ViTs). With a 64k batch size, we are able to train ViTs from scratch in\nminutes while maintaining competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_S/0/1/0/all/0/1\">Siqi Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Dual Dense Connection Network for Video Super-resolution. (arXiv:2203.02723v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02723","description":"<p>Video super-resolution (VSR) refers to the reconstruction of high-resolution\n(HR) video from the corresponding low-resolution (LR) video. Recently, VSR has\nreceived increasing attention. In this paper, we propose a novel dual dense\nconnection network that can generate high-quality super-resolution (SR)\nresults. The input frames are creatively divided into reference frame,\npre-temporal group and post-temporal group, representing information in\ndifferent time periods. This grouping method provides accurate information of\ndifferent time periods without causing time information disorder. Meanwhile, we\nproduce a new loss function, which is beneficial to enhance the convergence\nability of the model. Experiments show that our model is superior to other\nadvanced models in Vid4 datasets and SPMCS-11 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guofang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yonggui Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Approach for Seam Carving Detection using Deep Neural Networks. (arXiv:2203.02728v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02728","description":"<p>Seam carving is a computational method capable of resizing images for both\nreduction and expansion based on its content, instead of the image geometry.\nAlthough the technique is mostly employed to deal with redundant information,\ni.e., regions composed of pixels with similar intensity, it can also be used\nfor tampering images by inserting or removing relevant objects. Therefore,\ndetecting such a process is of extreme importance regarding the image security\ndomain. However, recognizing seam-carved images does not represent a\nstraightforward task even for human eyes, and robust computation tools capable\nof identifying such alterations are very desirable. In this paper, we propose\nan end-to-end approach to cope with the problem of automatic seam carving\ndetection that can obtain state-of-the-art results. Experiments conducted over\npublic and private datasets with several tampering configurations evidence the\nsuitability of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1\">Thierry P. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santana_M/0/1/0/all/0/1\">Marcos Cleison S. Santana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_L/0/1/0/all/0/1\">Leandro A. Passos Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_K/0/1/0/all/0/1\">Kelton Augusto P. da Costa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaxDropoutV2: An Improved Method to Drop out Neurons in Convolutional Neural Networks. (arXiv:2203.02740v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02740","description":"<p>In the last decade, exponential data growth supplied the machine\nlearning-based algorithms' capacity and enabled their usage in daily life\nactivities. Additionally, such an improvement is partially explained due to the\nadvent of deep learning techniques, i.e., stacks of simple architectures that\nend up in more complex models. Although both factors produce outstanding\nresults, they also pose drawbacks regarding the learning process since training\ncomplex models denotes an expensive task and results are prone to overfit the\ntraining data. A supervised regularization technique called MaxDropout was\nrecently proposed to tackle the latter, providing several improvements\nconcerning traditional regularization approaches. In this paper, we present its\nimproved version called MaxDropoutV2. Results considering two public datasets\nshow that the model performs faster than the standard version and, in most\ncases, provides more accurate results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Claudio Filipi Goncalves do Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roder_M/0/1/0/all/0/1\">Mateus Roder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_L/0/1/0/all/0/1\">Leandro A. Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o P. Papa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetaFormer: A Unified Meta Framework for Fine-Grained Recognition. (arXiv:2203.02751v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02751","description":"<p>Fine-Grained Visual Classification(FGVC) is the task that requires\nrecognizing the objects belonging to multiple subordinate categories of a\nsuper-category. Recent state-of-the-art methods usually design sophisticated\nlearning pipelines to tackle this task. However, visual information alone is\noften not sufficient to accurately differentiate between fine-grained visual\ncategories. Nowadays, the meta-information (e.g., spatio-temporal prior,\nattribute, and text description) usually appears along with the images. This\ninspires us to ask the question: Is it possible to use a unified and simple\nframework to utilize various meta-information to assist in fine-grained\nidentification? To answer this problem, we explore a unified and strong\nmeta-framework(MetaFormer) for fine-grained visual classification. In practice,\nMetaFormer provides a simple yet effective approach to address the joint\nlearning of vision and various meta-information. Moreover, MetaFormer also\nprovides a strong baseline for FGVC without bells and whistles. Extensive\nexperiments demonstrate that MetaFormer can effectively use various\nmeta-information to improve the performance of fine-grained recognition. In a\nfair comparison, MetaFormer can outperform the current SotA approaches with\nonly vision information on the iNaturalist2017 and iNaturalist2018 datasets.\nAdding meta-information, MetaFormer can exceed the current SotA approaches by\n5.9% and 5.3%, respectively. Moreover, MetaFormer can achieve 92.3% and 92.7%\non CUB-200-2011 and NABirds, which significantly outperforms the SotA\napproaches. The source code and pre-trained models are released\nathttps://github.com/dqshuai/MetaFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diao_Q/0/1/0/all/0/1\">Qishuai Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN. (arXiv:2203.02762v1 [cs.GR])","link":"http://arxiv.org/abs/2203.02762","description":"<p>The research topic of sketch-to-portrait generation has witnessed a boost of\nprogress with deep learning techniques. The recently proposed StyleGAN\narchitectures achieve state-of-the-art generation ability but the original\nStyleGAN is not friendly for sketch-based creation due to its unconditional\ngeneration nature. To address this issue, we propose a direct conditioning\nstrategy to better preserve the spatial information under the StyleGAN\nframework. Specifically, we introduce Spatially Conditioned StyleGAN\n(SC-StyleGAN for short), which explicitly injects spatial constraints to the\noriginal StyleGAN generation process. We explore two input modalities, sketches\nand semantic maps, which together allow users to express desired generation\nresults more precisely and easily. Based on SC-StyleGAN, we present\nDrawingInStyles, a novel drawing interface for non-professional users to easily\nproduce high-quality, photo-realistic face images with precise control, either\nfrom scratch or editing existing ones. Qualitative and quantitative evaluations\nshow the superior generation ability of our method to existing and alternative\nsolutions. The usability and expressiveness of our system are confirmed by a\nuser study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Wanchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shu-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation. (arXiv:2203.02764v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02764","description":"<p>Most existing works in vision-and-language navigation (VLN) focus on either\ndiscrete or continuous environments, training agents that cannot generalize\nacross the two. The fundamental difference between the two setups is that\ndiscrete navigation assumes prior knowledge of the connectivity graph of the\nenvironment, so that the agent can effectively transfer the problem of\nnavigation with low-level controls to jumping from node to node with high-level\nactions by grounding to an image of a navigable direction. To bridge the\ndiscrete-to-continuous gap, we propose a predictor to generate a set of\ncandidate waypoints during navigation, so that agents designed with high-level\nactions can be transferred to and trained in continuous environments. We refine\nthe connectivity graph of Matterport3D to fit the continuous\nHabitat-Matterport3D, and train the waypoints predictor with the refined graphs\nto produce accessible waypoints at each time step. Moreover, we demonstrate\nthat the predicted waypoints can be augmented during training to diversify the\nviews and paths, and therefore enhance agent's generalization ability. Through\nextensive experiments we show that agents navigating in continuous environments\nwith predicted waypoints perform significantly better than agents using\nlow-level actions, which reduces the absolute discrete-to-continuous gap by\n11.76% Success Weighted by Path Length (SPL) for the Cross-Modal Matching Agent\nand 18.24% SPL for the Recurrent VLN-BERT. Our agents, trained with a simple\nimitation learning objective, outperform previous methods by a large margin,\nachieving new state-of-the-art results on the testing environments of the\nR2R-CE and the RxR-CE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Part-aware Instance Segmentation for Industrial Bin Picking. (arXiv:2203.02767v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02767","description":"<p>Industrial bin picking is a challenging task that requires accurate and\nrobust segmentation of individual object instances. Particularly, industrial\nobjects can have irregular shapes, that is, thin and concave, whereas in\nbin-picking scenarios, objects are often closely packed with strong occlusion.\nTo address these challenges, we formulate a novel part-aware instance\nsegmentation pipeline. The key idea is to decompose industrial objects into\ncorrelated approximate convex parts and enhance the object-level segmentation\nwith part-level segmentation. We design a part-aware network to predict part\nmasks and part-to-part offsets, followed by a part aggregation module to\nassemble the recognized parts into instances. To guide the network learning, we\nalso propose an automatic label decoupling scheme to generate ground-truth\npart-level labels from instance-level labels. Finally, we contribute the first\ninstance segmentation dataset, which contains a variety of industrial objects\nthat are thin and have non-trivial shapes. Extensive experimental results on\nvarious industrial objects demonstrate that our method can achieve the best\nsegmentation results compared with the state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yidan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Be So Dense: Sparse-to-Sparse GAN Training Without Sacrificing Performance. (arXiv:2203.02770v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02770","description":"<p>Generative adversarial networks (GANs) have received an upsurging interest\nsince being proposed due to the high quality of the generated data. While\nachieving increasingly impressive results, the resource demands associated with\nthe large model size hinders the usage of GANs in resource-limited scenarios.\nFor inference, the existing model compression techniques can reduce the model\ncomplexity with comparable performance. However, the training efficiency of\nGANs has less been explored due to the fragile training process of GANs. In\nthis paper, we, for the first time, explore the possibility of directly\ntraining sparse GAN from scratch without involving any dense or pre-training\nsteps. Even more unconventionally, our proposed method enables directly\ntraining sparse unbalanced GANs with an extremely sparse generator from\nscratch. Instead of training full GANs, we start with sparse GANs and\ndynamically explore the parameter space spanned over the generator throughout\ntraining. Such a sparse-to-sparse training procedure enhances the capacity of\nthe highly sparse generator progressively while sticking to a fixed small\nparameter budget with appealing training and inference efficiency gains.\nExtensive experiments with modern GAN architectures validate the effectiveness\nof our method. Our sparsified GANs, trained from scratch in one single run, are\nable to outperform the ones learned by expensive iterative pruning and\nre-training. Perhaps most importantly, we find instead of inheriting parameters\nfrom expensive pre-trained GANs, directly training sparse GANs from scratch can\nbe a much more efficient solution. For example, only training with a 80% sparse\ngenerator and a 70% sparse discriminator, our method can achieve even better\nperformance than the dense BigGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuesong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rib Suppression in Digital Chest Tomosynthesis. (arXiv:2203.02772v1 [eess.IV])","link":"http://arxiv.org/abs/2203.02772","description":"<p>Digital chest tomosynthesis (DCT) is a technique to produce sectional 3D\nimages of a human chest for pulmonary disease screening, with 2D X-ray\nprojections taken within an extremely limited range of angles. However, under\nthe limited angle scenario, DCT contains strong artifacts caused by the\npresence of ribs, jamming the imaging quality of the lung area. Recently, great\nprogress has been achieved for rib suppression in a single X-ray image, to\nreveal a clearer lung texture. We firstly extend the rib suppression problem to\nthe 3D case at the software level. We propose a $\\textbf{T}$omosynthesis\n$\\textbf{RI}$b Su$\\textbf{P}$pression and $\\textbf{L}$ung\n$\\textbf{E}$nhancement $\\textbf{Net}$work (TRIPLE-Net) to model the 3D rib\ncomponent and provide a rib-free DCT. TRIPLE-Net takes the advantages from both\n2D and 3D domains, which model the ribs in DCT with the exact FBP procedure and\n3D depth information, respectively. The experiments on simulated datasets and\nclinical data have shown the effectiveness of TRIPLE-Net to preserve lung\ndetails as well as improve the imaging quality of pulmonary diseases. Finally,\nan expert user study confirms our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yihua Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Q/0/1/0/all/0/1\">Qingsong Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Y/0/1/0/all/0/1\">Yuanyuan Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jianji Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1\">Yi Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1\">Hongen Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Dual-Student with Differentiable Spatial Warping for Semi-Supervised Semantic Segmentation. (arXiv:2203.02792v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02792","description":"<p>A common challenge posed to robust semantic segmentation is the expensive\ndata annotation cost. Existing semi-supervised solutions show great potential\ntoward solving this problem. Their key idea is constructing consistency\nregularization with unsupervised data augmentation from unlabeled data for\nmodel training. The perturbations for unlabeled data enable the consistency\ntraining loss, which benefits semi-supervised semantic segmentation. However,\nthese perturbations destroy image context and introduce unnatural boundaries,\nwhich is harmful for semantic segmentation. Besides, the widely adopted\nsemi-supervised learning framework, i.e. mean-teacher, suffers performance\nlimitation since the student model finally converges to the teacher model. In\nthis paper, first of all, we propose a context friendly differentiable\ngeometric warping to conduct unsupervised data augmentation; secondly, a novel\nadversarial dual-student framework is proposed to improve the Mean-Teacher from\nthe following two aspects: (1) dual student models are learnt independently\nexcept for a stabilization constraint to encourage exploiting model\ndiversities; (2) adversarial training scheme is applied to both students and\nthe discriminators are resorted to distinguish reliable pseudo-label of\nunlabeled data for self-training. Effectiveness is validated via extensive\nexperiments on PASCAL VOC2012 and Citescapes. Our solution significantly\nimproves the performance and state-of-the-art results are achieved on both\ndatasets. Remarkably, compared with fully supervision, our solution achieves\ncomparable mIoU of 73.4% using only 12.5% annotated data on PASCAL VOC2012.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Cong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_H/0/1/0/all/0/1\">Huanjing Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer. (arXiv:2203.02794v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02794","description":"<p>The recent development of imaging and sequencing technologies enables\nsystematic advances in the clinical study of lung cancer. Meanwhile, the human\nmind is limited in effectively handling and fully utilizing the accumulation of\nsuch enormous amounts of data. Machine learning-based approaches play a\ncritical role in integrating and analyzing these large and complex datasets,\nwhich have extensively characterized lung cancer through the use of different\nperspectives from these accrued data. In this article, we provide an overview\nof machine learning-based approaches that strengthen the varying aspects of\nlung cancer diagnosis and therapy, including early detection, auxiliary\ndiagnosis, prognosis prediction and immunotherapy practice. Moreover, we\nhighlight the challenges and opportunities for future applications of machine\nlearning in lung cancer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guoqian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Dirichlet Process Gaussian Mixtures for Segmentation on Noisy Hyperspectral Images. (arXiv:2203.02820v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02820","description":"<p>Image segmentation is a fundamental step for the interpretation of Remote\nSensing Images. Clustering or segmentation methods usually precede the\nclassification task and are used as support tools for manual labeling. The most\ncommon algorithms, such as k-means, mean-shift, and MRS, require an extra\nmanual step to find the scale parameter. The segmentation results are severely\naffected if the parameters are not correctly tuned and diverge from the optimal\nvalues. Additionally, the search for the optimal scale is a costly task, as it\nrequires a comprehensive hyper-parameter search. This paper proposes and\nevaluates a method for segmentation of Hyperspectral Images using the Dirichlet\nProcess Gaussian Mixture Model. Our model can self-regulate the parameters\nuntil it finds the optimal values of scale and the number of clusters in a\ngiven dataset. The results demonstrate the potential of our method to find\nobjects in a Hyperspectral Image while bypassing the burden of manual search of\nthe optimal parameters. In addition, our model also produces similar results on\nnoisy datasets, while previous research usually required a pre-processing task\nfor noise reduction and spectral smoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mantripragada_K/0/1/0/all/0/1\">Kiran Mantripragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_F/0/1/0/all/0/1\">Faisal Z. Qureshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images. (arXiv:2203.02846v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02846","description":"<p>Top-down instance segmentation framework has shown its superiority in object\ndetection compared to the bottom-up framework. While it is efficient in\naddressing over-segmentation, top-down instance segmentation suffers from\nover-crop problem. However, a complete segmentation mask is crucial for\nbiological image analysis as it delivers important morphological properties\nsuch as shapes and volumes. In this paper, we propose a region proposal\nrectification (RPR) module to address this challenging incomplete segmentation\nproblem. In particular, we offer a progressive ROIAlign module to introduce\nneighbor information into a series of ROIs gradually. The ROI features are fed\ninto an attentive feed-forward network (FFN) for proposal box regression. With\nadditional neighbor information, the proposed RPR module shows significant\nimprovement in correction of region proposal locations and thereby exhibits\nfavorable instance segmentation performances on three biological image datasets\ncompared to state-of-the-art baseline methods. Experimental results demonstrate\nthat the proposed RPR module is effective in both anchor-based and anchor-free\ntop-down instance segmentation approaches, suggesting the proposed method can\nbe applied to general top-down instance segmentation of biological images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhangli_Q/0/1/0/all/0/1\">Qilong Zhangli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingru Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaoxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhaoyang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haiming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Self-Supervised Category-Level Object Pose and Size Estimation. (arXiv:2203.02884v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02884","description":"<p>This work presents a self-supervised framework for category-level object pose\nand size estimation from a single depth image. Unlike previous works that rely\non time-consuming and labor-intensive ground truth pose labels for supervision,\nwe leverage the geometric consistency residing in point clouds of the same\nshape for self-supervision. Specifically, given a normalized category template\nmesh in the object-coordinate system and the partially observed object instance\nin the scene, our key idea is to apply differentiable shape deformation,\nregistration, and rendering to enforce geometric consistency between the\npredicted and the observed scene object point cloud. We evaluate our approach\non real-world datasets and find that our approach outperforms the simple\ntraditional baseline by large margins while being competitive with some\nfully-supervised approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yisheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-class Token Transformer for Weakly Supervised Semantic Segmentation. (arXiv:2203.02891v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02891","description":"<p>This paper proposes a new transformer-based framework to learn class-specific\nobject localization maps as pseudo labels for weakly supervised semantic\nsegmentation (WSSS). Inspired by the fact that the attended regions of the\none-class token in the standard vision transformer can be leveraged to form a\nclass-agnostic localization map, we investigate if the transformer model can\nalso effectively capture class-specific attention for more discriminative\nobject localization by learning multiple class tokens within the transformer.\nTo this end, we propose a Multi-class Token Transformer, termed as MCTformer,\nwhich uses multiple class tokens to learn interactions between the class tokens\nand the patch tokens. The proposed MCTformer can successfully produce\nclass-discriminative object localization maps from class-to-patch attentions\ncorresponding to different class tokens. We also propose to use a patch-level\npairwise affinity, which is extracted from the patch-to-patch transformer\nattention, to further refine the localization maps. Moreover, the proposed\nframework is shown to fully complement the Class Activation Mapping (CAM)\nmethod, leading to remarkably superior WSSS results on the PASCAL VOC and MS\nCOCO datasets. These results underline the importance of the class token for\nWSSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boussaid_F/0/1/0/all/0/1\">Farid Boussaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Framework of Chromosome Straightening with ViT-Patch GAN. (arXiv:2203.02901v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02901","description":"<p>Chromosomes exhibit non-rigid and non-articulated nature with varying degrees\nof curvature. Chromosome straightening is an essential step for subsequent\nkaryotype construction, pathological diagnosis and cytogenetic map development.\nHowever, robust chromosome straightening remains challenging, due to the\nunavailability of training images, distorted chromosome details and shapes\nafter straightening, as well as poor generalization capability. We propose a\nnovel architecture, ViT-Patch GAN, consisting of a motion transformation\ngenerator and a Vision Transformer-based patch (ViT-Patch) discriminator. The\ngenerator learns the motion representation of chromosomes for straightening.\nWith the help of the ViT-Patch discriminator, the straightened chromosomes\nretain more shape and banding pattern details. The proposed framework is\ntrained on a small dataset and is able to straighten chromosome images with\nstate-of-the-art performance for two large datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fengrui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qirui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Y/0/1/0/all/0/1\">Yihan Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yongteng Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruomai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chunxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_F/0/1/0/all/0/1\">Frans Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1\">Jia Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1\">Kang Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jionglong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Image-specific Prototype Exploration for Weakly Supervised Semantic Segmentation. (arXiv:2203.02909v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02909","description":"<p>Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels\nhas attracted much attention due to low annotation costs. Existing methods\noften rely on Class Activation Mapping (CAM) that measures the correlation\nbetween image pixels and classifier weight. However, the classifier focuses\nonly on the discriminative regions while ignoring other useful information in\neach image, resulting in incomplete localization maps. To address this issue,\nwe propose a Self-supervised Image-specific Prototype Exploration (SIPE) that\nconsists of an Image-specific Prototype Exploration (IPE) and a\nGeneral-Specific Consistency (GSC) loss. Specifically, IPE tailors prototypes\nfor every image to capture complete regions, formed our Image-Specific CAM\n(IS-CAM), which is realized by two sequential steps. In addition, GSC is\nproposed to construct the consistency of general CAM and our specific IS-CAM,\nwhich further optimizes the feature representation and empowers a\nself-correction ability of prototype exploration. Extensive experiments are\nconducted on PASCAL VOC 2012 and MS COCO 2014 segmentation benchmark and\nresults show our SIPE achieves new state-of-the-art performance using only\nimage-level labels. The code is available at\nhttps://github.com/chenqi1126/SIPE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jianhuang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohua Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Dual-task Correlation for Pose Guided Person Image Generation. (arXiv:2203.02910v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02910","description":"<p>Pose Guided Person Image Generation (PGPIG) is the task of transforming a\nperson image from the source pose to a given target pose. Most of the existing\nmethods only focus on the ill-posed source-to-target task and fail to capture\nreasonable texture mapping. To address this problem, we propose a novel\nDual-task Pose Transformer Network (DPTN), which introduces an auxiliary task\n(i.e., source-to-source task) and exploits the dual-task correlation to promote\nthe performance of PGPIG. The DPTN is of a Siamese structure, containing a\nsource-to-source self-reconstruction branch, and a transformation branch for\nsource-to-target generation. By sharing partial weights between them, the\nknowledge learned by the source-to-source task can effectively assist the\nsource-to-target learning. Furthermore, we bridge the two branches with a\nproposed Pose Transformer Module (PTM) to adaptively explore the correlation\nbetween features from dual tasks. Such correlation can establish the\nfine-grained mapping of all the pixels between the sources and the targets, and\npromote the source texture transmission to enhance the details of the generated\ntarget images. Extensive experiments show that our DPTN outperforms\nstate-of-the-arts in terms of both PSNR and LPIPS. In addition, our DPTN only\ncontains 9.79 million parameters, which is significantly smaller than other\napproaches. Our code is available at:\nhttps://github.com/PangzeCheung/Dual-task-Pose-Transformer-Network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jianhuang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohua Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanFormer: a Transformer Based Model for Pan-sharpening. (arXiv:2203.02916v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02916","description":"<p>Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS)\nimage from a low-resolution (LR) multi-spectral (MS) image and its\ncorresponding panchromatic (PAN) image acquired by a same satellite. Inspired\nby a new fashion in recent deep learning community, we propose a novel\nTransformer based model for pan-sharpening. We explore the potential of\nTransformer in image feature extraction and fusion. Following the successful\ndevelopment of vision transformers, we design a two-stream network with the\nself-attention to extract the modality-specific features from the PAN and MS\nmodalities and apply a cross-attention module to merge the spectral and spatial\nfeatures. The pan-sharpened image is produced from the enhanced fused features.\nExtensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our\nTransformer based model achieves impressive results and outperforms many\nexisting CNN based methods, which shows the great potential of introducing\nTransformer to the pan-sharpening task. Codes are available at\nhttps://github.com/zhysora/PanFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huanyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02925","description":"<p>Weakly supervised temporal action localization aims to localize temporal\nboundaries of actions and simultaneously identify their categories with only\nvideo-level category labels. Many existing methods seek to generate pseudo\nlabels for bridging the discrepancy between classification and localization,\nbut usually only make use of limited contextual information for pseudo label\ngeneration. To alleviate this problem, we propose a representative snippet\nsummarization and propagation framework. Our method seeks to mine the\nrepresentative snippets in each video for propagating information between video\nsnippets to generate better pseudo labels. For each video, its own\nrepresentative snippets and the representative snippets from a memory bank are\npropagated to update the input features in an intra- and inter-video manner.\nThe pseudo labels are generated from the temporal class activation maps of the\nupdated features to rectify the predictions of the main branch. Our method\nobtains superior performance in comparison to the existing methods on two\nbenchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in\nterms of average mAP on THUMOS14.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linjiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (arXiv:2203.02928v1 [cs.LG])","link":"http://arxiv.org/abs/2203.02928","description":"<p>The challenge of interpreting predictions from deep neural networks has\nprompted the development of numerous interpretability methods. Many of\ninterpretability methods attempt to quantify the importance of input features\nwith respect to the class probabilities, and are called importance estimators\nor saliency maps. A popular approach to evaluate such interpretability methods\nis to perturb input features deemed important for predictions and observe the\ndecrease in accuracy. However, perturbation-based evaluation methods may\nconfound the sources of accuracy degradation. We conduct computational\nexperiments that allow to empirically estimate the $\\textit{fidelity}$ of\ninterpretability methods and the contribution of perturbation artifacts. All\nconsidered importance estimators clearly outperform a random baseline, which\ncontradicts the findings of ROAR [<a href=\"/abs/1806.10758\">arXiv:1806.10758</a>]. We further compare our\nresults to the crop-and-resize evaluation framework [<a href=\"/abs/1705.07857\">arXiv:1705.07857</a>], which\nare largely in agreement. Our study suggests that we can estimate the impact of\nartifacts and thus empirically evaluate interpretability methods without\nretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brocki_L/0/1/0/all/0/1\">Lennart Brocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Neo Christopher Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Parasitic Eggs from Microscopy Images and the emergence of a new dataset. (arXiv:2203.02940v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02940","description":"<p>Automatic detection of parasitic eggs in microscopy images has the potential\nto increase the efficiency of human experts whilst also providing an objective\nassessment. The time saved by such a process would both help ensure a prompt\ntreatment to patients, and off-load excessive work from experts' shoulders.\nAdvances in deep learning inspired us to exploit successful architectures for\ndetection, adapting them to tackle a different domain. We propose a framework\nthat exploits two such state-of-the-art models. Specifically, we demonstrate\nresults produced by both a Generative Adversarial Network (GAN) and\nFaster-RCNN, for image enhancement and object detection respectively, on\nmicroscopy images of varying quality. The use of these techniques yields\nencouraging results, though further improvements are still needed for certain\negg types whose detection still proves challenging. As a result, a new dataset\nhas been created and made publicly available, providing an even wider range of\nclasses and variability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mayo_P/0/1/0/all/0/1\">Perla Mayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achim_A/0/1/0/all/0/1\">Alin Achim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Steering Multi-Annotations per Sample for Multi-Task Learning. (arXiv:2203.02946v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02946","description":"<p>The study of multi-task learning has drawn great attention from the\ncommunity. Despite the remarkable progress, the challenge of optimally learning\ndifferent tasks simultaneously remains to be explored. Previous works attempt\nto modify the gradients from different tasks. Yet these methods give a\nsubjective assumption of the relationship between tasks, and the modified\ngradient may be less accurate. In this paper, we introduce Stochastic Task\nAllocation~(STA), a mechanism that addresses this issue by a task allocation\napproach, in which each sample is randomly allocated a subset of tasks. For\nfurther progress, we propose Interleaved Stochastic Task Allocation~(ISTA) to\niteratively allocate all tasks to each example during several consecutive\niterations. We evaluate STA and ISTA on various datasets and applications:\nNYUv2, Cityscapes, and COCO for scene understanding and instance segmentation.\nOur experiments show both STA and ISTA outperform current state-of-the-art\nmethods. The code will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qizhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Precise Point Spread Function Estimation. (arXiv:2203.02953v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02953","description":"<p>Point spread function (PSF) plays a crucial role in many fields, such as\nshape from focus/defocus, depth estimation, and imaging process in fluorescence\nmicroscopy. However, the mathematical model of the defocus process is still\nunclear because several variables in the point spread function are hard to\nmeasure accurately, such as the f-number of cameras, the physical size of a\npixel, the focus depth, etc. In this work, we develop a precise mathematical\nmodel of the camera's point spread function to describe the defocus process. We\nfirst derive the mathematical algorithm for the PSF and extract two parameters\nA and e. A is the composite of camera's f-number, pixel-size, output scale, and\nscaling factor of the circle of confusion; e is the deviation of the focus\ndepth. We design a novel metric based on the defocus histogram to evaluate the\ndifference between the simulated focused image and the actual focused image to\nobtain optimal A and e. We also construct a hardware system consisting of a\nfocusing system and a structured light system to acquire the all-in-focus\nimage, the focused image with corresponding focus depth, and the depth map in\nthe same view. The three types of images, as a dataset, are used to obtain the\nprecise PSF. Our experiments on standard planes and actual objects show that\nthe proposed algorithm can accurately describe the defocus process. The\naccuracy of our algorithm is further proved by evaluating the difference among\nthe actual focused images, the focused image generated by our algorithm, the\nfocused image generated by others. The results show that the loss of our\nalgorithm is 40% less than others on average. The dataset, code, and model are\navailable on GitHub: https://github.com/cubhe/\nprecise-point-spread-function-estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Renzhi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1\">Boya Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Perspective on Robotic Telepresence and Teleoperation using Cognition: Are we there yet?. (arXiv:2203.02959v1 [cs.RO])","link":"http://arxiv.org/abs/2203.02959","description":"<p>Telepresence and teleoperation robotics have attracted a great amount of\nattention in the last 10 years. With the Artificial Intelligence (AI)\nrevolution already being started, we can see a wide range of robotic\napplications being realized. Intelligent robotic systems are being deployed\nboth in industrial and domestic environments. Telepresence is the idea of being\npresent in a remote location virtually or via robotic avatars. Similarly, the\nidea of operating a robot from a remote location for various tasks is called\nteleoperation. These technologies find significant application in health care,\neducation, surveillance, disaster recovery, and corporate/government sectors.\nBut question still remains about their maturity, security and safety levels. We\nalso need to think about enhancing the user experience and trust in such\ntechnologies going into the next generation of computing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1\">Hrishav Bakul Barua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sau_A/0/1/0/all/0/1\">Ashis Sau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roychoudhury_R/0/1/0/all/0/1\">Ruddra dev Roychoudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for Temporal Sentence Grounding. (arXiv:2203.02966v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02966","description":"<p>Temporal sentence grounding aims to localize a target segment in an untrimmed\nvideo semantically according to a given sentence query. Most previous works\nfocus on learning frame-level features of each whole frame in the entire video,\nand directly match them with the textual information. Such frame-level feature\nextraction leads to the obstacles of these methods in distinguishing ambiguous\nvideo frames with complicated contents and subtle appearance differences, thus\nlimiting their performance. In order to differentiate fine-grained appearance\nsimilarities among consecutive frames, some state-of-the-art methods\nadditionally employ a detection model like Faster R-CNN to obtain detailed\nobject-level features in each frame for filtering out the redundant background\ncontents. However, these methods suffer from missing motion analysis since the\nobject detection module in Faster R-CNN lacks temporal modeling. To alleviate\nthe above limitations, in this paper, we propose a novel Motion- and\nAppearance-guided 3D Semantic Reasoning Network (MA3SRN), which incorporates\noptical-flow-guided motion-aware, detection-based appearance-aware, and\n3D-aware object-level features to better reason the spatial-temporal object\nrelations for accurately modelling the activity among consecutive frames.\nSpecifically, we first develop three individual branches for motion,\nappearance, and 3D encoding separately to learn fine-grained motion-guided,\nappearance-guided, and 3D-aware object features, respectively. Then, both\nmotion and appearance information from corresponding branches are associated to\nenhance the 3D-aware features for the final precise grounding. Extensive\nexperiments on three challenging datasets (ActivityNet Caption, Charades-STA\nand TACoS) demonstrate that the proposed MA3SRN model achieves a new\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Key-value Memory Enhanced Multi-step Graph Reasoning for Knowledge-based Visual Question Answering. (arXiv:2203.02985v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02985","description":"<p>Knowledge-based visual question answering (VQA) is a vision-language task\nthat requires an agent to correctly answer image-related questions using\nknowledge that is not presented in the given image. It is not only a more\nchallenging task than regular VQA but also a vital step towards building a\ngeneral VQA system. Most existing knowledge-based VQA systems process knowledge\nand image information similarly and ignore the fact that the knowledge base\n(KB) contains complete information about a triplet, while the extracted image\ninformation might be incomplete as the relations between two objects are\nmissing or wrongly detected. In this paper, we propose a novel model named\ndynamic knowledge memory enhanced multi-step graph reasoning (DMMGR), which\nperforms explicit and implicit reasoning over a key-value knowledge memory\nmodule and a spatial-aware image graph, respectively. Specifically, the memory\nmodule learns a dynamic knowledge representation and generates a\nknowledge-aware question representation at each reasoning step. Then, this\nrepresentation is used to guide a graph attention operator over the\nspatial-aware image graph. Our model achieves new state-of-the-art accuracy on\nthe KRVQR and FVQA datasets. We also conduct ablation experiments to prove the\neffectiveness of each component of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Coreference Relations in Visual Dialog. (arXiv:2203.02986v1 [cs.CV])","link":"http://arxiv.org/abs/2203.02986","description":"<p>Visual dialog is a vision-language task where an agent needs to answer a\nseries of questions grounded in an image based on the understanding of the\ndialog history and the image. The occurrences of coreference relations in the\ndialog makes it a more challenging task than visual question-answering. Most\nprevious works have focused on learning better multi-modal representations or\non exploring different ways of fusing visual and language features, while the\ncoreferences in the dialog are mainly ignored. In this paper, based on\nlinguistic knowledge and discourse features of human dialog we propose two soft\nconstraints that can improve the model's ability of resolving coreferences in\ndialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset\nshows that our model, which integrates two novel and linguistically inspired\nsoft constraints in a deep transformer neural architecture, obtains new\nstate-of-the-art performance in terms of recall at 1 and other evaluation\nmetrics compared to current existing models and this without pretraining on\nother vision-language datasets. Our qualitative results also demonstrate the\neffectiveness of the method that we propose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Aware Latent Space Exploration for Face Image Restoration. (arXiv:2203.03005v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03005","description":"<p>For image restoration, most existing deep learning based methods tend to\noverfit the training data leading to bad results when encountering unseen\ndegradations out of the assumptions for training. To improve the robustness,\ngenerative adversarial network (GAN) prior based methods have been proposed,\nrevealing a promising capability to restore photo-realistic and high-quality\nresults. But these methods suffer from semantic confusion, especially on\nsemantically significant images such as face images. In this paper, we propose\na semantic-aware latent space exploration method for image restoration (SAIR).\nBy explicitly modeling referenced semantics information, SAIR can consistently\nrestore severely degraded images not only to high-resolution highly-realistic\nlooks but also to correct semantics. Quantitative and qualitative experiments\ncollectively demonstrate the effectiveness of the proposed SAIR. Our code can\nbe found in https://github.com/Liamkuo/SAIR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fangzhou Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaolin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos. (arXiv:2203.03014v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03014","description":"<p>With the assumption that a video dataset is multimodality annotated in which\nauditory and visual modalities both are labeled or class-relevant, current\nmultimodal methods apply modality fusion or cross-modality attention. However,\neffectively leveraging the audio modality in vision-specific annotated videos\nfor action recognition is of particular challenge. To tackle this challenge, we\npropose a novel audio-visual framework that effectively leverages the audio\nmodality in any solely vision-specific annotated dataset. We adopt the language\nmodels (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD)\nthat maps each video label to its most K-relevant audio labels in which SAVLD\nserves as a bridge between audio and video datasets. Then, SAVLD along with a\npretrained audio multi-label model are used to estimate the audio-visual\nmodality relevance during the training phase. Accordingly, a novel learnable\nirrelevant modality dropout (IMD) is proposed to completely drop out the\nirrelevant audio modality and fuse only the relevant modalities. Moreover, we\npresent a new two-stream video Transformer for efficiently modeling the visual\nmodalities. Results on several vision-specific annotated datasets including\nKinetics400 and UCF-101 validated our framework as it outperforms most relevant\naction recognition methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfasly_S/0/1/0/all/0/1\">Saghir Alfasly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuru Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03041","description":"<p>We present a systematic study on a new task called dichotomous image\nsegmentation (DIS), which aims to segment highly accurate objects from natural\nimages. To this end, we collected the first large-scale dataset, called DIS5K,\nwhich contains 5,470 high-resolution (e.g., 2K, 4K or larger) images covering\ncamouflaged, salient, or meticulous objects in various backgrounds. All images\nare annotated with extremely fine-grained labels. In addition, we introduce a\nsimple intermediate supervision baseline (IS-Net) using both feature-level and\nmask-level guidance for DIS model training. Without tricks, IS-Net outperforms\nvarious cutting-edge baselines on the proposed DIS5K, making it a general\nself-learned supervision network that can help facilitate future research in\nDIS. Further, we design a new metric called human correction efforts (HCE)\nwhich approximates the number of mouse clicking operations required to correct\nthe false positives and false negatives. HCE is utilized to measure the gap\nbetween models and real-world applications and thus can complement existing\nmetrics. Finally, we conduct the largest-scale benchmark, evaluating 16\nrepresentative segmentation models, providing a more insightful discussion\nregarding object complexities, and showing several potential applications\n(e.g., background removal, art design, 3D reconstruction). Hoping these efforts\ncan open up promising directions for both academic and industries. We will\nrelease our DIS5Kdataset, IS-Net baseline, HCE metric, and the complete\nbenchmark results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xuebin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaobin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_a/0/1/0/all/0/1\">and Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social-Implicit: Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation. (arXiv:2203.03057v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03057","description":"<p>Best-of-N (BoN) Average Displacement Error (ADE)/ Final Displacement Error\n(FDE) is the most used metric for evaluating trajectory prediction models. Yet,\nthe BoN does not quantify the whole generated samples, resulting in an\nincomplete view of the model's prediction quality and performance. We propose a\nnew metric, Average Mahalanobis Distance (AMD) to tackle this issue. AMD is a\nmetric that quantifies how close the whole generated samples are to the ground\ntruth. We also introduce the Average Maximum Eigenvalue (AMV) metric that\nquantifies the overall spread of the predictions. Our metrics are validated\nempirically by showing that the ADE/FDE is not sensitive to distribution\nshifts, giving a biased sense of accuracy, unlike the AMD/AMV metrics. We\nintroduce the usage of Implicit Maximum Likelihood Estimation (IMLE) as a\nreplacement for traditional generative models to train our model,\nSocial-Implicit. IMLE training mechanism aligns with AMD/AMV objective of\npredicting trajectories that are close to the ground truth with a tight spread.\nSocial-Implicit is a memory efficient deep model with only 5.8K parameters that\nruns in real time of about 580Hz and achieves competitive results. Interactive\ndemo of the problem can be seen here\n\\url{https://www.abduallahmohamed.com/social-implicit-amdamv-adefde-demo}. Code\nis available at \\url{https://github.com/abduallahmohamed/Social-Implicit}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abduallah Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Deyao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_W/0/1/0/all/0/1\">Warren Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claudel_C/0/1/0/all/0/1\">Christian Claudel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual vs. Reality: External Validation of COVID-19 Classifiers using XCAT Phantoms for Chest Computed Tomography. (arXiv:2203.03074v1 [eess.IV])","link":"http://arxiv.org/abs/2203.03074","description":"<p>Research studies of artificial intelligence models in medical imaging have\nbeen hampered by poor generalization. This problem has been especially\nconcerning over the last year with numerous applications of deep learning for\nCOVID-19 diagnosis. Virtual imaging trials (VITs) could provide a solution for\nobjective evaluation of these models. In this work utilizing the VITs, we\ncreated the CVIT-COVID dataset including 180 virtually imaged computed\ntomography (CT) images from simulated COVID-19 and normal phantom models under\ndifferent COVID-19 morphology and imaging properties. We evaluated the\nperformance of an open-source, deep-learning model from the University of\nWaterloo trained with multi-institutional data and an in-house model trained\nwith the open clinical dataset called MosMed. We further validated the model's\nperformance against open clinical data of 305 CT images to understand virtual\nvs. real clinical data performance. The open-source model was published with\nnearly perfect performance on the original Waterloo dataset but showed a\nconsistent performance drop in external testing on another clinical dataset\n(AUC=0.77) and our simulated CVIT-COVID dataset (AUC=0.55). The in-house model\nachieved an AUC of 0.87 while testing on the internal test set (MosMed test\nset). However, performance dropped to an AUC of 0.65 and 0.69 when evaluated on\nclinical and our simulated CVIT-COVID dataset. The VIT framework offered\ncontrol over imaging conditions, allowing us to show there was no change in\nperformance as CT exposure was changed from 28.5 to 57 mAs. The VIT framework\nalso provided voxel-level ground truth, revealing that performance of in-house\nmodel was much higher at AUC=0.87 for diffuse COVID-19 infection size &gt;2.65%\nlung volume versus AUC=0.52 for focal disease with &lt;2.65% volume. The virtual\nimaging framework enabled these uniquely rigorous analyses of model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tushar_F/0/1/0/all/0/1\">Fakrul Islam Tushar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abadi_E/0/1/0/all/0/1\">Ehsan Abadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sotoudeh_Paima_S/0/1/0/all/0/1\">Saman Sotoudeh-Paima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fricks_R/0/1/0/all/0/1\">Rafael B. Fricks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Segars_W/0/1/0/all/0/1\">W. Paul Segars</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samei_E/0/1/0/all/0/1\">Ehsan Samei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lo_J/0/1/0/all/0/1\">Joseph Y. Lo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GlideNet: Global, Local and Intrinsic based Dense Embedding NETwork for Multi-category Attributes Prediction. (arXiv:2203.03079v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03079","description":"<p>Attaching attributes (such as color, shape, state, action) to object\ncategories is an important computer vision problem. Attribute prediction has\nseen exciting recent progress and is often formulated as a multi-label\nclassification problem. Yet significant challenges remain in: 1) predicting\ndiverse attributes over multiple categories, 2) modeling attributes-category\ndependency, 3) capturing both global and local scene context, and 4) predicting\nattributes of objects with low pixel-count. To address these issues, we propose\na novel multi-category attribute prediction deep architecture named GlideNet,\nwhich contains three distinct feature extractors. A global feature extractor\nrecognizes what objects are present in a scene, whereas a local one focuses on\nthe area surrounding the object of interest. Meanwhile, an intrinsic feature\nextractor uses an extension of standard convolution dubbed Informed Convolution\nto retrieve features of objects with low pixel-count. GlideNet uses gating\nmechanisms with binary masks and its self-learned category embedding to combine\nthe dense embeddings. Collectively, the Global-Local-Intrinsic blocks\ncomprehend the scene's global context while attending to the characteristics of\nthe local object of interest. Finally, using the combined features, an\ninterpreter predicts the attributes, and the length of the output is determined\nby the category, thereby removing unnecessary attributes. GlideNet can achieve\ncompelling results on two recent and challenging datasets -- VAW and CAR -- for\nlarge-scale attribute prediction. For instance, it obtains more than 5\\% gain\nover state of the art in the mean recall (mR) metric. GlideNet's advantages are\nespecially apparent when predicting attributes of objects with low pixel counts\nas well as attributes that demand global context understanding. Finally, we\nshow that GlideNet excels in training starved real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metwaly_K/0/1/0/all/0/1\">Kareem Metwaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Aerin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Branson_E/0/1/0/all/0/1\">Elliot Branson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HAR-GCNN: Deep Graph CNNs for Human Activity Recognition From Highly Unlabeled Mobile Sensor Data. (arXiv:2203.03087v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03087","description":"<p>The problem of human activity recognition from mobile sensor data applies to\nmultiple domains, such as health monitoring, personal fitness, daily life\nlogging, and senior care. A critical challenge for training human activity\nrecognition models is data quality. Acquiring balanced datasets containing\naccurate activity labels requires humans to correctly annotate and potentially\ninterfere with the subjects' normal activities in real-time. Despite the\nlikelihood of incorrect annotation or lack thereof, there is often an inherent\nchronology to human behavior. For example, we take a shower after we exercise.\nThis implicit chronology can be used to learn unknown labels and classify\nfuture activities. In this work, we propose HAR-GCCN, a deep graph CNN model\nthat leverages the correlation between chronologically adjacent sensor\nmeasurements to predict the correct labels for unclassified activities that\nhave at least one activity label. We propose a new training strategy enforcing\nthat the model predicts the missing activity labels by leveraging the known\nones. HAR-GCCN shows superior performance relative to previously used baseline\nmethods, improving classification accuracy by about 25% and up to 68% on\ndifferent datasets. Code is available at\n\\url{https://github.com/abduallahmohamed/HAR-GCNN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abduallah Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lejarza_F/0/1/0/all/0/1\">Fernando Lejarza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahail_S/0/1/0/all/0/1\">Stephanie Cahail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claudel_C/0/1/0/all/0/1\">Christian Claudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomaz_E/0/1/0/all/0/1\">Edison Thomaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPPF: Towards Robust Category-Level 9D Pose Estimation in the Wild. (arXiv:2203.03089v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03089","description":"<p>In this paper, we tackle the problem of category-level 9D pose estimation in\nthe wild, given a single RGB-D frame. Using supervised data of real-world 9D\nposes is tedious and erroneous, and also fails to generalize to unseen\nscenarios. Besides, category-level pose estimation requires a method to be able\nto generalize to unseen objects at test time, which is also challenging.\nDrawing inspirations from traditional point pair features (PPFs), in this\npaper, we design a novel Category-level PPF (CPPF) voting method to achieve\naccurate, robust and generalizable 9D pose estimation in the wild. To obtain\nrobust pose estimation, we sample numerous point pairs on an object, and for\neach pair our model predicts necessary SE(3)-invariant voting statistics on\nobject centers, orientations and scales. A novel coarse-to-fine voting\nalgorithm is proposed to eliminate noisy point pair samples and generate final\npredictions from the population. To get rid of false positives in the\norientation voting process, an auxiliary binary disambiguating classification\ntask is introduced for each sampled point pair. In order to detect objects in\nthe wild, we carefully design our sim-to-real pipeline by training on synthetic\npoint clouds only, unless objects have ambiguous poses in geometry. Under this\ncircumstance, color information is leveraged to disambiguate these poses.\nResults on standard benchmarks show that our method is on par with current\nstate of the arts with real-world training data. Extensive experiments further\nshow that our method is robust to noise and gives promising results under\nextremely challenging scenarios. Our code is available on\nhttps://github.com/qq456cvb/CPPF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Ruoxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behavior Recognition Based on the Integration of Multigranular Motion Features. (arXiv:2203.03097v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03097","description":"<p>The recognition of behaviors in videos usually requires a combinatorial\nanalysis of the spatial information about objects and their dynamic action\ninformation in the temporal dimension. Specifically, behavior recognition may\neven rely more on the modeling of temporal information containing short-range\nand long-range motions; this contrasts with computer vision tasks involving\nimages that focus on the understanding of spatial information. However, current\nsolutions fail to jointly and comprehensively analyze short-range motion\nbetween adjacent frames and long-range temporal aggregations at large scales in\nvideos. In this paper, we propose a novel behavior recognition method based on\nthe integration of multigranular (IMG) motion features. In particular, we\nachieve reliable motion information modeling through the synergy of a channel\nattention-based short-term motion feature enhancement module (CMEM) and a\ncascaded long-term motion feature integration module (CLIM). We evaluate our\nmodel on several action recognition benchmarks such as HMDB51,\nSomething-Something and UCF101. The experimental results demonstrate that our\napproach outperforms the previous state-of-the-art methods, which confirms its\neffectiveness and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lizong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1\">Bei Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shuxin Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Federated Learning with Local Regularization and Sparsification. (arXiv:2203.03106v1 [cs.LG])","link":"http://arxiv.org/abs/2203.03106","description":"<p>User-level differential privacy (DP) provides certifiable privacy guarantees\nto the information that is specific to any user's data in federated learning.\nExisting methods that ensure user-level DP come at the cost of severe accuracy\ndecrease. In this paper, we study the cause of model performance degradation in\nfederated learning under user-level DP guarantee. We find the key to solving\nthis issue is to naturally restrict the norm of local updates before executing\noperations that guarantee DP. To this end, we propose two techniques, Bounded\nLocal Update Regularization and Local Update Sparsification, to increase model\nquality without sacrificing privacy. We provide theoretical analysis on the\nconvergence of our framework and give rigorous privacy guarantees. Extensive\nexperiments show that our framework significantly improves the privacy-utility\ntrade-off over the state-of-the-arts for federated learning with user-level DP\nguarantee.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">Anda Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peisong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Sheryl Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer. (arXiv:2203.03121v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03121","description":"<p>While deep face recognition (FR) systems have shown amazing performance in\nidentification and verification, they also arouse privacy concerns for their\nexcessive surveillance on users, especially for public face images widely\nspread on social networks. Recently, some studies adopt adversarial examples to\nprotect photos from being identified by unauthorized face recognition systems.\nHowever, existing methods of generating adversarial face images suffer from\nmany limitations, such as awkward visual, white-box setting, weak\ntransferability, making them difficult to be applied to protect face privacy in\nreality. In this paper, we propose adversarial makeup transfer GAN (AMT-GAN), a\nnovel face protection method aiming at constructing adversarial face images\nthat preserve stronger black-box transferability and better visual quality\nsimultaneously. AMT-GAN leverages generative adversarial networks (GAN) to\nsynthesize adversarial face images with makeup transferred from reference\nimages. In particular, we introduce a new regularization module along with a\njoint training strategy to reconcile the conflicts between the adversarial\nnoises and the cycle consistence loss in makeup transfer, achieving a desirable\nbalance between the attack strength and visual changes. Extensive experiments\nverify that compared with state of the arts, AMT-GAN can not only preserve a\ncomfortable visual quality, but also achieve a higher attack success rate over\ncommercial FR APIs, including Face++, Aliyun, and Microsoft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengshan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaogeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yechao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leo Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Libing Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning. (arXiv:2203.03137v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03137","description":"<p>The key challenge of zero-shot learning (ZSL) is how to infer the latent\nsemantic knowledge between visual and attribute features on seen classes, and\nthus achieving a desirable knowledge transfer to unseen classes. Prior works\neither simply align the global features of an image with its associated class\nsemantic vector or utilize unidirectional attention to learn the limited latent\nsemantic representations, which could not effectively discover the intrinsic\nsemantic knowledge e.g., attribute semantics) between visual and attribute\nfeatures. To solve the above dilemma, we propose a Mutually Semantic\nDistillation Network (MSDN), which progressively distills the intrinsic\nsemantic representations between visual and attribute features for ZSL. MSDN\nincorporates an attribute$\\rightarrow$visual attention sub-net that learns\nattribute-based visual features, and a visual$\\rightarrow$attribute attention\nsub-net that learns visual-based attribute features. By further introducing a\nsemantic distillation loss, the two mutual attention sub-nets are capable of\nlearning collaboratively and teaching each other throughout the training\nprocess. The proposed MSDN yields significant improvements over the strong\nbaselines, leading to new state-of-the-art performances on three popular\nchallenging benchmarks, i.e., CUB, SUN, and AWA2. Our codes have been available\nat: \\url{https://github.com/shiming-chen/MSDN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Ziming Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Sen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end video instance segmentation via spatial-temporal graph neural networks. (arXiv:2203.03145v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03145","description":"<p>Video instance segmentation is a challenging task that extends image instance\nsegmentation to the video domain. Existing methods either rely only on\nsingle-frame information for the detection and segmentation subproblems or\nhandle tracking as a separate post-processing step, which limit their\ncapability to fully leverage and share useful spatial-temporal information for\nall the subproblems. In this paper, we propose a novel graph-neural-network\n(GNN) based method to handle the aforementioned limitation. Specifically, graph\nnodes representing instance features are used for detection and segmentation\nwhile graph edges representing instance relations are used for tracking. Both\ninter and intra-frame information is effectively propagated and shared via\ngraph updates and all the subproblems (i.e. detection, segmentation and\ntracking) are jointly optimized in an unified framework. The performance of our\nmethod shows great improvement on the YoutubeVIS validation dataset compared to\nexisting methods and achieves 35.2% AP with a ResNet-50 backbone, operating at\n22 FPS. Code is available at <a href=\"http://github.com/lucaswithai/visgraph.git\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kean Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Construction of Distribution-Free Prediction Intervals for an Image Regression Problem in Semiconductor Manufacturing. (arXiv:2203.03150v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03150","description":"<p>The high-volume manufacturing of the next generation of semiconductor devices\nrequires advances in measurement signal analysis. Many in the semiconductor\nmanufacturing community have reservations about the adoption of deep learning;\nthey instead prefer other model-based approaches for some image regression\nproblems, and according to the 2021 IEEE International Roadmap for Devices and\nSystems (IRDS) report on Metrology a SEMI standardization committee may endorse\nthis philosophy. The semiconductor manufacturing community does, however,\ncommunicate a need for state-of-the-art statistical analyses to reduce\nmeasurement uncertainty. Prediction intervals which characterize the\nreliability of the predictive performance of regression models can impact\ndecisions, build trust in machine learning, and be applied to other regression\nmodels. However, we are not aware of effective and sufficiently simple\ndistribution-free approaches that offer valid coverage for important classes of\nimage data, so we consider the distribution-free conformal prediction and\nconformalized quantile regression framework.The image regression problem that\nis the focus of this paper pertains to line edge roughness (LER) estimation\nfrom noisy scanning electron microscopy images. LER affects semiconductor\ndevice performance and reliability as well as the yield of the manufacturing\nprocess; the 2021 IRDS emphasizes the crucial importance of LER by devoting a\nwhite paper to it in addition to mentioning or discussing it in the reports of\nmultiple international focus teams. It is not immediately apparent how to\neffectively use normalized conformal prediction and quantile regression for LER\nestimation. The modeling techniques we apply appear to be novel for finding\ndistribution-free prediction intervals for image data and will be presented at\nthe 2022 SEMI Advanced Semiconductor Manufacturing Conference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akpabio_I/0/1/0/all/0/1\">Inimfon I. Akpabio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savari_S/0/1/0/all/0/1\">Serap A. Savari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SingleSketch2Mesh : Generating 3D Mesh model from Sketch. (arXiv:2203.03157v1 [cs.CV])","link":"http://arxiv.org/abs/2203.03157","description":"<p>Sketching is an important activity in any design process. Designers and\nstakeholders share their ideas through hand-drawn sketches. These sketches are\nfurther used to create 3D models. Current methods to generate 3D models from\nsketches are either manual or tightly coupled with 3D modeling platforms.\nTherefore, it requires users to have an experience of sketching on such\nplatform. Moreover, most of the existing approaches are based on geometric\nmanipulation and thus cannot be generalized. We propose a novel AI based\nensemble approach, SingleSketch2Mesh, for generating 3D models from hand-drawn\nsketches. Our approach is based on Generative Networks and Encoder-Decoder\nArchitecture to generate 3D mesh model from a hand-drawn sketch. We evaluate\nour solution with existing solutions. Our approach outperforms existing\napproaches on both - quantitative and qualitative evaluation criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_N/0/1/0/all/0/1\">Nitish Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_D/0/1/0/all/0/1\">Dhornala Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Alpana Dubey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satellite Image-based Localization via Learned Embeddings. (arXiv:1704.01133v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1704.01133","description":"<p>We propose a vision-based method that localizes a ground vehicle using\npublicly available satellite imagery as the only prior knowledge of the\nenvironment. Our approach takes as input a sequence of ground-level images\nacquired by the vehicle as it navigates, and outputs an estimate of the\nvehicle's pose relative to a georeferenced satellite image. We overcome the\nsignificant viewpoint and appearance variations between the images through a\nneural multi-view model that learns location-discriminative embeddings in which\nground-level images are matched with their corresponding satellite view of the\nscene. We use this learned function as an observation model in a filtering\nframework to maintain a distribution over the vehicle's pose. We evaluate our\nmethod on different benchmark datasets and demonstrate its ability localize\nground-level images in environments novel relative to training, despite the\nchallenges of significant viewpoint and appearance variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Ki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_M/0/1/0/all/0/1\">Matthew R. Walter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theme-Aware Aesthetic Distribution Prediction with Full Resolution Photos. (arXiv:1908.01308v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1908.01308","description":"<p>Aesthetic quality assessment (AQA) is a challenging task due to complex\naesthetic factors. Currently, it is common to conduct AQA using deep neural\nnetworks that require fixed-size inputs. Existing methods mainly transform\nimages by resizing, cropping, and padding or employ adaptive pooling to\nalternately capture the aesthetic features from fixed-size inputs. However,\nthese transformations potentially damage aesthetic features. To address this\nissue, we propose a simple but effective method to accomplish full-resolution\nimage AQA by combining image padding with region of image (RoM) pooling.\nPadding turns inputs into the same size. RoM pooling pools image features and\ndiscards extra padded features to eliminate the side effects of padding. In\naddition, the image aspect ratios are encoded and fused with visual features to\nremedy the shape information loss of RoM pooling. Furthermore, we observe that\nthe same image may receive different aesthetic evaluations under different\nthemes, which we call theme criterion bias. Hence, a theme-aware model that\nuses theme information to guide model predictions is proposed. Finally, we\ndesign an attention-based feature fusion module to effectively utilize both the\nshape and theme information. Extensive experiments prove the effectiveness of\nthe proposed method over state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Gengyun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peipei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ran He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Compensation for Deep Conditional Generative Networks. (arXiv:2001.08559v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2001.08559","description":"<p>In recent years, unsupervised/weakly-supervised conditional generative\nadversarial networks (GANs) have achieved many successes on the task of\nmodeling and generating data. However, one of their weaknesses lies in their\npoor ability to separate, or disentangle, the different factors that\ncharacterize the representation encoded in their latent space. To address this\nissue, we propose a novel structure for unsupervised conditional GANs powered\nby a novel Information Compensation Connection (IC-Connection). The proposed\nIC-Connection enables GANs to compensate for information loss incurred during\ndeconvolution operations. In addition, to quantify the degree of\ndisentanglement on both discrete and continuous latent variables, we design a\nnovel evaluation procedure. Our empirical results suggest that our method\nachieves better disentanglement compared to the state-of-the-art GANs in a\nconditional generation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zehao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oramas_J/0/1/0/all/0/1\">Jose Oramas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary Dynamic Vision Sensors. (arXiv:2006.00422v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.00422","description":"<p>As an alternative sensing paradigm, dynamic vision sensors (DVS) have been\nrecently explored to tackle scenarios where conventional sensors result in high\ndata rate and processing time. This paper presents a hybrid event-frame\napproach for detecting and tracking objects recorded by a stationary\nneuromorphic sensor, thereby exploiting the sparse DVS output in a low-power\nsetting for traffic monitoring. Specifically, we propose a hardware efficient\nprocessing pipeline that optimizes memory and computational needs that enable\nlong-term battery powered usage for IoT applications. To exploit the background\nremoval property of a static DVS, we propose an event-based binary image\ncreation that signals presence or absence of events in a frame duration. This\nreduces memory requirement and enables usage of simple algorithms like median\nfiltering and connected component labeling for denoise and region proposal\nrespectively. To overcome the fragmentation issue, a YOLO inspired neural\nnetwork based detector and classifier to merge fragmented region proposals has\nbeen proposed. Finally, a new overlap based tracker was implemented, exploiting\noverlap between detections and tracks is proposed with heuristics to overcome\nocclusion. The proposed pipeline is evaluated with more than 5 hours of traffic\nrecording spanning three different locations on two different neuromorphic\nsensors (DVS and CeleX) and demonstrate similar performance. Compared to\nexisting event-based feature trackers, our method provides similar accuracy\nwhile needing approx 6 times less computes. To the best of our knowledge, this\nis the first time a stationary DVS based traffic monitoring solution is\nextensively compared to simultaneously recorded RGB frame-based methods while\nshowing tremendous promise by outperforming state-of-the-art deep learning\nsolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohan_V/0/1/0/all/0/1\">Vivek Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_D/0/1/0/all/0/1\">Deepak Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulluri_T/0/1/0/all/0/1\">Tarun Pulluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ussa_A/0/1/0/all/0/1\">Andres Ussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_P/0/1/0/all/0/1\">Pradeep Kumar Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Pao-Sheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_B/0/1/0/all/0/1\">Bharath Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1\">Arindam Basu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks. (arXiv:2006.04270v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.04270","description":"<p>Dropout is a well-known regularization method by sampling a sub-network from\na larger deep neural network and training different sub-networks on different\nsubsets of the data. Inspired by the dropout concept, we propose EDropout as an\nenergy-based framework for pruning neural networks in classification tasks. In\nthis approach, a set of binary pruning state vectors (population) represents a\nset of corresponding sub-networks from an arbitrary provided original neural\nnetwork. An energy loss function assigns a scalar energy loss value to each\npruning state. The energy-based model stochastically evolves the population to\nfind states with lower energy loss. The best pruning state is then selected and\napplied to the original network. Similar to dropout, the kept weights are\nupdated using backpropagation in a probabilistic model. The energy-based model\nagain searches for better pruning states and the cycle continuous. Indeed, this\nprocedure is in fact switching between the energy model, which manages the\npruning states, and the probabilistic model, which updates the temporarily\nunpruned weights, in each iteration. The population can dynamically converge to\na pruning state. This can be interpreted as dropout leading to pruning the\nnetwork. From an implementation perspective, EDropout can prune typical neural\nnetworks without modification of the network architecture. We evaluated the\nproposed method on different flavours of ResNets, AlexNet, and SqueezeNet on\nthe Kuzushiji, Fashion, CIFAR-10, CIFAR-100, and Flowers datasets, and compared\nthe pruning rate and classification performance of the models. On average the\nnetworks trained with EDropout achieved a pruning rate of more than $50\\%$ of\nthe trainable parameters with approximately $&lt;5\\%$ and $&lt;1\\%$ drop of Top-1 and\nTop-5 classification accuracy, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salehinejad_H/0/1/0/all/0/1\">Hojjat Salehinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valaee_S/0/1/0/all/0/1\">Shahrokh Valaee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisImages: A Fine-Grained Expert-Annotated Visualization Dataset. (arXiv:2007.04584v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.04584","description":"<p>Images in visualization publications contain rich information, e.g., novel\nvisualization designs and implicit design patterns of visualizations. A\nsystematic collection of these images can contribute to the community in many\naspects, such as literature analysis and automated tasks for visualization. In\nthis paper, we build and make public a dataset, VisImages, which collects\n12,267 images with captions from 1,397 papers in IEEE InfoVis and VAST. Built\nupon a comprehensive visualization taxonomy, the dataset includes 35,096\nvisualizations and their bounding boxes in the images.We demonstrate the\nusefulness of VisImages through three use cases: 1) investigating the use of\nvisualizations in the publications with VisImages Explorer, 2) training and\nbenchmarking models for visualization classification, and 3) localizing\nvisualizations in the visual analytics systems automatically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Dazhen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yihong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xinhuan Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Siwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Weiwei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingcai Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flower: A Friendly Federated Learning Research Framework. (arXiv:2007.14390v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.14390","description":"<p>Federated Learning (FL) has emerged as a promising technique for edge devices\nto collaboratively learn a shared prediction model, while keeping their\ntraining data on the device, thereby decoupling the ability to do machine\nlearning from the need to store the data in the cloud. However, FL is difficult\nto implement realistically, both in terms of scale and systems heterogeneity.\nAlthough there are a number of research frameworks available to simulate FL\nalgorithms, they do not support the study of scalable FL workloads on\nheterogeneous edge devices.\n</p>\n<p>In this paper, we present Flower -- a comprehensive FL framework that\ndistinguishes itself from existing platforms by offering new facilities to\nexecute large-scale FL experiments and consider richly heterogeneous FL device\nscenarios. Our experiments show Flower can perform FL experiments up to 15M in\nclient size using only a pair of high-end GPUs. Researchers can then seamlessly\nmigrate experiments to real devices to examine other parts of the design space.\nWe believe Flower provides the community with a critical new tool for FL study\nand development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beutel_D/0/1/0/all/0/1\">Daniel J. Beutel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topal_T/0/1/0/all/0/1\">Taner Topal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Akhil Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xinchi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Marques_J/0/1/0/all/0/1\">Javier Fernandez-Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sani_L/0/1/0/all/0/1\">Lorenzo Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kwing Hei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1\">Pedro Porto Buarque de Gusm&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Out of Distribution Adversarial Attack using Latent Space Poisoning. (arXiv:2012.05027v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05027","description":"<p>Traditional adversarial attacks rely upon the perturbations generated by\ngradients from the network which are generally safeguarded by gradient guided\nsearch to provide an adversarial counterpart to the network. In this paper, we\npropose a novel mechanism of generating adversarial examples where the actual\nimage is not corrupted rather its latent space representation is utilized to\ntamper with the inherent structure of the image while maintaining the\nperceptual quality intact and to act as legitimate data samples. As opposed to\ngradient-based attacks, the latent space poisoning exploits the inclination of\nclassifiers to model the independent and identical distribution of the training\ndataset and tricks it by producing out of distribution samples. We train a\ndisentangled variational autoencoder (beta-VAE) to model the data in latent\nspace and then we add noise perturbations using a class-conditioned\ndistribution function to the latent space under the constraint that it is\nmisclassified to the target label. Our empirical results on MNIST, SVHN, and\nCelebA dataset validate that the generated adversarial examples can easily fool\nrobust l_0, l_2, l_inf norm classifiers designed using provably robust defense\nmechanisms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1\">Ujjwal Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1\">Prerana Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry Enhancements from Visual Content: Going Beyond Ground Truth. (arXiv:2012.08248v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.08248","description":"<p>This work presents a new cyclic architecture that extracts high-frequency\npatterns from images and re-insert them as geometric features. This procedure\nallows us to enhance the resolution of low-cost depth sensors capturing fine\ndetails on the one hand and being loyal to the scanned ground truth on the\nother. We present state-of-the-art results for depth super-resolution tasks and\nas well as visually attractive, enhanced generated 3D models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azaria_L/0/1/0/all/0/1\">Liran Azaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_D/0/1/0/all/0/1\">Dan Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Label Learning for Large-scale Medical Image Classification. (arXiv:2103.04053v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04053","description":"<p>The classification accuracy of deep learning models depends not only on the\nsize of their training sets, but also on the quality of their labels. In\nmedical image classification, large-scale datasets are becoming abundant, but\ntheir labels will be noisy when they are automatically extracted from radiology\nreports using natural language processing tools. Given that deep learning\nmodels can easily overfit these noisy-label samples, it is important to study\ntraining approaches that can handle label noise. In this paper, we adapt a\nstate-of-the-art (SOTA) noisy-label multi-class training approach to learn a\nmulti-label classifier for the dataset Chest X-ray14, which is a large scale\ndataset known to contain label noise in the training set. Given that this\ndataset also has label noise in the testing set, we propose a new theoretically\nsound method to estimate the performance of the model on a hidden clean testing\ndata, given the result on the noisy testing data. Using our clean data\nperformance estimation, we notice that the majority of label noise on Chest\nX-ray14 is present in the class 'No Finding', which is intuitively correct\nbecause this is the most likely class to contain one or more of the 14 diseases\ndue to labelling mistakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_F/0/1/0/all/0/1\">Filipe R. Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantic Segmentation. (arXiv:2103.11594v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11594","description":"<p>How deep neural networks (DNNs) learn from noisy labels has been studied\nextensively in image classification but much less in image segmentation. So\nfar, our understanding of the learning behavior of DNNs trained by noisy\nsegmentation labels remains limited. In this study, we address this deficiency\nin both binary segmentation of biological microscopy images and multi-class\nsegmentation of natural images. We generate extremely noisy labels by randomly\nsampling a small fraction (e.g., 10%) or flipping a large fraction (e.g., 90%)\nof the ground truth labels. When trained with these noisy labels, DNNs provide\nlargely the same segmentation performance as trained by the original ground\ntruth. This indicates that DNNs learn structures hidden in labels rather than\npixel-level labels per se in their supervised training for semantic\nsegmentation. We refer to these hidden structures in labels as meta-structures.\nWhen DNNs are trained by labels with different perturbations to the\nmeta-structure, we find consistent degradation in their segmentation\nperformance. In contrast, incorporation of meta-structure information\nsubstantially improves performance of an unsupervised segmentation model\ndeveloped for binary semantic segmentation. We define meta-structures\nmathematically as spatial density distributions and show both theoretically and\nexperimentally how this formulation explains key observed learning behavior of\nDNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yaoru Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guole Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanhao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfinityGAN: Towards Infinite-Pixel Image Synthesis. (arXiv:2104.03963v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03963","description":"<p>We present a novel framework, InfinityGAN, for arbitrary-sized image\ngeneration. The task is associated with several key challenges. First, scaling\nexisting models to an arbitrarily large image size is resource-constrained, in\nterms of both computation and availability of large-field-of-view training\ndata. InfinityGAN trains and infers in a seamless patch-by-patch manner with\nlow computational resources. Second, large images should be locally and\nglobally consistent, avoid repetitive patterns, and look realistic. To address\nthese, InfinityGAN disentangles global appearances, local structures, and\ntextures. With this formulation, we can generate images with spatial size and\nlevel of details not attainable before. Experimental evaluation validates that\nInfinityGAN generates images with superior realism compared to baselines and\nfeatures parallelizable inference. Finally, we show several applications\nunlocked by our approach, such as spatial style fusion, multi-modal\noutpainting, and image inbetweening. All applications can be operated with\narbitrary input and output sizes. Please find the full version of the paper at\nhttps://openreview.net/forum?id=ufGMqIM0a4b .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chieh Hubert Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yen-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Grounding with Transformers. (arXiv:2105.04281v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04281","description":"<p>In this paper, we propose a transformer based approach for visual grounding.\nUnlike previous proposal-and-rank frameworks that rely heavily on pretrained\nobject detectors or proposal-free frameworks that upgrade an off-the-shelf\none-stage detector by fusing textual embeddings, our approach is built on top\nof a transformer encoder-decoder and is independent of any pretrained detectors\nor word embedding models. Termed VGTR -- Visual Grounding with TRansformers,\nour approach is designed to learn semantic-discriminative visual features under\nthe guidance of the textual description without harming their location ability.\nThis information flow enables our VGTR to have a strong capability in capturing\ncontext-level semantics of both vision and language modalities, rendering us to\naggregate accurate visual clues implied by the description to locate the\ninterested object instance. Experiments show that our method outperforms\nstate-of-the-art proposal-free approaches by a considerable margin on five\nbenchmarks while maintaining fast inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Ye Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zehua Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation. (arXiv:2105.07059v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07059","description":"<p>Contrastive learning (CL) aims to learn useful representation without relying\non expert annotations in the context of medical image segmentation. Existing\napproaches mainly contrast a single positive vector (i.e., an augmentation of\nthe same image) against a set of negatives within the entire remainder of the\nbatch by simply mapping all input features into the same constant vector.\nDespite the impressive empirical performance, those methods have the following\nshortcomings: (1) it remains a formidable challenge to prevent the collapsing\nproblems to trivial solutions; and (2) we argue that not all voxels within the\nsame image are equally positive since there exist the dissimilar anatomical\nstructures with the same image. In this work, we present a novel Contrastive\nVoxel-wise Representation Learning (CVRL) method to effectively learn low-level\nand high-level features by capturing 3D spatial context and rich anatomical\ninformation along both the feature and the batch dimensions. Specifically, we\nfirst introduce a novel CL strategy to ensure feature diversity promotion among\nthe 3D representation dimensions. We train the framework through bi-level\ncontrastive optimization (i.e., low-level and high-level) on 3D images.\nExperiments on two benchmark datasets and different labeled settings\ndemonstrate the superiority of our proposed framework. More importantly, we\nalso prove that our method inherits the benefit of hardness-aware property from\nthe standard CL approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I2C2W: Image-to-Character-to-Word Transformers for Accurate Scene Text Recognition. (arXiv:2105.08383v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08383","description":"<p>Leveraging the advances of natural language processing, most recent scene\ntext recognizers adopt an encoder-decoder architecture where text images are\nfirst converted to representative features and then a sequence of characters\nvia `sequential decoding'. However, scene text images suffer from rich noises\nof different sources such as complex background and geometric distortions which\noften confuse the decoder and lead to incorrect alignment of visual features at\nnoisy decoding time steps. This paper presents I2C2W, a novel scene text\nrecognition technique that is tolerant to geometric and photometric degradation\nby decomposing scene text recognition into two inter-connected tasks. The first\ntask focuses on image-to-character (I2C) mapping which detects a set of\ncharacter candidates from images based on different alignments of visual\nfeatures in an non-sequential way. The second task tackles character-to-word\n(C2W) mapping which recognizes scene text by decoding words from the detected\ncharacter candidates. The direct learning from character semantics (instead of\nnoisy image features) corrects falsely detected character candidates\neffectively which improves the final text recognition accuracy greatly.\nExtensive experiments over nine public datasets show that the proposed I2C2W\noutperforms the state-of-the-art by large margins for challenging scene text\ndatasets with various curvature and perspective distortions. It also achieves\nvery competitive recognition performance over multiple normal scene text\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chuhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A structured latent space for human body motion generation. (arXiv:2106.04387v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04387","description":"<p>This work investigates learning a structured latent space to represent and\ngenerate temporally and spatially dense 4D human body motion. Once trained, the\nproposed model generates a multi-frame sequence of dense 3D meshes based on a\nsingle point in a low-dimensional latent space. Learning a generative model of\nhuman motion with an underlying structured latent space is important for a wide\nset of applications in computer vision and graphics, including virtual and\naugmented reality, 3D telepresence, and content generation for entertainment\napplications. We learn this latent motion representation in a data-driven\nframework that builds upon two existing lines of works. The first analyzes\ntemporally dense skeletal data to capture the global displacement, poses and\ntemporal evolution of the motion, while the second analyzes static densely\ncaptured human scans in 3D to represent realistic 3D human body surfaces in a\nlowdimensional space. Building upon the respective advantages of these two\nconcepts allows our model to simultaneously represent temporal motion\ninformation for sequences of varying duration and detailed 3D geometry at every\ntime instant of the motion. We experimentally demonstrate that the resulting\nlatent space is structured in the sense that similar motions form clusters in\nthis space, and use our latent space to generate plausible interpolations\nbetween different actions. We also illustrate the benefits of the approach for\n4D human motion completion, showing promising abilities of our model to learn\nspatio-temporal features of human motion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marsot_M/0/1/0/all/0/1\">Mathieu Marsot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuhrer_S/0/1/0/all/0/1\">Stefanie Wuhrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_J/0/1/0/all/0/1\">Jean-Sebastien Franco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durocher_S/0/1/0/all/0/1\">Stephane Durocher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification. (arXiv:2106.05517v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05517","description":"<p>Few-shot learning (FSL) aims to learn a classifier that can be easily adapted\nto accommodate new tasks not seen during training, given only a few examples.\nTo handle the limited-data problem in few-shot regimes, recent methods tend to\ncollectively use a set of local features to densely represent an image instead\nof using a mixed global feature. They generally explore a unidirectional\nquery-to-support paradigm in FSL, e.g., find the nearest/optimal support\nfeature for each query feature and aggregate these local matches for a joint\nclassification. In this paper, we propose a new method Mutual Centralized\nLearning (MCL) to fully affiliate the two disjoint sets of dense features in a\nbidirectional paradigm. We associate each local feature with a particle that\ncan bidirectionally random walk in a discrete feature space by the\naffiliations. To estimate the class probability, we propose the features'\naccessibility that measures the expected number of visits to the support\nfeatures of that class in a Markov process. We relate our method to learning a\ncentrality on an affiliation network and demonstrate its capability to be\nplugged in existing methods by highlighting centralized local features.\nExperiments show that our method achieves the state-of-the-art on both\nminiImageNet and tieredImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomy-XNet: A Semi-Supervised Anatomy Aware Convolutional Neural Network for Thoracic Disease Classification. (arXiv:2106.05915v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.05915","description":"<p>Thoracic disease detection from chest radiographs using deep learning methods\nhas been an active area of research in the last decade. Most previous methods\nattempt to focus on the diseased organs of the image by identifying spatial\nregions responsible for significant contributions to the model's prediction. In\ncontrast, expert radiologists first locate the prominent anatomical structures\nbefore determining if those regions are anomalous. Therefore, integrating\nanatomical knowledge within deep learning models could bring substantial\nimprovement in automatic disease classification. Motivated by this, we propose\nAnatomy-XNet, an anatomy-aware attention-based thoracic disease classification\nnetwork that prioritizes the spatial features guided by the pre-identified\nanatomy regions. We adopt a semi-supervised learning method by utilizing\navailable small-scale organ-level annotation to localize the anatomy regions in\nlarge-scale datasets where the organ-level annotations are absent. The proposed\nAnatomy-XNet uses the pre-trained DenseNet-121 as the backbone network with two\ncorresponding structured modules, the Anatomy Aware Attention (AAA) and\nProbabilistic Weighted Average Pooling (PWAP), in a cohesive framework for\nanatomical attention learning. We experimentally show that our proposed method\nsets a new state-of-the-art benchmark by achieving an AUC score of 85.66%,\n91.13%, and, 84.04% on three publicly available large-scale CXR datasets--NIH,\nStanford CheXpert, and MIMIC-CXR, respectively. This not only proves the\nefficacy of utilizing the anatomy segmentation knowledge to improve the\nthoracic disease classification but also demonstrates the generalizability of\nthe proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kamal_U/0/1/0/all/0/1\">Uday Kamal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zunaed_M/0/1/0/all/0/1\">Mohammad Zunaed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nizam_N/0/1/0/all/0/1\">Nusrat Binta Nizam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_T/0/1/0/all/0/1\">Taufiq Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving Deep into the Generalization of Vision Transformers under Distribution Shifts. (arXiv:2106.07617v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07617","description":"<p>Vision Transformers (ViTs) have achieved impressive performance on various\nvision tasks, yet their generalization under distribution shifts (DS) is rarely\nunderstood. In this work, we comprehensively study the out-of-distribution\n(OOD) generalization of ViTs. For systematic investigation, we first present a\ntaxonomy of DS. We then perform extensive evaluations of ViT variants under\ndifferent DS and compare their generalization with Convolutional Neural Network\n(CNN) models. Important observations are obtained: 1) ViTs learn weaker biases\non backgrounds and textures, while they are equipped with stronger inductive\nbiases towards shapes and structures, which is more consistent with human\ncognitive traits. Therefore, ViTs generalize better than CNNs under DS. With\nthe same or less amount of parameters, ViTs are ahead of corresponding CNNs by\nmore than 5% in top-1 accuracy under most types of DS. 2) As the model scale\nincreases, ViTs strengthen these biases and thus gradually narrow the\nin-distribution and OOD performance gap. To further improve the generalization\nof ViTs, we design the Generalization-Enhanced ViTs (GE-ViTs) from the\nperspectives of adversarial learning, information theory, and self-supervised\nlearning. By comprehensively investigating these GE-ViTs and comparing with\ntheir corresponding CNN models, we observe: 1) For the enhanced model, larger\nViTs still benefit more for the OOD generalization. 2) GE-ViTs are more\nsensitive to the hyper-parameters than their corresponding CNN models. We\ndesign a smoother learning strategy to achieve a stable training process and\nobtain performance improvements on OOD data by 4% from vanilla ViTs. We hope\nour comprehensive study could shed light on the design of more generalizable\nlearning architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanghang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Daisheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haiyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Transformer: A unified architecture for predicting multiple agent trajectories. (arXiv:2106.08417v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08417","description":"<p>Predicting the motion of multiple agents is necessary for planning in dynamic\nenvironments. This task is challenging for autonomous driving since agents\n(e.g. vehicles and pedestrians) and their associated behaviors may be diverse\nand influence one another. Most prior work have focused on predicting\nindependent futures for each agent based on all past motion, and planning\nagainst these independent predictions. However, planning against independent\npredictions can make it challenging to represent the future interaction\npossibilities between different agents, leading to sub-optimal planning. In\nthis work, we formulate a model for predicting the behavior of all agents\njointly, producing consistent futures that account for interactions between\nagents. Inspired by recent language modeling approaches, we use a masking\nstrategy as the query to our model, enabling one to invoke a single model to\npredict agent behavior in many ways, such as potentially conditioned on the\ngoal or full future trajectory of the autonomous vehicle or the behavior of\nother agents in the environment. Our model architecture employs attention to\ncombine features across road elements, agent interactions, and time steps. We\nevaluate our approach on autonomous driving datasets for both marginal and\njoint motion prediction, and achieve state of the art performance across two\npopular datasets. Through combining a scene-centric approach, agent permutation\nequivariant model, and a sequence masking strategy, we show that our model can\nunify a variety of motion prediction tasks from joint motion predictions to\nconditioned prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngiam_J/0/1/0/all/0/1\">Jiquan Ngiam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1\">Benjamin Caine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_V/0/1/0/all/0/1\">Vijay Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hao-Tien Lewis Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1\">Jeffrey Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bewley_A/0/1/0/all/0/1\">Alex Bewley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopal_A/0/1/0/all/0/1\">Ashish Venugopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1\">David Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Ben Sapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1\">Jonathon Shlens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ResViT: Residual vision transformers for multi-modal medical image synthesis. (arXiv:2106.16031v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.16031","description":"<p>Generative adversarial models with convolutional neural network (CNN)\nbackbones have recently been established as state-of-the-art in numerous\nmedical image synthesis tasks. However, CNNs are designed to perform local\nprocessing with compact filters, and this inductive bias compromises learning\nof contextual features. Here, we propose a novel generative adversarial\napproach for medical image synthesis, ResViT, that leverages the contextual\nsensitivity of vision transformers along with the precision of convolution\noperators and realism of adversarial learning.} ResViT's generator employs a\ncentral bottleneck comprising novel aggregated residual transformer (ART)\nblocks that synergistically combine residual convolutional and transformer\nmodules. Residual connections in ART blocks promote diversity in captured\nrepresentations, while a channel compression module distills task-relevant\ninformation. A weight sharing strategy is introduced among ART blocks to\nmitigate computational burden. A unified implementation is introduced to avoid\nthe need to rebuild separate synthesis models for varying source-target\nmodality configurations. Comprehensive demonstrations are performed for\nsynthesizing missing sequences in multi-contrast MRI, and CT images from MRI.\nOur results indicate superiority of ResViT against competing CNN- and\ntransformer-based methods in terms of qualitative observations and quantitative\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dalmaz_O/0/1/0/all/0/1\">Onat Dalmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review on vision-based analysis for automatic dietary assessment. (arXiv:2108.02947v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02947","description":"<p>Background: Maintaining a healthy diet is vital to avoid health-related\nissues, e.g., undernutrition, obesity and many non-communicable diseases. An\nindispensable part of the health diet is dietary assessment. Traditional manual\nrecording methods are not only burdensome but time-consuming, and contain\nsubstantial biases and errors. Recent advances in Artificial Intelligence (AI),\nespecially computer vision technologies, have made it possible to develop\nautomatic dietary assessment solutions, which are more convenient, less\ntime-consuming and even more accurate to monitor daily food intake. Scope and\napproach: This review presents Vision-Based Dietary Assessment (VBDA)\narchitectures, including multi-stage architecture and end-to-end one. The\nmulti-stage dietary assessment generally consists of three stages: food image\nanalysis, volume estimation and nutrient derivation. The prosperity of deep\nlearning makes VBDA gradually move to an end-to-end implementation, which\napplies food images to a single network to directly estimate the nutrition. The\nrecently proposed end-to-end methods are also discussed. We further analyze\nexisting dietary assessment datasets, indicating that one large-scale benchmark\nis urgently needed, and finally highlight critical challenges and future trends\nfor VBDA. Key findings and conclusions: After thorough exploration, we find\nthat multi-task end-to-end deep learning approaches are one important trend of\nVBDA. Despite considerable research progress, many challenges remain for VBDA\ndue to the meal complexity. We also provide the latest ideas for future\ndevelopment of VBDA, e.g., fine-grained food analysis and accurate volume\nestimation. This review aims to encourage researchers to propose more practical\nsolutions for VBDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03823","description":"<p>Deep learning-based computer-aided diagnosis is gradually deployed to review\nand analyze medical images. However, this paradigm is restricted in real-world\nclinical applications due to the poor robustness and generalization. The issue\nis more sinister with a lack of training data. In this paper, we address the\nchallenge from the transfer learning point of view. Different from the common\nsetting that transferring knowledge from the natural image domain to the\nmedical image domain, we find the knowledge from the same domain further boosts\nthe model robustness and generalization. Therefore, we propose a novel\ntwo-stage framework for robust generalized medical image segmentation. Firstly,\nan unsupervised tile-wise autoencoder pretraining architecture is proposed to\nlearn local and global knowledge. Secondly, the downstream segmentation model\ncoupled with an auxiliary reconstruction network is designed. The\nreconstruction branch encourages the model to capture more general semantic\nfeatures. Experiments of lung segmentation on multi chest X-ray datasets are\nconducted. Comprehensive results demonstrate the superior robustness of the\nproposed framework to corruption and high generalization performance on unseen\ndatasets, especially under the scenario of the limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertify: Attacks Against Neural Network Certification. (arXiv:2108.11299v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.11299","description":"<p>Certifiers for neural networks have made great progress towards provable\nrobustness guarantees against evasion attacks using adversarial examples.\nHowever, introducing certifiers into deep learning systems also opens up new\nattack vectors, which need to be considered before deployment. In this work, we\nconduct the first systematic analysis of training-time attacks against\ncertifiers in practical application pipelines, identifying new threat vectors\nthat can be exploited to degrade the overall system. Using these insights, we\ndesign two backdoor attacks against network certifiers, which can drastically\nreduce certified robustness. For example, adding 1% poisoned data points during\ntraining is sufficient to reduce certified robustness by up to 95 percentage\npoints, effectively rendering the certifier useless. We analyze how such novel\nattacks can compromise the overall system's integrity or availability. Our\nextensive experiments across multiple datasets, model architectures, and\ncertifiers demonstrate the wide applicability of these attacks. A first\ninvestigation into potential defenses shows that current approaches are\ninsufficient to mitigate the issue, highlighting the need for new, more\nspecific solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_T/0/1/0/all/0/1\">Tobias Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowska_M/0/1/0/all/0/1\">Marta Kwiatkowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FBSNet: A Fast Bilateral Symmetrical Network for Real-Time Semantic Segmentation. (arXiv:2109.00699v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00699","description":"<p>Real-time semantic segmentation, which can be visually understood as the\npixel-level classification task on the input image, currently has broad\napplication prospects, especially in the fast-developing fields of autonomous\ndriving and drone navigation. However, the huge burden of calculation together\nwith redundant parameters are still the obstacles to its technological\ndevelopment. In this paper, we propose a Fast Bilateral Symmetrical Network\n(FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a\nsymmetrical encoder-decoder structure with two branches, semantic information\nbranch and spatial detail branch. The Semantic Information Branch (SIB) is the\nmain branch with semantic architecture to acquire the contextual information of\nthe input image and meanwhile acquire sufficient receptive field. While the\nSpatial Detail Branch (SDB) is a shallow and simple network used to establish\nlocal dependencies of each pixel for preserving details, which is essential for\nrestoring the original resolution during the decoding phase. Meanwhile, a\nFeature Aggregation Module (FAM) is designed to effectively combine the output\nof these two branches. Experimental results of Cityscapes and CamVid show that\nthe proposed FBSNet can strike a good balance between accuracy and efficiency.\nSpecifically, it obtains 70.9\\% and 68.9\\% mIoU along with the inference speed\nof 90 fps and 120 fps on these two test datasets, respectively, with only 0.62\nmillion parameters on a single RTX 2080Ti GPU. The code is available at\nhttps://github.com/IVIPLab/FBSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guoan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huimin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications. (arXiv:2109.02740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02740","description":"<p>We address the problem of estimating the shape of a person's head, defined as\nthe geometry of the complete head surface, from a video taken with a single\nmoving camera, and determining the alignment of the fitted 3D head for all\nvideo frames, irrespective of the person's pose. 3D head reconstructions\ncommonly tend to focus on perfecting the face reconstruction, leaving the scalp\nto a statistical approximation. Our goal is to reconstruct the head model of\neach person to enable future mixed reality applications. To do this, we recover\na dense 3D reconstruction and camera information via structure-from-motion and\nmulti-view stereo. These are then used in a new two-stage fitting process to\nrecover the 3D head shape by iteratively fitting a 3D morphable model of the\nhead with the dense reconstruction in canonical space and fitting it to each\nperson's head, using both traditional facial landmarks and scalp features\nextracted from the head's segmentation mask. Our approach recovers consistent\ngeometry for varying head shapes, from videos taken by different people, with\ndifferent smartphones, and in a variety of environments from living rooms to\noutdoor spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mane_T/0/1/0/all/0/1\">Tejas Mane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayramova_A/0/1/0/all/0/1\">Aylar Bayramova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordohai_P/0/1/0/all/0/1\">Philippos Mordohai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardis_E/0/1/0/all/0/1\">Elena Bernardis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grouptron: Dynamic Multi-Scale Graph Convolutional Networks for Group-Aware Dense Crowd Trajectory Forecasting. (arXiv:2109.14128v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14128","description":"<p>Accurate, long-term forecasting of pedestrian trajectories in highly dynamic\nand interactive scenes is a long-standing challenge. Recent advances in using\ndata-driven approaches have achieved significant improvements in terms of\nprediction accuracy. However, the lack of group-aware analysis has limited the\nperformance of forecasting models. This is especially nonnegligible in highly\ncrowded scenes, where pedestrians are moving in groups and the interactions\nbetween groups are extremely complex and dynamic. In this paper, we present\nGrouptron, a multi-scale dynamic forecasting framework that leverages\npedestrian group detection and utilizes individual-level, group-level and\nscene-level information for better understanding and representation of the\nscenes. Our approach employs spatio-temporal clustering algorithms to identify\npedestrian groups, creates spatio-temporal graphs at the individual, group, and\nscene levels. It then uses graph neural networks to encode dynamics at\ndifferent scales and aggregate the embeddings for trajectory prediction. We\nconducted extensive comparisons and ablation experiments to demonstrate the\neffectiveness of our approach. Our method achieves 9.3% decrease in final\ndisplacement error (FDE) compared with state-of-the-art methods on ETH/UCY\nbenchmark datasets, and 16.1% decrease in FDE in more crowded scenes where\nextensive human group interactions are more frequently present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Rui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Huidong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhuo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Early-Learning Correction for Segmentation from Noisy Annotations. (arXiv:2110.03740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03740","description":"<p>Deep learning in the presence of noisy annotations has been studied\nextensively in classification, but much less in segmentation tasks. In this\nwork, we study the learning dynamics of deep segmentation networks trained on\ninaccurately-annotated data. We discover a phenomenon that has been previously\nreported in the context of classification: the networks tend to first fit the\nclean pixel-level labels during an \"early-learning\" phase, before eventually\nmemorizing the false annotations. However, in contrast to classification,\nmemorization in segmentation does not arise simultaneously for all semantic\ncategories. Inspired by these findings, we propose a new method for\nsegmentation from noisy annotations with two key elements. First, we detect the\nbeginning of the memorization phase separately for each category during\ntraining. This allows us to adaptively correct the noisy annotations in order\nto exploit early learning. Second, we incorporate a regularization term that\nenforces consistency across scales to boost robustness against annotation\nnoise. Our method outperforms standard approaches on a medical-imaging\nsegmentation task where noises are synthesized to mimic human annotation\nerrors. It also provides robustness to realistic noisy annotations present in\nweakly-supervised semantic segmentation, achieving state-of-the-art results on\nPASCAL VOC 2012. Code is available at https://github.com/Kangningthu/ADELE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kangning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Weicheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yiqiu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Editing as Teleoperation: A Case Study in 6DoF Kit Assembly. (arXiv:2110.04450v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2110.04450","description":"<p>Studies in robot teleoperation have been centered around action\nspecifications -- from continuous joint control to discrete end-effector pose\ncontrol. However, these robot-centric interfaces often require skilled\noperators with extensive robotics expertise. To make teleoperation accessible\nto non-expert users, we propose the framework \"Scene Editing as Teleoperation\"\n(SEaT), where the key idea is to transform the traditional \"robot-centric\"\ninterface into a \"scene-centric\" interface -- instead of controlling the robot,\nusers focus on specifying the task's goal by manipulating digital twins of the\nreal-world objects. As a result, a user can perform teleoperation without any\nexpert knowledge of the robot hardware. To achieve this goal, we utilize a\ncategory-agnostic scene-completion algorithm that translates the real-world\nworkspace (with unknown objects) into a manipulable virtual scene\nrepresentation and an action-snapping algorithm that refines the user input\nbefore generating the robot's action plan. To train the algorithms, we\nprocedurally generated a large-scale, diverse kit-assembly dataset that\ncontains object-kit pairs that mimic real-world object-kitting tasks. Our\nexperiments in simulation and on a real-world system demonstrate that our\nframework improves both the efficiency and success rate for 6DoF kit-assembly\ntasks. A user study demonstrates that SEaT framework participants achieve a\nhigher task success rate and report a lower subjective workload compared to an\nalternative robot-centric interface. Video can be found at\nhttps://www.youtube.com/watch?v=-NdR3mkPbQQ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shubham Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jen-Shuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feiner_S/0/1/0/all/0/1\">Steven K. Feiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency. (arXiv:2110.05182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05182","description":"<p>The explanation for deep neural networks has drawn extensive attention in the\ndeep learning community over the past few years. In this work, we study the\nvisual saliency, a.k.a. visual explanation, to interpret convolutional neural\nnetworks. Compared to iteration based saliency methods, single backward pass\nbased saliency methods benefit from faster speed, and they are widely used in\ndownstream visual tasks. Thus, we focus on single backward pass based methods.\nHowever, existing methods in this category struggle to uccessfully produce\nfine-grained saliency maps concentrating on specific target classes. That said,\nproducing faithful saliency maps satisfying both target-selectiveness and\nfine-grainedness using a single backward pass is a challenging problem in the\nfield. To mitigate this problem, we revisit the gradient flow inside the\nnetwork, and find that the entangled semantics and original weights may disturb\nthe propagation of target-relevant saliency. Inspired by those observations, we\npropose a novel visual saliency method, termed Target-Selective Gradient\nBackprop (TSGB), which leverages rectification operations to effectively\nemphasize target classes and further efficiently propagate the saliency to the\nimage space, thereby generating target-selective and fine-grained saliency\nmaps. The proposed TSGB consists of two components, namely, TSGB-Conv and\nTSGB-FC, which rectify the gradients for convolutional layers and\nfully-connected layers, respectively. Extensive qualitative and quantitative\nexperiments on the ImageNet and Pascal VOC datasets show that the proposed\nmethod achieves more accurate and reliable results than the other competitive\nmethods. Code is available at https://github.com/123fxdx/CNNvisualizationTSGB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yanjie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanzi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Optimal Conformal Classifiers. (arXiv:2110.09192v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.09192","description":"<p>Modern deep learning based classifiers show very high accuracy on test data\nbut this does not provide sufficient guarantees for safe deployment, especially\nin high-stake AI applications such as medical diagnosis. Usually, predictions\nare obtained without a reliable uncertainty estimate or a formal guarantee.\nConformal prediction (CP) addresses these issues by using the classifier's\npredictions, e.g., its probability estimates, to predict confidence sets\ncontaining the true class with a user-specified probability. However, using CP\nas a separate processing step after training prevents the underlying model from\nadapting to the prediction of confidence sets. Thus, this paper explores\nstrategies to differentiate through CP during training with the goal of\ntraining model with the conformal wrapper end-to-end. In our approach,\nconformal training (ConfTr), we specifically \"simulate\" conformalization on\nmini-batches during training. Compared to standard training, ConfTr reduces the\naverage confidence set size (inefficiency) of state-of-the-art CP methods\napplied after training. Moreover, it allows to \"shape\" the confidence sets\npredicted at test time, which is difficult for standard CP. On experiments with\nseveral datasets, we show ConfTr can influence how inefficiency is distributed\nacross classes, or guide the composition of confidence sets in terms of the\nincluded classes, while retaining the guarantees offered by CP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy/0/1/0/all/0/1\">Krishnamurthy</a> (Dj) <a href=\"http://arxiv.org/find/cs/1/au:+Dvijotham/0/1/0/all/0/1\">Dvijotham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cemgil_A/0/1/0/all/0/1\">Ali Taylan Cemgil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Scoring System of HER2 in Pathological Images under the Microscope. (arXiv:2110.12900v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2110.12900","description":"<p>Breast cancer is the most common cancer among women worldwide. The human\nepidermal growth factor receptor 2 (HER2) with immunohistochemical (IHC) is\nwidely used for pathological evaluation to provide the appropriate therapy for\npatients with breast cancer. However, the deficiency of pathologists and\nsubjective and susceptible to inter-observer variation of visual diagnosis are\nthe main challenges. Recently, with the rapid development of artificial\nintelligence (AI) in disease diagnosis, several automated HER2 scoring methods\nusing traditional computer vision or machine learning methods indicate the\nimprovement of the HER2 diagnostic accuracy, but the unreasonable\ninterpretation in pathology, as well as the expensive and ethical issues for\nannotation, make these methods still have a long way to deploy in hospitals to\nease pathologists' burden in real. In this paper, we propose a HER2 automated\nscoring system that strictly follows the HER2 scoring guidelines simulating the\nreal workflow of HER2 scores diagnosis by pathologists. Unlike the previous\nwork, our method considers the positive control of HER2 to make sure the assay\nperformance for each slide, eliminating work for repeated comparison between\nthe current field of view (FOV) and positive control FOV, especially for the\nborderline cases. Besides, for each selected FOV under the microscope, our\nsystem provides real-time HER2 scores analysis and visualizations of the\nmembrane staining intensity and completeness corresponding with the cell\nclassifications. Our rigorous workflow along with the flexible interactive\nadjustion in demand substantially assists pathologists to finish the HER2\ndiagnosis faster and improves the robustness and accuracy. The proposed system\nwill be embedded in our Thorough Eye platform for deployment in hospitals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Z/0/1/0/all/0/1\">Zichen Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_L/0/1/0/all/0/1\">Lang Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_S/0/1/0/all/0/1\">Shuhao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised GAN Detector. (arXiv:2111.06575v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06575","description":"<p>Although the recent advancement in generative models brings diverse\nadvantages to society, it can also be abused with malicious purposes, such as\nfraud, defamation, and fake news. To prevent such cases, vigorous research is\nconducted to distinguish the generated images from the real images, but\nchallenges still remain to distinguish the unseen generated images outside of\nthe training settings. Such limitations occur due to data dependency arising\nfrom the model's overfitting issue to the training data generated by specific\nGANs. To overcome this issue, we adopt a self-supervised scheme to propose a\nnovel framework. Our proposed method is composed of the artificial fingerprint\ngenerator reconstructing the high-quality artificial fingerprints of GAN images\nfor detailed analysis, and the GAN detector distinguishing GAN images by\nlearning the reconstructed artificial fingerprints. To improve the\ngeneralization of the artificial fingerprint generator, we build multiple\nautoencoders with different numbers of upconvolution layers. With numerous\nablation studies, the robust generalization of our method is validated by\noutperforming the generalization of the previous state-of-the-art algorithms,\neven without utilizing the GAN images of the training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1\">Pyounggeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Youngmin Ro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jongwon Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection. (arXiv:2111.09099v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09099","description":"<p>Anomaly detection is commonly pursued as a one-class classification problem,\nwhere models can only learn from normal training samples, while being evaluated\non both normal and abnormal test samples. Among the successful approaches for\nanomaly detection, a distinguished category of methods relies on predicting\nmasked information (e.g. patches, future frames, etc.) and leveraging the\nreconstruction error with respect to the masked information as an abnormality\nscore. Different from related methods, we propose to integrate the\nreconstruction-based functionality into a novel self-supervised predictive\narchitectural building block. The proposed self-supervised block is generic and\ncan easily be incorporated into various state-of-the-art anomaly detection\nmethods. Our block starts with a convolutional layer with dilated filters,\nwhere the center area of the receptive field is masked. The resulting\nactivation maps are passed through a channel attention module. Our block is\nequipped with a loss that minimizes the reconstruction error with respect to\nthe masked area in the receptive field. We demonstrate the generality of our\nblock by integrating it into several state-of-the-art frameworks for anomaly\ndetection on image and video, providing empirical evidence that shows\nconsiderable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We\nrelease our code as open source at https://github.com/ristea/sspcab.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_N/0/1/0/all/0/1\">Neelu Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrollahi_K/0/1/0/all/0/1\">Kamal Nasrollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1\">Thomas B. Moeslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization. (arXiv:2111.09136v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09136","description":"<p>Learning to synthesize data has emerged as a promising direction in zero-shot\nquantization (ZSQ), which represents neural networks by low-bit integer without\naccessing any of the real data. In this paper, we observe an interesting\nphenomenon of intra-class heterogeneity in real data and show that existing\nmethods fail to retain this property in their synthetic images, which causes a\nlimited performance increase. To address this issue, we propose a novel\nzero-shot quantization method referred to as IntraQ. First, we propose a local\nobject reinforcement that locates the target objects at different scales and\npositions of the synthetic images. Second, we introduce a marginal distance\nconstraint to form class-related features distributed in a coarse area. Lastly,\nwe devise a soft inception loss which injects a soft prior label to prevent the\nsynthetic images from being overfitting to a fixed object. Our IntraQ is\ndemonstrated to well retain the intra-class heterogeneity in the synthetic\nimages and also observed to perform state-of-the-art. For example, compared to\nthe advanced ZSQ, our IntraQ obtains 9.17\\% increase of the top-1 accuracy on\nImageNet when all layers of MobileNetV1 are quantized to 4-bit. Code is at\nhttps://github.com/viperit/InterQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Gongrui Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamLiFlow: Bidirectional Camera-LiDAR Fusion for Joint Optical Flow and Scene Flow Estimation. (arXiv:2111.10502v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10502","description":"<p>In this paper, we study the problem of jointly estimating the optical flow\nand scene flow from synchronized 2D and 3D data. Previous methods either employ\na complex pipeline which splits the joint task into independent stages, or fuse\n2D and 3D information in an \"early-fusion\" or \"late-fusion\" manner. Such\none-size-fits-all approaches suffer from a dilemma of failing to fully utilize\nthe characteristic of each modality or to maximize the inter-modality\ncomplementarity. To address the problem, we propose a novel end-to-end\nframework, called CamLiFlow. It consists of 2D and 3D branches with multiple\nbidirectional connections between them in specific layers. Different from\nprevious work, we apply a point-based 3D branch to better extract the geometric\nfeatures and design a symmetric learnable operator to fuse dense image features\nand sparse point features. We also propose a transformation for point clouds to\nsolve the non-linear issue of 3D-2D projection. Experiments show that CamLiFlow\nachieves better performance with fewer parameters. Our method ranks 1st on the\nKITTI Scene Flow benchmark, outperforming the previous art with 1/7 parameters.\nCode will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haisong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yihui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lijun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11133","description":"<p>Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalabilty. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for text-to-image and image-to-text\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation tasks without any finetuning or extra\nobject detection frameworks. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial results of bidirectional vision-language representation learning on\ngeneral domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Gwangmo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sihaeng Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Yewon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soonyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1\">Kyunghoon Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lepard: Learning partial point cloud matching in rigid and deformable scenes. (arXiv:2111.12591v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12591","description":"<p>We present Lepard, a Learning based approach for partial point cloud matching\nin rigid and deformable scenes. The key characteristics are the following\ntechniques that exploit 3D positional knowledge for point cloud matching: 1) An\narchitecture that disentangles point cloud representation into feature space\nand 3D position space. 2) A position encoding method that explicitly reveals 3D\nrelative distance information through the dot product of vectors. 3) A\nrepositioning technique that modifies the crosspoint-cloud relative positions.\nAblation studies demonstrate the effectiveness of the above techniques. In\nrigid cases, Lepard combined with RANSAC and ICP demonstrates state-of-the-art\nregistration recall of 93.9% / 71.3% on the 3DMatch / 3DLoMatch. In deformable\ncases, Lepard achieves +27.1% / +34.8% higher non-rigid feature matching recall\nthan the prior art on our newly constructed 4DMatch / 4DLoMatch benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximum Consensus by Weighted Influences of Monotone Boolean Functions. (arXiv:2112.00953v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00953","description":"<p>Robust model fitting is a fundamental problem in computer vision: used to\npre-process raw data in the presence of outliers. Maximisation of Consensus\n(MaxCon) is one of the most popular robust criteria and widely used. Recently\n(Tennakoon et al. CVPR2021), a connection has been made between MaxCon and\nestimation of influences of a Monotone Boolean function. Equipping the Boolean\ncube with different measures and adopting different sampling strategies (two\nsides of the same coin) can have differing effects: which leads to the current\nstudy. This paper studies the concept of weighted influences for solving\nMaxCon. In particular, we study endowing the Boolean cube with the Bernoulli\nmeasure and performing biased (as opposed to uniform) sampling. Theoretically,\nwe prove the weighted influences, under this measure, of points belonging to\nlarger structures are smaller than those of points belonging to smaller\nstructures in general. We also consider another \"natural\" family of\nsampling/weighting strategies, sampling with uniform measure concentrated on a\nparticular (Hamming) level of the cube.\n</p>\n<p>Based on weighted sampling, we modify the algorithm of Tennakoon et al., and\ntest on both synthetic and real datasets. This paper is not promoting a new\napproach per se, but rather studying the issue of weighted sampling.\nAccordingly, we are not claiming to have produced a superior algorithm: rather\nwe show some modest gains of Bernoulli sampling, and we illuminate some of the\ninteractions between structure in data and weighted sampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Erchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suter_D/0/1/0/all/0/1\">David Suter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennakoon_R/0/1/0/all/0/1\">Ruwan Tennakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1\">Alireza Bab-Hadiashar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_G/0/1/0/all/0/1\">Giang Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilani_S/0/1/0/all/0/1\">Syed Zulqarnain Gilani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equal Bits: Enforcing Equally Distributed Binary Network Weights. (arXiv:2112.03406v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.03406","description":"<p>Binary networks are extremely efficient as they use only two symbols to\ndefine the network: $\\{+1,-1\\}$. One can make the prior distribution of these\nsymbols a design choice. The recent IR-Net of Qin et al. argues that imposing a\nBernoulli distribution with equal priors (equal bit ratios) over the binary\nweights leads to maximum entropy and thus minimizes information loss. However,\nprior work cannot precisely control the binary weight distribution during\ntraining, and therefore cannot guarantee maximum entropy. Here, we show that\nquantizing using optimal transport can guarantee any bit ratio, including equal\nratios. We investigate experimentally that equal bit ratios are indeed\npreferable and show that our method leads to optimization benefits. We show\nthat our quantization method is effective when compared to state-of-the-art\nbinarization methods, even when using binary weight pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1\">Silvia L. Pintea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCGL: Temporal Contrastive Graph for Self-supervised Video Representation Learning. (arXiv:2112.03587v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03587","description":"<p>Video self-supervised learning is a challenging task, which requires\nsignificant expressive power from the model to leverage rich spatial-temporal\nknowledge and generate effective supervisory signals from large amounts of\nunlabeled videos. However, existing methods fail to increase the temporal\ndiversity of unlabeled videos and ignore elaborately modeling multi-scale\ntemporal dependencies in an explicit way. To overcome these limitations, we\ntake advantage of the multi-scale temporal dependencies within videos and\nproposes a novel video self-supervised learning framework named Temporal\nContrastive Graph Learning (TCGL), which jointly models the inter-snippet and\nintra-snippet temporal dependencies for temporal representation learning with a\nhybrid graph contrastive learning strategy. Specifically, a Spatial-Temporal\nKnowledge Discovering (STKD) module is first introduced to extract\nmotion-enhanced spatial-temporal representations from videos based on the\nfrequency domain analysis of discrete cosine transform. To explicitly model\nmulti-scale temporal dependencies of unlabeled videos, our TCGL integrates the\nprior knowledge about the frame and snippet orders into graph structures, i.e.,\nthe intra-/inter- snippet Temporal Contrastive Graphs (TCG). Then, specific\ncontrastive learning modules are designed to maximize the agreement between\nnodes in different graph views. To generate supervisory signals for unlabeled\nvideos, we introduce an Adaptive Snippet Order Prediction (ASOP) module which\nleverages the relational knowledge among video snippets to learn the global\ncontext representation and recalibrate the channel-wise features adaptively.\nExperimental results demonstrate the superiority of our TCGL over the\nstate-of-the-art methods on large-scale action recognition and video retrieval\nbenchmarks.The code is publicly available at\nhttps://github.com/YangLiu9208/TCGL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Haoyuan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Stopping for Deep Image Prior. (arXiv:2112.06074v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06074","description":"<p>Deep image prior (DIP) and its variants have showed remarkable potential for\nsolving inverse problems in computer vision, without any extra training data.\nPractical DIP models are often substantially overparameterized. During the\nfitting process, these models learn mostly the desired visual content first,\nand then pick up the potential modeling and observational noise, i.e.,\noverfitting. Thus, the practicality of DIP often depends critically on good\nearly stopping (ES) that captures the transition period. In this regard, the\nmajority of DIP works for vision tasks only demonstrates the potential of the\nmodels -- reporting the peak performance against the ground truth, but provides\nno clue about how to operationally obtain near-peak performance without access\nto the groundtruth. In this paper, we set to break this practicality barrier of\nDIP, and propose an efficient ES strategy, which consistently detects near-peak\nperformance across several vision tasks and DIP variants. Based on a simple\nmeasure of dispersion of consecutive DIP reconstructions, our ES method not\nonly outpaces the existing ones -- which only work in very narrow domains, but\nalso remains effective when combined with a number of methods that try to\nmitigate the overfitting. The code is available at\nhttps://github.com/sun-umn/Early_Stopping_for_DIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengkang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Taihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiancong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hengyue Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JoJoGAN: One Shot Face Stylization. (arXiv:2112.11641v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11641","description":"<p>A style mapper applies some fixed style to its input images (so, for example,\ntaking faces to cartoons). This paper describes a simple procedure -- JoJoGAN\n-- to learn a style mapper from a single example of the style. JoJoGAN uses a\nGAN inversion procedure and StyleGAN's style-mixing property to produce a\nsubstantial paired dataset from a single example style. The paired dataset is\nthen used to fine-tune a StyleGAN. An image can then be style mapped by\nGAN-inversion followed by the fine-tuned StyleGAN. JoJoGAN needs just one\nreference and as little as 30 seconds of training time. JoJoGAN can use extreme\nstyle references (say, animal faces) successfully. Furthermore, one can control\nwhat aspects of the style are used and how much of the style is applied.\nQualitative and quantitative evaluation show that JoJoGAN produces high quality\nhigh resolution images that vastly outperform the current state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflash Dropout in Image Super-Resolution. (arXiv:2112.12089v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12089","description":"<p>Dropout is designed to relieve the overfitting problem in high-level vision\ntasks but is rarely applied in low-level vision tasks, like image\nsuper-resolution (SR). As a classic regression problem, SR exhibits a different\nbehaviour as high-level tasks and is sensitive to the dropout operation.\nHowever, in this paper, we show that appropriate usage of dropout benefits SR\nnetworks and improves the generalization ability. Specifically, dropout is\nbetter embedded at the end of the network and is significantly helpful for the\nmulti-degradation settings. This discovery breaks our common sense and inspires\nus to explore its working mechanism. We further use two analysis tools -- one\nis from recent network interpretation works, and the other is specially\ndesigned for this task. The analysis results provide side proofs to our\nexperimental findings and show us a new perspective to understand SR networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangtao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xina Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation and Focal Loss. (arXiv:2201.01501v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01501","description":"<p>Depth estimation is solved as a regression or classification problem in\nexisting learning-based multi-view stereo methods. Although these two\nrepresentations have recently demonstrated their excellent performance, they\nstill have apparent shortcomings, e.g., regression methods tend to overfit due\nto the indirect learning cost volume, and classification methods cannot\ndirectly infer the exact depth due to its discrete prediction. In this paper,\nwe propose a novel representation, termed Unification, to unify the advantages\nof regression and classification. It can directly constrain the cost volume\nlike classification methods, but also realize the sub-pixel depth prediction\nlike regression methods. To excavate the potential of unification, we design a\nnew loss function named Unified Focal Loss, which is more uniform and\nreasonable to combat the challenge of sample imbalance. Combining these two\nunburdened modules, we present a coarse-to-fine framework, that we call\nUniMVSNet. The results of ranking first on both DTU and Tanks and Temples\nbenchmarks verify that our model not only performs the best but also has the\nbest generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Rui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yawen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ronggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08845","description":"<p>Volumetric neural rendering methods like NeRF generate high-quality view\nsynthesis results but are optimized per-scene leading to prohibitive\nreconstruction time. On the other hand, deep multi-view stereo methods can\nquickly reconstruct scene geometry via direct network inference. Point-NeRF\ncombines the advantages of these two approaches by using neural 3D point\nclouds, with associated neural features, to model a radiance field. Point-NeRF\ncan be rendered efficiently by aggregating neural point features near scene\nsurfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can\nbe initialized via direct inference of a pre-trained deep network to produce a\nneural point cloud; this point cloud can be finetuned to surpass the visual\nquality of NeRF with 30X faster training time. Point-NeRF can be combined with\nother 3D reconstruction methods and handles the errors and outliers in such\nmethods via a novel pruning and growing mechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiangeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Minimize the Remainder in Supervised Learning. (arXiv:2201.09193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09193","description":"<p>The learning process of deep learning methods usually updates the model's\nparameters in multiple iterations. Each iteration can be viewed as the\nfirst-order approximation of Taylor's series expansion. The remainder, which\nconsists of higher-order terms, is usually ignored in the learning process for\nsimplicity. This learning scheme empowers various multimedia based\napplications, such as image retrieval, recommendation system, and video search.\nGenerally, multimedia data (e.g., images) are semantics-rich and\nhigh-dimensional, hence the remainders of approximations are possibly non-zero.\nIn this work, we consider the remainder to be informative and study how it\naffects the learning process. To this end, we propose a new learning approach,\nnamely gradient adjustment learning (GAL), to leverage the knowledge learned\nfrom the past training iterations to adjust vanilla gradients, such that the\nremainders are minimized and the approximations are improved. The proposed GAL\nis model- and optimizer-agnostic, and is easy to adapt to the standard learning\nframework. It is evaluated on three tasks, i.e., image classification, object\ndetection, and regression, with state-of-the-art models and optimizers. The\nexperiments show that the proposed GAL consistently enhances the evaluated\nmodels, whereas the ablation studies validate various aspects of the proposed\nGAL. The code is available at\n\\url{https://github.com/luoyan407/gradient_adjustment.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan S. Kankanhalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalised Image Outpainting with U-Transformer. (arXiv:2201.11403v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11403","description":"<p>While most present image outpainting conducts horizontal extrapolation, we\nstudy the generalised image outpainting problem that extrapolates visual\ncontext all-side around a given image. To this end, we develop a novel\ntransformer-based generative adversarial network called U-Transformer able to\nextend image borders with plausible structure and details even for complicated\nscenery images. Specifically, we design a generator as an encoder-to-decoder\nstructure embedded with the popular Swin Transformer blocks. As such, our novel\nframework can better cope with image long-range dependencies which are\ncrucially important for generalised image outpainting. We propose additionally\na U-shaped structure and multi-view Temporal Spatial Predictor network to\nreinforce image self-reconstruction as well as unknown-part prediction smoothly\nand realistically. We experimentally demonstrate that our proposed method could\nproduce visually appealing results for generalized image outpainting against\nthe state-of-the-art image outpainting approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Penglei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yujie Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-confidence Samples Matter for Domain Adaptation. (arXiv:2202.02802v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02802","description":"<p>Domain adaptation (DA) aims to transfer knowledge from a label-rich source\ndomain to a related but label-scarce target domain. The conventional DA\nstrategy is to align the feature distributions of the two domains. Recently,\nincreasing researches have focused on self-training or other semi-supervised\nalgorithms to explore the data structure of the target domain. However, the\nbulk of them depend largely on confident samples in order to build reliable\npseudo labels, prototypes or cluster centers. Representing the target data\nstructure in such a way would overlook the huge low-confidence samples,\nresulting in sub-optimal transferability that is biased towards the samples\nsimilar to the source domain. To overcome this issue, we propose a novel\ncontrastive learning method by processing low-confidence samples, which\nencourages the model to make use of the target data structure through the\ninstance discrimination process. To be specific, we create positive and\nnegative pairs only using low-confidence samples, and then re-represent the\noriginal features with the classifier weights rather than directly utilizing\nthem, which can better encode the task-specific semantic information.\nFurthermore, we combine cross-domain mixup to augment the proposed contrastive\nloss. Consequently, the domain gap can be well bridged through contrastive\nlearning of intermediate representations across domains. We evaluate the\nproposed method in both unsupervised and semi-supervised DA settings, and\nextensive experimental results on benchmarks reveal that our method is\neffective and achieves state-of-the-art performance. The code can be found in\nhttps://github.com/zhyx12/MixLRCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyper-relationship Learning Network for Scene Graph Generation. (arXiv:2202.07271v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07271","description":"<p>Generating informative scene graphs from images requires integrating and\nreasoning from various graph components, i.e., objects and relationships.\nHowever, current scene graph generation (SGG) methods, including the unbiased\nSGG methods, still struggle to predict informative relationships due to the\nlack of 1) high-level inference such as transitive inference between\nrelationships and 2) efficient mechanisms that can incorporate all interactions\nof graph components. To address the issues mentioned above, we devise a\nhyper-relationship learning network, termed HLN, for SGG. Specifically, the\nproposed HLN stems from hypergraphs and two graph attention networks (GATs) are\ndesigned to infer relationships: 1) the object-relationship GAT or OR-GAT to\nexplore interactions between objects and relationships, and 2) the\nhyper-relationship GAT or HR-GAT to integrate transitive inference of\nhyper-relationships, i.e., the sequential relationships between three objects\nfor transitive reasoning. As a result, HLN significantly improves the\nperformance of scene graph generation by integrating and reasoning from object\ninteractions, relationship interactions, and transitive inference of\nhyper-relationships. We evaluate HLN on the most popular SGG dataset, i.e., the\nVisual Genome dataset, and the experimental results demonstrate its great\nsuperiority over recent state-of-the-art methods. For example, the proposed HLN\nimproves the recall per relationship from 11.3\\% to 13.1\\%, and maintains the\nrecall per image from 19.8\\% to 34.9\\%. We will release the source code and\npretrained models on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">BaoSheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Emerging Research Directions in Abstract Visual Reasoning. (arXiv:2202.10284v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.10284","description":"<p>Abstract Visual Reasoning (AVR) problems are commonly used to approximate\nhuman intelligence. They test the ability of applying previously gained\nknowledge, experience and skills in a completely new setting, which makes them\nparticularly well-suited for this task. Recently, the AVR problems have become\npopular as a proxy to study machine intelligence, which has led to emergence of\nnew distinct types of problems and multiple benchmark sets. In this work we\nreview this emerging AVR research and propose a taxonomy to categorise the AVR\ntasks along 5 dimensions: input shapes, hidden rules, target task, cognitive\nfunction, and main challenge. The perspective taken in this survey allows to\ncharacterise AVR problems with respect to their shared and distinct properties,\nprovides a unified view on the existing approaches for solving AVR tasks, shows\nhow the AVR problems relate to practical applications, and outlines promising\ndirections for future work. One of them refers to the observation that in the\nmachine learning literature different tasks are considered in isolation, which\nis in the stark contrast with the way the AVR tasks are used to measure human\nintelligence, where multiple types of problems are combined within a single IQ\ntest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malkinski_M/0/1/0/all/0/1\">Miko&#x142;aj Ma&#x142;ki&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CG-SSD: Corner Guided Single Stage 3D Object Detection from LiDAR Point Cloud. (arXiv:2202.11868v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11868","description":"<p>At present, the anchor-based or anchor-free models that use LiDAR point\nclouds for 3D object detection use the center assigner strategy to infer the 3D\nbounding boxes. However, in a real world scene, the LiDAR can only acquire a\nlimited object surface point clouds, but the center point of the object does\nnot exist. Obtaining the object by aggregating the incomplete surface point\nclouds will bring a loss of accuracy in direction and dimension estimation. To\naddress this problem, we propose a corner-guided anchor-free single-stage 3D\nobject detection model (CG-SSD ).Firstly, 3D sparse convolution backbone\nnetwork composed of residual layers and sub-manifold sparse convolutional\nlayers are used to construct bird's eye view (BEV) features for further deeper\nfeature mining by a lite U-shaped network; Secondly, a novel corner-guided\nauxiliary module (CGAM) is proposed to incorporate corner supervision signals\ninto the neural network. CGAM is explicitly designed and trained to detect\npartially visible and invisible corners to obtains a more accurate object\nfeature representation, especially for small or partial occluded objects;\nFinally, the deep features from both the backbone networks and CGAM module are\nconcatenated and fed into the head module to predict the classification and 3D\nbounding boxes of the objects in the scene. The experiments demonstrate CG-SSD\nachieves the state-of-art performance on the ONCE benchmark for supervised 3D\nobject detection using single frame point cloud data, with 62.77%mAP.\nAdditionally, the experiments on ONCE and Waymo Open Dataset show that CGAM can\nbe extended to most anchor-based models which use the BEV feature to detect\nobjects, as a plug-in and bring +1.17%-+14.27%AP improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruiqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bisheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yangzi Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zongtian Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TeachAugment: Data Augmentation Optimization Using Teacher Knowledge. (arXiv:2202.12513v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12513","description":"<p>Optimization of image transformation functions for the purpose of data\naugmentation has been intensively studied. In particular, adversarial data\naugmentation strategies, which search augmentation maximizing task loss, show\nsignificant improvement in the model generalization for many tasks. However,\nthe existing methods require careful parameter tuning to avoid excessively\nstrong deformations that take away image features critical for acquiring\ngeneralization. In this paper, we propose a data augmentation optimization\nmethod based on the adversarial strategy called TeachAugment, which can produce\ninformative transformed images to the model without requiring careful tuning by\nleveraging a teacher model. Specifically, the augmentation is searched so that\naugmented images are adversarial for the target model and recognizable for the\nteacher model. We also propose data augmentation using neural networks, which\nsimplifies the search space design and allows for updating of the data\naugmentation using the gradient method. We show that TeachAugment outperforms\nexisting methods in experiments of image classification, semantic segmentation,\nand unsupervised representation learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Teppei Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Name Your Style: An Arbitrary Artist-aware Image Style Transfer. (arXiv:2202.13562v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13562","description":"<p>Image style transfer has attracted widespread attention in the past few\nyears. Despite its remarkable results, it requires additional style images\navailable as references, making it less flexible and inconvenient. Using text\nis the most natural way to describe the style. More importantly, text can\ndescribe implicit abstract styles, like styles of specific artists or art\nmovements. In this paper, we propose a text-driven image style transfer (TxST)\nthat leverages advanced image-text encoders to control arbitrary style\ntransfer. We introduce a contrastive training strategy to effectively extract\nstyle descriptions from the image-text model (i.e., CLIP), which aligns\nstylization with the text description. To this end, we also propose a novel and\nefficient attention module that explores cross-attentions to fuse style and\ncontent features. Finally, we achieve an arbitrary artist-aware image style\ntransfer to learn and transfer specific artistic characters such as Picasso,\noil painting, or a rough sketch. Extensive experiments demonstrate that our\napproach outperforms the state-of-the-art methods on both image and textual\nstyles. Moreover, it can mimic the styles of one or many artists to achieve\nattractive results, thus highlighting a promising direction in image style\ntransfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhi-Song Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li-Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siu_W/0/1/0/all/0/1\">Wan-Chi Siu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1\">Vicky Kalogeiton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial samples for deep monocular 6D object pose estimation. (arXiv:2203.00302v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00302","description":"<p>Estimating 6D object pose from an RGB image is important for many real-world\napplications such as autonomous driving and robotic grasping. Recent deep\nlearning models have achieved significant progress on this task but their\nrobustness received little research attention. In this work, for the first\ntime, we study adversarial samples that can fool deep learning models with\nimperceptible perturbations to input image. In particular, we propose a Unified\n6D pose estimation Attack, namely U6DA, which can successfully attack several\nstate-of-the-art (SOTA) deep learning models for 6D pose estimation. The key\nidea of our U6DA is to fool the models to predict wrong results for object\ninstance localization and shape that are essential for correct 6D pose\nestimation. Specifically, we explore a transfer-based black-box attack to 6D\npose estimation. We design the U6DA loss to guide the generation of adversarial\nexamples, the loss aims to shift the segmentation attention map away from its\noriginal position. We show that the generated adversarial samples are not only\neffective for direct 6D pose estimation models, but also are able to attack\ntwo-stage models regardless of their robust RANSAC modules. Extensive\nexperiments were conducted to demonstrate the effectiveness, transferability,\nand anti-defense capability of our U6DA on large-scale public benchmarks. We\nalso introduce a new U6DA-Linemod dataset for robustness study of the 6D pose\nestimation task. Our codes and dataset will be available at\n\\url{https://github.com/cuge1995/U6DA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinlai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1\">Shuang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihong Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation. (arXiv:2203.01072v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01072","description":"<p>This paper proposes a universal framework, called OVE6D, for model-based 6D\nobject pose estimation from a single depth image and a target object mask. Our\nmodel is trained using purely synthetic data rendered from ShapeNet, and,\nunlike most of the existing methods, it generalizes well on new real-world\nobjects without any fine-tuning. We achieve this by decomposing the 6D pose\ninto viewpoint, in-plane rotation around the camera optical axis and\ntranslation, and introducing novel lightweight modules for estimating each\ncomponent in a cascaded manner. The resulting network contains less than 4M\nparameters while demonstrating excellent performance on the challenging T-LESS\nand Occluded LINEMOD datasets without any dataset-specific training. We show\nthat OVE6D outperforms some contemporary deep learning-based pose estimation\nmethods specifically trained for individual objects or datasets with real-world\ntraining data.\n</p>\n<p>The implementation and the pre-trained model will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Dingding Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkil&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Human Motion Prediction: A Survey. (arXiv:2203.01593v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01593","description":"<p>3D human motion prediction, predicting future poses from a given sequence, is\nan issue of great significance and challenge in computer vision and machine\nintelligence, which can help machines in understanding human behaviors. Due to\nthe increasing development and understanding of Deep Neural Networks (DNNs) and\nthe availability of large-scale human motion datasets, the human motion\nprediction has been remarkably advanced with a surge of interest among academia\nand industrial community. In this context, a comprehensive survey on 3D human\nmotion prediction is conducted for the purpose of retrospecting and analyzing\nrelevant works from existing released literature. In addition, a pertinent\ntaxonomy is constructed to categorize these existing approaches for 3D human\nmotion prediction. In this survey, relevant methods are categorized into three\ncategories: human pose representation, network structure design, and\n\\textit{prediction target}. We systematically review all relevant journal and\nconference papers in the field of human motion prediction since 2015, which are\npresented in detail based on proposed categorizations in this survey.\nFurthermore, the outline for the public benchmark datasets, evaluation\ncriteria, and performance comparisons are respectively presented in this paper.\nThe limitations of the state-of-the-art methods are discussed as well, hoping\nfor paving the way for future explorations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1\">Kedi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Beiqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruili Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2203.01601v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01601","description":"<p>Handwritten mathematical expression recognition (HMER) is a challenging task\nthat has many potential applications. Recent methods for HMER have achieved\noutstanding performance with an encoder-decoder architecture. However, these\nmethods adhere to the paradigm that the prediction is made \"from one character\nto another\", which inevitably yields prediction errors due to the complicated\nstructures of mathematical expressions or crabbed handwritings. In this paper,\nwe propose a simple and efficient method for HMER, which is the first to\nincorporate syntax information into an encoder-decoder network. Specifically,\nwe present a set of grammar rules for converting the LaTeX markup sequence of\neach expression into a parsing tree; then, we model the markup sequence\nprediction as a tree traverse process with a deep neural network. In this way,\nthe proposed method can effectively describe the syntax context of expressions,\navoiding the structure prediction errors of HMER. Experiments on two benchmark\ndatasets demonstrate that our method achieves significantly better recognition\nperformance than prior arts. To further validate the effectiveness of our\nmethod, we create a large-scale dataset consisting of 100k handwritten\nmathematical expression images acquired from ten thousand writers. The source\ncode, new dataset, and pre-trained models of this work will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dikubab_W/0/1/0/all/0/1\">Wondimu Dikubab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with Fuchs dystrophy. (arXiv:2203.01882v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01882","description":"<p>To estimate the corneal endothelial parameters from specular microscopy\nimages depicting cornea guttata (Fuchs endothelial dystrophy), we propose a new\ndeep learning methodology that includes a novel attention mechanism named\nfeedback non-local attention (fNLA). Our approach first infers the cell edges,\nthen selects the cells that are well detected, and finally applies a\npostprocessing method to correct mistakes and provide the binary segmentation\nfrom which the corneal parameters are estimated (cell density [ECD],\ncoefficient of variation [CV], and hexagonality [HEX]). In this study, we\nanalyzed 1203 images acquired with a Topcon SP-1P microscope, 500 of which\ncontained guttae. Manual segmentation was performed in all images. We compared\nthe results of different networks (UNet, ResUNeXt, DenseUNets, UNet++) and\nfound that DenseUNets with fNLA provided the best performance, with a mean\nabsolute error of 23.16 [cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%]\nin HEX, which was 3-6 times smaller than the error obtained by Topcon's\nbuilt-in software. Our approach handled the cells affected by guttae remarkably\nwell, detecting cell edges occluded by small guttae while discarding areas\ncovered by large guttae. fNLA made use of the local information, providing\nsharper edges in guttae areas and better results in the selection of\nwell-detected cells. Overall, the proposed method obtained reliable and\naccurate estimations in extremely challenging specular images with guttae,\nbeing the first method in the literature to solve this problem adequately. Code\nis available in our GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rooij_J/0/1/0/all/0/1\">Jeroen van Rooij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dooren_B/0/1/0/all/0/1\">Bart T.H. van Dooren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemij_H/0/1/0/all/0/1\">Hans G. Lemij</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islamaj_E/0/1/0/all/0/1\">Esma Islamaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vliet_L/0/1/0/all/0/1\">Lucas J. van Vliet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vermeer_K/0/1/0/all/0/1\">Koenraad A. Vermeer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCTrack: Temporal Contexts for Aerial Tracking. (arXiv:2203.01885v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01885","description":"<p>Temporal contexts among consecutive frames are far from being fully utilized\nin existing visual trackers. In this work, we present TCTrack, a comprehensive\nframework to fully exploit temporal contexts for aerial tracking. The temporal\ncontexts are incorporated at \\textbf{two levels}: the extraction of\n\\textbf{features} and the refinement of \\textbf{similarity maps}. Specifically,\nfor feature extraction, an online temporally adaptive convolution is proposed\nto enhance the spatial features using temporal information, which is achieved\nby dynamically calibrating the convolution weights according to the previous\nframes. For similarity map refinement, we propose an adaptive temporal\ntransformer, which first effectively encodes temporal knowledge in a\nmemory-efficient way, before the temporal knowledge is decoded for accurate\nadjustment of the similarity map. TCTrack is effective and efficient:\nevaluation on four aerial tracking benchmarks shows its impressive performance;\nreal-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX\nXavier.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Changhong Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NUQ: A Noise Metric for Diffusion MRI via Uncertainty Discrepancy Quantification. (arXiv:2203.01921v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01921","description":"<p>Diffusion MRI (dMRI) is the only non-invasive technique sensitive to tissue\nmicro-architecture, which can, in turn, be used to reconstruct tissue\nmicrostructure and white matter pathways. The accuracy of such tasks is\nhampered by the low signal-to-noise ratio in dMRI. Today, the noise is\ncharacterized mainly by visual inspection of residual maps and estimated\nstandard deviation. However, it is hard to estimate the impact of noise on\ndownstream tasks based only on such qualitative assessments. To address this\nissue, we introduce a novel metric, Noise Uncertainty Quantification (NUQ), for\nquantitative image quality analysis in the absence of a ground truth reference\nimage. NUQ uses a recent Bayesian formulation of dMRI models to estimate the\nuncertainty of microstructural measures. Specifically, NUQ uses the maximum\nmean discrepancy metric to compute a pooled quality score by comparing samples\ndrawn from the posterior distribution of the microstructure measures. We show\nthat NUQ allows a fine-grained analysis of noise, capturing details that are\nvisually imperceptible. We perform qualitative and quantitative comparisons on\nreal datasets, showing that NUQ generates consistent scores across different\ndenoisers and acquisitions. Lastly, by using NUQ on a cohort of schizophrenics\nand controls, we quantify the substantial impact of denoising on group\ndifferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fadnavis_S/0/1/0/all/0/1\">Shreyas Fadnavis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sjolund_J/0/1/0/all/0/1\">Jens Sj&#xf6;lund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_A/0/1/0/all/0/1\">Anders Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garyfallidis_E/0/1/0/all/0/1\">Eleftherios Garyfallidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining. (arXiv:2203.01969v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.01969","description":"<p>Retrospective analysis of brain MRI scans acquired in the clinic has the\npotential to enable neuroimaging studies with sample sizes much larger than\nthose found in research datasets. However, analysing such clinical images \"in\nthe wild\" is challenging, since subjects are scanned with highly variable\nprotocols (MR contrast, resolution, orientation, etc.). Nevertheless, recent\nadvances in convolutional neural networks (CNNs) and domain randomisation for\nimage segmentation, best represented by the publicly available method SynthSeg,\nmay enable morphometry of clinical MRI at scale. In this work, we first\nevaluate SynthSeg on an uncurated, heterogeneous dataset of more than 10,000\nscans acquired at Massachusetts General Hospital. We show that SynthSeg is\ngenerally robust, but frequently falters on scans with low signal-to-noise\nratio or poor tissue contrast. Next, we propose SynthSeg+, a novel method that\ngreatly mitigates these problems using a hierarchy of conditional segmentation\nand denoising CNNs. We show that this method is considerably more robust than\nSynthSeg, while also outperforming cascaded networks and state-of-the-art\nsegmentation denoising methods. Finally, we apply our approach to a\nproof-of-concept volumetric study of ageing, where it closely replicates\natrophy patterns observed in research studies conducted on high-quality, 1mm,\nT1-weighted scans. The code and trained model are publicly available at\nhttps://github.com/BBillot/SynthSeg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1\">Benjamin Billot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colin_M/0/1/0/all/0/1\">Magdamo Colin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_S/0/1/0/all/0/1\">Sean E. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sudeshna Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan. E. Iglesias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Neural Architecture Search for Lightweight Dense Prediction Networks. (arXiv:2203.01994v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01994","description":"<p>We present LDP, a lightweight dense prediction neural architecture search\n(NAS) framework. Starting from a pre-defined generic backbone, LDP applies the\nnovel Assisted Tabu Search for efficient architecture exploration. LDP is fast\nand suitable for various dense estimation problems, unlike previous NAS methods\nthat are either computational demanding or deployed only for a single subtask.\nThe performance of LPD is evaluated on monocular depth estimation, semantic\nsegmentation, and image super-resolution tasks on diverse datasets, including\nNYU-Depth-v2, KITTI, Cityscapes, COCO-stuff, DIV2K, Set5, Set14, BSD100,\nUrban100. Experiments show that the proposed framework yields consistent\nimprovements on all tested dense prediction tasks, while being $5\\%-315\\%$ more\ncompact in terms of the number of model parameters than prior arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1\">Lam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1\">Esa Rahtu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1\">Janne Heikkila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}