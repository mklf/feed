{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Flexible Clustering Pipeline for Mining Text Intentions. (arXiv:2202.01211v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01211","description":"<p>Mining the latent intentions from large volumes of natural language inputs is\na key step to help data analysts design and refine Intelligent Virtual\nAssistants (IVAs) for customer service and sales support. We created a flexible\nand scalable clustering pipeline within the Verint Intent Manager (VIM) that\nintegrates the fine-tuning of language models, a high performing k-NN library\nand community detection techniques to help analysts quickly surface and\norganize relevant user intentions from conversational texts. The fine-tuning\nstep is necessary because pre-trained language models cannot encode texts to\nefficiently surface particular clustering structures when the target texts are\nfrom an unseen domain or the clustering task is not topic detection. We\ndescribe the pipeline and demonstrate its performance using BERT on three\nreal-world text mining tasks. As deployed in the VIM application, this\nclustering pipeline produces high quality results, improving the performance of\ndata analysts and reducing the time it takes to surface intentions from\ncustomer service data, thereby reducing the time it takes to build and deploy\nIVAs in new domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaver_I/0/1/0/all/0/1\">Ian Beaver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. (arXiv:2202.01279v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01279","description":"<p>PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1\">Srulik Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1\">Jason Alan Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1\">Maged S. Al-shaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shanya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almubarak_K/0/1/0/all/0/1\">Khalid Almubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mike Tian-Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR-Aware End-to-end Neural Diarization. (arXiv:2202.01286v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01286","description":"<p>We present a Conformer-based end-to-end neural diarization (EEND) model that\nuses both acoustic input and features derived from an automatic speech\nrecognition (ASR) model. Two categories of features are explored: features\nderived directly from ASR output (phones, position-in-word and word boundaries)\nand features derived from a lexical speaker change detection model, trained by\nfine-tuning a pretrained BERT model on the ASR output. Three modifications to\nthe Conformer-based EEND architecture are proposed to incorporate the features.\nFirst, ASR features are concatenated with acoustic features. Second, we propose\na new attention mechanism called contextualized self-attention that utilizes\nASR features to build robust speaker representations. Finally, multi-task\nlearning is used to train the model to minimize classification loss for the ASR\nfeatures along with diarization loss. Experiments on the two-speaker English\nconversations of Switchboard+SRE data sets show that multi-task learning with\nposition-in-word information is the most effective way of utilizing ASR\nfeatures, reducing the diarization error rate (DER) by 20% relative to the\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khare_A/0/1/0/all/0/1\">Aparna Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_E/0/1/0/all/0/1\">Eunjung Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of Online Hate on Reddit and 4chan: A Case Study of the 2020 US Election. (arXiv:2202.01302v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01302","description":"<p>The rapid integration of the Internet into our daily lives has led to many\nbenefits but also to a number of new, wide-spread threats such as online hate,\ntrolling, bullying, and generally aggressive behaviours. While research has\ntraditionally explored online hate, in particular, on one platform, the reality\nis that such hate is a phenomenon that often makes use of multiple online\nnetworks. In this article, we seek to advance the discussion into online hate\nby harnessing a comparative approach, where we make use of various Natural\nLanguage Processing (NLP) techniques to computationally analyse hateful content\nfrom Reddit and 4chan relating to the 2020 US Presidential Elections. Our\nfindings show how content and posting activity can differ depending on the\nplatform being used. Through this, we provide initial comparison into the\nplatform-specific behaviours of online hate, and how different platforms can\nserve specific purposes. We further provide several avenues for future research\nutilising a cross-platform approach so as to gain a more comprehensive\nunderstanding of the global hate ecosystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zahrah_F/0/1/0/all/0/1\">Fatima Zahrah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1\">Jason R. C. Nurse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldsmith_M/0/1/0/all/0/1\">Michael Goldsmith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regression Transformer: Concurrent Conditional Generation and Regression by Blending Numerical and Textual Tokens. (arXiv:2202.01338v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01338","description":"<p>We report the Regression Transformer (RT), a method that abstracts regression\nas a conditional sequence modeling problem. The RT casts continuous properties\nas sequences of numerical tokens and encodes them jointly with conventional\ntokens. This yields a dichotomous model that can seamlessly transition between\nsolving regression tasks and conditional generation tasks; solely governed by\nthe mask location. We propose several extensions to the XLNet objective and\nadopt an alternating training scheme to concurrently optimize property\nprediction and conditional text generation based on a self-consistency loss.\n</p>\n<p>Our experiments on both chemical and protein languages demonstrate that the\nperformance of traditional regression models can be surpassed despite training\nwith cross entropy loss. Importantly, priming the same model with continuous\nproperties yields a highly competitive conditional generative models that\noutperforms specialized approaches in a constrained property optimization\nbenchmark. In sum, the Regression Transformer opens the door for \"swiss army\nknife\" models that excel at both regression and conditional generation. This\nfinds application particularly in property-driven, local exploration of the\nchemical or protein space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Born_J/0/1/0/all/0/1\">Jannis Born</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manica_M/0/1/0/all/0/1\">Matteo Manica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mSLAM: Massively multilingual joint pre-training for speech and text. (arXiv:2202.01374v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01374","description":"<p>We present mSLAM, a multilingual Speech and LAnguage Model that learns\ncross-lingual cross-modal representations of speech and text by pre-training\njointly on large amounts of unlabeled speech and text in multiple languages.\nmSLAM combines w2v-BERT pre-training on speech with SpanBERT pre-training on\ncharacter-level text, along with Connectionist Temporal Classification (CTC)\nlosses on paired speech and transcript data, to learn a single model capable of\nlearning from and representing both speech and text signals in a shared\nrepresentation space. We evaluate mSLAM on several downstream speech\nunderstanding tasks and find that joint pre-training with text improves quality\non speech translation, speech intent classification and speech language-ID\nwhile being competitive on multilingual ASR, when compared against speech-only\npre-training. Our speech translation model demonstrates zero-shot text\ntranslation without seeing any text translation data, providing evidence for\ncross-modal alignment of representations. mSLAM also benefits from multi-modal\nfine-tuning, further improving the quality of speech translation by directly\nleveraging text translation data during the fine-tuning process. Our empirical\nanalysis highlights several opportunities and challenges arising from\nlarge-scale multimodal pre-training, suggesting directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1\">Colin Cherry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1\">Simran Khanuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Speech Recognition and Audio Captioning. (arXiv:2202.01405v1 [eess.AS])","link":"http://arxiv.org/abs/2202.01405","description":"<p>Speech samples recorded in both indoor and outdoor environments are often\ncontaminated with secondary audio sources. Most end-to-end monaural speech\nrecognition systems either remove these background sounds using speech\nenhancement or train noise-robust models. For better model interpretability and\nholistic understanding, we aim to bring together the growing field of automated\naudio captioning (AAC) and the thoroughly studied automatic speech recognition\n(ASR). The goal of AAC is to generate natural language descriptions of contents\nin audio samples. We propose several approaches for end-to-end joint modeling\nof ASR and AAC tasks and demonstrate their advantages over traditional\napproaches, which model these tasks independently. A major hurdle in evaluating\nour proposed approach is the lack of labeled audio datasets with both speech\ntranscriptions and audio captions. Therefore we also create a multi-task\ndataset by mixing the clean speech Wall Street Journal corpus with multiple\nlevels of background noises chosen from the AudioCaps dataset. We also perform\nextensive experimental evaluation and show improvements of our proposed methods\nas compared to existing state-of-the-art ASR and AAC methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Narisetty_C/0/1/0/all/0/1\">Chaitanya Narisetty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsunoo_E/0/1/0/all/0/1\">Emiru Tsunoo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kashiwagi_Y/0/1/0/all/0/1\">Yosuke Kashiwagi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hentschel_M/0/1/0/all/0/1\">Michael Hentschel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFA: TDNN with Multi-scale Frequency-channel Attention for Text-independent Speaker Verification with Short Utterances. (arXiv:2202.01624v1 [cs.SD])","link":"http://arxiv.org/abs/2202.01624","description":"<p>The time delay neural network (TDNN) represents one of the state-of-the-art\nof neural solutions to text-independent speaker verification. However, they\nrequire a large number of filters to capture the speaker characteristics at any\nlocal frequency region. In addition, the performance of such systems may\ndegrade under short utterance scenarios. To address these issues, we propose a\nmulti-scale frequency-channel attention (MFA), where we characterize speakers\nat different scales through a novel dual-path design which consists of a\nconvolutional neural network and TDNN. We evaluate the proposed MFA on the\nVoxCeleb database and observe that the proposed framework with MFA can achieve\nstate-of-the-art performance while reducing parameters and computation\ncomplexity. Further, the MFA mechanism is found to be effective for speaker\nverification with short test utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rohan Kumar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The relationship between sentiment score and COVID-19 cases in the United States. (arXiv:2202.01708v1 [cs.CY])","link":"http://arxiv.org/abs/2202.01708","description":"<p>The coronavirus disease (COVID-19) continues to have devastating effects\nacross the globe. No nation has been free from the uncertainty brought by this\npandemic. The health, social and economic tolls associated with it are causing\nstrong emotions and spreading fear in people of all ages, genders, and races.\nSince the beginning of the COVID-19 pandemic, many have expressed their\nfeelings and opinions related to a wide range of aspects of their lives via\nTwitter. In this study, we consider a framework for extracting sentiment scores\nand opinions from COVID-19 related tweets. We connect users' sentiment with\nCOVID-19 cases across the USA and investigate the effect of specific COVID-19\nmilestones on public sentiment. The results of this work may help with the\ndevelopment of pandemic-related legislation, serve as a guide for scientific\nwork, as well as inform and educate the public on core issues related to the\npandemic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luu_T/0/1/0/all/0/1\">Truong Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Follmann_R/0/1/0/all/0/1\">Rosangela Follmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Coherent and Consistent Use of Entities in Narrative Generation. (arXiv:2202.01709v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01709","description":"<p>Large pre-trained language models (LMs) have demonstrated impressive\ncapabilities in generating long, fluent text; however, there is little to no\nanalysis on their ability to maintain entity coherence and consistency. In this\nwork, we focus on the end task of narrative generation and systematically\nanalyse the long-range entity coherence and consistency in generated stories.\nFirst, we propose a set of automatic metrics for measuring model performance in\nterms of entity usage. Given these metrics, we quantify the limitations of\ncurrent LMs. Next, we propose augmenting a pre-trained LM with a dynamic entity\nmemory in an end-to-end manner by using an auxiliary entity-related loss for\nguiding the reads and writes to the memory. We demonstrate that the dynamic\nentity memory increases entity coherence according to both automatic and human\njudgment and helps preserving entity-related information especially in settings\nwith a limited context window. Finally, we also validate that our automatic\nmetrics are correlated with human ratings and serve as a good indicator of the\nquality of generated stories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papalampidi_P/0/1/0/all/0/1\">Pinelopi Papalampidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kris Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocisky_T/0/1/0/all/0/1\">Tomas Kocisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension. (arXiv:2202.01764v1 [cs.CL])","link":"http://arxiv.org/abs/2202.01764","description":"<p>Question Answering (QA) is a task in which a machine understands a given\ndocument and a question to find an answer. Despite impressive progress in the\nNLP area, QA is still a challenging problem, especially for non-English\nlanguages due to the lack of annotated datasets. In this paper, we present the\nJapanese Question Answering Dataset, JaQuAD, which is annotated by humans.\nJaQuAD consists of 39,696 extractive question-answer pairs on Japanese\nWikipedia articles. We finetuned a baseline model which achieves 78.92% for F1\nscore and 63.38% for EM on test set. The dataset and our experiments are\navailable at https://github.com/SkelterLabsInc/JaQuAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+So_B/0/1/0/all/0/1\">ByungHoon So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_K/0/1/0/all/0/1\">Kyuhong Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_K/0/1/0/all/0/1\">Kyungwon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Seongjin Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Language Models for Interactive Decision-Making. (arXiv:2202.01771v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01771","description":"<p>Language model (LM) pre-training has proven useful for a wide variety of\nlanguage processing tasks, but can such pre-training be leveraged for more\ngeneral machine learning problems? We investigate the effectiveness of language\nmodeling to scaffold learning and generalization in autonomous decision-making.\nWe describe a framework for imitation learning in which goals and observations\nare represented as a sequence of embeddings, and translated into actions using\na policy network initialized with a pre-trained transformer LM. We demonstrate\nthat this framework enables effective combinatorial generalization across\ndifferent environments, such as VirtualHome and BabyAI. In particular, for test\ntasks involving novel goals or novel scenes, initializing policies with\nlanguage models improves task completion rates by 43.6% in VirtualHome. We\nhypothesize and investigate three possible factors underlying the effectiveness\nof LM-based policy initialization. We find that sequential representations (vs.\nfixed-dimensional feature vectors) and the LM objective (not just the\ntransformer architecture) are both important for generalization. Surprisingly,\nhowever, the format of the policy inputs encoding (e.g. as a natural language\nstring vs. an arbitrary sequential encoding) has little influence. Together,\nthese results suggest that language modeling induces representations that are\nuseful for modeling not just language, but also goals and plans; these\nrepresentations can aid learning and generalization even outside of language\nprocessing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1\">Xavier Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clinton Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Akyurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Wrong Answer or a Wrong Question? An Intricate Relationship between Question Reformulation and Answer Selection in Conversational Question Answering. (arXiv:2010.06835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.06835","description":"<p>The dependency between an adequate question formulation and correct answer\nselection is a very intriguing but still underexplored area. In this paper, we\nshow that question rewriting (QR) of the conversational context allows to shed\nmore light on this phenomenon and also use it to evaluate robustness of\ndifferent answer selection approaches. We introduce a simple framework that\nenables an automated analysis of the conversational question answering (QA)\nperformance using question rewrites, and present the results of this analysis\non the TREC CAsT and QuAC (CANARD) datasets. Our experiments uncover\nsensitivity to question formulation of the popular state-of-the-art models for\nreading comprehension and passage ranking. Our results demonstrate that the\nreading comprehension model is insensitive to question formulation, while the\npassage ranking changes dramatically with a little variation in the input\nquestion. The benefit of QR is that it allows us to pinpoint and group such\ncases automatically. We show how to use this methodology to verify whether QA\nmodels are really learning the task or just finding shortcuts in the dataset,\nand better understand the frequent types of error they make.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhucheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantha_R/0/1/0/all/0/1\">Raviteja Anantha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00269","description":"<p>Pre-trained models learn contextualized word representations on large-scale\ntext corpus through a self-supervised learning method, which has achieved\npromising performance after fine-tuning. These models, however, suffer from\npoor robustness and lack of interpretability. Pre-trained models with knowledge\ninjection, which we call knowledge enhanced pre-trained models (KEPTMs),\npossess deep understanding and logical reasoning and introduce interpretability\nto some extent. In this survey, we provide a comprehensive overview of KEPTMs\nfor natural language processing. We first introduce the progress of pre-trained\nmodels and knowledge representation learning. Then we systematically categorize\nexisting KEPTMs from three different perspectives. Finally, we outline some\npotential directions of KEPTMs for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Gang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinghui Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting intermediate convolutional layers in unsupervised acoustic word classification. (arXiv:2110.02375v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.02375","description":"<p>Understanding how deep convolutional neural networks classify data has been\nsubject to extensive research. This paper proposes a technique to visualize and\ninterpret intermediate layers of unsupervised deep convolutional networks by\naveraging over individual feature maps in each convolutional layer and\ninferring underlying distributions of words with non-linear regression\ntechniques. A GAN-based architecture (ciwGAN <a href=\"/abs/2006.02951\">arXiv:2006.02951</a>) that includes a\nGenerator, a Discriminator, and a classifier was trained on unlabeled sliced\nlexical items from TIMIT. The training process results in a deep convolutional\nnetwork that learns to classify words into discrete classes only from the\nrequirement of the Generator to output informative data. This classifier\nnetwork has no access to the training data -- only to the generated data. We\npropose a technique to visualize individual convolutional layers in the\nclassifier that yields highly informative time-series data for each\nconvolutional layer and apply it to unobserved test data. Using non-linear\nregression, we infer underlying distributions for each word which allows us to\nanalyze both absolute values and shapes of individual words at different\nconvolutional layers, as well as perform hypothesis testing on their acoustic\nproperties. The technique also allows us to test individual phone contrasts and\nhow they are represented at each layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in End-to-End Automatic Speech Recognition. (arXiv:2111.01690v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2111.01690","description":"<p>Recently, the speech community is seeing a significant trend of moving from\ndeep neural network based hybrid modeling to end-to-end (E2E) modeling for\nautomatic speech recognition (ASR). While E2E models achieve the\nstate-of-the-art results in most benchmarks in terms of ASR accuracy, hybrid\nmodels are still used in a large proportion of commercial ASR systems at the\ncurrent time. There are lots of practical factors that affect the production\nmodel deployment decision. Traditional hybrid models, being optimized for\nproduction for decades, are usually good at these factors. Without providing\nexcellent solutions to all these factors, it is hard for E2E models to be\nwidely commercialized. In this paper, we will overview the recent advances in\nE2E models, focusing on technologies addressing those challenges from the\nindustry's perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text classification problems via BERT embedding method and graph convolutional neural network. (arXiv:2111.15379v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.15379","description":"<p>This paper presents the novel way combining the BERT embedding method and the\ngraph convolutional neural network. This combination is employed to solve the\ntext classification problem. Initially, we apply the BERT embedding method to\nthe texts (in the BBC news dataset and the IMDB movie reviews dataset) in order\nto transform all the texts to numerical vector. Then, the graph convolutional\nneural network will be applied to these numerical vectors to classify these\ntexts into their ap-propriate classes/labels. Experiments show that the\nperformance of the graph convolutional neural network model is better than the\nperfor-mances of the combination of the BERT embedding method with clas-sical\nmachine learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1\">Loc Hoang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_A/0/1/0/all/0/1\">An Mai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Structured Inference with Randomization. (arXiv:2112.03638v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.03638","description":"<p>Deep discrete structured models have seen considerable progress recently, but\ntraditional inference using dynamic programming (DP) typically works with a\nsmall number of states (less than hundreds), which severely limits model\ncapacity. At the same time, across machine learning, there is a recent trend of\nusing randomized truncation techniques to accelerate computations involving\nlarge sums. Here, we propose a family of randomized dynamic programming (RDP)\nalgorithms for scaling structured models to tens of thousands of latent states.\nOur method is widely applicable to classical DP-based inference (partition,\nmarginal, reparameterization, entropy) and different graph structures (chains,\ntrees, and more general hypergraphs). It is also compatible with automatic\ndifferentiation: it can be integrated with neural networks seamlessly and\nlearned with gradient-based optimizers. Our core technique approximates the\nsum-product by restricting and reweighting DP on a small subset of nodes, which\nreduces computation by orders of magnitude. We further achieve low bias and\nvariance via Rao-Blackwellization and importance sampling. Experiments over\ndifferent graphs demonstrate the accuracy and efficiency of our approach.\nFurthermore, when using RDP for training a structured variational autoencoder\nwith a scaled inference network, we achieve better test likelihood than\nbaselines and successfully prevent posterior collapse\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1\">John P. Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of Training Data Diversity on Stability and Robustness. (arXiv:2202.01208v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01208","description":"<p>Ultrasound b-mode imaging is a qualitative approach and diagnostic quality\nstrongly depends on operators' training and experience. Quantitative approaches\ncan provide information about tissue properties; therefore, can be used for\nidentifying various tissue types, e.g., speed-of-sound in the tissue can be\nused as a biomarker for tissue malignancy, especially in breast imaging. Recent\nstudies showed the possibility of speed-of-sound reconstruction using deep\nneural networks that are fully trained on simulated data. However, because of\nthe ever present domain shift between simulated and measured data, the\nstability and performance of these models in real setups are still under\ndebate. In this study, we investigated the impacts of training data diversity\non the robustness of these networks by using multiple kinds of geometrical and\nnatural simulated phantom structures. On the simulated data, we investigated\nthe performance of the networks on out-of-domain echogenicity, geometries, and\nin the presence of noise. We further inspected the stability of employing such\ntissue modeling in a real data acquisition setup. We demonstrated that training\nthe network with a joint set of datasets including both geometrical and natural\ntissue models improves the stability of the predicted speed-of-sound values\nboth on simulated and measured data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jush_F/0/1/0/all/0/1\">Farnaz Khun Jush</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biele_M/0/1/0/all/0/1\">Markus Biele</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dueppenbecker_P/0/1/0/all/0/1\">Peter M. Dueppenbecker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Semantic Descriptors for Image-Based Localization. (arXiv:2202.01212v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01212","description":"<p>Vision based solutions for the localization of vehicles have become popular\nrecently. We employ an image retrieval based visual localization approach. The\ndatabase images are kept with GPS coordinates and the location of the retrieved\ndatabase image serves as an approximate position of the query image. We show\nthat localization can be performed via descriptors solely extracted from\nsemantically segmented images. It is reliable especially when the environment\nis subjected to severe illumination and seasonal changes. Our experiments\nreveal that the localization performance of a semantic descriptor can increase\nup to the level of state-of-the-art RGB image based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cinaroglu_I/0/1/0/all/0/1\">Ibrahim Cinaroglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastanlar_Y/0/1/0/all/0/1\">Yalin Bastanlar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated processing of X-ray computed tomography images via panoptic segmentation for modeling woven composite textiles. (arXiv:2202.01265v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01265","description":"<p>A new, machine learning-based approach for automatically generating 3D\ndigital geometries of woven composite textiles is proposed to overcome the\nlimitations of existing analytical descriptions and segmentation methods. In\nthis approach, panoptic segmentation is leveraged to produce instance segmented\nsemantic masks from X-ray computed tomography (CT) images. This effort\nrepresents the first deep learning based automated process for segmenting\nunique yarn instances in a woven composite textile. Furthermore, it improves on\nexisting methods by providing instance-level segmentation on low contrast CT\ndatasets. Frame-to-frame instance tracking is accomplished via an\nintersection-over-union (IoU) approach adopted from video panoptic segmentation\nfor assembling a 3D geometric model. A corrective recognition algorithm is\ndeveloped to improve the recognition quality (RQ). The panoptic quality (PQ)\nmetric is adopted to provide a new universal evaluation metric for\nreconstructed woven composite textiles. It is found that the panoptic\nsegmentation network generalizes well to new CT images that are similar to the\ntraining set but does not extrapolate well to CT images of differing geometry,\ntexture, and contrast. The utility of this approach is demonstrated by\ncapturing yarn flow directions, contact regions between individual yarns, and\nthe spatially varying cross-sectional areas of the yarns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allred_A/0/1/0/all/0/1\">Aaron Allred</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbott_L/0/1/0/all/0/1\">Lauren J. Abbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doostan_A/0/1/0/all/0/1\">Alireza Doostan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maute_K/0/1/0/all/0/1\">Kurt Maute</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Images: Label Noise Transition Matrix Estimation for Tasks with Lower-Quality Features. (arXiv:2202.01273v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01273","description":"<p>The label noise transition matrix, denoting the transition probabilities from\nclean labels to noisy labels, is crucial knowledge for designing statistically\nrobust solutions. Existing estimators for noise transition matrices, e.g.,\nusing either anchor points or clusterability, focus on computer vision tasks\nthat are relatively easier to obtain high-quality representations. However, for\nother tasks with lower-quality features, the uninformative variables may\nobscure the useful counterpart and make anchor-point or clusterability\nconditions hard to satisfy. We empirically observe the failures of these\napproaches on a number of commonly used datasets. In this paper, to handle this\nissue, we propose a generally practical information-theoretic approach to\ndown-weight the less informative parts of the lower-quality features. The\nsalient technical challenge is to compute the relevant information-theoretical\nmetrics using only noisy labels instead of clean ones. We prove that the\ncelebrated $f$-mutual information measure can often preserve the order when\ncalculated using noisy labels. The necessity and effectiveness of the proposed\nmethod is also demonstrated by evaluating the estimation error on a varied set\nof tabular data and text classification tasks with lower-quality features. Code\nis available at github.com/UCSC-REAL/Est-T-MI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhaowei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cyclical Pruning for Sparse Neural Networks. (arXiv:2202.01290v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01290","description":"<p>Current methods for pruning neural network weights iteratively apply\nmagnitude-based pruning on the model weights and re-train the resulting model\nto recover lost accuracy. In this work, we show that such strategies do not\nallow for the recovery of erroneously pruned weights. To enable weight\nrecovery, we propose a simple strategy called \\textit{cyclical pruning} which\nrequires the pruning schedule to be periodic and allows for weights pruned\nerroneously in one cycle to recover in subsequent ones. Experimental results on\nboth linear models and large-scale deep neural networks show that cyclical\npruning outperforms existing pruning algorithms, especially at high sparsity\nratios. Our approach is easy to tune and can be readily incorporated into\nexisting pruning pipelines to boost performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_S/0/1/0/all/0/1\">Suraj Srinivas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzmin_A/0/1/0/all/0/1\">Andrey Kuzmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1\">Markus Nagel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baalen_M/0/1/0/all/0/1\">Mart van Baalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skliar_A/0/1/0/all/0/1\">Andrii Skliar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blankevoort_T/0/1/0/all/0/1\">Tijmen Blankevoort</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Resolution Factor Graph Based Stereo Correspondence Algorithm. (arXiv:2202.01309v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01309","description":"<p>A dense depth-map of a scene at an arbitrary view orientation can be\nestimated from dense view correspondences among multiple lower-dimensional\nviews of the scene. These low-dimensional view correspondences are dependent on\nthe geometrical relationship among the views and the scene. Determining dense\nview correspondences is difficult in part due to presence of homogeneous\nregions in the scene and due to presence of occluded regions and illumination\ndifferences among the views. We present a new multi-resolution factor\ngraph-based stereo matching algorithm (MR-FGS) that utilizes both intra- and\ninter-resolution dependencies among the views as well as among the disparity\nestimates. The proposed framework allows exchange of information among multiple\nresolutions of the correspondence problem and is useful for handling larger\nhomogeneous regions in a scene. The MR-FGS algorithm was evaluated\nqualitatively and quantitatively using stereo pairs in the Middlebury stereo\nbenchmark dataset based on commonly used performance measures. When compared to\na recently developed factor graph model (FGS), the MR-FGS algorithm provided\nmore accurate disparity estimates without requiring the commonly used\npost-processing procedure known as the left-right consistency check. The\nmulti-resolution dependency constraint within the factor-graph model\nsignificantly improved contrast along depth boundaries in the MR-FGS generated\ndisparity maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_H/0/1/0/all/0/1\">Hanieh Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1\">Madhusudhanan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PanoDepth: A Two-Stage Approach for Monocular Omnidirectional Depth Estimation. (arXiv:2202.01323v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01323","description":"<p>Omnidirectional 3D information is essential for a wide range of applications\nsuch as Virtual Reality, Autonomous Driving, Robotics, etc. In this paper, we\npropose a novel, model-agnostic, two-stage pipeline for omnidirectional\nmonocular depth estimation. Our proposed framework PanoDepth takes one 360\nimage as input, produces one or more synthesized views in the first stage, and\nfeeds the original image and the synthesized images into the subsequent stereo\nmatching stage. In the second stage, we propose a differentiable Spherical\nWarping Layer to handle omnidirectional stereo geometry efficiently and\neffectively. By utilizing the explicit stereo-based geometric constraints in\nthe stereo matching stage, PanoDepth can generate dense high-quality depth. We\nconducted extensive experiments and ablation studies to evaluate PanoDepth with\nboth the full pipeline as well as the individual modules in each stage. Our\nresults show that PanoDepth outperforms the state-of-the-art approaches by a\nlarge margin for 360 monocular depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhixin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Ye Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizability of Machine Learning Models: Quantitative Evaluation of Three Methodological Pitfalls. (arXiv:2202.01337v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01337","description":"<p>Despite the great potential of machine learning, the lack of generalizability\nhas hindered the widespread adoption of these technologies in routine clinical\npractice. We investigate three methodological pitfalls: (1) violation of\nindependence assumption, (2) model evaluation with an inappropriate performance\nindicator, and (3) batch effect and how these pitfalls could affect the\ngeneralizability of machine learning models. We implement random forest and\ndeep convolutional neural network models using several medical imaging\ndatasets, including head and neck CT, lung CT, chest X-Ray, and\nhistopathological images, to quantify and illustrate the effect of these\npitfalls. We develop these models with and without the pitfall and compare the\nperformance of the resulting models in terms of accuracy, precision, recall,\nand F1 score. Our results showed that violation of the independence assumption\ncould substantially affect model generalizability. More specifically, (I)\napplying oversampling before splitting data into train, validation and test\nsets; (II) performing data augmentation before splitting data; (III)\ndistributing data points for a subject across training, validation, and test\nsets; and (IV) applying feature selection before splitting data led to\nsuperficial boosts in model performance. We also observed that inappropriate\nperformance indicators could lead to erroneous conclusions. Also, batch effect\ncould lead to developing models that lack generalizability. The aforementioned\nmethodological pitfalls lead to machine learning models with over-optimistic\nperformance. These errors, if made, cannot be captured using internal model\nevaluation, and the inaccurate predictions made by the model may lead to wrong\nconclusions and interpretations. Therefore, avoiding these pitfalls is a\nnecessary condition for developing generalizable models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maleki_F/0/1/0/all/0/1\">Farhad Maleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovens_K/0/1/0/all/0/1\">Katie Ovens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rajiv Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reinhold_C/0/1/0/all/0/1\">Caroline Reinhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spatz_A/0/1/0/all/0/1\">Alan Spatz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forghani_R/0/1/0/all/0/1\">Reza Forghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Sub-skeleton Trajectories for Interpretable Recognition of Sign Language. (arXiv:2202.01390v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01390","description":"<p>Recent advances in tracking sensors and pose estimation software enable smart\nsystems to use trajectories of skeleton joint locations for supervised\nlearning. We study the problem of accurately recognizing sign language words,\nwhich is key to narrowing the communication gap between hard and non-hard of\nhearing people.\n</p>\n<p>Our method explores a geometric feature space that we call `sub-skeleton'\naspects of movement. We assess similarity of feature space trajectories using\nnatural, speed invariant distance measures, which enables clear and insightful\nnearest neighbor classification. The simplicity and scalability of our basic\nmethod allows for immediate application in different data domains with little\nto no parameter tuning.\n</p>\n<p>We demonstrate the effectiveness of our basic method, and a boosted\nvariation, with experiments on data from different application domains and\ntracking technologies. Surprisingly, our simple methods improve sign\nrecognition over recent, state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gudmundsson_J/0/1/0/all/0/1\">Joachim Gudmundsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seybold_M/0/1/0/all/0/1\">Martin P. Seybold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_J/0/1/0/all/0/1\">John Pfeifer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocBed: A Multi-Stage OCR Solution for Documents with Complex Layouts. (arXiv:2202.01414v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01414","description":"<p>Digitization of newspapers is of interest for many reasons including\npreservation of history, accessibility and search ability, etc. While\ndigitization of documents such as scientific articles and magazines is\nprevalent in literature, one of the main challenges for digitization of\nnewspaper lies in its complex layout (e.g. articles spanning multiple columns,\ntext interrupted by images) analysis, which is necessary to preserve human\nread-order. This work provides a major breakthrough in the digitization of\nnewspapers on three fronts: first, releasing a dataset of 3000 fully-annotated,\nreal-world newspaper images from 21 different U.S. states representing an\nextensive variety of complex layouts for document layout analysis; second,\nproposing layout segmentation as a precursor to existing optical character\nrecognition (OCR) engines, where multiple state-of-the-art image segmentation\nmodels and several post-processing methods are explored for document layout\nsegmentation; third, providing a thorough and structured evaluation protocol\nfor isolated layout segmentation and end-to-end OCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenzhen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokhandan_N/0/1/0/all/0/1\">Negin Sokhandan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_S/0/1/0/all/0/1\">Sujitha Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyanarayana_S/0/1/0/all/0/1\">Suchitra Sathyanarayana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterization of Semantic Segmentation Models on Mobile Platforms for Self-Navigation in Disaster-Struck Zones. (arXiv:2202.01421v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01421","description":"<p>The role of unmanned vehicles for searching and localizing the victims in\ndisaster impacted areas such as earthquake-struck zones is getting more\nimportant. Self-navigation on an earthquake zone has a unique challenge of\ndetecting irregularly shaped obstacles such as road cracks, debris on the\nstreets, and water puddles. In this paper, we characterize a number of\nstate-of-the-art FCN models on mobile embedded platforms for self-navigation at\nthese sites containing extremely irregular obstacles. We evaluate the models in\nterms of accuracy, performance, and energy efficiency. We present a few\noptimizations for our designed vision system. Lastly, we discuss the trade-offs\nof these models for a couple of mobile platforms that can each perform\nself-navigation. To enable vehicles to safely navigate earthquake-struck zones,\nwe compiled a new annotated image database of various earthquake impacted\nregions that is different than traditional road damage databases. We train our\ndatabase with a number of state-of-the-art semantic segmentation models in\norder to identify obstacles unique to earthquake-struck zones. Based on the\nstatistics and tradeoffs, an optimal CNN model is selected for the mobile\nvehicular platforms, which we apply to both low-power and extremely low-power\nconfigurations of our design. To our best knowledge, this is the first study\nthat identifies unique challenges and discusses the accuracy, performance, and\nenergy impact of edge-based self-navigation mobile vehicles for\nearthquake-struck zones. Our proposed database and trained models are publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zelek_R/0/1/0/all/0/1\">Ryan Zelek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hyeran Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimized Potential Initialization for Low-latency Spiking Neural Networks. (arXiv:2202.01440v1 [cs.NE])","link":"http://arxiv.org/abs/2202.01440","description":"<p>Spiking Neural Networks (SNNs) have been attached great importance due to the\ndistinctive properties of low power consumption, biological plausibility, and\nadversarial robustness. The most effective way to train deep SNNs is through\nANN-to-SNN conversion, which have yielded the best performance in deep network\nstructure and large-scale datasets. However, there is a trade-off between\naccuracy and latency. In order to achieve high precision as original ANNs, a\nlong simulation time is needed to match the firing rate of a spiking neuron\nwith the activation value of an analog neuron, which impedes the practical\napplication of SNN. In this paper, we aim to achieve high-performance converted\nSNNs with extremely low latency (fewer than 32 time-steps). We start by\ntheoretically analyzing ANN-to-SNN conversion and show that scaling the\nthresholds does play a similar role as weight normalization. Instead of\nintroducing constraints that facilitate ANN-to-SNN conversion at the cost of\nmodel capacity, we applied a more direct way by optimizing the initial membrane\npotential to reduce the conversion loss in each layer. Besides, we demonstrate\nthat optimal initialization of membrane potentials can implement expected\nerror-free ANN-to-SNN conversion. We evaluate our algorithm on the CIFAR-10,\nCIFAR-100 and ImageNet datasets and achieve state-of-the-art accuracy, using\nfewer time-steps. For example, we reach top-1 accuracy of 93.38\\% on CIFAR-10\nwith 16 time-steps. Moreover, our method can be applied to other ANN-SNN\nconversion methodologies and remarkably promote performance when the time-steps\nis small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_T/0/1/0/all/0/1\">Tong Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jianhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Bottleneck Model with Additional Unsupervised Concepts. (arXiv:2202.01459v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01459","description":"<p>With the increasing demands for accountability, interpretability is becoming\nan essential capability for real-world AI applications. However, most methods\nutilize post-hoc approaches rather than training the interpretable model. In\nthis article, we propose a novel interpretable model based on the concept\nbottleneck model (CBM). CBM uses concept labels to train an intermediate layer\nas the additional visible layer. However, because the number of concept labels\nrestricts the dimension of this layer, it is difficult to obtain high accuracy\nwith a small number of labels. To address this issue, we integrate supervised\nconcepts with unsupervised ones trained with self-explaining neural networks\n(SENNs). By seamlessly training these two types of concepts while reducing the\namount of computation, we can obtain both supervised and unsupervised concepts\nsimultaneously, even for large-sized images. We refer to the proposed model as\nthe concept bottleneck model with additional unsupervised concepts (CBM-AUC).\nWe experimentally confirmed that the proposed model outperformed CBM and SENN.\nWe also visualized the saliency map of each concept and confirmed that it was\nconsistent with the semantic meanings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sawada_Y/0/1/0/all/0/1\">Yoshihide Sawada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1\">Keigo Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Monocular Depth Estimation with Sparse Guided Points. (arXiv:2202.01470v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01470","description":"<p>Existing monocular depth estimation shows excellent robustness in the wild,\nbut the affine-invariant prediction requires aligning with the ground truth\nglobally while being converted into the metric depth. In this work, we firstly\npropose a modified locally weighted linear regression strategy to leverage\nsparse ground truth and generate a flexible depth transformation to correct the\ncoarse misalignment brought by global recovery strategy. Applying this\nstrategy, we achieve significant improvement (more than 50% at most) over most\nrecent state-of-the-art methods on five zero-shot datasets. Moreover, we train\na robust depth estimation model with 6.3 million data and analyze the training\nprocess by decoupling the inaccuracy into coarse misalignment inaccuracy and\ndetail missing inaccuracy. As a result, our model based on ResNet50 even\noutperforms the state-of-the-art DPT ViT-Large model with the help of our\nrecovery strategy. In addition to accuracy, the consistency is also boosted for\nsimple per-frame video depth estimation. Compared with monocular depth\nestimation, robust video depth estimation, and depth completion methods, our\npipeline obtains state-of-the-art performance on video depth estimation without\nany post-processing. Experiments of 3D scene reconstruction from consistent\nvideo depth are conducted for intuitive comparison as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kai Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory Forecasting from Detection with Uncertainty-Aware Motion Encoding. (arXiv:2202.01478v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01478","description":"<p>Trajectory forecasting is critical for autonomous platforms to make safe\nplanning and actions. Currently, most trajectory forecasting methods assume\nthat object trajectories have been extracted and directly develop trajectory\npredictors based on the ground truth trajectories. However, this assumption\ndoes not hold in practical situations. Trajectories obtained from object\ndetection and tracking are inevitably noisy, which could cause serious\nforecasting errors to predictors built on ground truth trajectories. In this\npaper, we propose a trajectory predictor directly based on detection results\nwithout relying on explicitly formed trajectories. Different from the\ntraditional methods which encode the motion cue of an agent based on its\nclearly defined trajectory, we extract the motion information only based on the\naffinity cues among detection results, in which an affinity-aware state update\nmechanism is designed to take the uncertainty of association into account. In\naddition, considering that there could be multiple plausible matching\ncandidates, we aggregate the states of them. This design relaxes the\nundesirable effect of noisy trajectory obtained from data association.\nExtensive ablation experiments validate the effectiveness of our method and its\ngeneralization ability on different detectors. Cross-comparison to other\nforecasting schemes further proves the superiority of our method. Code will be\nreleased upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jianru Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jianwu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Computing and Intuitive Interaction: Bringing Mixed Reality and Robotics Together. (arXiv:2202.01493v1 [cs.RO])","link":"http://arxiv.org/abs/2202.01493","description":"<p>Spatial computing -- the ability of devices to be aware of their surroundings\nand to represent this digitally -- offers novel capabilities in human-robot\ninteraction. In particular, the combination of spatial computing and egocentric\nsensing on mixed reality devices enables them to capture and understand human\nactions and translate these to actions with spatial meaning, which offers\nexciting new possibilities for collaboration between humans and robots. This\npaper presents several human-robot systems that utilize these capabilities to\nenable novel robot use cases: mission planning for inspection, gesture-based\ncontrol, and immersive teleoperation. These works demonstrate the power of\nmixed reality as a tool for human-robot interaction, and the potential of\nspatial computing and mixed reality to drive the future of human-robot\ninteraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delmerico_J/0/1/0/all/0/1\">Jeffrey Delmerico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poranne_R/0/1/0/all/0/1\">Roi Poranne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogo_F/0/1/0/all/0/1\">Federica Bogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oleynikova_H/0/1/0/all/0/1\">Helen Oleynikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollenweider_E/0/1/0/all/0/1\">Eric Vollenweider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coros_S/0/1/0/all/0/1\">Stelian Coros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_J/0/1/0/all/0/1\">Juan Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PARCEL: Physics-based unsupervised contrastive representation learning for parallel MR imaging. (arXiv:2202.01494v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01494","description":"<p>With the successful application of deep learning in magnetic resonance\nimaging, parallel imaging techniques based on neural networks have attracted\nwide attentions. However, without high-quality fully sampled datasets for\ntraining, the performance of these methods tends to be limited. To address this\nissue, this paper proposes a physics based unsupervised contrastive\nrepresentation learning (PARCEL) method to speed up parallel MR imaging.\nSpecifically, PARCEL has three key ingredients to achieve direct deep learning\nfrom the undersampled k-space data. Namely, a parallel framework has been\ndeveloped by learning two branches of model-based networks unrolled with the\nconjugate gradient algorithm; Augmented undersampled k-space data randomly\ndrawn from the obtained k-space data are used to help the parallel network to\ncapture the detailed information. A specially designed co-training loss is\ndesigned to guide the two networks to capture the inherent features and\nrepresentations of the-to-be-reconstructed MR image. The proposed method has\nbeen evaluated on in vivo datasets and compared to five state-of-the-art\nmethods, whose results show PARCEL is able to learn useful representations for\nmore accurate MR reconstructions without the reliance on the fully-sampled\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_R/0/1/0/all/0/1\">Ruoyou Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_J/0/1/0/all/0/1\">Juan Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hairong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bending Graphs: Hierarchical Shape Matching using Gated Optimal Transport. (arXiv:2202.01537v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01537","description":"<p>Shape matching has been a long-studied problem for the computer graphics and\nvision community. The objective is to predict a dense correspondence between\nmeshes that have a certain degree of deformation. Existing methods either\nconsider the local description of sampled points or discover correspondences\nbased on global shape information. In this work, we investigate a hierarchical\nlearning design, to which we incorporate local patch-level information and\nglobal shape-level structures. This flexible representation enables\ncorrespondence prediction and provides rich features for the matching stage.\nFinally, we propose a novel optimal transport solver by recurrently updating\nfeatures on non-confident nodes to learn globally consistent correspondences\nbetween the shapes. Our results on publicly available datasets suggest robust\nperformance in presence of severe deformations without the need for extensive\ntraining or refinement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_M/0/1/0/all/0/1\">Mahdi Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shun-Cheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosmo_L/0/1/0/all/0/1\">Luca Cosmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Nuclei Segmentation via Instance Learning. (arXiv:2202.01564v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01564","description":"<p>Weakly supervised nuclei segmentation is a critical problem for pathological\nimage analysis and greatly benefits the community due to the significant\nreduction of labeling cost. Adopting point annotations, previous methods mostly\nrely on less expressive representations for nuclei instances and thus have\ndifficulty in handling crowded nuclei. In this paper, we propose to decouple\nweakly supervised semantic and instance segmentation in order to enable more\neffective subtask learning and to promote instance-aware representation\nlearning. To achieve this, we design a modular deep network with two branches:\na semantic proposal network and an instance encoding network, which are trained\nin a two-stage manner with an instance-sensitive loss. Empirical results show\nthat our approach achieves the state-of-the-art performance on two public\nbenchmarks of pathological images from different types of organs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Weizhen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FORML: Learning to Reweight Data for Fairness. (arXiv:2202.01719v1 [cs.LG])","link":"http://arxiv.org/abs/2202.01719","description":"<p>Deployed machine learning models are evaluated by multiple metrics beyond\naccuracy, such as fairness and robustness. However, such models are typically\ntrained to minimize the average loss for a single metric, which is typically a\nproxy for accuracy. Training to optimize a single metric leaves these models\nprone to fairness violations, especially when the population of sub-groups in\nthe training data are imbalanced. This work addresses the challenge of jointly\noptimizing fairness and predictive performance in the multi-class\nclassification setting by introducing Fairness Optimized Reweighting via\nMeta-Learning (FORML), a training algorithm that balances fairness constraints\nand accuracy by jointly optimizing training sample weights and a neural\nnetwork's parameters. The approach increases fairness by learning to weight\neach training datum's contribution to the loss according to its impact on\nreducing fairness violations, balancing the contributions from both over- and\nunder-represented sub-groups. We empirically validate FORML on a range of\nbenchmark and real-world classification datasets and show that our approach\nimproves equality of opportunity fairness criteria over existing\nstate-of-the-art reweighting methods by approximately 1% on image\nclassification tasks and by approximately 5% on a face attribute prediction\ntask. This improvement is achieved without pre-processing data or\npost-processing model outputs, without learning an additional weighting\nfunction, and while maintaining accuracy on the original predictive metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bobby Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seto_S/0/1/0/all/0/1\">Skyler Seto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-Based Action Segmentation with Multi-Stage Spatial-Temporal Graph Convolutional Neural Networks. (arXiv:2202.01727v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01727","description":"<p>The ability to identify and temporally segment fine-grained actions in motion\ncapture sequences is crucial for applications in human movement analysis.\nMotion capture is typically performed with optical or inertial measurement\nsystems, which encode human movement as a time series of human joint locations\nand orientations or their higher-order representations. State-of-the-art action\nsegmentation approaches use multiple stages of temporal convolutions. The main\nidea is to generate an initial prediction with several layers of temporal\nconvolutions and refine these predictions over multiple stages, also with\ntemporal convolutions. Although these approaches capture long-term temporal\npatterns, the initial predictions do not adequately consider the spatial\nhierarchy among the human joints. To address this limitation, we present\nmulti-stage spatial-temporal graph convolutional neural networks (MS-GCN). Our\nframework decouples the architecture of the initial prediction generation stage\nfrom the refinement stages. Specifically, we replace the initial stage of\ntemporal convolutions with spatial-temporal graph convolutions, which better\nexploit the spatial configuration of the joints and their temporal dynamics.\nOur framework was compared to four strong baselines on five tasks. Experimental\nresults demonstrate that our framework achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filtjens_B/0/1/0/all/0/1\">Benjamin Filtjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanrumste_B/0/1/0/all/0/1\">Bart Vanrumste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slaets_P/0/1/0/all/0/1\">Peter Slaets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Online Video Super-Resolution with Deformable Attention Pyramid. (arXiv:2202.01731v1 [eess.IV])","link":"http://arxiv.org/abs/2202.01731","description":"<p>Video super-resolution (VSR) has many applications that pose strict causal,\nreal-time, and latency constraints, including video streaming and TV. We\naddress the VSR problem under these settings, which poses additional important\nchallenges since information from future frames are unavailable. Importantly,\ndesigning efficient, yet effective frame alignment and fusion modules remain\ncentral problems. In this work, we propose a recurrent VSR architecture based\non a deformable attention pyramid (DAP). Our DAP aligns and integrates\ninformation from the recurrent state into the current frame prediction. To\ncircumvent the computational cost of traditional attention-based methods, we\nonly attend to a limited number of spatial locations, which are dynamically\npredicted by the DAP. Comprehensive experiments and analysis of the proposed\nkey innovations show the effectiveness of our approach. We significantly reduce\nprocessing time in comparison to state-of-the-art methods, while maintaining a\nhigh performance. We surpass state-of-the-art method EDVR-M on two standard\nbenchmarks with a speed-up of over 3x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fuoli_D/0/1/0/all/0/1\">Dario Fuoli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Met Dataset: Instance-level Recognition for Artworks. (arXiv:2202.01747v1 [cs.CV])","link":"http://arxiv.org/abs/2202.01747","description":"<p>This work introduces a dataset for large-scale instance-level recognition in\nthe domain of artworks. The proposed benchmark exhibits a number of different\nchallenges such as large inter-class similarity, long tail distribution, and\nmany classes. We rely on the open access collection of The Met museum to form a\nlarge training set of about 224k classes, where each class corresponds to a\nmuseum exhibit with photos taken under studio conditions. Testing is primarily\nperformed on photos taken by museum guests depicting exhibits, which introduces\na distribution shift between training and testing. Testing is additionally\nperformed on a set of images not related to Met exhibits making the task\nresemble an out-of-distribution detection problem. The proposed benchmark\nfollows the paradigm of other recent datasets for instance-level recognition on\ndifferent domains to encourage research on domain independent approaches. A\nnumber of suitable approaches are evaluated to offer a testbed for future\ncomparisons. Self-supervised and supervised contrastive learning are\neffectively combined to train the backbone which is used for non-parametric\nclassification that is shown as a promising direction. Dataset webpage:\n<a href=\"http://cmp.felk.cvut.cz/met/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ypsilantis_N/0/1/0/all/0/1\">Nikolaos-Antonios Ypsilantis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimi_S/0/1/0/all/0/1\">Sarah Ibrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_N/0/1/0/all/0/1\">Nanne Van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting future action sequences with attention: a new approach to weakly supervised action forecasting. (arXiv:1912.04608v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.04608","description":"<p>Future human action forecasting from partial observations of activities is an\nimportant problem in many practical applications such as assistive robotics,\nvideo surveillance and security. We present a method to forecast actions for\nthe unseen future of the video using a neural machine translation technique\nthat uses encoder-decoder architecture. The input to this model is the observed\nRGB video, and the objective is to forecast the correct future symbolic action\nsequence. Unlike prior methods that make action predictions for some unseen\npercentage of video one for each frame, we predict the complete action sequence\nthat is required to accomplish the activity. We coin this task action sequence\nforecasting. To cater for two types of uncertainty in the future predictions,\nwe propose a novel loss function. We show a combination of optimal transport\nand future uncertainty losses help to improve results.\n</p>\n<p>We extend our action sequence forecasting model to perform weakly supervised\naction forecasting on two challenging datasets, the Breakfast and the 50Salads.\nSpecifically, we propose a model to predict actions of future unseen frames\nwithout using frame level annotations during training. Using Fisher vector\nfeatures, our supervised model outperforms the state-of-the-art action\nforecasting model by 0.83% and 7.09% on the Breakfast and the 50Salads datasets\nrespectively. Our weakly supervised model is only 0.6% behind the most recent\nstate-of-the-art supervised model and obtains comparable results to other\npublished fully supervised methods, and sometimes even outperforms them on the\nBreakfast dataset. Most interestingly, our weakly supervised model outperforms\nprior models by 1.04% leveraging on proposed weakly supervised architecture,\nand effective use of attention mechanism and loss functions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_Y/0/1/0/all/0/1\">Yan Bin Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Rain Attack and Defensive Deraining for DNN Perception. (arXiv:2009.09205v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.09205","description":"<p>Rain often poses inevitable threats to deep neural network (DNN) based\nperception systems, and a comprehensive investigation of the potential risks of\nthe rain to DNNs is of great importance. However, it is rather difficult to\ncollect or synthesize rainy images that can represent all rain situations that\nwould possibly occur in the real world. To this end, in this paper, we start\nfrom a new perspective and propose to combine two totally different studies,\ni.e., rainy image synthesis and adversarial attack. We first present an\nadversarial rain attack, with which we could simulate various rain situations\nwith the guidance of deployed DNNs and reveal the potential threat factors that\ncan be brought by rain. In particular, we design a factor-aware rain generation\nthat synthesizes rain streaks according to the camera exposure process and\nmodels the learnable rain factors for adversarial attack. With this generator,\nwe perform the adversarial rain attack against the image classification and\nobject detection. To defend the DNNs from the negative rain effect, we also\npresent a defensive deraining strategy, for which we design an adversarial rain\naugmentation that uses mixed adversarial rain layers to enhance deraining\nmodels for downstream DNN perception. Our large-scale evaluation on various\ndatasets demonstrates that our synthesized rainy images with realistic\nappearances not only exhibit strong adversarial capability against DNNs, but\nalso boost the deraining models for defensive purposes, building the foundation\nfor further rain-robust perception studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_L/0/1/0/all/0/1\">Liming Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Shengchao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Boundary Loss for Semantic Segmentation. (arXiv:2102.02696v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.02696","description":"<p>This paper proposes a novel active boundary loss for semantic segmentation.\nIt can progressively encourage the alignment between predicted boundaries and\nground-truth boundaries during end-to-end training, which is not explicitly\nenforced in commonly used cross-entropy loss. Based on the predicted boundaries\ndetected from the segmentation results using current network parameters, we\nformulate the boundary alignment problem as a differentiable direction vector\nprediction problem to guide the movement of predicted boundaries in each\niteration. Our loss is model-agnostic and can be plugged in to the training of\nsegmentation networks to improve the boundary details. Experimental results\nshow that training with the active boundary loss can effectively improve the\nboundary F-score and mean Intersection-over-Union on challenging image and\nvideo object segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunke Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">XianSheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated freezing of gait assessment with marker-based motion capture and multi-stage spatial-temporal graph convolutional neural networks. (arXiv:2103.15449v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15449","description":"<p>Freezing of gait (FOG) is a common and debilitating gait impairment in\nParkinson's disease. Further insight into this phenomenon is hampered by the\ndifficulty to objectively assess FOG. To meet this clinical need, this paper\nproposes an automated motion-capture-based FOG assessment method driven by a\nnovel deep neural network. Automated FOG assessment can be formulated as an\naction segmentation problem, where temporal models are tasked to recognize and\ntemporally localize the FOG segments in untrimmed motion capture trials. This\npaper takes a closer look at the performance of state-of-the-art action\nsegmentation models when tasked to automatically assess FOG. Furthermore, a\nnovel deep neural network architecture is proposed that aims to better capture\nthe spatial and temporal dependencies than the state-of-the-art baselines. The\nproposed network, termed multi-stage spatial-temporal graph convolutional\nnetwork (MS-GCN), combines the spatial-temporal graph convolutional network\n(ST-GCN) and the multi-stage temporal convolutional network (MS-TCN). The\nST-GCN captures the hierarchical spatial-temporal motion among the joints\ninherent to motion capture, while the multi-stage component reduces\nover-segmentation errors by refining the predictions over multiple stages. The\nexperiments indicate that the proposed model outperforms four state-of-the-art\nbaselines. Moreover, FOG outcomes derived from MS-GCN predictions had an\nexcellent (r=0.93 [0.87, 0.97]) and moderately strong (r=0.75 [0.55, 0.87])\nlinear relationship with FOG outcomes derived from manual annotations. The\nproposed MS-GCN may provide an automated and objective alternative to\nlabor-intensive clinician-based FOG assessment. Future work is now possible\nthat aims to assess the generalization of MS-GCN to a larger and more varied\nverification cohort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Filtjens_B/0/1/0/all/0/1\">Benjamin Filtjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginis_P/0/1/0/all/0/1\">Pieter Ginis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieuwboer_A/0/1/0/all/0/1\">Alice Nieuwboer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slaets_P/0/1/0/all/0/1\">Peter Slaets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanrumste_B/0/1/0/all/0/1\">Bart Vanrumste</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototype Memory for Large-scale Face Representation Learning. (arXiv:2105.02103v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02103","description":"<p>Face representation learning using datasets with a massive number of\nidentities requires appropriate training methods. Softmax-based approach,\ncurrently the state-of-the-art in face recognition, in its usual \"full softmax\"\nform is not suitable for datasets with millions of persons. Several methods,\nbased on the \"sampled softmax\" approach, were proposed to remove this\nlimitation. These methods, however, have a set of disadvantages. One of them is\na problem of \"prototype obsolescence\": classifier weights (prototypes) of the\nrarely sampled classes receive too scarce gradients and become outdated and\ndetached from the current encoder state, resulting in incorrect training\nsignals. This problem is especially serious in ultra-large-scale datasets. In\nthis paper, we propose a novel face representation learning model called\nPrototype Memory, which alleviates this problem and allows training on a\ndataset of any size. Prototype Memory consists of the limited-size memory\nmodule for storing recent class prototypes and employs a set of algorithms to\nupdate it in appropriate way. New class prototypes are generated on the fly\nusing exemplar embeddings in the current mini-batch. These prototypes are\nenqueued to the memory and used in a role of classifier weights for softmax\nclassification-based training. To prevent obsolescence and keep the memory in\nclose connection with the encoder, prototypes are regularly refreshed, and\noldest ones are dequeued and disposed of. Prototype Memory is computationally\nefficient and independent of dataset size. It can be used with various loss\nfunctions, hard example mining algorithms and encoder architectures. We prove\nthe effectiveness of the proposed model by extensive experiments on popular\nface recognition benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smirnov_E/0/1/0/all/0/1\">Evgeny Smirnov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garaev_N/0/1/0/all/0/1\">Nikita Garaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galyuk_V/0/1/0/all/0/1\">Vasiliy Galyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukyanets_E/0/1/0/all/0/1\">Evgeny Lukyanets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trust It or Not: Confidence-Guided Automatic Radiology Report Generation. (arXiv:2106.10887v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10887","description":"<p>Medical imaging plays a pivotal role in diagnosis and treatment in clinical\npractice. Inspired by the significant progress in automatic image captioning,\nvarious deep learning (DL)-based methods have been proposed to generate\nradiology reports for medical images. Despite promising results, previous works\noverlook the uncertainties of their models and are thus unable to provide\nclinicians with the reliability/confidence of the generated radiology reports\nto assist their decision-making. In this paper, we propose a novel method to\nexplicitly quantify both the visual uncertainty and the textual uncertainty for\nDL-based radiology report generation. Such multi-modal uncertainties can\nsufficiently capture the model confidence degree at both the report level and\nthe sentence level, and thus they are further leveraged to weight the losses\nfor more comprehensive model optimization. Experimental results have\ndemonstrated that the proposed method for model uncertainty characterization\nand estimation can produce more reliable confidence scores for radiology report\ngeneration, and the modified loss function, which takes into account the\nuncertainties, leads to better model performance on two public radiology report\ndatasets. In addition, the quality of the automatically generated reports was\nmanually evaluated by human raters and the results also indicate that the\nproposed uncertainties can reflect the variance of clinical diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zihao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adversarial Training incorporating Forgery Attention for Image Forgery Localization. (arXiv:2107.02434v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02434","description":"<p>Image editing techniques enable people to modify the content of an image\nwithout leaving visual traces and thus may cause serious security risks. Hence\nthe detection and localization of these forgeries become quite necessary and\nchallenging. Furthermore, unlike other tasks with extensive data, there is\nusually a lack of annotated forged images for training due to annotation\ndifficulties. In this paper, we propose a self-adversarial training strategy\nand a reliable coarse-to-fine network that utilizes a self-attention mechanism\nto localize forged regions in forgery images. The self-attention module is\nbased on a Channel-Wise High Pass Filter block (CW-HPF). CW-HPF leverages\ninter-channel relationships of features and extracts noise features by high\npass filters. Based on the CW-HPF, a self-attention mechanism, called forgery\nattention, is proposed to capture rich contextual dependencies of intrinsic\ninconsistency extracted from tampered regions. Specifically, we append two\ntypes of attention modules on top of CW-HPF respectively to model internal\ninterdependencies in spatial dimension and external dependencies among\nchannels. We exploit a coarse-to-fine network to enhance the noise\ninconsistency between original and tampered regions. More importantly, to\naddress the issue of insufficient training data, we design a self-adversarial\ntraining strategy that expands training data dynamically to achieve more robust\nperformance. Specifically, in each training iteration, we perform adversarial\nattacks against our network to generate adversarial examples and train our\nmodel on them. Extensive experimental results demonstrate that our proposed\nalgorithm steadily outperforms state-of-the-art methods by a clear margin in\ndifferent benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1\">Long Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sparse Interaction Graphs of Partially Detected Pedestrians for Trajectory Prediction. (arXiv:2107.07056v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2107.07056","description":"<p>Multi-pedestrian trajectory prediction is an indispensable element of\nautonomous systems that safely interact with crowds in unstructured\nenvironments. Many recent efforts in trajectory prediction algorithms have\nfocused on understanding social norms behind pedestrian motions. Yet we observe\nthese works usually hold two assumptions, which prevent them from being\nsmoothly applied to robot applications: (1) positions of all pedestrians are\nconsistently tracked, and (2) the target agent pays attention to all\npedestrians in the scene. The first assumption leads to biased interaction\nmodeling with incomplete pedestrian data. The second assumption introduces\naggregation of redundant surrounding information, and the target agent may be\naffected by unimportant neighbors or present overly conservative motion. Thus,\nwe propose Gumbel Social Transformer, in which an Edge Gumbel Selector samples\na sparse interaction graph of partially detected pedestrians at each time step.\nA Node Transformer Encoder and a Masked LSTM encode pedestrian features with\nsampled sparse graphs to predict trajectories. We demonstrate that our model\novercomes potential problems caused by the aforementioned assumptions, and our\napproach outperforms related works in trajectory prediction benchmarks. Code is\navailable at \\url{https://github.com/tedhuang96/gst}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruohua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1\">Kazuki Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Deep Learning Classification Methods on a Small Environmental Microorganism Image Dataset (EMDS-6): from Convolutional Neural Networks to Visual Transformers. (arXiv:2107.07699v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07699","description":"<p>In recent years, deep learning has made brilliant achievements in\nEnvironmental Microorganism (EM) image classification. However, image\nclassification of small EM datasets has still not obtained good research\nresults. Therefore, researchers need to spend a lot of time searching for\nmodels with good classification performance and suitable for the current\nequipment working environment. To provide reliable references for researchers,\nwe conduct a series of comparison experiments on 21 deep learning models. The\nexperiment includes direct classification, imbalanced training, and\nhyperparameter tuning experiments. During the experiments, we find\ncomplementarities among the 21 models, which is the basis for feature fusion\nrelated experiments. We also find that the data augmentation method of\ngeometric deformation is difficult to improve the performance of VTs (ViT,\nDeiT, BotNet and T2T-ViT) series models. In terms of model performance,\nXception has the best classification performance, the ViT model consumes the\nleast time for training, and the ShuffleNet-V2 model has the least number of\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02234","description":"<p>In this paper, we propose a novel hand-based person recognition method for\nthe purpose of criminal investigations since the hand image is often the only\navailable information in cases of serious crime such as sexual abuse. Our\nproposed method, Multi-Branch with Attention Network (MBA-Net), incorporates\nboth channel and spatial attention modules in branches in addition to a global\n(without attention) branch to capture global structural information for\ndiscriminative feature learning. The attention modules focus on the relevant\nfeatures of the hand image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. Extensive\nevaluations on two large multi-ethnic and publicly available hand datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nsurpassing the existing hand-based identification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. (arXiv:2108.10869v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10869","description":"<p>We introduce DROID-SLAM, a new deep learning based SLAM system. DROID-SLAM\nconsists of recurrent iterative updates of camera pose and pixelwise depth\nthrough a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving\nlarge improvements over prior work, and robust, suffering from substantially\nfewer catastrophic failures. Despite training on monocular video, it can\nleverage stereo or RGB-D video to achieve improved performance at test time.\nThe URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teed_Z/0/1/0/all/0/1\">Zachary Teed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap between Events and Frames through Unsupervised Domain Adaptation. (arXiv:2109.02618v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02618","description":"<p>Reliable perception during fast motion maneuvers or in high dynamic range\nenvironments is crucial for robotic systems. Since event cameras are robust to\nthese challenging conditions, they have great potential to increase the\nreliability of robot vision. However, event-based vision has been held back by\nthe shortage of labeled datasets due to the novelty of event cameras. To\novercome this drawback, we propose a task transfer method to train models\ndirectly with labeled images and unlabeled event data. Compared to previous\napproaches, (i) our method transfers from single images to events instead of\nhigh frame rate videos, and (ii) does not rely on paired sensor data. To\nachieve this, we leverage the generative event model to split event features\ninto content and motion features. This split enables efficient matching between\nlatent spaces for events and images, which is crucial for successful task\ntransfer. Thus, our approach unlocks the vast amount of existing image datasets\nfor the training of event-based neural networks. Our task transfer method\nconsistently outperforms methods targeting Unsupervised Domain Adaptation for\nobject detection by 0.26 mAP (increase by 93%) and classification by 2.7%\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messikommer_N/0/1/0/all/0/1\">Nico Messikommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_M/0/1/0/all/0/1\">Mathias Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-Local Dynamic Feature Alignment Network for Person Re-Identification. (arXiv:2109.05759v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05759","description":"<p>The misalignment of human images caused by bounding box detection errors or\npartial occlusions is one of the main challenges in person Re-Identification\n(Re-ID) tasks. Previous local-based methods mainly focus on learning local\nfeatures in predefined semantic regions of pedestrians. These methods usually\nuse local hard alignment methods or introduce auxiliary information such as key\nhuman pose points to match local features, which are often not applicable when\nlarge scene differences are encountered. To solve these problems, we propose a\nsimple and efficient Local Sliding Alignment (LSA) strategy to dynamically\nalign the local features of two images by setting a sliding window on the local\nstripes of the pedestrian. LSA can effectively suppress spatial misalignment\nand does not need to introduce extra supervision information. Then, we design a\nGlobal-Local Dynamic Feature Alignment Network (GLDFA-Net) framework, which\ncontains both global and local branches. We introduce LSA into the local branch\nof GLDFA-Net to guide the computation of distance metrics, which can further\nimprove the accuracy of the testing phase. Evaluation experiments on several\nmainstream evaluation datasets including Market-1501, DukeMTMC-reID, CUHK03 and\nMSMT17 show that our method has competitive accuracy over the several\nstate-of-the-art person Re-ID methods. Specifically, it achieves 86.1% mAP and\n94.8% Rank-1 accuracy on Market1501.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zhangqiang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianrong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Min Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Dimensional Collapse in Contrastive Self-supervised Learning. (arXiv:2110.09348v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09348","description":"<p>Self-supervised visual representation learning aims to learn useful\nrepresentations without relying on human annotations. Joint embedding approach\nbases on maximizing the agreement between embedding vectors from different\nviews of the same image. Various methods have been proposed to solve the\ncollapsing problem where all embedding vectors collapse to a trivial constant\nsolution. Among these methods, contrastive learning prevents collapse via\nnegative sample pairs. It has been shown that non-contrastive methods suffer\nfrom a lesser collapse problem of a different nature: dimensional collapse,\nwhereby the embedding vectors end up spanning a lower-dimensional subspace\ninstead of the entire available embedding space. Here, we show that dimensional\ncollapse also happens in contrastive learning. In this paper, we shed light on\nthe dynamics at play in contrastive learning that leads to dimensional\ncollapse. Inspired by our theory, we propose a novel contrastive learning\nmethod, called DirectCLR, which directly optimizes the representation space\nwithout relying on a trainable projector. Experiments show that DirectCLR\noutperforms SimCLR with a trainable linear projector on ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Li Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1\">Pascal Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Bayesian Models for Organ Contouring in Head and Neck Radiotherapy. (arXiv:2111.01134v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.01134","description":"<p>Deep learning models for organ contouring in radiotherapy are poised for\nclinical usage, but currently, there exist few tools for automated quality\nassessment (QA) of the predicted contours. Using Bayesian models and their\nassociated uncertainty, one can potentially automate the process of detecting\ninaccurate predictions. We investigate two Bayesian models for auto-contouring,\nDropOut and FlipOut, using a quantitative measure - expected calibration error\n(ECE) and a qualitative measure - region-based accuracy-vs-uncertainty (R-AvU)\ngraphs. It is well understood that a model should have low ECE to be considered\ntrustworthy. However, in a QA context, a model should also have high\nuncertainty in inaccurate regions and low uncertainty in accurate regions. Such\nbehaviour could direct visual attention of expert users to potentially\ninaccurate regions, leading to a speed up in the QA process. Using R-AvU\ngraphs, we qualitatively compare the behaviour of different models in accurate\nand inaccurate regions. Experiments are conducted on the MICCAI2015 Head and\nNeck Segmentation Challenge and on the DeepMindTCIA CT dataset using three\nmodels: DropOut-DICE, Dropout-CE (Cross Entropy) and FlipOut-CE. Quantitative\nresults show that DropOut-DICE has the highest ECE, while Dropout-CE and\nFlipOut-CE have the lowest ECE. To better understand the difference between\nDropOut-CE and FlipOut-CE, we use the R-AvU graph which shows that FlipOut-CE\nhas better uncertainty coverage in inaccurate regions than DropOut-CE. Such a\ncombination of quantitative and qualitative metrics explores a new approach\nthat helps to select which model can be deployed as a QA tool in clinical\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mody_P/0/1/0/all/0/1\">Prerak Mody</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaves_de_Plaza_N/0/1/0/all/0/1\">Nicolas Chaves-de-Plaza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hildebrandt_K/0/1/0/all/0/1\">Klaus Hildebrandt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egmond_R/0/1/0/all/0/1\">Rene van Egmond</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ridder_H/0/1/0/all/0/1\">Huib de Ridder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Staring_M/0/1/0/all/0/1\">Marius Staring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HRNET: AI on Edge for mask detection and social distancing. (arXiv:2111.15208v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15208","description":"<p>The purpose of the paper is to provide innovative emerging technology\nframework for community to combat epidemic situations. The paper proposes a\nunique outbreak response system framework based on artificial intelligence and\nedge computing for citizen centric services to help track and trace people\neluding safety policies like mask detection and social distancing measure in\npublic or workplace setup. The framework further provides implementation\nguideline in industrial setup as well for governance and contact tracing tasks.\nThe adoption will thus lead in smart city planning and development focusing on\ncitizen health systems contributing to improved quality of life. The conceptual\nframework presented is validated through quantitative data analysis via\nsecondary data collection from researcher's public websites, GitHub\nrepositories and renowned journals and further benchmarking were conducted for\nexperimental results in Microsoft Azure cloud environment. The study includes\nselective AI-models for benchmark analysis and were assessed on performance and\naccuracy in edge computing environment for large scale societal setup. Overall\nYOLO model Outperforms in object detection task and is faster enough for mask\ndetection and HRNetV2 outperform semantic segmentation problem applied to solve\nsocial distancing task in AI-Edge inferencing environmental setup. The paper\nproposes new Edge-AI algorithm for building technology-oriented solutions for\ndetecting mask in human movement and social distance. The paper enriches the\ntechnological advancement in artificial intelligence and edge-computing applied\nto problems in society and healthcare systems. The framework further equips\ngovernment agency, system providers to design and constructs\ntechnology-oriented models in community setup to Increase the quality of life\nusing emerging technologies into smart urban environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_K/0/1/0/all/0/1\">Kinshuk Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Praveen Ranjan Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-to-Image Translation-based Data Augmentation for Robust EV Charging Inlet Detection. (arXiv:2112.05290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05290","description":"<p>This work addresses the task of electric vehicle (EV) charging inlet\ndetection for autonomous EV charging robots. Recently, automated EV charging\nsystems have received huge attention to improve users' experience and to\nefficiently utilize charging infrastructures and parking lots. However, most\nrelated works have focused on system design, robot control, planning, and\nmanipulation. Towards robust EV charging inlet detection, we propose a new\ndataset (EVCI dataset) and a novel data augmentation method that is based on\nimage-to-image translation where typical image-to-image translation methods\nsynthesize a new image in a different domain given an image. To the best of our\nknowledge, the EVCI dataset is the first EV charging inlet dataset. For the\ndata augmentation method, we focus on being able to control synthesized images'\ncaptured environments (e.g., time, lighting) in an intuitive way. To achieve\nthis, we first propose the environment guide vector that humans can intuitively\ninterpret. We then propose a novel image-to-image translation network that\ntranslates a given image towards the environment described by the vector.\nAccordingly, it aims to synthesize a new image that has the same content as the\ngiven image while looking like captured in the provided environment by the\nenvironment guide vector. Lastly, we train a detection method using the\naugmented dataset. Through experiments on the EVCI dataset, we demonstrate that\nthe proposed method outperforms the state-of-the-art methods. We also show that\nthe proposed method is able to control synthesized images using an image and\nenvironment guide vectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yeonjun Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeejin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Byeongkeun Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persistent Object Identification Leveraging Non-Visual Markers. (arXiv:2112.06809v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06809","description":"<p>Our objective is to locate and provide a unique identifier for each mouse in\na cluttered home-cage environment through time, as a precursor to automated\nbehaviour recognition for biological research. This is a very challenging\nproblem due to (i) the lack of distinguishing visual features for each mouse,\nand (ii) the close confines of the scene with constant occlusion, making\nstandard visual tracking approaches unusable. However, a coarse estimate of\neach mouse's location is available from a unique RFID implant, so there is the\npotential to optimally combine information from (weak) tracking with coarse\ninformation on identity. To achieve our objective, we make the following key\ncontributions: (a) the formulation of the object identification problem as an\nassignment problem (solved using Integer Linear Programming), and (b) a novel\nprobabilistic model of the affinity between tracklets and RFID data. The latter\nis a crucial part of the model, as it provides a principled probabilistic\ntreatment of object detections given coarse localisation. Our approach achieves\n77% accuracy on this animal identification problem, and is able to reject\nspurious detections when the animals are hidden.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camilleri_M/0/1/0/all/0/1\">Michael P. J. Camilleri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bains_R/0/1/0/all/0/1\">Rasneer S. Bains</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher K. I. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JoJoGAN: One Shot Face Stylization. (arXiv:2112.11641v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11641","description":"<p>A style mapper applies some fixed style to its input images (so, for example,\ntaking faces to cartoons). This paper describes a simple procedure -- JoJoGAN\n-- to learn a style mapper from a single example of the style. JoJoGAN uses a\nGAN inversion procedure and StyleGAN's style-mixing property to produce a\nsubstantial paired dataset from a single example style. The paired dataset is\nthen used to fine-tune a StyleGAN. An image can then be style mapped by\nGAN-inversion followed by the fine-tuned StyleGAN. JoJoGAN needs just one\nreference and as little as 30 seconds of training time. JoJoGAN can use extreme\nstyle references (say, animal faces) successfully. Furthermore, one can control\nwhat aspects of the style are used and how much of the style is applied.\nQualitative and quantitative evaluation show that JoJoGAN produces high quality\nhigh resolution images that vastly outperform the current state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_M/0/1/0/all/0/1\">Min Jin Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPC: Transformation-Specific Smoothing for Point Cloud Models. (arXiv:2201.12733v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12733","description":"<p>Point cloud models with neural network architectures have achieved great\nsuccess and have been widely used in safety-critical applications, such as\nLidar-based recognition systems in autonomous vehicles. However, such models\nare shown vulnerable against adversarial attacks which aim to apply stealthy\nsemantic transformations such as rotation and tapering to mislead model\npredictions. In this paper, we propose a transformation-specific smoothing\nframework TPC, which provides tight and scalable robustness guarantees for\npoint cloud models against semantic transformation attacks. We first categorize\ncommon 3D transformations into three categories: additive (e.g., shearing),\ncomposable (e.g., rotation), and indirectly composable (e.g., tapering), and we\npresent generic robustness certification strategies for all categories\nrespectively. We then specify unique certification protocols for a range of\nspecific semantic transformations and their compositions. Extensive experiments\non several common 3D transformations show that TPC significantly outperforms\nthe state of the art. For example, our framework boosts the certified accuracy\nagainst twisting transformation along z-axis (within 20$^\\circ$) from 20.3$\\%$\nto 83.8$\\%$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wenda Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim2Real Object-Centric Keypoint Detection and Description. (arXiv:2202.00448v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00448","description":"<p>Keypoint detection and description play a central role in computer vision.\nMost existing methods are in the form of scene-level prediction, without\nreturning the object classes of different keypoints. In this paper, we propose\nthe object-centric formulation, which, beyond the conventional setting,\nrequires further identifying which object each interest point belongs to. With\nsuch fine-grained information, our framework enables more downstream\npotentials, such as object-level matching and pose estimation in a clustered\nenvironment. To get around the difficulty of label collection in the real\nworld, we develop a sim2real contrastive learning mechanism that can generalize\nthe model trained in simulation to real-world applications. The novelties of\nour training method are three-fold: (i) we integrate the uncertainty into the\nlearning framework to improve feature description of hard cases, e.g.,\nless-textured or symmetric patches; (ii) we decouple the object descriptor into\ntwo output branches -- intra-object salience and inter-object distinctness,\nresulting in a better pixel-wise description; (iii) we enforce cross-view\nsemantic consistency for enhanced robustness in representation learning.\nComprehensive experiments on image matching and 6D pose estimation verify the\nencouraging generalization ability of our method from simulation to reality.\nParticularly for 6D pose estimation, our method significantly outperforms\ntypical unsupervised/sim2real methods, achieving a closer gap with the fully\nsupervised counterpart. Additional results and videos can be found at\nhttps://zhongcl-thu.github.io/rock/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Chengliang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jinshan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1\">Xiaodong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Embarrassingly Simple Consistency Regularization Method for Semi-Supervised Medical Image Segmentation. (arXiv:2202.00677v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.00677","description":"<p>The scarcity of pixel-level annotation is a prevalent problem in medical\nimage segmentation tasks. In this paper, we introduce a novel regularization\nstrategy involving interpolation-based mixing for semi-supervised medical image\nsegmentation. The proposed method is a new consistency regularization strategy\nthat encourages segmentation of interpolation of two unlabelled data to be\nconsistent with the interpolation of segmentation maps of those data. This\nmethod represents a specific type of data-adaptive regularization paradigm\nwhich aids to minimize the overfitting of labelled data under high confidence\nvalues. The proposed method is advantageous over adversarial and generative\nmodels as it requires no additional computation. Upon evaluation on two\npublicly available MRI datasets: ACDC and MMWHS, experimental results\ndemonstrate the superiority of the proposed method in comparison to existing\nsemi-supervised models. Code is available at:\nhttps://github.com/hritam-98/ICT-MedSeg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Basak_H/0/1/0/all/0/1\">Hritam Basak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhattacharya_R/0/1/0/all/0/1\">Rajarshi Bhattacharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_R/0/1/0/all/0/1\">Rukhshanda Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_A/0/1/0/all/0/1\">Agniv Chatterjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Transfer: Learning to Route Transferrable Representations. (arXiv:2202.01011v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01011","description":"<p>Knowledge transfer between heterogeneous source and target networks and tasks\nhas received a lot of attention in recent times as large amounts of quality\nlabelled data can be difficult to obtain in many applications. Existing\napproaches typically constrain the target deep neural network (DNN) feature\nrepresentations to be close to the source DNNs feature representations, which\ncan be limiting. We, in this paper, propose a novel adversarial multi-armed\nbandit approach which automatically learns to route source representations to\nappropriate target representations following which they are combined in\nmeaningful ways to produce accurate target models. We see upwards of 5%\naccuracy improvements compared with the state-of-the-art knowledge transfer\nmethods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67,\nand Stanford40 where the source dataset is ImageNet. We qualitatively analyze\nthe goodness of our transfer scheme by showing individual examples of the\nimportant features our target network focuses on in different layers compared\nwith the (closest) competitors. We also observe that our improvement over other\nmethods is higher for smaller target datasets making it an effective tool for\nsmall data applications that may benefit from transfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1\">Keerthiram Murugesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadashivaiah_V/0/1/0/all/0/1\">Vijay Sadashivaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luss_R/0/1/0/all/0/1\">Ronny Luss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhurandhar_A/0/1/0/all/0/1\">Amit Dhurandhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VOS: Learning What You Don't Know by Virtual Outlier Synthesis. (arXiv:2202.01197v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01197","description":"<p>Out-of-distribution (OOD) detection has received much attention lately due to\nits importance in the safe deployment of neural networks. One of the key\nchallenges is that models lack supervision signals from unknown data, and as a\nresult, can produce overconfident predictions on OOD data. Previous approaches\nrely on real outlier datasets for model regularization, which can be costly and\nsometimes infeasible to obtain in practice. In this paper, we present VOS, a\nnovel framework for OOD detection by adaptively synthesizing virtual outliers\nthat can meaningfully regularize the model's decision boundary during training.\nSpecifically, VOS samples virtual outliers from the low-likelihood region of\nthe class-conditional distribution estimated in the feature space. Alongside,\nwe introduce a novel unknown-aware training objective, which contrastively\nshapes the uncertainty space between the ID data and synthesized outlier data.\nVOS achieves state-of-the-art performance on both object detection and image\nclassification models, reducing the FPR95 by up to 7.87% compared to the\nprevious best method. Code is available at\nhttps://github.com/deeplearning-wisc/vos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xuefeng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Image Super-Resolution with Optimal Transport Maps. (arXiv:2202.01116v1 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2202.01116","description":"<p>Real-world image super-resolution (SR) tasks often do not have paired\ndatasets limiting the application of supervised techniques. As a result, the\ntasks are usually approached by unpaired techniques based on Generative\nAdversarial Networks (GANs) which yield complex training losses with several\nregularization terms such as content and identity losses. We theoretically\ninvestigate the optimization problems which arise in such models and find two\nsurprising observations. First, the learned SR map is always an optimal\ntransport (OT) map. Second, we empirically show that the learned map is biased,\ni.e., it may not actually transform the distribution of low-resolution images\nto high-resolution images. Inspired by these findings, we propose an algorithm\nfor unpaired SR which learns an unbiased OT map for the perceptual transport\ncost. Unlike existing GAN-based alternatives, our algorithm has a simple\noptimization objective reducing the neccesity to perform complex hyperparameter\nselection and use additional regularizations. At the same time, it provides\nnearly state-of-the-art performance on the large-scale unpaired AIM-19 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gazdieva_M/0/1/0/all/0/1\">Milena Gazdieva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rout_L/0/1/0/all/0/1\">Litu Rout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korotin_A/0/1/0/all/0/1\">Alexander Korotin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Filippov_A/0/1/0/all/0/1\">Alexander Filippov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}