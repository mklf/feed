{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02639","description":"<p>As humans, we navigate the world through all our senses, using perceptual\ninput from each one to correct the others. We introduce MERLOT Reserve, a model\nthat represents videos jointly over time -- through a new training objective\nthat learns from audio, subtitles, and video frames. Given a video, we replace\nsnippets of text and audio with a MASK token; the model learns by choosing the\ncorrect masked-out snippet. Our objective learns faster than alternatives, and\nperforms well at scale: we pretrain on 20 million YouTube videos.\n</p>\n<p>Empirical results show that MERLOT Reserve learns strong representations\nabout videos through all constituent modalities. When finetuned, it sets a new\nstate-of-the-art on both VCR and TVQA, outperforming prior work by 5% and 7%\nrespectively. Ablations show that both tasks benefit from audio pretraining --\neven VCR, a QA task centered around images (without sound). Moreover, our\nobjective enables out-of-the-box prediction, revealing strong multimodal\ncommonsense understanding. In a fully zero-shot setting, our model obtains\ncompetitive results on four video understanding tasks, even outperforming\nsupervised approaches on the recently proposed Situated Reasoning (STAR)\nbenchmark.\n</p>\n<p>We analyze why incorporating audio leads to better vision-language\nrepresentations, suggesting significant opportunities for future research. We\nconclude by discussing ethical and societal implications of multimodal\npretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational Lens on Cognition: Study Of Autobiographical Versus Imagined Stories With Large-Scale Language Models. (arXiv:2201.02662v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02662","description":"<p>Lifelong experiences and learned knowledge lead to shared expectations about\nhow common situations tend to unfold. Such knowledge enables people to\ninterpret story narratives and identify salient events effortlessly. We study\ndifferences in the narrative flow of events in autobiographical versus imagined\nstories using GPT-3, one of the largest neural language models created to date.\nThe diary-like stories were written by crowdworkers about either a recently\nexperienced event or an imagined event on the same topic. To analyze the\nnarrative flow of events of these stories, we measured sentence\n*sequentiality*, which compares the probability of a sentence with and without\nits preceding story context. We found that imagined stories have higher\nsequentiality than autobiographical stories, and that the sequentiality of\nautobiographical stories is higher when they are retold than when freshly\nrecalled. Through an annotation of events in story sentences, we found that the\nstory types contain similar proportions of major salient events, but that the\nautobiographical stories are denser in factual minor events. Furthermore, in\ncomparison to imagined stories, autobiographical stories contain more concrete\nwords and words related to the first person, cognitive processes, time, space,\nnumbers, social words, and core drives and needs. Our findings highlight the\nopportunity to investigate memory and cognition with large-scale statistical\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafarpour_A/0/1/0/all/0/1\">Anna Jafarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pennebaker_J/0/1/0/all/0/1\">James W. Pennebaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Amharic Speech Emotion Dataset and Classification Benchmark. (arXiv:2201.02710v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02710","description":"<p>In this paper we present the Amharic Speech Emotion Dataset (ASED), which\ncovers four dialects (Gojjam, Wollo, Shewa and Gonder) and five different\nemotions (neutral, fearful, happy, sad and angry). We believe it is the first\nSpeech Emotion Recognition (SER) dataset for the Amharic language. 65 volunteer\nparticipants, all native speakers, recorded 2,474 sound samples, two to four\nseconds in length. Eight judges assigned emotions to the samples with high\nagreement level (Fleiss kappa = 0.8). The resulting dataset is freely available\nfor download. Next, we developed a four-layer variant of the well-known VGG\nmodel which we call VGGb. Three experiments were then carried out using VGGb\nfor SER, using ASED. First, we investigated whether Mel-spectrogram features or\nMel-frequency Cepstral coefficient (MFCC) features work best for Amharic. This\nwas done by training two VGGb SER models on ASED, one using Mel-spectrograms\nand the other using MFCC. Four forms of training were tried, standard\ncross-validation, and three variants based on sentences, dialects and speaker\ngroups. Thus, a sentence used for training would not be used for testing, and\nthe same for a dialect and speaker group. The conclusion was that MFCC features\nare superior under all four training schemes. MFCC was therefore adopted for\nExperiment 2, where VGGb and three other existing models were compared on ASED:\nRESNet50, Alex-Net and LSTM. VGGb was found to have very good accuracy (90.73%)\nas well as the fastest training time. In Experiment 3, the performance of VGGb\nwas compared when trained on two existing SER datasets, RAVDESS (English) and\nEMO-DB (German) as well as on ASED (Amharic). Results are comparable across\nthese languages, with ASED being the highest. This suggests that VGGb can be\nsuccessfully applied to other languages. We hope that ASED will encourage\nresearchers to experiment with other models for Amharic SER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Retta_E/0/1/0/all/0/1\">Ephrem A. Retta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almekhlafi_E/0/1/0/all/0/1\">Eiad Almekhlafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutcliffe_R/0/1/0/all/0/1\">Richard Sutcliffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mhamed_M/0/1/0/all/0/1\">Mustafa Mhamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1\">Haider Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jun Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Rank Constraints for Fast Inference in Structured Models. (arXiv:2201.02715v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02715","description":"<p>Structured distributions, i.e. distributions over combinatorial spaces, are\ncommonly used to learn latent probabilistic representations from observed data.\nHowever, scaling these models is bottlenecked by the high computational and\nmemory complexity with respect to the size of the latent representations.\nCommon models such as Hidden Markov Models (HMMs) and Probabilistic\nContext-Free Grammars (PCFGs) require time and space quadratic and cubic in the\nnumber of hidden states respectively. This work demonstrates a simple approach\nto reduce the computational and memory complexity of a large class of\nstructured models. We show that by viewing the central inference step as a\nmatrix-vector product and using a low-rank constraint, we can trade off model\nexpressivity and speed via the rank. Experiments with neural parameterized\nstructured models for language modeling, polyphonic music modeling,\nunsupervised grammar induction, and video modeling show that our approach\nmatches the accuracy of standard models at large state spaces while providing\npractical speedups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin T. Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuntian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02732","description":"<p>Conversational recommender systems (CRS) aim to recommend suitable items to\nusers through natural language conversations. For developing effective CRSs, a\nmajor technical issue is how to accurately infer user preference from very\nlimited conversation context. To address issue, a promising solution is to\nincorporate external data for enriching the context information. However, prior\nstudies mainly focus on designing fusion models tailored for some specific type\nof external data, which is not general to model and utilize multi-type external\ndata.\n</p>\n<p>To effectively leverage multi-type external data, we propose a novel\ncoarse-to-fine contrastive learning framework to improve data semantic fusion\nfor CRS. In our approach, we first extract and represent multi-grained semantic\nunits from different data signals, and then align the associated multi-type\nsemantic units in a coarse-to-fine way. To implement this framework, we design\nboth coarse-grained and fine-grained procedures for modeling user preference,\nwhere the former focuses on more general, coarse-grained semantic fusion and\nthe latter focuses on more specific, fine-grained semantic fusion. Such an\napproach can be extended to incorporate more kinds of external data. Extensive\nexperiments on two public CRS datasets have demonstrated the effectiveness of\nour approach in both recommendation and conversation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuanhang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">He Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Robustness of a BiLSTM-based Structural Story Classifier. (arXiv:2201.02733v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02733","description":"<p>The growing prevalence of counterfeit stories on the internet has fostered\nsignificant interest towards fast and scalable detection of fake news in the\nmachine learning community. While several machine learning techniques for this\npurpose have emerged, we observe that there is a need to evaluate the impact of\nnoise on these techniques' performance, where noise constitutes news articles\nbeing mistakenly labeled as fake (or real). This work takes a step in that\ndirection, where we examine the impact of noise on a state-of-the-art,\nstructural model based on BiLSTM (Bidirectional Long-Short Term Model) for fake\nnews detection, Hierarchical Discourse-level Structure for Fake News Detection\nby Karimi and Tang (Reference no. 9).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Aftab Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanduri_S/0/1/0/all/0/1\">Sai Durga Prasad Nanduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seenuvasavarathan_S/0/1/0/all/0/1\">Sneha Seenuvasavarathan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Human-like Communicative Intelligence: A Grounded Perspective. (arXiv:2201.02734v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02734","description":"<p>Modern Artificial Intelligence (AI) systems excel at diverse tasks, from\nimage classification to strategy games, even outperforming humans in many of\nthese domains. After making astounding progress in language learning in the\nrecent decade, AI systems, however, seem to approach the ceiling that does not\nreflect important aspects of human communicative capacities. Unlike human\nlearners, communicative AI systems often fail to systematically generalize to\nnew data, suffer from sample inefficiency, fail to capture common-sense\nsemantic knowledge, and do not translate to real-world communicative\nsituations. Cognitive Science offers several insights on how AI could move\nforward from this point. This paper aims to: (1) suggest that the dominant\ncognitively-inspired AI directions, based on nativist and symbolic paradigms,\nlack necessary substantiation and concreteness to guide progress in modern AI,\nand (2) articulate an alternative, \"grounded\", perspective on AI advancement,\ninspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research.\nI review results on 4E research lines in Cognitive Science to distinguish the\nmain aspects of naturalistic learning conditions that play causal roles for\nhuman language development. I then use this analysis to propose a list of\nconcrete, implementable components for building \"grounded\" linguistic\nintelligence. These components include embodying machines in a\nperception-action cycle, equipping agents with active exploration mechanisms so\nthey can build their own curriculum, allowing agents to gradually develop motor\nabilities to promote piecemeal language development, and endowing the agents\nwith adaptive feedback from their physical and social environment. I hope that\nthese ideas can direct AI research towards building machines that develop\nhuman-like language abilities through their experiences with the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubova_M/0/1/0/all/0/1\">Marina Dubova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Approach to Integrate Human-Level Understanding in a Chatbot. (arXiv:2201.02735v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02735","description":"<p>In recent times, a large number of people have been involved in establishing\ntheir own businesses. Unlike humans, chatbots can serve multiple customers at a\ntime, are available 24/7 and reply in less than a fraction of a second. Though\nchatbots perform well in task-oriented activities, in most cases they fail to\nunderstand personalized opinions, statements or even queries which later impact\nthe organization for poor service management. Lack of understanding\ncapabilities in bots disinterest humans to continue conversations with them.\nUsually, chatbots give absurd responses when they are unable to interpret a\nuser's text accurately. Extracting the client reviews from conversations by\nusing chatbots, organizations can reduce the major gap of understanding between\nthe users and the chatbot and improve their quality of products and\nservices.Thus, in our research we incorporated all the key elements that are\nnecessary for a chatbot to analyse and understand an input text precisely and\naccurately. We performed sentiment analysis, emotion detection, intent\nclassification and named-entity recognition using deep learning to develop\nchatbots with humanistic understanding and intelligence. The efficiency of our\napproach can be demonstrated accordingly by the detailed analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abedin_A/0/1/0/all/0/1\">Afia Fairoose Abedin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamun_A/0/1/0/all/0/1\">Amirul Islam Al Mamun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowrin_R/0/1/0/all/0/1\">Rownak Jahan Nowrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_A/0/1/0/all/0/1\">Amitabha Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostakim_M/0/1/0/all/0/1\">Moin Mostakim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitive Computing to Optimize IT Services. (arXiv:2201.02737v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02737","description":"<p>In this paper, the challenges of maintaining a healthy IT operational\nenvironment have been addressed by proactively analyzing IT Service Desk\ntickets, customer satisfaction surveys, and social media data. A Cognitive\nsolution goes beyond the traditional structured data analysis by deep analyses\nof both structured and unstructured text. The salient features of the proposed\nplatform include language identification, translation, hierarchical extraction\nof the most frequently occurring topics, entities and their relationships, text\nsummarization, sentiments, and knowledge extraction from the unstructured text\nusing Natural Language Processing techniques. Moreover, the insights from\nunstructured text combined with structured data allow the development of\nvarious classification, segmentation, and time-series forecasting use-cases on\nthe incident, problem, and change datasets. Further, the text and predictive\ninsights together with raw data are used for visualization and exploration of\nactionable insights on a rich and interactive dashboard. However, it is hard\nnot only to find these insights using traditional structured data analysis but\nit might also take a very long time to discover them, especially while dealing\nwith a massive amount of unstructured data. By taking action on these insights,\norganizations can benefit from a significant reduction of ticket volume,\nreduced operational costs, and increased customer satisfaction. In various\nexperiments, on average, upto 18-25% of yearly ticket volume has been reduced\nusing the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Abbas Raza Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traffic event description based on Twitter data using Unsupervised Learning Methods for Indian road conditions. (arXiv:2201.02738v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02738","description":"<p>Non-recurrent and unpredictable traffic events directly influence road\ntraffic conditions. There is a need for dynamic monitoring and prediction of\nthese unpredictable events to improve road network management. The problem with\nthe existing traditional methods (flow or speed studies) is that the coverage\nof many Indian roads is very sparse and reproducible methods to identify and\ndescribe the events are not available. Addition of some other form of data is\nessential to help with this problem. This could be real-time speed monitoring\ndata like Google Maps, Waze, etc. or social data like Twitter, Facebook, etc.\nIn this paper, an unsupervised learning model is used to perform effective\ntweet classification for enhancing Indian traffic data. The model uses\nword-embeddings to calculate semantic similarity and achieves a test score of\n94.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kilaru_Y/0/1/0/all/0/1\">Yasaswi Sri Chandra Gandhi Kilaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_I/0/1/0/all/0/1\">Indrajit Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Beam Search to Enhance On-device Abstractive Summarization. (arXiv:2201.02739v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02739","description":"<p>We receive several essential updates on our smartphones in the form of SMS,\ndocuments, voice messages, etc. that get buried beneath the clutter of content.\nWe often do not realize the key information without going through the full\ncontent. SMS notifications sometimes help by giving an idea of what the message\nis about, however, they merely offer a preview of the beginning content. One\nway to solve this is to have a single efficient model that can adapt and\nsummarize data from varied sources. In this paper, we tackle this issue and for\nthe first time, propose a novel Adaptive Beam Search to improve the quality of\non-device abstractive summarization that can be applied to SMS, voice messages\nand can be extended to documents. To the best of our knowledge, this is the\nfirst on-device abstractive summarization pipeline to be proposed that can\nadapt to multiple data sources addressing privacy concerns of users as compared\nto the majority of existing summarization systems that send data to a server.\nWe reduce the model size by 30.9% using knowledge distillation and show that\nthis model with a 97.6% lesser memory footprint extracts the same or more key\ninformation as compared to BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_H/0/1/0/all/0/1\">Harichandana B S S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sumit Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Best of Both Worlds: A Hybrid Approach for Multi-Hop Explanation with Declarative Facts. (arXiv:2201.02740v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02740","description":"<p>Language-enabled AI systems can answer complex, multi-hop questions to high\naccuracy, but supporting answers with evidence is a more challenging task which\nis important for the transparency and trustworthiness to users. Prior work in\nthis area typically makes a trade-off between efficiency and accuracy;\nstate-of-the-art deep neural network systems are too cumbersome to be useful in\nlarge-scale applications, while the fastest systems lack reliability. In this\nwork, we integrate fast syntactic methods with powerful semantic methods for\nmulti-hop explanation generation based on declarative facts. Our best system,\nwhich learns a lightweight operation to simulate multi-hop reasoning over\npieces of evidence and fine-tunes language models to re-rank generated\nexplanation chains, outperforms a purely syntactic baseline from prior work by\nup to 7% in gold explanation retrieval rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1\">Aishwarya Reganti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defining maximum acceptable latency of AI-enhanced CAI tools. (arXiv:2201.02792v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02792","description":"<p>Recent years have seen an increasing number of studies around the design of\ncomputer-assisted interpreting tools with integrated automatic speech\nprocessing and their use by trainees and professional interpreters. This paper\ndiscusses the role of system latency of such tools and presents the results of\nan experiment designed to investigate the maximum system latency that is\ncognitively acceptable for interpreters working in the simultaneous modality.\nThe results show that interpreters can cope with a system latency of 3 seconds\nwithout any major impact in the rendition of the original text, both in terms\nof accuracy and fluency. This value is above the typical latency of available\nAI-based CAI tools and paves the way to experiment with larger context-based\nlanguage models and higher latencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fantinuoli_C/0/1/0/all/0/1\">Claudio Fantinuoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montecchio_M/0/1/0/all/0/1\">Maddalena Montecchio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Review of Deep Learning for Automated Medical Coding. (arXiv:2201.02797v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02797","description":"<p>Automated medical coding, an essential task for healthcare operation and\ndelivery, makes unstructured data manageable by predicting medical codes from\nclinical documents. Recent advances in deep learning models in natural language\nprocessing have been widely applied to this task. However, it lacks a unified\nview of the design of neural network architectures for medical coding. This\nreview proposes a unified framework to provide a general understanding of the\nbuilding blocks of medical coding models and summarizes recent advanced models\nunder the proposed framework. Our unified framework decomposes medical coding\ninto four main components, i.e., encoder modules for text feature extraction,\nmechanisms for building deep encoder architectures, decoder modules for\ntransforming hidden representations into medical codes, and the usage of\nauxiliary information. Finally, we discuss key research challenges and future\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering Text Using Attention. (arXiv:2201.02816v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02816","description":"<p>Clustering Text has been an important problem in the domain of Natural\nLanguage Processing. While there are techniques to cluster text based on using\nconventional clustering techniques on top of contextual or non-contextual\nvector space representations, it still remains a prevalent area of research\npossible to various improvements in performance and implementation of these\ntechniques. This paper discusses a novel technique to cluster text using\nattention mechanisms. Attention Mechanisms have proven to be highly effective\nin various NLP tasks in recent times. This paper extends the idea of attention\nmechanism in clustering space and sheds some light on a whole new area of\nresearch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_L/0/1/0/all/0/1\">Lovedeep Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coherence-Based Distributed Document Representation Learning for Scientific Documents. (arXiv:2201.02846v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02846","description":"<p>Distributed document representation is one of the basic problems in natural\nlanguage processing. Currently distributed document representation methods\nmainly consider the context information of words or sentences. These methods do\nnot take into account the coherence of the document as a whole, e.g., a\nrelation between the paper title and abstract, headline and description, or\nadjacent bodies in the document. The coherence shows whether a document is\nmeaningful, both logically and syntactically, especially in scientific\ndocuments (papers or patents, etc.). In this paper, we propose a coupled text\npair embedding (CTPE) model to learn the representation of scientific\ndocuments, which maintains the coherence of the document with coupled text\npairs formed by segmenting the document. First, we divide the document into two\nparts (e.g., title and abstract, etc) which construct a coupled text pair.\nThen, we adopt negative sampling to construct uncoupled text pairs whose two\nparts are from different documents. Finally, we train the model to judge\nwhether the text pair is coupled or uncoupled and use the obtained embedding of\ncoupled text pairs as the embedding of documents. We perform experiments on\nthree datasets for one information retrieval task and two recommendation tasks.\nThe experimental results verify the effectiveness of the proposed CTPE model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shicheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Toxic Review Content on Overall Product Sentiment. (arXiv:2201.02857v1 [cs.HC])","link":"http://arxiv.org/abs/2201.02857","description":"<p>Toxic contents in online product review are a common phenomenon. A content is\nperceived to be toxic when it is rude, disrespectful, or unreasonable and make\nindividuals leave the discussion. Machine learning algorithms helps the sell\nside community to identify such toxic patterns and eventually moderate such\ninputs. Yet, the extant literature provides fewer information about the\nsentiment of a prospective consumer on the perception of a product after being\nexposed to such toxic review content. In this study, we collect a balanced data\nset of review comments from 18 different players segregated into three\ndifferent sectors from google play-store. Then we calculate the sentence-level\nsentiment and toxicity score of individual review content. Finally, we use\nstructural equation modelling to quantitatively study the influence of toxic\ncontent on overall product sentiment. We observe that comment toxicity\nnegatively influences overall product sentiment but do not exhibit a mediating\neffect over reviewer score to influence sector-wise relative rating.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_M/0/1/0/all/0/1\">Mayukh Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahney_S/0/1/0/all/0/1\">Sangeeta Sahney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indian Language Wordnets and their Linkages with Princeton WordNet. (arXiv:2201.02977v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02977","description":"<p>Wordnets are rich lexico-semantic resources. Linked wordnets are extensions\nof wordnets, which link similar concepts in wordnets of different languages.\nSuch resources are extremely useful in many Natural Language Processing (NLP)\napplications, primarily those based on knowledge-based approaches. In such\napproaches, these resources are considered as gold standard/oracle. Thus, it is\ncrucial that these resources hold correct information. Thereby, they are\ncreated by human experts. However, human experts in multiple languages are hard\nto come by. Thus, the community would benefit from sharing of such manually\ncreated resources. In this paper, we release mappings of 18 Indian language\nwordnets linked with Princeton WordNet. We believe that availability of such\nresources will have a direct impact on the progress in NLP for these languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Kevin Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethink Stealthy Backdoor Attacks in Natural Language Processing. (arXiv:2201.02993v1 [cs.CL])","link":"http://arxiv.org/abs/2201.02993","description":"<p>Recently, it has been shown that natural language processing (NLP) models are\nvulnerable to a kind of security threat called the Backdoor Attack, which\nutilizes a `backdoor trigger' paradigm to mislead the models. The most\nthreatening backdoor attack is the stealthy backdoor, which defines the\ntriggers as text style or syntactic. Although they have achieved an incredible\nhigh attack success rate (ASR), we find that the principal factor contributing\nto their ASR is not the `backdoor trigger' paradigm. Thus the capacity of these\nstealthy backdoor attacks is overestimated when categorized as backdoor\nattacks. Therefore, to evaluate the real attack power of backdoor attacks, we\npropose a new metric called attack successful rate difference (ASRD), which\nmeasures the ASR difference between clean state and poison state models.\nBesides, since the defenses against stealthy backdoor attacks are absent, we\npropose Trigger Breaker, consisting of two too simple tricks that can defend\nagainst stealthy backdoor attacks effectively. Experiments on text\nclassification tasks show that our method achieves significantly better\nperformance than state-of-the-art defense methods against stealthy backdoor\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot and Few-Shot Classification of Biomedical Articles in Context of the COVID-19 Pandemic. (arXiv:2201.03017v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03017","description":"<p>MeSH (Medical Subject Headings) is a large thesaurus created by the National\nLibrary of Medicine and used for fine-grained indexing of publications in the\nbiomedical domain. In the context of the COVID-19 pandemic, MeSH descriptors\nhave emerged in relation to articles published on the corresponding topic.\nZero-shot classification is an adequate response for timely labeling of the\nstream of papers with MeSH categories. In this work, we hypothesise that rich\nsemantic information available in MeSH has potential to improve BioBERT\nrepresentations and make them more suitable for zero-shot/few-shot tasks. We\nframe the problem as determining if MeSH term definitions, concatenated with\npaper abstracts are valid instances or not, and leverage multi-task learning to\ninduce the MeSH hierarchy in the representations thanks to a seq2seq task.\nResults establish a baseline on the MedLine and LitCovid datasets, and probing\nshows that the resulting representations convey the hierarchical relations\npresent in MeSH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lupart_S/0/1/0/all/0/1\">Simon Lupart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favre_B/0/1/0/all/0/1\">Benoit Favre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ait_Mokhtar_S/0/1/0/all/0/1\">Salah Ait-Mokhtar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ensemble Approach to Acronym Extraction using Transformers. (arXiv:2201.03026v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03026","description":"<p>Acronyms are abbreviated units of a phrase constructed by using initial\ncomponents of the phrase in a text. Automatic extraction of acronyms from a\ntext can help various Natural Language Processing tasks like machine\ntranslation, information retrieval, and text summarisation. This paper\ndiscusses an ensemble approach for the task of Acronym Extraction, which\nutilises two different methods to extract acronyms and their corresponding long\nforms. The first method utilises a multilingual contextual language model and\nfine-tunes the model to perform the task. The second method relies on a\nconvolutional neural network architecture to extract acronyms and append them\nto the output of the previous method. We also augment the official training\ndataset with additional training samples extracted from several open-access\njournals to help improve the task performance. Our dataset analysis also\nhighlights the noise within the current task dataset. Our approach achieves the\nfollowing macro-F1 scores on test data released with the task: Danish (0.74),\nEnglish-Legal (0.72), English-Scientific (0.73), French (0.63), Persian (0.57),\nSpanish (0.65), Vietnamese (0.65). We release our code and models publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Prashant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilio_L/0/1/0/all/0/1\">Leonardo Zilio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Or&#x103;san</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medication Error Detection Using Contextual Language Models. (arXiv:2201.03035v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03035","description":"<p>Medication errors most commonly occur at the ordering or prescribing stage,\npotentially leading to medical complications and poor health outcomes. While it\nis possible to catch these errors using different techniques; the focus of this\nwork is on textual and contextual analysis of prescription information to\ndetect and prevent potential medication errors. In this paper, we demonstrate\nhow to use BERT-based contextual language models to detect anomalies in written\nor spoken text based on a data set extracted from real-world medical data of\nthousands of patient records. The proposed models are able to learn patterns of\ntext dependency and predict erroneous output based on contextual information\nsuch as patient data. The experimental results yield accuracy up to 96.63% for\ntext input and up to 79.55% for speech input, which is satisfactory for most\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poellabauer_C/0/1/0/all/0/1\">Christian Poellabauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projection: A Mixed-Initiative Research Process. (arXiv:2201.03107v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03107","description":"<p>Communication of dense information between humans and machines is relatively\nlow bandwidth. Many modern search and recommender systems operate as machine\nlearning black boxes, giving little insight as to how they represent\ninformation or why they take certain actions. We present Projection, a\nmixed-initiative interface that aims to increase the bandwidth of communication\nbetween humans and machines throughout the research process. The interface\nsupports adding context to searches and visualizing information in multiple\ndimensions with techniques such as hierarchical clustering and spatial\nprojections. Potential customers have shown interest in the application\nintegrating their research outlining and search processes, enabling them to\nstructure their searches in hierarchies, and helping them visualize related\nspaces of knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silveria_A/0/1/0/all/0/1\">Austin Silveria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Next 1000 Languages in Multilingual Machine Translation: Exploring the Synergy Between Supervised and Self-Supervised Learning. (arXiv:2201.03110v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03110","description":"<p>Achieving universal translation between all human language pairs is the\nholy-grail of machine translation (MT) research. While recent progress in\nmassively multilingual MT is one step closer to reaching this goal, it is\nbecoming evident that extending a multilingual MT system simply by training on\nmore parallel data is unscalable, since the availability of labeled data for\nlow-resource and non-English-centric language pairs is forbiddingly limited. To\nthis end, we present a pragmatic approach towards building a multilingual MT\nmodel that covers hundreds of languages, using a mixture of supervised and\nself-supervised objectives, depending on the data availability for different\nlanguage pairs. We demonstrate that the synergy between these two training\nparadigms enables the model to produce high-quality translations in the\nzero-resource setting, even surpassing supervised translation quality for low-\nand mid-resource languages. We conduct a wide array of experiments to\nunderstand the effect of the degree of multilingual supervision, domain\nmismatches and amounts of parallel and monolingual data on the quality of our\nself-supervised multilingual models. To demonstrate the scalability of the\napproach, we train models with over 200 languages and demonstrate high\nperformance on zero-resource translation on several previously under-studied\nlanguages. We hope our findings will serve as a stepping stone towards enabling\ntranslation for the next thousand languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mia Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic and sentiment analysis of selected Bhagavad Gita translations using BERT-based language framework. (arXiv:2201.03115v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03115","description":"<p>It is well known that translations of songs and poems not only breaks rhythm\nand rhyming patterns, but also results in loss of semantic information. The\nBhagavad Gita is an ancient Hindu philosophical text originally written in\nSanskrit that features a conversation between Lord Krishna and Arjuna prior to\nthe Mahabharata war. The Bhagavad Gita is also one of the key sacred texts in\nHinduism and known as the forefront of the Vedic corpus of Hinduism. In the\nlast two centuries, there has been a lot of interest in Hindu philosophy by\nwestern scholars and hence the Bhagavad Gita has been translated in a number of\nlanguages. However, there is not much work that validates the quality of the\nEnglish translations. Recent progress of language models powered by deep\nlearning has enabled not only translations but better understanding of language\nand texts with semantic and sentiment analysis. Our work is motivated by the\nrecent progress of language models powered by deep learning methods. In this\npaper, we compare selected translations (mostly from Sanskrit to English) of\nthe Bhagavad Gita using semantic and sentiment analyses. We use hand-labelled\nsentiment dataset for tuning state-of-art deep learning-based language model\nknown as \\textit{bidirectional encoder representations from transformers}\n(BERT). We use novel sentence embedding models to provide semantic analysis for\nselected chapters and verses across translations. Finally, we use the\naforementioned models for sentiment and semantic analyses and provide\nvisualisation of results. Our results show that although the style and\nvocabulary in the respective Bhagavad Gita translations vary widely, the\nsentiment analysis and semantic similarity shows that the message conveyed are\nmostly similar across the translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_V/0/1/0/all/0/1\">Venkatesh Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Gender Bias in Consumer Culture. (arXiv:2201.03173v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03173","description":"<p>Cultural items like songs have an important impact in creating and\nreinforcing stereotypes, biases, and discrimination. But the actual nature of\nsuch items is often less transparent. Take songs, for example. Are lyrics\nbiased against women? And how have any such biases changed over time? Natural\nlanguage processing of a quarter of a million songs over 50 years quantifies\nmisogyny. Women are less likely to be associated with desirable traits (i.e.,\ncompetence), and while this bias has decreased, it persists. Ancillary analyses\nfurther suggest that song lyrics may help drive shifts in societal stereotypes\ntowards women, and that lyrical shifts are driven by male artists (as female\nartists were less biased to begin with). Overall, these results shed light on\ncultural evolution, subtle measures of bias and discrimination, and how natural\nlanguage processing and machine learning can provide deeper insight into\nstereotypes and cultural change.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boghrati_R/0/1/0/all/0/1\">Reihane Boghrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_J/0/1/0/all/0/1\">Jonah Berger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style, Content, and the Success of Ideas. (arXiv:2201.03174v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03174","description":"<p>Why do some things succeed in the marketplace of ideas? While some argue that\ncontent drives success, others suggest that style, or the way ideas are\npresented, also plays an important role. To provide a stringent test of style's\nimportance, we examine it in a context where content should be paramount:\nacademic research. While scientists often see writing as a disinterested way to\ncommunicate unobstructed truth, a multi-method investigation indicates that\nwriting style shapes impact. Separating style from content can be difficult as\npapers that tend to use certain language may also write about certain topics.\nConsequently, we focus on a unique class of words linked to style (i.e.,\nfunction words such as \"and,\" \"the,\" and \"on\") that are completely devoid of\ncontent. Natural language processing of almost 30,000 articles from a range of\ndisciplines finds that function words explain 13-27% of language's impact on\ncitations. Ancillary analyses explore specific categories of function words to\nsuggest how style matters, highlighting the role of writing simplicity,\npersonal voice, and temporal perspective. Experiments further underscore the\ncausal impact of style. The results suggest how to boost communication's impact\nand highlight the value of natural language processing for understanding the\nsuccess of ideas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boghrati_R/0/1/0/all/0/1\">Reihane Boghrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_J/0/1/0/all/0/1\">Jonah Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Packard_G/0/1/0/all/0/1\">Grant Packard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Writing Style Aware Document-level Event Extraction. (arXiv:2201.03188v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03188","description":"<p>Event extraction, the technology that aims to automatically get the\nstructural information from documents, has attracted more and more attention in\nmany fields. Most existing works discuss this issue with the token-level\nmulti-label classification framework by distinguishing the tokens as different\nroles while ignoring the writing styles of documents. The writing style is a\nspecial way of content organizing for documents and it is relative fixed in\ndocuments with a special field (e.g. financial, medical documents, etc.). We\nargue that the writing style contains important clues for judging the roles for\ntokens and the ignorance of such patterns might lead to the performance\ndegradation for the existing works. To this end, we model the writing style in\ndocuments as a distribution of argument roles, i.e., Role-Rank Distribution,\nand propose an event extraction model with the Role-Rank Distribution based\nSupervision Mechanism to capture this pattern through the supervised training\nprocess of an event extraction task. We compare our model with state-of-the-art\nmethods on several real-world datasets. The empirical results show that our\napproach outperforms other alternatives with the captured patterns. This\nverifies the writing style contains valuable information that could improve the\nperformance of the event extraction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lixin Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully automatic scoring of handwritten descriptive answers in Japanese language tests. (arXiv:2201.03215v1 [cs.LG])","link":"http://arxiv.org/abs/2201.03215","description":"<p>This paper presents an experiment of automatically scoring handwritten\ndescriptive answers in the trial tests for the new Japanese university entrance\nexamination, which were made for about 120,000 examinees in 2017 and 2018.\nThere are about 400,000 answers with more than 20 million characters. Although\nall answers have been scored by human examiners, handwritten characters are not\nlabelled. We present our attempt to adapt deep neural network-based handwriting\nrecognizers trained on a labelled handwriting dataset into this unlabeled\nanswer set. Our proposed method combines different training strategies,\nensembles multiple recognizers, and uses a language model built from a large\ngeneral corpus to avoid overfitting into specific data. In our experiment, the\nproposed method records character accuracy of over 97% using about 2,000\nverified labelled answers that account for less than 0.5% of the dataset. Then,\nthe recognized answers are fed into a pre-trained automatic scoring system\nbased on the BERT model without correcting misrecognized characters and\nproviding rubric annotations. The automatic scoring system achieves from 0.84\nto 0.98 of Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents\nacceptable similarity of scoring between the automatic scoring system and the\nhuman examiners. These results are promising for further research on end-to-end\nautomatic scoring of descriptive answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oka_H/0/1/0/all/0/1\">Haruki Oka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishioka_T/0/1/0/all/0/1\">Tsunenori Ishioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical study on BlenderBot 2.0 Errors Analysis in terms of Model, Data and User-Centric Approach. (arXiv:2201.03239v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03239","description":"<p>BlenderBot 2.0 is a dialogue model that represents open-domain chatbots by\nreflecting real-time information and remembering user information for an\nextended period using an internet search module and multi-session. Nonetheless,\nthe model still has room for improvement. To this end, we examined BlenderBot\n2.0 limitations and errors from three perspectives: model, data, and user. From\nthe data point of view, we highlight the unclear guidelines provided to workers\nduring the crowdsourcing process, as well as a lack of a process for refining\nhate speech in the collected data and verifying the accuracy of internet-based\ninformation. From a user perspective, we identify nine types of problems of\nBlenderBot 2.0, and their causes are thoroughly investigated. Furthermore, for\neach point of view, practical improvement methods are proposed, and we discuss\nseveral potential future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jungseob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_M/0/1/0/all/0/1\">Midan Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Suhyune Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yujin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Planck Radiation and Quantization Scheme for Human Cognition and Language. (arXiv:2201.03306v1 [q-bio.NC])","link":"http://arxiv.org/abs/2201.03306","description":"<p>As a result of the identification of 'identity' and 'indistinguishability'\nand strong experimental evidence for the presence of the associated\nBose-Einstein statistics in human cognition and language, we argued in previous\nwork for an extension of the research domain of quantum cognition. In addition\nto quantum complex vector spaces and quantum probability models, we showed that\nquantization itself, with words as quanta, is relevant and potentially\nimportant to human cognition. In the present work, we build on this result, and\nintroduce a powerful radiation quantization scheme for human cognition. We show\nthat the lack of independence of the Bose-Einstein statistics compared to the\nMaxwell-Boltzmann statistics can be explained by the presence of a 'meaning\ndynamics', which causes words to be attracted to the same words. And so words\nclump together in the same states, a phenomenon well known for photons in the\nearly years of quantum mechanics, leading to fierce disagreements between\nPlanck and Einstein. Using a simple example, we introduce all the elements to\nget a better and detailed view of this 'meaning dynamics', such as micro and\nmacro states, and Maxwell-Boltzmann, Bose-Einstein and Fermi-Dirac numbers and\nweights, and compare this example and its graphs, with the radiation\nquantization scheme of a Winnie the Pooh story, also with its graphs. By\nconnecting a concept directly to human experience, we show that entanglement is\na necessity for preserving the 'meaning dynamics' we identified, and it becomes\nclear in what way Fermi-Dirac addresses human memory. There, in spaces with\ninternal parameters identical words can nevertheless be assigned different\nstates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Aerts_D/0/1/0/all/0/1\">Diederik Aerts</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Beltran_L/0/1/0/all/0/1\">Lester Beltran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiltedBERT: Resource Adjustable Version of BERT. (arXiv:2201.03327v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03327","description":"<p>In this paper, we proposed a novel adjustable fine-tuning method that\nimproves the training and inference time of the BERT model on downstream tasks.\nIn the proposed method, we first detect more important word vectors in each\nlayer by our proposed redundancy metric and then eliminate the less important\nword vectors with our proposed strategy. In our method, the word vector\nelimination rate in each layer is controlled by the Tilt-Rate hyper-parameter,\nand the model learns to work with a considerably lower number of Floating Point\nOperations (FLOPs) than the original BERT\\textsubscript{base} model. Our\nproposed method does not need any extra training steps, and also it can be\ngeneralized to other transformer-based models. We perform extensive experiments\nthat show the word vectors in higher layers have an impressive amount of\nredundancy that can be eliminated and decrease the training and inference time.\nExperimental results on extensive sentiment analysis, classification and\nregression datasets, and benchmarks like IMDB and GLUE showed that our proposed\nmethod is effective in various datasets. By applying our method on the\nBERT\\textsubscript{base} model, we decrease the inference time up to 5.3 times\nwith less than 0.85\\% accuracy degradation on average. After the fine-tuning\nstage, the inference time of our model can be adjusted with our method\noffline-tuning property for a wide range of the Tilt-Rate value selections.\nAlso, we propose a mathematical speedup analysis that can estimate the speedup\nof our method accurately. With the help of this analysis, the Tilt-Rate\nhyper-parameter can be selected before fine-tuning or while offline-tuning\nstages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kachuee_S/0/1/0/all/0/1\">Sajjad Kachuee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifkhani_M/0/1/0/all/0/1\">Mohammad Sharifkhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03335","description":"<p>We present a new open-source and extensible knowledge extraction toolkit,\ncalled DeepKE (Deep learning based Knowledge Extraction), supporting standard\nfully supervised, low-resource few-shot and document-level scenarios. DeepKE\nimplements various information extraction tasks, including named entity\nrecognition, relation extraction and attribute extraction. With a unified\nframework, DeepKE allows developers and researchers to customize datasets and\nmodels to extract information from unstructured texts according to their\nrequirements. Specifically, DeepKE not only provides various functional modules\nand model implementation for different tasks and scenarios but also organizes\nall components by consistent frameworks to maintain sufficient modularity and\nextensibility. Besides, we present an online platform in\n\\url{<a href=\"http://deepke.zjukg.cn/\">this http URL</a>} for real-time extraction of various tasks. DeepKE\nhas been equipped with Google Colab tutorials and comprehensive documents for\nbeginners. We release the source code at\n\\url{https://github.com/zjunlp/DeepKE}, with a demo video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Liankuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guozhou Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphological Analysis of Japanese Hiragana Sentences using the BI-LSTM CRF Model. (arXiv:2201.03366v1 [cs.CL])","link":"http://arxiv.org/abs/2201.03366","description":"<p>This study proposes a method to develop neural models of the morphological\nanalyzer for Japanese Hiragana sentences using the Bi-LSTM CRF model.\nMorphological analysis is a technique that divides text data into words and\nassigns information such as parts of speech. This technique plays an essential\nrole in downstream applications in Japanese natural language processing systems\nbecause the Japanese language does not have word delimiters between words.\nHiragana is a type of Japanese phonogramic characters, which is used for texts\nfor children or people who cannot read Chinese characters. Morphological\nanalysis of Hiragana sentences is more difficult than that of ordinary Japanese\nsentences because there is less information for dividing. For morphological\nanalysis of Hiragana sentences, we demonstrated the effectiveness of\nfine-tuning using a model based on ordinary Japanese text and examined the\ninfluence of training data on texts of various genres.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izutsu_J/0/1/0/all/0/1\">Jun Izutsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komiya_K/0/1/0/all/0/1\">Kanako Komiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Clinical Trial Reports: Extracting Medical Entities and Their Relations. (arXiv:2010.03550v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.03550","description":"<p>The best evidence concerning comparative treatment effectiveness comes from\nclinical trials, the results of which are reported in unstructured articles.\nMedical experts must manually extract information from articles to inform\ndecision-making, which is time-consuming and expensive. Here we consider the\nend-to-end task of both (a) extracting treatments and outcomes from full-text\narticles describing clinical trials (entity identification) and, (b) inferring\nthe reported results for the former with respect to the latter (relation\nextraction). We introduce new data for this task, and evaluate models that have\nrecently achieved state-of-the-art results on similar tasks in Natural Language\nProcessing. We then propose a new method motivated by how trial results are\ntypically presented that outperforms these purely data-driven baselines.\nFinally, we run a fielded evaluation of the model with a non-profit seeking to\nidentify existing drugs that might be re-purposed for cancer, showing the\npotential utility of end-to-end evidence extraction systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nye_B/0/1/0/all/0/1\">Benjamin E. Nye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeYoung_J/0/1/0/all/0/1\">Jay DeYoung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehman_E/0/1/0/all/0/1\">Eric Lehman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1\">Ani Nenkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_I/0/1/0/all/0/1\">Iain J. Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images. (arXiv:2010.10563v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.10563","description":"<p>Every year physicians face an increasing demand of image-based diagnosis from\npatients, a problem that can be addressed with recent artificial intelligence\nmethods. In this context, we survey works in the area of automatic report\ngeneration from medical images, with emphasis on methods using deep neural\nnetworks, with respect to: (1) Datasets, (2) Architecture Design, (3)\nExplainability and (4) Evaluation Metrics. Our survey identifies interesting\ndevelopments, but also remaining challenges. Among them, the current evaluation\nof generated reports is especially weak, since it mostly relies on traditional\nNatural Language Processing (NLP) metrics, which do not accurately capture\nmedical correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messina_P/0/1/0/all/0/1\">Pablo Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_P/0/1/0/all/0/1\">Pablo Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besa_C/0/1/0/all/0/1\">Cecilia Besa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uribe_S/0/1/0/all/0/1\">Sergio Uribe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+andia_M/0/1/0/all/0/1\">Marcelo and&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejos_C/0/1/0/all/0/1\">Cristian Tejos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capurro_D/0/1/0/all/0/1\">Daniel Capurro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Lay Language Summarization of Biomedical Scientific Reviews. (arXiv:2012.12573v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.12573","description":"<p>Health literacy has emerged as a crucial factor in making appropriate health\ndecisions and ensuring treatment outcomes. However, medical jargon and the\ncomplex structure of professional language in this domain make health\ninformation especially hard to interpret. Thus, there is an urgent unmet need\nfor automated methods to enhance the accessibility of the biomedical literature\nto the general population. This problem can be framed as a type of translation\nproblem between the language of healthcare professionals, and that of the\ngeneral public. In this paper, we introduce the novel task of automated\ngeneration of lay language summaries of biomedical scientific reviews, and\nconstruct a dataset to support the development and evaluation of automated\nmethods through which to enhance the accessibility of the biomedical\nliterature. We conduct analyses of the various challenges in solving this task,\nincluding not only summarization of the key points but also explanation of\nbackground knowledge and simplification of professional language. We experiment\nwith state-of-the-art summarization models as well as several data augmentation\ntechniques, and evaluate their performance using both automated metrics and\nhuman assessment. Results indicate that automatically generated summaries\nproduced using contemporary neural architectures can achieve promising quality\nand readability as compared with reference summaries developed for the lay\npublic by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score\nof 13.30). We also discuss the limitations of the current attempt, providing\ninsights and directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yue Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding. (arXiv:2012.14740v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14740","description":"<p>Pre-training of text and layout has proved effective in a variety of\nvisually-rich document understanding tasks due to its effective model\narchitecture and the advantage of large-scale unlabeled scanned/digital-born\ndocuments. We propose LayoutLMv2 architecture with new pre-training tasks to\nmodel the interaction among text, layout, and image in a single multi-modal\nframework. Specifically, with a two-stream multi-modal Transformer encoder,\nLayoutLMv2 uses not only the existing masked visual-language modeling task but\nalso the new text-image alignment and text-image matching tasks, which make it\nbetter capture the cross-modality interaction in the pre-training stage.\nMeanwhile, it also integrates a spatial-aware self-attention mechanism into the\nTransformer architecture so that the model can fully understand the relative\npositional relationship among different text blocks. Experiment results show\nthat LayoutLMv2 outperforms LayoutLM by a large margin and achieves new\nstate-of-the-art results on a wide variety of downstream visually-rich document\nunderstanding tasks, including FUNSD (0.7895 $\\to$ 0.8420), CORD (0.9493 $\\to$\n0.9601), SROIE (0.9524 $\\to$ 0.9781), Kleister-NDA (0.8340 $\\to$ 0.8520),\nRVL-CDIP (0.9443 $\\to$ 0.9564), and DocVQA (0.7295 $\\to$ 0.8672). We made our\nmodel and code publicly available at \\url{https://aka.ms/layoutlmv2}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lidong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Schema Curation via Causal Association Rule Mining. (arXiv:2104.08811v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08811","description":"<p>Event schemas are structured knowledge sources defining typical real-world\nscenarios (e.g., going to an airport). We present a framework for efficient\nhuman-in-the-loop construction of a schema library, based on a novel script\ninduction system and a well-crafted interface that allows non-experts to\n\"program\" complex event structures. Associated with this work we release a\nschema library: a machine readable resource of 232 detailed event schemas, each\nof which describe a distinct typical scenario in terms of its relevant\nsub-event structure (what happens in the scenario), participants (who plays a\nrole in the scenario), fine-grained typing of each participant, and the implied\nrelational constraints between them. We make our schema library and the\nSchemaBlocks interface available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_N/0/1/0/all/0/1\">Noah Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belyy_A/0/1/0/all/0/1\">Anton Belyy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1\">Nils Holzenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1\">Rachel Rudinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Genetic Algorithms For Extractive Summarization. (arXiv:2105.02365v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.02365","description":"<p>Most current work in NLP utilizes deep learning, which requires a lot of\ntraining data and computational power. This paper investigates the strengths of\nGenetic Algorithms (GAs) for extractive summarization, as we hypothesized that\nGAs could construct more efficient solutions for the summarization task due to\ntheir relative customizability relative to deep learning models. This is done\nby building a vocabulary set, the words of which are represented as an array of\nweights, and optimizing those set of weights with the GA. These weights can be\nused to build an overall weighting of a sentence, which can then be passed to\nsome threshold for extraction. Our results showed that the GA was able to learn\na weight representation that could filter out excessive vocabulary and thus\ndictate sentence importance based on common English words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramos_K/0/1/0/all/0/1\">Kensal Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullaguri_K/0/1/0/all/0/1\">Kalyan Naidu Mullaguri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Annie S. Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotional Voice Conversion: Theory, Databases and ESD. (arXiv:2105.14762v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14762","description":"<p>In this paper, we first provide a review of the state-of-the-art emotional\nvoice conversion research, and the existing emotional speech databases. We then\nmotivate the development of a novel emotional speech database (ESD) that\naddresses the increasing research need. With this paper, the ESD database is\nnow made available to the research community. The ESD database consists of 350\nparallel utterances spoken by 10 native English and 10 native Chinese speakers\nand covers 5 emotion categories (neutral, happy, angry, sad and surprise). More\nthan 29 hours of speech data were recorded in a controlled acoustic\nenvironment. The database is suitable for multi-speaker and cross-lingual\nemotional voice conversion studies. As case studies, we implement several\nstate-of-the-art emotional voice conversion systems on the ESD database. This\npaper provides a reference study on ESD in conjunction with its release.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis. (arXiv:2106.01444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01444","description":"<p>The open-ended nature of visual captioning makes it a challenging area for\nevaluation. The majority of proposed models rely on specialized training to\nimprove human-correlation, resulting in limited adoption, generalizability, and\nexplainabilty. We introduce \"typicality\", a new formulation of evaluation\nrooted in information theory, which is uniquely suited for problems lacking a\ndefinite ground truth. Typicality serves as our framework to develop a novel\nsemantic comparison, SPARCS, as well as referenceless fluency evaluation\nmetrics. Over the course of our analysis, two separate dimensions of fluency\nnaturally emerge: style, captured by metric SPURTS, and grammar, captured in\nthe form of grammatical outlier penalties. Through extensive experiments and\nablation studies on benchmark datasets, we show how these decomposed dimensions\nof semantics and fluency provide greater system-level insight into captioner\ndifferences. Our proposed metrics along with their combination, SMURF, achieve\nstate-of-the-art correlation with human judgment when compared with other\nrule-based evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feinglass_J/0/1/0/all/0/1\">Joshua Feinglass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Communication of Generalizations. (arXiv:2106.02668v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.02668","description":"<p>To build agents that can collaborate effectively with others, recent research\nhas trained artificial agents to communicate with each other in Lewis-style\nreferential games. However, this often leads to successful but uninterpretable\ncommunication. We argue that this is due to the game objective: communicating\nabout a single object in a shared visual context is prone to overfitting and\ndoes not encourage language useful beyond concrete reference. In contrast,\nhuman language conveys a rich variety of abstract ideas. To promote such\nskills, we propose games that require communicating generalizations over sets\nof objects representing abstract visual concepts, optionally with separate\ncontexts for each agent. We find that these games greatly improve systematicity\nand interpretability of the learned languages, according to several metrics in\nthe literature. Finally, we propose a method for identifying logical operations\nembedded in the emergent languages by learning an approximate compositional\nreconstruction of the language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jesse Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06965","description":"<p>Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Changchang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02359","description":"<p>Video captioning combines video understanding and language generation.\nDifferent from image captioning that describes a static image with details of\nalmost every object, video captioning usually considers a sequence of frames\nand biases towards focused objects, e.g., the objects that stay in focus\nregardless of the changing background. Therefore, detecting and properly\naccommodating focused objects is critical in video captioning. To enforce the\ndescription of focused objects and achieve controllable video captioning, we\npropose an Object-Oriented Non-Autoregressive approach (O2NA), which performs\ncaption generation in three steps: 1) identify the focused objects and predict\ntheir locations in the target caption; 2) generate the related attribute words\nand relation words of these focused objects to form a draft caption; and 3)\ncombine video information to refine the draft caption to a fluent final\ncaption. Since the focused objects are generated and located ahead of other\nwords, it is difficult to apply the word-by-word autoregressive generation\nprocess; instead, we adopt a non-autoregressive approach. The experiments on\ntwo benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness\nof O2NA, which achieves results competitive with the state-of-the-arts but with\nboth higher diversity and higher inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13897","description":"<p>The MS MARCO ranking dataset has been widely used for training deep learning\nmodels for IR tasks, achieving considerable effectiveness on diverse zero-shot\nscenarios. However, this type of resource is scarce in languages other than\nEnglish. In this work, we present mMARCO, a multilingual version of the MS\nMARCO passage ranking dataset comprising 13 languages that was created using\nmachine translation. We evaluated mMARCO by fine-tuning monolingual and\nmultilingual re-ranking models, as well as a dense multilingual model on this\ndataset. Experimental results demonstrate that multilingual models fine-tuned\non our translated dataset achieve superior effectiveness to models fine-tuned\non the original English version alone. Our distilled multilingual re-ranker is\ncompetitive with non-distilled models while having 5.4 times fewer parameters.\nLastly, we show a positive correlation between translation quality and\nretrieval effectiveness, providing evidence that improvements in translation\nmethods might lead to improvements in multilingual information retrieval. The\ntranslated datasets and fine-tuned models are available at\nhttps://github.com/unicamp-dl/mMARCO.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Henrique Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeronymo_V/0/1/0/all/0/1\">Vitor Jeronymo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abonizio_H/0/1/0/all/0/1\">Hugo Queiroz Abonizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campiotti_I/0/1/0/all/0/1\">Israel Campiotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fadaee_M/0/1/0/all/0/1\">Marzieh Fadaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03481","description":"<p>Contrastive learning models have achieved great success in unsupervised\nvisual representation learning, which maximize the similarities between feature\nrepresentations of different views of the same image, while minimize the\nsimilarities between feature representations of views of different images. In\ntext summarization, the output summary is a shorter form of the input document\nand they have similar meanings. In this paper, we propose a contrastive\nlearning model for supervised abstractive text summarization, where we view a\ndocument, its gold summary and its model generated summaries as different views\nof the same mean representation and maximize the similarities between them\nduring training. We improve over a strong sequence-to-sequence text generation\nmodel (i.e., BART) on three different summarization datasets. Human evaluation\nalso shows that our model achieves better faithfulness ratings compared to its\ncounterpart without contrastive objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shusheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models. (arXiv:2111.02840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02840","description":"<p>Large-scale pre-trained language models have achieved tremendous success\nacross a wide range of natural language understanding (NLU) tasks, even\nsurpassing human performance. However, recent studies reveal that the\nrobustness of these models can be challenged by carefully crafted textual\nadversarial examples. While several individual datasets have been proposed to\nevaluate model robustness, a principled and comprehensive benchmark is still\nmissing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task\nbenchmark to quantitatively and thoroughly explore and evaluate the\nvulnerabilities of modern large-scale language models under various types of\nadversarial attacks. In particular, we systematically apply 14 textual\nadversarial attack methods to GLUE tasks to construct AdvGLUE, which is further\nvalidated by humans for reliable annotations. Our findings are summarized as\nfollows. (i) Most existing adversarial attack algorithms are prone to\ngenerating invalid or ambiguous adversarial examples, with around 90% of them\neither changing the original semantic meanings or misleading human annotators\nas well. Therefore, we perform a careful filtering process to curate a\nhigh-quality benchmark. (ii) All the language models and robust training\nmethods we tested perform poorly on AdvGLUE, with scores lagging far behind the\nbenign accuracy. We hope our work will motivate the development of new\nadversarial attacks that are more stealthy and semantic-preserving, as well as\nnew robust language models against sophisticated adversarial attacks. AdvGLUE\nis available at https://adversarialglue.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chejian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Impact Score Generation using Ktrain-BERT to Identify Hate Words from Twitter Discussions. (arXiv:2111.12939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.12939","description":"<p>Social media has seen a worrying rise in hate speech in recent times.\nBranching to several distinct categories of cyberbullying, gender\ndiscrimination, or racism, the combined label for such derogatory content can\nbe classified as toxic content in general. This paper presents experimentation\nwith a Keras wrapped lightweight BERT model to successfully identify hate\nspeech and predict probabilistic impact score for the same to extract the\nhateful words within sentences. The dataset used for this task is the Hate\nSpeech and Offensive Content Detection (HASOC 2021) data from FIRE 2021 in\nEnglish. Our system obtained a validation accuracy of 82.60%, with a maximum\nF1-Score of 82.68%. Subsequently, our predictive cases performed significantly\nwell in generating impact scores for successful identification of the hate\ntweets as well as the hateful words from tweet pools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sourav Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_P/0/1/0/all/0/1\">Prasanta Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterji_S/0/1/0/all/0/1\">Sanjay Chatterji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CO2Sum:Contrastive Learning for Factual-Consistent Abstractive Summarization. (arXiv:2112.01147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.01147","description":"<p>Generating factual-consistent summaries is a challenging task for abstractive\nsummarization. Previous works mainly encode factual information or perform\npost-correct/rank after decoding. In this paper, we provide a\nfactual-consistent solution from the perspective of contrastive learning, which\nis a natural extension of previous works. We propose CO2Sum (Contrastive for\nConsistency), a contrastive learning scheme that can be easily applied on\nsequence-to-sequence models for factual-consistent abstractive summarization,\nproving that the model can be fact-aware without modifying the architecture.\nCO2Sum applies contrastive learning on the encoder, which can help the model be\naware of the factual information contained in the input article, or performs\ncontrastive learning on the decoder, which makes the model to generate\nfactual-correct output summary. What's more, these two schemes are orthogonal\nand can be combined to further improve faithfulness. Comprehensive experiments\non public benchmarks demonstrate that CO2Sum improves the faithfulness on large\npre-trained language models and reaches competitive results compared to other\nstrong factual-consistent summarization baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huanqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_W/0/1/0/all/0/1\">Wenjing Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_D/0/1/0/all/0/1\">Dan Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JABER and SABER: Junior and Senior Arabic BERt. (arXiv:2112.04329v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04329","description":"<p>Language-specific pre-trained models have proven to be more accurate than\nmultilingual ones in a monolingual evaluation setting, Arabic is no exception.\nHowever, we found that previously released Arabic BERT models were\nsignificantly under-trained. In this technical report, we present JABER and\nSABER, Junior and Senior Arabic BERt respectively, our pre-trained language\nmodel prototypes dedicated for Arabic. We conduct an empirical study to\nsystematically evaluate the performance of models across a diverse set of\nexisting Arabic NLU tasks. Experimental results show that JABER and SABER\nachieve state-of-the-art performances on ALUE, a new benchmark for Arabic\nLanguage Understanding Evaluation, as well as on a well-established NER\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yimeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_K/0/1/0/all/0/1\">Khalil Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1\">Chao Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xinyu_D/0/1/0/all/0/1\">Duan Xinyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A cognitively driven weighted-entropy model for embedding semantic categories in hyperbolic geometry. (arXiv:2112.06876v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06876","description":"<p>In this paper, an unsupervised and cognitively driven weighted-entropy method\nfor embedding semantic categories in hyperbolic geometry is proposed. The model\nis driven by two fields of research in cognitive linguistics: the first is the\nstatistical learning theory of language acquisition and the proposal of using\nhigh-dimensional networks to represent semantic knowledge in cognition, and the\nsecond is the domain-specific approach to semantic communication. Weighted\nconditional entropy of word co-occurrence is proposed as the embedding metric,\nand the two weighting parameters are collocation diversity and conditional\nprobability ranking in the corresponding statistical distribution. The\nBoltzmann distribution is then used on the weighted-entropy metric and embedded\ninto a hyperbolic Poincare disk model. Testing has been in particular performed\nin the domains of basic color and kinship words, which belong to the classes\nthat domain-specificity focused research in cognitive semantics has most\nintensively investigated. Results show that this new approach can successfully\nmodel and map the semantic relationships of popularity and similarity for most\nof the basic color and kinship words in English and have potential to be\ngeneralized to other semantic domains and different languages. Generally, this\npaper contributes to both computational cognitive semantics and the research on\nnetwork and geometry-driven language embedding in computational linguistics and\nNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_E/0/1/0/all/0/1\">Eugene Yu Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Answer Type and Relation Prediction Task (SMART 2021). (arXiv:2112.07606v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07606","description":"<p>Each year the International Semantic Web Conference organizes a set of\nSemantic Web Challenges to establish competitions that will advance\nstate-of-the-art solutions in some problem domains. The Semantic Answer Type\nand Relation Prediction Task (SMART) task is one of the ISWC 2021 Semantic Web\nchallenges. This is the second year of the challenge after a successful SMART\n2020 at ISWC 2020. This year's version focuses on two sub-tasks that are very\nimportant to Knowledge Base Question Answering (KBQA): Answer Type Prediction\nand Relation Prediction. Question type and answer type prediction can play a\nkey role in knowledge base question answering systems providing insights about\nthe expected answer that are helpful to generate correct queries or rank the\nanswer candidates. More concretely, given a question in natural language, the\nfirst task is, to predict the answer type using a target ontology (e.g.,\nDBpedia or Wikidata. Similarly, the second task is to identify relations in the\nnatural language query and link them to the relations in a target ontology.\nThis paper discusses the task descriptions, benchmark datasets, and evaluation\nmetrics. For more information, please visit https://smart-task.github.io/2021/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_M/0/1/0/all/0/1\">Mohnish Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehmann_J/0/1/0/all/0/1\">Jens Lehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_U/0/1/0/all/0/1\">Uttam Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNMT: Neural Machine Translation Toolkit. (arXiv:2112.15272v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15272","description":"<p>We present an open-source toolkit for neural machine translation (NMT). The\nnew toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along\nwith many other improvements detailed below, in order to create a\nself-contained, simple to use, consistent and comprehensive framework for\nMachine Translation tasks of various domains. It is tooled to support both\nbilingual and multilingual translation tasks, starting from building the model\nfrom respective corpora, to inferring new predictions or packaging the model to\nserving-capable JIT format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_N/0/1/0/all/0/1\">Nguyen Hoang Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dat_N/0/1/0/all/0/1\">Nguyen Thanh Dat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_N/0/1/0/all/0/1\">Nguyen Hoang Minh Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Nguyen Van Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinh_N/0/1/0/all/0/1\">Ngo Thi Vinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thai_N/0/1/0/all/0/1\">Nguyen Phuong Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viet_T/0/1/0/all/0/1\">Tran Hong Viet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypers at ComMA@ICON: Modelling Aggressiveness, Gender Bias and Communal Bias Identification. (arXiv:2112.15417v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.15417","description":"<p>Due to the exponentially increasing reach of social media, it is essential to\nfocus on its negative aspects as it can potentially divide society and incite\npeople into violence. In this paper, we present our system description of work\non the shared task ComMA@ICON, where we have to classify how aggressive the\nsentence is and if the sentence is gender-biased or communal biased. These\nthree could be the primary reasons to cause significant problems in society. As\nteam Hypers we have proposed an approach that utilizes different pretrained\nmodels with Attention and mean pooling methods. We were able to get Rank 3 with\n0.223 Instance F1 score on Bengali, Rank 2 with 0.322 Instance F1 score on\nMulti-lingual set, Rank 4 with 0.129 Instance F1 score on Meitei and Rank 5\nwith 0.336 Instance F1 score on Hindi. The source code and the pretrained\nmodels of this work can be found here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_R/0/1/0/all/0/1\">Roshan Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navaneethakrishnan_S/0/1/0/all/0/1\">Subalalitha Chinnaudayar Navaneethakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi6_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi6</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERN2: an advanced neural biomedical named entity recognition and normalization tool. (arXiv:2201.02080v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02080","description":"<p>In biomedical natural language processing, named entity recognition (NER) and\nnamed entity normalization (NEN) are key tasks that enable the automatic\nextraction of biomedical entities (e.g., diseases and chemicals) from the\never-growing biomedical literature. In this paper, we present BERN2 (Advanced\nBiomedical Entity Recognition and Normalization), a tool that improves the\nprevious neural network-based NER tool (Kim et al., 2019) by employing a\nmulti-task NER model and neural network-based NEN models to achieve much faster\nand more accurate inference. We hope that our tool can help annotate\nlarge-scale biomedical texts more accurately for various tasks such as\nbiomedical knowledge graph construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minbyul Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yonghwa Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Summarization Based on Video-text Modelling. (arXiv:2201.02494v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02494","description":"<p>Modern video summarization methods are based on deep neural networks which\nrequire a large amount of annotated data for training. However, existing\ndatasets for video summarization are small-scale, easily leading to\nover-fitting of the deep models. Considering that the annotation of large-scale\ndatasets is time-consuming, we propose a multimodal self-supervised learning\nframework to obtain semantic representations of videos, which benefits the\nvideo summarization task. Specifically, we explore the semantic consistency\nbetween the visual information and text information of videos, for the\nself-supervised pretraining of a multimodal encoder on a newly-collected\ndataset of video-text pairs. Additionally, we introduce a progressive video\nsummarization method, where the important content in a video is pinpointed\nprogressively to generate better summaries. Finally, an objective evaluation\nframework is proposed to measure the quality of video summaries based on video\nclassification. Extensive experiments have proved the effectiveness and\nsuperiority of our method in rank correlation coefficients, F-score, and the\nproposed objective evaluation compared to the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haopeng_L/0/1/0/all/0/1\">Li Haopeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiuhong_K/0/1/0/all/0/1\">Ke Qiuhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingming_G/0/1/0/all/0/1\">Gong Mingming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_Z/0/1/0/all/0/1\">Zhang Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Compressing Models with Few Samples: Mimicking then Replacing. (arXiv:2201.02620v1 [cs.LG])","link":"http://arxiv.org/abs/2201.02620","description":"<p>Few-sample compression aims to compress a big redundant model into a small\ncompact one with only few samples. If we fine-tune models with these limited\nfew samples directly, models will be vulnerable to overfit and learn almost\nnothing. Hence, previous methods optimize the compressed model layer-by-layer\nand try to make every layer have the same outputs as the corresponding layer in\nthe teacher model, which is cumbersome. In this paper, we propose a new\nframework named Mimicking then Replacing (MiR) for few-sample compression,\nwhich firstly urges the pruned model to output the same features as the\nteacher's in the penultimate layer, and then replaces teacher's layers before\npenultimate with a well-tuned compact one. Unlike previous layer-wise\nreconstruction methods, our MiR optimizes the entire network holistically,\nwhich is not only simple and effective, but also unsupervised and general. MiR\noutperforms previous methods with large margins. Codes will be available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huanyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Y/0/1/0/all/0/1\">Yang Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zhenhua Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Microdosing: Knowledge Distillation for GAN based Compression. (arXiv:2201.02624v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02624","description":"<p>Recently, significant progress has been made in learned image and video\ncompression. In particular the usage of Generative Adversarial Networks has\nlead to impressive results in the low bit rate regime. However, the model size\nremains an important issue in current state-of-the-art proposals and existing\nsolutions require significant computation effort on the decoding side. This\nlimits their usage in realistic scenarios and the extension to video\ncompression. In this paper, we demonstrate how to leverage knowledge\ndistillation to obtain equally capable image decoders at a fraction of the\noriginal number of parameters. We investigate several aspects of our solution\nincluding sequence specialization with side information for image coding.\nFinally, we also show how to transfer the obtained benefits into the setting of\nvideo compression. Overall, this allows us to reduce the model size by a factor\nof 20 and to achieve 50% reduction in decoding time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Helminger_L/0/1/0/all/0/1\">Leonhard Helminger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azevedo_R/0/1/0/all/0/1\">Roberto Azevedo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Djelouah_A/0/1/0/all/0/1\">Abdelaziz Djelouah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_M/0/1/0/all/0/1\">Markus Gross</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schroers_C/0/1/0/all/0/1\">Christopher Schroers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexHDR: Modelling Alignment and Exposure Uncertainties for Flexible HDR Imaging. (arXiv:2201.02625v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02625","description":"<p>High dynamic range (HDR) imaging is of fundamental importance in modern\ndigital photography pipelines and used to produce a high-quality photograph\nwith well exposed regions despite varying illumination across the image. This\nis typically achieved by merging multiple low dynamic range (LDR) images taken\nat different exposures. However, over-exposed regions and misalignment errors\ndue to poorly compensated motion result in artefacts such as ghosting. In this\npaper, we present a new HDR imaging technique that specifically models\nalignment and exposure uncertainties to produce high quality HDR results. We\nintroduce a strategy that learns to jointly align and assess the alignment and\nexposure reliability using an HDR-aware, uncertainty-driven attention map that\nrobustly merges the frames into a single high quality HDR image. Further, we\nintroduce a progressive, multi-stage image fusion approach that can flexibly\nmerge any number of LDR images in a permutation-invariant manner. Experimental\nresults show our method can produce better quality HDR images with up to 0.8dB\nPSNR improvement to the state-of-the-art, and subjective improvements in terms\nof better detail, colours, and fewer artefacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Catley_Chandar_S/0/1/0/all/0/1\">Sibi Catley-Chandar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanay_T/0/1/0/all/0/1\">Thomas Tanay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandroux_L/0/1/0/all/0/1\">Lucas Vandroux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slabaugh_G/0/1/0/all/0/1\">Gregory Slabaugh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perez_Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez-Pellitero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with less labels in Digital Pathology via Scribble Supervision from natural images. (arXiv:2201.02627v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02627","description":"<p>A critical challenge of training deep learning models in the Digital\nPathology (DP) domain is the high annotation cost by medical experts. One way\nto tackle this issue is via transfer learning from the natural image domain\n(NI), where the annotation cost is considerably cheaper. Cross-domain transfer\nlearning from NI to DP is shown to be successful via class\nlabels~\\cite{teh2020learning}. One potential weakness of relying on class\nlabels is the lack of spatial information, which can be obtained from spatial\nlabels such as full pixel-wise segmentation labels and scribble labels. We\ndemonstrate that scribble labels from NI domain can boost the performance of DP\nmodels on two cancer classification datasets (Patch Camelyon Breast Cancer and\nColorectal Cancer dataset). Furthermore, we show that models trained with\nscribble labels yield the same performance boost as full pixel-wise\nsegmentation labels despite being significantly easier and faster to collect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Teh_E/0/1/0/all/0/1\">Eu Wern Teh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taylor_G/0/1/0/all/0/1\">Graham W. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"United adversarial learning for liver tumor segmentation and detection of multi-modality non-contrast MRI. (arXiv:2201.02629v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02629","description":"<p>Simultaneous segmentation and detection of liver tumors (hemangioma and\nhepatocellular carcinoma (HCC)) by using multi-modality non-contrast magnetic\nresonance imaging (NCMRI) are crucial for the clinical diagnosis. However, it\nis still a challenging task due to: (1) the HCC information on NCMRI is\ninvisible or insufficient makes extraction of liver tumors feature difficult;\n(2) diverse imaging characteristics in multi-modality NCMRI causes feature\nfusion and selection difficult; (3) no specific information between hemangioma\nand HCC on NCMRI cause liver tumors detection difficult. In this study, we\npropose a united adversarial learning framework (UAL) for simultaneous liver\ntumors segmentation and detection using multi-modality NCMRI. The UAL first\nutilizes a multi-view aware encoder to extract multi-modality NCMRI information\nfor liver tumor segmentation and detection. In this encoder, a novel edge\ndissimilarity feature pyramid module is designed to facilitate the\ncomplementary multi-modality feature extraction. Second, the newly designed\nfusion and selection channel is used to fuse the multi-modality feature and\nmake the decision of the feature selection. Then, the proposed mechanism of\ncoordinate sharing with padding integrates the multi-task of segmentation and\ndetection so that it enables multi-task to perform united adversarial learning\nin one discriminator. Lastly, an innovative multi-phase radiomics guided\ndiscriminator exploits the clear and specific tumor information to improve the\nmulti-task performance via the adversarial learning strategy. The UAL is\nvalidated in corresponding multi-modality NCMRI (i.e. T1FS pre-contrast MRI,\nT2FS MRI, and DWI) and three phases contrast-enhanced MRI of 255 clinical\nsubjects. The experiments show that UAL has great potential in the clinical\ndiagnosis of liver tumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1\">Jianfeng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1\">Dengwang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shuo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound. (arXiv:2201.02639v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02639","description":"<p>As humans, we navigate the world through all our senses, using perceptual\ninput from each one to correct the others. We introduce MERLOT Reserve, a model\nthat represents videos jointly over time -- through a new training objective\nthat learns from audio, subtitles, and video frames. Given a video, we replace\nsnippets of text and audio with a MASK token; the model learns by choosing the\ncorrect masked-out snippet. Our objective learns faster than alternatives, and\nperforms well at scale: we pretrain on 20 million YouTube videos.\n</p>\n<p>Empirical results show that MERLOT Reserve learns strong representations\nabout videos through all constituent modalities. When finetuned, it sets a new\nstate-of-the-art on both VCR and TVQA, outperforming prior work by 5% and 7%\nrespectively. Ablations show that both tasks benefit from audio pretraining --\neven VCR, a QA task centered around images (without sound). Moreover, our\nobjective enables out-of-the-box prediction, revealing strong multimodal\ncommonsense understanding. In a fully zero-shot setting, our model obtains\ncompetitive results on four video understanding tasks, even outperforming\nsupervised approaches on the recently proposed Situated Reasoning (STAR)\nbenchmark.\n</p>\n<p>We analyze why incorporating audio leads to better vision-language\nrepresentations, suggesting significant opportunities for future research. We\nconclude by discussing ethical and societal implications of multimodal\npretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiasen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanpeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPU-Net: Lightweight U-Net with more diverse features. (arXiv:2201.02656v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02656","description":"<p>Image segmentation is an important task in the medical image field and many\nconvolutional neural networks (CNNs) based methods have been proposed, among\nwhich U-Net and its variants show promising performance. In this paper, we\npropose GP-module and GPU-Net based on U-Net, which can learn more diverse\nfeatures by introducing Ghost module and atrous spatial pyramid pooling (ASPP).\nOur method achieves better performance with more than 4 times fewer parameters\nand 2 times fewer FLOPs, which provides a new potential direction for future\nresearch. Our plug-and-play module can also be applied to existing segmentation\nmethods to further improve their performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Heng Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1\">Di Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_W/0/1/0/all/0/1\">Weihu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Coding for Machines: Partial transmission of SIFT features. (arXiv:2201.02689v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02689","description":"<p>The paper deals with Video Coding for Machines that is a new paradigm in\nvideo coding related to consumption of decoded video by humans and machines.\nFor such tasks, joint transmission of compressed video and features is\nconsidered. In this paper, we focus our considerations of features on SIFT\nkeypoints. They can be extracted from the decoded video with losses in number\nof keypoints and their parameters as compared to the SIFT keypoints extracted\nfrom the original video. Such losses are studied for HEVC and VVC as functions\nof the quantization parameter and the bitrate. In the paper, we propose to\ntransmit the residual feature data together with the compressed video.\nTherefore, even for strongly compressed video, the transmission of whole all\nSIFT keypoint information is avoided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mackowiak_S/0/1/0/all/0/1\">S&#x142;awomir Ma&#x107;kowiak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Doma&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rozek_S/0/1/0/all/0/1\">S&#x142;awomir R&#xf3;&#x17c;ek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cywinski_D/0/1/0/all/0/1\">Dominik Cywi&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szkielda_J/0/1/0/all/0/1\">Jakub Szkie&#x142;da</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing. (arXiv:2201.02693v1 [cs.LG])","link":"http://arxiv.org/abs/2201.02693","description":"<p>Although mission-critical applications require the use of deep neural\nnetworks (DNNs), their continuous execution at mobile devices results in a\nsignificant increase in energy consumption. While edge offloading can decrease\nenergy consumption, erratic patterns in channel quality, network and edge\nserver load can lead to severe disruption of the system's key operations. An\nalternative approach, called split computing, generates compressed\nrepresentations within the model (called \"bottlenecks\"), to reduce bandwidth\nusage and energy consumption. Prior work has proposed approaches that introduce\nadditional layers, to the detriment of energy consumption and latency. For this\nreason, we propose a new framework called BottleFit, which, in addition to\ntargeted DNN architecture modifications, includes a novel training strategy to\nachieve high accuracy even with strong compression rates. We apply BottleFit on\ncutting-edge DNN models in image classification, and show that BottleFit\nachieves 77.1% data compression with up to 0.6% accuracy loss on ImageNet\ndataset, while state of the art such as SPINN loses up to 6% in accuracy. We\nexperimentally measure the power consumption and latency of an image\nclassification application running on an NVIDIA Jetson Nano board (GPU-based)\nand a Raspberry PI board (GPU-less). We show that BottleFit decreases power\nconsumption and latency respectively by up to 49% and 89% with respect to\n(w.r.t.) local computing and by 37% and 55% w.r.t. edge offloading. We also\ncompare BottleFit with state-of-the-art autoencoders-based approaches, and show\nthat (i) BottleFit reduces power consumption and execution time respectively by\nup to 54% and 44% on the Jetson and 40% and 62% on Raspberry PI; (ii) the size\nof the head model executed on the mobile device is 83 times smaller. The code\nrepository will be published for full reproducibility of the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callegaro_D/0/1/0/all/0/1\">Davide Callegaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restuccia_F/0/1/0/all/0/1\">Francesco Restuccia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of Automatic Tree Counting Software from UAV Based Aerial Images With Machine Learning. (arXiv:2201.02698v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02698","description":"<p>Unmanned aerial vehicles (UAV) are used successfully in many application\nareas such as military, security, monitoring, emergency aid, tourism,\nagriculture, and forestry. This study aims to automatically count trees in\ndesignated areas on the Siirt University campus from high-resolution images\nobtained by UAV. Images obtained at 30 meters height with 20% overlap were\nstitched offline at the ground station using Adobe Photoshop's photo merge\ntool. The resulting image was denoised and smoothed by applying the 3x3 median\nand mean filter, respectively. After generating the orthophoto map of the\naerial images captured by the UAV in certain regions, the bounding boxes of\ndifferent objects on these maps were labeled in the modalities of HSV (Hue\nSaturation Value), RGB (Red Green Blue) and Gray. Training, validation, and\ntest datasets were generated and then have been evaluated for classification\nsuccess rates related to tree detection using various machine learning\nalgorithms. In the last step, a ground truth model was established by obtaining\nthe actual tree numbers, and then the prediction performance was calculated by\ncomparing the reference ground truth data with the proposed model. It is\nconsidered that significant success has been achieved for tree count with an\naverage accuracy rate of 87% obtained using the MLP classifier in predetermined\nregions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atas_M/0/1/0/all/0/1\">Musa Ata&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talay_A/0/1/0/all/0/1\">Ayhan Talay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block Walsh-Hadamard Transform Based Binary Layers in Deep Neural Networks. (arXiv:2201.02711v1 [cs.LG])","link":"http://arxiv.org/abs/2201.02711","description":"<p>Convolution has been the core operation of modern deep neural networks. It is\nwell-known that convolutions can be implemented in the Fourier Transform\ndomain. In this paper, we propose to use binary block Walsh-Hadamard transform\n(WHT) instead of the Fourier transform. We use WHT-based binary layers to\nreplace some of the regular convolution layers in deep neural networks. We\nutilize both one-dimensional (1-D) and two-dimensional (2-D) binary WHTs in\nthis paper. In both 1-D and 2-D layers, we compute the binary WHT of the input\nfeature map and denoise the WHT domain coefficients using a nonlinearity which\nis obtained by combining soft-thresholding with the tanh function. After\ndenoising, we compute the inverse WHT. We use 1D-WHT to replace the $1\\times 1$\nconvolutional layers, and 2D-WHT layers can replace the 3$\\times$3 convolution\nlayers and Squeeze-and-Excite layers. 2D-WHT layers with trainable weights can\nbe also inserted before the Global Average Pooling (GAP) layers to assist the\ndense layers. In this way, we can reduce the number of trainable parameters\nsignificantly with a slight decrease in trainable parameters. In this paper, we\nimplement the WHT layers into MobileNet-V2, MobileNet-V3-Large, and ResNet to\nreduce the number of parameters significantly with negligible accuracy loss.\nMoreover, according to our speed test, the 2D-FWHT layer runs about 24 times as\nfast as the regular $3\\times 3$ convolution with 19.51\\% less RAM usage in an\nNVIDIA Jetson Nano experiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_D/0/1/0/all/0/1\">Diaa Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1\">Ahmet Enis Cetin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-labelling and Meta Reweighting Learning for Image Aesthetic Quality Assessment. (arXiv:2201.02714v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02714","description":"<p>In the tasks of image aesthetic quality evaluation, it is difficult to reach\nboth the high score area and low score area due to the normal distribution of\naesthetic datasets. To reduce the error in labeling and solve the problem of\nnormal data distribution, we propose a new aesthetic mixed dataset with\nclassification and regression called AMD-CR, and we train a meta reweighting\nnetwork to reweight the loss of training data differently. In addition, we\nprovide a training strategy acccording to different stages, based on pseudo\nlabels of the binary classification task, and then we use it for aesthetic\ntraining acccording to different stages in classification and regression tasks.\nIn the construction of the network structure, we construct an aesthetic\nadaptive block (AAB) structure that can adapt to any size of the input images.\nBesides, we also use the efficient channel attention (ECA) to strengthen the\nfeature extracting ability of each task. The experimental result shows that our\nmethod improves 0.1112 compared with the conventional methods in SROCC. The\nmethod can also help to find best aesthetic path planning for unmanned aerial\nvehicles (UAV) and vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_H/0/1/0/all/0/1\">Hao Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_H/0/1/0/all/0/1\">Huang Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuai Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaokun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiqiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Rail Recognition Based on 3D Point Clouds. (arXiv:2201.02726v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02726","description":"<p>Accurate rail location is a crucial part in the railway support driving\nsystem for safety monitoring. LiDAR can obtain point clouds that carry 3D\ninformation for the railway environment, especially in darkness and terrible\nweather conditions. In this paper, a real-time rail recognition method based on\n3D point clouds is proposed to solve the challenges, such as disorderly, uneven\ndensity and large volume of the point clouds. A voxel down-sampling method is\nfirst presented for density balanced of railway point clouds, and pyramid\npartition is designed to divide the 3D scanning area into the voxels with\ndifferent volumes. Then, a feature encoding module is developed to find the\nnearest neighbor points and to aggregate their local geometric features for the\ncenter point. Finally, a multi-scale neural network is proposed to generate the\nprediction results of each voxel and the rail location. The experiments are\nconducted under 9 sequences of 3D point cloud data for the railway. The results\nshow that the method has good performance in detecting straight, curved and\nother complex topologies rails.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Weiqi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuecheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expert Knowledge-guided Geometric Representation Learning for Magnetic Resonance Imaging-based Glioma Grading. (arXiv:2201.02746v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02746","description":"<p>Radiomics and deep learning have shown high popularity in automatic glioma\ngrading. Radiomics can extract hand-crafted features that quantitatively\ndescribe the expert knowledge of glioma grades, and deep learning is powerful\nin extracting a large number of high-throughput features that facilitate the\nfinal classification. However, the performance of existing methods can still be\nimproved as their complementary strengths have not been sufficiently\ninvestigated and integrated. Furthermore, lesion maps are usually needed for\nthe final prediction at the testing phase, which is very troublesome. In this\npaper, we propose an expert knowledge-guided geometric representation learning\n(ENROL) framework . Geometric manifolds of hand-crafted features and learned\nfeatures are constructed to mine the implicit relationship between deep\nlearning and radiomics, and therefore to dig mutual consent and essential\nrepresentation for the glioma grades. With a specially designed manifold\ndiscrepancy measurement, the grading model can exploit the input image data and\nexpert knowledge more effectively in the training phase and get rid of the\nrequirement of lesion segmentation maps at the testing phase. The proposed\nframework is flexible regarding deep learning architectures to be utilized.\nThree different architectures have been evaluated and five models have been\ncompared, which show that our framework can always generate promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yeqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Longfei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_Y/0/1/0/all/0/1\">Yan Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hairong Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Yusong Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuadTree Attention for Vision Transformers. (arXiv:2201.02767v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02767","description":"<p>Transformers have been successful in many vision tasks, thanks to their\ncapability of capturing long-range dependency. However, their quadratic\ncomputational complexity poses a major obstacle for applying them to vision\ntasks requiring dense predictions, such as object detection, feature matching,\nstereo, etc. We introduce QuadTree Attention, which reduces the computational\ncomplexity from quadratic to linear. Our quadtree transformer builds token\npyramids and computes attention in a coarse-to-fine manner. At each level, the\ntop K patches with the highest attention scores are selected, such that at the\nnext level, attention is only evaluated within the relevant regions\ncorresponding to these top K patches. We demonstrate that quadtree attention\nachieves state-of-the-art performance in various vision tasks, e.g. with 4.0%\nimprovement in feature matching on ScanNet, about 50% flops reduction in stereo\nmatching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,\n1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on\nsemantic segmentation over previous state-of-the-art transformers. The codes\nare available at\nhttps://github.com/Tangshitao/QuadtreeAttention}{https://github.com/Tangshitao/QuadtreeAttention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shitao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02771","description":"<p>Instead of using current deep-learning segmentation models (like the UNet and\nvariants), we approach the segmentation problem using trained Convolutional\nNeural Network (CNN) classifiers, which automatically extract important\nfeatures from classified targets for image classification. Those extracted\nfeatures can be visualized and formed heatmaps using Gradient-weighted Class\nActivation Mapping (Grad-CAM). This study tested whether the heatmaps could be\nused to segment the classified targets. We also proposed an evaluation method\nfor the heatmaps; that is, to re-train the CNN classifier using images filtered\nby heatmaps and examine its performance. We used the mean-Dice coefficient to\nevaluate segmentation results. Results from our experiments show that heatmaps\ncan locate and segment partial tumor areas. But only use of the heatmaps from\nCNN classifiers may not be an optimal approach for segmentation. In addition,\nwe have verified that the predictions of CNN classifiers mainly depend on tumor\nareas, and dark regions in Grad-CAM's heatmaps also contribute to\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval. (arXiv:2201.02772v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02772","description":"<p>Cross-Modal Retrieval (CMR) is an important research topic across multimodal\ncomputing and information retrieval, which takes one type of data as the query\nto retrieve relevant data of another type, and has been widely used in many\nreal-world applications. Recently, the vision-language pre-trained model\nrepresented by CLIP has demonstrated its superiority of learning visual and\ntextual representations and its impressive performance on various vision and\nlanguage related tasks. Although CLIP as well as the previous pre-trained\nmodels have shown great performance improvement in unsupervised CMR, the\nperformance and impact of these pre-trained models on supervised CMR were\nrarely explored due to the lack of multimodal class-level associations.\n</p>\n<p>In this paper, we take CLIP as the current representative vision-language\npre-trained model to conduct a comprehensive empirical study and provide\ninsights on its performance and impact on supervised CMR. To this end, we first\npropose a novel model CLIP4CMR (\\textbf{CLIP For} supervised\n\\textbf{C}ross-\\textbf{M}odal \\textbf{R}etrieval) that employs pre-trained CLIP\nas backbone network to perform supervised CMR. We then revisit the existing\nloss function design in CMR, including the most common pair-wise losses,\nclass-wise losses and hybrid ones, and provide insights on applying CLIP.\nMoreover, we investigate several concerned issues in supervised CMR and provide\nnew perspectives for this field via CLIP4CMR, including the robustness to\nmodality imbalance and the sensitivity to hyper-parameters. Extensive\nexperimental results show that the CLIP4CMR achieves SOTA results with\nsignificant improvements on the benchmark datasets Wikipedia, NUS-WIDE,\nPascal-Sentence and XmediaNet. Our data and codes are publicly available at\nhttps://github.com/zhixiongz/CLIP4CMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhixiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Wenji Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline Statistical Method For Robust User-Assisted Multiple Segmentation. (arXiv:2201.02779v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02779","description":"<p>Recently, several image segmentation methods that welcome and leverage\ndifferent types of user assistance have been developed. In these methods, the\nuser inputs can be provided by drawing bounding boxes over image objects,\ndrawing scribbles or planting seeds that help to differentiate between image\nboundaries or by interactively refining the missegmented image regions. Due to\nthe variety in the types and the amounts of these inputs, relative assessment\nof different segmentation methods becomes difficult. As a possible solution, we\npropose a simple yet effective, statistical segmentation method that can handle\nand utilize different input types and amounts. The proposed method is based on\nrobust hypothesis testing, specifically the DGL test, and can be implemented\nwith time complexity that is linear in the number of pixels and quadratic in\nthe number of image regions. Therefore, it is suitable to be used as a baseline\nmethod for quick benchmarking and assessing the relative performance\nimprovements of different types of user-assisted segmentation algorithms. We\nprovide a mathematical analysis on the operation of the proposed method,\ndiscuss its capabilities and limitations, provide design guidelines and present\nsimulations that validate its operation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afser_H/0/1/0/all/0/1\">Huseyin Afser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relieving Long-tailed Instance Segmentation via Pairwise Class Balance. (arXiv:2201.02784v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02784","description":"<p>Long-tailed instance segmentation is a challenging task due to the extreme\nimbalance of training samples among classes. It causes severe biases of the\nhead classes (with majority samples) against the tailed ones. This renders \"how\nto appropriately define and alleviate the bias\" one of the most important\nissues. Prior works mainly use label distribution or mean score information to\nindicate a coarse-grained bias. In this paper, we explore to excavate the\nconfusion matrix, which carries the fine-grained misclassification details, to\nrelieve the pairwise biases, generalizing the coarse one. To this end, we\npropose a novel Pairwise Class Balance (PCB) method, built upon a confusion\nmatrix which is updated during training to accumulate the ongoing prediction\npreferences. PCB generates fightback soft labels for regularization during\ntraining. Besides, an iterative learning paradigm is developed to support a\nprogressive and smooth regularization in such debiasing. PCB can be plugged and\nplayed to any existing method as a complement. Experimental results on LVIS\ndemonstrate that our method achieves state-of-the-art performance without bells\nand whistles. Superior results across various architectures show the\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yin-Yin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peizhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiu-Shen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RARA: Zero-shot Sim2Real Visual Navigation with Following Foreground Cues. (arXiv:2201.02798v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02798","description":"<p>The gap between simulation and the real-world restrains many machine learning\nbreakthroughs in computer vision and reinforcement learning from being\napplicable in the real world. In this work, we tackle this gap for the specific\ncase of camera-based navigation, formulating it as following a visual cue in\nthe foreground with arbitrary backgrounds. The visual cue in the foreground can\noften be simulated realistically, such as a line, gate or cone. The challenge\nthen lies in coping with the unknown backgrounds and integrating both. As such,\nthe goal is to train a visual agent on data captured in an empty simulated\nenvironment except for this foreground cue and test this model directly in a\nvisually diverse real world. In order to bridge this big gap, we show it's\ncrucial to combine following techniques namely: Randomized augmentation of the\nfore- and background, regularization with both deep supervision and triplet\nloss and finally abstraction of the dynamics by using waypoints rather than\ndirect velocity commands. The various techniques are ablated in our\nexperimental results both qualitatively and quantitatively finally\ndemonstrating a successful transfer from simulation to the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kelchtermans_K/0/1/0/all/0/1\">Klaas Kelchtermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counteracting Dark Web Text-Based CAPTCHA with Generative Adversarial Learning for Proactive Cyber Threat Intelligence. (arXiv:2201.02799v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02799","description":"<p>Automated monitoring of dark web (DW) platforms on a large scale is the first\nstep toward developing proactive Cyber Threat Intelligence (CTI). While there\nare efficient methods for collecting data from the surface web, large-scale\ndark web data collection is often hindered by anti-crawling measures. In\nparticular, text-based CAPTCHA serves as the most prevalent and prohibiting\ntype of these measures in the dark web. Text-based CAPTCHA identifies and\nblocks automated crawlers by forcing the user to enter a combination of\nhard-to-recognize alphanumeric characters. In the dark web, CAPTCHA images are\nmeticulously designed with additional background noise and variable character\nlength to prevent automated CAPTCHA breaking. Existing automated CAPTCHA\nbreaking methods have difficulties in overcoming these dark web challenges. As\nsuch, solving dark web text-based CAPTCHA has been relying heavily on human\ninvolvement, which is labor-intensive and time-consuming. In this study, we\npropose a novel framework for automated breaking of dark web CAPTCHA to\nfacilitate dark web data collection. This framework encompasses a novel\ngenerative method to recognize dark web text-based CAPTCHA with noisy\nbackground and variable character length. To eliminate the need for human\ninvolvement, the proposed framework utilizes Generative Adversarial Network\n(GAN) to counteract dark web background noise and leverages an enhanced\ncharacter segmentation algorithm to handle CAPTCHA images with variable\ncharacter length. Our proposed framework, DW-GAN, was systematically evaluated\non multiple dark web CAPTCHA testbeds. DW-GAN significantly outperformed the\nstate-of-the-art benchmark methods on all datasets, achieving over 94.4%\nsuccess rate on a carefully collected real-world dark web dataset...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mohammadreza Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsinchun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral Image Denoising Using Non-convex Local Low-rank and Sparse Separation with Spatial-Spectral Total Variation Regularization. (arXiv:2201.02812v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02812","description":"<p>In this paper, we propose a novel nonconvex approach to robust principal\ncomponent analysis for HSI denoising, which focuses on simultaneously\ndeveloping more accurate approximations to both rank and column-wise sparsity\nfor the low-rank and sparse components, respectively. In particular, the new\nmethod adopts the log-determinant rank approximation and a novel\n$\\ell_{2,\\log}$ norm, to restrict the local low-rank or column-wisely sparse\nproperties for the component matrices, respectively. For the\n$\\ell_{2,\\log}$-regularized shrinkage problem, we develop an efficient,\nclosed-form solution, which is named $\\ell_{2,\\log}$-shrinkage operator. The\nnew regularization and the corresponding operator can be generally used in\nother problems that require column-wise sparsity. Moreover, we impose the\nspatial-spectral total variation regularization in the log-based nonconvex RPCA\nmodel, which enhances the global piece-wise smoothness and spectral consistency\nfrom the spatial and spectral views in the recovered HSI. Extensive experiments\non both simulated and real HSIs demonstrate the effectiveness of the proposed\nmethod in denoising HSIs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Chong Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yongyong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xinxin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_A/0/1/0/all/0/1\">Andrew Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chenglizhao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Q/0/1/0/all/0/1\">Qiang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Hyperspectral Images by Using Spectral Data and Fully Connected Neural Network. (arXiv:2201.02821v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02821","description":"<p>It is observed that high classification performance is achieved for one- and\ntwo-dimensional signals by using deep learning methods. In this context, most\nresearchers have tried to classify hyperspectral images by using deep learning\nmethods and classification success over 90% has been achieved for these images.\nDeep neural networks (DNN) actually consist of two parts: i) Convolutional\nneural network (CNN) and ii) fully connected neural network (FCNN). While CNN\ndetermines the features, FCNN is used in classification. In classification of\nthe hyperspectral images, it is observed that almost all of the researchers\nused 2D or 3D convolution filters on the spatial data beside spectral data\n(features). It is convenient to use convolution filters on images or time\nsignals. In hyperspectral images, each pixel is represented by a signature\nvector which consists of individual features that are independent of each\nother. Since the order of the features in the vector can be changed, it doesn't\nmake sense to use convolution filters on these features as on time signals. At\nthe same time, since the hyperspectral images do not have a textural structure,\nthere is no need to use spatial data besides spectral data. In this study,\nhyperspectral images of Indian pines, Salinas, Pavia centre, Pavia university\nand Botswana are classified by using only fully connected neural network and\nthe spectral data with one dimensional. An average accuracy of 97.5% is\nachieved for the test sets of all hyperspectral images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dokur_Z/0/1/0/all/0/1\">Zumray Dokur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Olmez_T/0/1/0/all/0/1\">Tamer Olmez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwnannoma and Cochlea Segmentation. (arXiv:2201.02831v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02831","description":"<p>Domain Adaptation (DA) has recently raised strong interests in the medical\nimaging community. While a large variety of DA techniques has been proposed for\nimage segmentation, most of these techniques have been validated either on\nprivate datasets or on small publicly available datasets. Moreover, these\ndatasets mostly addressed single-class problems. To tackle these limitations,\nthe Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in\nconjunction with the 24th International Conference on Medical Image Computing\nand Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large\nand multi-class benchmark for unsupervised cross-modality DA. The challenge's\ngoal is to segment two key brain structures involved in the follow-up and\ntreatment planning of vestibular schwannoma (VS): the VS and the cochleas.\nCurrently, the diagnosis and surveillance in patients with VS are performed\nusing contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in\nusing non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore,\nwe created an unsupervised cross-modality segmentation benchmark. The training\nset provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105).\nThe aim was to automatically perform unilateral VS and bilateral cochlea\nsegmentation on hrT2 as provided in the testing set (N=137). A total of 16\nteams submitted their algorithm for the evaluation phase. The level of\nperformance reached by the top-performing teams is strikingly high (best median\nDice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice -\nVS:92.5%; Cochleas:87.7%). All top-performing methods made use of an\nimage-to-image translation approach to transform the source-domain images into\npseudo-target-domain images. A segmentation network was then trained using\nthese generated images and the manual annotations provided for the source\nimage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dorent_R/0/1/0/all/0/1\">Reuben Dorent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kujawa_A/0/1/0/all/0/1\">Aaron Kujawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ivory_M/0/1/0/all/0/1\">Marina Ivory</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rieke_N/0/1/0/all/0/1\">Nicola Rieke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joutard_S/0/1/0/all/0/1\">Samuel Joutard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_J/0/1/0/all/0/1\">Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belkov_A/0/1/0/all/0/1\">Arseniy Belkov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Calisto_M/0/1/0/all/0/1\">Maria Baldeon Calisto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1\">Jae Won Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1\">Benoit M. Dawant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_H/0/1/0/all/0/1\">Hexin Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heinrich_M/0/1/0/all/0/1\">Mattias P. Heinrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joshi_S/0/1/0/all/0/1\">Smriti Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kashtanova_V/0/1/0/all/0/1\">Victoriya Kashtanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hyeon Gyu Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kondo_S/0/1/0/all/0/1\">Satoshi Kondo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kruse_C/0/1/0/all/0/1\">Christian N. Kruse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_Yuen_S/0/1/0/all/0/1\">Susana K. Lai-Yuen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ly_B/0/1/0/all/0/1\">Buntheng Ly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shin_H/0/1/0/all/0/1\">Hyungseob Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_Z/0/1/0/all/0/1\">Zixian Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jianghao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shapey_J/0/1/0/all/0/1\">Jonathan Shapey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGUIE-Net: Semantic Attention Guided Underwater Image Enhancement with Multi-Scale Perception. (arXiv:2201.02832v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02832","description":"<p>Due to the wavelength-dependent light attenuation, refraction and scattering,\nunderwater images usually suffer from color distortion and blurred details.\nHowever, due to the limited number of paired underwater images with undistorted\nimages as reference, training deep enhancement models for diverse degradation\ntypes is quite difficult. To boost the performance of data-driven approaches,\nit is essential to establish more effective learning mechanisms that mine\nricher supervised information from limited training sample resources. In this\npaper, we propose a novel underwater image enhancement network, called\nSGUIE-Net, in which we introduce semantic information as high-level guidance\nacross different images that share common semantic regions. Accordingly, we\npropose semantic region-wise enhancement module to perceive the degradation of\ndifferent semantic regions from multiple scales and feed it back to the global\nattention features extracted from its original scale. This strategy helps to\nachieve robust and visually pleasant enhancements to different semantic\nobjects, which should thanks to the guidance of semantic information for\ndifferentiated enhancement. More importantly, for those degradation types that\nare not common in the training sample distribution, the guidance connects them\nwith the already well-learned types according to their semantic relevance.\nExtensive experiments on the publicly available datasets and our proposed\ndataset demonstrated the impressive performance of SGUIE-Net. The code and\nproposed dataset are available at: https://trentqq.github.io/SGUIE-Net.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qi_Q/0/1/0/all/0/1\">Qi Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Kunqian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Haiyong Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_G/0/1/0/all/0/1\">Guojia Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1\">Kun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighted Encoding Optimization for Dynamic Single-pixel Imaging and Sensing. (arXiv:2201.02833v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02833","description":"<p>Using single-pixel detection, the end-to-end neural network that jointly\noptimizes both encoding and decoding enables high-precision imaging and\nhigh-level semantic sensing. However, for varied sampling rates, the\nlarge-scale network requires retraining that is laboursome and\ncomputation-consuming. In this letter, we report a weighted optimization\ntechnique for dynamic rate-adaptive single-pixel imaging and sensing, which\nonly needs to train the network for one time that is available for any sampling\nrates. Specifically, we introduce a novel weighting scheme in the encoding\nprocess to characterize different patterns' modulation efficiency. While the\nnetwork is training at a high sampling rate, the modulation patterns and\ncorresponding weights are updated iteratively, which produces optimal ranked\nencoding series when converged. In the experimental implementation, the optimal\npattern series with the highest weights are employed for light modulation, thus\nachieving highly-efficient imaging and sensing. The reported strategy saves the\nadditional training of another low-rate network required by the existing\ndynamic single-pixel networks, which further doubles training efficiency.\nExperiments on the MNIST dataset validated that once the network is trained\nwith a sampling rate of 1, the average imaging PSNR reaches 23.50 dB at 0.1\nsampling rate, and the image-free classification accuracy reaches up to 95.00\\%\nat a sampling rate of 0.03 and 97.91\\% at a sampling rate of 0.1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhan_X/0/1/0/all/0/1\">Xinrui Zhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_L/0/1/0/all/0/1\">Liheng Bian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_C/0/1/0/all/0/1\">Chunli Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-aligned Spatial Feature Extraction Network for UAV Vehicle Re-identification. (arXiv:2201.02836v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02836","description":"<p>Compared with existing vehicle re-identification (ReID) tasks conducted with\ndatasets collected by fixed surveillance cameras, vehicle ReID for unmanned\naerial vehicle (UAV) is still under-explored and could be more challenging.\nVehicles with the same color and type show extremely similar appearance from\nthe UAV's perspective so that mining fine-grained characteristics becomes\nnecessary. Recent works tend to extract distinguishing information by regional\nfeatures and component features. The former requires input images to be aligned\nand the latter entails detailed annotations, both of which are difficult to\nmeet in UAV application. In order to extract efficient fine-grained features\nand avoid tedious annotating work, this letter develops an unsupervised\nself-aligned network consisting of three branches. The network introduced a\nself-alignment module to convert the input images with variable orientations to\na uniform orientation, which is implemented under the constraint of triple loss\nfunction designed with spatial features. On this basis, spatial features,\nobtained by vertical and horizontal segmentation methods, and global features\nare integrated to improve the representation ability in embedded space.\nExtensive experiments are conducted on UAV-VeID dataset, and our method\nachieves the best performance compared with recent ReID works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_A/0/1/0/all/0/1\">Aihuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiahao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1\">Ping Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mushrooms Detection, Localization and 3D Pose Estimation using RGB-D Sensor for Robotic-picking Applications. (arXiv:2201.02837v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02837","description":"<p>In this paper, we propose mushrooms detection, localization and 3D pose\nestimation algorithm using RGB-D data acquired from a low-cost consumer RGB-D\nsensor. We use the RGB and depth information for different purposes. From RGB\ncolor, we first extract initial contour locations of the mushrooms and then\nprovide both the initial contour locations and the original image to active\ncontour for mushrooms segmentation. These segmented mushrooms are then used as\ninput to a circular Hough transform for each mushroom detection including its\ncenter and radius. Once each mushroom's center position in the RGB image is\nknown, we then use the depth information to locate it in 3D space i.e. in world\ncoordinate system. In case of missing depth information at the detected center\nof each mushroom, we estimate from the nearest available depth information\nwithin the radius of each mushroom. We also estimate the 3D pose of each\nmushroom using a pre-prepared upright mushroom model. We use a global\nregistration followed by local refine registration approach for this 3D pose\nestimation. From the estimated 3D pose, we use only the rotation part expressed\nin quaternion as an orientation of each mushroom. These estimated (X,Y,Z)\npositions, diameters and orientations of the mushrooms are used for\nrobotic-picking applications. We carry out extensive experiments on both 3D\nprinted and real mushrooms which show that our method has an interesting\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Diri_B/0/1/0/all/0/1\">Bashir Al-Diri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sample Importance for Cross-Scenario Video Temporal Grounding. (arXiv:2201.02848v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02848","description":"<p>The task of temporal grounding aims to locate video moment in an untrimmed\nvideo, with a given sentence query. This paper for the first time investigates\nsome superficial biases that are specific to the temporal grounding task, and\nproposes a novel targeted solution. Most alarmingly, we observe that existing\ntemporal ground models heavily rely on some biases (e.g., high preference on\nfrequent concepts or certain temporal intervals) in the visual modal. This\nleads to inferior performance when generalizing the model in cross-scenario\ntest setting. To this end, we propose a novel method called Debiased Temporal\nLanguage Localizer (DebiasTLL) to prevent the model from naively memorizing the\nbiases and enforce it to ground the query sentence based on true inter-modal\nrelationship. Debias-TLL simultaneously trains two models. By our design, a\nlarge discrepancy of these two models' predictions when judging a sample\nreveals higher probability of being a biased sample. Harnessing the informative\ndiscrepancy, we devise a data re-weighing scheme for mitigating the data\nbiases. We evaluate the proposed model in cross-scenario temporal grounding,\nwhere the train / test data are heterogeneously sourced. Experiments show\nlarge-margin superiority of the proposed method in comparison with\nstate-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_P/0/1/0/all/0/1\">Peijun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yadong Mu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition. (arXiv:2201.02849v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02849","description":"<p>Capturing the dependencies between joints is critical in skeleton-based\naction recognition task. Transformer shows great potential to model the\ncorrelation of important joints. However, the existing Transformer-based\nmethods cannot capture the correlation of different joints between frames,\nwhich the correlation is very useful since different body parts (such as the\narms and legs in \"long jump\") between adjacent frames move together. Focus on\nthis problem, A novel spatio-temporal tuples Transformer (STTFormer) method is\nproposed. The skeleton sequence is divided into several parts, and several\nconsecutive frames contained in each part are encoded. And then a\nspatio-temporal tuples self-attention module is proposed to capture the\nrelationship of different joints in consecutive frames. In addition, a feature\naggregation module is introduced between non-adjacent frames to enhance the\nability to distinguish similar actions. Compared with the state-of-the-art\nmethods, our method achieves better performance on two large-scale datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Helei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Biao Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-based Automatic Dial Meter Reading in Unconstrained Scenarios. (arXiv:2201.02850v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02850","description":"<p>The replacement of analog meters with smart meters is costly, laborious, and\nfar from complete in developing countries. The Energy Company of Parana (Copel)\n(Brazil) performs more than 4 million meter readings (almost entirely of\nnon-smart devices) per month, and we estimate that 850 thousand of them are\nfrom dial meters. Therefore, an image-based automatic reading system can reduce\nhuman errors, create a proof of reading, and enable the customers to perform\nthe reading themselves through a mobile application. We propose novel\napproaches for Automatic Dial Meter Reading (ADMR) and introduce a new dataset\nfor ADMR in unconstrained scenarios, called UFPR-ADMR-v2. Our best-performing\nmethod combines YOLOv4 with a novel regression approach (AngReg), and explores\nseveral postprocessing techniques. Compared to previous works, it decreased the\nMean Absolute Error (MAE) from 1,343 to 129 and achieved a meter recognition\nrate (MRR) of 98.90% -- with an error tolerance of 1 Kilowatt-hour (kWh).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salomon_G/0/1/0/all/0/1\">Gabriel Salomon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake Hilsa Fish Detection Using Machine Vision. (arXiv:2201.02853v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02853","description":"<p>Hilsa is the national fish of Bangladesh. Bangladesh is earning a lot of\nforeign currency by exporting this fish. Unfortunately, in recent days, some\nunscrupulous businessmen are selling fake Hilsa fishes to gain profit. The\nSardines and Sardinella are the most sold in the market as Hilsa. The\ngovernment agency of Bangladesh, namely Bangladesh Food Safety Authority said\nthat these fake Hilsa fish contain high levels of cadmium and lead which are\ndetrimental for humans. In this research, we have proposed a method that can\nreadily identify original Hilsa fish and fake Hilsa fish. Based on the research\navailable on online literature, we are the first to do research on identifying\noriginal Hilsa fish. We have collected more than 16,000 images of original and\ncounterfeit Hilsa fish. To classify these images, we have used several deep\nlearning-based models. Then, the performance has been compared between them.\nAmong those models, DenseNet201 achieved the highest accuracy of 97.02%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mirajul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ani_J/0/1/0/all/0/1\">Jannatul Ferdous Ani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1\">Abdur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaman_Z/0/1/0/all/0/1\">Zakia Zaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Makes Weakly Supervised Local Feature Better. (arXiv:2201.02861v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02861","description":"<p>Weakly supervised learning can help local feature methods to overcome the\nobstacle of acquiring a large-scale dataset with densely labeled\ncorrespondences. However, since weak supervision cannot distinguish the losses\ncaused by the detection and description steps, directly conducting weakly\nsupervised learning within a joint describe-then-detect pipeline suffers\nlimited performance. In this paper, we propose a decoupled describe-then-detect\npipeline tailored for weakly supervised local feature learning. Within our\npipeline, the detection step is decoupled from the description step and\npostponed until discriminative and robust descriptors are learned. In addition,\nwe introduce a line-to-window search strategy to explicitly use the camera pose\ninformation for better descriptor learning. Extensive experiments show that our\nmethod, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous\nfully and weakly supervised methods and achieves state-of-the-art performance\non a wide range of downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LongguangWang/0/1/0/all/0/1\">LongguangWang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1\">Qing Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscop. (arXiv:2201.02867v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02867","description":"<p>Recent breakthroughs in high resolution imaging of biomolecules in solution\nwith cryo-electron microscopy (cryo-EM) have unlocked new doors for the\nreconstruction of molecular volumes, thereby promising further advances in\nbiology, chemistry, and pharmacological research amongst others. Despite\nsignificant headway, the immense challenges in cryo-EM data analysis remain\nlegion and intricately inter-disciplinary in nature, requiring insights from\nphysicists, structural biologists, computer scientists, statisticians, and\napplied mathematicians. Meanwhile, recent next-generation volume reconstruction\nalgorithms that combine generative modeling with end-to-end unsupervised deep\nlearning techniques have shown promising results on simulated data, but still\nface considerable hurdles when applied to experimental cryo-EM images. In light\nof the proliferation of such methods and given the interdisciplinary nature of\nthe task, we propose here a critical review of recent advances in the field of\ndeep generative modeling for high resolution cryo-EM volume reconstruction. The\npresent review aims to (i) compare and contrast these new methods, while (ii)\npresenting them from a perspective and using terminology familiar to scientists\nin each of the five aforementioned fields with no specific background in\ncryo-EM. The review begins with an introduction to the mathematical and\ncomputational challenges of deep generative models for cryo-EM volume\nreconstruction, along with an overview of the baseline methodology shared\nacross this class of algorithms. Having established the common thread weaving\nthrough these different models, we provide a practical comparison of these\nstate-of-the-art algorithms, highlighting their relative strengths and\nweaknesses, along with the assumptions that they rely on. This allows us to\nidentify bottlenecks in current methods and avenues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Donnat_C/0/1/0/all/0/1\">Claire Donnat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Levy_A/0/1/0/all/0/1\">Axel Levy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poitevin_F/0/1/0/all/0/1\">Frederic Poitevin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defocus Deblur Microscopy via feature interactive coarse-to-fine network. (arXiv:2201.02876v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02876","description":"<p>The clarity of microscopic images is vital in biology research and diagnosis.\nWhen taking microscopy images at cell or molecule level, mechanical drift\noccurs and could be difficult and expansive to counter. Such a problem could be\novercome by developing an end-to-end deep learning-based workflow capable of\npredicting in focused microscopic images from out-of-focused counterparts. In\nour model, we adopt a structure of multi-level U-net, each level connected\nhead-to-tail with corresponding convolution layers from each other. In contrast\nto the conventional coarse-to-fine model, our model uses the knowledge\ndistilled from the coarse network transferred to the finer network. We evaluate\nthe performance of our model and found our method to be effective and has a\nbetter performance by comparing the results with existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiahe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_B/0/1/0/all/0/1\">Boran Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision. (arXiv:2201.02885v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02885","description":"<p>UAV-based image retrieval in modern agriculture enables gathering large\namounts of spatially referenced crop image data. In large-scale experiments,\nhowever, UAV images suffer from containing a multitudinous amount of crops in a\ncomplex canopy architecture. Especially for the observation of temporal\neffects, this complicates the recognition of individual plants over several\nimages and the extraction of relevant information tremendously. In this work,\nwe present a hands-on workflow for the automatized temporal and spatial\nidentification and individualization of crop images from UAVs abbreviated as\n\"cataloging\" based on comprehensible computer vision methods. We evaluate the\nworkflow on two real-world datasets. One dataset is recorded for observation of\nCercospora leaf spot - a fungal disease - in sugar beet over an entire growing\ncycle. The other one deals with harvest prediction of cauliflower plants. The\nplant catalog is utilized for the extraction of single plant images seen over\nmultiple time points. This gathers large-scale spatio-temporal image dataset\nthat in turn can be applied to train further machine learning models including\nvarious data layers. The presented approach improves analysis and\ninterpretation of UAV data in agriculture significantly. By validation with\nsome reference data, our method shows an accuracy that is similar to more\ncomplex deep learning-based recognition techniques. Our workflow is able to\nautomatize plant cataloging and training image extraction, especially for large\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunder_M/0/1/0/all/0/1\">Maurice G&#xfc;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamati_F/0/1/0/all/0/1\">Facundo R. Ispizua Yamati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kierdorf_J/0/1/0/all/0/1\">Jana Kierdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1\">Ribana Roscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahlein_A/0/1/0/all/0/1\">Anne-Katrin Mahlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauckhage_C/0/1/0/all/0/1\">Christian Bauckhage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resolving Camera Position for a Practical Application of Gaze Estimation on Edge Devices. (arXiv:2201.02946v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02946","description":"<p>Most Gaze estimation research only works on a setup condition that a camera\nperfectly captures eyes gaze. They have not literarily specified how to set up\na camera correctly for a given position of a person. In this paper, we carry\nout a study on gaze estimation with a logical camera setup position. We further\nbring our research in a practical application by using inexpensive edge devices\nwith a realistic scenario. That is, we first set up a shopping environment\nwhere we want to grasp customers gazing behaviors. This setup needs an optimal\ncamera position in order to maintain estimation accuracy from existing gaze\nestimation research. We then apply the state-of-the-art of few-shot learning\ngaze estimation to reduce training sampling in the inference phase. In the\nexperiment, we perform our implemented research on NVIDIA Jetson TX2 and\nachieve a reasonable speed, 12 FPS which is faster compared with our reference\nwork, without much degradation of gaze estimation accuracy. The source code is\nreleased at https://github.com/linh-gist/GazeEstimationTX2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linh Van Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tin Trung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box2Seg: Learning Semantics of 3D Point Clouds with Box-Level Supervision. (arXiv:2201.02963v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02963","description":"<p>Learning dense point-wise semantics from unstructured 3D point clouds with\nfewer labels, although a realistic problem, has been under-explored in\nliterature. While existing weakly supervised methods can effectively learn\nsemantics with only a small fraction of point-level annotations, we find that\nthe vanilla bounding box-level annotation is also informative for semantic\nsegmentation of large-scale 3D point clouds. In this paper, we introduce a\nneural architecture, termed Box2Seg, to learn point-level semantics of 3D point\nclouds with bounding box-level supervision. The key to our approach is to\ngenerate accurate pseudo labels by exploring the geometric and topological\nstructure inside and outside each bounding box. Specifically, an\nattention-based self-training (AST) technique and Point Class Activation\nMapping (PCAM) are utilized to estimate pseudo-labels. The network is further\ntrained and refined with pseudo labels. Experiments on two large-scale\nbenchmarks including S3DIS and ScanNet demonstrate the competitive performance\nof the proposed method. In particular, the proposed network can be trained with\ncheap, or even off-the-shelf bounding box-level annotations and subcloud-level\ntags.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02973","description":"<p>Recent progress on Transformers and multi-layer perceptron (MLP) models\nprovide new network architectural designs for computer vision tasks. Although\nthese models proved to be effective in many vision tasks such as image\nrecognition, there remain challenges in adapting them for low-level vision. The\ninflexibility to support high-resolution images and limitations of local\nattention are perhaps the main bottlenecks for using Transformers and MLPs in\nimage restoration. In this work we present a multi-axis MLP based architecture,\ncalled MAXIM, that can serve as an efficient and flexible general-purpose\nvision backbone for image processing tasks. MAXIM uses a UNet-shaped\nhierarchical structure and supports long-range interactions enabled by\nspatially-gated MLPs. Specifically, MAXIM contains two MLP-based building\nblocks: a multi-axis gated MLP that allows for efficient and scalable spatial\nmixing of local and global visual cues, and a cross-gating block, an\nalternative to cross-attention, which accounts for cross-feature mutual\nconditioning. Both these modules are exclusively based on MLPs, but also\nbenefit from being both global and `fully-convolutional', two properties that\nare desirable for image processing. Our extensive experimental results show\nthat the proposed MAXIM model achieves state-of-the-art performance on more\nthan ten benchmarks across a range of image processing tasks, including\ndenoising, deblurring, deraining, dehazing, and enhancement while requiring\nfewer or comparable numbers of parameters and FLOPs than competitive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yinxiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced total variation minimization for stable image reconstruction. (arXiv:2201.02979v1 [eess.IV])","link":"http://arxiv.org/abs/2201.02979","description":"<p>The total variation (TV) regularization has phenomenally boosted various\nvariational models for image processing tasks. We propose combining the\nbackward diffusion process in the earlier literature of image enhancement with\nthe TV regularization and show that the resulting enhanced TV minimization\nmodel is particularly effective for reducing the loss of contrast, which is\noften encountered by models using the TV regularization. We establish stable\nreconstruction guarantees for the enhanced TV model from noisy subsampled\nmeasurements; non-adaptive linear measurements and variable-density sampled\nFourier measurements are considered. In particular, under some weaker\nrestricted isometry property conditions, the enhanced TV minimization model is\nshown to have tighter reconstruction error bounds than various TV-based models\nfor the scenario where the level of noise is significant and the amount of\nmeasurements is limited. The advantages of the enhanced TV model are also\nnumerically validated by preliminary experiments on the reconstruction of some\nsynthetic, natural, and medical images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+An_C/0/1/0/all/0/1\">Congpei An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Hao-Ning Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoming Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02980","description":"<p>Deep convolutional neural networks (CNNs) are broadly considered to be\nstate-of-the-art generic end-to-end image classification systems. However, they\nare known to underperform when training data are limited and thus require data\naugmentation strategies that render the method computationally expensive and\nnot always effective. Rather than using a data augmentation strategy to encode\ninvariances as typically done in machine learning, here we propose to\nmathematically augment a nearest subspace classification model in\nsliced-Wasserstein space by exploiting certain mathematical properties of the\nRadon Cumulative Distribution Transform (R-CDT), a recently introduced image\ntransform. We demonstrate that for a particular type of learning problem, our\nmathematical solution has advantages over data augmentation with deep CNNs in\nterms of classification accuracy and computational complexity, and is\nparticularly effective under a limited training data setting. The method is\nsimple, effective, computationally efficient, non-iterative, and requires no\nparameters to be tuned. Python code implementing our method is available at\nhttps://github.com/rohdelab/mathematical_augmentation. Our method is integrated\nas a part of the software package PyTransKit, which is available at\nhttps://github.com/rohdelab/PyTransKit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shifat_E_Rabbi_M/0/1/0/all/0/1\">Mohammad Shifat-E-Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubaiyat_A/0/1/0/all/0/1\">Abu Hasnat Mohammad Rubaiyat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1\">Gustavo K. Rohde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Face Recognition Systems. (arXiv:2201.02991v1 [cs.CV])","link":"http://arxiv.org/abs/2201.02991","description":"<p>Face Recognition has proven to be one of the most successful technology and\nhas impacted heterogeneous domains. Deep learning has proven to be the most\nsuccessful at computer vision tasks because of its convolution-based\narchitecture. Since the advent of deep learning, face recognition technology\nhas had a substantial increase in its accuracy. In this paper, some of the most\nimpactful face recognition systems were surveyed. Firstly, the paper gives an\noverview of a general face recognition system. Secondly, the survey covers\nvarious network architectures and training losses that have had a substantial\nimpact. Finally, the paper talks about various databases that are used to\nevaluate the capabilities of a face recognition system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_J/0/1/0/all/0/1\">Jash Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bafna_S/0/1/0/all/0/1\">Sanket Bafna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagaria_D/0/1/0/all/0/1\">Devansh Bagaria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virnodkar_S/0/1/0/all/0/1\">Shyamal Virnodkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskMTL: Attribute prediction in masked facial images with deep multitask learning. (arXiv:2201.03002v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03002","description":"<p>Predicting attributes in the landmark free facial images is itself a\nchallenging task which gets further complicated when the face gets occluded due\nto the usage of masks. Smart access control gates which utilize identity\nverification or the secure login to personal electronic gadgets may utilize\nface as a biometric trait. Particularly, the Covid-19 pandemic increasingly\nvalidates the essentiality of hygienic and contactless identity verification.\nIn such cases, the usage of masks become more inevitable and performing\nattribute prediction helps in segregating the target vulnerable groups from\ncommunity spread or ensuring social distancing for them in a collaborative\nenvironment. We create a masked face dataset by efficiently overlaying masks of\ndifferent shape, size and textures to effectively model variability generated\nby wearing mask. This paper presents a deep Multi-Task Learning (MTL) approach\nto jointly estimate various heterogeneous attributes from a single masked\nfacial image. Experimental results on benchmark face attribute UTKFace dataset\ndemonstrate that the proposed approach supersedes in performance to other\ncompeting techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_P/0/1/0/all/0/1\">Prerana Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_V/0/1/0/all/0/1\">Vinay Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Ronak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_R/0/1/0/all/0/1\">Ritika Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanwadi_D/0/1/0/all/0/1\">Daneshwari Kankanwadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lall_B/0/1/0/all/0/1\">Brejesh Lall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThreshNet: An Efficient DenseNet using Threshold Mechanism to Reduce Connections. (arXiv:2201.03013v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03013","description":"<p>With the continuous development of neural networks in computer vision tasks,\nmore and more network architectures have achieved outstanding success. As one\nof the most advanced neural network architectures, DenseNet shortcuts all\nfeature maps to solve the problem of model depth. Although this network\narchitecture has excellent accuracy at low MACs (multiplications and\naccumulations), it takes excessive inference time. To solve this problem,\nHarDNet reduces the connections between feature maps, making the remaining\nconnections resemble harmonic waves. However, this compression method may\nresult in decreasing model accuracy and increasing MACs and model size. This\nnetwork architecture only reduces the memory access time, its overall\nperformance still needs to be improved. Therefore, we propose a new network\narchitecture using threshold mechanism to further optimize the method of\nconnections. Different numbers of connections for different convolutional\nlayers are discarded to compress the feature maps in ThreshNet. The proposed\nnetwork architecture used three datasets, CIFAR-10, CIFAR-100, and SVHN, to\nevaluate the performance for image classifications. Experimental results show\nthat ThreshNet achieves up to 60% reduction in inference time compared to\nDenseNet, and up to 35% faster training speed and 20% reduction in error rate\ncompared to HarDNet on these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1\">Rui-Yang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jia-Hao Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jen-Shiun Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei-Bin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Glance and Focus Networks for Dynamic Visual Recognition. (arXiv:2201.03014v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03014","description":"<p>Spatial redundancy widely exists in visual recognition tasks, i.e.,\ndiscriminative features in an image or video frame usually correspond to only a\nsubset of pixels, while the remaining regions are irrelevant to the task at\nhand. Therefore, static models which process all the pixels with an equal\namount of computation result in considerable redundancy in terms of time and\nspace consumption. In this paper, we formulate the image recognition problem as\na sequential coarse-to-fine feature learning process, mimicking the human\nvisual system. Specifically, the proposed Glance and Focus Network (GFNet)\nfirst extracts a quick global representation of the input image at a low\nresolution scale, and then strategically attends to a series of salient (small)\nregions to learn finer features. The sequential process naturally facilitates\nadaptive inference at test time, as it can be terminated once the model is\nsufficiently confident about its prediction, avoiding further redundant\ncomputation. It is worth noting that the problem of locating discriminant\nregions in our model is formulated as a reinforcement learning task, thus\nrequiring no additional manual annotations other than classification labels.\nGFNet is general and flexible as it is compatible with any off-the-shelf\nbackbone models (such as MobileNets, EfficientNets and TSM), which can be\nconveniently deployed as the feature extractor. Extensive experiments on a\nvariety of image classification and video recognition tasks and with various\nbackbone models demonstrate the remarkable efficiency of our method. For\nexample, it reduces the average latency of the highly efficient MobileNet-V3 on\nan iPhone XS Max by 1.3x without sacrificing accuracy. Code and pre-trained\nmodels are available at https://github.com/blackfeather-wang/GFNet-Pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_K/0/1/0/all/0/1\">Kangchen Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haojun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1\">Pengfei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning class prototypes from Synthetic InSAR with Vision Transformers. (arXiv:2201.03016v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03016","description":"<p>The detection of early signs of volcanic unrest preceding an eruption, in the\nform of ground deformation in Interferometric Synthetic Aperture Radar (InSAR)\ndata is critical for assessing volcanic hazard. In this work we treat this as a\nbinary classification problem of InSAR images, and propose a novel deep\nlearning methodology that exploits a rich source of synthetically generated\ninterferograms to train quality classifiers that perform equally well in real\ninterferograms. The imbalanced nature of the problem, with orders of magnitude\nfewer positive samples, coupled with the lack of a curated database with\nlabeled InSAR data, sets a challenging task for conventional deep learning\narchitectures. We propose a new framework for domain adaptation, in which we\nlearn class prototypes from synthetic data with vision transformers. We report\ndetection accuracy that surpasses the state of the art on volcanic unrest\ndetection. Moreover, we built upon this knowledge by learning a new,\nnon-linear, projection between the learnt representations and prototype space,\nusing pseudo labels produced by our model from an unlabeled real InSAR dataset.\nThis leads to the new state of the art with $97.1%$ accuracy on our test set.\nWe demonstrate the robustness of our approach by training a simple ResNet-18\nConvolutional Neural Network on the unlabeled real InSAR dataset with\npseudo-labels generated from our top transformer-prototype model. Our\nmethodology provides a significant improvement in performance without the need\nof manually labeling any sample, opening the road for further exploitation of\nsynthetic InSAR data in various remote sensing applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bountos_N/0/1/0/all/0/1\">Nikolaos Ioannis Bountos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michail_D/0/1/0/all/0/1\">Dimitrios Michail</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papoutsis_I/0/1/0/all/0/1\">Ioannis Papoutsis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Feature Learning from Partial Point Clouds via Pose Disentanglement. (arXiv:2201.03018v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03018","description":"<p>Self-supervised learning on point clouds has gained a lot of attention\nrecently, since it addresses the label-efficiency and domain-gap problems on\npoint cloud tasks. In this paper, we propose a novel self-supervised framework\nto learn informative representations from partial point clouds. We leverage\npartial point clouds scanned by LiDAR that contain both content and pose\nattributes, and we show that disentangling such two factors from partial point\nclouds enhances feature representation learning. To this end, our framework\nconsists of three main parts: 1) a completion network to capture holistic\nsemantics of point clouds; 2) a pose regression network to understand the\nviewing angle where partial data is scanned from; 3) a partial reconstruction\nnetwork to encourage the model to learn content and pose features. To\ndemonstrate the robustness of the learnt feature representations, we conduct\nseveral downstream tasks including classification, part segmentation, and\nregistration, with comparisons against state-of-the-art methods. Our method not\nonly outperforms existing self-supervised methods, but also shows a better\ngeneralizability across synthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1\">Meng-Shiun Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1\">Pei-Ze Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-driven Attentive Few-shot Learning over Clean and Noisy Samples. (arXiv:2201.03043v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03043","description":"<p>Over the last couple of years few-shot learning (FSL) has attracted great\nattention towards minimizing the dependency on labeled training examples. An\ninherent difficulty in FSL is the handling of ambiguities resulting from having\ntoo few training samples per class. To tackle this fundamental challenge in\nFSL, we aim to train meta-learner models that can leverage prior semantic\nknowledge about novel classes to guide the classifier synthesis process. In\nparticular, we propose semantically-conditioned feature attention and sample\nattention mechanisms that estimate the importance of representation dimensions\nand training instances. We also study the problem of sample noise in FSL,\ntowards the utilization of meta-learners in more realistic and imperfect\nsettings. Our experimental results demonstrate the effectiveness of the\nproposed semantic FSL model with and without sample noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baran_O/0/1/0/all/0/1\">Orhun Bu&#x11f;ra Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan G&#xf6;kberk Cinbi&#x15f;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Artificial Intelligence for Age Estimation in Digital Forensic Investigations. (arXiv:2201.03045v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03045","description":"<p>The precise age estimation of child sexual abuse and exploitation (CSAE)\nvictims is one of the most significant digital forensic challenges.\nInvestigators often need to determine the age of victims by looking at images\nand interpreting the sexual development stages and other human characteristics.\nThe main priority - safeguarding children -- is often negatively impacted by a\nhuge forensic backlog, cognitive bias and the immense psychological stress that\nthis work can entail. This paper evaluates existing facial image datasets and\nproposes a new dataset tailored to the needs of similar digital forensic\nresearch contributions. This small, diverse dataset of 0 to 20-year-old\nindividuals contains 245 images and is merged with 82 unique images from the\nFG-NET dataset, thus achieving a total of 327 images with high image diversity\nand low age range density. The new dataset is tested on the Deep EXpectation\n(DEX) algorithm pre-trained on the IMDB-WIKI dataset. The overall results for\nyoung adolescents aged 10 to 15 and older adolescents/adults aged 16 to 20 are\nvery encouraging -- achieving MAEs as low as 1.79, but also suggest that the\naccuracy for children aged 0 to 10 needs further work. In order to determine\nthe efficacy of the prototype, valuable input of four digital forensic experts,\nincluding two forensic investigators, has been taken into account to improve\nage estimation results. Further research is required to extend datasets both\nconcerning image density and the equal distribution of factors such as gender\nand racial diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grubl_T/0/1/0/all/0/1\">Thomas Grubl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lallie_H/0/1/0/all/0/1\">Harjinder Singh Lallie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lung infection and normal region segmentation from CT volumes of COVID-19 cases. (arXiv:2201.03050v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03050","description":"<p>This paper proposes an automated segmentation method of infection and normal\nregions in the lung from CT volumes of COVID-19 patients. From December 2019,\nnovel coronavirus disease 2019 (COVID-19) spreads over the world and giving\nsignificant impacts to our economic activities and daily lives. To diagnose the\nlarge number of infected patients, diagnosis assistance by computers is needed.\nChest CT is effective for diagnosis of viral pneumonia including COVID-19. A\nquantitative analysis method of condition of the lung from CT volumes by\ncomputers is required for diagnosis assistance of COVID-19. This paper proposes\nan automated segmentation method of infection and normal regions in the lung\nfrom CT volumes using a COVID-19 segmentation fully convolutional network\n(FCN). In diagnosis of lung diseases including COVID-19, analysis of conditions\nof normal and infection regions in the lung is important. Our method recognizes\nand segments lung normal and infection regions in CT volumes. To segment\ninfection regions that have various shapes and sizes, we introduced dense\npooling connections and dilated convolutions in our FCN. We applied the\nproposed method to CT volumes of COVID-19 cases. From mild to severe cases of\nCOVID-19, the proposed method correctly segmented normal and infection regions\nin the lung. Dice scores of normal and infection regions were 0.911 and 0.753,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yuichiro Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1\">Yoshito Otake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_M/0/1/0/all/0/1\">Masahiro Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akashi_T/0/1/0/all/0/1\">Toshiaki Akashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Infection Segmentation from Chest CT Images Based on Scale Uncertainty. (arXiv:2201.03053v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03053","description":"<p>This paper proposes a segmentation method of infection regions in the lung\nfrom CT volumes of COVID-19 patients. COVID-19 spread worldwide, causing many\ninfected patients and deaths. CT image-based diagnosis of COVID-19 can provide\nquick and accurate diagnosis results. An automated segmentation method of\ninfection regions in the lung provides a quantitative criterion for diagnosis.\nPrevious methods employ whole 2D image or 3D volume-based processes. Infection\nregions have a considerable variation in their sizes. Such processes easily\nmiss small infection regions. Patch-based process is effective for segmenting\nsmall targets. However, selecting the appropriate patch size is difficult in\ninfection region segmentation. We utilize the scale uncertainty among various\nreceptive field sizes of a segmentation FCN to obtain infection regions. The\nreceptive field sizes can be defined as the patch size and the resolution of\nvolumes where patches are clipped from. This paper proposes an infection\nsegmentation network (ISNet) that performs patch-based segmentation and a scale\nuncertainty-aware prediction aggregation method that refines the segmentation\nresult. We design ISNet to segment infection regions that have various\nintensity values. ISNet has multiple encoding paths to process patch volumes\nnormalized by multiple intensity ranges. We collect prediction results\ngenerated by ISNets having various receptive field sizes. Scale uncertainty\namong the prediction results is extracted by the prediction aggregation method.\nWe use an aggregation FCN to generate a refined segmentation result considering\nscale uncertainty among the predictions. In our experiments using 199 chest CT\nvolumes of COVID-19 cases, the prediction aggregation method improved the dice\nsimilarity score from 47.6% to 62.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_T/0/1/0/all/0/1\">Tong Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayashi_Y/0/1/0/all/0/1\">Yuichiro Hayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1\">Yoshito Otake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hashimoto_M/0/1/0/all/0/1\">Masahiro Hashimoto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akashi_T/0/1/0/all/0/1\">Toshiaki Akashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aoki_S/0/1/0/all/0/1\">Shigeki Aoki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The State of Aerial Surveillance: A Survey. (arXiv:2201.03080v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03080","description":"<p>The rapid emergence of airborne platforms and imaging sensors are enabling\nnew forms of aerial surveillance due to their unprecedented advantages in\nscale, mobility, deployment and covert observation capabilities. This paper\nprovides a comprehensive overview of human-centric aerial surveillance tasks\nfrom a computer vision and pattern recognition perspective. It aims to provide\nreaders with an in-depth systematic review and technical analysis of the\ncurrent state of aerial surveillance tasks using drones, UAVs and other\nairborne platforms. The main object of interest is humans, where single or\nmultiple subjects are to be detected, identified, tracked, re-identified and\nhave their behavior analyzed. More specifically, for each of these four tasks,\nwe first discuss unique challenges in performing these tasks in an aerial\nsetting compared to a ground-based setting. We then review and analyze the\naerial datasets publicly available for each task, and delve deep into the\napproaches in the aerial literature and investigate how they presently address\nthe aerial challenges. We conclude the paper with discussion on the missing\ngaps and open research questions to inform future research avenues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingli Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageSubject: A Large-scale Dataset for Subject Detection. (arXiv:2201.03101v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03101","description":"<p>Main subjects usually exist in the images or videos, as they are the objects\nthat the photographer wants to highlight. Human viewers can easily identify\nthem but algorithms often confuse them with other objects. Detecting the main\nsubjects is an important technique to help machines understand the content of\nimages and videos. We present a new dataset with the goal of training models to\nunderstand the layout of the objects and the context of the image then to find\nthe main subjects among them. This is achieved in three aspects. By gathering\nimages from movie shots created by directors with professional shooting skills,\nwe collect the dataset with strong diversity, specifically, it contains\n107\\,700 images from 21\\,540 movie shots. We labeled them with the bounding box\nlabels for two classes: subject and non-subject foreground object. We present a\ndetailed analysis of the dataset and compare the task with saliency detection\nand object detection. ImageSubject is the first dataset that tries to localize\nthe subject in an image that the photographer wants to highlight. Moreover, we\nfind the transformer-based detection model offers the best result among other\npopular model architectures. Finally, we discuss the potential applications and\nconclude with the importance of the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xin Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preserving Domain Private Representation via Mutual Information Maximization. (arXiv:2201.03102v1 [cs.LG])","link":"http://arxiv.org/abs/2201.03102","description":"<p>Recent advances in unsupervised domain adaptation have shown that mitigating\nthe domain divergence by extracting the domain-invariant representation could\nsignificantly improve the generalization of a model to an unlabeled data\ndomain. Nevertheless, the existing methods fail to effectively preserve the\nrepresentation that is private to the label-missing domain, which could\nadversely affect the generalization. In this paper, we propose an approach to\npreserve such representation so that the latent distribution of the unlabeled\ndomain could represent both the domain-invariant features and the individual\ncharacteristics that are private to the unlabeled domain. In particular, we\ndemonstrate that maximizing the mutual information between the unlabeled domain\nand its latent space while mitigating the domain divergence can achieve such\npreservation. We also theoretically and empirically validate that preserving\nthe representation that is private to the unlabeled domain is important and of\nnecessity for the cross-domain generalization. Our approach outperforms\nstate-of-the-art methods on several public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weipeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kuangen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Clarence W. de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signal Reconstruction from Quantized Noisy Samples of the Discrete Fourier Transform. (arXiv:2201.03114v1 [eess.SP])","link":"http://arxiv.org/abs/2201.03114","description":"<p>In this paper, we present two variations of an algorithm for signal\nreconstruction from one-bit or two-bit noisy observations of the discrete\nFourier transform (DFT). The one-bit observations of the DFT correspond to the\nsign of its real part, whereas, the two-bit observations of the DFT correspond\nto the signs of both the real and imaginary parts of the DFT. We focus on\nimages for analysis and simulations, thus using the sign of the 2D-DFT. This\nchoice of the class of signals is inspired by previous works on this problem.\nFor our algorithm, we show that the expected mean squared error (MSE) in signal\nreconstruction is asymptotically proportional to the inverse of the sampling\nrate. The samples are affected by additive zero-mean noise of known\ndistribution. We solve this signal estimation problem by designing an algorithm\nthat uses contraction mapping, based on the Banach fixed point theorem.\nNumerical tests with four benchmark images are provided to show the\neffectiveness of our algorithm. Various metrics for image reconstruction\nquality assessment such as PSNR, SSIM, ESSIM, and MS-SSIM are employed. On all\nfour benchmark images, our algorithm outperforms the state-of-the-art in all of\nthese metrics by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goyal_M/0/1/0/all/0/1\">Mohak Goyal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Animesh Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematic biases when using deep neural networks for annotating large catalogs of astronomical images. (arXiv:2201.03131v1 [astro-ph.GA])","link":"http://arxiv.org/abs/2201.03131","description":"<p>Deep convolutional neural networks (DCNNs) have become the most common\nsolution for automatic image annotation due to their non-parametric nature,\ngood performance, and their accessibility through libraries such as TensorFlow.\nAmong other fields, DCNNs are also a common approach to the annotation of large\nastronomical image databases acquired by digital sky surveys. One of the main\ndownsides of DCNNs is the complex non-intuitive rules that make DCNNs act as a\n``black box\", providing annotations in a manner that is unclear to the user.\nTherefore, the user is often not able to know what information is used by the\nDCNNs for the classification. Here we demonstrate that the training of a DCNN\nis sensitive to the context of the training data such as the location of the\nobjects in the sky. We show that for basic classification of elliptical and\nspiral galaxies, the sky location of the galaxies used for training affects the\nbehavior of the algorithm, and leads to a small but consistent and\nstatistically significant bias. That bias exhibits itself in the form of\ncosmological-scale anisotropy in the distribution of basic galaxy morphology.\nTherefore, while DCNNs are powerful tools for annotating images of extended\nsources, the construction of training sets for galaxy morphology should take\ninto consideration more aspects than the visual appearance of the object. In\nany case, catalogs created with deep neural networks that exhibit signs of\ncosmological anisotropy should be interpreted with the possibility of\nconsistent bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Dhar_S/0/1/0/all/0/1\">Sanchari Dhar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Level Attention for Unsupervised Person Re-Identification. (arXiv:2201.03141v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03141","description":"<p>The attention mechanism is widely used in deep learning because of its\nexcellent performance in neural networks without introducing additional\ninformation. However, in unsupervised person re-identification, the attention\nmodule represented by multi-headed self-attention suffers from attention\nspreading in the condition of non-ground truth. To solve this problem, we\ndesign pixel-level attention module to provide constraints for multi-headed\nself-attention. Meanwhile, for the trait that the identification targets of\nperson re-identification data are all pedestrians in the samples, we design\ndomain-level attention module to provide more comprehensive pedestrian\nfeatures. We combine head-level, pixel-level and domain-level attention to\npropose multi-level attention block and validate its performance on for large\nperson re-identification datasets (Market-1501, DukeMTMC-reID and MSMT17 and\nPersonX).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Low-Light Images in Real World via Cross-Image Disentanglement. (arXiv:2201.03145v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03145","description":"<p>Images captured in the low-light condition suffer from low visibility and\nvarious imaging artifacts, e.g., real noise. Existing supervised enlightening\nalgorithms require a large set of pixel-aligned training image pairs, which are\nhard to prepare in practice. Though weakly-supervised or unsupervised methods\ncan alleviate such challenges without using paired training images, some\nreal-world artifacts inevitably get falsely amplified because of the lack of\ncorresponded supervision. In this paper, instead of using perfectly aligned\nimages for training, we creatively employ the misaligned real-world images as\nthe guidance, which are considerably easier to collect. Specifically, we\npropose a Cross-Image Disentanglement Network (CIDN) to separately extract\ncross-image brightness and image-specific content features from\nlow/normal-light images. Based on that, CIDN can simultaneously correct the\nbrightness and suppress image artifacts in the feature domain, which largely\nincreases the robustness to the pixel shifts. Furthermore, we collect a new\nlow-light image enhancement dataset consisting of misaligned training images\nwith real-world corruptions. Experimental results show that our model achieves\nstate-of-the-art performances on both the newly proposed dataset and other\npopular low-light datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_L/0/1/0/all/0/1\">Lanqing Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_R/0/1/0/all/0/1\">Renjie Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kot_A/0/1/0/all/0/1\">Alex Kot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TFS Recognition: Investigating MPH]{Thai Finger Spelling Recognition: Investigating MediaPipe Hands Potentials. (arXiv:2201.03170v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03170","description":"<p>Thai Finger Spelling (TFS) sign recognition could benefit a community of\nhearing-difficulty people in bridging to a major hearing population. With a\nrelatively large number of alphabets, TFS employs multiple signing schemes. Two\nschemes of more common signing -- static and dynamic single-hand signing,\nwidely used in other sign languages -- have been addressed in several previous\nworks. To complete the TFS sign recognition, the remaining two of quite\ndistinct signing schemes -- static and dynamic point-on-hand signing -- need to\nbe sufficiently addressed.\n</p>\n<p>With the advent of many off-the-shelf hand skeleton prediction models and\nthat training a model to recognize a sign language from scratch is expensive,\nwe explore an approach building upon recently launched MediaPipe Hands (MPH).\nMPH is a high-precision well-trained model for hand-keypoint detection.\n</p>\n<p>We have investigated MPH on three TFS schemes: static-single-hand (S1),\nsimplified dynamic-single-hand (S2) and static-point-on-hand (P1) schemes.\n</p>\n<p>Our results show that MPH can satisfactorily address single-hand schemes with\naccuracy of 84.57% on both S1 and S2.\n</p>\n<p>However, our finding reveals a shortcoming of MPH in addressing a\npoint-on-hand scheme, whose accuracy is 23.66% on P1 conferring to 69.19%\nobtained from conventional classification trained from scratch. This\nshortcoming has been investigated and attributed to self occlusion and\nhandedness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanalohit_J/0/1/0/all/0/1\">Jinnavat Sanalohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katanyukul_T/0/1/0/all/0/1\">Tatpong Katanyukul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond. (arXiv:2201.03176v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03176","description":"<p>Pedestrian detection is the cornerstone of many vision based applications,\nstarting from object tracking to video surveillance and more recently,\nautonomous driving. With the rapid development of deep learning in object\ndetection, pedestrian detection has achieved very good performance in\ntraditional single-dataset training and evaluation setting. However, in this\nstudy on generalizable pedestrian detectors, we show that, current pedestrian\ndetectors poorly handle even small domain shifts in cross-dataset evaluation.\nWe attribute the limited generalization to two main factors, the method and the\ncurrent sources of data. Regarding the method, we illustrate that biasness\npresent in the design choices (e.g anchor settings) of current pedestrian\ndetectors are the main contributing factor to the limited generalization. Most\nmodern pedestrian detectors are tailored towards target dataset, where they do\nachieve high performance in traditional single training and testing pipeline,\nbut suffer a degrade in performance when evaluated through cross-dataset\nevaluation. Consequently, a general object detector performs better in\ncross-dataset evaluation compared with state of the art pedestrian detectors,\ndue to its generic design. As for the data, we show that the autonomous driving\nbenchmarks are monotonous in nature, that is, they are not diverse in scenarios\nand dense in pedestrians. Therefore, benchmarks curated by crawling the web\n(which contain diverse and dense scenarios), are an efficient source of\npre-training for providing a more robust representation. Accordingly, we\npropose a progressive fine-tuning strategy which improves generalization. Code\nand models cab accessed at https://github.com/hasanirtiza/Pedestron.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_I/0/1/0/all/0/1\">Irtiza Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akram_S/0/1/0/all/0/1\">Saad Ullah Akram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swin transformers make strong contextual encoders for VHR image road extraction. (arXiv:2201.03178v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03178","description":"<p>Significant progress has been made in automatic road extra-ction or\nsegmentation based on deep learning, but there are still margins to improve in\nterms of the completeness and connectivity of the results. This is mainly due\nto the challenges of large intra-class variances, ambiguous inter-class\ndistinctions, and occlusions from shadows, trees, and buildings. Therefore,\nbeing able to perceive global context and model geometric information is\nessential to further improve the accuracy of road segmentation. In this paper,\nwe design a novel dual-branch encoding block CoSwin which exploits the\ncapability of global context modeling of Swin Transformer and that of local\nfeature extraction of ResNet. Furthermore, we also propose a context-guided\nfilter block named CFilter, which can filter out context-independent noisy\nfeatures for better reconstructing of the details. We use CoSwin and CFilter in\na U-shaped network architecture. Experiments on Massachusetts and CHN6-CUG\ndatasets show that the proposed method outperforms other state-of-the-art\nmethods on the metrics of F1, IoU, and OA. Further analysis reveals that the\nimprovement in accuracy comes from better integrity and connectivity of\nsegmented roads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daguang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Scene Text Recognition in Indian Languages. (arXiv:2201.03180v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03180","description":"<p>Scene text recognition in low-resource Indian languages is challenging\nbecause of complexities like multiple scripts, fonts, text size, and\norientations. In this work, we investigate the power of transfer learning for\nall the layers of deep scene text recognition networks from English to two\ncommon Indian languages. We perform experiments on the conventional CRNN model\nand STAR-Net to ensure generalisability. To study the effect of change in\ndifferent scripts, we initially run our experiments on synthetic word images\nrendered using Unicode fonts. We show that the transfer of English models to\nsimple synthetic datasets of Indian languages is not practical. Instead, we\npropose to apply transfer learning techniques among Indian languages due to\nsimilarity in their n-gram distributions and visual features like the vowels\nand conjunct characters. We then study the transfer learning among six Indian\nlanguages with varying complexities in fonts and word length statistics. We\nalso demonstrate that the learned features of the models transferred from other\nIndian languages are visually closer (and sometimes even better) to the\nindividual model features than those transferred from English. We finally set\nnew benchmarks for scene-text recognition on Hindi, Telugu, and Malayalam\ndatasets from IIIT-ILST and Bangla dataset from MLT-17 by achieving 6%, 5%, 2%,\nand 23% gains in Word Recognition Rates (WRRs) compared to previous works. We\nfurther improve the MLT-17 Bangla results by plugging in a novel correction\nBiLSTM into our model. We additionally release a dataset of around 440 scene\nimages containing 500 Gujarati and 2535 Tamil words. WRRs improve over the\nbaselines by 8%, 4%, 5%, and 3% on the MLT-19 Hindi and Bangla datasets and the\nGujarati and Tamil datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunna_S/0/1/0/all/0/1\">Sanjana Gunna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saluja_R/0/1/0/all/0/1\">Rohit Saluja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C. V. Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Boosting the Accuracy of Non-Latin Scene Text Recognition. (arXiv:2201.03185v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03185","description":"<p>Scene-text recognition is remarkably better in Latin languages than the\nnon-Latin languages due to several factors like multiple fonts, simplistic\nvocabulary statistics, updated data generation tools, and writing systems. This\npaper examines the possible reasons for low accuracy by comparing English\ndatasets with non-Latin languages. We compare various features like the size\n(width and height) of the word images and word length statistics. Over the last\ndecade, generating synthetic datasets with powerful deep learning techniques\nhas tremendously improved scene-text recognition. Several controlled\nexperiments are performed on English, by varying the number of (i) fonts to\ncreate the synthetic data and (ii) created word images. We discover that these\nfactors are critical for the scene-text recognition systems. The English\nsynthetic datasets utilize over 1400 fonts while Arabic and other non-Latin\ndatasets utilize less than 100 fonts for data generation. Since some of these\nlanguages are a part of different regions, we garner additional fonts through a\nregion-based search to improve the scene-text recognition models in Arabic and\nDevanagari. We improve the Word Recognition Rates (WRRs) on Arabic MLT-17 and\nMLT-19 datasets by 24.54% and 2.32% compared to previous works or baselines. We\nachieve WRR gains of 7.88% and 3.72% for IIIT-ILST and MLT-19 Devanagari\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunna_S/0/1/0/all/0/1\">Sanjana Gunna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saluja_R/0/1/0/all/0/1\">Rohit Saluja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C. V. Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MyoPS: A Benchmark of Myocardial Pathology Segmentation Combining Three-Sequence Cardiac Magnetic Resonance Images. (arXiv:2201.03186v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03186","description":"<p>Assessment of myocardial viability is essential in diagnosis and treatment\nmanagement of patients suffering from myocardial infarction, and classification\nof pathology on myocardium is the key to this assessment. This work defines a\nnew task of medical image analysis, i.e., to perform myocardial pathology\nsegmentation (MyoPS) combining three-sequence cardiac magnetic resonance (CMR)\nimages, which was first proposed in the MyoPS challenge, in conjunction with\nMICCAI 2020. The challenge provided 45 paired and pre-aligned CMR images,\nallowing algorithms to combine the complementary information from the three CMR\nsequences for pathology segmentation. In this article, we provide details of\nthe challenge, survey the works from fifteen participants and interpret their\nmethods according to five aspects, i.e., preprocessing, data augmentation,\nlearning strategy, model architecture and post-processing. In addition, we\nanalyze the results with respect to different factors, in order to examine the\nkey obstacles and explore potential of solutions, as well as to provide a\nbenchmark for future research. We conclude that while promising results have\nbeen reported, the research is still in the early stage, and more in-depth\nexploration is needed before a successful application to the clinics. Note that\nMyoPS data and evaluation tool continue to be publicly available upon\nregistration via its homepage\n(www.sdspeople.fudan.edu.cn/zhuangxiahai/0/myops20/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1\">Fuping Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Sihan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xinzhe Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martin_Isla_C/0/1/0/all/0/1\">Carlos Martin-Isla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_S/0/1/0/all/0/1\">Shuwei Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianpeng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu7_Y/0/1/0/all/0/1\">Yanfei Liu7</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ankenbrand_M/0/1/0/all/0/1\">Markus J. Ankenbrand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1\">Haochuan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoran Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Linhong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arega_T/0/1/0/all/0/1\">Tewodros Weldebirhan Arega</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Altunok_E/0/1/0/all/0/1\">Elif Altunok</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Feiyan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoping Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puybareau_E/0/1/0/all/0/1\">Elodie Puybareau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oksuz_I/0/1/0/all/0/1\">Ilkay Oksuz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bricq_S/0/1/0/all/0/1\">Stephanie Bricq</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Weisheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Punithakumar_K/0/1/0/all/0/1\">Kumaradevan Punithakumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schreiber_L/0/1/0/all/0/1\">Laura M. Schreiber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mingjing Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1\">Guocai Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification. (arXiv:2201.03194v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03194","description":"<p>Hierarchical multi-granularity classification (HMC) assigns hierarchical\nmulti-granularity labels to each object and focuses on encoding the label\nhierarchy, e.g., [\"Albatross\", \"Laysan Albatross\"] from coarse-to-fine levels.\nHowever, the definition of what is fine-grained is subjective, and the image\nquality may affect the identification. Thus, samples could be observed at any\nlevel of the hierarchy, e.g., [\"Albatross\"] or [\"Albatross\", \"Laysan\nAlbatross\"], and examples discerned at coarse categories are often neglected in\nthe conventional setting of HMC. In this paper, we study the HMC problem in\nwhich objects are labeled at any level of the hierarchy. The essential designs\nof the proposed method are derived from two motivations: (1) learning with\nobjects labeled at various levels should transfer hierarchical knowledge\nbetween levels; (2) lower-level classes should inherit attributes related to\nupper-level superclasses. The proposed combinatorial loss maximizes the\nmarginal probability of the observed ground truth label by aggregating\ninformation from related labels defined in the tree hierarchy. If the observed\nlabel is at the leaf level, the combinatorial loss further imposes the\nmulti-class cross-entropy loss to increase the weight of fine-grained\nclassification loss. Considering the hierarchical feature interaction, we\npropose a hierarchical residual network (HRN), in which granularity-specific\nfeatures from parent levels acting as residual connections are added to\nfeatures of children levels. Experiments on three commonly used datasets\ndemonstrate the effectiveness of our approach compared to the state-of-the-art\nHMC approaches and fine-grained visual classification (FGVC) methods exploiting\nthe label hierarchy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingzhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuntao Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end lossless compression of high precision depth maps guided by pseudo-residual. (arXiv:2201.03195v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03195","description":"<p>As a fundamental data format representing spatial information, depth map is\nwidely used in signal processing and computer vision fields. Massive amount of\nhigh precision depth maps are produced with the rapid development of equipment\nlike laser scanner or LiDAR. Therefore, it is urgent to explore a new\ncompression method with better compression ratio for high precision depth maps.\nUtilizing the wide spread deep learning environment, we propose an end-to-end\nlearning-based lossless compression method for high precision depth maps. The\nwhole process is comprised of two sub-processes, named pre-processing of depth\nmaps and deep lossless compression of processed depth maps. The deep lossless\ncompression network consists of two sub-networks, named lossy compression\nnetwork and lossless compression network. We leverage the concept of\npseudo-residual to guide the generation of distribution for residual and avoid\nintroducing context models. Our end-to-end lossless compression network\nachieves competitive performance over engineered codecs and has low\ncomputational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yuyang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Based Image Signal Processors via Learnable Dictionaries. (arXiv:2201.03210v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03210","description":"<p>Digital cameras transform sensor RAW readings into RGB images by means of\ntheir Image Signal Processor (ISP). Computational photography tasks such as\nimage denoising and colour constancy are commonly performed in the RAW domain,\nin part due to the inherent hardware design, but also due to the appealing\nsimplicity of noise statistics that result from the direct sensor readings.\nDespite this, the availability of RAW images is limited in comparison with the\nabundance and diversity of available RGB data. Recent approaches have attempted\nto bridge this gap by estimating the RGB to RAW mapping: handcrafted\nmodel-based methods that are interpretable and controllable usually require\nmanual parameter fine-tuning, while end-to-end learnable neural networks\nrequire large amounts of training data, at times with complex training\nprocedures, and generally lack interpretability and parametric control. Towards\naddressing these existing limitations, we present a novel hybrid model-based\nand data-driven ISP that builds on canonical ISP operations and is both\nlearnable and interpretable. Our proposed invertible model, capable of\nbidirectional mapping between RAW and RGB domains, employs end-to-end learning\nof rich parameter representations, i.e. dictionaries, that are free from direct\nparametric supervision and additionally enable simple and plausible data\naugmentation. We evidence the value of our data generation process by extensive\nexperiments under both RAW image reconstruction and RAW image denoising tasks,\nobtaining state-of-the-art performance in both. Additionally, we show that our\nISP can learn meaningful mappings from few data samples, and that denoising\nmodels trained with our dictionary-based data augmentation are competitive\ndespite having only few or zero ground-truth labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maggioni_M/0/1/0/all/0/1\">Matteo Maggioni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perez_Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez-Pellitero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why-So-Deep: Towards Boosting Previously Trained Models for Visual Place Recognition. (arXiv:2201.03212v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03212","description":"<p>Deep learning-based image retrieval techniques for the loop closure detection\ndemonstrate satisfactory performance. However, it is still challenging to\nachieve high-level performance based on previously trained models in different\ngeographical regions. This paper addresses the problem of their deployment with\nsimultaneous localization and mapping (SLAM) systems in the new environment.\nThe general baseline approach uses additional information, such as GPS,\nsequential keyframes tracking, and re-training the whole environment to enhance\nthe recall rate. We propose a novel approach for improving image retrieval\nbased on previously trained models. We present an intelligent method, MAQBOOL,\nto amplify the power of pre-trained models for better image recall and its\napplication to real-time multiagent SLAM systems. We achieve comparable image\nretrieval results at a low descriptor dimension (512-D), compared to the high\ndescriptor dimension (4096-D) of state-of-the-art methods. We use spatial\ninformation to improve the recall rate in image retrieval on pre-trained\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhutta_M/0/1/0/all/0/1\">M. Usman Maqbool Bhutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_D/0/1/0/all/0/1\">Darwin Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully automatic scoring of handwritten descriptive answers in Japanese language tests. (arXiv:2201.03215v1 [cs.LG])","link":"http://arxiv.org/abs/2201.03215","description":"<p>This paper presents an experiment of automatically scoring handwritten\ndescriptive answers in the trial tests for the new Japanese university entrance\nexamination, which were made for about 120,000 examinees in 2017 and 2018.\nThere are about 400,000 answers with more than 20 million characters. Although\nall answers have been scored by human examiners, handwritten characters are not\nlabelled. We present our attempt to adapt deep neural network-based handwriting\nrecognizers trained on a labelled handwriting dataset into this unlabeled\nanswer set. Our proposed method combines different training strategies,\nensembles multiple recognizers, and uses a language model built from a large\ngeneral corpus to avoid overfitting into specific data. In our experiment, the\nproposed method records character accuracy of over 97% using about 2,000\nverified labelled answers that account for less than 0.5% of the dataset. Then,\nthe recognized answers are fed into a pre-trained automatic scoring system\nbased on the BERT model without correcting misrecognized characters and\nproviding rubric annotations. The automatic scoring system achieves from 0.84\nto 0.98 of Quadratic Weighted Kappa (QWK). As QWK is over 0.8, it represents\nacceptable similarity of scoring between the automatic scoring system and the\nhuman examiners. These results are promising for further research on end-to-end\nautomatic scoring of descriptive answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oka_H/0/1/0/all/0/1\">Haruki Oka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishioka_T/0/1/0/all/0/1\">Tsunenori Ishioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swin Transformer for Fast MRI. (arXiv:2201.03230v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03230","description":"<p>Magnetic resonance imaging (MRI) is an important non-invasive clinical tool\nthat can produce high-resolution and reproducible images. However, a long\nscanning time is required for high-quality MR images, which leads to exhaustion\nand discomfort of patients, inducing more artefacts due to voluntary movements\nof the patients and involuntary physiological movements. To accelerate the\nscanning process, methods by k-space undersampling and deep learning based\nreconstruction have been popularised. This work introduced SwinMR, a novel Swin\ntransformer based method for fast MRI reconstruction. The whole network\nconsisted of an input module (IM), a feature extraction module (FEM) and an\noutput module (OM). The IM and OM were 2D convolutional layers and the FEM was\ncomposed of a cascaded of residual Swin transformer blocks (RSTBs) and 2D\nconvolutional layers. The RSTB consisted of a series of Swin transformer layers\n(STLs). The shifted windows multi-head self-attention (W-MSA/SW-MSA) of STL was\nperformed in shifted windows rather than the multi-head self-attention (MSA) of\nthe original transformer in the whole image space. A novel multi-channel loss\nwas proposed by using the sensitivity maps, which was proved to reserve more\ntextures and details. We performed a series of comparative studies and ablation\nstudies in the Calgary-Campinas public brain MR dataset and conducted a\ndownstream segmentation experiment in the Multi-modal Brain Tumour Segmentation\nChallenge 2017 dataset. The results demonstrate our SwinMR achieved\nhigh-quality reconstruction compared with other benchmark methods, and it shows\ngreat robustness with different undersampling masks, under noise interruption\nand on different datasets. The code is publicly available at\nhttps://github.com/ayanglab/SwinMR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yingying Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huanjun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small Object Detection using Deep Learning. (arXiv:2201.03243v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03243","description":"<p>Now a days, UAVs such as drones are greatly used for various purposes like\nthat of capturing and target detection from ariel imagery etc. Easy access of\nthese small ariel vehicles to public can cause serious security threats. For\ninstance, critical places may be monitored by spies blended in public using\ndrones. Study in hand proposes an improved and efficient Deep Learning based\nautonomous system which can detect and track very small drones with great\nprecision. The proposed system consists of a custom deep learning model Tiny\nYOLOv3, one of the flavors of very fast object detection model You Look Only\nOnce (YOLO) is built and used for detection. The object detection algorithm\nwill efficiently the detect the drones. The proposed architecture has shown\nsignificantly better performance as compared to the previous YOLO version. The\nimprovement is observed in the terms of resource usage and time complexity. The\nperformance is measured using the metrics of recall and precision that are 93%\nand 91% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ajaz_A/0/1/0/all/0/1\">Aleena Ajaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salar_A/0/1/0/all/0/1\">Ayesha Salar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamal_T/0/1/0/all/0/1\">Tauseef Jamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asif Ullah Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision in adverse weather: Augmentation using CycleGANs with various object detectors for robust perception in autonomous racing. (arXiv:2201.03246v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03246","description":"<p>In an autonomous driving system, perception - identification of features and\nobjects from the environment - is crucial. In autonomous racing, high speeds\nand small margins demand rapid and accurate detection systems. During the race,\nthe weather can change abruptly, causing significant degradation in perception,\nresulting in ineffective manoeuvres. In order to improve detection in adverse\nweather, deep-learning-based models typically require extensive datasets\ncaptured in such conditions - the collection of which is a tedious, laborious,\nand costly process. However, recent developments in CycleGAN architectures\nallow the synthesis of highly realistic scenes in multiple weather conditions.\nTo this end, we introduce an approach of using synthesised adverse condition\ndatasets in autonomous racing (generated using CycleGAN) to improve the\nperformance of four out of five state-of-the-art detectors by an average of\n42.7 and 4.4 mAP percentage points in the presence of night-time conditions and\ndroplets, respectively. Furthermore, we present a comparative analysis of five\nobject detectors - identifying the optimal pairing of detector and training\ndata for use during autonomous racing in challenging conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1\">Izzeddin Teeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_V/0/1/0/all/0/1\">Valentina Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rast_A/0/1/0/all/0/1\">Alexander Rast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1\">Fabio Cuzzolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1\">Andrew Bradley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A statistical shape model for radiation-free assessment and classification of craniosynostosis. (arXiv:2201.03288v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03288","description":"<p>The assessment of craniofacial deformities requires patient data which is\nsparsely available. Statistical shape models provide realistic and synthetic\ndata enabling comparisons of existing methods on a common dataset.\n</p>\n<p>We build the first publicly available statistical 3D head model of\ncraniosynostosis patients and the first model focusing on infants younger than\n1.5 years. For correspondence establishment, we test and evaluate four template\nmorphing approaches. We further present an original, shape-model-based\nclassification approach for craniosynostosis on photogrammetric surface scans.\nTo the best of our knowledge, our study uses the largest dataset of\ncraniosynostosis patients in a classification study for craniosynostosis and\nstatistical shape modeling to date.\n</p>\n<p>We demonstrate that our shape model performs similar to other statistical\nshape models of the human head. Craniosynostosis-specific pathologies are\nrepresented in the first eigenmodes of the model. Regarding the automatic\nclassification of craniosynostis, our classification approach yields an\naccuracy of 97.3%, comparable to other state-of-the-art methods using both\ncomputed tomography scans and stereophotogrammetry.\n</p>\n<p>Our publicly available, craniosynostosis-specific statistical shape model\nenables the assessment of craniosynostosis on realistic and synthetic data. We\nfurther present a state-of-the-art shape-model-based classification approach\nfor a radiation-free diagnosis of craniosynostosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schaufelberger_M/0/1/0/all/0/1\">Matthias Schaufelberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuhle_R/0/1/0/all/0/1\">Reinald Peter K&#xfc;hle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wachter_A/0/1/0/all/0/1\">Andreas Wachter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weichel_F/0/1/0/all/0/1\">Frederic Weichel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hagen_N/0/1/0/all/0/1\">Niclas Hagen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ringwald_F/0/1/0/all/0/1\">Friedemann Ringwald</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eisenmann_U/0/1/0/all/0/1\">Urs Eisenmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_J/0/1/0/all/0/1\">J&#xfc;rgen Hoffmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Engel_M/0/1/0/all/0/1\">Michael Engel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Freudlsperger_C/0/1/0/all/0/1\">Christian Freudlsperger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahm_W/0/1/0/all/0/1\">Werner Nahm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GhostNets on Heterogeneous Devices via Cheap Operations. (arXiv:2201.03297v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03297","description":"<p>Deploying convolutional neural networks (CNNs) on mobile devices is difficult\ndue to the limited memory and computation resources. We aim to design efficient\nneural networks for heterogeneous devices including CPU and GPU, by exploiting\nthe redundancy in feature maps, which has rarely been investigated in neural\narchitecture design. For CPU-like devices, we propose a novel CPU-efficient\nGhost (C-Ghost) module to generate more feature maps from cheap operations.\nBased on a set of intrinsic feature maps, we apply a series of linear\ntransformations with cheap cost to generate many ghost feature maps that could\nfully reveal information underlying intrinsic features. The proposed C-Ghost\nmodule can be taken as a plug-and-play component to upgrade existing\nconvolutional neural networks. C-Ghost bottlenecks are designed to stack\nC-Ghost modules, and then the lightweight C-GhostNet can be easily established.\nWe further consider the efficient networks for GPU devices. Without involving\ntoo many GPU-inefficient operations (e.g.,, depth-wise convolution) in a\nbuilding stage, we propose to utilize the stage-wise feature redundancy to\nformulate GPU-efficient Ghost (G-Ghost) stage structure. The features in a\nstage are split into two parts where the first part is processed using the\noriginal block with fewer output channels for generating intrinsic features,\nand the other are generated using cheap operations by exploiting stage-wise\nredundancy. Experiments conducted on benchmarks demonstrate the effectiveness\nof the proposed C-Ghost module and the G-Ghost stage. C-GhostNet and G-GhostNet\ncan achieve the optimal trade-off of accuracy and latency for CPU and GPU,\nrespectively. Code is available at https://github.com/huawei-noah/CV-Backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Avoiding Overfitting: A Survey on Regularization Methods for Convolutional Neural Networks. (arXiv:2201.03299v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03299","description":"<p>Several image processing tasks, such as image classification and object\ndetection, have been significantly improved using Convolutional Neural Networks\n(CNN). Like ResNet and EfficientNet, many architectures have achieved\noutstanding results in at least one dataset by the time of their creation. A\ncritical factor in training concerns the network's regularization, which\nprevents the structure from overfitting. This work analyzes several\nregularization methods developed in the last few years, showing significant\nimprovements for different CNN models. The works are classified into three main\nareas: the first one is called \"data augmentation\", where all the techniques\nfocus on performing changes in the input data. The second, named \"internal\nchanges\", which aims to describe procedures to modify the feature maps\ngenerated by the neural network or the kernels. The last one, called \"label\",\nconcerns transforming the labels of a given input. This work presents two main\ndifferences comparing to other available surveys about regularization: (i) the\nfirst concerns the papers gathered in the manuscript, which are not older than\nfive years, and (ii) the second distinction is about reproducibility, i.e., all\nworks refered here have their code available in public repositories or they\nhave been directly implemented in some framework, such as TensorFlow or Torch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Claudio Filipi Gon&#xe7;alves dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Representation Learning Techniques for Tracking in time resolved 3D Ultrasound. (arXiv:2201.03319v1 [eess.IV])","link":"http://arxiv.org/abs/2201.03319","description":"<p>3D ultrasound (3DUS) becomes more interesting for target tracking in\nradiation therapy due to its capability to provide volumetric images in\nreal-time without using ionizing radiation. It is potentially usable for\ntracking without using fiducials. For this, a method for learning meaningful\nrepresentations would be useful to recognize anatomical structures in different\ntime frames in representation space (r-space). In this study, 3DUS patches are\nreduced into a 128-dimensional r-space using conventional autoencoder,\nvariational autoencoder and sliced-wasserstein autoencoder. In the r-space, the\ncapability of separating different ultrasound patches as well as recognizing\nsimilar patches is investigated and compared based on a dataset of liver\nimages. Two metrics to evaluate the tracking capability in the r-space are\nproposed. It is shown that ultrasound patches with different anatomical\nstructures can be distinguished and sets of similar patches can be clustered in\nr-space. The results indicate that the investigated autoencoders have different\nlevels of usability for target tracking in 3DUS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wulff_D/0/1/0/all/0/1\">Daniel Wulff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hagenah_J/0/1/0/all/0/1\">Jannis Hagenah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ernst_F/0/1/0/all/0/1\">Floris Ernst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gait Recognition Based on Deep Learning: A Survey. (arXiv:2201.03323v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03323","description":"<p>In general, biometry-based control systems may not rely on individual\nexpected behavior or cooperation to operate appropriately. Instead, such\nsystems should be aware of malicious procedures for unauthorized access\nattempts. Some works available in the literature suggest addressing the problem\nthrough gait recognition approaches. Such methods aim at identifying human\nbeings through intrinsic perceptible features, despite dressed clothes or\naccessories. Although the issue denotes a relatively long-time challenge, most\nof the techniques developed to handle the problem present several drawbacks\nrelated to feature extraction and low classification rates, among other issues.\nHowever, deep learning-based approaches recently emerged as a robust set of\ntools to deal with virtually any image and computer-vision related problem,\nproviding paramount results for gait recognition as well. Therefore, this work\nprovides a surveyed compilation of recent works regarding biometric detection\nthrough gait recognition with a focus on deep learning approaches, emphasizing\ntheir benefits, and exposing their weaknesses. Besides, it also presents\ncategorized and characterized descriptions of the datasets, approaches, and\narchitectures employed to tackle associated constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_C/0/1/0/all/0/1\">Claudio Filipi Gon&#xe7;alves dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Diego de Souza Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_L/0/1/0/all/0/1\">Leandro A. Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pires_R/0/1/0/all/0/1\">Rafael Gon&#xe7;alves Pires</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_D/0/1/0/all/0/1\">Daniel Felipe Silva Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valem_L/0/1/0/all/0/1\">Lucas Pascotti Valem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_T/0/1/0/all/0/1\">Thierry P. Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santana_M/0/1/0/all/0/1\">Marcos Cleison S. Santana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roder_M/0/1/0/all/0/1\">Mateus Roder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_D/0/1/0/all/0/1\">Danilo Colombo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COIN: Counterfactual Image Generation for VQA Interpretation. (arXiv:2201.03342v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03342","description":"<p>Due to the significant advancement of Natural Language Processing and\nComputer Vision-based models, Visual Question Answering (VQA) systems are\nbecoming more intelligent and advanced. However, they are still error-prone\nwhen dealing with relatively complex questions. Therefore, it is important to\nunderstand the behaviour of the VQA models before adopting their results. In\nthis paper, we introduce an interpretability approach for VQA models by\ngenerating counterfactual images. Specifically, the generated image is supposed\nto have the minimal possible change to the original image and leads the VQA\nmodel to give a different answer. In addition, our approach ensures that the\ngenerated image is realistic. Since quantitative metrics cannot be employed to\nevaluate the interpretability of the model, we carried out a user study to\nassess different aspects of our approach. In addition to interpreting the\nresult of VQA models on single images, the obtained results and the discussion\nprovides an extensive explanation of VQA models' behaviour.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_T/0/1/0/all/0/1\">Timo Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurjens_J/0/1/0/all/0/1\">Jan J&#xfc;rjens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMFIM: A Generative Mask-guided Facial Image Manipulation Model for Privacy Preservation. (arXiv:2201.03353v1 [cs.CV])","link":"http://arxiv.org/abs/2201.03353","description":"<p>The use of social media websites and applications has become very popular and\npeople share their photos on these networks. Automatic recognition and tagging\nof people's photos on these networks has raised privacy preservation issues and\nusers seek methods for hiding their identities from these algorithms.\nGenerative adversarial networks (GANs) are shown to be very powerful in\ngenerating face images in high diversity and also in editing face images. In\nthis paper, we propose a Generative Mask-guided Face Image Manipulation (GMFIM)\nmodel based on GANs to apply imperceptible editing to the input face image to\npreserve the privacy of the person in the image. Our model consists of three\nmain components: a) the face mask module to cut the face area out of the input\nimage and omit the background, b) the GAN-based optimization module for\nmanipulating the face image and hiding the identity and, c) the merge module\nfor combining the background of the input image and the manipulated\nde-identified face image. Different criteria are considered in the loss\nfunction of the optimization step to produce high-quality images that are as\nsimilar as possible to the input image while they cannot be recognized by AFR\nsystems. The results of the experiments on different datasets show that our\nmodel can achieve better performance against automated face recognition systems\nin comparison to the state-of-the-art methods and it catches a higher attack\nsuccess rate in most experiments from a total of 18. Moreover, the generated\nimages of our proposed model have the highest quality and are more pleasing to\nhuman eyes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khojaste_M/0/1/0/all/0/1\">Mohammad Hossein Khojaste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farid_N/0/1/0/all/0/1\">Nastaran Moradzadeh Farid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-resolution Ecosystem Mapping in Repetitive Environments Using Dual Camera SLAM. (arXiv:2201.03364v1 [cs.RO])","link":"http://arxiv.org/abs/2201.03364","description":"<p>Structure from Motion (SfM) techniques are being increasingly used to create\n3D maps from images in many domains including environmental monitoring.\nHowever, SfM techniques are often confounded in visually repetitive\nenvironments as they rely primarily on globally distinct image features.\nSimultaneous Localization and Mapping (SLAM) techniques offer a potential\nsolution in visually repetitive environments since they use local feature\nmatching, but SLAM approaches work best with wide-angle cameras that are often\nunsuitable for documenting the environmental system of interest. We resolve\nthis issue by proposing a dual-camera SLAM approach that uses a forward facing\nwide-angle camera for localization and a downward facing narrower angle,\nhigh-resolution camera for documentation. Video frames acquired by the forward\nfacing camera video are processed using a standard SLAM approach providing a\ntrajectory of the imaging system through the environment which is then used to\nguide the registration of the documentation camera images. Fragmentary maps,\ninitially produced from the documentation camera images via monocular SLAM, are\nsubsequently scaled and aligned with the localization camera trajectory and\nfinally subjected to a global optimization procedure to produce a unified,\nrefined map. An experimental comparison with several state-of-the-art SfM\napproaches shows the dual-camera SLAM approach to perform better in repetitive\nenvironmental systems based on select samples of ground control point markers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hopkinson_B/0/1/0/all/0/1\">Brian M. Hopkinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhandarkar_S/0/1/0/all/0/1\">Suchendra M. Bhandarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Deformations of Point Configurations Viewed By a Pinhole Model Camera. (arXiv:1505.08070v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1505.08070","description":"<p>This paper is a theoretical study of the following Non-Rigid Structure from\nMotion problem. What can be computed from a monocular view of a parametrically\ndeforming set of points? We treat various variations of this problem for affine\nand polynomial deformations with calibrated and uncalibrated cameras. We show\nthat in general at least three images with quasi-identical two deformations are\nneeded in order to have a finite set of solutions of the points' structure and\ncalculate some simple examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaminski_Y/0/1/0/all/0/1\">Yirmeyahu Kaminski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1\">Michael Werman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeoNav: Improving the Generalization of Visual Navigation via Generating Next Expected Observations. (arXiv:1906.07207v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1906.07207","description":"<p>We propose improving the cross-target and cross-scene generalization of\nvisual navigation through learning an agent that is guided by conceiving the\nnext observations it expects to see. This is achieved by learning a variational\nBayesian model, called NeoNav, which generates the next expected observations\n(NEO) conditioned on the current observations of the agent and the target view.\nOur generative model is learned through optimizing a variational objective\nencompassing two key designs. First, the latent distribution is conditioned on\ncurrent observations and the target view, leading to a model-based,\ntarget-driven navigation. Second, the latent space is modeled with a Mixture of\nGaussians conditioned on the current observation and the next best action. Our\nuse of mixture-of-posteriors prior effectively alleviates the issue of\nover-regularized latent space, thus significantly boosting the model\ngeneralization for new targets and in novel scenes. Moreover, the NEO\ngeneration models the forward dynamics of agent-environment interaction, which\nimproves the quality of approximate inference and hence benefits data\nefficiency. We have conducted extensive evaluations on both real-world and\nsynthetic benchmarks, and show that our model consistently outperforms the\nstate-of-the-art models in terms of success rate, data efficiency, and\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiaoyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning-based Architectures for Semantic Segmentation on 2D images. (arXiv:1912.10230v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.10230","description":"<p>Semantic segmentation is the pixel-wise labelling of an image. Since the\nproblem is defined at the pixel level, determining image class labels only is\nnot acceptable, but localising them at the original image pixel resolution is\nnecessary. Boosted by the extraordinary ability of convolutional neural\nnetworks (CNN) in creating semantic, high level and hierarchical image\nfeatures; several deep learning-based 2D semantic segmentation approaches have\nbeen proposed within the last decade. In this survey, we mainly focus on the\nrecent scientific developments in semantic segmentation, specifically on deep\nlearning-based methods using 2D images. We started with an analysis of the\npublic image sets and leaderboards for 2D semantic segmentation, with an\noverview of the techniques employed in performance evaluation. In examining the\nevolution of the field, we chronologically categorised the approaches into\nthree main periods, namely pre-and early deep learning era, the fully\nconvolutional era, and the post-FCN era. We technically analysed the solutions\nput forward in terms of solving the fundamental problems of the field, such as\nfine-grained localisation and scale invariance. Before drawing our conclusions,\nwe present a table of methods from all mentioned eras, with a summary of each\napproach that explains their contribution to the field. We conclude the survey\nby discussing the current challenges of the field and to what extent they have\nbeen solved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulku_I/0/1/0/all/0/1\">Irem Ulku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akagunduz_E/0/1/0/all/0/1\">Erdem Akagunduz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"1D Probabilistic Undersampling Pattern Optimization for MR Image Reconstruction. (arXiv:2003.03797v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2003.03797","description":"<p>Magnetic resonance imaging (MRI) is mainly limited by long scanning time and\nvulnerable to human tissue motion artifacts, in 3D clinical scenarios. Thus,\nk-space undersampling is used to accelerate the acquisition of MRI while\nleading to visually poor MR images. Recently, some studies 1) use effective\nundersampling patterns, or 2) design deep neural networks to improve the\nquality of resulting images. However, they are considered as two separate\noptimization strategies. In this paper, we propose a cross-domain network for\nMR image reconstruction, in a retrospective data-driven manner, under limited\nsampling rates. Our method can simultaneously obtain the optimal undersampling\npattern (in k-space) and the reconstruction model, which are customized to the\ntype of training data, by using an end-to-end learning strategy. We propose a\n1D probabilistic undersampling layer, to obtain the optimal undersampling\npattern and its probability distribution in a differentiable way. We propose a\n1D inverse Fourier transform layer, which connects the Fourier domain and the\nimage domain during the forward pass and the backpropagation. In addition, by\ntraining 3D fully-sampled k-space data and MR images with the traditional\nEuclidean loss, we discover the universal relationship between the probability\ndistribution of the optimal undersampling pattern and its corresponding\nsampling rate. Experiments show that the quantitative and qualitative results\nof recovered MR images by our 1D probabilistic undersampling pattern obviously\noutperform those of several existing sampling strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xue_S/0/1/0/all/0/1\">Shengke Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_R/0/1/0/all/0/1\">Ruiliang Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_X/0/1/0/all/0/1\">Xinyu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching CNNs to mimic Human Visual Cognitive Process & regularise Texture-Shape bias. (arXiv:2006.14722v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.14722","description":"<p>Recent experiments in computer vision demonstrate texture bias as the primary\nreason for supreme results in models employing Convolutional Neural Networks\n(CNNs), conflicting with early works claiming that these networks identify\nobjects using shape. It is believed that the cost function forces the CNN to\ntake a greedy approach and develop a proclivity for local information like\ntexture to increase accuracy, thus failing to explore any global statistics. We\npropose CognitiveCNN, a new intuitive architecture, inspired from feature\nintegration theory in psychology to utilise human interpretable feature like\nshape, texture, edges etc. to reconstruct, and classify the image. We define\nnovel metrics to quantify the \"relevance\" of \"abstract information\" present in\nthese modalities using attention maps. We further introduce a regularisation\nmethod which ensures that each modality like shape, texture etc. gets\nproportionate influence in a given task, as it does for reconstruction; and\nperform experiments to show the resulting boost in accuracy and robustness,\nbesides imparting explainability to these CNNs for achieving superior\nperformance in object recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohla_S/0/1/0/all/0/1\">Satyam Mohla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasery_A/0/1/0/all/0/1\">Anshul Nasery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative study of deep learning methods for the automatic segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients. (arXiv:2007.15546v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.15546","description":"<p>Recent research on COVID-19 suggests that CT imaging provides useful\ninformation to assess disease progression and assist diagnosis, in addition to\nhelp understanding the disease. There is an increasing number of studies that\npropose to use deep learning to provide fast and accurate quantification of\nCOVID-19 using chest CT scans. The main tasks of interest are the automatic\nsegmentation of lung and lung lesions in chest CT scans of confirmed or\nsuspected COVID-19 patients. In this study, we compare twelve deep learning\nalgorithms using a multi-center dataset, including both open-source and\nin-house developed algorithms. Results show that ensembling different methods\ncan boost the overall test set performance for lung segmentation, binary lesion\nsegmentation and multiclass lesion segmentation, resulting in mean Dice scores\nof 0.982, 0.724 and 0.469, respectively. The resulting binary lesions were\nsegmented with a mean absolute volume error of 91.3 ml. In general, the task of\ndistinguishing different lesion types was more difficult, with a mean absolute\nvolume difference of 152 ml and mean Dice scores of 0.369 and 0.523 for\nconsolidation and ground glass opacity, respectively. All methods perform\nbinary lesion segmentation with an average volume error that is better than\nvisual assessment by human raters, suggesting these methods are mature enough\nfor a large-scale evaluation for use in clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tilborghs_S/0/1/0/all/0/1\">Sofie Tilborghs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dirks_I/0/1/0/all/0/1\">Ine Dirks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willems_S/0/1/0/all/0/1\">Siri Willems</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eelbode_T/0/1/0/all/0/1\">Tom Eelbode</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertels_J/0/1/0/all/0/1\">Jeroen Bertels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ilsen_B/0/1/0/all/0/1\">Bart Ilsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brys_A/0/1/0/all/0/1\">Arne Brys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubbeldam_A/0/1/0/all/0/1\">Adriana Dubbeldam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buls_N/0/1/0/all/0/1\">Nico Buls</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonidakis_P/0/1/0/all/0/1\">Panagiotis Gonidakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_S/0/1/0/all/0/1\">Sebasti&#xe1;n Amador S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snoeckx_A/0/1/0/all/0/1\">Annemiek Snoeckx</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parizel_P/0/1/0/all/0/1\">Paul M. Parizel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mey_J/0/1/0/all/0/1\">Johan de Mey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandermeulen_D/0/1/0/all/0/1\">Dirk Vandermeulen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robben_D/0/1/0/all/0/1\">David Robben</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smeets_D/0/1/0/all/0/1\">Dirk Smeets</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_F/0/1/0/all/0/1\">Frederik Maes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandemeulebroucke_J/0/1/0/all/0/1\">Jef Vandemeulebroucke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suetens_P/0/1/0/all/0/1\">Paul Suetens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving concave point detection to better segment overlapped objects in images. (arXiv:2008.00997v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.00997","description":"<p>This paper presents a method that improve state-of-the-art of the concave\npoint detection methods as a first step to segment overlapping objects on\nimages. It is based on the analysis of the curvature of the objects contour.\nThe method has three main steps. First, we pre-process the original image to\nobtain the value of the curvature on each contour point. Second, we select\nregions with higher curvature and we apply a recursive algorithm to refine the\nprevious selected regions. Finally, we obtain a concave point from each region\nbased on the analysis of the relative position of their neighbourhood We\nexperimentally demonstrated that a better concave points detection implies a\nbetter cluster division. In order to evaluate the quality of the concave point\ndetection algorithm, we constructed a synthetic dataset to simulate overlapping\nobjects, providing the position of the concave points as a ground truth. As a\ncase study, the performance of a well-known application is evaluated, such as\nthe splitting of overlapped cells in images of peripheral blood smears samples\nof patients with sickle cell anaemia. We used the proposed method to detect the\nconcave points in clusters of cells and then we separate this clusters by\nellipse fitting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1\">Miquel Mir&#xf3;-Nicolau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moya_Alcover_B/0/1/0/all/0/1\">Biel Moy&#xe0;-Alcover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Hidalgo_M/0/1/0/all/0/1\">Manuel Gonz&#xe0;lez-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1\">Antoni Jaume-i-Cap&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images. (arXiv:2010.10563v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.10563","description":"<p>Every year physicians face an increasing demand of image-based diagnosis from\npatients, a problem that can be addressed with recent artificial intelligence\nmethods. In this context, we survey works in the area of automatic report\ngeneration from medical images, with emphasis on methods using deep neural\nnetworks, with respect to: (1) Datasets, (2) Architecture Design, (3)\nExplainability and (4) Evaluation Metrics. Our survey identifies interesting\ndevelopments, but also remaining challenges. Among them, the current evaluation\nof generated reports is especially weak, since it mostly relies on traditional\nNatural Language Processing (NLP) metrics, which do not accurately capture\nmedical correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Messina_P/0/1/0/all/0/1\">Pablo Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_P/0/1/0/all/0/1\">Pablo Pino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parra_D/0/1/0/all/0/1\">Denis Parra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besa_C/0/1/0/all/0/1\">Cecilia Besa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uribe_S/0/1/0/all/0/1\">Sergio Uribe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+andia_M/0/1/0/all/0/1\">Marcelo and&#xed;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejos_C/0/1/0/all/0/1\">Cristian Tejos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prieto_C/0/1/0/all/0/1\">Claudia Prieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capurro_D/0/1/0/all/0/1\">Daniel Capurro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIGPrior: Towards Decoupling Learned Prior Hallucination and Data Fidelity in Image Restoration. (arXiv:2011.01406v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.01406","description":"<p>Classic image-restoration algorithms use a variety of priors, either\nimplicitly or explicitly. Their priors are hand-designed and their\ncorresponding weights are heuristically assigned. Hence, deep learning methods\noften produce superior image restoration quality. Deep networks are, however,\ncapable of inducing strong and hardly predictable hallucinations. Networks\nimplicitly learn to be jointly faithful to the observed data while learning an\nimage prior; and the separation of original data and hallucinated data\ndownstream is then not possible. This limits their wide-spread adoption in\nimage restoration. Furthermore, it is often the hallucinated part that is\nvictim to degradation-model overfitting.\n</p>\n<p>We present an approach with decoupled network-prior based hallucination and\ndata fidelity terms. We refer to our framework as the Bayesian Integration of a\nGenerative Prior (BIGPrior). Our method is rooted in a Bayesian framework and\ntightly connected to classic restoration methods. In fact, it can be viewed as\na generalization of a large family of classic restoration algorithms. We use\nnetwork inversion to extract image prior information from a generative network.\nWe show that, on image colorization, inpainting and denoising, our framework\nconsistently improves the inversion results. Our method, though partly reliant\non the quality of the generative network inversion, is competitive with\nstate-of-the-art supervised and task-specific restoration methods. It also\nprovides an additional metric that sets forth the degree of prior reliance per\npixel relative to data fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helou_M/0/1/0/all/0/1\">Majed El Helou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Three-Stage Self-Training Framework for Semi-Supervised Semantic Segmentation. (arXiv:2012.00827v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.00827","description":"<p>Semantic segmentation has been widely investigated in the community, in which\nthe state of the art techniques are based on supervised models. Those models\nhave reported unprecedented performance at the cost of requiring a large set of\nhigh quality segmentation masks. To obtain such annotations is highly expensive\nand time consuming, in particular, in semantic segmentation where pixel-level\nannotations are required. In this work, we address this problem by proposing a\nholistic solution framed as a three-stage self-training framework for\nsemi-supervised semantic segmentation. The key idea of our technique is the\nextraction of the pseudo-masks statistical information to decrease uncertainty\nin the predicted probability whilst enforcing segmentation consistency in a\nmulti-task fashion. We achieve this through a three-stage solution. Firstly, we\ntrain a segmentation network to produce rough pseudo-masks which predicted\nprobability is highly uncertain. Secondly, we then decrease the uncertainty of\nthe pseudo-masks using a multi-task model that enforces consistency whilst\nexploiting the rich statistical information of the data. We compare our\napproach with existing methods for semi-supervised semantic segmentation and\ndemonstrate its state-of-the-art performance with extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_R/0/1/0/all/0/1\">Rihuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica Aviles-Rivero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Saurabh Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Saikumar Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACE-Net: Fine-Level Face Alignment through Anchors and Contours Estimation. (arXiv:2012.01461v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01461","description":"<p>We propose a novel facial Anchors and Contours Estimation framework, ACE-Net,\nfor fine-level face alignment tasks. ACE-Net predicts facial anchors and\ncontours that are richer than traditional facial landmarks while overcoming\nambiguities and inconsistencies in their definitions. We introduce a weakly\nsupervised loss enabling ACE-Net to learn from existing facial landmarks\ndatasets without the need for reannotation. Instead, synthetic data, from which\nGT contours can be easily obtained, is used during training to bridge the\ndensity gap between landmarks and true facial contours. We evaluate the face\nalignment accuracy of ACE-Net with respect to the HELEN dataset which has 194\nannotated facial landmarks, while it is trained with only 68 or 36 landmarks\nfrom the 300-W dataset. We show that ACE-Net generated contours are better than\ncontours interpolated straight from the 68 GT landmarks and ACE-Net also\noutperforms models trained only with full supervision from GT landmarks-based\ncontours.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jihua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamrakar_A/0/1/0/all/0/1\">Amir Tamrakar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphology on categorical distributions. (arXiv:2012.07315v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.07315","description":"<p>The categorical distribution is a natural representation of uncertainty in\nmulti-class segmentations. In the two-class case the categorical distribution\nreduces to the Bernoulli distribution, for which grayscale morphology provides\na range of useful operations. In the general case, applying morphological\noperations on uncertain multi-class segmentations is not straightforward as an\nimage of categorical distributions is not a complete lattice. Although\nmorphology on color images has received wide attention, this is not so for\ncolor-coded or categorical images and even less so for images of categorical\ndistributions. In this work, we establish a set of requirements for morphology\non categorical distributions by combining classic morphology with a\nprobabilistic view. We then define operators respecting these requirements,\nintroduce protected operations on categorical distributions and illustrate the\nutility of these operators on two example tasks: modeling annotator bias in\nbrain tumor segmentations and segmenting vesicle instances from the predictions\nof a multi-class U-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orting_S/0/1/0/all/0/1\">Silas Nyboe &#xd8;rting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stephensen_H/0/1/0/all/0/1\">Hans Jacob Teglbj&#xe6;rg Stephensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sporring_J/0/1/0/all/0/1\">Jon Sporring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles. (arXiv:2101.06549v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2101.06549","description":"<p>As self-driving systems become better, simulating scenarios where the\nautonomy stack may fail becomes more important. Traditionally, those scenarios\nare generated for a few scenes with respect to the planning module that takes\nground-truth actor states as input. This does not scale and cannot identify all\npossible autonomy failures, such as perception failures due to occlusion. In\nthis paper, we propose AdvSim, an adversarial framework to generate\nsafety-critical scenarios for any LiDAR-based autonomy system. Given an initial\ntraffic scenario, AdvSim modifies the actors' trajectories in a physically\nplausible manner and updates the LiDAR sensor data to match the perturbed\nworld. Importantly, by simulating directly from sensor data, we obtain\nadversarial scenarios that are safety-critical for the full autonomy stack. Our\nexperiments show that our approach is general and can identify thousands of\nsemantically meaningful safety-critical scenarios for a wide range of modern\nself-driving systems. Furthermore, we show that the robustness and safety of\nthese systems can be further improved by training them with scenarios generated\nby AdvSim.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingkang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_A/0/1/0/all/0/1\">Ava Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1\">James Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1\">Sivabalan Manivasagam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadat_A/0/1/0/all/0/1\">Abbas Sadat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1\">Sergio Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1\">Raquel Urtasun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Instance Retrieval: A Survey. (arXiv:2101.11282v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11282","description":"<p>In recent years a vast amount of visual content has been generated and shared\nfrom many fields, such as social media platforms, medical imaging, and\nrobotics. This abundance of content creation and sharing has introduced new\nchallenges, particularly that of searching databases for similar\ncontent-Content Based Image Retrieval (CBIR)-a long-established research area\nin which improved efficiency and accuracy are needed for real-time retrieval.\nArtificial intelligence has made progress in CBIR and has significantly\nfacilitated the process of instance search. In this survey we review recent\ninstance retrieval works that are developed based on deep learning algorithms\nand techniques, with the survey organized by deep network architecture types,\ndeep features, feature embedding and aggregation methods, and network\nfine-tuning strategies. Our survey considers a wide variety of recent methods,\nwhereby we identify milestone work, reveal connections among various methods\nand present the commonly used benchmarks, evaluation results, common\nchallenges, and propose promising future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakker_E/0/1/0/all/0/1\">Erwin Bakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiou_T/0/1/0/all/0/1\">Theodoros Georgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1\">Paul Fieguth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lew_M/0/1/0/all/0/1\">Michael S. Lew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiny Adversarial Mulit-Objective Oneshot Neural Architecture Search. (arXiv:2103.00363v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.00363","description":"<p>Due to limited computational cost and energy consumption, most neural network\nmodels deployed in mobile devices are tiny. However, tiny neural networks are\ncommonly very vulnerable to attacks. Current research has proved that larger\nmodel size can improve robustness, but little research focuses on how to\nenhance the robustness of tiny neural networks. Our work focuses on how to\nimprove the robustness of tiny neural networks without seriously deteriorating\nof clean accuracy under mobile-level resources. To this end, we propose a\nmulti-objective oneshot network architecture search (NAS) algorithm to obtain\nthe best trade-off networks in terms of the adversarial accuracy, the clean\naccuracy and the model size. Specifically, we design a novel search space based\non new tiny blocks and channels to balance model size and adversarial\nperformance. Moreover, since the supernet significantly affects the performance\nof subnets in our NAS algorithm, we reveal the insights into how the supernet\nhelps to obtain the best subnet under white-box adversarial attacks.\nConcretely, we explore a new adversarial training paradigm by analyzing the\nadversarial transferability, the width of the supernet and the difference\nbetween training the subnets from scratch and fine-tuning. Finally, we make a\nstatistical analysis for the layer-wise combination of certain blocks and\nchannels on the first non-dominated front, which can serve as a guideline to\ndesign tiny neural network architectures for the resilience of adversarial\nperturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guoyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinbao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaochu Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feedback Refined Local-Global Network for Super-Resolution of Hyperspectral Imagery. (arXiv:2103.04354v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.04354","description":"<p>With the development of deep learning technology, multi-spectral image\nsuper-resolution methods based on convolutional neural network have recently\nachieved great progress. However, the single hyperspectral image\nsuper-resolution remains a challenging problem due to the high-dimensional and\ncomplex spectral characteristics of hyperspectral data, which make it difficult\nto simultaneously capture spatial and spectral information. To deal with this\nissue, we propose a novel Feedback Refined Local-Global Network (FRLGN) for the\nsuper-resolution of hyperspectral image. To be specific, we develop a new\nFeedback Structure and a Local-Global Spectral Block to alleviate the\ndifficulty in spatial and spectral feature extraction. The Feedback Structure\ncan transfer the high-level information to guide the generation process of\nlow-level feature, which is achieved by a recurrent structure with finite\nunfoldings. Furthermore, in order to effectively use the high-level information\npassed back, a Local-Global Spectral Block is constructed to handle the\nfeedback connections. The Local-Global Spectral Block utilizes the feedback\nhigh-level information to correct the low-level feature from local spectral\nbands and generates powerful high-level representations among global spectral\nbands. By incorporating the Feedback Structure and Local-Global Spectral Block,\nthe FRLGN can fully exploit spatial-spectral correlations among spectral bands\nand gradually reconstruct high-resolution hyperspectral images. The source code\nof FRLGN is available at https://github.com/tangzhenjie/FRLGN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zhenjie Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qing Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_B/0/1/0/all/0/1\">Bin Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reframing Neural Networks: Deep Structure in Overcomplete Representations. (arXiv:2103.05804v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.05804","description":"<p>In comparison to classical shallow representation learning techniques, deep\nneural networks have achieved superior performance in nearly every application\nbenchmark. But despite their clear empirical advantages, it is still not well\nunderstood what makes them so effective. To approach this question, we\nintroduce deep frame approximation: a unifying framework for constrained\nrepresentation learning with structured overcomplete frames. While exact\ninference requires iterative optimization, it may be approximated by the\noperations of a feed-forward deep neural network. We indirectly analyze how\nmodel capacity relates to frame structures induced by architectural\nhyperparameters such as depth, width, and skip connections. We quantify these\nstructural differences with the deep frame potential, a data-independent\nmeasure of coherence linked to representation uniqueness and stability. As a\ncriterion for model selection, we show correlation with generalization error on\na variety of common deep network architectures and datasets. We also\ndemonstrate how recurrent networks implementing iterative optimization\nalgorithms can achieve performance comparable to their feed-forward\napproximations while improving adversarial robustness. This connection to the\nestablished theory of overcomplete representations suggests promising new\ndirections for principled deep network architecture design with less reliance\non ad-hoc engineering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murdock_C/0/1/0/all/0/1\">Calvin Murdock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cazenavette_G/0/1/0/all/0/1\">George Cazenavette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-Specific Autoencoders for Exploring, Editing and Transmitting Videos. (arXiv:2103.17261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17261","description":"<p>We study video-specific autoencoders that allow a human user to explore,\nedit, and efficiently transmit videos. Prior work has independently looked at\nthese problems (and sub-problems) and proposed different formulations. In this\nwork, we train a simple autoencoder (from scratch) on multiple frames of a\nspecific video. We observe: (1) latent codes learned by a video-specific\nautoencoder capture spatial and temporal properties of that video; and (2)\nautoencoders can project out-of-sample inputs onto the video-specific manifold.\nThese two properties allow us to explore, edit, and efficiently transmit a\nvideo using one learned representation. For e.g., linear operations on latent\ncodes allow users to visualize the contents of a video. Associating latent\ncodes of a video and manifold projection enables users to make desired edits.\nInterpolating latent codes and manifold projection allows the transmission of\nsparse low-res frames over a network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kevin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Aayush Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust partial Fourier reconstruction for diffusion-weighted imaging using a recurrent convolutional neural network. (arXiv:2105.09378v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.09378","description":"<p>Purpose: To develop an algorithm for robust partial Fourier (PF)\nreconstruction applicable to diffusion-weighted (DW) images with non-smooth\nphase variations.\n</p>\n<p>Methods: Based on an unrolled proximal splitting algorithm, a neural network\narchitecture is derived which alternates between data consistency operations\nand regularization implemented by recurrent convolutions. In order to exploit\ncorrelations, multiple repetitions of the same slice are jointly reconstructed\nunder consideration of permutation-equivariance. The algorithm is trained on DW\nliver data of 60 volunteers and evaluated on retrospectively and prospectively\nsub-sampled data of different anatomies and resolutions.\n</p>\n<p>Results: The proposed method is able to significantly outperform conventional\nPF techniques on retrospectively sub-sampled data in terms of quantitative\nmeasures as well as perceptual image quality. In this context, joint\nreconstruction of repetitions as well as the particular type of recurrent\nnetwork unrolling are found to be beneficial with respect to reconstruction\nquality. On prospectively PF-sampled data, the proposed method enables DW\nimaging with higher signal without sacrificing image resolution or introducing\nadditional artifacts. Alternatively, it can be used to counter the TE increase\nin acquisitions with higher resolution. Further, generalizability can be shown\nto prospective brain data exhibiting anatomies and contrasts not present in the\ntraining set.\n</p>\n<p>Conclusion: This work demonstrates that robust PF reconstruction of DW data\nis feasible even at strong PF factors in anatomies prone to phase variations.\nSince the proposed method does not rely on smoothness priors of the phase but\nuses learned recurrent convolutions instead, artifacts of conventional PF\nmethods can be avoided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gadjimuradov_F/0/1/0/all/0/1\">Fasil Gadjimuradov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benkert_T/0/1/0/all/0/1\">Thomas Benkert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nickel_M/0/1/0/all/0/1\">Marcel Dominik Nickel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embracing New Techniques in Deep Learning for Estimating Image Memorability. (arXiv:2105.10598v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10598","description":"<p>Various work has suggested that the memorability of an image is consistent\nacross people, and thus can be treated as an intrinsic property of an image.\nUsing computer vision models, we can make specific predictions about what\npeople will remember or forget. While older work has used now-outdated deep\nlearning architectures to predict image memorability, innovations in the field\nhave given us new techniques to apply to this problem. Here, we propose and\nevaluate five alternative deep learning models which exploit developments in\nthe field from the last five years, largely the introduction of residual neural\nnetworks, which are intended to allow the model to use semantic information in\nthe memorability estimation process. These new models were tested against the\nprior state of the art with a combined dataset built to optimize both\nwithin-category and across-category predictions. Our findings suggest that the\nkey prior memorability network had overstated its generalizability and was\noverfit on its training set. Our new models outperform this prior model,\nleading us to conclude that Residual Networks outperform simpler convolutional\nneural networks in memorability regression. We make our new state-of-the-art\nmodel readily available to the research community, allowing memory researchers\nto make predictions about memorability on a wider range of images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Needell_C/0/1/0/all/0/1\">Coen D. Needell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bainbridge_W/0/1/0/all/0/1\">Wilma A. Bainbridge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis. (arXiv:2106.01444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01444","description":"<p>The open-ended nature of visual captioning makes it a challenging area for\nevaluation. The majority of proposed models rely on specialized training to\nimprove human-correlation, resulting in limited adoption, generalizability, and\nexplainabilty. We introduce \"typicality\", a new formulation of evaluation\nrooted in information theory, which is uniquely suited for problems lacking a\ndefinite ground truth. Typicality serves as our framework to develop a novel\nsemantic comparison, SPARCS, as well as referenceless fluency evaluation\nmetrics. Over the course of our analysis, two separate dimensions of fluency\nnaturally emerge: style, captured by metric SPURTS, and grammar, captured in\nthe form of grammatical outlier penalties. Through extensive experiments and\nablation studies on benchmark datasets, we show how these decomposed dimensions\nof semantics and fluency provide greater system-level insight into captioner\ndifferences. Our proposed metrics along with their combination, SMURF, achieve\nstate-of-the-art correlation with human judgment when compared with other\nrule-based evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feinglass_J/0/1/0/all/0/1\">Joshua Feinglass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Implicit Neural Representation for Fonts. (arXiv:2106.06866v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06866","description":"<p>Fonts are ubiquitous across documents and come in a variety of styles. They\nare either represented in a native vector format or rasterized to produce fixed\nresolution images. In the first case, the non-standard representation prevents\nbenefiting from latest network architectures for neural representations; while,\nin the latter case, the rasterized representation, when encoded via networks,\nresults in loss of data fidelity, as font-specific discontinuities like edges\nand corners are difficult to represent using neural networks. Based on the\nobservation that complex fonts can be represented by a superposition of a set\nof simpler occupancy functions, we introduce \\textit{multi-implicits} to\nrepresent fonts as a permutation-invariant set of learned implict functions,\nwithout losing features (e.g., edges and corners). However, while\nmulti-implicits locally preserve font features, obtaining supervision in the\nform of ground truth multi-channel signals is a problem in itself. Instead, we\npropose how to train such a representation with only local supervision, while\nthe proposed neural architecture directly finds globally consistent\nmulti-implicits for font families. We extensively evaluate the proposed\nrepresentation for various tasks including reconstruction, interpolation, and\nsynthesis to demonstrate clear advantages with existing alternatives.\nAdditionally, the representation naturally enables glyph completion, wherein a\nsingle characteristic font is used to synthesize a whole font family in the\ntarget style.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_P/0/1/0/all/0/1\">Pradyumna Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhifei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Attention for Automatic Chest X-ray Report Generation. (arXiv:2106.06965v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06965","description":"<p>Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Changchang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v8 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2106.08208","description":"<p>Adaptive gradient methods have shown excellent performances for solving many\nmachine learning problems. Although multiple adaptive gradient methods were\nrecently studied, they mainly focus on either empirical or theoretical aspects\nand also only work for specific problems by using some specific adaptive\nlearning rates. Thus, it is desired to design a universal framework for\npractical algorithms of adaptive gradients with theoretical guarantee to solve\ngeneral problems. To fill this gap, we propose a faster and universal framework\nof adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive\nmatrix that includes most existing adaptive gradient forms. Moreover, our\nframework can flexibly integrate the momentum and variance reduced techniques.\nIn particular, our novel framework provides the convergence analysis support\nfor adaptive gradient methods under the nonconvex setting. In theoretical\nanalysis, we prove that our SUPER-ADAM algorithm can achieve the best known\ngradient (i.e., stochastic first-order oracle (SFO)) complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms. Code is available at\nhttps://github.com/LIJUNYI95/SuperAdam\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation Studies: A Review. (arXiv:2106.09862v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09862","description":"<p>Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly\nused to visualize and quantify left atrial (LA) scars. The position and extent\nof scars provide important information of the pathophysiology and progression\nof atrial fibrillation (AF). Hence, LA scar segmentation and quantification\nfrom LGE MRI can be useful in computer-assisted diagnosis and treatment\nstratification of AF patients. Since manual delineation can be time-consuming\nand subject to intra- and inter-expert variability, automating this computing\nis highly desired, which nevertheless is still challenging and\nunder-researched.\n</p>\n<p>This paper aims to provide a systematic review on computing methods for LA\ncavity, wall, scar and ablation gap segmentation and quantification from LGE\nMRI, and the related literature for AF studies. Specifically, we first\nsummarize AF-related imaging techniques, particularly LGE MRI. Then, we review\nthe methodologies of the four computing tasks in detail, and summarize the\nvalidation strategies applied in each task. Finally, the possible future\ndevelopments are outlined, with a brief survey on the potential clinical\napplications of the aforementioned methods. The review shows that the research\ninto this topic is still in early stages. Although several methods have been\nproposed, especially for LA segmentation, there is still large scope for\nfurther algorithmic developments due to performance issues related to the high\nvariability of enhancement appearance and differences in image acquisition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmer_V/0/1/0/all/0/1\">Veronika A. Zimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action Transformer: A Self-Attention Model for Short-Time Pose-Based Human Action Recognition. (arXiv:2107.00606v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00606","description":"<p>Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time,\nshort-time HAR. The proposed methodology was extensively tested on MPOSE2021\nand compared to several state-of-the-art architectures, proving the\neffectiveness of the AcT model and laying the foundations for future work on\nHAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_F/0/1/0/all/0/1\">Federico Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00652","description":"<p>We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a mathematical analysis\nof the effect of the stripe width and vary the stripe width for different\nlayers of the Transformer network which achieves strong modeling capability\nwhile limiting the computation cost. We also introduce Locally-enhanced\nPositional Encoding (LePE), which handles the local positional information\nbetter than existing encoding schemes. LePE naturally supports arbitrary input\nresolutions, and is thus especially effective and friendly for downstream\ntasks. Incorporated with these designs and a hierarchical structure, CSWin\nTransformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4\\% Top-1 accuracy on ImageNet-1K without any\nextra training data or label, 53.9 box AP and 46.4 mask AP on the COCO\ndetection task, and 52.2 mIOU on the ADE20K semantic segmentation task,\nsurpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0,\n+1.4, and +2.0 respectively under the similar FLOPs setting. By further\npretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy\non ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. The\ncode and models are available at\nhttps://github.com/microsoft/CSWin-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Parser: Representing Part-whole Hierarchies with Transformers. (arXiv:2107.05790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05790","description":"<p>Human vision is able to capture the part-whole hierarchical information from\nthe entire scene. This paper presents the Visual Parser (ViP) that explicitly\nconstructs such a hierarchy with transformers. ViP divides visual\nrepresentations into two levels, the part level and the whole level.\nInformation of each part represents a combination of several independent\nvectors within the whole. To model the representations of the two levels, we\nfirst encode the information from the whole into part vectors through an\nattention mechanism, then decode the global information within the part vectors\nback into the whole representation. By iteratively parsing the two levels with\nthe proposed encoder-decoder interaction, the model can gradually refine the\nfeatures on both levels. Experimental results demonstrate that ViP can achieve\nvery competitive performance on three major tasks e.g. classification,\ndetection and instance segmentation. In particular, it can surpass the previous\nstate-of-the-art CNN backbones by a large margin on object detection. The tiny\nmodel of the ViP family with $7.2\\times$ fewer parameters and $10.9\\times$\nfewer FLOPS can perform comparably with the largest model\nResNeXt-101-64$\\times$4d of ResNe(X)t family. Visualization results also\ndemonstrate that the learnt parts are highly informative of the predicting\nclass, making ViP more explainable than previous fundamental architectures.\nCode is available at https://github.com/kevin-ssy/ViP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiaoyu Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip Torr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents. (arXiv:2108.00273v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00273","description":"<p>Traffic accident anticipation is a vital function of Automated Driving\nSystems (ADSs) for providing a safety-guaranteed driving experience. An\naccident anticipation model aims to predict accidents promptly and accurately\nbefore they occur. Existing Artificial Intelligence (AI) models of accident\nanticipation lack a human-interpretable explanation of their decision-making.\nAlthough these models perform well, they remain a black-box to the ADS users,\nthus difficult to get their trust. To this end, this paper presents a Gated\nRecurrent Unit (GRU) network that learns spatio-temporal relational features\nfor the early anticipation of traffic accidents from dashcam video data. A\npost-hoc attention mechanism named Grad-CAM is integrated into the network to\ngenerate saliency maps as the visual explanation of the accident anticipation\ndecision. An eye tracker captures human eye fixation points for generating\nhuman attention maps. The explainability of network-generated saliency maps is\nevaluated in comparison to human attention maps. Qualitative and quantitative\nresults on a public crash dataset confirm that the proposed explainable network\ncan anticipate an accident on average 4.57 seconds before it occurs, with\n94.02% average precision. In further, various post-hoc attention-based XAI\nmethods are evaluated and compared. It confirms that the Grad-CAM chosen by\nthis study can generate high-quality, human-interpretable saliency maps (with\n1.23 Normalized Scanpath Saliency) for explaining the crash anticipation\ndecision. Importantly, results confirm that the proposed AI model, with a\nhuman-inspired design, can outperform humans in the accident anticipation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02359","description":"<p>Video captioning combines video understanding and language generation.\nDifferent from image captioning that describes a static image with details of\nalmost every object, video captioning usually considers a sequence of frames\nand biases towards focused objects, e.g., the objects that stay in focus\nregardless of the changing background. Therefore, detecting and properly\naccommodating focused objects is critical in video captioning. To enforce the\ndescription of focused objects and achieve controllable video captioning, we\npropose an Object-Oriented Non-Autoregressive approach (O2NA), which performs\ncaption generation in three steps: 1) identify the focused objects and predict\ntheir locations in the target caption; 2) generate the related attribute words\nand relation words of these focused objects to form a draft caption; and 3)\ncombine video information to refine the draft caption to a fluent final\ncaption. Since the focused objects are generated and located ahead of other\nwords, it is difficult to apply the word-by-word autoregressive generation\nprocess; instead, we adopt a non-autoregressive approach. The experiments on\ntwo benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness\nof O2NA, which achieves results competitive with the state-of-the-arts but with\nboth higher diversity and higher inference speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVT: Point-Voxel Transformer for Point Cloud Learning. (arXiv:2108.06076v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06076","description":"<p>The recently developed pure Transformer architectures have attained promising\naccuracy on point cloud learning benchmarks compared to convolutional neural\nnetworks. However, existing point cloud Transformers are computationally\nexpensive since they waste a significant amount of time on structuring the\nirregular data. To solve this shortcoming, we present Sparse Window Attention\n(SWA) module to gather coarse-grained local features from non-empty voxels,\nwhich not only bypasses the expensive irregular data structuring and invalid\nempty voxel computation, but also obtains linear computational complexity with\nrespect to voxel resolution. Meanwhile, to gather fine-grained features about\nthe global shape, we introduce relative attention (RA) module, a more robust\nself-attention variant for rigid transformations of objects. Equipped with the\nSWA and RA, we construct our neural architecture called PVT that integrates\nboth modules into a joint framework for point cloud learning. Compared with\nprevious Transformer-based and attention-based models, our method attains top\naccuracy of 94.0% on classification benchmark and 10x inference speedup on\naverage. Extensive experiments also valid the effectiveness of PVT on part and\nsemantic segmentation benchmarks (86.6% and 69.2% mIoU, respectively).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Haocheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xinyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Specificity-preserving RGB-D Saliency Detection. (arXiv:2108.08162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08162","description":"<p>Salient object detection (SOD) on RGB and depth images has attracted more and\nmore research interests, due to its effectiveness and the fact that depth cues\ncan now be conveniently captured. Existing RGB-D SOD models usually adopt\ndifferent fusion strategies to learn a shared representation from the two\nmodalities (\\ie, RGB and depth), while few methods explicitly consider how to\npreserve modality-specific characteristics. In this study, we propose a novel\nframework, termed SPNet} (Specificity-preserving network), which benefits SOD\nperformance by exploring both the shared information and modality-specific\nproperties (\\eg, specificity). Specifically, we propose to adopt two\nmodality-specific networks and a shared learning network to generate individual\nand shared saliency prediction maps, respectively. To effectively fuse\ncross-modal features in the shared learning network, we propose a\ncross-enhanced integration module (CIM) and then propagate the fused feature to\nthe next layer for integrating cross-level information. Moreover, to capture\nrich complementary multi-modal information for boosting the SOD performance, we\npropose a multi-modal feature aggregation (MFA) module to integrate the\nmodality-specific features from each individual decoder into the shared\ndecoder. By using a skip connection, the hierarchical features between the\nencoder and decoder layers can be fully combined. Extensive experiments\ndemonstrate that our~\\ours~outperforms cutting-edge approaches on six popular\nRGB-D SOD and three camouflaged object detection benchmarks. The project is\npublicly available at: https://github.com/taozh2017/SPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GlassNet: Label Decoupling-based Three-stream Neural Network for Robust Image Glass Detection. (arXiv:2108.11117v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11117","description":"<p>Most of the existing object detection methods generate poor glass detection\nresults, due to the fact that the transparent glass shares the same appearance\nwith arbitrary objects behind it in an image. Different from traditional deep\nlearning-based wisdoms that simply use the object boundary as auxiliary\nsupervision, we exploit label decoupling to decompose the original labeled\nground-truth (GT) map into an interior-diffusion map and a boundary-diffusion\nmap. The GT map in collaboration with the two newly generated maps breaks the\nimbalanced distribution of the object boundary, leading to improved glass\ndetection quality. We have three key contributions to solve the transparent\nglass detection problem: (1) We propose a three-stream neural network (call\nGlassNet for short) to fully absorb beneficial features in the three maps. (2)\nWe design a multi-scale interactive dilation module to explore a wider range of\ncontextual information. (3) We develop an attention-based boundary-aware\nfeature Mosaic module to integrate multi-modal information. Extensive\nexperiments on the benchmark dataset exhibit clear improvements of our method\nover SOTAs, in terms of both the overall glass detection accuracy and boundary\nclearness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">C. Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">D. Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">X. Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">D. Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wei_M/0/1/0/all/0/1\">M. wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">X. Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Y. Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">H. Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02860","description":"<p>Graph convolutional networks (GCNs) have emerged as dominant methods for\nskeleton-based action recognition.\n</p>\n<p>However, they still suffer from two problems, namely, neighborhood\nconstraints and entangled spatiotemporal feature representations.\n</p>\n<p>Most studies have focused on improving the design of graph topology to solve\nthe first problem but they have yet to fully explore the latter.\n</p>\n<p>In this work, we design a disentangled spatiotemporal transformer (DSTT)\nblock to overcome the above limitations of GCNs in three steps: (i) feature\ndisentanglement for spatiotemporal decomposition;(ii) global spatiotemporal\nattention for capturing correlations in the global context; and (iii) local\ninformation enhancement for utilizing more local information.\n</p>\n<p>Thereon, we propose a novel architecture, named Hierarchical Graph\nConvolutional skeleton Transformer (HGCT), to employ the complementary\nadvantages of GCN (i.e., local topology, temporal dynamics and hierarchy) and\nTransformer (i.e., global context and dynamic attention).\n</p>\n<p>HGCT is lightweight and computationally efficient.\n</p>\n<p>Quantitative analysis demonstrates the superiority and good interpretability\nof HGCT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1\">Ruwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengfa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Miao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds. (arXiv:2109.05566v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05566","description":"<p>3D scene understanding from point clouds plays a vital role for various\nrobotic applications. Unfortunately, current state-of-the-art methods use\nseparate neural networks for different tasks like object detection or room\nlayout estimation. Such a scheme has two limitations: 1) Storing and running\nseveral networks for different tasks are expensive for typical robotic\nplatforms. 2) The intrinsic structure of separate outputs are ignored and\npotentially violated. To this end, we propose the first transformer\narchitecture that predicts 3D objects and layouts simultaneously, using point\ncloud inputs. Unlike existing methods that either estimate layout keypoints or\nedges, we directly parameterize room layout as a set of quads. As such, the\nproposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the\nnovel quad representation, we propose a tailored physical constraint loss\nfunction that discourages object-layout interference. The quantitative and\nqualitative evaluations on the public benchmark ScanNet show that the proposed\nPQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a\nquasi-real-time (8.91 FPS) rate without efficiency-oriented optimization.\nMoreover, the new physical constraint loss can improve strong baselines, and\nthe F1-score of the room layout is significantly promoted from 37.9% to 57.9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoxue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya-Qin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Decomposition with Phase-Correlation Networks. (arXiv:2110.03473v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03473","description":"<p>The ability to decompose scenes into their object components is a desired\nproperty for autonomous agents, allowing them to reason and act in their\nsurroundings. Recently, different methods have been proposed to learn\nobject-centric representations from data in an unsupervised manner. These\nmethods often rely on latent representations learned by deep neural networks,\nhence requiring high computational costs and large amounts of curated data.\nSuch models are also difficult to interpret. To address these challenges, we\npropose the Phase-Correlation Decomposition Network (PCDNet), a novel model\nthat decomposes a scene into its object components, which are represented as\ntransformed versions of a set of learned object prototypes. The core building\nblock in PCDNet is the Phase-Correlation Cell (PC Cell), which exploits the\nfrequency-domain representation of the images in order to estimate the\ntransformation between an object prototype and its transformed version in the\nimage. In our experiments, we show how PCDNet outperforms state-of-the-art\nmethods for unsupervised object discovery and segmentation on simple benchmark\ndatasets and on more challenging data, while using a small number of learnable\nparameters and being fully interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villar_Corrales_A/0/1/0/all/0/1\">Angel Villar-Corrales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishing Natural and Computer-Generated Images using Multi-Colorspace fused EfficientNet. (arXiv:2110.09428v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09428","description":"<p>The problem of distinguishing natural images from photo-realistic\ncomputer-generated ones either addresses natural images versus computer\ngraphics or natural images versus GAN images, at a time. But in a real-world\nimage forensic scenario, it is highly essential to consider all categories of\nimage generation, since in most cases image generation is unknown. We, for the\nfirst time, to our best knowledge, approach the problem of distinguishing\nnatural images from photo-realistic computer-generated images as a three-class\nclassification task classifying natural, computer graphics, and GAN images. For\nthe task, we propose a Multi-Colorspace fused EfficientNet model by parallelly\nfusing three EfficientNet networks that follow transfer learning methodology\nwhere each network operates in different colorspaces, RGB, LCH, and HSV, chosen\nafter analyzing the efficacy of various colorspace transformations in this\nimage forensics problem. Our model outperforms the baselines in terms of\naccuracy, robustness towards post-processing, and generalizability towards\nother datasets. We conduct psychophysics experiments to understand how\naccurately humans can distinguish natural, computer graphics, and GAN images\nwhere we could observe that humans find difficulty in classifying these images,\nparticularly the computer-generated images, indicating the necessity of\ncomputational algorithms for the task. We also analyze the behavior of our\nmodel through visual explanations to understand salient regions that contribute\nto the model's decision making and compare with manual explanations provided by\nhuman participants in the form of region markings, where we could observe\nsimilarities in both the explanations indicating the powerful nature of our\nmodel to take the decisions meaningfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangan_M/0/1/0/all/0/1\">Manjary P Gangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_A/0/1/0/all/0/1\">Anoop K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_L/0/1/0/all/0/1\">Lajish V L</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based fully automatic assessment of open surgery suturing skills. (arXiv:2110.13972v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13972","description":"<p>The goal of this study was to develop new reliable open surgery suturing\nsimulation system for training medical students in situation where resources\nare limited or in the domestic setup. Namely, we developed an algorithm for\ntools and hands localization as well as identifying the interactions between\nthem based on simple webcam video data, calculating motion metrics for\nassessment of surgical skill. Twenty-five participants performed multiple\nsuturing tasks using our simulator. The YOLO network has been modified to a\nmulti-task network, for the purpose of tool localization and tool-hand\ninteraction detection. This was accomplished by splitting the YOLO detection\nheads so that they supported both tasks with minimal addition to computer\nrun-time. Furthermore, based on the outcome of the system, motion metrics were\ncalculated. These metrics included traditional metrics such as time and path\nlength as well as new metrics assessing the technique participants use for\nholding the tools. The dual-task network performance was similar to that of two\nnetworks, while computational load was only slightly bigger than one network.\nIn addition, the motion metrics showed significant differences between experts\nand novices. While video capture is an essential part of minimally invasive\nsurgery, it is not an integral component of open surgery. Thus, new algorithms,\nfocusing on the unique challenges open surgery videos present, are required. In\nthis study, a dual-task network was developed to solve both a localization task\nand a hand-tool interaction task. The dual network may be easily expanded to a\nmulti-task network, which may be useful for images with multiple layers and for\nevaluating the interaction between these different layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldbraikh_A/0/1/0/all/0/1\">Adam Goldbraikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAngelo_A/0/1/0/all/0/1\">Anne-Lise D&#x27;Angelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1\">Carla M. Pugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laufer_S/0/1/0/all/0/1\">Shlomi Laufer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FabricFlowNet: Bimanual Cloth Manipulation with a Flow-based Policy. (arXiv:2111.05623v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.05623","description":"<p>We address the problem of goal-directed cloth manipulation, a challenging\ntask due to the deformability of cloth. Our insight is that optical flow, a\ntechnique normally used for motion estimation in video, can also provide an\neffective representation for corresponding cloth poses across observation and\ngoal images. We introduce FabricFlowNet (FFN), a cloth manipulation policy that\nleverages flow as both an input and as an action representation to improve\nperformance. FabricFlowNet also elegantly switches between bimanual and\nsingle-arm actions based on the desired goal. We show that FabricFlowNet\nsignificantly outperforms state-of-the-art model-free and model-based cloth\nmanipulation policies that take image input. We also present real-world\nexperiments on a bimanual system, demonstrating effective sim-to-real transfer.\nFinally, we show that our method generalizes when trained on a single square\ncloth to other cloth shapes, such as T-shirts and rectangular cloths. Video and\nother supplementary materials are available at:\nhttps://sites.google.com/view/fabricflownet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_T/0/1/0/all/0/1\">Thomas Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajracharya_S/0/1/0/all/0/1\">Sujay Bajracharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_K/0/1/0/all/0/1\">Khush Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of the Influence of Transfer Learning When Measuring the Tortuosity of Blood Vessels. (arXiv:2111.10255v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.10255","description":"<p>Characterizing blood vessels in digital images is important for the diagnosis\nof many types of diseases as well as for assisting current researches regarding\nvascular systems. The automated analysis of blood vessels typically requires\nthe identification, or segmentation, of the blood vessels in an image or a set\nof images, which is usually a challenging task. Convolutional Neural Networks\n(CNNs) have been shown to provide excellent results regarding the segmentation\nof blood vessels. One important aspect of CNNs is that they can be trained on\nlarge amounts of data and then be made available, for instance, in image\nprocessing software for wide use. The pre-trained CNNs can then be easily\napplied in downstream blood vessel characterization tasks such as the\ncalculation of the length, tortuosity, or caliber of the blood vessels. Yet, it\nis still unclear if pre-trained CNNs can provide robust, unbiased, results on\ndownstream tasks when applied to datasets that they were not trained on. Here,\nwe focus on measuring the tortuosity of blood vessels and investigate to which\nextent CNNs may provide biased tortuosity values even after fine-tuning the\nnetwork to the new dataset under study. We show that the tortuosity values\nobtained by a CNN trained from scratch on a dataset may not agree with those\nobtained by a fine-tuned network that was pre-trained on a dataset having\ndifferent tortuosity statistics. In addition, we show that the improvement in\nsegmentation performance when fine-tuning the network does not necessarily lead\nto a respective improvement on the estimation of the tortuosity. To mitigate\nthe aforementioned issues, we propose the application of specific data\naugmentation techniques even in situations where they do not improve\nsegmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Silva_M/0/1/0/all/0/1\">Matheus V. da Silva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouellette_J/0/1/0/all/0/1\">Julie Ouellette</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lacoste_B/0/1/0/all/0/1\">Baptiste Lacoste</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Comin_C/0/1/0/all/0/1\">Cesar H. Comin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Graph-Convolutional Variational AutoEncoding for Generative Modelling of Human Motion. (arXiv:2111.12602v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12602","description":"<p>Models of human motion commonly focus either on trajectory prediction or\naction classification but rarely both. The marked heterogeneity and intricate\ncompositionality of human motion render each task vulnerable to the data\ndegradation and distributional shift common to real-world scenarios. A\nsufficiently expressive generative model of action could in theory enable data\nconditioning and distributional resilience within a unified framework\napplicable to both tasks. Here we propose a novel architecture based on\nhierarchical variational autoencoders and deep graph convolutional neural\nnetworks for generating a holistic model of action over multiple time-scales.\nWe show this Hierarchical Graph-convolutional Variational Autoencoder (HG-VAE)\nto be capable of generating coherent actions, detecting out-of-distribution\ndata, and imputing missing data by gradient ascent on the model's posterior.\nTrained and evaluated on H3.6M and the largest collection of open source human\nmotion data, AMASS, we show HG-VAE can facilitate downstream discriminative\nlearning better than baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bourached_A/0/1/0/all/0/1\">Anthony Bourached</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1\">Robert Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_R/0/1/0/all/0/1\">Ryan-Rhys Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1\">Ashwani Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PGGANet: Pose Guided Graph Attention Network for Person Re-identification. (arXiv:2111.14411v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14411","description":"<p>Person re-identification (reID) aims at retrieving a person from images\ncaptured by different cameras. For deep-learning-based reID methods, it has\nbeen proved that using local features together with global feature could help\nto give robust representation for person retrieval. Human pose information\ncould provide the locations of human skeleton to effectively guide the network\nto pay more attention on these key areas and could also help to reduce the\nnoise distractions from background or occlusion. However, methods proposed by\nprevious pose-based works might not be able to fully exploit the benefits of\npose information and few of them take into consideration the different\ncontributions of separate local features. In this paper, we propose a pose\nguided graph attention network, a multi-branch architecture consisting of one\nbranch for global feature, one branch for mid-granular body features and one\nbranch for fine-granular key point features. We use a pre-trained pose\nestimator to generate the key-point heatmaps for local feature learning and\ncarefully design a graph attention convolution layer to re-assign the\ncontribution weights of extracted local features by modeling the similarities\nrelations. Experiment results demonstrate the effectiveness of our approach on\ndiscriminative feature learning and we show that our model achieves\nstate-of-the-art performances on several mainstream evaluation datasets. We\nalso conduct a plenty of ablation studies and design different kinds of\ncomparison experiments for our network to prove its effectiveness and\nrobustness, including occluded experiments and cross-domain tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhijun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenquan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffSDFSim: Differentiable Rigid-Body Dynamics With Implicit Shapes. (arXiv:2111.15318v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15318","description":"<p>Differentiable physics is a powerful tool in computer vision and robotics for\nscene understanding and reasoning about interactions. Existing approaches have\nfrequently been limited to objects with simple shape or shapes that are known\nin advance. In this paper, we propose a novel approach to differentiable\nphysics with frictional contacts which represents object shapes implicitly\nusing signed distance fields (SDFs). Our simulation supports contact point\ncalculation even when the involved shapes are nonconvex. Moreover, we propose\nways for differentiating the dynamics for the object shape to facilitate shape\noptimization using gradient-based methods. In our experiments, we demonstrate\nthat our approach allows for model-based inference of physical parameters such\nas friction coefficients, mass, forces or shape parameters from trajectory and\ndepth image observations in several challenging synthetic scenarios and a real\nimage sequence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strecke_M/0/1/0/all/0/1\">Michael Strecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1\">Joerg Stueckler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data. (arXiv:2112.00054v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00054","description":"<p>Pre-training models on Imagenet or other massive datasets of real images has\nled to major advances in computer vision, albeit accompanied with shortcomings\nrelated to curation cost, privacy, usage rights, and ethical issues. In this\npaper, for the first time, we study the transferability of pre-trained models\nbased on synthetic data generated by graphics simulators to downstream tasks\nfrom very different domains. In using such synthetic data for pre-training, we\nfind that downstream performance on different tasks are favored by different\nconfigurations of simulation parameters (e.g. lighting, object pose,\nbackgrounds, etc.), and that there is no one-size-fits-all solution. It is thus\nbetter to tailor synthetic pre-training data to a specific downstream task, for\nbest performance. We introduce Task2Sim, a unified model mapping downstream\ntask representations to optimal simulation parameters to generate synthetic\npre-training data for them. Task2Sim learns this mapping by training to find\nthe set of best parameters on a set of \"seen\" tasks. Once trained, it can then\nbe used to predict best simulation parameters for novel \"unseen\" tasks in one\nshot, without requiring additional training. Given a budget in number of images\nper class, our extensive experiments with 20 diverse downstream tasks show\nTask2Sim's task-adaptive pre-training data results in significantly better\ndownstream performance than non-adaptively choosing simulation parameters on\nboth seen and unseen tasks. It is even competitive with pre-training on real\nimages from Imagenet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1\">Cheng Perng Phoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio S. Feris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Spatiotemporal Representation Learning by Exploiting Video Continuity. (arXiv:2112.05883v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05883","description":"<p>Recent self-supervised video representation learning methods have found\nsignificant success by exploring essential properties of videos, e.g. speed,\ntemporal order, etc. This work exploits an essential yet under-explored\nproperty of videos, the video continuity, to obtain supervision signals for\nself-supervised representation learning. Specifically, we formulate three novel\ncontinuity-related pretext tasks, i.e. continuity justification, discontinuity\nlocalization, and missing section approximation, that jointly supervise a\nshared backbone for video representation learning. This self-supervision\napproach, termed as Continuity Perception Network (CPNet), solves the three\ntasks altogether and encourages the backbone network to learn local and\nlong-ranged motion and context representations. It outperforms prior arts on\nmultiple downstream tasks, such as action recognition, video retrieval, and\naction localization. Additionally, the video continuity can be complementary to\nother coarse-grained video properties for representation learning, and\nintegrating the proposed pretext task to prior arts can yield much performance\ngains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quader_N/0/1/0/all/0/1\">Niamul Quader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zhixiang Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Juwei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAC-GAN: Structure-Aware Image-to-Image Composition for Self-Driving. (arXiv:2112.06596v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06596","description":"<p>We present a compositional approach to image augmentation for self-driving\napplications. It is an end-to-end neural network that is trained to seamlessly\ncompose an object (e.g., a vehicle or pedestrian) represented as a cropped\npatch from an object image, into a background scene image. As our approach\nemphasizes more on semantic and structural coherence of the composed images,\nrather than their pixel-level RGB accuracies, we tailor the input and output of\nour network with structure-aware features and design our network losses\naccordingly. Specifically, our network takes the semantic layout features from\nthe input scene image, features encoded from the edges and silhouette in the\ninput object patch, as well as a latent code as inputs, and generates a 2D\nspatial affine transform defining the translation and scaling of the object\npatch. The learned parameters are further fed into a differentiable spatial\ntransformer network to transform the object patch into the target image, where\nour model is trained adversarially using an affine transform discriminator and\na layout discriminator. We evaluate our network, coined SAC-GAN for\nstructure-aware composition, on prominent self-driving datasets in terms of\nquality, composability, and generalizability of the composite images.\nComparisons are made to state-of-the-art alternatives, confirming superiority\nof our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing Compact Building Models from Point Clouds Using Deep Implicit Fields. (arXiv:2112.13142v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13142","description":"<p>Three-dimensional (3D) building models play an increasingly pivotal role in\nmany real-world applications while obtaining a compact representation of\nbuildings remains an open problem. In this paper, we present a novel framework\nfor reconstructing compact, watertight, polygonal building models from point\nclouds. Our framework comprises three components: (a) a cell complex is\ngenerated via adaptive space partitioning that provides a polyhedral embedding\nas the candidate set; (b) an implicit field is learned by a deep neural network\nthat facilitates building occupancy estimation; (c) a Markov random field is\nformulated to extract the outer surface of a building via combinatorial\noptimization. We evaluate and compare our method with state-of-the-art methods\nin shape reconstruction, surface approximation, and geometry simplification.\nExperiments on both synthetic and real-world point clouds have demonstrated\nthat, with our neural-guided strategy, high-quality building models can be\nobtained with significant advantages in fidelity, compactness, and\ncomputational efficiency. Our method shows robustness to noise and insufficient\nmeasurements, and it can directly generalize from synthetic scans to real-world\nmeasurements. The source code of this work is freely available at\nhttps://github.com/chenzhaiyu/points2poly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_S/0/1/0/all/0/1\">Seyran Khademi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1\">Hugo Ledoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation. (arXiv:2201.00462v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00462","description":"<p>Computer-aided medical image segmentation has been applied widely in\ndiagnosis and treatment to obtain clinically useful information of shapes and\nvolumes of target organs and tissues. In the past several years, convolutional\nneural network (CNN) based methods (e.g., U-Net) have dominated this area, but\nstill suffered from inadequate long-range information capturing. Hence, recent\nwork presented computer vision Transformer variants for medical image\nsegmentation tasks and obtained promising performances. Such Transformers model\nlong-range dependency by computing pair-wise patch relations. However, they\nincur prohibitive computational costs, especially on 3D medical images (e.g.,\nCT and MRI). In this paper, we propose a new method called Dilated Transformer,\nwhich conducts self-attention for pair-wise patch relations captured\nalternately in local and global scopes. Inspired by dilated convolution\nkernels, we conduct the global self-attention in a dilated manner, enlarging\nreceptive fields without increasing the patches involved and thus reducing\ncomputational costs. Based on this design of Dilated Transformer, we construct\na U-shaped encoder-decoder hierarchical architecture called D-Former for 3D\nmedical image segmentation. Experiments on the Synapse and ACDC datasets show\nthat our D-Former model, trained from scratch, outperforms various competitive\nCNN-based or Transformer-based segmentation models at a low computational cost\nwithout time-consuming per-training process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yixuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kuanlun Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jintai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Honghao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAT-CADNet: Graph Attention Network for Panoptic Symbol Spotting in CAD Drawings. (arXiv:2201.00625v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00625","description":"<p>Spotting graphical symbols from the computer-aided design (CAD) drawings is\nessential to many industrial applications. Different from raster images, CAD\ndrawings are vector graphics consisting of geometric primitives such as\nsegments, arcs, and circles. By treating each CAD drawing as a graph, we\npropose a novel graph attention network GAT-CADNet to solve the panoptic symbol\nspotting problem: vertex features derived from the GAT branch are mapped to\nsemantic labels, while their attention scores are cascaded and mapped to\ninstance prediction. Our key contributions are three-fold: 1) the instance\nsymbol spotting task is formulated as a subgraph detection problem and solved\nby predicting the adjacency matrix; 2) a relative spatial encoding (RSE) module\nexplicitly encodes the relative positional and geometric relation among\nvertices to enhance the vertex attention; 3) a cascaded edge encoding (CEE)\nmodule extracts vertex attentions from multiple stages of GAT and treats them\nas edge encoding to predict the adjacency matrix. The proposed GAT-CADNet is\nintuitive yet effective and manages to solve the panoptic symbol spotting\nproblem in one consolidated network. Extensive experiments and ablation studies\non the public benchmark show that our graph-based approach surpasses existing\nstate-of-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhaohua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianfang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Honghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petzold_F/0/1/0/all/0/1\">Frank Petzold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HWRCNet: Handwritten Word Recognition in JPEG Compressed Domain using CNN-BiLSTM Network. (arXiv:2201.00947v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.00947","description":"<p>The handwritten word recognition from images using deep learning is an active\nresearch area with promising performance. It practical scenario, it might be\nrequired to process the handwritten images in the compressed domain due to due\nto security reasons. However, the utilization of deep learning is still very\nlimited for the processing of compressed images. Motivated by the need of\nprocessing document images in the compressed domain using recent developments\nin deep learning, we propose a HWRCNet model for handwritten word recognition\nin JPEG compressed domain. The proposed model combines the Convolutional Neural\nNetwork (CNN) and Bi-Directional Long Short Term Memory (BiLSTM) based\nRecurrent Neural Network (RNN). Basically, we train the model using compressed\ndomain images and observe a very appealing performance with 89.05% word\nrecognition accuracy and 13.37% character error rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajesh_B/0/1/0/all/0/1\">Bulla Rajesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhishek Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1\">Ayush Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1\">Mohammed Javed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01293","description":"<p>This paper presents a transformer-based Siamese network architecture\n(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of\nco-registered remote sensing images. Different from recent CD frameworks, which\nare based on fully convolutional networks (ConvNets), the proposed method\nunifies hierarchically structured transformer encoder with Multi-Layer\nPerception (MLP) decoder in a Siamese network architecture to efficiently\nrender multi-scale long-range details required for accurate CD. Experiments on\ntwo CD datasets show that the proposed end-to-end trainable ChangeFormer\narchitecture achieves better CD performance than previous counterparts. Our\ncode is available at https://github.com/wgcban/ChangeFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Variational State Space Filtering. (arXiv:2201.01353v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.01353","description":"<p>We introduce Variational State-Space Filters (VSSF), a new method for\nunsupervised learning, identification, and filtering of latent Markov state\nspace models from raw pixels. We present a theoretically sound framework for\nlatent state space inference under heterogeneous sensor configurations. The\nresulting model can integrate an arbitrary subset of the sensor measurements\nused during training, enabling the learning of semi-supervised state\nrepresentations, thus enforcing that certain components of the learned latent\nstate space to agree with interpretable measurements. From this framework we\nderive L-VSSF, an explicit instantiation of this model with linear latent\ndynamics and Gaussian distribution parameterizations. We experimentally\ndemonstrate L-VSSF's ability to filter in latent space beyond the sequence\nlength of the training dataset across several different test environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfrommer_D/0/1/0/all/0/1\">Daniel Pfrommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1\">Nikolai Matni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Scene Parsing: From Tile-level Scene Classification to Pixel-wise Semantic Labeling. (arXiv:2201.01953v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01953","description":"<p>Given an aerial image, aerial scene parsing (ASP) targets to interpret the\nsemantic structure of the image content, e.g., by assigning a semantic label to\nevery pixel of the image. With the popularization of data-driven methods, the\npast decades have witnessed promising progress on ASP by approaching the\nproblem with the schemes of tile-level scene classification or\nsegmentation-based image analysis, when using high-resolution aerial images.\nHowever, the former scheme often produces results with tile-wise boundaries,\nwhile the latter one needs to handle the complex modeling process from pixels\nto semantics, which often requires large-scale and well-annotated image samples\nwith pixel-wise semantic labels. In this paper, we address these issues in ASP,\nwith perspectives from tile-level scene classification to pixel-wise semantic\nlabeling. Specifically, we first revisit aerial image interpretation by a\nliterature review. We then present a large-scale scene classification dataset\nthat contains one million aerial images termed Million-AID. With the presented\ndataset, we also report benchmarking experiments using classical convolutional\nneural networks (CNNs). Finally, we perform ASP by unifying the tile-level\nscene classification and object-based image analysis to achieve pixel-wise\nsemantic labeling. Intensive experiments show that Million-AID is a challenging\nyet useful dataset, which can serve as a benchmark for evaluating newly\ndeveloped algorithms. When transferring knowledge from Million-AID, fine-tuning\nCNN models pretrained on Million-AID perform consistently better than those\npretrained ImageNet for aerial scene classification. Moreover, our designed\nhierarchical multi-task learning method achieves the state-of-the-art\npixel-wise classification on the challenging GID, bridging the tile-level scene\nclassification toward pixel-wise semantic labeling for aerial image\ninterpretation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deren Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02428","description":"<p>Today, deep convolutional neural networks (CNNs) have demonstrated\nstate-of-the-art performance for medical image segmentation, on various imaging\nmodalities and tasks. Despite early success, segmentation networks may still\ngenerate anatomically aberrant segmentations, with holes or inaccuracies near\nthe object boundaries. To enforce anatomical plausibility, recent research\nstudies have focused on incorporating prior knowledge such as object shape or\nboundary, as constraints in the loss function. Prior integrated could be\nlow-level referring to reformulated representations extracted from the\nground-truth segmentations, or high-level representing external medical\ninformation such as the organ's shape or size. Over the past few years,\nprior-based losses exhibited a rising interest in the research field since they\nallow integration of expert knowledge while still being architecture-agnostic.\nHowever, given the diversity of prior-based losses on different medical imaging\nchallenges and tasks, it has become hard to identify what loss works best for\nwhich dataset. In this paper, we establish a benchmark of recent prior-based\nlosses for medical image segmentation. The main objective is to provide\nintuition onto which losses to choose given a particular task or dataset. To\nthis end, four low-level and high-level prior-based losses are selected. The\nconsidered losses are validated on 8 different datasets from a variety of\nmedical image segmentation challenges including the Decathlon, the ISLES and\nthe WMH challenge. Results show that whereas low-level prior-based losses can\nguarantee an increase in performance over the Dice loss baseline regardless of\nthe dataset characteristics, high-level prior-based losses can increase\nanatomical plausibility as per data characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jurdi_R/0/1/0/all/0/1\">Rosana El Jurdi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petitjean_C/0/1/0/all/0/1\">Caroline Petitjean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honeine_P/0/1/0/all/0/1\">Paul Honeine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdallah_F/0/1/0/all/0/1\">Fahed Abdallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Summarization Based on Video-text Modelling. (arXiv:2201.02494v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02494","description":"<p>Modern video summarization methods are based on deep neural networks which\nrequire a large amount of annotated data for training. However, existing\ndatasets for video summarization are small-scale, easily leading to\nover-fitting of the deep models. Considering that the annotation of large-scale\ndatasets is time-consuming, we propose a multimodal self-supervised learning\nframework to obtain semantic representations of videos, which benefits the\nvideo summarization task. Specifically, we explore the semantic consistency\nbetween the visual information and text information of videos, for the\nself-supervised pretraining of a multimodal encoder on a newly-collected\ndataset of video-text pairs. Additionally, we introduce a progressive video\nsummarization method, where the important content in a video is pinpointed\nprogressively to generate better summaries. Finally, an objective evaluation\nframework is proposed to measure the quality of video summaries based on video\nclassification. Extensive experiments have proved the effectiveness and\nsuperiority of our method in rank correlation coefficients, F-score, and the\nproposed objective evaluation compared to the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haopeng_L/0/1/0/all/0/1\">Li Haopeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiuhong_K/0/1/0/all/0/1\">Ke Qiuhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingming_G/0/1/0/all/0/1\">Gong Mingming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_Z/0/1/0/all/0/1\">Zhang Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Incremental Learning Driven Instance Segmentation Framework to Recognize Highly Cluttered Instances of the Contraband Items. (arXiv:2201.02560v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02560","description":"<p>Screening cluttered and occluded contraband items from baggage X-ray scans is\na cumbersome task even for the expert security staff. This paper presents a\nnovel strategy that extends a conventional encoder-decoder architecture to\nperform instance-aware segmentation and extract merged instances of contraband\nitems without using any additional sub-network or an object detector. The\nencoder-decoder network first performs conventional semantic segmentation and\nretrieves cluttered baggage items. The model then incrementally evolves during\ntraining to recognize individual instances using significantly reduced training\nbatches. To avoid catastrophic forgetting, a novel objective function minimizes\nthe network loss in each iteration by retaining the previously acquired\nknowledge while learning new class representations and resolving their complex\nstructural inter-dependencies through Bayesian inference. A thorough evaluation\nof our framework on two publicly available X-ray datasets shows that it\noutperforms state-of-the-art methods, especially within the challenging\ncluttered scenarios, while achieving an optimal trade-off between detection\naccuracy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_T/0/1/0/all/0/1\">Taimur Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1\">Mohammed Bennamoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Twenty-thousand Classes using Image-level Supervision. (arXiv:2201.02605v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02605","description":"<p>Current object detectors are limited in vocabulary size due to the small\nscale of detection datasets. Image classifiers, on the other hand, reason about\nmuch larger vocabularies, as their datasets are larger and easier to collect.\nWe propose Detic, which simply trains the classifiers of a detector on image\nclassification data and thus expands the vocabulary of detectors to tens of\nthousands of concepts. Unlike prior work, Detic does not assign image labels to\nboxes based on model predictions, making it much easier to implement and\ncompatible with a range of detection architectures and backbones. Our results\nshow that Detic yields excellent detectors even for classes without box\nannotations. It outperforms prior work on both open-vocabulary and long-tail\ndetection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3\nmAP for novel classes on the open-vocabulary LVIS benchmark. On the standard\nLVIS benchmark, Detic reaches 41.7 mAP for all classes and 41.7 mAP for rare\nclasses. For the first time, we train a detector with all the\ntwenty-one-thousand classes of the ImageNet dataset and show that it\ngeneralizes to new datasets without fine-tuning. Code is available at\nhttps://github.com/facebookresearch/Detic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1\">Rohit Girdhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Phillip Kr&#xe4;henb&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}