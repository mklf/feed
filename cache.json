{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Feature Extraction based Model for Hate Speech Identification. (arXiv:2201.04227v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04227","description":"<p>The detection of hate speech online has become an important task, as\noffensive language such as hurtful, obscene and insulting content can harm\nmarginalized people or groups. This paper presents TU Berlin team experiments\nand results on the task 1A and 1B of the shared task on hate speech and\noffensive content identification in Indo-European languages 2021. The success\nof different Natural Language Processing models is evaluated for the respective\nsubtasks throughout the competition. We tested different models based on\nrecurrent neural networks in word and character levels and transfer learning\napproaches based on Bert on the provided dataset by the competition. Among the\ntested models that have been used for the experiments, the transfer\nlearning-based models achieved the best results in both subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohtaj_S/0/1/0/all/0/1\">Salar Mohtaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_V/0/1/0/all/0/1\">Vera Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhysNLU: A Language Resource for Evaluating Natural Language Understanding and Explanation Coherence in Physics. (arXiv:2201.04275v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04275","description":"<p>In order for language models to aid physics research, they must first encode\nrepresentations of mathematical and natural language discourse which lead to\ncoherent explanations, with correct ordering and relevance of statements. We\npresent a collection of datasets developed to evaluate the performance of\nlanguage models in this regard, which measure capabilities with respect to\nsentence ordering, position, section prediction, and discourse coherence.\nAnalysis of the data reveals equations and sub-disciplines which are most\ncommon in physics discourse, as well as the sentence-level frequency of\nequations and expressions. We present baselines which demonstrate how\ncontemporary language models are challenged by coherence related tasks in\nphysics, even when trained on mathematical natural language objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meadows_J/0/1/0/all/0/1\">Jordan Meadows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptBERT: Improving BERT Sentence Embeddings with Prompts. (arXiv:2201.04337v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04337","description":"<p>The poor performance of the original BERT for sentence semantic similarity\nhas been widely discussed in previous works. We find that unsatisfactory\nperformance is mainly due to the static token embeddings biases and the\nineffective BERT layers, rather than the high cosine similarity of the sentence\nembeddings. To this end, we propose a prompt based sentence embeddings method\nwhich can reduce token embeddings biases and make the original BERT layers more\neffective. By reformulating the sentence embeddings task as the\nfillin-the-blanks problem, our method significantly improves the performance of\noriginal BERT. We discuss two prompt representing methods and three prompt\nsearching methods for prompt based sentence embeddings. Moreover, we propose a\nnovel unsupervised training objective by the technology of template denoising,\nwhich substantially shortens the performance gap between the supervised and\nunsupervised setting. For experiments, we evaluate our method on both non\nfine-tuned and fine-tuned settings. Even a non fine-tuned method can outperform\nthe fine-tuned methods like unsupervised ConSERT on STS tasks. Our fine-tuned\nmethod outperforms the state-of-the-art method SimCSE in both unsupervised and\nsupervised settings. Compared to SimCSE, we achieve 2.29 and 2.58 points\nimprovements on BERT and RoBERTa respectively under the unsupervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Ting Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Deqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1\">Fuzhen Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haizhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational analyses of the topics, sentiments, literariness, creativity and beauty of texts in a large Corpus of English Literature. (arXiv:2201.04356v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04356","description":"<p>The Gutenberg Literary English Corpus (GLEC, Jacobs, 2018a) provides a rich\nsource of textual data for research in digital humanities, computational\nlinguistics or neurocognitive poetics. In this study we address differences\namong the different literature categories in GLEC, as well as differences\nbetween authors. We report the results of three studies providing i) topic and\nsentiment analyses for six text categories of GLEC (i.e., children and youth,\nessays, novels, plays, poems, stories) and its &gt;100 authors, ii) novel measures\nof semantic complexity as indices of the literariness, creativity and book\nbeauty of the works in GLEC (e.g., Jane Austen's six novels), and iii) two\nexperiments on text classification and authorship recognition using novel\nfeatures of semantic complexity. The data on two novel measures estimating a\ntext's literariness, intratextual variance and stepwise distance (van\nCranenburgh et al., 2019) revealed that plays are the most literary texts in\nGLEC, followed by poems and novels. Computation of a novel index of text\ncreativity (Gray et al., 2016) revealed poems and plays as the most creative\ncategories with the most creative authors all being poets (Milton, Pope, Keats,\nByron, or Wordsworth). We also computed a novel index of perceived beauty of\nverbal art (Kintsch, 2012) for the works in GLEC and predict that Emma is the\ntheoretically most beautiful of Austen's novels. Finally, we demonstrate that\nthese novel measures of semantic complexity are important features for text\nclassification and authorship recognition with overall predictive accuracies in\nthe range of .75 to .97. Our data pave the way for future computational and\nempirical studies of literature or experiments in reading psychology and offer\nmultiple baselines and benchmarks for analysing and validating other book\ncorpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_A/0/1/0/all/0/1\">Arthur M. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinder_A/0/1/0/all/0/1\">Annette Kinder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Modeling on Podcast Short-Text Metadata. (arXiv:2201.04419v1 [cs.IR])","link":"http://arxiv.org/abs/2201.04419","description":"<p>Podcasts have emerged as a massively consumed online content, notably due to\nwider accessibility of production means and scaled distribution through large\nstreaming platforms. Categorization systems and information access technologies\ntypically use topics as the primary way to organize or navigate podcast\ncollections. However, annotating podcasts with topics is still quite\nproblematic because the assigned editorial genres are broad, heterogeneous or\nmisleading, or because of data challenges (e.g. short metadata text, noisy\ntranscripts). Here, we assess the feasibility to discover relevant topics from\npodcast metadata, titles and descriptions, using topic modeling techniques for\nshort text. We also propose a new strategy to leverage named entities (NEs),\noften present in podcast metadata, in a Non-negative Matrix Factorization (NMF)\ntopic modeling framework. Our experiments on two existing datasets from Spotify\nand iTunes and Deezer, a new dataset from an online service providing a catalog\nof podcasts, show that our proposed document representation, NEiCE, leads to\nimproved topic coherence over the baselines. We release the code for\nexperimental reproducibility of the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valero_F/0/1/0/all/0/1\">Francisco B. Valero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baranes_M/0/1/0/all/0/1\">Marion Baranes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epure_E/0/1/0/all/0/1\">Elena V. Epure</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiating Geographic Movement Described in Text Documents. (arXiv:2201.04427v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04427","description":"<p>Understanding movement described in text documents is important since text\ndescriptions of movement contain a wealth of geographic and contextual\ninformation about the movement of people, wildlife, goods, and much more. Our\nresearch makes several contributions to improve our understanding of movement\ndescriptions in text. First, we show how interpreting geographic movement\ndescribed in text is challenging because of general spatial terms, linguistic\nconstructions that make the thing(s) moving unclear, and many types of temporal\nreferences and groupings, among others. Next, as a step to overcome these\nchallenges, we report on an experiment with human subjects through which we\nidentify multiple important characteristics of movement descriptions (found in\ntext) that humans use to differentiate one movement description from another.\nBased on our empirical results, we provide recommendations for computational\nanalysis using movement described in text documents. Our findings contribute\ntowards an improved understanding of the important characteristics of the\nunderused information about geographic movement that is in the form of text\ndescriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pezanowski_S/0/1/0/all/0/1\">Scott Pezanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacEachren_A/0/1/0/all/0/1\">Alan M. MacEachren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Prasenjit Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biaffine Discourse Dependency Parsing. (arXiv:2201.04450v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04450","description":"<p>We provide a study of using the biaffine model for neural discourse\ndependency parsing and achieve significant performance improvement compared\nwith the baseline parsers. We compare the Eisner algorithm and the\nChu-Liu-Edmonds algorithm in the task and find that using the Chu-Liu-Edmonds\nalgorithm generates deeper trees and achieves better performance. We also\nevaluate the structure of the output of the parser with average maximum path\nlength and average proportion of leaf nodes and find that the dependency trees\ngenerated by the parser are close to the gold trees. As the corpus allows\nnon-projective structures, we analyze the complexity of non-projectivity of the\ncorpus and find that the dependency structures in this corpus have gap degree\nat most one and edge degree at most one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingxue Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diagnosing BERT with Retrieval Heuristics. (arXiv:2201.04458v1 [cs.IR])","link":"http://arxiv.org/abs/2201.04458","description":"<p>Word embeddings, made widely popular in 2013 with the release of word2vec,\nhave become a mainstay of NLP engineering pipelines. Recently, with the release\nof BERT, word embeddings have moved from the term-based embedding space to the\ncontextual embedding space -- each term is no longer represented by a single\nlow-dimensional vector but instead each term and \\emph{its context} determine\nthe vector weights. BERT's setup and architecture have been shown to be general\nenough to be applicable to many natural language tasks. Importantly for\nInformation Retrieval (IR), in contrast to prior deep learning solutions to IR\nproblems which required significant tuning of neural net architectures and\ntraining regimes, \"vanilla BERT\" has been shown to outperform existing\nretrieval algorithms by a wide margin, including on tasks and corpora that have\nlong resisted retrieval effectiveness gains over traditional IR baselines (such\nas Robust04). In this paper, we employ the recently proposed axiomatic dataset\nanalysis technique -- that is, we create diagnostic datasets that each fulfil a\nretrieval heuristic (both term matching and semantic-based) -- to explore what\nBERT is able to learn. In contrast to our expectations, we find BERT, when\napplied to a recently released large-scale web corpus with ad-hoc topics, to\n\\emph{not} adhere to any of the explored axioms. At the same time, BERT\noutperforms the traditional query likelihood retrieval model by 40\\%. This\nmeans that the axiomatic approach to IR (and its extension of diagnostic\ndatasets created for retrieval heuristics) may in its current form not be\napplicable to large-scale corpora. Additional -- different -- axioms are\nneeded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camara_A/0/1/0/all/0/1\">Arthur C&#xe2;mara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauff_C/0/1/0/all/0/1\">Claudia Hauff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Data Corruption Affect Natural Language Understanding Models? A Study on GLUE datasets. (arXiv:2201.04467v1 [cs.CL])","link":"http://arxiv.org/abs/2201.04467","description":"<p>A central question in natural language understanding (NLU) research is\nwhether high performance demonstrates the models' strong reasoning\ncapabilities. We present an extensive series of controlled experiments where\npre-trained language models are exposed to data that have undergone specific\ncorruption transformations. The transformations involve removing instances of\nspecific word classes and often lead to non-sensical sentences. Our results\nshow that performance remains high for most GLUE tasks when the models are\nfine-tuned or tested on corrupted data, suggesting that the models leverage\nother cues for prediction even in non-sensical contexts. Our proposed data\ntransformations can be used as a diagnostic tool for assessing the extent to\nwhich a specific dataset constitutes a proper testbed for evaluating models'\nlanguage understanding capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talman_A/0/1/0/all/0/1\">Aarne Talman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzikyriakidis_S/0/1/0/all/0/1\">Stergios Chatzikyriakidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1\">J&#xf6;rg Tiedemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interacting with Explanations through Critiquing. (arXiv:2005.11067v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.11067","description":"<p>Using personalized explanations to support recommendations has been shown to\nincrease trust and perceived quality. However, to actually obtain better\nrecommendations, there needs to be a means for users to modify the\nrecommendation criteria by interacting with the explanation. We present a novel\ntechnique using aspect markers that learns to generate personalized\nexplanations of recommendations from review texts, and we show that human users\nsignificantly prefer these explanations over those produced by state-of-the-art\ntechniques. Our work's most important innovation is that it allows users to\nreact to a recommendation by critiquing the textual explanation: removing\n(symmetrically adding) certain aspects they dislike or that are no longer\nrelevant (symmetrically that are of interest). The system updates its user\nmodel and the resulting recommendations according to the critique. This is\nbased on a novel unsupervised critiquing method for single- and multi-step\ncritiquing with textual explanations. Experiments on two real-world datasets\nshow that our system is the first to achieve good performance in adapting to\nthe preferences expressed in multi-step critiquing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03481","description":"<p>Contrastive learning models have achieved great success in unsupervised\nvisual representation learning, which maximize the similarities between feature\nrepresentations of different views of the same image, while minimize the\nsimilarities between feature representations of views of different images. In\ntext summarization, the output summary is a shorter form of the input document\nand they have similar meanings. In this paper, we propose a contrastive\nlearning model for supervised abstractive text summarization, where we view a\ndocument, its gold summary and its model generated summaries as different views\nof the same mean representation and maximize the similarities between them\nduring training. We improve over a strong sequence-to-sequence text generation\nmodel (i.e., BART) on three different summarization datasets. Human evaluation\nalso shows that our model achieves better faithfulness ratings compared to its\ncounterpart without contrastive objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shusheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05052","description":"<p>Knowledge-dependent tasks typically use two sources of knowledge: parametric,\nlearned at training time, and contextual, given as a passage at inference time.\nTo understand how models use these sources together, we formalize the problem\nof knowledge conflicts, where the contextual information contradicts the\nlearned information. Analyzing the behaviour of popular models, we measure\ntheir over-reliance on memorized information (the cause of hallucinations), and\nuncover important factors that exacerbate this behaviour. Lastly, we propose a\nsimple method to mitigate over-reliance on parametric knowledge, which\nminimizes hallucination, and improves out-of-distribution generalization by\n4%-7%. Our findings demonstrate the importance for practitioners to evaluate\nmodel tendency to hallucinate rather than read, and show that our mitigation\nstrategy encourages generalization to evolving information (i.e.,\ntime-dependent queries). To encourage these practices, we have released our\nframework for generating knowledge conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perisetla_K/0/1/0/all/0/1\">Kartik Perisetla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_N/0/1/0/all/0/1\">Nikhil Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Chris DuBois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text is no more Enough! A Benchmark for Profile-based Spoken Language Understanding. (arXiv:2112.11953v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.11953","description":"<p>Current researches on spoken language understanding (SLU) heavily are limited\nto a simple setting: the plain text-based SLU that takes the user utterance as\ninput and generates its corresponding semantic frames (e.g., intent and slots).\nUnfortunately, such a simple setting may fail to work in complex real-world\nscenarios when an utterance is semantically ambiguous, which cannot be achieved\nby the text-based SLU models. In this paper, we first introduce a new and\nimportant task, Profile-based Spoken Language Understanding (ProSLU), which\nrequires the model that not only relies on the plain text but also the\nsupporting profile information to predict the correct intents and slots. To\nthis end, we further introduce a large-scale human-annotated Chinese dataset\nwith over 5K utterances and their corresponding supporting profile information\n(Knowledge Graph (KG), User Profile (UP), Context Awareness (CA)). In addition,\nwe evaluate several state-of-the-art baseline models and explore a multi-level\nknowledge adapter to effectively incorporate profile information. Experimental\nresults reveal that all existing text-based SLU models fail to work when the\nutterances are semantically ambiguous and our proposed framework can\neffectively fuse the supporting information for sentence-level intent detection\nand token-level slot filling. Finally, we summarize key challenges and provide\nnew points for future directions, which hopes to facilitate the research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiji Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Overview of the HECKTOR Challenge at MICCAI 2021: Automatic Head and Neck Tumor Segmentation and Outcome Prediction in PET/CT Images. (arXiv:2201.04138v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04138","description":"<p>This paper presents an overview of the second edition of the HEad and neCK\nTumOR (HECKTOR) challenge, organized as a satellite event of the 24th\nInternational Conference on Medical Image Computing and Computer Assisted\nIntervention (MICCAI) 2021. The challenge is composed of three tasks related to\nthe automatic analysis of PET/CT images for patients with Head and Neck cancer\n(H&amp;N), focusing on the oropharynx region. Task 1 is the automatic segmentation\nof H&amp;N primary Gross Tumor Volume (GTVt) in FDG-PET/CT images. Task 2 is the\nautomatic prediction of Progression Free Survival (PFS) from the same\nFDG-PET/CT. Finally, Task 3 is the same as Task 2 with ground truth GTVt\nannotations provided to the participants. The data were collected from six\ncenters for a total of 325 images, split into 224 training and 101 testing\ncases. The interest in the challenge was highlighted by the important\nparticipation with 103 registered teams and 448 result submissions. The best\nmethods obtained a Dice Similarity Coefficient (DSC) of 0.7591 in the first\ntask, and a Concordance index (C-index) of 0.7196 and 0.6978 in Tasks 2 and 3,\nrespectively. In all tasks, simplicity of the approach was found to be key to\nensure generalization performance. The comparison of the PFS prediction\nperformance in Tasks 2 and 3 suggests that providing the GTVt contour was not\ncrucial to achieve best results, which indicates that fully automatic methods\ncan be used. This potentially obviates the need for GTVt contouring, opening\navenues for reproducible and large scale radiomics studies including thousands\npotential subjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Andrearczyk_V/0/1/0/all/0/1\">Vincent Andrearczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oreiller_V/0/1/0/all/0/1\">Valentin Oreiller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boughdad_S/0/1/0/all/0/1\">Sarah Boughdad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rest_C/0/1/0/all/0/1\">Catherine Chez Le Rest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elhalawani_H/0/1/0/all/0/1\">Hesham Elhalawani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jreige_M/0/1/0/all/0/1\">Mario Jreige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prior_J/0/1/0/all/0/1\">John O. Prior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vallieres_M/0/1/0/all/0/1\">Martin Valli&#xe8;res</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visvikis_D/0/1/0/all/0/1\">Dimitris Visvikis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hatt_M/0/1/0/all/0/1\">Mathieu Hatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Depeursinge_A/0/1/0/all/0/1\">Adrien Depeursinge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning. (arXiv:2201.04182v1 [cs.LG])","link":"http://arxiv.org/abs/2201.04182","description":"<p>In this work we propose a HyperTransformer, a transformer-based model for\nfew-shot learning that generates weights of a convolutional neural network\n(CNN) directly from support samples. Since the dependence of a small generated\nCNN model on a specific task is encoded by a high-capacity transformer model,\nwe effectively decouple the complexity of the large task space from the\ncomplexity of individual tasks. Our method is particularly effective for small\ntarget CNN architectures where learning a fixed universal task-independent\nembedding is not optimal and better performance is attained when the\ninformation about the task can modulate all model parameters. For larger models\nwe discover that generating the last layer alone allows us to produce\ncompetitive or better results than those obtained with state-of-the-art methods\nwhile being end-to-end differentiable. Finally, we extend our approach to a\nsemi-supervised regime utilizing unlabeled samples in the support set and\nfurther improving few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Capacitance: A New Perspective of Neural Network Selection via Edge Dynamics. (arXiv:2201.04194v1 [cs.LG])","link":"http://arxiv.org/abs/2201.04194","description":"<p>Efficient model selection for identifying a suitable pre-trained neural\nnetwork to a downstream task is a fundamental yet challenging task in deep\nlearning. Current practice requires expensive computational costs in model\ntraining for performance prediction. In this paper, we propose a novel\nframework for neural network selection by analyzing the governing dynamics over\nsynaptic connections (edges) during training. Our framework is built on the\nfact that back-propagation during neural network training is equivalent to the\ndynamical evolution of synaptic connections. Therefore, a converged neural\nnetwork is associated with an equilibrium state of a networked system composed\nof those edges. To this end, we construct a network mapping $\\phi$, converting\na neural network $G_A$ to a directed line graph $G_B$ that is defined on those\nedges in $G_A$. Next, we derive a neural capacitance metric $\\beta_{\\rm eff}$\nas a predictive measure universally capturing the generalization capability of\n$G_A$ on the downstream task using only a handful of early training results. We\ncarried out extensive experiments using 17 popular pre-trained ImageNet models\nand five benchmark datasets, including CIFAR10, CIFAR100, SVHN, Fashion MNIST\nand Birds, to evaluate the fine-tuning performance of our framework. Our neural\ncapacitance metric is shown to be a powerful indicator for model selection\nbased only on early training results and is more efficient than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chunheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedapati_T/0/1/0/all/0/1\">Tejaswini Pedapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianxi Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDPose: Human Skeletal Motion Reconstruction Using WiFi Micro-Doppler Signatures. (arXiv:2201.04212v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04212","description":"<p>Motion tracking systems based on optical sensors typically often suffer from\nissues, such as poor lighting conditions, occlusion, limited coverage, and may\nraise privacy concerns. More recently, radio frequency (RF)-based approaches\nusing commercial WiFi devices have emerged which offer low-cost ubiquitous\nsensing whilst preserving privacy. However, the output of an RF sensing system,\nsuch as Range-Doppler spectrograms, cannot represent human motion intuitively\nand usually requires further processing. In this study, MDPose, a novel\nframework for human skeletal motion reconstruction based on WiFi micro-Doppler\nsignatures, is proposed. It provides an effective solution to track human\nactivities by reconstructing a skeleton model with 17 key points, which can\nassist with the interpretation of conventional RF sensing outputs in a more\nunderstandable way. Specifically, MDPose has various incremental stages to\ngradually address a series of challenges: First, a denoising algorithm is\nimplemented to remove any unwanted noise that may affect the feature extraction\nand enhance weak Doppler signatures. Secondly, the convolutional neural network\n(CNN)-recurrent neural network (RNN) architecture is applied to learn\ntemporal-spatial dependency from clean micro-Doppler signatures and restore key\npoints' velocity information. Finally, a pose optimising mechanism is employed\nto estimate the initial state of the skeleton and to limit the increase of\nerror. We have conducted comprehensive tests in a variety of environments using\nnumerous subjects with a single receiver radar system to demonstrate the\nperformance of MDPose, and report 29.4mm mean absolute error over all key\npoints positions, which outperforms state-of-the-art RF-based pose estimation\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_S/0/1/0/all/0/1\">Shelly Vishwakarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Fangzhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julier_S/0/1/0/all/0/1\">Simon Julier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetty_K/0/1/0/all/0/1\">Kevin Chetty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-based Layout Analysis of Music Score Images. (arXiv:2201.04214v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04214","description":"<p>The Layout Analysis (LA) stage is of vital importance to the correct\nperformance of an Optical Music Recognition (OMR) system. It identifies the\nregions of interest, such as staves or lyrics, which must then be processed in\norder to transcribe their content. Despite the existence of modern approaches\nbased on deep learning, an exhaustive study of LA in OMR has not yet been\ncarried out with regard to the precision of different models, their\ngeneralization to different domains or, more importantly, their impact on\nsubsequent stages of the pipeline. This work focuses on filling this gap in\nliterature by means of an experimental study of different neural architectures,\nmusic document types and evaluation scenarios. The need for training data has\nalso led to a proposal for a new semi-synthetic data generation technique that\nenables the efficient applicability of LA approaches in real scenarios. Our\nresults show that: (i) the choice of the model and its performance are crucial\nfor the entire transcription process; (ii) the metrics commonly used to\nevaluate the LA stage do not always correlate with the final performance of the\nOMR system, and (iii) the proposed data-generation technique enables\nstate-of-the-art results to be achieved with a limited set of labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castellanos_F/0/1/0/all/0/1\">Francisco J. Castellanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrido_Munoz_C/0/1/0/all/0/1\">Carlos Garrido-Munoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_Vila_A/0/1/0/all/0/1\">Antonio R&#xed;os-Vila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calvo_Zaragoza_J/0/1/0/all/0/1\">Jorge Calvo-Zaragoza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain Signals Analysis Based Deep Learning Methods: Recent advances in the study of non-invasive brain signals. (arXiv:2201.04229v1 [q-bio.NC])","link":"http://arxiv.org/abs/2201.04229","description":"<p>Brain signals constitute the information that are processed by millions of\nbrain neurons (nerve cells and brain cells). These brain signals can be\nrecorded and analyzed using various of non-invasive techniques such as the\nElectroencephalograph (EEG), Magneto-encephalograph (MEG) as well as\nbrain-imaging techniques such as Magnetic Resonance Imaging (MRI), Computed\nTomography (CT) and others, which will be discussed briefly in this paper. This\npaper discusses about the currently emerging techniques such as the usage of\ndifferent Deep Learning (DL) algorithms for the analysis of these brain signals\nand how these algorithms will be helpful in determining the neurological status\nof a person by applying the signal decoding strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Essa_A/0/1/0/all/0/1\">Almabrok Essa</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kotte_H/0/1/0/all/0/1\">Hari Kotte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmartDet: Context-Aware Dynamic Control of Edge Task Offloading for Mobile Object Detection. (arXiv:2201.04235v1 [cs.DC])","link":"http://arxiv.org/abs/2201.04235","description":"<p>Mobile devices increasingly rely on object detection (OD) through deep neural\nnetworks (DNNs) to perform critical tasks. Due to their high complexity, the\nexecution of these DNNs requires excessive time and energy. Low-complexity\nobject tracking (OT) can be used with OD, where the latter is periodically\napplied to generate \"fresh\" references for tracking. However, the frames\nprocessed with OD incur large delays, which may make the reference outdated and\ndegrade tracking quality. Herein, we propose to use edge computing in this\ncontext, and establish parallel OT (at the mobile device) and OD (at the edge\nserver) processes that are resilient to large OD latency. We propose Katch-Up,\na novel tracking mechanism that improves the system resilience to excessive OD\ndelay. However, while Katch-Up significantly improves performance, it also\nincreases the computing load of the mobile device. Hence, we design SmartDet, a\nlow-complexity controller based on deep reinforcement learning (DRL) that\nlearns controlling the trade-off between resource utilization and OD\nperformance. SmartDet takes as input context-related information related to the\ncurrent video content and the current network conditions to optimize frequency\nand type of OD offloading, as well as Katch-Up utilization. We extensively\nevaluate SmartDet on a real-world testbed composed of a JetSon Nano as mobile\ndevice and a GTX 980 Ti as edge server, connected through a Wi-Fi link.\nExperimental results show that SmartDet achieves an optimal balance between\ntracking performance - mean Average Recall (mAR) and resource usage. With\nrespect to a baseline with full Katch-Upusage and maximum channel usage, we\nstill increase mAR by 4% while using 50% less of the channel and 30% power\nresources associated with Katch-Up. With respect to a fixed strategy using\nminimal resources, we increase mAR by 20% while using Katch-Up on 1/3 of the\nframes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Callegaro_D/0/1/0/all/0/1\">Davide Callegaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restuccia_F/0/1/0/all/0/1\">Francesco Restuccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incidents1M: a large-scale dataset of images with natural disasters, damage, and incidents. (arXiv:2201.04236v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04236","description":"<p>Natural disasters, such as floods, tornadoes, or wildfires, are increasingly\npervasive as the Earth undergoes global warming. It is difficult to predict\nwhen and where an incident will occur, so timely emergency response is critical\nto saving the lives of those endangered by destructive events. Fortunately,\ntechnology can play a role in these situations. Social media posts can be used\nas a low-latency data source to understand the progression and aftermath of a\ndisaster, yet parsing this data is tedious without automated methods. Prior\nwork has mostly focused on text-based filtering, yet image and video-based\nfiltering remains largely unexplored. In this work, we present the Incidents1M\nDataset, a large-scale multi-label dataset which contains 977,088 images, with\n43 incident and 49 place categories. We provide details of the dataset\nconstruction, statistics and potential biases; introduce and train a model for\nincident detection; and perform image-filtering experiments on millions of\nimages on Flickr and Twitter. We also present some applications on incident\nanalysis to encourage and enable future work in computer vision for\nhumanitarian aid. Code, data, and models are available at\n<a href=\"http://incidentsdataset.csail.mit.edu.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_E/0/1/0/all/0/1\">Ethan Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_D/0/1/0/all/0/1\">Dim P. Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamical Audio-Visual Navigation: Catching Unheard Moving Sound Sources in Unmapped 3D Environments. (arXiv:2201.04279v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04279","description":"<p>Recent work on audio-visual navigation targets a single static sound in\nnoise-free audio environments and struggles to generalize to unheard sounds. We\nintroduce the novel dynamic audio-visual navigation benchmark in which an\nembodied AI agent must catch a moving sound source in an unmapped environment\nin the presence of distractors and noisy sounds. We propose an end-to-end\nreinforcement learning approach that relies on a multi-modal architecture that\nfuses the spatial audio-visual information from a binaural audio signal and\nspatial occupancy maps to encode the features needed to learn a robust\nnavigation policy for our new complex task settings. We demonstrate that our\napproach outperforms the current state-of-the-art with better generalization to\nunheard sounds and better robustness to noisy scenarios on the two challenging\n3D scanned real-world datasets Replica and Matterport3D, for the static and\ndynamic audio-visual navigation benchmarks. Our novel benchmark will be made\navailable at <a href=\"http://dav-nav.cs.uni-freiburg.de.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Younes_A/0/1/0/all/0/1\">Abdelrahman Younes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiview Transformers for Video Recognition. (arXiv:2201.04288v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04288","description":"<p>Video understanding requires reasoning at multiple spatiotemporal resolutions\n-- from short fine-grained motions to events taking place over longer\ndurations. Although transformer architectures have recently advanced the\nstate-of-the-art, they have not explicitly modelled different spatiotemporal\nresolutions. To this end, we present Multiview Transformers for Video\nRecognition (MTV). Our model consists of separate encoders to represent\ndifferent views of the input video with lateral connections to fuse information\nacross views. We present thorough ablation studies of our model and show that\nMTV consistently performs better than single-view counterparts in terms of\naccuracy and computational cost across a range of model sizes. Furthermore, we\nachieve state-of-the-art results on five standard datasets, and improve even\nfurther with large-scale pretraining. We will release code and pretrained\ncheckpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1\">Xuehan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Contrastive Learning against Noisy Views. (arXiv:2201.04309v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04309","description":"<p>Contrastive learning relies on an assumption that positive pairs contain\nrelated views, e.g., patches of an image or co-occurring multimodal signals of\na video, that share certain underlying information about an instance. But what\nif this assumption is violated? The literature suggests that contrastive\nlearning produces suboptimal representations in the presence of noisy views,\ne.g., false positive pairs with no apparent shared information. In this work,\nwe propose a new contrastive loss function that is robust against noisy views.\nWe provide rigorous theoretical justifications by showing connections to robust\nsymmetric losses for noisy binary classification and by establishing a new\ncontrastive bound for mutual information maximization based on the Wasserstein\ndistance measure. The proposed loss is completely modality-agnostic and a\nsimple drop-in replacement for the InfoNCE loss, which makes it easy to apply\nto existing contrastive frameworks. We show that our approach provides\nconsistent improvements over the state-of-the-art on image, video, and graph\ncontrastive learning benchmarks that exhibit a variety of real-world noise\npatterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_C/0/1/0/all/0/1\">Ching-Yao Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1\">R Devon Hjelm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neel Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegelka_S/0/1/0/all/0/1\">Stefanie Jegelka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knee Cartilage Defect Assessment by Graph Representation and Surface Convolution. (arXiv:2201.04318v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04318","description":"<p>Knee osteoarthritis (OA) is the most common osteoarthritis and a leading\ncause of disability. Cartilage defects are regarded as major manifestations of\nknee OA, which are visible by magnetic resonance imaging (MRI). Thus early\ndetection and assessment for knee cartilage defects are important for\nprotecting patients from knee OA. In this way, many attempts have been made on\nknee cartilage defect assessment by applying convolutional neural networks\n(CNNs) to knee MRI. However, the physiologic characteristics of the cartilage\nmay hinder such efforts: the cartilage is a thin curved layer, implying that\nonly a small portion of voxels in knee MRI can contribute to the cartilage\ndefect assessment; heterogeneous scanning protocols further challenge the\nfeasibility of the CNNs in clinical practice; the CNN-based knee cartilage\nevaluation results lack interpretability. To address these challenges, we model\nthe cartilages structure and appearance from knee MRI into a graph\nrepresentation, which is capable of handling highly diverse clinical data.\nThen, guided by the cartilage graph representation, we design a non-Euclidean\ndeep learning network with the self-attention mechanism, to extract cartilage\nfeatures in the local and global, and to derive the final assessment with a\nvisualized result. Our comprehensive experiments show that the proposed method\nyields superior performance in knee cartilage defect assessment, plus its\nconvenient 3D visualization for interpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zixu Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Si_L/0/1/0/all/0/1\">Liping Si</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xuan_K/0/1/0/all/0/1\">Kai Xuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouyang_X/0/1/0/all/0/1\">Xi Ouyang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhan_Y/0/1/0/all/0/1\">Yiqiang Zhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_Z/0/1/0/all/0/1\">Zhong Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lichi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_W/0/1/0/all/0/1\">Weiwu Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Residual Flow Fields for Efficient Video Representations. (arXiv:2201.04329v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04329","description":"<p>Implicit neural representation (INR) has emerged as a powerful paradigm for\nrepresenting signals, such as images, videos, 3D shapes, etc. Although it has\nshown the ability to represent fine details, its efficiency as a data\nrepresentation has not been extensively studied. In INR, the data is stored in\nthe form of parameters of a neural network and general purpose optimization\nalgorithms do not generally exploit the spatial and temporal redundancy in\nsignals. In this paper, we suggest a novel INR approach to representing and\ncompressing videos by explicitly removing data redundancy. Instead of storing\nraw RGB colors, we propose Neural Residual Flow Fields (NRFF), using motion\ninformation across video frames and residuals that are necessary to reconstruct\na video. Maintaining the motion information, which is usually smoother and less\ncomplex than the raw signals, requires far fewer parameters. Furthermore,\nreusing redundant pixel values further improves the network parameter\nefficiency. Experimental results have shown that the proposed method\noutperforms the baseline methods by a significant margin. The code is available\nin https://github.com/daniel03c1/eff_video_representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rho_D/0/1/0/all/0/1\">Daniel Rho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jong Hwan Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1\">Eunbyung Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDS-Net: A Multi-scale Depth Stratification Based Monocular 3D Object Detection Algorithm. (arXiv:2201.04341v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04341","description":"<p>Monocular 3D object detection is very challenging in autonomous driving due\nto the lack of depth information. This paper proposes a one-stage monocular 3D\nobject detection algorithm based on multi-scale depth stratification, which\nuses the anchor-free method to detect 3D objects in a per-pixel prediction. In\nthe proposed MDS-Net, a novel depth-based stratification structure is developed\nto improve the network's ability of depth prediction by establishing\nmathematical models between depth and image size of objects. A new angle loss\nfunction is then developed to further improve the accuracy of the angle\nprediction and increase the convergence speed of training. An optimized\nsoft-NMS is finally applied in the post-processing stage to adjust the\nconfidence of candidate boxes. Experiments on the KITTI benchmark show that the\nMDS-Net outperforms the existing monocular 3D detection methods in 3D detection\nand BEV detection tasks while fulfilling real-time requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhouzhen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zecheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chunyi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution. (arXiv:2201.04358v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04358","description":"<p>Reference-based super-resolution (RefSR) has made significant progress in\nproducing realistic textures using an external reference (Ref) image. However,\nexisting RefSR methods obtain high-quality correspondence matchings consuming\nquadratic computation resources with respect to the input size, limiting its\napplication. Moreover, these approaches usually suffer from scale misalignments\nbetween the low-resolution (LR) image and Ref image. In this paper, we propose\nan Accelerated Multi-Scale Aggregation network (AMSA) for Reference-based\nSuper-Resolution, including Coarse-to-Fine Embedded PatchMatch (CFE-PatchMatch)\nand Multi-Scale Dynamic Aggregation (MSDA) module. To improve matching\nefficiency, we design a novel Embedded PatchMacth scheme with random samples\npropagation, which involves end-to-end training with asymptotic linear\ncomputational cost to the input size. To further reduce computational cost and\nspeed up convergence, we apply the coarse-to-fine strategy on Embedded\nPatchMacth constituting CFE-PatchMatch. To fully leverage reference information\nacross multiple scales and enhance robustness to scale misalignment, we develop\nthe MSDA module consisting of Dynamic Aggregation and Multi-Scale Aggregation.\nThe Dynamic Aggregation corrects minor scale misalignment by dynamically\naggregating features, and the Multi-Scale Aggregation brings robustness to\nlarge scale misalignment by fusing multi-scale information. Experimental\nresults show that the proposed AMSA achieves superior performance over\nstate-of-the-art approaches on both quantitative and qualitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_Y/0/1/0/all/0/1\">Yucheng Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCSNet: An Efficient Paradigm for Learning Simultaneously Image Colorization and Super-Resolution. (arXiv:2201.04364v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04364","description":"<p>In the practical application of restoring low-resolution gray-scale images,\nwe generally need to run three separate processes of image colorization,\nsuper-resolution, and dows-sampling operation for the target device. However,\nthis pipeline is redundant and inefficient for the independent processes, and\nsome inner features could have been shared. Therefore, we present an efficient\nparadigm to perform {S}imultaneously Image {C}olorization and\n{S}uper-resolution (SCS) and propose an end-to-end SCSNet to achieve this goal.\nThe proposed method consists of two parts: colorization branch for learning\ncolor information that employs the proposed plug-and-play \\emph{Pyramid Valve\nCross Attention} (PVCAttn) module to aggregate feature maps between source and\nreference images; and super-resolution branch for integrating color and texture\ninformation to predict target images, which uses the designed \\emph{Continuous\nPixel Mapping} (CPM) module to predict high-resolution images at continuous\nmagnification. Furthermore, our SCSNet supports both automatic and referential\nmodes that is more flexible for practical application. Abundant experiments\ndemonstrate the superiority of our method for generating authentic images over\nstate-of-the-art methods, e.g., averagely decreasing FID by 1.8$\\downarrow$ and\n5.1 $\\downarrow$ compared with current best scores for automatic and\nreferential modes, respectively, while owning fewer parameters (more than\n$\\times$2$\\downarrow$) and faster running speed (more than\n$\\times$3$\\uparrow$).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yue Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Alzheimer's Disease Using 3DMgNet. (arXiv:2201.04370v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04370","description":"<p>Alzheimer's disease (AD) is an irreversible neurode generative disease of the\nbrain.The disease may causes memory loss, difficulty communicating and\ndisorientation. For the diagnosis of Alzheimer's disease, a series of scales\nare often needed to evaluate the diagnosis clinically, which not only increases\nthe workload of doctors, but also makes the results of diagnosis highly\nsubjective. Therefore, for Alzheimer's disease, imaging means to find early\ndiagnostic markers has become a top priority.\n</p>\n<p>In this paper, we propose a novel 3DMgNet architecture which is a unified\nframework of multigrid and convolutional neural network to diagnose Alzheimer's\ndisease (AD). The model is trained using an open dataset (ADNI dataset) and\nthen test with a smaller dataset of ours. Finally, the model achieved 92.133%\naccuracy for AD vs NC classification and significantly reduced the model\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yelu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Huang Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximizing Self-supervision from Thermal Image for Effective Self-supervised Learning of Depth and Ego-motion. (arXiv:2201.04387v1 [cs.RO])","link":"http://arxiv.org/abs/2201.04387","description":"<p>Recently, self-supervised learning of depth and ego-motion from thermal\nimages shows strong robustness and reliability under challenging scenarios.\nHowever, the inherent thermal image properties such as weak contrast, blurry\nedges, and noise hinder to generate effective self-supervision from thermal\nimages. Therefore, most research relies on additional self-supervision sources\nsuch as well-lit RGB images, generative models, and Lidar information. In this\npaper, we conduct an in-depth analysis of thermal image characteristics that\ndegenerates self-supervision from thermal images. Based on the analysis, we\npropose an effective thermal image mapping method that significantly increases\nimage information, such as overall structure, contrast, and details, while\npreserving temporal consistency. The proposed method shows outperformed depth\nand pose results than previous state-of-the-art networks without leveraging\nadditional RGB guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_U/0/1/0/all/0/1\">Ukcheol Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byeong-Uk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OCSampler: Compressing Videos to One Clip with Single-step Sampling. (arXiv:2201.04388v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04388","description":"<p>In this paper, we propose a framework named OCSampler to explore a compact\nyet effective video representation with one short clip for efficient video\nrecognition. Recent works prefer to formulate frame sampling as a sequential\ndecision task by selecting frames one by one according to their importance,\nwhile we present a new paradigm of learning instance-specific video\ncondensation policies to select informative frames for representing the entire\nvideo only in a single step. Our basic motivation is that the efficient video\nrecognition task lies in processing a whole sequence at once rather than\npicking up frames sequentially. Accordingly, these policies are derived from a\nlight-weighted skim network together with a simple yet effective policy network\nwithin one step. Moreover, we extend the proposed method with a frame number\nbudget, enabling the framework to produce correct predictions in high\nconfidence with as few frames as possible. Experiments on four benchmarks,\ni.e., ActivityNet, Mini-Kinetics, FCVID, Mini-Sports1M, demonstrate the\neffectiveness of our OCSampler over previous methods in terms of accuracy,\ntheoretical computational expense, actual inference speed. We also evaluate its\ngeneralization power across different classifiers, sampled frames, and search\nspaces. Especially, we achieve 76.9% mAP and 21.7 GFLOPs on ActivityNet with an\nimpressive throughput: 123.9 Videos/s on a single TITAN Xp GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jintao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Haodong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Adversarially Robust Deep Image Denoising. (arXiv:2201.04397v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04397","description":"<p>This work systematically investigates the adversarial robustness of deep\nimage denoisers (DIDs), i.e, how well DIDs can recover the ground truth from\nnoisy observations degraded by adversarial perturbations. Firstly, to evaluate\nDIDs' robustness, we propose a novel adversarial attack, namely\nObservation-based Zero-mean Attack ({\\sc ObsAtk}), to craft adversarial\nzero-mean perturbations on given noisy images. We find that existing DIDs are\nvulnerable to the adversarial noise generated by {\\sc ObsAtk}. Secondly, to\nrobustify DIDs, we propose an adversarial training strategy, hybrid adversarial\ntraining ({\\sc HAT}), that jointly trains DIDs with adversarial and\nnon-adversarial noisy data to ensure that the reconstruction quality is high\nand the denoisers around non-adversarial data are locally smooth. The resultant\nDIDs can effectively remove various types of synthetic and adversarial noise.\nWe also uncover that the robustness of DIDs benefits their generalization\ncapability on unseen real-world noise. Indeed, {\\sc HAT}-trained DIDs can\nrecover high-quality clean images from real-world noise even without training\non real noisy data. Extensive experiments on benchmark datasets, including\nSet68, PolyU, and SIDD, corroborate the effectiveness of {\\sc ObsAtk} and {\\sc\nHAT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sugiyama_M/0/1/0/all/0/1\">Masashi Sugiyama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y. F. Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoViDNN: A Mobile Platform for Evaluating Video Quality Enhancement with Deep Neural Networks. (arXiv:2201.04402v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04402","description":"<p>Deep neural network (DNN) based approaches have been intensively studied to\nimprove video quality thanks to their fast advancement in recent years. These\napproaches are designed mainly for desktop devices due to their high\ncomputational cost. However, with the increasing performance of mobile devices\nin recent years, it became possible to execute DNN based approaches in mobile\ndevices. Despite having the required computational power, utilizing DNNs to\nimprove the video quality for mobile devices is still an active research area.\nIn this paper, we propose an open-source mobile platform, namely MoViDNN, to\nevaluate DNN based video quality enhancement methods, such as super-resolution,\ndenoising, and deblocking. Our proposed platform can be used to evaluate the\nDNN based approaches both objectively and subjectively. For objective\nevaluation, we report common metrics such as execution time, PSNR, and SSIM.\nFor subjective evaluation, Mean Score Opinion (MOS) is reported. The proposed\nplatform is available publicly at https://github.com/cd-athena/MoViDNN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cetinkaya_E/0/1/0/all/0/1\">Ekrem &#xc7;etinkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timmerer_C/0/1/0/all/0/1\">Christian Timmerer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Prediction of MGMT Promoter Methylation from MRI Scans using Adversarial Learning. (arXiv:2201.04416v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04416","description":"<p>Glioblastoma Multiforme (GBM) is a malignant brain cancer forming around 48%\nof al brain and Central Nervous System (CNS) cancers. It is estimated that\nannually over 13,000 deaths occur in the US due to GBM, making it crucial to\nhave early diagnosis systems that can lead to predictable and effective\ntreatment. The most common treatment after GBM diagnosis is chemotherapy, which\nworks by sending rapidly dividing cells to apoptosis. However, this form of\ntreatment is not effective when the MGMT promoter sequence is methylated, and\ninstead leads to severe side effects decreasing patient survivability.\nTherefore, it is important to be able to identify the MGMT promoter methylation\nstatus through non-invasive magnetic resonance imaging (MRI) based machine\nlearning (ML) models. This is accomplished using the Brain Tumor Segmentation\n(BraTS) 2021 dataset, which was recently used for an international Kaggle\ncompetition. We developed four primary models - two radiomic models and two CNN\nmodels - each solving the binary classification task with progressive\nimprovements. We built a novel ML model termed as the Intermediate State\nGenerator which was used to normalize the slice thicknesses of all MRI scans.\nWith further improvements, our best model was able to achieve performance\nsignificantly ($p &lt; 0.05$) better than the best performing Kaggle model with a\n6% increase in average cross-validation accuracy. This improvement could\npotentially lead to a more informed choice of chemotherapy as a treatment\noption, prolonging lives of thousands of patients with GBM each year.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sauman Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Visible: A Survey on Cross-spectral Face Recognition. (arXiv:2201.04435v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04435","description":"<p>Cross-spectral face recognition (CFR) is aimed at recognizing individuals,\nwhere compared face images stem from different sensing modalities, for example\ninfrared vs. visible. While CFR is inherently more challenging than classical\nface recognition due to significant variation in facial appearance associated\nto a modality gap, it is superior in scenarios with limited or challenging\nillumination, as well as in the presence of presentation attacks. Recent\nadvances in artificial intelligence related to convolutional neural networks\n(CNNs) have brought to the fore a significant performance improvement in CFR.\nMotivated by this, the contributions of this survey are three-fold. We provide\nan overview of CFR, targeted to compare face images captured in different\nspectra, by firstly formalizing CFR and then presenting concrete related\napplications. Secondly, we explore suitable spectral bands for recognition and\ndiscuss recent CFR-methods, placing emphasis on deep neural networks. In\nparticular we revisit techniques that have been proposed to extract and compare\nheterogeneous features, as well as datasets. We enumerate strengths and\nlimitations of different spectra and associated algorithms. Finally, we discuss\nresearch challenges and future lines of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anghelone_D/0/1/0/all/0/1\">David Anghelone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cunjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dantcheva_A/0/1/0/all/0/1\">Antitza Dantcheva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Style Modelling of Human Locomotion via Feature-Wise Transformations and Local Motion Phases. (arXiv:2201.04439v1 [cs.GR])","link":"http://arxiv.org/abs/2201.04439","description":"<p>Controlling the manner in which a character moves in a real-time animation\nsystem is a challenging task with useful applications. Existing style transfer\nsystems require access to a reference content motion clip, however, in\nreal-time systems the future motion content is unknown and liable to change\nwith user input. In this work we present a style modelling system that uses an\nanimation synthesis network to model motion content based on local motion\nphases. An additional style modulation network uses feature-wise\ntransformations to modulate style in real-time. To evaluate our method, we\ncreate and release a new style modelling dataset, 100STYLE, containing over 4\nmillion frames of stylised locomotion data in 100 different styles that present\na number of challenges for existing systems. To model these styles, we extend\nthe local phase calculation with a contact-free formulation. In comparison to\nother methods for real-time style modelling, we show our system is more robust\nand efficient in its style representation while improving motion quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mason_I/0/1/0/all/0/1\">Ian Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Starke_S/0/1/0/all/0/1\">Sebastian Starke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globally Optimal Multi-Scale Monocular Hand-Eye Calibration Using Dual Quaternions. (arXiv:2201.04473v1 [cs.RO])","link":"http://arxiv.org/abs/2201.04473","description":"<p>In this work, we present an approach for monocular hand-eye calibration from\nper-sensor ego-motion based on dual quaternions. Due to non-metrically scaled\ntranslations of monocular odometry, a scaling factor has to be estimated in\naddition to the rotation and translation calibration. For this, we derive a\nquadratically constrained quadratic program that allows a combined estimation\nof all extrinsic calibration parameters. Using dual quaternions leads to low\nrun-times due to their compact representation. Our problem formulation further\nallows to estimate multiple scalings simultaneously for different sequences of\nthe same sensor setup. Based on our problem formulation, we derive both, a fast\nlocal and a globally optimal solving approach. Finally, our algorithms are\nevaluated and compared to state-of-the-art approaches on simulated and\nreal-world data, e.g., the EuRoC MAV dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wodtko_T/0/1/0/all/0/1\">Thomas Wodtko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1\">Markus Horn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchholz_M/0/1/0/all/0/1\">Michael Buchholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Estimation from Single-shot Monocular Endoscope Image Using Image Domain Adaptation And Edge-Aware Depth Estimation. (arXiv:2201.04485v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04485","description":"<p>We propose a depth estimation method from a single-shot monocular endoscopic\nimage using Lambertian surface translation by domain adaptation and depth\nestimation using multi-scale edge loss. We employ a two-step estimation process\nincluding Lambertian surface translation from unpaired data and depth\nestimation. The texture and specular reflection on the surface of an organ\nreduce the accuracy of depth estimations. We apply Lambertian surface\ntranslation to an endoscopic image to remove these texture and reflections.\nThen, we estimate the depth by using a fully convolutional network (FCN).\nDuring the training of the FCN, improvement of the object edge similarity\nbetween an estimated image and a ground truth depth image is important for\ngetting better results. We introduced a muti-scale edge loss function to\nimprove the accuracy of depth estimation. We quantitatively evaluated the\nproposed method using real colonoscopic images. The estimated depth values were\nproportional to the real depth values. Furthermore, we applied the estimated\ndepth images to automated anatomical location identification of colonoscopic\nimages using a convolutional neural network. The identification accuracy of the\nnetwork improved from 69.2% to 74.1% by using the estimated depth images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oda_M/0/1/0/all/0/1\">Masahiro Oda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Itoh_H/0/1/0/all/0/1\">Hayato Itoh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1\">Kiyohito Tanaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takabatake_H/0/1/0/all/0/1\">Hirotsugu Takabatake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_M/0/1/0/all/0/1\">Masaki Mori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Natori_H/0/1/0/all/0/1\">Hiroshi Natori</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SensatUrban: Learning Semantics from Urban-Scale Photogrammetric Point Clouds. (arXiv:2201.04494v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04494","description":"<p>With the recent availability and affordability of commercial depth sensors\nand 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets\nhave been publicized to facilitate research in 3D computer vision. However,\nexisting datasets either cover relatively small areas or have limited semantic\nannotations. Fine-grained understanding of urban-scale 3D scenes is still in\nits infancy. In this paper, we introduce SensatUrban, an urban-scale UAV\nphotogrammetry point cloud dataset consisting of nearly three billion points\ncollected from three UK cities, covering 7.6 km^2. Each point in the dataset\nhas been labelled with fine-grained semantic annotations, resulting in a\ndataset that is three times the size of the previous existing largest\nphotogrammetric point cloud dataset. In addition to the more commonly\nencountered categories such as road and vegetation, urban-level categories\nincluding rail, bridge, and river are also included in our dataset. Based on\nthis dataset, we further build a benchmark to evaluate the performance of\nstate-of-the-art segmentation algorithms. In particular, we provide a\ncomprehensive analysis and identify several key challenges limiting urban-scale\npoint cloud understanding. The dataset is available at\n<a href=\"http://point-cloud-analysis.cs.ox.ac.uk.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_S/0/1/0/all/0/1\">Sheikh Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure and position-aware graph neural network for airway labeling. (arXiv:2201.04532v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04532","description":"<p>We present a novel graph-based approach for labeling the anatomical branches\nof a given airway tree segmentation. The proposed method formulates airway\nlabeling as a branch classification problem in the airway tree graph, where\nbranch features are extracted using convolutional neural networks (CNN) and\nenriched using graph neural networks. Our graph neural network is\nstructure-aware by having each node aggregate information from its local\nneighbors and position-aware by encoding node positions in the graph.\n</p>\n<p>We evaluated the proposed method on 220 airway trees from subjects with\nvarious severity stages of Chronic Obstructive Pulmonary Disease (COPD). The\nresults demonstrate that our approach is computationally efficient and\nsignificantly improves branch classification performance than the baseline\nmethod. The overall average accuracy of our method reaches 91.18\\% for labeling\nall 18 segmental airway branches, compared to 83.83\\% obtained by the standard\nCNN method. We published our source code at\nhttps://github.com/DIAGNijmegen/spgnn. The proposed algorithm is also publicly\navailable at\nhttps://grand-challenge.org/algorithms/airway-anatomical-labeling/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weiyi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_C/0/1/0/all/0/1\">Colin Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charbonnier_J/0/1/0/all/0/1\">Jean-Paul Charbonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Get your Foes Fooled: Proximal Gradient Split Learning for Defense against Model Inversion Attacks on IoMT data. (arXiv:2201.04569v1 [cs.CR])","link":"http://arxiv.org/abs/2201.04569","description":"<p>The past decade has seen a rapid adoption of Artificial Intelligence (AI),\nspecifically the deep learning networks, in Internet of Medical Things (IoMT)\necosystem. However, it has been shown recently that the deep learning networks\ncan be exploited by adversarial attacks that not only make IoMT vulnerable to\nthe data theft but also to the manipulation of medical diagnosis. The existing\nstudies consider adding noise to the raw IoMT data or model parameters which\nnot only reduces the overall performance concerning medical inferences but also\nis ineffective to the likes of deep leakage from gradients method. In this\nwork, we propose proximal gradient split learning (PSGL) method for defense\nagainst the model inversion attacks. The proposed method intentionally attacks\nthe IoMT data when undergoing the deep neural network training process at\nclient side. We propose the use of proximal gradient method to recover gradient\nmaps and a decision-level fusion strategy to improve the recognition\nperformance. Extensive analysis show that the PGSL not only provides effective\ndefense mechanism against the model inversion attacks but also helps in\nimproving the recognition performance on publicly available datasets. We report\n17.9$\\%$ and 36.9$\\%$ gains in accuracy over reconstructed and adversarial\nattacked images, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khowaja_S/0/1/0/all/0/1\">Sunder Ali Khowaja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Ik Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_K/0/1/0/all/0/1\">Kapal Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarwar_M/0/1/0/all/0/1\">Muhammad Aslam Jarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_N/0/1/0/all/0/1\">Nawab Muhammad Faseeh Qureshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONet: Efficient Convolutional Online Likelihood Network for Scribble-based Interactive Segmentation. (arXiv:2201.04584v1 [eess.IV])","link":"http://arxiv.org/abs/2201.04584","description":"<p>Automatic segmentation of lung lesions associated with COVID-19 in CT images\nrequires large amount of annotated volumes. Annotations mandate expert\nknowledge and are time-intensive to obtain through fully manual segmentation\nmethods. Additionally, lung lesions have large inter-patient variations, with\nsome pathologies having similar visual appearance as healthy lung tissues. This\nposes a challenge when applying existing semi-automatic interactive\nsegmentation techniques for data labelling. To address these challenges, we\npropose an efficient convolutional neural networks (CNNs) that can be learned\nonline while the annotator provides scribble-based interaction. To accelerate\nlearning from only the samples labelled through user-interactions, a\npatch-based approach is used for training the network. Moreover, we use\nweighted cross-entropy loss to address the class imbalance that may result from\nuser-interactions. During online inference, the learned network is applied to\nthe whole input volume using a fully convolutional approach. We compare our\nproposed method with state-of-the-art and show that it outperforms existing\nmethods on the task of annotating lung lesions associated with COVID-19,\nachieving 16% higher Dice score while reducing execution time by 3$\\times$ and\nrequiring 9000 lesser scribbles-based labelled voxels. Due to the online\nlearning aspect, our approach adapts quickly to user input, resulting in high\nquality segmentation labels. Source code will be made available upon\nacceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asad_M/0/1/0/all/0/1\">Muhammad Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsely Annotated Object Detection: A Region-based Semi-supervised Approach. (arXiv:2201.04620v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04620","description":"<p>Research shows a noticeable drop in performance of object detectors when the\ntraining data has missing annotations, i.e. sparsely annotated data.\nContemporary methods focus on proxies for missing ground-truth annotations\neither in the form of pseudo-labels or by re-weighing gradients for unlabeled\nboxes during training. In this work, we revisit the formulation of sparsely\nannotated object detection. We observe that sparsely annotated object detection\ncan be considered a semi-supervised object detection problem at a region level.\nBuilding on this insight, we propose a region-based semi-supervised algorithm,\nthat automatically identifies regions containing unlabeled foreground objects.\nOur algorithm then processes the labeled and un-labeled foreground regions\ndifferently, a common practice in semi-supervised methods. To evaluate the\neffectiveness of the proposed approach, we conduct exhaustive experiments on\nfive splits commonly used by sparsely annotated approaches on the PASCAL-VOC\nand COCO datasets and achieve state-of-the-art performance. In addition to\nthis, we show that our approach achieves competitive performance on standard\nsemi-supervised setups demonstrating the strength and broad applicability of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rambhatla_S/0/1/0/all/0/1\">Sai Saketh Rambhatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suri_S/0/1/0/all/0/1\">Saksham Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtual Elastic Objects. (arXiv:2201.04623v1 [cs.CV])","link":"http://arxiv.org/abs/2201.04623","description":"<p>We present Virtual Elastic Objects (VEOs): virtual objects that not only look\nlike their real-world counterparts but also behave like them, even when subject\nto novel interactions. Achieving this presents multiple challenges: not only do\nobjects have to be captured including the physical forces acting on them, then\nfaithfully reconstructed and rendered, but also plausible material parameters\nfound and simulated. To create VEOs, we built a multi-view capture system that\ncaptures objects under the influence of a compressed air stream. Building on\nrecent advances in model-free, dynamic Neural Radiance Fields, we reconstruct\nthe objects and corresponding deformation fields. We propose to use a\ndifferentiable, particle-based simulator to use these deformation fields to\nfind representative material parameters, which enable us to run new\nsimulations. To render simulated objects, we devise a method for integrating\nthe simulation results with Neural Radiance Fields. The resulting method is\napplicable to a wide range of scenarios: it can handle objects composed of\ninhomogeneous material, with very different shapes, and it can simulate\ninteractions with other virtual objects. We present our results using a newly\ncollected dataset of 12 objects under a variety of force fields, which will be\nshared with the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hsiao-yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tretschk_E/0/1/0/all/0/1\">Edgar Tretschk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuyck_T/0/1/0/all/0/1\">Tuur Stuyck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadlecek_P/0/1/0/all/0/1\">Petr Kadlecek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavan_L/0/1/0/all/0/1\">Ladislav Kavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vouga_E/0/1/0/all/0/1\">Etienne Vouga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning under Extreme Memory Constraints. (arXiv:2008.01510v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.01510","description":"<p>Continual Learning (CL) aims to develop agents emulating the human ability to\nsequentially learn new tasks while being able to retain knowledge obtained from\npast experiences. In this paper, we introduce the novel problem of\nMemory-Constrained Online Continual Learning (MC-OCL) which imposes strict\nconstraints on the memory overhead that a possible algorithm can use to avoid\ncatastrophic forgetting. As most, if not all, previous CL methods violate these\nconstraints, we propose an algorithmic solution to MC-OCL: Batch-level\nDistillation (BLD), a regularization-based CL approach, which effectively\nbalances stability and plasticity in order to learn from data streams, while\npreserving the ability to solve old tasks through distillation. Our extensive\nexperimental evaluation, conducted on three publicly available benchmarks,\nempirically demonstrates that our approach successfully addresses the MC-OCL\nproblem and achieves comparable accuracy to prior distillation methods\nrequiring higher memory overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1\">St&#xe9;phane Lathuili&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MED-TEX: Transferring and Explaining Knowledge with Less Data from Pretrained Medical Imaging Models. (arXiv:2008.02593v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.02593","description":"<p>Deep learning methods usually require a large amount of training data and\nlack interpretability. In this paper, we propose a novel knowledge distillation\nand model interpretation framework for medical image classification that\njointly solves the above two issues. Specifically, to address the data-hungry\nissue, a small student model is learned with less data by distilling knowledge\nfrom a cumbersome pretrained teacher model. To interpret the teacher model and\nassist the learning of the student, an explainer module is introduced to\nhighlight the regions of an input that are important for the predictions of the\nteacher model. Furthermore, the joint framework is trained by a principled way\nderived from the information-theoretic perspective. Our framework outperforms\non the knowledge distillation and model interpretation tasks compared to\nstate-of-the-art methods on a fundus dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Duc_T/0/1/0/all/0/1\">Thanh Nguyen-Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobileSal: Extremely Efficient RGB-D Salient Object Detection. (arXiv:2012.13095v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13095","description":"<p>The high computational cost of neural networks has prevented recent successes\nin RGB-D salient object detection (SOD) from benefiting real-world\napplications. Hence, this paper introduces a novel network, MobileSal, which\nfocuses on efficient RGB-D SOD using mobile networks for deep feature\nextraction. However, mobile networks are less powerful in feature\nrepresentation than cumbersome networks. To this end, we observe that the depth\ninformation of color images can strengthen the feature representation related\nto SOD if leveraged properly. Therefore, we propose an implicit depth\nrestoration (IDR) technique to strengthen the mobile networks' feature\nrepresentation capability for RGB-D SOD. IDR is only adopted in the training\nphase and is omitted during testing, so it is computationally free. Besides, we\npropose compact pyramid refinement (CPR) for efficient multi-level feature\naggregation to derive salient objects with clear boundaries. With IDR and CPR\nincorporated, MobileSal performs favorably against state-of-the-art methods on\nsix challenging RGB-D SOD datasets with much faster speed (450fps for the input\nsize of 320 $\\times$ 320) and fewer parameters (6.5M). The code is released at\nhttps://mmcheng.net/mobilesal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu-Huan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jia-Wang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu-Chao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProxyFAUG: Proximity-based Fingerprint Augmentation. (arXiv:2102.02706v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.02706","description":"<p>The proliferation of data-demanding machine learning methods has brought to\nlight the necessity for methodologies which can enlarge the size of training\ndatasets, with simple, rule-based methods. In-line with this concept, the\nfingerprint augmentation scheme proposed in this work aims to augment\nfingerprint datasets which are used to train positioning models. The proposed\nmethod utilizes fingerprints which are recorded in spacial proximity, in order\nto perform fingerprint augmentation, creating new fingerprints which combine\nthe features of the original ones. The proposed method of composing the new,\naugmented fingerprints is inspired by the crossover and mutation operators of\ngenetic algorithms. The ProxyFAUG method aims to improve the achievable\npositioning accuracy of fingerprint datasets, by introducing a rule-based,\nstochastic, proximity-based method of fingerprint augmentation. The performance\nof ProxyFAUG is evaluated in an outdoor Sigfox setting using a public dataset.\nThe best performing published positioning method on this dataset is improved by\n40% in terms of median error and 6% in terms of mean error, with the use of the\naugmented dataset. The analysis of the results indicate a systematic and\nsignificant performance improvement at the lower error quartiles, as indicated\nby the impressive improvement of the median error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_G/0/1/0/all/0/1\">Grigorios G. Anagnostopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CharacterGAN: Few-Shot Keypoint Character Animation and Reposing. (arXiv:2102.03141v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03141","description":"<p>We introduce CharacterGAN, a generative model that can be trained on only a\nfew samples (8 - 15) of a given character. Our model generates novel poses\nbased on keypoint locations, which can be modified in real time while providing\ninteractive feedback, allowing for intuitive reposing and animation. Since we\nonly have very limited training samples, one of the key challenges lies in how\nto address (dis)occlusions, e.g. when a hand moves behind or in front of a\nbody. To address this, we introduce a novel layering approach which explicitly\nsplits the input keypoints into different layers which are processed\nindependently. These layers represent different parts of the character and\nprovide a strong implicit bias that helps to obtain realistic results even with\nstrong (dis)occlusions. To combine the features of individual layers we use an\nadaptive scaling approach conditioned on all keypoints. Finally, we introduce a\nmask connectivity constraint to reduce distortion artifacts that occur with\nextreme out-of-distribution poses at test time. We show that our approach\noutperforms recent baselines and creates realistic animations for diverse\ncharacters. We also show that our model can handle discrete state changes, for\nexample a profile facing left or right, that the different layers do indeed\nlearn features specific for the respective keypoints in those layers, and that\nour model scales to larger datasets when more data is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hinz_T/0/1/0/all/0/1\">Tobias Hinz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1\">Oliver Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie. (arXiv:2103.04715v6 [stat.ME] UPDATED)","link":"http://arxiv.org/abs/2103.04715","description":"<p>Since the seminal work of Venkatakrishnan et al. in 2013, Plug &amp; Play (PnP)\nmethods have become ubiquitous in Bayesian imaging. These methods derive\nMinimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for\ninverse problems in imaging by combining an explicit likelihood function with a\nprior that is implicitly defined by an image denoising algorithm. The PnP\nalgorithms proposed in the literature mainly differ in the iterative schemes\nthey use for optimisation or for sampling. In the case of optimisation schemes,\nsome recent works guarantee the convergence to a fixed point, albeit not\nnecessarily a MAP estimate. In the case of sampling schemes, to the best of our\nknowledge, there is no known proof of convergence. There also remain important\nopen questions regarding whether the underlying Bayesian models and estimators\nare well defined, well-posed, and have the basic regularity properties required\nto support these numerical schemes. To address these limitations, this paper\ndevelops theory, methods, and provably convergent algorithms for performing\nBayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA\n(Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference;\nand 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent\nresults on the quantitative convergence of Markov chains, we establish detailed\nconvergence guarantees for these two algorithms under realistic assumptions on\nthe denoising operators used, with special attention to denoisers based on deep\nneural networks. We also show that these algorithms approximately target a\ndecision-theoretically optimal Bayesian model that is well-posed. The proposed\nalgorithms are demonstrated on several canonical problems such as image\ndeblurring, inpainting, and denoising, where they are used for point estimation\nas well as for uncertainty visualisation and quantification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Laumont_R/0/1/0/all/0/1\">R&#xe9;mi Laumont</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1\">Valentin de Bortoli</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Delon_J/0/1/0/all/0/1\">Julie Delon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pereyra_M/0/1/0/all/0/1\">Marcelo Pereyra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Domain Invariant Representations for Generalizable Person Re-Identification. (arXiv:2103.15890v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15890","description":"<p>Generalizable person Re-Identification (ReID) has attracted growing attention\nin recent computer vision community. In this work, we construct a structural\ncausal model among identity labels, identity-specific factors (clothes/shoes\ncolor etc), and domain-specific factors (background, viewpoints etc). According\nto the causal analysis, we propose a novel Domain Invariant Representation\nLearning for generalizable person Re-Identification (DIR-ReID) framework.\nSpecifically, we first propose to disentangle the identity-specific and\ndomain-specific feature spaces, based on which we propose an effective\nalgorithmic implementation for backdoor adjustment, essentially serving as a\ncausal intervention towards the SCM. Extensive experiments have been conducted,\nshowing that DIR-ReID outperforms state-of-the-art methods on large-scale\ndomain generalization ReID benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin3D: Detection and Longitudinal Tracking of Pigmented Skin Lesions in 3D Total-Body Textured Meshes. (arXiv:2105.00374v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00374","description":"<p>We present an automated approach to detect and longitudinally track skin\nlesions on 3D total-body skin surface scans. The acquired 3D mesh of the\nsubject is unwrapped to a 2D texture image, where a trained objected detection\nmodel, Faster R-CNN, localizes the lesions within the 2D domain. These detected\nskin lesions are mapped back to the 3D surface of the subject and, for subjects\nimaged multiple times, we construct a graph-based matching procedure to\nlongitudinally track lesions that considers the anatomical correspondences\namong pairs of meshes and the geodesic proximity of corresponding lesions and\nthe inter-lesion geodesic distances.\n</p>\n<p>We evaluated the proposed approach using 3DBodyTex, a publicly available\ndataset composed of 3D scans imaging the coloured skin (textured meshes) of 200\nhuman subjects. We manually annotated locations that appeared to the human eye\nto contain a pigmented skin lesion as well as tracked a subset of lesions\noccurring on the same subject imaged in different poses. Our results, when\ncompared to three human annotators, suggest that the trained Faster R-CNN\ndetects lesions at a similar performance level as the human annotators. Our\nlesion tracking algorithm achieves an average matching accuracy of 88% on a set\nof detected corresponding pairs of prominent lesions of subjects imaged in\ndifferent poses, and an average longitudinal accuracy of 71% when encompassing\nadditional errors due to lesion detection. As there currently is no other\nlarge-scale publicly available dataset of 3D total-body skin lesions, we\npublicly release over 25,000 3DBodyTex manual annotations, which we hope will\nfurther research on total-body skin lesion analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengliu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_J/0/1/0/all/0/1\">Jeremy Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_K/0/1/0/all/0/1\">Kumar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamanian_S/0/1/0/all/0/1\">Sajjad Shamanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KVT: k-NN Attention for Boosting Vision Transformers. (arXiv:2106.00515v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00515","description":"<p>Convolutional Neural Networks (CNNs) have dominated computer vision for\nyears, due to its ability in capturing locality and translation invariance.\nRecently, many vision transformer architectures have been proposed and they\nshow promising performance. A key component in vision transformers is the\nfully-connected self-attention which is more powerful than CNNs in modelling\nlong range dependencies. However, since the current dense self-attention uses\nall image patches (tokens) to compute attention matrix, it may neglect locality\nof images patches and involve noisy tokens (e.g., clutter background and\nocclusion), leading to a slow training process and potential degradation of\nperformance. To address these problems, we propose the $k$-NN attention for\nboosting vision transformers. Specifically, instead of involving all the tokens\nfor attention matrix calculation, we only select the top-$k$ similar tokens\nfrom the keys for each query to compute the attention map. The proposed $k$-NN\nattention naturally inherits the local bias of CNNs without introducing\nconvolutional operations, as nearby tokens tend to be more similar than others.\nIn addition, the $k$-NN attention allows for the exploration of long range\ncorrelation and at the same time filters out irrelevant tokens by choosing the\nmost similar tokens from the entire image. Despite its simplicity, we verify,\nboth theoretically and empirically, that $k$-NN attention is powerful in\nspeeding up training and distilling noise from input tokens. Extensive\nexperiments are conducted by using 11 different vision transformer\narchitectures to verify that the proposed $k$-NN attention can work with any\nexisting transformer architectures to improve its prediction performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuning Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v9 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2106.08208","description":"<p>Adaptive gradient methods have shown excellent performances for solving many\nmachine learning problems. Although multiple adaptive gradient methods were\nrecently studied, they mainly focus on either empirical or theoretical aspects\nand also only work for specific problems by using some specific adaptive\nlearning rates. Thus, it is desired to design a universal framework for\npractical algorithms of adaptive gradients with theoretical guarantee to solve\ngeneral problems. To fill this gap, we propose a faster and universal framework\nof adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive\nmatrix that includes most existing adaptive gradient forms. Moreover, our\nframework can flexibly integrate the momentum and variance reduced techniques.\nIn particular, our novel framework provides the convergence analysis support\nfor adaptive gradient methods under the nonconvex setting. In theoretical\nanalysis, we prove that our SUPER-ADAM algorithm can achieve the best known\ngradient (i.e., stochastic first-order oracle (SFO)) complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms. Code is available at\nhttps://github.com/LIJUNYI95/SuperAdam\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects. (arXiv:2108.07368v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07368","description":"<p>Segmenting medical images accurately and reliably is important for disease\ndiagnosis and treatment. It is a challenging task because of the wide variety\nof objects' sizes, shapes, and scanning modalities. Recently, many\nconvolutional neural networks (CNN) have been designed for segmentation tasks\nand achieved great success. Few studies, however, have fully considered the\nsizes of objects and thus most demonstrate poor performance on segmentation of\nsmall objects segmentation. This can have significant impact on early detection\nof disease. This paper proposes a Context Axial Reserve Attention Network\n(CaraNet) to improve the segmentation performance on small objects compared\nwith recent state-of-the-art models. We test our CaraNet on brain tumor (BraTS\n2018) and polyp (Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300 and\nETIS-LaribPolypDB) segmentation. Our CaraNet not only achieves the top-rank\nmean Dice segmentation accuracy, but also shows a distinct advantage in\nsegmentation of small medical objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1\">Ange Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep MRI Reconstruction with Radial Subsampling. (arXiv:2108.07619v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.07619","description":"<p>In spite of its extensive adaptation in almost every medical diagnostic and\nexaminatorial application, Magnetic Resonance Imaging (MRI) is still a slow\nimaging modality which limits its use for dynamic imaging. In recent years,\nParallel Imaging (PI) and Compressed Sensing (CS) have been utilised to\naccelerate the MRI acquisition. In clinical settings, subsampling the k-space\nmeasurements during scanning time using Cartesian trajectories, such as\nrectilinear sampling, is currently the most conventional CS approach applied\nwhich, however, is prone to producing aliased reconstructions. With the advent\nof the involvement of Deep Learning (DL) in accelerating the MRI,\nreconstructing faithful images from subsampled data became increasingly\npromising. Retrospectively applying a subsampling mask onto the k-space data is\na way of simulating the accelerated acquisition of k-space data in real\nclinical setting. In this paper we compare and provide a review for the effect\nof applying either rectilinear or radial retrospective subsampling on the\nquality of the reconstructions outputted by trained deep neural networks. With\nthe same choice of hyper-parameters, we train and evaluate two distinct\nRecurrent Inference Machines (RIMs), one for each type of subsampling. The\nqualitative and quantitative results of our experiments indicate that the model\ntrained on data with radial subsampling attains higher performance and learns\nto estimate reconstructions with higher fidelity paving the way for other DL\napproaches to involve radial subsampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yiasemis_G/0/1/0/all/0/1\">George Yiasemis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1\">Clara I. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sonke_J/0/1/0/all/0/1\">Jan-Jakob Sonke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving Object Detection for Event-based Vision using k-means Clustering. (arXiv:2109.01879v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01879","description":"<p>Moving object detection is important in computer vision. Event-based cameras\nare bio-inspired cameras that work by mimicking the working of the human eye.\nThese cameras have multiple advantages over conventional frame-based cameras,\nlike reduced latency, HDR, reduced motion blur during high motion, low power\nconsumption, etc. In spite of these advantages, event-based cameras are\nnoise-sensitive and have low resolution. Moreover, the task of moving object\ndetection in these cameras is difficult, as event-based sensors lack useful\nvisual features like texture and color. In this paper, we investigate the\napplication of the k-means clustering technique in detecting moving objects in\nevent-based data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Anindya Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mayukhmali Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Product Quantization for Deep Unsupervised Image Retrieval. (arXiv:2109.02244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02244","description":"<p>Supervised deep learning-based hash and vector quantization are enabling fast\nand large-scale image retrieval systems. By fully exploiting label annotations,\nthey are achieving outstanding retrieval performances compared to the\nconventional methods. However, it is painstaking to assign labels precisely for\na vast amount of training data, and also, the annotation process is\nerror-prone. To tackle these issues, we propose the first deep unsupervised\nimage retrieval method dubbed Self-supervised Product Quantization (SPQ)\nnetwork, which is label-free and trained in a self-supervised manner. We design\na Cross Quantized Contrastive learning strategy that jointly learns codewords\nand deep visual descriptors by comparing individually transformed images\n(views). Our method analyzes the image contents to extract descriptive\nfeatures, allowing us to understand image representations for accurate\nretrieval. By conducting extensive experiments on benchmarks, we demonstrate\nthat the proposed method yields state-of-the-art results even without\nsupervised pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young Kyun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaled ReLU Matters for Training Vision Transformers. (arXiv:2109.03810v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03810","description":"<p>Vision transformers (ViTs) have been an alternative design paradigm to\nconvolutional neural networks (CNNs). However, the training of ViTs is much\nharder than CNNs, as it is sensitive to the training parameters, such as\nlearning rate, optimizer and warmup epoch. The reasons for training difficulty\nare empirically analysed in ~\\cite{xiao2021early}, and the authors conjecture\nthat the issue lies with the \\textit{patchify-stem} of ViT models and propose\nthat early convolutions help transformers see better. In this paper, we further\ninvestigate this problem and extend the above conclusion: only early\nconvolutions do not help for stable training, but the scaled ReLU operation in\nthe \\textit{convolutional stem} (\\textit{conv-stem}) matters. We verify, both\ntheoretically and empirically, that scaled ReLU in \\textit{conv-stem} not only\nimproves training stabilization, but also increases the diversity of patch\ntokens, thus boosting peak performance with a large margin via adding few\nparameters and flops. In addition, extensive experiments are conducted to\ndemonstrate that previous ViTs are far from being well trained, further showing\nthat ViTs have great potential to be a better substitute of CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingkai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IFBiD: Inference-Free Bias Detection. (arXiv:2109.04374v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04374","description":"<p>This paper is the first to explore an automatic way to detect bias in deep\nconvolutional neural networks by simply looking at their weights. Furthermore,\nit is also a step towards understanding neural networks and how they work. We\nshow that it is indeed possible to know if a model is biased or not simply by\nlooking at its weights, without the model inference for an specific input. We\nanalyze how bias is encoded in the weights of deep networks through a toy\nexample using the Colored MNIST database and we also provide a realistic case\nstudy in gender detection from face images using state-of-the-art methods and\nexperimental resources. To do so, we generated two databases with 36K and 48K\nbiased models each. In the MNIST models we were able to detect whether they\npresented a strong or low bias with more than 99% accuracy, and we were also\nable to classify between four levels of bias with more than 70% accuracy. For\nthe face models, we achieved 90% accuracy in distinguishing between models\nbiased towards Asian, Black, or Caucasian ethnicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARKitScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data. (arXiv:2111.08897v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08897","description":"<p>Scene understanding is an active research area. Commercial depth sensors,\nsuch as Kinect, have enabled the release of several RGB-D datasets over the\npast few years which spawned novel methods in 3D scene understanding. More\nrecently with the launch of the LiDAR sensor in Apple's iPads and iPhones, high\nquality RGB-D data is accessible to millions of people on a device they\ncommonly use. This opens a whole new era in scene understanding for the\nComputer Vision community as well as app developers. The fundamental research\nin scene understanding together with the advances in machine learning can now\nimpact people's everyday experiences. However, transforming these scene\nunderstanding methods to real-world experiences requires additional innovation\nand development. In this paper we introduce ARKitScenes. It is not only the\nfirst RGB-D dataset that is captured with a now widely available depth sensor,\nbut to our best knowledge, it also is the largest indoor scene understanding\ndata released. In addition to the raw and processed data from the mobile\ndevice, ARKitScenes includes high resolution depth maps captured using a\nstationary laser scanner, as well as manually labeled 3D oriented bounding\nboxes for a large taxonomy of furniture. We further analyze the usefulness of\nthe data for two downstream tasks: 3D object detection and color-guided depth\nupsampling. We demonstrate that our dataset can help push the boundaries of\nexisting state-of-the-art methods and it introduces new challenges that better\nrepresent real-world scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baruch_G/0/1/0/all/0/1\">Gilad Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_A/0/1/0/all/0/1\">Afshin Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimry_T/0/1/0/all/0/1\">Tal Dimry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feigin_Y/0/1/0/all/0/1\">Yuri Feigin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1\">Peter Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebauer_T/0/1/0/all/0/1\">Thomas Gebauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joffe_B/0/1/0/all/0/1\">Brandon Joffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_D/0/1/0/all/0/1\">Daniel Kurz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_A/0/1/0/all/0/1\">Arik Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shulman_E/0/1/0/all/0/1\">Elad Shulman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Convolutional Neural Networks to Detect Compression Algorithms. (arXiv:2111.09034v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09034","description":"<p>Machine learning is penetrating various domains virtually, thereby\nproliferating excellent results. It has also found an outlet in digital\nforensics, wherein it is becoming the prime driver of computational efficiency.\nA prominent feature that exhibits the effectiveness of ML algorithms is feature\nextraction that can be instrumental in the applications for digital forensics.\nConvolutional Neural Networks are further used to identify parts of the file.\nTo this end, we observed that the literature does not include sufficient\ninformation about the identification of the algorithms used to compress file\nfragments. With this research, we attempt to address this gap as compression\nalgorithms are beneficial in generating higher entropy comparatively as they\nmake the data more compact. We used a base dataset, compressed every file with\nvarious algorithms, and designed a model based on that. The used model was\naccurately able to identify files compressed using compress, lzip and bzip2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Shubham Bharadwaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence Propagation Cluster: Unleash Full Potential of Object Detectors. (arXiv:2112.00342v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00342","description":"<p>It has been a long history that most object detection methods obtain objects\nby using the non-maximum suppression (NMS) and its improved versions like\nSoft-NMS to remove redundant bounding boxes. We challenge those NMS-based\nmethods from three aspects: 1) The bounding box with highest confidence value\nmay not be the true positive having the biggest overlap with the ground-truth\nbox. 2) Not only suppression is required for redundant boxes, but also\nconfidence enhancement is needed for those true positives. 3) Sorting candidate\nboxes by confidence values is not necessary so that full parallelism is\nachievable.\n</p>\n<p>In this paper, inspired by belief propagation (BP), we propose the Confidence\nPropagation Cluster (CP-Cluster) to replace NMS-based methods, which is fully\nparallelizable as well as better in accuracy. In CP-Cluster, we borrow the\nmessage passing mechanism from BP to penalize redundant boxes and enhance true\npositives simultaneously in an iterative way until convergence. We verified the\neffectiveness of CP-Cluster by applying it to various mainstream detectors such\nas FasterRCNN, SSD, FCOS, YOLOv3, YOLOv5, Centernet etc. Experiments on MS COCO\nshow that our plug and play method, without retraining detectors, is able to\nsteadily improve average mAP of all those state-of-the-art models with a clear\nmargin from 0.2 to 1.9 respectively when compared with NMS-based methods.\nSource code is available at https://github.com/shenyi0220/CP-Cluster\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yichun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wanli Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rundong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Junghyun Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HHF: Hashing-guided Hinge Function for Deep Hashing Retrieval. (arXiv:2112.02225v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02225","description":"<p>Deep hashing has shown promising performance in large-scale image retrieval.\nHowever, latent codes extracted by Deep Neural Networks (DNNs) will inevitably\nlose semantic information during the binarization process, which damages the\nretrieval accuracy and makes it challenging. Although many existing approaches\nperform regularization to alleviate quantization errors, we figure out an\nincompatible conflict between metric learning and quantization learning. The\nmetric loss penalizes the inter-class distances to push different classes\nunconstrained far away. Worse still, it tends to map the latent code deviate\nfrom ideal binarization point and generate severe ambiguity in the binarization\nprocess. Based on the minimum distance of the binary linear code, we creatively\npropose Hashing-guided Hinge Function (HHF) to avoid such conflict. In detail,\nthe carefully-designed inflection point, which relies on the hash bit length\nand category numbers, is explicitly adopted to balance the metric term and\nquantization term. Such a modification prevents the network from falling into\nlocal metric optimal minima in deep hashing. Extensive experiments in CIFAR-10,\nCIFAR-100, ImageNet, and MS-COCO show that HHF consistently outperforms\nexisting techniques, and is robust and flexible to transplant into other\nmethods. Code is available at https://github.com/JerryXu0129/HHF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengyin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zenghao Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhengzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongjia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_Q/0/1/0/all/0/1\">Qiruyi Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encouraging Disentangled and Convex Representation with Controllable Interpolation Regularization. (arXiv:2112.03163v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03163","description":"<p>We focus on controllable disentangled representation learning (C-Dis-RL),\nwhere users can control the partition of the disentangled latent space to\nfactorize dataset attributes (concepts) for downstream tasks. Two general\nproblems remain under-explored in current methods: (1) They lack comprehensive\ndisentanglement constraints, especially missing the minimization of mutual\ninformation between different attributes across latent and observation domains.\n(2) They lack convexity constraints in disentangled latent space, which is\nimportant for meaningfully manipulating specific attributes for downstream\ntasks. To encourage both comprehensive C-Dis-RL and convexity simultaneously,\nwe propose a simple yet efficient method: Controllable Interpolation\nRegularization (CIR), which creates a positive loop where the disentanglement\nand convexity can help each other. Specifically, we conduct controlled\ninterpolation in latent space during training and 'reuse' the encoder to help\nform a 'perfect disentanglement' regularization. In that case, (a)\ndisentanglement loss implicitly enlarges the potential 'understandable'\ndistribution to encourage convexity; (b) convexity can in turn improve robust\nand precise disentanglement. CIR is a general module and we merge CIR with\nthree different algorithms: ELEGANT, I2I-Dis, and GZS-Net to show the\ncompatibility and effectiveness. Qualitative and quantitative experiments show\nimprovement in C-Dis-RL and latent convexity by CIR. This further improves\ndownstream tasks: controllable image synthesis, cross-modality image\ntranslation and zero-shot synthesis. More experiments demonstrate CIR can also\nimprove other downstream tasks, such as new attribute value mining, data\naugmentation, and eliminating bias for fairness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yunhao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_G/0/1/0/all/0/1\">Gan Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yunkui Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1\">Laurent Itti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory-Constrained Deep Latent Visual Attention for Improved Local Planning in Presence of Heterogeneous Terrain. (arXiv:2112.04684v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.04684","description":"<p>We present a reward-predictive, model-based deep learning method featuring\ntrajectory-constrained visual attention for use in mapless, local visual\nnavigation tasks. Our method learns to place visual attention at locations in\nlatent image space which follow trajectories caused by vehicle control actions\nto enhance predictive accuracy during planning. The attention model is jointly\noptimized by the task-specific loss and an additional trajectory-constraint\nloss, allowing adaptability yet encouraging a regularized structure for\nimproved generalization and reliability. Importantly, visual attention is\napplied in latent feature map space instead of raw image space to promote\nefficient planning. We validated our model in visual navigation tasks of\nplanning low turbulence, collision-free trajectories in off-road settings and\nhill climbing with locking differentials in the presence of slippery terrain.\nExperiments involved randomized procedural generated simulation and real-world\nenvironments. We found our method improved generalization and learning\nefficiency when compared to no-attention and self-attention alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wapnick_S/0/1/0/all/0/1\">Stefan Wapnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manderson_T/0/1/0/all/0/1\">Travis Manderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1\">David Meger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1\">Gregory Dudek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Spatiotemporal Representation Learning by Exploiting Video Continuity. (arXiv:2112.05883v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05883","description":"<p>Recent self-supervised video representation learning methods have found\nsignificant success by exploring essential properties of videos, e.g. speed,\ntemporal order, etc. This work exploits an essential yet under-explored\nproperty of videos, the video continuity, to obtain supervision signals for\nself-supervised representation learning. Specifically, we formulate three novel\ncontinuity-related pretext tasks, i.e. continuity justification, discontinuity\nlocalization, and missing section approximation, that jointly supervise a\nshared backbone for video representation learning. This self-supervision\napproach, termed as Continuity Perception Network (CPNet), solves the three\ntasks altogether and encourages the backbone network to learn local and\nlong-ranged motion and context representations. It outperforms prior arts on\nmultiple downstream tasks, such as action recognition, video retrieval, and\naction localization. Additionally, the video continuity can be complementary to\nother coarse-grained video properties for representation learning, and\nintegrating the proposed pretext task to prior arts can yield much performance\ngains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quader_N/0/1/0/all/0/1\">Niamul Quader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zhixiang Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Juwei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection. (arXiv:2112.11088v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11088","description":"<p>Recently, fusing the LiDAR point cloud and camera image to improve the\nperformance and robustness of 3D object detection has received more and more\nattention, as these two modalities naturally possess strong complementarity. In\nthis paper, we propose EPNet++ for multi-modal 3D object detection by\nintroducing a novel Cascade Bi-directional Fusion~(CB-Fusion) module and a\nMulti-Modal Consistency~(MC) loss. More concretely, the proposed CB-Fusion\nmodule boosts the plentiful semantic information of point features with the\nimage features in a cascade bi-directional interaction fusion manner, leading\nto more comprehensive and discriminative feature representations. The MC loss\nexplicitly guarantees the consistency between predicted scores from two\nmodalities to obtain more comprehensive and reliable confidence scores. The\nexperiment results on the KITTI, JRDB and SUN-RGBD datasets demonstrate the\nsuperiority of EPNet++ over the state-of-the-art methods. Besides, we emphasize\na critical but easily overlooked problem, which is to explore the performance\nand robustness of a 3D detector in a sparser scene. Extensive experiments\npresent that EPNet++ outperforms the existing SOTA methods with remarkable\nmargins in highly sparse point cloud cases, which might be an available\ndirection to reduce the expensive cost of LiDAR sensors. Code will be released\nin the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tengteng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiwu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The cluster structure function. (arXiv:2201.01222v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.01222","description":"<p>For each partition of a data set into a given number of parts there is a\npartition such that every part is as much as possible a good model (an\n\"algorithmic sufficient statistic\") for the data in that part. Since this can\nbe done for every number between one and the number of data, the result is a\nfunction, the cluster structure function. It maps the number of parts of a\npartition to values related to the deficiencies of being good models by the\nparts. Such a function starts with a value at least zero for no partition of\nthe data set and descents to zero for the partition of the data set into\nsingleton parts. The optimal clustering is the one chosen to minimize the\ncluster structure function. The theory behind the method is expressed in\nalgorithmic information theory (Kolmogorov complexity). In practice the\nKolmogorov complexities involved are approximated by a concrete compressor. We\ngive examples using real data sets: the MNIST handwritten digits and the\nsegmentation of real cells as used in stem cell research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_A/0/1/0/all/0/1\">Andrew R. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitanyi_P/0/1/0/all/0/1\">Paul M.B. Vit&#xe1;nyi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02428","description":"<p>Today, deep convolutional neural networks (CNNs) have demonstrated\nstate-of-the-art performance for medical image segmentation, on various imaging\nmodalities and tasks. Despite early success, segmentation networks may still\ngenerate anatomically aberrant segmentations, with holes or inaccuracies near\nthe object boundaries. To enforce anatomical plausibility, recent research\nstudies have focused on incorporating prior knowledge such as object shape or\nboundary, as constraints in the loss function. Prior integrated could be\nlow-level referring to reformulated representations extracted from the\nground-truth segmentations, or high-level representing external medical\ninformation such as the organ's shape or size. Over the past few years,\nprior-based losses exhibited a rising interest in the research field since they\nallow integration of expert knowledge while still being architecture-agnostic.\nHowever, given the diversity of prior-based losses on different medical imaging\nchallenges and tasks, it has become hard to identify what loss works best for\nwhich dataset. In this paper, we establish a benchmark of recent prior-based\nlosses for medical image segmentation. The main objective is to provide\nintuition onto which losses to choose given a particular task or dataset. To\nthis end, four low-level and high-level prior-based losses are selected. The\nconsidered losses are validated on 8 different datasets from a variety of\nmedical image segmentation challenges including the Decathlon, the ISLES and\nthe WMH challenge. Results show that whereas low-level prior-based losses can\nguarantee an increase in performance over the Dice loss baseline regardless of\nthe dataset characteristics, high-level prior-based losses can increase\nanatomical plausibility as per data characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jurdi_R/0/1/0/all/0/1\">Rosana El Jurdi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petitjean_C/0/1/0/all/0/1\">Caroline Petitjean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honeine_P/0/1/0/all/0/1\">Paul Honeine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdallah_F/0/1/0/all/0/1\">Fahed Abdallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision in adverse weather: Augmentation using CycleGANs with various object detectors for robust perception in autonomous racing. (arXiv:2201.03246v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03246","description":"<p>In an autonomous driving system, perception - identification of features and\nobjects from the environment - is crucial. In autonomous racing, high speeds\nand small margins demand rapid and accurate detection systems. During the race,\nthe weather can change abruptly, causing significant degradation in perception,\nresulting in ineffective manoeuvres. In order to improve detection in adverse\nweather, deep-learning-based models typically require extensive datasets\ncaptured in such conditions - the collection of which is a tedious, laborious,\nand costly process. However, recent developments in CycleGAN architectures\nallow the synthesis of highly realistic scenes in multiple weather conditions.\nTo this end, we introduce an approach of using synthesised adverse condition\ndatasets in autonomous racing (generated using CycleGAN) to improve the\nperformance of four out of five state-of-the-art detectors by an average of\n42.7 and 4.4 mAP percentage points in the presence of night-time conditions and\ndroplets, respectively. Furthermore, we present a comparative analysis of five\nobject detectors - identifying the optimal pairing of detector and training\ndata for use during autonomous racing in challenging conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1\">Izzeddin Teeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_V/0/1/0/all/0/1\">Valentina Musat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rast_A/0/1/0/all/0/1\">Alexander Rast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1\">Fabio Cuzzolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1\">Andrew Bradley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similarity-based Gray-box Adversarial Attack Against Deep Face Recognition. (arXiv:2201.04011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04011","description":"<p>The majority of adversarial attack techniques perform well against deep face\nrecognition when the full knowledge of the system is revealed\n(\\emph{white-box}). However, such techniques act unsuccessfully in the gray-box\nsetting where the face templates are unknown to the attackers. In this work, we\npropose a similarity-based gray-box adversarial attack (SGADV) technique with a\nnewly developed objective function. SGADV utilizes the dissimilarity score to\nproduce the optimized adversarial example, i.e., similarity-based adversarial\nattack. This technique applies to both white-box and gray-box attacks against\nauthentication systems that determine genuine or imposter users using the\ndissimilarity score. To validate the effectiveness of SGADV, we conduct\nextensive experiments on face datasets of LFW, CelebA, and CelebA-HQ against\ndeep face recognition models of FaceNet and InsightFace in both white-box and\ngray-box settings. The results suggest that the proposed method significantly\noutperforms the existing adversarial attack techniques in the gray-box setting.\nWe hence summarize that the similarity-base approaches to develop the\nadversarial example could satisfactorily cater to the gray-box attack scenarios\nfor de-authentication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanrui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhe Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yandan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cunjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tistarell_M/0/1/0/all/0/1\">Massimo Tistarell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}