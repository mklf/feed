{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning Dialogue Representations from Consecutive Utterances. (arXiv:2205.13568v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13568","description":"<p>Learning high-quality dialogue representations is essential for solving a\nvariety of dialogue-oriented tasks, especially considering that dialogue\nsystems often suffer from data scarcity. In this paper, we introduce Dialogue\nSentence Embedding (DSE), a self-supervised contrastive learning method that\nlearns effective dialogue representations suitable for a wide range of dialogue\ntasks. DSE learns from dialogues by taking consecutive utterances of the same\ndialogue as positive pairs for contrastive learning. Despite its simplicity,\nDSE achieves significantly better representation capability than other dialogue\nrepresentation and universal sentence representation models. We evaluate DSE on\nfive downstream dialogue tasks that examine dialogue representation at\ndifferent semantic granularities. Experiments in few-shot and zero-shot\nsettings show that DSE outperforms baselines by a large margin. For example, it\nachieves 13 average performance improvement over the strongest unsupervised\nbaseline in 1-shot intent classification on 6 datasets. We also provide\nanalyses on the benefits and limitations of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingwall_N/0/1/0/all/0/1\">Nicholas Dingwall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaofei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical Dialogue Transcription Error Correction using Seq2Seq Models. (arXiv:2205.13572v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13572","description":"<p>Good communication is critical to good healthcare. Clinical dialogue is a\nconversation between health practitioners and their patients, with the explicit\ngoal of obtaining and sharing medical information. This information contributes\nto medical decision-making regarding the patient and plays a crucial role in\ntheir healthcare journey. The reliance on note taking and manual scribing\nprocesses are extremely inefficient and leads to manual transcription errors\nwhen digitizing notes. Automatic Speech Recognition (ASR) plays a significant\nrole in speech-to-text applications, and can be directly used as a text\ngenerator in conversational applications. However, recording clinical dialogue\npresents a number of general and domain-specific challenges. In this paper, we\npresent a seq2seq learning approach for ASR transcription error correction of\nclinical dialogues. We introduce a new Gastrointestinal Clinical Dialogue (GCD)\nDataset which was gathered by healthcare professionals from a NHS Inflammatory\nBowel Disease clinic and use this in a comparative study with four commercial\nASR systems. Using self-supervision strategies, we fine-tune a seq2seq model on\na mask-filling task using a domain-specific PubMed dataset which we have shared\npublicly for future research. The BART model fine-tuned for mask-filling was\nable to correct transcription errors and achieve lower word error rates for\nthree out of four commercial ASR outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nanayakkara_G/0/1/0/all/0/1\">Gayani Nanayakkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiratunga_N/0/1/0/all/0/1\">Nirmalie Wiratunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corsar_D/0/1/0/all/0/1\">David Corsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_K/0/1/0/all/0/1\">Kyle Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijekoon_A/0/1/0/all/0/1\">Anjana Wijekoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Decoding in Large Language Models. (arXiv:2205.13621v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13621","description":"<p>Recent large-scale natural language processing (NLP) systems use a\npre-trained Large Language Model (LLM) on massive and diverse corpora as a\nheadstart. In practice, the pre-trained model is adapted to a wide array of\ntasks via fine-tuning on task-specific datasets. LLMs, while effective, have\nbeen shown to memorize instances of training data thereby potentially revealing\nprivate information processed during pre-training. The potential leakage might\nfurther propagate to the downstream tasks for which LLMs are fine-tuned. On the\nother hand, privacy-preserving algorithms usually involve retraining from\nscratch, which is prohibitively expensive for LLMs. In this work, we propose a\nsimple, easy to interpret, and computationally lightweight perturbation\nmechanism to be applied to an already trained model at the decoding stage. Our\nperturbation mechanism is model-agnostic and can be used in conjunction with\nany LLM. We provide theoretical analysis showing that the proposed mechanism is\ndifferentially private, and experimental results showing a privacy-utility\ntrade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majmudar_J/0/1/0/all/0/1\">Jimit Majmudar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuy_C/0/1/0/all/0/1\">Christophe Dupuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaili_S/0/1/0/all/0/1\">Sami Smaili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rahul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1\">Richard Zemel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quark: Controllable Text Generation with Reinforced Unlearning. (arXiv:2205.13636v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13636","description":"<p>Large-scale language models often learn behaviors that are misaligned with\nuser expectations. Generated text may contain offensive or toxic language,\ncontain significant repetition, or be of a different sentiment than desired by\nthe user. We consider the task of unlearning these misalignments by fine-tuning\nthe language model on signals of what not to do. We introduce Quantized Reward\nKonditioning (Quark), an algorithm for optimizing a reward function that\nquantifies an (un)wanted property, while not straying too far from the original\nmodel. Quark alternates between (i) collecting samples with the current\nlanguage model, (ii) sorting them into quantiles based on reward, with each\nquantile identified by a reward token prepended to the language model's input,\nand (iii) using a standard language modeling loss on samples from each quantile\nconditioned on its reward token, while remaining nearby the original language\nmodel via a KL-divergence penalty. By conditioning on a high-reward token at\ngeneration time, the model generates text that exhibits less of the unwanted\nproperty. For unlearning toxicity, negative sentiment, and repetition, our\nexperiments show that Quark outperforms both strong baselines and\nstate-of-the-art reinforcement learning methods like PPO (Schulman et al.\n2017), while relying only on standard language modeling primitives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Lianhui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Adapters for Personalized Speech Recognition in Neural Transducers. (arXiv:2205.13660v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13660","description":"<p>Personal rare word recognition in end-to-end Automatic Speech Recognition\n(E2E ASR) models is a challenge due to the lack of training data. A standard\nway to address this issue is with shallow fusion methods at inference time.\nHowever, due to their dependence on external language models and the\ndeterministic approach to weight boosting, their performance is limited. In\nthis paper, we propose training neural contextual adapters for personalization\nin neural transducer based ASR models. Our approach can not only bias towards\nuser-defined words, but also has the flexibility to work with pretrained ASR\nmodels. Using an in-house dataset, we demonstrate that contextual adapters can\nbe applied to any general purpose pretrained ASR model to improve\npersonalization. Our method outperforms shallow fusion, while retaining\nfunctionality of the pretrained models by not altering any of the model\nweights. We further show that the adapter style training is superior to\nfull-fine-tuning of the ASR models on datasets with user-defined content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sathyendra_K/0/1/0/all/0/1\">Kanthashree Mysore Sathyendra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muniyappa_T/0/1/0/all/0/1\">Thejaswi Muniyappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_F/0/1/0/all/0/1\">Feng-Ju Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinru Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunzmann_S/0/1/0/all/0/1\">Siegfried Kunzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Normalization for Streaming Speech Recognition in a Modular Framework. (arXiv:2205.13674v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13674","description":"<p>We introduce the Globally Normalized Autoregressive Transducer (GNAT) for\naddressing the label bias problem in streaming speech recognition. Our solution\nadmits a tractable exact computation of the denominator for the sequence-level\nnormalization. Through theoretical and empirical results, we demonstrate that\nby switching to a globally normalized model, the word error rate gap between\nstreaming and non-streaming speech-recognition models can be greatly reduced\n(by more than 50\\% on the Librispeech dataset). This model is developed in a\nmodular framework which encompasses all the common neural speech recognition\nmodels. The modularity of this framework enables controlled comparison of\nmodelling choices and creation of new models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Variani_E/0/1/0/all/0/1\">Ehsan Variani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Ke Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riley_M/0/1/0/all/0/1\">Michael Riley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1\">David Rybach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shannon_M/0/1/0/all/0/1\">Matt Shannon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allauzen_C/0/1/0/all/0/1\">Cyril Allauzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiJoNLP at SemEval-2022 Task 2: Detecting Idiomaticity of Multiword Expressions using Multilingual Pretrained Language Models. (arXiv:2205.13708v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13708","description":"<p>This paper describes an approach to detect idiomaticity only from the\ncontextualized representation of a MWE over multilingual pretrained language\nmodels. Our experiments find that larger models are usually more effective in\nidiomaticity detection. However, using a higher layer of the model may not\nguarantee a better performance. In multilingual scenarios, the convergence of\ndifferent languages are not consistent and rich-resource languages have big\nadvantages over other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Minghuan Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Foundation Models Help Us Achieve Perfect Secrecy?. (arXiv:2205.13722v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13722","description":"<p>A key promise of machine learning is the ability to assist users with\npersonal tasks. Because the personal context required to make accurate\npredictions is often sensitive, we require systems that protect privacy. A gold\nstandard privacy-preserving system will satisfy perfect secrecy, meaning that\ninteractions with the system provably reveal no additional private information\nto adversaries. This guarantee should hold even as we perform multiple personal\ntasks over the same underlying data. However, privacy and quality appear to be\nin tension in existing systems for personal tasks. Neural models typically\nrequire lots of training to perform well, while individual users typically hold\na limited scale of data, so the systems propose to learn from the aggregate\ndata of multiple users. This violates perfect secrecy and instead, in the last\nfew years, academics have defended these solutions using statistical notions of\nprivacy -- i.e., the probability of learning private information about a user\nshould be reasonably low. Given the vulnerabilities of these solutions, we\nexplore whether the strong perfect secrecy guarantee can be achieved using\nrecent zero-to-few sample adaptation techniques enabled by foundation models.\nIn response, we propose FOCUS, a framework for personal tasks. Evaluating on\npopular privacy benchmarks, we find the approach, satisfying perfect secrecy,\ncompetes with strong collaborative learning baselines on 6 of 7 tasks. We\nempirically analyze the proposal, highlighting the opportunities and\nlimitations across task types, and model inductive biases and sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Simran Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Long Programming Languages with Structure-Aware Sparse Attention. (arXiv:2205.13730v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13730","description":"<p>Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have\nachieved great success in many downstream code-related tasks. Since the memory\nand computational complexity of self-attention in the Transformer grow\nquadratically with the sequence length, PPLMs typically limit the code length\nto 512. However, codes in real-world applications are generally long, such as\ncode searches, which cannot be processed efficiently by existing PPLMs. To\nsolve this problem, in this paper, we present SASA, a Structure-Aware Sparse\nAttention mechanism, which reduces the complexity and improves performance for\nlong code understanding tasks. The key components in SASA are top-$k$ sparse\nattention and Abstract Syntax Tree (AST)-based structure-aware attention. With\ntop-$k$ sparse attention, the most crucial attention relation can be obtained\nwith a lower computational cost. As the code structure represents the logic of\nthe code statements, which is a complement to the code sequence\ncharacteristics, we further introduce AST structures into attention. Extensive\nexperiments on CodeXGLUE tasks show that SASA achieves better performance than\nthe competing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Ming Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Aoying Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLU for Game-based Learning in Real: Initial Evaluations. (arXiv:2205.13754v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13754","description":"<p>Intelligent systems designed for play-based interactions should be\ncontextually aware of the users and their surroundings. Spoken Dialogue Systems\n(SDS) are critical for these interactive agents to carry out effective\ngoal-oriented communication with users in real-time. For the real-world (i.e.,\nin-the-wild) deployment of such conversational agents, improving the Natural\nLanguage Understanding (NLU) module of the goal-oriented SDS pipeline is\ncrucial, especially with limited task-specific datasets. This study explores\nthe potential benefits of a recently proposed transformer-based multi-task NLU\narchitecture, mainly to perform Intent Recognition on small-size\ndomain-specific educational game datasets. The evaluation datasets were\ncollected from children practicing basic math concepts via play-based\ninteractions in game-based learning settings. We investigate the NLU\nperformances on the initial proof-of-concept game datasets versus the\nreal-world deployment datasets and observe anticipated performance drops\nin-the-wild. We have shown that compared to the more straightforward baseline\napproaches, Dual Intent and Entity Transformer (DIET) architecture is robust\nenough to handle real-world data to a large extent for the Intent Recognition\ntask on these domain-specific in-the-wild game datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okur_E/0/1/0/all/0/1\">Eda Okur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachman_L/0/1/0/all/0/1\">Lama Nachman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGLU 2022: Interactive Grounded Language Understanding in a Collaborative Environment at NeurIPS 2022. (arXiv:2205.13771v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13771","description":"<p>Human intelligence has the remarkable ability to adapt to new tasks and\nenvironments quickly. Starting from a very young age, humans acquire new skills\nand learn how to solve new tasks either by imitating the behavior of others or\nby following provided natural language instructions. To facilitate research in\nthis direction, we propose IGLU: Interactive Grounded Language Understanding in\na Collaborative Environment. The primary goal of the competition is to approach\nthe problem of how to develop interactive embodied agents that learn to solve a\ntask while provided with grounded natural language instructions in a\ncollaborative environment. Understanding the complexity of the challenge, we\nsplit it into sub-tasks to make it feasible for participants.\n</p>\n<p>This research challenge is naturally related, but not limited, to two fields\nof study that are highly relevant to the NeurIPS community: Natural Language\nUnderstanding and Generation (NLU/G) and Reinforcement Learning (RL).\nTherefore, the suggested challenge can bring two communities together to\napproach one of the crucial challenges in AI. Another critical aspect of the\nchallenge is the dedication to perform a human-in-the-loop evaluation as a\nfinal evaluation for the agents developed by contestants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zholus_A/0/1/0/all/0/1\">Artem Zholus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shrestha Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabzadeh_N/0/1/0/all/0/1\">Negar Arabzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teruel_M/0/1/0/all/0/1\">Milagro Teruel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volovikova_Z/0/1/0/all/0/1\">Zoya Volovikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinet_K/0/1/0/all/0/1\">Kavya Srinet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Based Automatic Personality Prediction Using KGrAt-Net; A Knowledge Graph Attention Network Classifier. (arXiv:2205.13780v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13780","description":"<p>Nowadays, a tremendous amount of human communications take place on the\nInternet-based communication infrastructures, like social networks, email,\nforums, organizational communication platforms, etc. Indeed, the automatic\nprediction or assessment of individuals' personalities through their written or\nexchanged text would be advantageous to ameliorate the relationships among\nthem. To this end, this paper aims to propose KGrAt-Net which is a Knowledge\nGraph Attention Network text classifier. For the first time, it applies the\nknowledge graph attention network to perform Automatic Personality Prediction\n(APP), according to the Big Five personality traits. After performing some\npreprocessing activities, first, it tries to acquire a knowingful\nrepresentation of the knowledge behind the concepts in the input text through\nbuilding its equivalent knowledge graph. A knowledge graph is a graph-based\ndata model that formally represents the semantics of the existing concepts in\nthe input text and models the knowledge behind them. Then, applying the\nattention mechanism, it efforts to pay attention to the most relevant parts of\nthe graph to predict the personality traits of the input text. The results\ndemonstrated that KGrAt-Net considerably improved the personality prediction\naccuracies. Furthermore, KGrAt-Net also uses the knowledge graphs' embeddings\nto enrich the classification, which makes it even more accurate in APP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohammad-Ali Balafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nearest Neighbor Zero-Shot Inference. (arXiv:2205.13792v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13792","description":"<p>We introduce kNN-Prompt, a simple and effective technique to use k-nearest\nneighbor (kNN) retrieval augmentation (Khandelwal et al., 2021) for zero-shot\ninference with language models (LMs). Key to our approach is the introduction\nof fuzzy verbalizers which leverage the sparse kNN distribution for downstream\ntasks by automatically associating each classification label with a set of\nnatural language tokens. Across eleven diverse end-tasks (spanning text\nclassification, fact retrieval and question answering), using kNN-Prompt with\nGPT-2 Large yields significant performance boosts over zero-shot baselines (14%\nabsolute improvement over the base LM on average). Extensive experiments show\nthat kNN-Prompt is effective for domain adaptation with no further training,\nand that the benefits of retrieval increase with the size of the model used for\nkNN retrieval. Overall, we show that augmenting a language model with retrieval\ncan bring significant gains for zero-shot inference, with the possibility that\nlarger retrieval models may yield even greater benefits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weijia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Julian Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word Embeddings. (arXiv:2205.13858v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13858","description":"<p>Word embeddings have advanced the state of the art in NLP across numerous\ntasks. Understanding the contents of dense neural representations is of utmost\ninterest to the computational semantics community. We propose to focus on\nrelating these opaque word vectors with human-readable definitions, as found in\ndictionaries. This problem naturally divides into two subtasks: converting\ndefinitions into embeddings, and converting embeddings into definitions. This\ntask was conducted in a multilingual setting, using comparable sets of\nembeddings trained homogeneously.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mickus_T/0/1/0/all/0/1\">Timothee Mickus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_M/0/1/0/all/0/1\">Mathieu Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Automate Follow-up Question Generation using Process Knowledge for Depression Triage on Reddit Posts. (arXiv:2205.13884v1 [cs.AI])","link":"http://arxiv.org/abs/2205.13884","description":"<p>Conversational Agents (CAs) powered with deep language models (DLMs) have\nshown tremendous promise in the domain of mental health. Prominently, the CAs\nhave been used to provide informational or therapeutic services to patients.\nHowever, the utility of CAs to assist in mental health triaging has not been\nexplored in the existing work as it requires a controlled generation of\nfollow-up questions (FQs), which are often initiated and guided by the mental\nhealth professionals (MHPs) in clinical settings. In the context of depression,\nour experiments show that DLMs coupled with process knowledge in a mental\nhealth questionnaire generate 12.54% and 9.37% better FQs based on similarity\nand longest common subsequence matches to questions in the PHQ-9 dataset\nrespectively, when compared with DLMs without process knowledge support.\nDespite coupling with process knowledge, we find that DLMs are still prone to\nhallucination, i.e., generating redundant, irrelevant, and unsafe FQs. We\ndemonstrate the challenge of using existing datasets to train a DLM for\ngenerating FQs that adhere to clinical process knowledge. To address this\nlimitation, we prepared an extended PHQ-9 based dataset, PRIMATE, in\ncollaboration with MHPs. PRIMATE contains annotations regarding whether a\nparticular question in the PHQ-9 dataset has already been answered in the\nuser's initial description of the mental health condition. We used PRIMATE to\ntrain a DLM in a supervised setting to identify which of the PHQ-9 questions\ncan be answered directly from the user's post and which ones would require more\ninformation from the user. Using performance analysis based on MCC scores, we\nshow that PRIMATE is appropriate for identifying questions in PHQ-9 that could\nguide generative DLMs towards controlled FQ generation suitable for aiding\ntriaging. Dataset created as a part of this research:\nhttps://github.com/primate-mh/Primate2022\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shrey Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Anmol Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vignesh Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1\">Ponnurangam Kumaraguru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoInHindi: A Multi-label Emotion and Intensity Annotated Dataset in Hindi for Emotion Recognition in Dialogues. (arXiv:2205.13908v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13908","description":"<p>The long-standing goal of Artificial Intelligence (AI) has been to create\nhuman-like conversational systems. Such systems should have the ability to\ndevelop an emotional connection with the users, hence emotion recognition in\ndialogues is an important task. Emotion detection in dialogues is a challenging\ntask because humans usually convey multiple emotions with varying degrees of\nintensities in a single utterance. Moreover, emotion in an utterance of a\ndialogue may be dependent on previous utterances making the task more complex.\nEmotion recognition has always been in great demand. However, most of the\nexisting datasets for multi-label emotion and intensity detection in\nconversations are in English. To this end, we create a large conversational\ndataset in Hindi named EmoInHindi for multi-label emotion and intensity\nrecognition in conversations containing 1,814 dialogues with a total of 44,247\nutterances. We prepare our dataset in a Wizard-of-Oz manner for mental health\nand legal counselling of crime victims. Each utterance of the dialogue is\nannotated with one or more emotion categories from the 16 emotion classes\nincluding neutral, and their corresponding intensity values. We further propose\nstrong contextual baselines that can detect emotion(s) and the corresponding\nintensity of an utterance given the conversational context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gopendra Vikram Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priya_P/0/1/0/all/0/1\">Priyanshu Priya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firdaus_M/0/1/0/all/0/1\">Mauajama Firdaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense and Named Entity Aware Knowledge Grounded Dialogue Generation. (arXiv:2205.13928v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13928","description":"<p>Grounding dialogue on external knowledge and interpreting linguistic patterns\nin dialogue history context, such as ellipsis, anaphora, and co-references is\ncritical for dialogue comprehension and generation. In this paper, we present a\nnovel open-domain dialogue generation model which effectively utilizes the\nlarge-scale commonsense and named entity based knowledge in addition to the\nunstructured topic-specific knowledge associated with each utterance. We\nenhance the commonsense knowledge with named entity-aware structures using\nco-references. Our proposed model utilizes a multi-hop attention layer to\npreserve the most accurate and critical parts of the dialogue history and the\nassociated knowledge. In addition, we employ a Commonsense and Named Entity\nEnhanced Attention Module, which starts with the extracted triples from various\nsources and gradually finds the relevant supporting set of triples using\nmulti-hop attention with the query vector obtained from the interactive\ndialogue-knowledge module. Empirical results on two benchmark dataset\ndemonstrate that our model significantly outperforms the state-of-the-art\nmethods in terms of both automatic evaluation metrics and human judgment. Our\ncode is publicly available at\n\\href{https://github.com/deekshaVarshney/CNTF}{https://github.com/deekshaVarshney/CNTF};\n\\href{https://www.iitp.ac.in/~ai-nlp-ml/resources/codes/CNTF.zip}{https://www.iitp.ac.in/-ai-nlp-ml/resources/\ncodes/CNTF.zip}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_D/0/1/0/all/0/1\">Deeksha Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_A/0/1/0/all/0/1\">Akshara Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Punctuation Restoration in Spanish Customer Support Transcripts using Transfer Learning. (arXiv:2205.13961v1 [cs.CL])","link":"http://arxiv.org/abs/2205.13961","description":"<p>Automatic Speech Recognition (ASR) systems typically produce unpunctuated\ntranscripts that have poor readability. In addition, building a punctuation\nrestoration system is challenging for low-resource languages, especially for\ndomain-specific applications. In this paper, we propose a Spanish punctuation\nrestoration system designed for a real-time customer support transcription\nservice. To address the data sparsity of Spanish transcripts in the customer\nsupport domain, we introduce two transfer-learning-based strategies: 1) domain\nadaptation using out-of-domain Spanish text data; 2) cross-lingual transfer\nlearning leveraging in-domain English transcript data. Our experiment results\nshow that these strategies improve the accuracy of the Spanish punctuation\nrestoration system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiliang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardiner_S/0/1/0/all/0/1\">Shayna Gardiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossouw_D/0/1/0/all/0/1\">David Rossouw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roldan_T/0/1/0/all/0/1\">Tere Rold&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corston_Oliver_S/0/1/0/all/0/1\">Simon Corston-Oliver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14014","description":"<p>Transformers have made progress in miscellaneous tasks, but suffer from\nquadratic computational and memory complexities. Recent works propose sparse\nTransformers with attention on sparse graphs to reduce complexity and remain\nstrong performance. While effective, the crucial parts of how dense a graph\nneeds to be to perform well are not fully explored. In this paper, we propose\nNormalized Information Payload (NIP), a graph scoring function measuring\ninformation transfer on graph, which provides an analysis tool for trade-offs\nbetween performance and complexity. Guided by this theoretical analysis, we\npresent Hypercube Transformer, a sparse Transformer that models token\ninteractions in a hypercube and shows comparable or even better results with\nvanilla Transformer while yielding $O(N\\log N)$ complexity with sequence length\n$N$. Experiments on tasks requiring various sequence lengths lay validation for\nour graph function well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chu-Tak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes. (arXiv:2205.14036v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14036","description":"<p>Analyzing ethnic or religious bias is important for improving fairness,\naccountability, and transparency of natural language processing models.\nHowever, many techniques rely on human-compiled lists of bias terms, which are\nexpensive to create and are limited in coverage. In this study, we present a\nfully data-driven pipeline for generating a knowledge graph (KG) of cultural\nknowledge and stereotypes. Our resulting KG covers 5 religious groups and 5\nnationalities and can easily be extended to include more entities. Our human\nevaluation shows that the majority (59.2%) of non-singleton entries are\ncoherent and complete stereotypes. We further show that performing intermediate\nmasked language model training on the verbalized KG leads to a higher level of\ncultural awareness in the model and has the potential to increase\nclassification performance on knowledge-crucial samples on a related task,\ni.e., hate speech detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Awantee Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UAlberta at SemEval 2022 Task 2: Leveraging Glosses and Translations for Multilingual Idiomaticity Detection. (arXiv:2205.14084v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14084","description":"<p>We describe the University of Alberta systems for the SemEval-2022 Task 2 on\nmultilingual idiomaticity detection. Working under the assumption that\nidiomatic expressions are noncompositional, our first method integrates\ninformation on the meanings of the individual words of an expression into a\nbinary classifier. Further hypothesizing that literal and idiomatic expressions\ntranslate differently, our second method translates an expression in context,\nand uses a lexical knowledge base to determine if the translation is literal.\nOur approaches are grounded in linguistic phenomena, and leverage existing\nsources of lexical knowledge. Our results offer support for both approaches,\nparticularly the former.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaura_S/0/1/0/all/0/1\">Seeratpal Jaura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omarov_T/0/1/0/all/0/1\">Talgat Omarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patching Leaks in the Charformer for Efficient Character-Level Generation. (arXiv:2205.14086v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14086","description":"<p>Character-based representations have important advantages over subword-based\nones for morphologically rich languages. They come with increased robustness to\nnoisy input and do not need a separate tokenization step. However, they also\nhave a crucial disadvantage: they notably increase the length of text\nsequences. The GBST method from Charformer groups (aka downsamples) characters\nto solve this, but allows information to leak when applied to a Transformer\ndecoder. We solve this information leak issue, thereby enabling character\ngrouping in the decoder. We show that Charformer downsampling has no apparent\nbenefits in NMT over previous downsampling methods in terms of translation\nquality, however it can be trained roughly 30% faster. Promising performance on\nEnglish--Turkish translation indicate the potential of character-level models\nfor morphologically-rich languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edman_L/0/1/0/all/0/1\">Lukas Edman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_G/0/1/0/all/0/1\">Gertjan van Noord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior. (arXiv:2205.14140v1 [cs.CL])","link":"http://arxiv.org/abs/2205.14140","description":"<p>The increasing size and complexity of modern ML systems has improved their\npredictive capabilities but made their behavior harder to explain. Many\ntechniques for model explanation have been developed in response, but we lack\nclear criteria for assessing these techniques. In this paper, we cast model\nexplanation as the causal inference problem of estimating causal effects of\nreal-world concepts on the output behavior of ML models given actual input\ndata. We introduce CEBaB, a new benchmark dataset for assessing concept-based\nexplanation methods in Natural Language Processing (NLP). CEBaB consists of\nshort restaurant reviews with human-generated counterfactual reviews in which\nan aspect (food, noise, ambiance, service) of the dining experience was\nmodified. Original and counterfactual reviews are annotated with\nmultiply-validated sentiment ratings at the aspect-level and review-level. The\nrich structure of CEBaB allows us to go beyond input features to study the\neffects of abstract, real-world concepts on model behavior. We use CEBaB to\ncompare the quality of a range of concept-based explanation methods covering\ndifferent assumptions and conceptions of the problem, and we seek to establish\nnatural metrics for comparative assessments of these methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abraham_E/0/1/0/all/0/1\">Eldar David Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DOosterlinck_K/0/1/0/all/0/1\">Karel D&#x27;Oosterlinck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_Y/0/1/0/all/0/1\">Yair Ori Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Atticus Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.05268","description":"<p>Recently generating natural language explanations has shown very promising\nresults in not only offering interpretable explanations but also providing\nadditional information and supervision for prediction. However, existing\napproaches usually require a large set of human annotated explanations for\ntraining while collecting a large set of explanations is not only time\nconsuming but also expensive. In this paper, we develop a general framework for\ninterpretable natural language understanding that requires only a small set of\nhuman annotated explanations for training. Our framework treats natural\nlanguage explanations as latent variables that model the underlying reasoning\nprocess of a neural model. We develop a variational EM framework for\noptimization where an explanation generation module and an\nexplanation-augmented prediction module are alternatively optimized and\nmutually enhance each other. Moreover, we further propose an explanation-based\nself-training method under this framework for semi-supervised learning. It\nalternates between assigning pseudo-labels to unlabeled data and generating new\nexplanations to iteratively improve each other. Experiments on two natural\nlanguage understanding tasks demonstrate that our framework can not only make\neffective predictions in both supervised and semi-supervised settings, but also\ngenerate good natural language explanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03824","description":"<p>We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.13856","description":"<p>Large-scale models for learning fixed-dimensional cross-lingual sentence\nrepresentations like LASER (Artetxe and Schwenk, 2019b) lead to significant\nimprovement in performance on downstream tasks. However, further increases and\nmodifications based on such large-scale models are usually impractical due to\nmemory limitations. In this work, we introduce a lightweight dual-transformer\narchitecture with just 2 layers for generating memory-efficient cross-lingual\nsentence representations. We explore different training tasks and observe that\ncurrent cross-lingual training tasks leave a lot to be desired for this shallow\narchitecture. To ameliorate this, we propose a novel cross-lingual language\nmodel, which combines the existing single-word masked language model with the\nnewly proposed cross-lingual token-level reconstruction task. We further\naugment the training task by the introduction of two computationally-lite\nsentence-level contrastive learning tasks to enhance the alignment of\ncross-lingual sentence representation space, which compensates for the learning\nbottleneck of the lightweight transformer for generative tasks. Our comparisons\nwith competing models on cross-lingual sentence retrieval and multilingual\ndocument classification confirm the effectiveness of the newly proposed\ntraining tasks for a shallow model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Korean-English Machine Translation with Multiple Tokenization Strategy. (arXiv:2105.14274v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14274","description":"<p>This work was conducted to find out how tokenization methods affect the\ntraining results of machine translation models. In this work, alphabet\ntokenization, morpheme tokenization, and BPE tokenization were applied to\nKorean as the source language and English as the target language respectively,\nand the comparison experiment was conducted by repeating 50,000 epochs of each\n9 models using the Transformer neural network. As a result of measuring the\nBLEU scores of the experimental models, the model that applied BPE tokenization\nto Korean and morpheme tokenization to English recorded 35.73, showing the best\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dojun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Youngjin Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Harksoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar Accuracy Evaluation (GAE): Quantifiable Quantitative Evaluation of Machine Translation Models. (arXiv:2105.14277v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14277","description":"<p>Natural Language Generation (NLG) refers to the operation of expressing the\ncalculation results of a system in human language. Since the quality of\ngenerated sentences from an NLG model cannot be fully represented using only\nquantitative evaluation, they are evaluated using qualitative evaluation by\nhumans in which the meaning or grammar of a sentence is scored according to a\nsubjective criterion. Nevertheless, the existing evaluation methods have a\nproblem as a large score deviation occurs depending on the criteria of\nevaluators. In this paper, we propose Grammar Accuracy Evaluation (GAE) that\ncan provide the specific evaluating criteria. As a result of analyzing the\nquality of machine translation by BLEU and GAE, it was confirmed that the BLEU\nscore does not represent the absolute performance of machine translation models\nand GAE compensates for the shortcomings of BLEU with flexible evaluation of\nalternative synonyms and changes in sentence structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dojun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Youngjin Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Harksoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assisting Decision Making in Scholarly Peer Review: A Preference Learning Perspective. (arXiv:2109.01190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01190","description":"<p>Peer review is the primary means of quality control in academia; as an\noutcome of a peer review process, program and area chairs make acceptance\ndecisions for each paper based on the review reports and scores they received.\nQuality of scientific work is multi-faceted; coupled with the subjectivity of\nreviewing, this makes final decision making difficult and time-consuming. To\nsupport this final step of peer review, we formalize it as a paper ranking\nproblem. We introduce a novel, multi-faceted generic evaluation framework for\nranking submissions based on peer reviews that takes into account\neffectiveness, efficiency and fairness. We propose a preference learning\nperspective on the task that considers both review texts and scores to\nalleviate the inevitable bias and noise in reviews. Our experiments on peer\nreview data from the ACL 2018 conference demonstrate the superiority of our\npreference-learning-based approach over baselines and prior work, while\nhighlighting the importance of using both review texts and scores to rank\nsubmissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dycke_N/0/1/0/all/0/1\">Nils Dycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_E/0/1/0/all/0/1\">Edwin Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.02358","description":"<p>We present a unified Vision-Language pretrained Model (VLMo) that jointly\nlearns a dual encoder and a fusion encoder with a modular Transformer network.\nSpecifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,\nwhere each block contains a pool of modality-specific experts and a shared\nself-attention layer. Because of the modeling flexibility of MoME, pretrained\nVLMo can be fine-tuned as a fusion encoder for vision-language classification\ntasks, or used as a dual encoder for efficient image-text retrieval. Moreover,\nwe propose a stagewise pre-training strategy, which effectively leverages\nlarge-scale image-only and text-only data besides image-text pairs.\nExperimental results show that VLMo achieves state-of-the-art results on\nvarious vision-language tasks, including VQA, NLVR2 and image-text retrieval.\nThe code and pretrained models are available at https://aka.ms/vlmo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_O/0/1/0/all/0/1\">Owais Khan Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kriti Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Som_S/0/1/0/all/0/1\">Subhojit Som</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\textsc{CIS}^2}: A Simplified Commonsense Inference Evaluation for Story Prose. (arXiv:2202.07880v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07880","description":"<p>Transformers have been showing near-human performance on a variety of tasks,\nbut they are not without their limitations. We discuss the issue of conflating\nresults of transformers that are instructed to do multiple tasks\nsimultaneously. In particular, we focus on the domain of commonsense reasoning\nwithin story prose, which we call contextual commonsense inference (CCI). We\nlook at the GLUCOSE (Mostafazadeh et al. 2020) dataset and task for predicting\nimplicit commonsense inferences between story sentences. Since the GLUCOSE task\nsimultaneously generates sentences and predicts the CCI relation, there is a\nconflation in the results. Is the model really measuring CCI or is its ability\nto generate grammatical text carrying the results? In this paper, we introduce\nthe task contextual commonsense inference in sentence selection\n($\\textsc{CIS}^2}), a simplified task that avoids conflation by eliminating\nlanguage generation altogether. Our findings emphasize the necessity of future\nwork to disentangle language generation from the desired NLP tasks at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bryan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_L/0/1/0/all/0/1\">Lara J. Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning English with Peppa Pig. (arXiv:2202.12917v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12917","description":"<p>Recent computational models of the acquisition of spoken language via\ngrounding in perception exploit associations between the spoken and visual\nmodalities and learn to represent speech and visual data in a joint vector\nspace. A major unresolved issue from the point of ecological validity is the\ntraining data, typically consisting of images or videos paired with spoken\ndescriptions of what is depicted. Such a setup guarantees an unrealistically\nstrong correlation between speech and the visual data. In the real world the\ncoupling between the linguistic and the visual modality is loose, and often\nconfounded by correlations with non-semantic aspects of the speech signal. Here\nwe address this shortcoming by using a dataset based on the children's cartoon\nPeppa Pig. We train a simple bi-modal architecture on the portion of the data\nconsisting of dialog between characters, and evaluate on segments containing\ndescriptive narrations. Despite the weak and confounded signal in this training\ndata our model succeeds at learning aspects of the visual semantics of spoken\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolaus_M/0/1/0/all/0/1\">Mitja Nikolaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alishahi_A/0/1/0/all/0/1\">Afra Alishahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00598","description":"<p>Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attarian_M/0/1/0/all/0/1\">Maria Attarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Adrian Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welker_S/0/1/0/all/0/1\">Stefan Welker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_A/0/1/0/all/0/1\">Aveek Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindhwani_V/0/1/0/all/0/1\">Vikas Sindhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Johnny Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1\">Vincent Vanhoucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Training of Neural Transducer for Speech Recognition. (arXiv:2204.10586v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10586","description":"<p>As one of the most popular sequence-to-sequence modeling approaches for\nspeech recognition, the RNN-Transducer has achieved evolving performance with\nmore and more sophisticated neural network models of growing size and\nincreasing training epochs. While strong computation resources seem to be the\nprerequisite of training superior models, we try to overcome it by carefully\ndesigning a more efficient training pipeline. In this work, we propose an\nefficient 3-stage progressive training pipeline to build highly-performing\nneural transducer models from scratch with very limited computation resources\nin a reasonable short time period. The effectiveness of each stage is\nexperimentally verified on both Librispeech and Switchboard corpora. The\nproposed pipeline is able to train transducer models approaching\nstate-of-the-art performance with a single GPU in just 2-3 weeks. Our best\nconformer transducer achieves 4.1% WER on Librispeech test-other with only 35\nepochs of training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1\">Wilfried Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021. (arXiv:2205.02388v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02388","description":"<p>Human intelligence has the remarkable ability to quickly adapt to new tasks\nand environments. Starting from a very young age, humans acquire new skills and\nlearn how to solve new tasks either by imitating the behavior of others or by\nfollowing provided natural language instructions. To facilitate research in\nthis direction, we propose \\emph{IGLU: Interactive Grounded Language\nUnderstanding in a Collaborative Environment}.\n</p>\n<p>The primary goal of the competition is to approach the problem of how to\nbuild interactive agents that learn to solve a task while provided with\ngrounded natural language instructions in a collaborative environment.\nUnderstanding the complexity of the challenge, we split it into sub-tasks to\nmake it feasible for participants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiseleva_J/0/1/0/all/0/1\">Julia Kiseleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohanty_S/0/1/0/all/0/1\">Shrestha Mohanty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1\">Mikhail Burtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrynnik_A/0/1/0/all/0/1\">Alexey Skrynnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zholus_A/0/1/0/all/0/1\">Artem Zholus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1\">Aleksandr Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinet_K/0/1/0/all/0/1\">Kavya Srinet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cote_M/0/1/0/all/0/1\">Marc-Alexandre C&#xf4;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdrazakov_L/0/1/0/all/0/1\">Linar Abdrazakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churin_I/0/1/0/all/0/1\">Igor Churin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manggala_P/0/1/0/all/0/1\">Putra Manggala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naszadi_K/0/1/0/all/0/1\">Kata Naszadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meer_M/0/1/0/all/0/1\">Michiel van der Meer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of COVID-19 Pandemic on LGBTQ Online Communities. (arXiv:2205.09511v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2205.09511","description":"<p>The COVID-19 pandemic has disproportionately impacted the lives of\nminorities, such as members of the LGBTQ community (lesbian, gay, bisexual,\ntransgender, and queer) due to pre-existing social disadvantages and health\ndisparities. Although extensive research has been carried out on the impact of\nthe COVID-19 pandemic on different aspects of the general population's lives,\nfew studies are focused on the LGBTQ population. In this paper, we identify a\ngroup of Twitter users who self-disclose to belong to the LGBTQ community. We\ndevelop and evaluate two sets of machine learning classifiers using a\npre-pandemic and a during pandemic dataset to identify Twitter posts exhibiting\nminority stress, which is a unique pressure faced by the members of the LGBTQ\npopulation due to their sexual and gender identities. For this task, we collect\na set of 20,593,823 posts by 7,241 self-disclosed LGBTQ users and annotate a\nrandomly selected subset of 2800 posts. We demonstrate that our best\npre-pandemic and during pandemic models show strong and stable performance for\ndetecting posts that contain minority stress. We investigate the linguistic\ndifferences in minority stress posts across pre- and during-pandemic periods.\nWe find that anger words are strongly associated with minority stress during\nthe COVID-19 pandemic. We explore the impact of the pandemic on the emotional\nstates of the LGBTQ population by conducting controlled comparisons with the\ngeneral population. We adopt propensity score-based matching to perform a\ncausal analysis. The results show that the LBGTQ population have a greater\nincrease in the usage of cognitive words and worsened observable attribute in\nthe usage of positive emotion words than the group of the general population\nwith similar pre-pandemic behavioral attributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1\">Gaurav Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_B/0/1/0/all/0/1\">Barbara Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aledavood_T/0/1/0/all/0/1\">Talayeh Aledavood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let the Model Decide its Curriculum for Multitask Learning. (arXiv:2205.09898v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.09898","description":"<p>Curriculum learning strategies in prior multi-task learning approaches\narrange datasets in a difficulty hierarchy either based on human perception or\nby exhaustively searching the optimal arrangement. However, human perception of\ndifficulty may not always correlate well with machine interpretation leading to\npoor performance and exhaustive search is computationally expensive. Addressing\nthese concerns, we propose two classes of techniques to arrange training\ninstances into a learning curriculum based on difficulty scores computed via\nmodel-based approaches. The two classes i.e Dataset-level and Instance-level\ndiffer in granularity of arrangement. Through comprehensive experiments with 12\ndatasets, we show that instance-level and dataset-level techniques result in\nstrong representations as they lead to an average performance improvement of\n4.17% and 3.15% over their respective baselines. Furthermore, we find that most\nof this improvement comes from correctly answering the difficult instances,\nimplying a greater efficacy of our techniques on difficult tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11211","description":"<p>End-to-End Speech Translation (E2E-ST) has received increasing attention due\nto the potential of its less error propagation, lower latency, and fewer\nparameters. However, the effectiveness of neural-based approaches to this task\nis severely limited by the available training corpus, especially for domain\nadaptation where in-domain triplet training data is scarce or nonexistent. In\nthis paper, we propose a novel non-parametric method that leverages\ndomain-specific text translation corpus to achieve domain adaptation for the\nE2E-ST system. To this end, we first incorporate an additional encoder into the\npre-trained E2E-ST model to realize text translation modelling, and then unify\nthe decoder's output representation for text and speech translation tasks by\nreducing the correspondent representation mismatch in available triplet\ntraining data. During domain adaptation, a k-nearest-neighbor (kNN) classifier\nis introduced to produce the final translation distribution using the external\ndatastore built by the domain-specific text translation corpus, while the\nuniversal output representation is adopted to perform a similarity search.\nExperiments on the Europarl-ST benchmark demonstrate that when in-domain text\ntranslation data is involved only, our proposed approach significantly improves\nbaseline by 12.82 BLEU on average in all translation directions, even\noutperforming the strong in-domain fine-tuning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"CA-UDA: Class-Aware Unsupervised Domain Adaptation with Optimal Assignment and Pseudo-Label Refinement. (arXiv:2205.13579v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13579","description":"<p>Recent works on unsupervised domain adaptation (UDA) focus on the selection\nof good pseudo-labels as surrogates for the missing labels in the target data.\nHowever, source domain bias that deteriorates the pseudo-labels can still exist\nsince the shared network of the source and target domains are typically used\nfor the pseudo-label selections. The suboptimal feature space source-to-target\ndomain alignment can also result in unsatisfactory performance. In this paper,\nwe propose CA-UDA to improve the quality of the pseudo-labels and UDA results\nwith optimal assignment, a pseudo-label refinement strategy and class-aware\ndomain alignment. We use an auxiliary network to mitigate the source domain\nbias for pseudo-label refinement. Our intuition is that the underlying\nsemantics in the target domain can be fully exploited to help refine the\npseudo-labels that are inferred from the source features under domain shift.\nFurthermore, our optimal assignment can optimally align features in the\nsource-to-target domains and our class-aware domain alignment can\nsimultaneously close the domain gap while preserving the classification\ndecision boundaries. Extensive experiments on several benchmark datasets show\nthat our method can achieve state-of-the-art performance in the image\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis. (arXiv:2205.13583v1 [cs.AI])","link":"http://arxiv.org/abs/2205.13583","description":"<p>Eosinophilic esophagitis (EoE) is a chronic allergic inflammatory condition\nof the esophagus associated with elevated esophageal eosinophils. Second only\nto gastroesophageal reflux disease, EoE is one of the leading causes of chronic\nrefractory dysphagia in adults and children. EoE diagnosis requires enumerating\nthe density of esophageal eosinophils in esophageal biopsies, a somewhat\nsubjective task that is time-consuming, thus reducing the ability to process\nthe complex tissue structure. Previous artificial intelligence (AI) approaches\nthat aimed to improve histology-based diagnosis focused on recapitulating\nidentification and quantification of the area of maximal eosinophil density.\nHowever, this metric does not account for the distribution of eosinophils or\nother histological features, over the whole slide image. Here, we developed an\nartificial intelligence platform that infers local and spatial biomarkers based\non semantic segmentation of intact eosinophils and basal zone distributions.\nBesides the maximal density of eosinophils (referred to as Peak Eosinophil\nCount [PEC]) and a maximal basal zone fraction, we identify two additional\nmetrics that reflect the distribution of eosinophils and basal zone fractions.\nThis approach enables a decision support system that predicts EoE activity and\nclassifies the histological severity of EoE patients. We utilized a cohort that\nincludes 1066 biopsy slides from 400 subjects to validate the system's\nperformance and achieved a histological severity classification accuracy of\n86.70%, sensitivity of 84.50%, and specificity of 90.09%. Our approach\nhighlights the importance of systematically analyzing the distribution of\nbiopsy features over the entire slide and paves the way towards a personalized\ndecision support system that will assist not only in counting cells but can\nalso potentially improve diagnosis and provide treatment prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larey_A/0/1/0/all/0/1\">Ariel Larey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aknin_E/0/1/0/all/0/1\">Eliel Aknin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniel_N/0/1/0/all/0/1\">Nati Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osswald_G/0/1/0/all/0/1\">Garrett A. Osswald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caldwell_J/0/1/0/all/0/1\">Julie M. Caldwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rochman_M/0/1/0/all/0/1\">Mark Rochman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserman_T/0/1/0/all/0/1\">Tanya Wasserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Margaret H. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arva_N/0/1/0/all/0/1\">Nicoleta C. Arva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rothenberg_M/0/1/0/all/0/1\">Marc E. Rothenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savir_Y/0/1/0/all/0/1\">Yonatan Savir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VectorAdam for Rotation Equivariant Geometry Optimization. (arXiv:2205.13599v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13599","description":"<p>The rise of geometric problems in machine learning has necessitated the\ndevelopment of equivariant methods, which preserve their output under the\naction of rotation or some other transformation. At the same time, the Adam\noptimization algorithm has proven remarkably effective across machine learning\nand even traditional tasks in geometric optimization. In this work, we observe\nthat naively applying Adam to optimize vector-valued data is not rotation\nequivariant, due to per-coordinate moment updates, and in fact this leads to\nsignificant artifacts and biases in practice. We propose to resolve this\ndeficiency with VectorAdam, a simple modification which makes Adam\nrotation-equivariant by accounting for the vector structure of optimization\nvariables. We demonstrate this approach on problems in machine learning and\ntraditional geometric optimization, showing that equivariant VectorAdam\nresolves the artifacts and biases of traditional Adam when applied to\nvector-valued data, with equivalent or even improved rates of convergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1\">Selena Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharp_N/0/1/0/all/0/1\">Nicholas Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1\">Alec Jacobson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Circumventing Backdoor Defenses That Are Based on Latent Separability. (arXiv:2205.13613v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13613","description":"<p>Deep learning models are vulnerable to backdoor poisoning attacks. In\nparticular, adversaries can embed hidden backdoors into a model by only\nmodifying a very small portion of its training data. On the other hand, it has\nalso been commonly observed that backdoor poisoning attacks tend to leave a\ntangible signature in the latent space of the backdoored model i.e. poison\nsamples and clean samples form two separable clusters in the latent space.\nThese observations give rise to the popularity of latent separability\nassumption, which states that the backdoored DNN models will learn separable\nlatent representations for poison and clean populations. A number of popular\ndefenses (e.g. Spectral Signature, Activation Clustering, SCAn, etc.) are\nexactly built upon this assumption. However, in this paper, we show that the\nlatent separation can be significantly suppressed via designing adaptive\nbackdoor poisoning attacks with more sophisticated poison strategies, which\nconsequently render state-of-the-art defenses based on this assumption less\neffective (and often completely fail). More interestingly, we find that our\nadaptive attacks can even evade some other typical backdoor defenses that do\nnot explicitly build on this separability assumption. Our results show that\nadaptive backdoor poisoning attacks that can breach the latent separability\nassumption should be seriously considered for evaluating existing and future\ndefenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiangyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tinghao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations. (arXiv:2205.13616v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13616","description":"<p>In this work, we study poison samples detection for defending against\nbackdoor poisoning attacks on deep neural networks (DNNs). A principled idea\nunderlying prior arts on this problem is to utilize the backdoored models'\ndistinguishable behaviors on poison and clean populations to distinguish\nbetween these two different populations themselves and remove the identified\npoison. Many prior arts build their detectors upon a latent separability\nassumption, which states that backdoored models trained on the poisoned dataset\nwill learn separable latent representations for backdoor and clean samples.\nAlthough such separation behaviors empirically exist for many existing attacks,\nthere is no control on the separability and the extent of separation can vary a\nlot across different poison strategies, datasets, as well as the training\nconfigurations of backdoored models. Worse still, recent adaptive poison\nstrategies can greatly reduce the \"distinguishable behaviors\" and consequently\nrender most prior arts less effective (or completely fail). We point out that\nthese limitations directly come from the passive reliance on some\ndistinguishable behaviors that are not controlled by defenders. To mitigate\nsuch limitations, in this work, we propose the idea of active defense -- rather\nthan passively assuming backdoored models will have certain distinguishable\nbehaviors on poison and clean samples, we propose to actively enforce the\ntrained models to behave differently on these two different populations.\nSpecifically, we introduce confusion training as a concrete instance of active\ndefense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiangyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tinghao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahloujifar_S/0/1/0/all/0/1\">Saeed Mahloujifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Prateek Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denial-of-Service Attack on Object Detection Model Using Universal Adversarial Perturbation. (arXiv:2205.13618v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13618","description":"<p>Adversarial attacks against deep learning-based object detectors have been\nstudied extensively in the past few years. The proposed attacks aimed solely at\ncompromising the models' integrity (i.e., trustworthiness of the model's\nprediction), while adversarial attacks targeting the models' availability, a\ncritical aspect in safety-critical domains such as autonomous driving, have not\nbeen explored by the machine learning research community. In this paper, we\npropose NMS-Sponge, a novel approach that negatively affects the decision\nlatency of YOLO, a state-of-the-art object detector, and compromises the\nmodel's availability by applying a universal adversarial perturbation (UAP). In\nour experiments, we demonstrate that the proposed UAP is able to increase the\nprocessing time of individual frames by adding \"phantom\" objects while\npreserving the detection of the original objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shapira_A/0/1/0/all/0/1\">Avishag Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zolfi_A/0/1/0/all/0/1\">Alon Zolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demetrio_L/0/1/0/all/0/1\">Luca Demetrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1\">Asaf Shabtai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Neural Autoencoder for Sensory Neuroprostheses and Its Applications in Bionic Vision. (arXiv:2205.13623v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13623","description":"<p>Sensory neuroprostheses are emerging as a promising technology to restore\nlost sensory function or augment human capacities. However, sensations elicited\nby current devices often appear artificial and distorted. Although current\nmodels can often predict the neural or perceptual response to an electrical\nstimulus, an optimal stimulation strategy solves the inverse problem: what is\nthe required stimulus to produce a desired response? Here we frame this as an\nend-to-end optimization problem, where a deep neural network encoder is trained\nto invert a known, fixed forward model that approximates the underlying\nbiological system. As a proof of concept, we demonstrate the effectiveness of\nour hybrid neural autoencoder (HNA) on the use case of visual neuroprostheses.\nWe found that HNA is able to produce high-fidelity stimuli from the MNIST and\nCOCO datasets that outperform conventional encoding strategies and surrogate\ntechniques across all tested conditions. Overall this is an important step\ntowards the long-standing challenge of restoring high-quality vision to people\nliving with incurable blindness and may prove a promising solution for a\nvariety of neuroprosthetic technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Granley_J/0/1/0/all/0/1\">Jacob Granley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Relic_L/0/1/0/all/0/1\">Lucas Relic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyeler_M/0/1/0/all/0/1\">Michael Beyeler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation. (arXiv:2205.13629v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13629","description":"<p>Robust environment perception for autonomous vehicles is a tremendous\nchallenge, which makes a diverse sensor set with e.g. camera, lidar and radar\ncrucial. In the process of understanding the recorded sensor data, 3D semantic\nsegmentation plays an important role. Therefore, this work presents a\npyramid-based deep fusion architecture for lidar and camera to improve 3D\nsemantic segmentation of traffic scenes. Individual sensor backbones extract\nfeature maps of camera images and lidar point clouds. A novel Pyramid Fusion\nBackbone fuses these feature maps at different scales and combines the\nmultimodal features in a feature pyramid to compute valuable multimodal,\nmulti-scale features. The Pyramid Fusion Head aggregates these pyramid features\nand further refines them in a late fusion step, incorporating the final\nfeatures of the sensor backbones. The approach is evaluated on two challenging\noutdoor datasets and different fusion strategies and setups are investigated.\nIt outperforms recent range view based lidar approaches as well as all so far\nproposed fusion strategies and architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schieber_H/0/1/0/all/0/1\">Hannah Schieber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duerr_F/0/1/0/all/0/1\">Fabian Duerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoen_T/0/1/0/all/0/1\">Torsten Schoen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyerer_J/0/1/0/all/0/1\">J&#xfc;rgen Beyerer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-temporally separable non-linear latent factor learning: an application to somatomotor cortex fMRI data. (arXiv:2205.13640v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13640","description":"<p>Functional magnetic resonance imaging (fMRI) data contain complex\nspatiotemporal dynamics, thus researchers have developed approaches that reduce\nthe dimensionality of the signal while extracting relevant and interpretable\ndynamics. Models of fMRI data that can perform whole-brain discovery of\ndynamical latent factors are understudied. The benefits of approaches such as\nlinear independent component analysis models have been widely appreciated,\nhowever, nonlinear extensions of these models present challenges in terms of\nidentification. Deep learning methods provide a way forward, but new methods\nfor efficient spatial weight-sharing are critical to deal with the high\ndimensionality of the data and the presence of noise. Our approach generalizes\nweight sharing to non-Euclidean neuroimaging data by first performing spectral\nclustering based on the structural and functional similarity between voxels.\nThe spectral clusters and their assignments can then be used as patches in an\nadapted multi-layer perceptron (MLP)-mixer model to share parameters among\ninput points. To encourage temporally independent latent factors, we use an\nadditional total correlation term in the loss. Our approach is evaluated on\ndata with multiple motor sub-tasks to assess whether the model captures\ndisentangled latent factors that correspond to each sub-task. Then, to assess\nthe latent factors we find further, we compare the spatial location of each\nlatent factor to the motor homunculus. Finally, we show that our approach\ncaptures task effects better than the current gold standard of source signal\nseparation, independent component analysis (ICA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geenjaar_E/0/1/0/all/0/1\">Eloy Geenjaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_A/0/1/0/all/0/1\">Amrit Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_N/0/1/0/all/0/1\">Noah Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_R/0/1/0/all/0/1\">Robyn Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1\">Vince Calhoun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Membership Inference Attack Using Self Influence Functions. (arXiv:2205.13680v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13680","description":"<p>Member inference (MI) attacks aim to determine if a specific data sample was\nused to train a machine learning model. Thus, MI is a major privacy threat to\nmodels trained on private sensitive data, such as medical records. In MI\nattacks one may consider the black-box settings, where the model's parameters\nand activations are hidden from the adversary, or the white-box case where they\nare available to the attacker. In this work, we focus on the latter and present\na novel MI attack for it that employs influence functions, or more specifically\nthe samples' self-influence scores, to perform the MI prediction. We evaluate\nour attack on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets, using versatile\narchitectures such as AlexNet, ResNet, and DenseNet. Our attack method achieves\nnew state-of-the-art results for both training with and without data\naugmentations. Code is available at\nhttps://github.com/giladcohen/sif_mi_attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_G/0/1/0/all/0/1\">Gilad Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANISE: Assembly-based Neural Implicit Surface rEconstruction. (arXiv:2205.13682v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13682","description":"<p>We present ANISE, a method that reconstructs a 3D shape from partial\nobservations (images or sparse point clouds) using a part-aware neural implicit\nshape representation. It is formulated as an assembly of neural implicit\nfunctions, each representing a different shape part. In contrast to previous\napproaches, the prediction of this representation proceeds in a coarse-to-fine\nmanner. Our network first predicts part transformations which are associated\nwith part neural implicit functions conditioned on those transformations. The\npart implicit functions can then be combined into a single, coherent shape,\nenabling part-aware shape reconstructions from images and point clouds. Those\nreconstructions can be obtained in two ways: (i) by directly decoding combining\nthe refined part implicit functions; or (ii) by using part latents to query\nsimilar parts in a part database and assembling them in a single shape. We\ndemonstrate that, when performing reconstruction by decoding part\nrepresentations into implicit functions, our method achieves state-of-the-art\npart-aware reconstruction results from both images and sparse point clouds.\nWhen reconstructing shapes by assembling parts queried from a dataset, our\napproach significantly outperforms traditional shape retrieval methods even\nwhen significantly restricting the size of the shape database. We present our\nresults in well-known sparse point cloud reconstruction and single-view\nreconstruction benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petrov_D/0/1/0/all/0/1\">Dmitry Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadelha_M/0/1/0/all/0/1\">Matheus Gadelha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mech_R/0/1/0/all/0/1\">Radomir Mech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences. (arXiv:2205.13713v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13713","description":"<p>Point cloud sequences are irregular and unordered in the spatial dimension\nwhile exhibiting regularities and order in the temporal dimension. Therefore,\nexisting grid based convolutions for conventional video processing cannot be\ndirectly applied to spatio-temporal modeling of raw point cloud sequences. In\nthis paper, we propose a point spatio-temporal (PST) convolution to achieve\ninformative representations of point cloud sequences. The proposed PST\nconvolution first disentangles space and time in point cloud sequences. Then, a\nspatial convolution is employed to capture the local structure of points in the\n3D space, and a temporal convolution is used to model the dynamics of the\nspatial regions along the time dimension. Furthermore, we incorporate the\nproposed PST convolution into a deep network, namely PSTNet, to extract\nfeatures of point cloud sequences in a hierarchical manner. Extensive\nexperiments on widely-used 3D action recognition and 4D semantic segmentation\ndatasets demonstrate the effectiveness of PSTNet to model point cloud\nsequences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hehe Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuhang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Abstract Reasoning with Dual-Contrast Network. (arXiv:2205.13720v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13720","description":"<p>As a step towards improving the abstract reasoning capability of machines, we\naim to solve Raven's Progressive Matrices (RPM) with neural networks, since\nsolving RPM puzzles is highly correlated with human intelligence. Unlike\nprevious methods that use auxiliary annotations or assume hidden rules to\nproduce appropriate feature representation, we only use the ground truth answer\nof each question for model learning, aiming for an intelligent agent to have a\nstrong learning capability with a small amount of supervision. Based on the RPM\nproblem formulation, the correct answer filled into the missing entry of the\nthird row/column has to best satisfy the same rules shared between the first\ntwo rows/columns. Thus we design a simple yet effective Dual-Contrast Network\n(DCNet) to exploit the inherent structure of RPM puzzles. Specifically, a rule\ncontrast module is designed to compare the latent rules between the filled\nrow/column and the first two rows/columns; a choice contrast module is designed\nto increase the relative differences between candidate choices. Experimental\nresults on the RAVEN and PGM datasets show that DCNet outperforms the\nstate-of-the-art methods by a large margin of 5.77%. Further experiments on few\ntraining samples and model generalization also show the effectiveness of DCNet.\nCode is available at https://github.com/visiontao/dcnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_T/0/1/0/all/0/1\">Tao Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DLTTA: Dynamic Learning Rate for Test-time Adaptation on Cross-domain Medical Images. (arXiv:2205.13723v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13723","description":"<p>Test-time adaptation (TTA) has increasingly been an important topic to\nefficiently tackle the cross-domain distribution shift at test time for medical\nimages from different institutions. Previous TTA methods have a common\nlimitation of using a fixed learning rate for all the test samples. Such a\npractice would be sub-optimal for TTA, because test data may arrive\nsequentially therefore the scale of distribution shift would change frequently.\nTo address this problem, we propose a novel dynamic learning rate adjustment\nmethod for test-time adaptation, called DLTTA, which dynamically modulates the\namount of weights update for each test image to account for the differences in\ntheir distribution shift. Specifically, our DLTTA is equipped with a memory\nbank based estimation scheme to effectively measure the discrepancy of a given\ntest sample. Based on this estimated discrepancy, a dynamic learning rate\nadjustment strategy is then developed to achieve a suitable degree of\nadaptation for each test sample. The effectiveness and general applicability of\nour DLTTA is extensively demonstrated on three tasks including retinal optical\ncoherence tomography (OCT) segmentation, histopathological image\nclassification, and prostate 3D MRI segmentation. Our method achieves effective\nand fast test-time adaptation with consistent performance improvement over\ncurrent state-of-the-art test-time adaptation methods. Code is available at:\nhttps://github.com/med-air/DLTTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongzheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meirui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quande Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jianfeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V-Doc : Visual questions answers with Documents. (arXiv:2205.13724v1 [cs.AI])","link":"http://arxiv.org/abs/2205.13724","description":"<p>We propose V-Doc, a question-answering tool using document images and PDF,\nmainly for researchers and general non-deep learning experts looking to\ngenerate, process, and understand the document visual question answering tasks.\nThe V-Doc supports generating and using both extractive and abstractive\nquestion-answer pairs using documents images. The extractive QA selects a\nsubset of tokens or phrases from the document contents to predict the answers,\nwhile the abstractive QA recognises the language in the content and generates\nthe answer based on the trained model. Both aspects are crucial to\nunderstanding the documents, especially in an image format. We include a\ndetailed scenario of question generation for the abstractive QA task. V-Doc\nsupports a wide range of datasets and models, and is highly extensible through\na declarative, framework-agnostic platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yihao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Runlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuzhong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyunsuk Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Reconstruction of Multi Branch Feature Multiplexing Fusion Network with Mixed Multi-layer Attention. (arXiv:2205.13738v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13738","description":"<p>Image super-resolution reconstruction achieves better results than\ntraditional methods with the help of the powerful nonlinear representation\nability of convolution neural network. However, some existing algorithms also\nhave some problems, such as insufficient utilization of phased features,\nignoring the importance of early phased feature fusion to improve network\nperformance, and the inability of the network to pay more attention to\nhigh-frequency information in the reconstruction process. To solve these\nproblems, we propose a multi-branch feature multiplexing fusion network with\nmixed multi-layer attention (MBMFN), which realizes the multiple utilization of\nfeatures and the multistage fusion of different levels of features. To further\nimprove the networks performance, we propose a lightweight enhanced residual\nchannel attention (LERCA), which can not only effectively avoid the loss of\nchannel information but also make the network pay more attention to the key\nchannel information and benefit from it. Finally, the attention mechanism is\nintroduced into the reconstruction process to strengthen the restoration of\nedge texture and other details. A large number of experiments on several\nbenchmark sets show that, compared with other advanced reconstruction\nalgorithms, our algorithm produces highly competitive objective indicators and\nrestores more image detail texture information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuxi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huicheng Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Instance Representation Banks for Aerial Scene Classification. (arXiv:2205.13744v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13744","description":"<p>Aerial scenes are more complicated in terms of object distribution and\nspatial arrangement than natural scenes due to the bird view, and thus remain\nchallenging to learn discriminative scene representation. Recent solutions\ndesign \\textit{local semantic descriptors} so that region of interests (RoIs)\ncan be properly highlighted. However, each local descriptor has limited\ndescription capability and the overall scene representation remains to be\nrefined. In this paper, we solve this problem by designing a novel\nrepresentation set named \\textit{instance representation bank} (IRB), which\nunifies multiple local descriptors under the multiple instance learning (MIL)\nformulation. This unified framework is not trivial as all the local semantic\ndescriptors can be aligned to the same scene scheme, enhancing the scene\nrepresentation capability. Specifically, our IRB learning framework consists of\na backbone, an instance representation bank, a semantic fusion module and a\nscene scheme alignment loss function. All the components are organized in an\nend-to-end manner. Extensive experiments on three aerial scene benchmarks\ndemonstrate that our proposed method outperforms the state-of-the-art\napproaches by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingjun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Beichen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Awareness Multiple Instance Neural Network. (arXiv:2205.13750v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13750","description":"<p>Multiple instance learning is qualified for many pattern recognition tasks\nwith weakly annotated data. The combination of artificial neural network and\nmultiple instance learning offers an end-to-end solution and has been widely\nutilized. However, challenges remain in two-folds. Firstly, current MIL pooling\noperators are usually pre-defined and lack flexibility to mine key instances.\nSecondly, in current solutions, the bag-level representation can be inaccurate\nor inaccessible. To this end, we propose an attention awareness multiple\ninstance neural network framework in this paper. It consists of an\ninstance-level classifier, a trainable MIL pooling operator based on spatial\nattention and a bag-level classification layer. Exhaustive experiments on a\nseries of pattern recognition tasks demonstrate that our framework outperforms\nmany state-of-the-art MIL methods and validates the effectiveness of our\nproposed attention MIL pooling operators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingjun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Beichen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIGMO: Categorical invariant representations in a deep generative framework. (arXiv:2205.13758v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13758","description":"<p>Data of general object images have two most common structures: (1) each\nobject of a given shape can be rendered in multiple different views, and (2)\nshapes of objects can be categorized in such a way that the diversity of shapes\nis much larger across categories than within a category. Existing deep\ngenerative models can typically capture either structure, but not both. In this\nwork, we introduce a novel deep generative model, called CIGMO, that can learn\nto represent category, shape, and view factors from image data. The model is\ncomprised of multiple modules of shape representations that are each\nspecialized to a particular category and disentangled from view representation,\nand can be learned using a group-based weakly supervised learning method. By\nempirical investigation, we show that our model can effectively discover\ncategories of object shapes despite large view variation and quantitatively\nsupersede various previous methods including the state-of-the-art invariant\nclustering algorithm. Further, we show that our approach using\ncategory-specialization can enhance the learned shape representation to better\nperform down-stream tasks such as one-shot object identification as well as\nshape-view disentanglement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosoya_H/0/1/0/all/0/1\">Haruo Hosoya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Convolutional One-Stage 3D Object Detection on LiDAR Range Images. (arXiv:2205.13764v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13764","description":"<p>We present a simple yet effective fully convolutional one-stage 3D object\ndetector for LiDAR point clouds of autonomous driving scenes, termed\nFCOS-LiDAR. Unlike the dominant methods that use the bird-eye view (BEV), our\nproposed detector detects objects from the range view (RV, a.k.a. range image)\nof the LiDAR points. Due to the range view's compactness and compatibility with\nthe LiDAR sensors' sampling process on self-driving cars, the range view-based\nobject detector can be realized by solely exploiting the vanilla 2D\nconvolutions, departing from the BEV-based methods which often involve\ncomplicated voxelization operations and sparse convolutions.\n</p>\n<p>For the first time, we show that an RV-based 3D detector with standard 2D\nconvolutions alone can achieve comparable performance to state-of-the-art\nBEV-based detectors while being significantly faster and simpler. More\nimportantly, almost all previous range view-based detectors only focus on\nsingle-frame point clouds, since it is challenging to fuse multi-frame point\nclouds into a single range view. In this work, we tackle this challenging issue\nwith a novel range view projection mechanism, and for the first time\ndemonstrate the benefits of fusing multi-frame point clouds for a range-view\nbased detector. Extensive experiments on nuScenes show the superiority of our\nproposed method and we believe that our work can be strong evidence that an\nRV-based 3D detector can compare favourably with the current mainstream\nBEV-based detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-aware Dense Representation Learning for Remote Sensing Image Change Detection. (arXiv:2205.13769v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13769","description":"<p>Training deep learning-based change detection (CD) model heavily depends on\nlabeled data. Contemporary transfer learning-based methods to alleviate the CD\nlabel insufficiency mainly upon ImageNet pre-training. A recent trend is using\nremote sensing (RS) data to obtain in-domain representations via supervised or\nself-supervised learning (SSL). Here, different from traditional supervised\npre-training that learns the mapping from image to label, we leverage semantic\nsupervision in a contrastive manner. There are typically multiple objects of\ninterest (e.g., buildings) distributed in varying locations in RS images. We\npropose dense semantic-aware pre-training for RS image CD via sampling multiple\nclass-balanced points. Instead of manipulating image-level representations that\nlack spatial information, we constrain pixel-level cross-view consistency and\ncross-semantic discrimination to learn spatially-sensitive features, thus\nbenefiting downstream dense CD. Apart from learning illumination invariant\nfeatures, we fulfill consistent foreground features insensitive to irrelevant\nbackground changes via a synthetic view using background swapping. We\nadditionally achieve discriminative representations to distinguish foreground\nland-covers and others. We collect large-scale image-mask pairs freely\navailable in the RS community for pre-training. Extensive experiments on three\nCD datasets verify the effectiveness of our method. Ours significantly\noutperforms ImageNet, in-domain supervision, and several SSL methods. Empirical\nresults indicate ours well alleviates data insufficiency in CD. Notably, we\nachieve competitive results using only 20% training data than baseline (random)\nusing 100% data. Both quantitative and qualitative results demonstrate the\ngeneralization ability of our pre-trained model to downstream images even\nremaining domain gaps with the pre-training data. Our Code will make public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEAF + AIO: Edge-Assisted Energy-Aware Object Detection for Mobile Augmented Reality. (arXiv:2205.13770v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13770","description":"<p>Today very few deep learning-based mobile augmented reality (MAR)\napplications are applied in mobile devices because they are significantly\nenergy-guzzling. In this paper, we design an edge-based energy-aware MAR system\nthat enables MAR devices to dynamically change their configurations, such as\nCPU frequency, computation model size, and image offloading frequency based on\nuser preferences, camera sampling rates, and available radio resources. Our\nproposed dynamic MAR configuration adaptations can minimize the per frame\nenergy consumption of multiple MAR clients without degrading their preferred\nMAR performance metrics, such as latency and detection accuracy. To thoroughly\nanalyze the interactions among MAR configurations, user preferences, camera\nsampling rate, and energy consumption, we propose, to the best of our\nknowledge, the first comprehensive analytical energy model for MAR devices.\nBased on the proposed analytical model, we design a LEAF optimization algorithm\nto guide the MAR configuration adaptation and server radio resource allocation.\nAn image offloading frequency orchestrator, coordinating with the LEAF, is\ndeveloped to adaptively regulate the edge-based object detection invocations\nand to further improve the energy efficiency of MAR devices. Extensive\nevaluations are conducted to validate the performance of the proposed\nanalytical model and algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">BaekGyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of COVID-19 Patients with their Severity Level from Chest CT Scans using Transfer Learning. (arXiv:2205.13774v1 [eess.IV])","link":"http://arxiv.org/abs/2205.13774","description":"<p>Background and Objective: During pandemics, the use of artificial\nintelligence (AI) approaches combined with biomedical science play a\nsignificant role in reducing the burden on the healthcare systems and\nphysicians. The rapid increment in cases of COVID-19 has led to an increase in\ndemand for hospital beds and other medical equipment. However, since medical\nfacilities are limited, it is recommended to diagnose patients as per the\nseverity of the infection. Keeping this in mind, we share our research in\ndetecting COVID-19 as well as assessing its severity using chest-CT scans and\nDeep Learning pre-trained models. Dataset: We have collected a total of 1966 CT\nScan images for three different class labels, namely, Non-COVID, Severe COVID,\nand Non-Severe COVID, out of which 714 CT images belong to the Non-COVID\ncategory, 713 CT images are for Non-Severe COVID category and 539 CT images are\nof Severe COVID category. Methods: All of the images are initially\npre-processed using the Contrast Limited Histogram Equalization (CLAHE)\napproach. The pre-processed images are then fed into the VGG-16 network for\nextracting features. Finally, the retrieved characteristics are categorized and\nthe accuracy is evaluated using a support vector machine (SVM) with 10-fold\ncross-validation (CV). Result and Conclusion: In our study, we have combined\nwell-known strategies for pre-processing, feature extraction, and\nclassification which brings us to a remarkable success rate of disease and its\nseverity recognition with an accuracy of 96.05% (97.7% for Non-Severe COVID-19\nimages and 93% for Severe COVID-19 images). Our model can therefore help\nradiologists detect COVID-19 and the extent of its severity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gupta_M/0/1/0/all/0/1\">Mansi Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swaraj_A/0/1/0/all/0/1\">Aman Swaraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verma_K/0/1/0/all/0/1\">Karan Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Long-Tailed Visual Recognition. (arXiv:2205.13775v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13775","description":"<p>The heavy reliance on data is one of the major reasons that currently limit\nthe development of deep learning. Data quality directly dominates the effect of\ndeep learning models, and the long-tailed distribution is one of the factors\naffecting data quality. The long-tailed phenomenon is prevalent due to the\nprevalence of power law in nature. In this case, the performance of deep\nlearning models is often dominated by the head classes while the learning of\nthe tail classes is severely underdeveloped. In order to learn adequately for\nall classes, many researchers have studied and preliminarily addressed the\nlong-tailed problem. In this survey, we focus on the problems caused by\nlong-tailed data distribution, sort out the representative long-tailed visual\nrecognition datasets and summarize some mainstream long-tailed studies.\nSpecifically, we summarize these studies into ten categories from the\nperspective of representation learning, and outline the highlights and\nlimitations of each category. Besides, we have studied four quantitative\nmetrics for evaluating the imbalance, and suggest using the Gini coefficient to\nevaluate the long-tailedness of a dataset. Based on the Gini coefficient, we\nquantitatively study 20 widely-used and large-scale visual datasets proposed in\nthe last decade, and find that the long-tailed phenomenon is widespread and has\nnot been fully studied. Finally, we provide several future directions for the\ndevelopment of long-tailed learning to provide more ideas for readers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">He Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework. (arXiv:2205.13790v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13790","description":"<p>Fusing the camera and LiDAR information has become a de-facto standard for 3D\nobject detection tasks. Current methods rely on point clouds from the LiDAR\nsensor as queries to leverage the feature from the image space. However, people\ndiscover that this underlying assumption makes the current fusion framework\ninfeasible to produce any prediction when there is a LiDAR malfunction,\nregardless of minor or major. This fundamentally limits the deployment\ncapability to realistic autonomous driving scenarios. In contrast, we propose a\nsurprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera\nstream does not depend on the input of LiDAR data, thus addressing the downside\nof previous methods. We empirically show that our framework surpasses the\nstate-of-the-art methods under the normal training settings. Under the\nrobustness training settings that simulate various LiDAR malfunctions, our\nframework significantly surpasses the state-of-the-art methods by 15.7% to\n28.9% mAP. To the best of our knowledge, we are the first to handle realistic\nLiDAR malfunction and can be deployed to realistic scenarios without any\npost-processing procedure. The code is available at\nhttps://github.com/ADLab-AutoDrive/BEVFusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tingting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kaicheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhongyu Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhi Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Morphing: Fooling a Face Recognition System Is Simple!. (arXiv:2205.13796v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13796","description":"<p>State-of-the-art face recognition (FR) approaches have shown remarkable\nresults in predicting whether two faces belong to the same identity, yielding\naccuracies between 92% and 100% depending on the difficulty of the protocol.\nHowever, the accuracy drops substantially when exposed to morphed faces,\nspecifically generated to look similar to two identities. To generate morphed\nfaces, we integrate a simple pretrained FR model into a generative adversarial\nnetwork (GAN) and modify several loss functions for face morphing. In contrast\nto previous works, our approach and analyses are not limited to pairs of\nfrontal faces with the same ethnicity and gender. Our qualitative and\nquantitative results affirm that our approach achieves a seamless change\nbetween two faces even in unconstrained scenarios. Despite using features from\na simpler FR model for face morphing, we demonstrate that even recent FR\nsystems struggle to distinguish the morphed face from both identities obtaining\nan accuracy of only 55-70%. Besides, we provide further insights into how\nknowing the FR system makes it particularly vulnerable to face morphing\nattacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hormann_S/0/1/0/all/0/1\">Stefan H&#xf6;rmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tianlin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teepe_T/0/1/0/all/0/1\">Torben Teepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_F/0/1/0/all/0/1\">Fabian Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoche_M/0/1/0/all/0/1\">Martin Knoche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions. (arXiv:2205.13803v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13803","description":"<p>A significant gap remains between today's visual pattern recognition models\nand human-level visual cognition especially when it comes to few-shot learning\nand compositional reasoning of novel concepts. We introduce Bongard-HOI, a new\nvisual reasoning benchmark that focuses on compositional learning of\nhuman-object interactions (HOIs) from natural images. It is inspired by two\ndesirable characteristics from the classical Bongard problems (BPs): 1)\nfew-shot concept learning, and 2) context-dependent reasoning. We carefully\ncurate the few-shot instances with hard negatives, where positive and negative\nimages only disagree on action labels, making mere recognition of object\ncategories insufficient to complete our benchmarks. We also design multiple\ntest sets to systematically study the generalization of visual learning models,\nwhere we vary the overlap of the HOI concepts between the training and test\nsets of few-shot instances, from partial to no overlaps. Bongard-HOI presents a\nsubstantial challenge to today's visual recognition models. The\nstate-of-the-art HOI detection model achieves only 62% accuracy on few-shot\nbinary prediction while even amateur human testers on MTurk have 91% accuracy.\nWith the Bongard-HOI benchmark, we hope to further advance research efforts in\nvisual reasoning, especially in holistic perception-reasoning systems and\nbetter representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huaizu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weili Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-ViT: High Performance Linear Vision Transformer without Softmax. (arXiv:2205.13805v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13805","description":"<p>Vision transformers have become one of the most important models for computer\nvision tasks. Although they outperform prior works, they require heavy\ncomputational resources on a scale that is quadratic to the number of tokens,\n$N$. This is a major drawback of the traditional self-attention (SA) algorithm.\nHere, we propose the X-ViT, ViT with a novel SA mechanism that has linear\ncomplexity. The main approach of this work is to eliminate nonlinearity from\nthe original SA. We factorize the matrix multiplication of the SA mechanism\nwithout complicated linear approximation. By modifying only a few lines of code\nfrom the original SA, the proposed models outperform most transformer-based\nmodels on image classification and dense prediction tasks on most capacity\nregimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jeonggeun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heung-Chang Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Look at Improving Robustness in Visual-inertial SLAM by Moment Matching. (arXiv:2205.13821v1 [cs.RO])","link":"http://arxiv.org/abs/2205.13821","description":"<p>The fusion of camera sensor and inertial data is a leading method for\nego-motion tracking in autonomous and smart devices. State estimation\ntechniques that rely on non-linear filtering are a strong paradigm for solving\nthe associated information fusion task. The de facto inference method in this\nspace is the celebrated extended Kalman filter (EKF), which relies on\nfirst-order linearizations of both the dynamical and measurement model. This\npaper takes a critical look at the practical implications and limitations posed\nby the EKF, especially under faulty visual feature associations and the\npresence of strong confounding noise. As an alternative, we revisit the assumed\ndensity formulation of Bayesian filtering and employ a moment matching\n(unscented Kalman filtering) approach to both visual-inertial odometry and\nvisual SLAM. Our results highlight important aspects in robustness both in\ndynamics propagation and visual measurement updates, and we show\nstate-of-the-art results on EuRoC MAV drone data benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solin_A/0/1/0/all/0/1\">Arno Solin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilzer_A/0/1/0/all/0/1\">Andrea Pilzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Fetal Ultrasound Video Model Match Human Observers in Biometric Measurements. (arXiv:2205.13835v1 [eess.IV])","link":"http://arxiv.org/abs/2205.13835","description":"<p>Objective. This work investigates the use of deep convolutional neural\nnetworks (CNN) to automatically perform measurements of fetal body parts,\nincluding head circumference, biparietal diameter, abdominal circumference and\nfemur length, and to estimate gestational age and fetal weight using fetal\nultrasound videos. Approach. We developed a novel multi-task CNN-based\nspatio-temporal fetal US feature extraction and standard plane detection\nalgorithm (called FUVAI) and evaluated the method on 50 freehand fetal US video\nscans. We compared FUVAI fetal biometric measurements with measurements made by\nfive experienced sonographers at two time points separated by at least two\nweeks. Intra- and inter-observer variabilities were estimated. Main results. We\nfound that automated fetal biometric measurements obtained by FUVAI were\ncomparable to the measurements performed by experienced sonographers The\nobserved differences in measurement values were within the range of inter- and\nintra-observer variability. Moreover, analysis has shown that these differences\nwere not statistically significant when comparing any individual medical expert\nto our model. Significance. We argue that FUVAI has the potential to assist\nsonographers who perform fetal biometric measurements in clinical settings by\nproviding them with suggestions regarding the best measuring frames, along with\nautomated measurements. Moreover, FUVAI is able perform these tasks in just a\nfew seconds, which is a huge difference compared to the average of six minutes\ntaken by sonographers. This is significant, given the shortage of medical\nexperts capable of interpreting fetal ultrasound images in numerous countries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lisowska_A/0/1/0/all/0/1\">Aneta Lisowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seliga_Siwecka_J/0/1/0/all/0/1\">Joanna Seliga-Siwecka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textural-Structural Joint Learning for No-Reference Super-Resolution Image Quality Assessment. (arXiv:2205.13847v1 [eess.IV])","link":"http://arxiv.org/abs/2205.13847","description":"<p>Image super-resolution (SR) has been widely investigated in recent years.\nHowever, it is challenging to fairly estimate the performances of various SR\nmethods, as the lack of reliable and accurate criteria for perceptual quality.\nExisting SR image quality assessment (IQA) metrics usually concentrate on the\nspecific kind of degradation without distinguishing the visual sensitive areas,\nwhich have no adaptive ability to describe the diverse SR degeneration\nsituations. In this paper, we focus on the textural and structural degradation\nof image SR which acts as a critical role for visual perception, and design a\ndual stream network to jointly explore the textural and structural information\nfor quality prediction, dubbed TSNet. By mimicking the human vision system\n(HVS) that pays more attention to the significant areas of the image, we\ndevelop the spatial attention mechanism to make the visual-sensitive areas more\ndistinguishable, which improves the prediction accuracy. Feature normalization\n(F-Norm) is also developed to investigate the inherent spatial correlation of\nSR features and boost the network representation capacity. Experimental results\nshow the proposed TSNet predicts the visual quality more accurate than the\nstate-of-the-art IQA methods, and demonstrates better consistency with the\nhuman's perspective. The source code will be made available at\n<a href=\"http://github.com/yuqing-liu-dut/NRIQA_SR.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Patterns in Visualized Data by Adding Redundant Visual Information. (arXiv:2205.13856v1 [stat.CO])","link":"http://arxiv.org/abs/2205.13856","description":"<p>We present \"PATRED\", a technique that uses the addition of redundant\ninformation to facilitate the detection of specific, generally described\npatterns in line-charts during the visual exploration of the charts. We\ncompared different versions of this technique, that differed in the way\nredundancy was added, using nine distance metrics (such as Euclidean, Pearson,\nMutual Information and Jaccard) with judgments from data scientists which\nserved as the \"ground truth\". Results were analyzed with correlations (R2), F1\nscores and Mutual Information with the average ranking by the data scientists.\nSome distance metrics consistently benefit from the addition of redundant\ninformation, while others are only enhanced for specific types of data\nperturbations. The results demonstrate the value of adding redundancy to\nimprove the identification of patterns in time-series data during visual\nexploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Eisler_S/0/1/0/all/0/1\">Salomon Eisler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Meyer_J/0/1/0/all/0/1\">Joachim Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrackNet: A Triplet metric-based method for Multi-Target Multi-Camera Vehicle Tracking. (arXiv:2205.13857v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13857","description":"<p>We present TrackNet, a method for Multi-Target Multi-Camera (MTMC) vehicle\ntracking from traffic video sequences. Cross-camera vehicle tracking has proved\nto be a challenging task due to perspective, scale and speed variance, as well\nocclusions and noise conditions. Our method is based on a modular approach that\nfirst detects vehicles frame-by-frame using Faster R-CNN, then tracks\ndetections through single camera using Kalman filter, and finally matches\ntracks by a triplet metric learning strategy. We conduct experiments on\nTrackNet within the AI City Challenge framework, and present competitive IDF1\nresults of 0.4733.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serrano_D/0/1/0/all/0/1\">David Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Net_F/0/1/0/all/0/1\">Francesc Net</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan Antonio Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ugarte_I/0/1/0/all/0/1\">Igor Ugarte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Deep Learning Segmentation and Multigrader-annotated Mandibular Canals of Multicenter CBCT scans. (arXiv:2205.13874v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13874","description":"<p>Deep learning approach has been demonstrated to automatically segment the\nbilateral mandibular canals from CBCT scans, yet systematic studies of its\nclinical and technical validation are scarce. To validate the mandibular canal\nlocalization accuracy of a deep learning system (DLS) we trained it with 982\nCBCT scans and evaluated using 150 scans of five scanners from clinical\nworkflow patients of European and Southeast Asian Institutes, annotated by four\nradiologists. The interobserver variability was compared to the variability\nbetween the DLS and the radiologists. In addition, the generalization of DLS to\nCBCT scans from scanners not used in the training data was examined to evaluate\nthe out-of-distribution generalization capability. The DLS had lower\nvariability to the radiologists than the interobserver variability between them\nand it was able to generalize to three new devices. For the radiologists'\nconsensus segmentation, used as gold standard, the DLS had a symmetric mean\ncurve distance of 0.39 mm compared to those of the individual radiologists with\n0.62 mm, 0.55 mm, 0.47 mm, and 0.42 mm. The DLS showed comparable or slightly\nbetter performance in the segmentation of the mandibular canal with the\nradiologists and generalization capability to new scanners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarnstedt_J/0/1/0/all/0/1\">Jorma J&#xe4;rnstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlsten_J/0/1/0/all/0/1\">Jaakko Sahlsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaskari_J/0/1/0/all/0/1\">Joel Jaskari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehtonen_H/0/1/0/all/0/1\">Helena Mehtonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziyuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hietanen_A/0/1/0/all/0/1\">Ari Hietanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundqvist_O/0/1/0/all/0/1\">Osku Sundqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varjonen_V/0/1/0/all/0/1\">Vesa Varjonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattila_V/0/1/0/all/0/1\">Vesa Mattila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prapayasotok_S/0/1/0/all/0/1\">Sangsom Prapayasotok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nalampang_S/0/1/0/all/0/1\">Sakarat Nalampang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TraClets: Harnessing the power of computer vision for trajectory classification. (arXiv:2205.13880v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13880","description":"<p>Due to the advent of new mobile devices and tracking sensors in recent years,\nhuge amounts of data are being produced every day. Therefore, novel\nmethodologies need to emerge that dive through this vast sea of information and\ngenerate insights and meaningful information. To this end, researchers have\ndeveloped several trajectory classification algorithms over the years that are\nable to annotate tracking data. Similarly, in this research, a novel\nmethodology is presented that exploits image representations of trajectories,\ncalled TraClets, in order to classify trajectories in an intuitive humans way,\nthrough computer vision techniques. Several real-world datasets are used to\nevaluate the proposed approach and compare its classification performance to\nother state-of-the-art trajectory classification algorithms. Experimental\nresults demonstrate that TraClets achieves a classification performance that is\ncomparable to, or in most cases, better than the state-of-the-art, acting as a\nuniversal, high-accuracy approach for trajectory classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kontopoulos_I/0/1/0/all/0/1\">Ioannis Kontopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makris_A/0/1/0/all/0/1\">Antonios Makris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tserpes_K/0/1/0/all/0/1\">Konstantinos Tserpes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogorny_V/0/1/0/all/0/1\">Vania Bogorny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Domain Generalization. (arXiv:2205.13913v1 [cs.LG])","link":"http://arxiv.org/abs/2205.13913","description":"<p>Domain generalization (DG) is a fundamental yet very challenging research\ntopic in machine learning. The existing arts mainly focus on learning\ndomain-invariant features with limited source domains in a static model.\nUnfortunately, there is a lack of training-free mechanism to adjust the model\nwhen generalized to the agnostic target domains. To tackle this problem, we\ndevelop a brand-new DG variant, namely Dynamic Domain Generalization (DDG), in\nwhich the model learns to twist the network parameters to adapt the data from\ndifferent domains. Specifically, we leverage a meta-adjuster to twist the\nnetwork parameters based on the static model with respect to different data\nfrom different domains. In this way, the static model is optimized to learn\ndomain-shared features, while the meta-adjuster is designed to learn\ndomain-specific features. To enable this process, DomainMix is exploited to\nsimulate data from diverse domains during teaching the meta-adjuster to adapt\nto the upcoming agnostic target domains. This learning mechanism urges the\nmodel to generalize to different agnostic target domains via adjusting the\nmodel without training. Extensive experiments demonstrate the effectiveness of\nour proposed method. Code is available at: https://github.com/MetaVisionLab/DDG\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhishu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhifeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Luojun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhifeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shicai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DILG: Irregular Latent Grids for 3D Generative Modeling. (arXiv:2205.13914v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13914","description":"<p>We propose a new representation for encoding 3D shapes as neural fields. The\nrepresentation is designed to be compatible with the transformer architecture\nand to benefit both shape reconstruction and shape generation. Existing works\non neural fields are grid-based representations with latents defined on a\nregular grid. In contrast, we define latents on irregular grids, enabling our\nrepresentation to be sparse and adaptive. In the context of shape\nreconstruction from point clouds, our shape representation built on irregular\ngrids improves upon grid-based methods in terms of reconstruction accuracy. For\nshape generation, our representation promotes high-quality shape generation\nusing auto-regressive probabilistic models. We show different applications that\nimprove over the current state of the art. First, we show results for\nprobabilistic shape reconstruction from a single higher resolution image.\nSecond, we train a probabilistic model conditioned on very low resolution\nimages. Third, we apply our model to category-conditioned generation. All\nprobabilistic experiments confirm that we are able to generate detailed and\nhigh quality shapes to yield the new state of the art in generative 3D shape\nmodeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREAM: Weakly Supervised Object Localization via Class RE-Activation Mapping. (arXiv:2205.13922v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13922","description":"<p>Weakly Supervised Object Localization (WSOL) aims to localize objects with\nimage-level supervision. Existing works mainly rely on Class Activation Mapping\n(CAM) derived from a classification model. However, CAM-based methods usually\nfocus on the most discriminative parts of an object (i.e., incomplete\nlocalization problem). In this paper, we empirically prove that this problem is\nassociated with the mixup of the activation values between less discriminative\nforeground regions and the background. To address it, we propose Class\nRE-Activation Mapping (CREAM), a novel clustering-based approach to boost the\nactivation values of the integral object regions. To this end, we introduce\nclass-specific foreground and background context embeddings as cluster\ncentroids. A CAM-guided momentum preservation strategy is developed to learn\nthe context embeddings during training. At the inference stage, the\nre-activation mapping is formulated as a parameter estimation problem under\nGaussian Mixture Model, which can be solved by deriving an unsupervised\nExpectation-Maximization based soft-clustering algorithm. By simply integrating\nCREAM into various WSOL approaches, our method significantly improves their\nperformance. CREAM achieves the state-of-the-art performance on CUB, ILSVRC and\nOpenImages benchmark datasets. Code will be available at\nhttps://github.com/Jazzcharles/CREAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jilan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junlin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuejie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui-Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep face recognition with clustering based domain adaptation. (arXiv:2205.13937v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13937","description":"<p>Despite great progress in face recognition tasks achieved by deep convolution\nneural networks (CNNs), these models often face challenges in real world tasks\nwhere training images gathered from Internet are different from test images\nbecause of different lighting condition, pose and image quality. These factors\nincrease domain discrepancy between training (source domain) and testing\n(target domain) database and make the learnt models degenerate in application.\nMeanwhile, due to lack of labeled target data, directly fine-tuning the\npre-learnt models becomes intractable and impractical. In this paper, we\npropose a new clustering-based domain adaptation method designed for face\nrecognition task in which the source and target domain do not share any\nclasses. Our method effectively learns the discriminative target feature by\naligning the feature domain globally, and, at the meantime, distinguishing the\ntarget clusters locally. Specifically, it first learns a more reliable\nrepresentation for clustering by minimizing global domain discrepancy to reduce\ndomain gaps, and then applies simplified spectral clustering method to generate\npseudo-labels in the domain-invariant feature space, and finally learns\ndiscriminative target representation. Comprehensive experiments on widely-used\nGBU, IJB-A/B/C and RFW databases clearly demonstrate the effectiveness of our\nnewly proposed approach. State-of-the-art performance of GBU data set is\nachieved by only unsupervised adaptation from the target training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13943","description":"<p>Masked image modeling (MIM), an emerging self-supervised pre-training method,\nhas shown impressive success across numerous downstream vision tasks with\nVision transformers (ViT). Its underlying idea is simple: a portion of the\ninput image is randomly masked out and then reconstructed via the pre-text\ntask. However, why MIM works well is not well explained, and previous studies\ninsist that MIM primarily works for the Transformer family but is incompatible\nwith CNNs. In this paper, we first study interactions among patches to\nunderstand what knowledge is learned and how it is acquired via the MIM task.\nWe observe that MIM essentially teaches the model to learn better middle-level\ninteractions among patches and extract more generalized features. Based on this\nfact, we propose an Architecture-Agnostic Masked Image Modeling framework\n(A$^2$MIM), which is compatible with not only Transformers but also CNNs in a\nunified way. Extensive experiments on popular benchmarks show that our A$^2$MIM\nlearns better representations and endows the backbone model with the stronger\ncapability to transfer to various downstream tasks for both Transformers and\nCNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan.Z.Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle Label-Consistent Networks for Unsupervised Domain Adaptation. (arXiv:2205.13957v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13957","description":"<p>Domain adaptation aims to leverage a labeled source domain to learn a\nclassifier for the unlabeled target domain with a different distribution.\nPrevious methods mostly match the distribution between two domains by global or\nclass alignment. However, global alignment methods cannot achieve a\nfine-grained class-to-class overlap; class alignment methods supervised by\npseudo-labels cannot guarantee their reliability. In this paper, we propose a\nsimple yet efficient domain adaptation method, i.e. Cycle Label-Consistent\nNetwork (CLCN), by exploiting the cycle consistency of classification label,\nwhich applies dual cross-domain nearest centroid classification procedures to\ngenerate a reliable self-supervised signal for the discrimination in the target\ndomain. The cycle label-consistent loss reinforces the consistency between\nground-truth labels and pseudo-labels of source samples leading to\nstatistically similar latent representations between source and target domains.\nThis new loss can easily be added to any existing classification network with\nalmost no computational overhead. We demonstrate the effectiveness of our\napproach on MNIST-USPS-SVHN, Office-31, Office-Home and Image CLEF-DA\nbenchmarks. Results validate that the proposed method can alleviate the\nnegative influence of falsely-labeled samples and learn more discriminative\nfeatures, leading to the absolute improvement over source-only model by 9.4% on\nOffice-31 and 6.3% on Image CLEF-DA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video2StyleGAN: Disentangling Local and Global Variations in a Video. (arXiv:2205.13996v1 [cs.CV])","link":"http://arxiv.org/abs/2205.13996","description":"<p>Image editing using a pretrained StyleGAN generator has emerged as a powerful\nparadigm for facial editing, providing disentangled controls over age,\nexpression, illumination, etc. However, the approach cannot be directly adopted\nfor video manipulations. We hypothesize that the main missing ingredient is the\nlack of fine-grained and disentangled control over face location, face pose,\nand local facial expressions. In this work, we demonstrate that such a\nfine-grained control is indeed achievable using pretrained StyleGAN by working\nacross multiple (latent) spaces (namely, the positional space, the W+ space,\nand the S space) and combining the optimization results across the multiple\nspaces. Building on this enabling component, we introduce Video2StyleGAN that\ntakes a target image and driving video(s) to reenact the local and global\nlocations and expressions from the driving video in the identity of the target\nimage. We evaluate the effectiveness of our method over multiple challenging\nscenarios and demonstrate clear improvements over alternative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdal_R/0/1/0/all/0/1\">Rameen Abdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Future Transformer for Long-term Action Anticipation. (arXiv:2205.14022v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14022","description":"<p>The task of predicting future actions from a video is crucial for a\nreal-world agent interacting with others. When anticipating actions in the\ndistant future, we humans typically consider long-term relations over the whole\nsequence of actions, i.e., not only observed actions in the past but also\npotential actions in the future. In a similar spirit, we propose an end-to-end\nattention model for action anticipation, dubbed Future Transformer (FUTR), that\nleverages global attention over all input frames and output tokens to predict a\nminutes-long sequence of future actions. Unlike the previous autoregressive\nmodels, the proposed method learns to predict the whole sequence of future\nactions in parallel decoding, enabling more accurate and fast inference for\nlong-term anticipation. We evaluate our method on two standard benchmarks for\nlong-term action anticipation, Breakfast and 50 Salads, achieving\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dayoung Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Manjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Seong Jong Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lesion classification by model-based feature extraction: A differential affine invariant model of soft tissue elasticity. (arXiv:2205.14029v1 [eess.IV])","link":"http://arxiv.org/abs/2205.14029","description":"<p>The elasticity of soft tissues has been widely considered as a characteristic\nproperty to differentiate between healthy and vicious tissues and, therefore,\nmotivated several elasticity imaging modalities, such as Ultrasound\nElastography, Magnetic Resonance Elastography, and Optical Coherence\nElastography. This paper proposes an alternative approach of modeling the\nelasticity using Computed Tomography (CT) imaging modality for model-based\nfeature extraction machine learning (ML) differentiation of lesions. The model\ndescribes a dynamic non-rigid (or elastic) deformation in differential manifold\nto mimic the soft tissues elasticity under wave fluctuation in vivo. Based on\nthe model, three local deformation invariants are constructed by two tensors\ndefined by the first and second order derivatives from the CT images and used\nto generate elastic feature maps after normalization via a novel signal\nsuppression method. The model-based elastic image features are extracted from\nthe feature maps and fed to machine learning to perform lesion classifications.\nTwo pathologically proven image datasets of colon polyps (44 malignant and 43\nbenign) and lung nodules (46 malignant and 20 benign) were used to evaluate the\nproposed model-based lesion classification. The outcomes of this modeling\napproach reached the score of area under the curve of the receiver operating\ncharacteristics of 94.2 % for the polyps and 87.4 % for the nodules, resulting\nin an average gain of 5 % to 30 % over ten existing state-of-the-art lesion\nclassification methods. The gains by modeling tissue elasticity for ML\ndifferentiation of lesions are striking, indicating the great potential of\nexploring the modeling strategy to other tissue properties for ML\ndifferentiation of lesions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cao_W/0/1/0/all/0/1\">Weiguo Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pomeroy_M/0/1/0/all/0/1\">Marc J. Pomeroy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Z/0/1/0/all/0/1\">Zhengrong Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yongfeng Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yongyi Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_J/0/1/0/all/0/1\">Jiaxing Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_F/0/1/0/all/0/1\">Fangfang Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1\">Jianhua Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1\">Hongbin Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_A/0/1/0/all/0/1\">Almas F. Abbasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pickhardt_P/0/1/0/all/0/1\">Perry J. Pickhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Pedestrian Attribute Recognition with Group Optimization Reward. (arXiv:2205.14042v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14042","description":"<p>Pedestrian Attribute Recognition (PAR) is a challenging task in intelligent\nvideo surveillance. Two key challenges in PAR include complex alignment\nrelations between images and attributes, and imbalanced data distribution.\nExisting approaches usually formulate PAR as a recognition task. Different from\nthem, this paper addresses it as a decision-making task via a reinforcement\nlearning framework. Specifically, PAR is formulated as a Markov decision\nprocess (MDP) by designing ingenious states, action space, reward function and\nstate transition. To alleviate the inter-attribute imbalance problem, we apply\nan Attribute Grouping Strategy (AGS) by dividing all attributes into subgroups\naccording to their region and category information. Then we employ an agent to\nrecognize each group of attributes, which is trained with Deep Q-learning\nalgorithm. We also propose a Group Optimization Reward (GOR) function to\nalleviate the intra-attribute imbalance problem. Experimental results on the\nthree benchmark datasets of PETA, RAP and PA100K illustrate the effectiveness\nand competitiveness of the proposed approach and demonstrate that the\napplication of reinforcement learning to PAR is a valuable research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengjia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning deep learning models for stereo matching using results from semi-global matching. (arXiv:2205.14051v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14051","description":"<p>Deep learning (DL) methods are widely investigated for stereo image matching\ntasks due to their reported high accuracies. However, their\ntransferability/generalization capabilities are limited by the instances seen\nin the training data. With satellite images covering large-scale areas with\nvariances in locations, content, land covers, and spatial patterns, we expect\ntheir performances to be impacted. Increasing the number and diversity of\ntraining data is always an option, but with the ground-truth disparity being\nlimited in remote sensing due to its high cost, it is almost impossible to\nobtain the ground-truth for all locations. Knowing that classical stereo\nmatching methods such as Census-based semi-global-matching (SGM) are widely\nadopted to process different types of stereo data, we therefore, propose a\nfinetuning method that takes advantage of disparity maps derived from SGM on\ntarget stereo data. Our proposed method adopts a simple scheme that uses the\nenergy map derived from the SGM algorithm to select high confidence disparity\nmeasurements, at the same utilizing the images to limit these selected\ndisparity measurements on texture-rich regions. Our approach aims to\ninvestigate the possibility of improving the transferability of current DL\nmethods to unseen target data without having their ground truth as a\nrequirement. To perform a comprehensive study, we select 20 study-sites around\nthe world to cover a variety of complexities and densities. We choose\nwell-established DL methods like geometric and context network (GCNet), pyramid\nstereo matching network (PSMNet), and LEAStereo for evaluation. Our results\nindicate an improvement in the transferability of the DL methods across\ndifferent regions visually and numerically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albanwan_H/0/1/0/all/0/1\">Hessah Albanwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Harmonization with Region-wise Contrastive Learning. (arXiv:2205.14058v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14058","description":"<p>Image harmonization task aims at harmonizing different composite foreground\nregions according to specific background image. Previous methods would rather\nfocus on improving the reconstruction ability of the generator by some internal\nenhancements such as attention, adaptive normalization and light adjustment,\n$etc.$. However, they pay less attention to discriminating the foreground and\nbackground appearance features within a restricted generator, which becomes a\nnew challenge in image harmonization task. In this paper, we propose a novel\nimage harmonization framework with external style fusion and region-wise\ncontrastive learning scheme. For the external style fusion, we leverage the\nexternal background appearance from the encoder as the style reference to\ngenerate harmonized foreground in the decoder. This approach enhances the\nharmonization ability of the decoder by external background guidance. Moreover,\nfor the contrastive learning scheme, we design a region-wise contrastive loss\nfunction for image harmonization task. Specifically, we first introduce a\nstraight-forward samples generation method that selects negative samples from\nthe output harmonized foreground region and selects positive samples from the\nground-truth background region. Our method attempts to bring together\ncorresponding positive and negative samples by maximizing the mutual\ninformation between the foreground and background styles, which desirably makes\nour harmonization network more robust to discriminate the foreground and\nbackground style features when harmonizing composite images. Extensive\nexperiments on the benchmark datasets show that our method can achieve a clear\nimprovement in harmonization quality and demonstrate the good generalization\ncapability in real-scenario applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingtang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos. (arXiv:2205.14065v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14065","description":"<p>Unsupervised object-centric learning aims to represent the modular,\ncompositional, and causal structure of a scene as a set of object\nrepresentations and thereby promises to resolve many critical limitations of\ntraditional single-vector representations such as poor systematic\ngeneralization. Although there have been many remarkable advances in recent\nyears, one of the most critical problems in this direction has been that\nprevious methods work only with simple and synthetic scenes but not with\ncomplex and naturalistic images or videos. In this paper, we propose STEVE, an\nunsupervised model for object-centric learning in videos. Our proposed model\nmakes a significant advancement by demonstrating its effectiveness on various\ncomplex and naturalistic videos unprecedented in this line of research.\nInterestingly, this is achieved by neither adding complexity to the model\narchitecture nor introducing a new objective or weak supervision. Rather, it is\nachieved by a surprisingly simple architecture that uses a transformer-based\nimage decoder conditioned on slots and the learning objective is simply to\nreconstruct the observation. Our experiment results on various complex and\nnaturalistic videos show significant improvements compared to the previous\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gautam Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Fu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharpness-Aware Training for Free. (arXiv:2205.14083v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14083","description":"<p>Modern deep neural networks (DNNs) have achieved state-of-the-art\nperformances but are typically over-parameterized. The over-parameterization\nmay result in undesirably large generalization error in the absence of other\ncustomized training strategies. Recently, a line of research under the name of\nSharpness-Aware Minimization (SAM) has shown that minimizing a sharpness\nmeasure, which reflects the geometry of the loss landscape, can significantly\nreduce the generalization error. However, SAM-like methods incur a two-fold\ncomputational overhead of the given base optimizer (e.g. SGD) for approximating\nthe sharpness measure. In this paper, we propose Sharpness-Aware Training for\nFree, or SAF, which mitigates the sharp landscape at almost zero additional\ncomputational cost over the base optimizer. Intuitively, SAF achieves this by\navoiding sudden drops in the loss in the sharp local minima throughout the\ntrajectory of the updates of the weights. Specifically, we suggest a novel\ntrajectory loss, based on the KL-divergence between the outputs of DNNs with\nthe current weights and past weights, as a replacement of the SAM's sharpness\nmeasure. This loss captures the rate of change of the training loss along the\nmodel's update trajectory. By minimizing it, SAF ensures the convergence to a\nflat minimum with improved generalization capabilities. Extensive empirical\nresults show that SAF minimizes the sharpness in the same way that SAM does,\nyielding better results on the ImageNet dataset with essentially the same\ncomputational cost as the base optimizer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Daquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vincent Y.F. Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenCalib: A multi-sensor calibration toolbox for autonomous driving. (arXiv:2205.14087v1 [cs.RO])","link":"http://arxiv.org/abs/2205.14087","description":"<p>Accurate sensor calibration is a prerequisite for multi-sensor perception and\nlocalization systems for autonomous vehicles. The intrinsic parameter\ncalibration of the sensor is to obtain the mapping relationship inside the\nsensor, and the extrinsic parameter calibration is to transform two or more\nsensors into a unified spatial coordinate system. Most sensors need to be\ncalibrated after installation to ensure the accuracy of sensor measurements. To\nthis end, we present OpenCalib, a calibration toolbox that contains a rich set\nof various sensor calibration methods. OpenCalib covers manual calibration\ntools, automatic calibration tools, factory calibration tools, and online\ncalibration tools for different application scenarios. At the same time, to\nevaluate the calibration accuracy and subsequently improve the accuracy of the\ncalibration algorithm, we released a corresponding benchmark dataset. This\npaper introduces various features and calibration methods of this toolbox. To\nour knowledge, this is the first open-sourced calibration codebase containing\nthe full set of autonomous-driving-related calibration approaches in this area.\nWe wish that the toolbox could be helpful to autonomous driving researchers. We\nhave open-sourced our code on GitHub to benefit the community. Code is\navailable at https://github.com/PJLab-ADG/SensorsCalibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guohang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuochun_L/0/1/0/all/0/1\">Liu Zhuochun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chunlei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengjin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xinyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zebin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Ming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zheng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIT: A Generative Image-to-text Transformer for Vision and Language. (arXiv:2205.14100v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14100","description":"<p>In this paper, we design and train a Generative Image-to-text Transformer,\nGIT, to unify vision-language tasks such as image/video captioning and question\nanswering. While generative models provide a consistent network architecture\nbetween pre-training and fine-tuning, existing work typically contains complex\nstructures (uni/multi-modal encoder/decoder) and depends on external modules\nsuch as object detectors/taggers and optical character recognition (OCR). In\nGIT, we simplify the architecture as one image encoder and one text decoder\nunder a single language modeling task. We also scale up the pre-training data\nand the model size to boost the model performance. Without bells and whistles,\nour GIT establishes new state of the arts on 12 challenging benchmarks with a\nlarge margin. For instance, our model surpasses the human performance for the\nfirst time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a\nnew scheme of generation-based image classification and scene text recognition,\nachieving decent performance on standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Interpretability via Polynomials. (arXiv:2205.14108v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14108","description":"<p>Generalized Additive Models (GAMs) have quickly become the leading choice for\nfully-interpretable machine learning. However, unlike uninterpretable methods\nsuch as DNNs, they lack expressive power and easy scalability, and are hence\nnot a feasible alternative for real-world tasks. We present a new class of GAMs\nthat use tensor rank decompositions of polynomials to learn powerful,\n$\\textit{fully-interpretable}$ models. Our approach, titled Scalable Polynomial\nAdditive Models (SPAM) is effortlessly scalable and models $\\textit{all}$\nhigher-order feature interactions without a combinatorial parameter explosion.\nSPAM outperforms all current interpretable approaches, and matches DNN/XGBoost\nperformance on a series of real-world benchmarks with up to hundreds of\nthousands of features. We demonstrate by human subject evaluations that SPAMs\nare demonstrably more interpretable in practice, and are hence an effortless\nreplacement for DNNs for creating interpretable and high-performance systems\nsuitable for large-scale machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhimanyu Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Road Segmentation in Challenging Domains Using Similar Place Priors. (arXiv:2205.14112v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14112","description":"<p>Road segmentation in challenging domains, such as night, snow or rain, is a\ndifficult task. Most current approaches boost performance using fine-tuning,\ndomain adaptation, style transfer, or by referencing previously acquired\nimagery. These approaches share one or more of three significant limitations: a\nreliance on large amounts of annotated training data that can be costly to\nobtain, both anticipation of and training data from the type of environmental\nconditions expected at inference time, and/or imagery captured from a previous\nvisit to the location. In this research, we remove these restrictions by\nimproving road segmentation based on similar places. We use Visual Place\nRecognition (VPR) to find similar but geographically distinct places, and fuse\nsegmentations for query images and these similar place priors using a Bayesian\napproach and novel segmentation quality metric. Ablation studies show the need\nto re-evaluate notions of VPR utility for this task. We demonstrate the system\nachieving state-of-the-art road segmentation performance across multiple\nchallenging condition scenarios including night time and snow, without\nrequiring any prior training or previous access to the same geographical\nlocations. Furthermore, we show that this method is network agnostic, improves\nmultiple baseline techniques and is competitive against methods specialised for\nroad prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malone_C/0/1/0/all/0/1\">Connor Malone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peynot_T/0/1/0/all/0/1\">Thierry Peynot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient textual explanations for complex road and traffic scenarios based on semantic segmentation. (arXiv:2205.14118v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14118","description":"<p>The complex driving environment brings great challenges to the visual\nperception of autonomous vehicles. The accuracy of visual perception drops off\nsharply under diverse weather conditions and uncertain traffic flow. Black box\nmodel makes it difficult to interpret the mechanisms of visual perception. To\nenhance the user acceptance and reliability of the visual perception system, a\ntextual explanation of the scene evolvement is essential. It analyzes the\ngeometry and topology structure in the complex environment and offers clues to\ndecision and control. However, the existing scene explanation has been\nimplemented as a separate model. It cannot detect comprehensive textual\ninformation and requires a high computational load and time consumption. Thus,\nthis study proposed a comprehensive and efficient textual explanation model for\ncomplex road and traffic scenarios. From 336k video frames of the driving\nenvironment, critical images of complex road and traffic scenarios were\nselected into a dataset. Through transfer learning, this study established an\naccurate and efficient segmentation model to gain semantic information. Based\non the XGBoost algorithm, a comprehensive model was developed. The model\nobtained textual information including road types, the motion of conflict\nobjects, and scenario complexity. The approach was verified on the real-world\nroad. It improved the perception accuracy of critical traffic elements to\n78.8%. The time consumption reached 13 minutes for each epoch, which was 11.5\ntimes more efficient compared with the pre-trained network. The textual\ninformation analyzed from the model was also accordant with reality. The\nfindings explain how autonomous vehicle detects the driving environment, which\nlays a foundation for subsequent decision and control. It can improve the\nperception ability by enriching the prior knowledge and judgments for complex\ntraffic situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiyue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_X/0/1/0/all/0/1\">Xinyu Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_C/0/1/0/all/0/1\">Chen Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenxuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Basis Models for Interpretability. (arXiv:2205.14120v1 [cs.LG])","link":"http://arxiv.org/abs/2205.14120","description":"<p>Due to the widespread use of complex machine learning models in real-world\napplications, it is becoming critical to explain model predictions. However,\nthese models are typically black-box deep neural networks, explained post-hoc\nvia methods with known faithfulness limitations. Generalized Additive Models\n(GAMs) are an inherently interpretable class of models that address this\nlimitation by learning a non-linear shape function for each feature separately,\nfollowed by a linear model on top. However, these models are typically\ndifficult to train, require numerous parameters, and are difficult to scale.\n</p>\n<p>We propose an entirely new subfamily of GAMs that utilizes basis\ndecomposition of shape functions. A small number of basis functions are shared\namong all features, and are learned jointly for a given task, thus making our\nmodel scale much better to large-scale data with high-dimensional features,\nespecially when features are sparse. We propose an architecture denoted as the\nNeural Basis Model (NBM) which uses a single neural network to learn these\nbases. On a variety of tabular and image datasets, we demonstrate that for\ninterpretable machine learning, NBMs are the state-of-the-art in accuracy,\nmodel size, and, throughput and can easily model all higher-order feature\ninteractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhimanyu Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v1 [cs.CV])","link":"http://arxiv.org/abs/2205.14141","description":"<p>Masked image modeling (MIM) learns representations with remarkably good\nfine-tuning performances, overshadowing previous prevalent pre-training\napproaches such as image classification, instance contrastive learning, and\nimage-text alignment. In this paper, we show that the inferior fine-tuning\nperformance of these pre-training approaches can be significantly improved by a\nsimple post-processing in the form of feature distillation (FD). The feature\ndistillation converts the old representations to new representations that have\na few desirable properties just like those representations produced by MIM.\nThese properties, which we aggregately refer to as optimization friendliness,\nare identified and analyzed by a set of attention- and optimization-related\ndiagnosis tools. With these properties, the new representations show strong\nfine-tuning performance. Specifically, the contrastive self-supervised learning\nmethods are made as competitive in fine-tuning as the state-of-the-art masked\nimage modeling (MIM) algorithms. The CLIP models' fine-tuning performance is\nalso significantly improved, with a CLIP ViT-L model reaching 89.0% top-1\naccuracy on ImageNet-1K classification. More importantly, our work provides a\nway for the future research to focus more effort on the generality and\nscalability of the learnt representations without being pre-occupied with\noptimization friendliness since it can be enhanced rather easily. The code will\nbe available at https://github.com/SwinTransformer/Feature-Distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09124","description":"<p>While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4\\%/16.3\\% of\nResNet-101/ResNet-50. Code is available at https://github.\ncom/Yuting-Gao/DisCo-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame. (arXiv:2105.06340v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06340","description":"<p>Facial expression spotting is the preliminary step for micro- and\nmacro-expression analysis. The task of reliably spotting such expressions in\nvideo sequences is currently unsolved. The current best systems depend upon\noptical flow methods to extract regional motion features, before categorisation\nof that motion into a specific class of facial movement. Optical flow is\nsusceptible to drift error, which introduces a serious problem for motions with\nlong-term dependencies, such as high frame-rate macro-expression. We propose a\npurely deep learning solution which, rather than tracking frame differential\nmotion, compares via a convolutional model, each frame with two temporally\nlocal reference frames. Reference frames are sampled according to calculated\nmicro- and macro-expression duration. As baseline for MEGC2021 using\nleave-one-subject-out evaluation method, we show that our solution achieves\nF1-score of 0.105 in a high frame-rate (200 fps) SAMM long videos dataset\n(SAMM-LV) and is competitive in a low frame-rate (30 fps) (CAS(ME)2) dataset.\nOn unseen MEGC2022 challenge dataset, the baseline results are 0.1176 on SAMM\nChallenge dataset, 0.1739 on CAS(ME)3 and overall performance of 0.1531 on both\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yap_C/0/1/0/all/0/1\">Chuin Hong Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Adrian K. Davison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kendrick_C/0/1/0/all/0/1\">Connah Kendrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_R/0/1/0/all/0/1\">Ryan Cunningham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06011","description":"<p>In the context of visual navigation, the capacity to map a novel environment\nis necessary for an agent to exploit its observation history in the considered\nplace and efficiently reach known goals. This ability can be associated with\nspatial reasoning, where an agent is able to perceive spatial relationships and\nregularities, and discover object characteristics. Recent work introduces\nlearnable policies parametrized by deep neural networks and trained with\nReinforcement Learning (RL). In classical RL setups, the capacity to map and\nreason spatially is learned end-to-end, from reward alone. In this setting, we\nintroduce supplementary supervision in the form of auxiliary tasks designed to\nfavor the emergence of spatial perception capabilities in agents trained for a\ngoal-reaching downstream objective. We show that learning to estimate metrics\nquantifying the spatial relationships between an agent at a given location and\na goal to reach has a high positive impact in Multi-Object Navigation settings.\nOur method significantly improves the performance of different baseline agents,\nthat either build an explicit or implicit representation of the environment,\neven matching the performance of incomparable oracle agents taking ground-truth\nmaps as input. A learning-based agent from the literature trained with the\nproposed auxiliary losses was the winning entry to the Multi-Object Navigation\nChallenge, part of the CVPR 2021 Embodied AI Workshop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marza_P/0/1/0/all/0/1\">Pierre Marza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matignon_L/0/1/0/all/0/1\">Laetitia Matignon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonin_O/0/1/0/all/0/1\">Olivier Simonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts. (arXiv:2108.11926v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11926","description":"<p>Thanks to their ability to learn flexible data-driven losses, Generative\nAdversarial Networks (GANs) are an integral part of many semi- and\nweakly-supervised methods for medical image segmentation. GANs jointly optimise\na generator and an adversarial discriminator on a set of training data. After\ntraining is complete, the discriminator is usually discarded, and only the\ngenerator is used for inference. But should we discard discriminators? In this\nwork, we argue that training stable discriminators produces expressive loss\nfunctions that we can re-use at inference to detect and \\textit{correct}\nsegmentation mistakes. First, we identify key challenges and suggest possible\nsolutions to make discriminators re-usable at inference. Then, we show that we\ncan combine discriminators with image reconstruction costs (via decoders) to\nendow a causal perspective to test-time training and further improve the model.\nOur method is simple and improves the test-time performance of pre-trained\nGANs. Moreover, we show that it is compatible with standard post-processing\ntechniques and it has the potential to be used for Online Continual Learning.\nWith our work, we open new research avenues for re-using adversarial\ndiscriminators at inference. Our code is available at\nhttps://vios-s.github.io/adversarial-test-time-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1\">Gabriele Valvano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leo_A/0/1/0/all/0/1\">Andrea Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfAnFace: Bridging the infant-adult domain gap in facial landmark estimation in the wild. (arXiv:2110.08935v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08935","description":"<p>We lay the groundwork for research in the algorithmic comprehension of infant\nfaces, in anticipation of applications from healthcare to psychology,\nespecially in the early prediction of developmental disorders. Specifically, we\nintroduce the first-ever dataset of infant faces annotated with facial landmark\ncoordinates and pose attributes, demonstrate the inadequacies of existing\nfacial landmark estimation algorithms in the infant domain, and train new\nstate-of-the-art models that significantly improve upon those algorithms using\ndomain adaptation techniques. We touch on the closely related task of facial\ndetection for infants, and also on a challenging case study of infrared baby\nmonitor images gathered by our lab as part of in-field research into the\naforementioned developmental issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Michael Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shaotong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1\">Lingfei Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prateek_G/0/1/0/all/0/1\">Gulati Prateek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaofei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_Mette_R/0/1/0/all/0/1\">Rebecca Schwartz-Mette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_M/0/1/0/all/0/1\">Marie Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_E/0/1/0/all/0/1\">Emily Zimmerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IMPROVE Visiolinguistic Performance with Re-Query. (arXiv:2110.10206v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.10206","description":"<p>We humans regularly ask for clarification if we are confused when discussing\nthe visual world, yet the commonplace requirement in visiolinguistic problems\nlike Visual Dialog, VQA, and Referring Expression Comprehension is to force a\ndecision based on a single, static language input. Since this assumption does\nnot match human practice, we relax it and allow our model to request new\nlanguage inputs to refine the prediction for a task. Through the exemplar task\nof referring expression comprehension, we formalize and motivate the problem,\nintroduce an evaluation method, and propose \\textit{Iterative Multiplication of\nProbabilities for Re-query Of Verbal Expressions} (IMPROVE) -- a re-query\nmethod that updates the model's prediction across multiple queries. We\ndemonstrate IMPROVE on two different referring expression comprehension models\nand show it can improve accuracy by up to 6.23% without additional training or\nmodification to the model's architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemmer_S/0/1/0/all/0/1\">Stephan J. Lemmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason J. Corso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13285","description":"<p>Due to the success of generative flows to model data distributions, they have\nbeen explored in inverse problems. Given a pre-trained generative flow,\nprevious work proposed to minimize the 2-norm of the latent variables as a\nregularization term. The intuition behind it was to ensure high likelihood\nlatent variables that produce the closest restoration. However, high-likelihood\nlatent variables may generate unrealistic samples as we show in our\nexperiments. We therefore propose a solver to directly produce high-likelihood\nreconstructions. We hypothesize that our approach could make generative flows a\ngeneral purpose solver for inverse problems. Furthermore, we propose 1 x 1\ncoupling functions to introduce permutations in a generative flow. It has the\nadvantage that its inverse does not require to be calculated in the generation\nprocess. Finally, we evaluate our method for denoising, deblurring, inpainting,\nand colorization. We observe a compelling improvement of our method over prior\nworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavez_J/0/1/0/all/0/1\">Jos&#xe9; A. Ch&#xe1;vez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.02358","description":"<p>We present a unified Vision-Language pretrained Model (VLMo) that jointly\nlearns a dual encoder and a fusion encoder with a modular Transformer network.\nSpecifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,\nwhere each block contains a pool of modality-specific experts and a shared\nself-attention layer. Because of the modeling flexibility of MoME, pretrained\nVLMo can be fine-tuned as a fusion encoder for vision-language classification\ntasks, or used as a dual encoder for efficient image-text retrieval. Moreover,\nwe propose a stagewise pre-training strategy, which effectively leverages\nlarge-scale image-only and text-only data besides image-text pairs.\nExperimental results show that VLMo achieves state-of-the-art results on\nvarious vision-language tasks, including VQA, NLVR2 and image-text retrieval.\nThe code and pretrained models are available at https://aka.ms/vlmo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_O/0/1/0/all/0/1\">Owais Khan Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_K/0/1/0/all/0/1\">Kriti Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Som_S/0/1/0/all/0/1\">Subhojit Som</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Domain Generalization in Real World: New Benchmark and Strong Baseline. (arXiv:2111.10221v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10221","description":"<p>Conventional domain generalization aims to learn domain invariant\nrepresentation from multiple domains, which requires accurate annotations. In\nrealistic application scenarios, however, it is too cumbersome or even\ninfeasible to collect and annotate the large mass of data. Yet, web data\nprovides a free lunch to access a huge amount of unlabeled data with rich style\ninformation that can be harnessed to augment domain generalization ability. In\nthis paper, we introduce a novel task, termed as semi-supervised domain\ngeneralization, to study how to interact the labeled and unlabeled domains, and\nestablish two benchmarks including a web-crawled dataset, which poses a novel\nyet realistic challenge to push the limits of existing technologies. To tackle\nthis task, a straightforward solution is to propagate the class information\nfrom the labeled to the unlabeled domains via pseudo labeling in conjunction\nwith domain confusion training. Considering narrowing domain gap can improve\nthe quality of pseudo labels and further advance domain invariant feature\nlearning for generalization, we propose a cycle learning framework to encourage\nthe positive feedback between label propagation and domain generalization, in\nfavor of an evolving intermediate domain bridging the labeled and unlabeled\ndomains in a curriculum learning manner. Experiments are conducted to validate\nthe effectiveness of our framework. It is worth highlighting that web-crawled\ndata benefits domain generalization as demonstrated in our results. Our code\nwill be available later.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Luojun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Han Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhifeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhishu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuanlong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shicai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks. (arXiv:2111.12965v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2111.12965","description":"<p>One major goal of the AI security community is to securely and reliably\nproduce and deploy deep learning models for real-world applications. To this\nend, data poisoning based backdoor attacks on deep neural networks (DNNs) in\nthe production stage (or training stage) and corresponding defenses are\nextensively explored in recent years. Ironically, backdoor attacks in the\ndeployment stage, which can often happen in unprofessional users' devices and\nare thus arguably far more threatening in real-world scenarios, draw much less\nattention of the community. We attribute this imbalance of vigilance to the\nweak practicality of existing deployment-stage backdoor attack algorithms and\nthe insufficiency of real-world attack demonstrations. To fill the blank, in\nthis work, we study the realistic threat of deployment-stage backdoor attacks\non DNNs. We base our study on a commonly used deployment-stage attack paradigm\n-- adversarial weight attack, where adversaries selectively modify model\nweights to embed backdoor into deployed DNNs. To approach realistic\npracticality, we propose the first gray-box and physically realizable weights\nattack algorithm for backdoor injection, namely subnet replacement attack\n(SRA), which only requires architecture information of the victim model and can\nsupport physical triggers in the real world. Extensive experimental simulations\nand system-level real-world attack demonstrations are conducted. Our results\nnot only suggest the effectiveness and practicality of the proposed attack\nalgorithm, but also reveal the practical risk of a novel type of computer virus\nthat may widely spread and stealthily inject backdoor into DNN models in user\ndevices. By our study, we call for more attention to the vulnerability of DNNs\nin the deployment stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiangyu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tinghao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Ruizhe Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_K/0/1/0/all/0/1\">Kai Bu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anonymization for Skeleton Action Recognition. (arXiv:2111.15129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15129","description":"<p>Skeleton-based action recognition attracts practitioners and researchers due\nto the lightweight, compact nature of datasets. Compared with RGB-video-based\naction recognition, skeleton-based action recognition is a safer way to protect\nthe privacy of subjects while having competitive recognition performance.\nHowever, due to improvements in skeleton estimation algorithms as well as\nmotion- and depth-sensors, more details of motion characteristics can be\npreserved in the skeleton dataset, leading to potential privacy leakage. To\ninvestigate the potential privacy leakage from skeleton datasets, we first\ntrain a classifier to categorize sensitive private information from\ntrajectories of joints. Our preliminary experiments show that the gender\nclassifier achieves 87% accuracy on average and the re-identification task\nachieves 80% accuracy on average for three baseline models: Shift-GCN, MS-G3D,\nand 2s-AGCN. We propose an adversarial anonymization algorithm to protect\npotential privacy leakage from the skeleton dataset. Experimental results show\nthat an anonymized dataset can reduce the risk of privacy leakage while having\nmarginal effects on action recognition performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Saemi Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myeonghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks. (arXiv:2112.15139v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15139","description":"<p>Quantized neural networks typically require smaller memory footprints and\nlower computation complexity, which is crucial for efficient deployment.\nHowever, quantization inevitably leads to a distribution divergence from the\noriginal network, which generally degrades the performance. To tackle this\nissue, massive efforts have been made, but most existing approaches lack\nstatistical considerations and depend on several manual configurations. In this\npaper, we present an adaptive-mapping quantization method to learn an optimal\nlatent sub-distribution that is inherent within models and smoothly\napproximated with a concrete Gaussian Mixture (GM). In particular, the network\nweights are projected in compliance with the GM-approximated sub-distribution.\nThis sub-distribution evolves along with the weight update in a co-tuning\nschema guided by the direct task-objective optimization. Sufficient experiments\non image classification and object detection over various modern architectures\ndemonstrate the effectiveness, generalization property, and transferability of\nthe proposed method. Besides, an efficient deployment flow for the mobile CPU\nis developed, achieving up to 7.46$\\times$ inference acceleration on an\nocta-core ARM CPU. Our codes have been publicly released at\n\\url{https://github.com/RunpeiDong/DGMS}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Runpei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhanhong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengdi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis. (arXiv:2202.02444v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02444","description":"<p>Neural implicit representations, which encode a surface as the level set of a\nneural network applied to spatial coordinates, have proven to be remarkably\neffective for optimizing, compressing, and generating 3D geometry. Although\nthese representations are easy to fit, it is not clear how to best evaluate\ngeometric queries on the shape, such as intersecting against a ray or finding a\nclosest point. The predominant approach is to encourage the network to have a\nsigned distance property. However, this property typically holds only\napproximately, leading to robustness issues, and holds only at the conclusion\nof training, inhibiting the use of queries in loss functions. Instead, this\nwork presents a new approach to perform queries directly on general neural\nimplicit functions for a wide range of existing architectures. Our key tool is\nthe application of range analysis to neural networks, using automatic\narithmetic rules to bound the output of a network over a region; we conduct a\nstudy of range analysis on neural networks, and identify variants of affine\narithmetic which are highly effective. We use the resulting bounds to develop\ngeometric queries including ray casting, intersection testing, constructing\nspatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating\nbulk properties, and more. Our queries can be efficiently evaluated on GPUs,\nand offer concrete accuracy guarantees even on randomly-initialized networks,\nenabling their use in training objectives and beyond. We also show a\npreliminary application to inverse rendering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharp_N/0/1/0/all/0/1\">Nicholas Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1\">Alec Jacobson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Walks for Adversarial Meshes. (arXiv:2202.07453v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07453","description":"<p>A polygonal mesh is the most-commonly used representation of surfaces in\ncomputer graphics. Therefore, it is not surprising that a number of mesh\nclassification networks have recently been proposed. However, while adversarial\nattacks are wildly researched in 2D, the field of adversarial meshes is under\nexplored. This paper proposes a novel, unified, and general adversarial attack,\nwhich leads to misclassification of several state-of-the-art mesh\nclassification neural networks. Our attack approach is black-box, i.e. it has\naccess only to the network's predictions, but not to the network's full\narchitecture or gradients. The key idea is to train a network to imitate a\ngiven classification network. This is done by utilizing random walks along the\nmesh surface, which gather geometric information. These walks provide insight\nonto the regions of the mesh that are important for the correct prediction of\nthe given classification network. These mesh regions are then modified more\nthan other regions in order to attack the network in a manner that is barely\nvisible to the naked eye.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belder_A/0/1/0/all/0/1\">Amir Belder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yefet_G/0/1/0/all/0/1\">Gal Yefet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izhak_R/0/1/0/all/0/1\">Ran Ben Izhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tal_A/0/1/0/all/0/1\">Ayellet Tal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Universal Rotation Equivariant Point-cloud Network. (arXiv:2203.01216v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01216","description":"<p>Equivariance to permutations and rigid motions is an important inductive bias\nfor various 3D learning problems. Recently it has been shown that the\nequivariant Tensor Field Network architecture is universal -- it can\napproximate any equivariant function. In this paper we suggest a much simpler\narchitecture, prove that it enjoys the same universality guarantees and\nevaluate its performance on Modelnet40. The code to reproduce our experiments\nis available at \\url{https://github.com/simpleinvariance/UniversalNetwork}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Finkelshtein_B/0/1/0/all/0/1\">Ben Finkelshtein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1\">Chaim Baskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dym_N/0/1/0/all/0/1\">Nadav Dym</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03937","description":"<p>Recently, Transformers have shown promising performance in various vision\ntasks. To reduce the quadratic computation complexity caused by each query\nattending to all keys/values, various methods have constrained the range of\nattention within local regions, where each query only attends to keys/values\nwithin a hand-crafted window. However, these hand-crafted window partition\nmechanisms are data-agnostic and ignore their input content, so it is likely\nthat one query maybe attends to irrelevant keys/values. To address this issue,\nwe propose a Dynamic Group Attention (DG-Attention), which dynamically divides\nall queries into multiple groups and selects the most relevant keys/values for\neach group. Our DG-Attention can flexibly model more relevant dependencies\nwithout any spatial constraint that is used in hand-crafted window based\nattention. Built on the DG-Attention, we develop a general vision transformer\nbackbone named Dynamic Group Transformer (DGT). Extensive experiments show that\nour models can outperform the state-of-the-art methods on multiple common\nvision tasks, including image classification, semantic segmentation, object\ndetection, and instance segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4. (arXiv:2203.06649v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2203.06649","description":"<p>Modern high-scoring models of vision in the brain score competition do not\nstem from Vision Transformers. However, in this paper, we provide evidence\nagainst the unexpected trend of Vision Transformers (ViT) being not\nperceptually aligned with human visual representations by showing how a\ndual-stream Transformer, a CrossViT$~\\textit{a la}$ Chen et al. (2021), under a\njoint rotationally-invariant and adversarial optimization procedure yields 2nd\nplace in the aggregate Brain-Score 2022 competition(Schrimpf et al., 2020b)\naveraged across all visual categories, and at the time of the competition held\n1st place for the highest explainable variance of area V4. In addition, our\ncurrent Transformer-based model also achieves greater explainable variance for\nareas V4, IT and Behaviour than a biologically-inspired CNN (ResNet50) that\nintegrates a frontal V1-like computation module (Dapello et al.,2020). To\nassess the contribution of the optimization scheme with respect to the CrossViT\narchitecture, we perform several additional experiments on differently\noptimized CrossViT's regarding adversarial robustness, common corruption\nbenchmarks, mid-ventral stimuli interpretation and feature inversion. Against\nour initial expectations, our family of results provides tentative support for\nan $\\textit{\"All roads lead to Rome\"}$ argument enforced via a joint\noptimization rule even for non biologically-motivated models of vision such as\nVision Transformers. Code is available at\nhttps://github.com/williamberrios/BrainScore-Transformers\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Berrios_W/0/1/0/all/0/1\">William Berrios</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Deza_A/0/1/0/all/0/1\">Arturo Deza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two-Stage Federated Transfer Learning Framework in Medical Images Classification on Limited Data: A COVID-19 Case Study. (arXiv:2203.12803v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.12803","description":"<p>COVID-19 pandemic has spread rapidly and caused a shortage of global medical\nresources. The efficiency of COVID-19 diagnosis has become highly significant.\nAs deep learning and convolutional neural network (CNN) has been widely\nutilized and been verified in analyzing medical images, it has become a\npowerful tool for computer-assisted diagnosis. However, there are two most\nsignificant challenges in medical image classification with the help of deep\nlearning and neural networks, one of them is the difficulty of acquiring enough\nsamples, which may lead to model overfitting. Privacy concerns mainly bring the\nother challenge since medical-related records are often deemed patients'\nprivate information and protected by laws such as GDPR and HIPPA. Federated\nlearning can ensure the model training is decentralized on different devices\nand no data is shared among them, which guarantees privacy. However, with data\nlocated on different devices, the accessible data of each device could be\nlimited. Since transfer learning has been verified in dealing with limited data\nwith good performance, therefore, in this paper, We made a trial to implement\nfederated learning and transfer learning techniques using CNNs to classify\nCOVID-19 using lung CT scans. We also explored the impact of dataset\ndistribution at the client-side in federated learning and the number of\ntraining epochs a model is trained. Finally, we obtained very high performance\nwith federated learning, demonstrating our success in leveraging accuracy and\nprivacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1\">Alexandros Shikun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1\">Naomi Fengqi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. (arXiv:2203.13457v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13457","description":"<p>Recently, contrastive learning has risen to be a promising approach for\nlarge-scale self-supervised learning. However, theoretical understanding of how\nit works is still unclear. In this paper, we propose a new guarantee on the\ndownstream performance without resorting to the conditional independence\nassumption that is widely adopted in previous work but hardly holds in\npractice. Our new theory hinges on the insight that the support of different\nintra-class samples will become more overlapped under aggressive data\naugmentations, thus simply aligning the positive samples (augmented views of\nthe same sample) could make contrastive learning cluster intra-class samples\ntogether. Based on this augmentation overlap perspective, theoretically, we\nobtain asymptotically closed bounds for downstream performance under weaker\nassumptions, and empirically, we propose an unsupervised model selection metric\nARC that aligns well with downstream accuracy. Our theory suggests an\nalternative understanding of contrastive learning: the role of aligning\npositive samples is more like a surrogate task than an ultimate goal, and the\noverlapped augmented views (i.e., the chaos) create a ladder for contrastive\nlearning to gradually learn class-separated representations. The code for\ncomputing ARC is available at https://github.com/zhangq327/ARC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiansheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable Model from a Hybrid Dataset. (arXiv:2203.14057v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14057","description":"<p>We present FaceVerse, a fine-grained 3D Neural Face Model, which is built\nfrom hybrid East Asian face datasets containing 60K fused RGB-D images and 2K\nhigh-fidelity 3D head scan models. A novel coarse-to-fine structure is proposed\nto take better advantage of our hybrid dataset. In the coarse module, we\ngenerate a base parametric model from large-scale RGB-D images, which is able\nto predict accurate rough 3D face models in different genders, ages, etc. Then\nin the fine module, a conditional StyleGAN architecture trained with\nhigh-fidelity scan models is introduced to enrich elaborate facial geometric\nand texture details. Note that different from previous methods, our base and\ndetailed modules are both changeable, which enables an innovative application\nof adjusting both the basic attributes and the facial details of 3D face\nmodels. Furthermore, we propose a single-image fitting framework based on\ndifferentiable rendering. Rich experiments show that our method outperforms the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lizhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UV Volumes for Real-time Rendering of Editable Free-view Human Performance. (arXiv:2203.14402v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14402","description":"<p>Neural volume rendering enables photo-realistic renderings of a human\nperformer in free-view, a critical task in immersive VR/AR applications. But\nthe practice is severely limited by high computational costs in the rendering\nprocess. To solve this problem, we propose the UV Volumes, a new approach that\ncan render an editable free-view video of a human performer in realtime. It\nseparates the high-frequency (i.e., non-smooth) human appearance from the 3D\nvolume, and encodes them into 2D neural texture stacks (NTS). The smooth UV\nvolumes allow much smaller and shallower neural networks to obtain densities\nand texture coordinates in 3D while capturing detailed appearance in 2D NTS.\nFor editability, the mapping between the parameterized human model and the\nsmooth texture coordinates allows us a better generalization on novel poses and\nshapes. Furthermore, the use of NTS enables interesting applications, e.g.,\nretexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M\ndatasets show that our model can render 960 * 540 images in 30FPS on average\nwith comparable photo-realism to state-of-the-art methods. The project and\nsupplementary materials are available at https://fanegg.github.io/UV-Volumes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00598","description":"<p>Large pretrained (e.g., \"foundation\") models exhibit distinct capabilities\ndepending on the domain of data they are trained on. While these domains are\ngeneric, they may only barely overlap. For example, visual-language models\n(VLMs) are trained on Internet-scale image captions, but large language models\n(LMs) are further trained on Internet-scale text with no images (e.g.,\nspreadsheets, SAT questions, code). As a result, these models store different\nforms of commonsense knowledge across different domains. In this work, we show\nthat this diversity is symbiotic, and can be leveraged through Socratic Models\n(SMs): a modular framework in which multiple pretrained models may be composed\nzero-shot i.e., via multimodal-informed prompting, to exchange information with\neach other and capture new multimodal capabilities, without requiring\nfinetuning. With minimal engineering, SMs are not only competitive with\nstate-of-the-art zero-shot image captioning and video-to-text retrieval, but\nalso enable new applications such as (i) answering free-form questions about\negocentric video, (ii) engaging in multimodal assistive dialogue with people\n(e.g., for cooking recipes) by interfacing with external APIs and databases\n(e.g., web search), and (iii) robot perception and planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attarian_M/0/1/0/all/0/1\">Maria Attarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1\">Brian Ichter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1\">Krzysztof Choromanski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Adrian Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welker_S/0/1/0/all/0/1\">Stefan Welker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_A/0/1/0/all/0/1\">Aveek Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sindhwani_V/0/1/0/all/0/1\">Vikas Sindhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Johnny Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1\">Vincent Vanhoucke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E^2TAD: An Energy-Efficient Tracking-based Action Detector. (arXiv:2204.04416v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04416","description":"<p>Video action detection (spatio-temporal action localization) is usually the\nstarting point for human-centric intelligent analysis of videos nowadays. It\nhas high practical impacts for many applications across robotics, security,\nhealthcare, etc. The two-stage paradigm of Faster R-CNN inspires a standard\nparadigm of video action detection in object detection, i.e., firstly\ngenerating person proposals and then classifying their actions. However, none\nof the existing solutions could provide fine-grained action detection to the\n\"who-when-where-what\" level. This paper presents a tracking-based solution to\naccurately and efficiently localize predefined key actions spatially (by\npredicting the associated target IDs and locations) and temporally (by\npredicting the time in exact frame indices). This solution won first place in\nthe UAV-Video Track of 2021 Low-Power Computer Vision Challenge (LPCVC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hao-Yu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1\">Siqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1\">Taiyu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pi_P/0/1/0/all/0/1\">Pengcheng Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Multi-grid Methods for Minimizing Curvature Energy. (arXiv:2204.07921v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.07921","description":"<p>The geometric high-order regularization methods such as mean curvature and\nGaussian curvature, have been intensively studied during the last decades due\nto their abilities in preserving geometric properties including image edges,\ncorners, and image contrast. However, the dilemma between restoration quality\nand computational efficiency is an essential roadblock for high-order methods.\nIn this paper, we propose fast multi-grid algorithms for minimizing both mean\ncurvature and Gaussian curvature energy functionals without sacrificing the\naccuracy for efficiency. Unlike the existing approaches based on operator\nsplitting and the Augmented Lagrangian method (ALM), no artificial parameters\nare introduced in our formulation, which guarantees the robustness of the\nproposed algorithm. Meanwhile, we adopt the domain decomposition method to\npromote parallel computing and use the fine-to-coarse structure to accelerate\nthe convergence. Numerical experiments are presented on both image denoising\nand CT reconstruction problem to demonstrate the ability to recover image\ntexture and the efficiency of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenwei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Y/0/1/0/all/0/1\">Yuping Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (arXiv:2204.14057v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.14057","description":"<p>We present an approach to learn voice-face representations from the talking\nface videos, without any identity labels. Previous works employ cross-modal\ninstance discrimination tasks to establish the correlation of voice and face.\nThese methods neglect the semantic content of different videos, introducing\nfalse-negative pairs as training noise. Furthermore, the positive pairs are\nconstructed based on the natural correlation between audio clips and visual\nframes. However, this correlation might be weak or inaccurate in a large amount\nof real-world data, which leads to deviating positives into the contrastive\nparadigm. To address these issues, we propose the cross-modal prototype\ncontrastive learning (CMPC), which takes advantage of contrastive methods and\nresists adverse effects of false negatives and deviate positives. On one hand,\nCMPC could learn the intra-class invariance by constructing semantic-wise\npositives via unsupervised clustering in different modalities. On the other\nhand, by comparing the similarities of cross-modal instances from that of\ncross-modal prototypes, we dynamically recalibrate the unlearnable instances'\ncontribution to overall loss. Experiments show that the proposed approach\noutperforms state-of-the-art unsupervised methods on various voice-face\nassociation evaluation protocols. Additionally, in the low-shot supervision\nsetting, our method also has a significant improvement compared to previous\ninstance-wise contrastive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Boqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changjian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zheng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huaimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuxing Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers. (arXiv:2204.14217v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.14217","description":"<p>The development of the transformer-based text-to-image models are impeded by\nits slow generation and complexity for high-resolution images. In this work, we\nput forward a solution based on hierarchical transformers and local parallel\nauto-regressive generation. We pretrain a 6B-parameter transformer with a\nsimple and flexible self-supervised task, Cross-modal general language model\n(CogLM), and finetune it for fast super-resolution. The new text-to-image\nsystem, CogView2, shows very competitive generation compared to concurrent\nstate-of-the-art DALL-E-2, and naturally supports interactive text-guided\nediting on images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wendi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenyi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations. (arXiv:2205.05167v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05167","description":"<p>Recent neural network architectures have claimed to explain data from the\nhuman visual cortex. Their demonstrated performance is however still limited by\nthe dependence on exploiting low-level features for solving visual tasks. This\nstrategy limits their performance in case of out-of-distribution/adversarial\ndata. Humans, meanwhile learn abstract concepts and are mostly unaffected by\neven extreme image distortions. Humans and networks employ strikingly different\nstrategies to solve visual tasks. To probe this, we introduce a novel set of\nimage transforms and evaluate humans and networks on an object recognition\ntask. We found performance for a few common networks quickly decreases while\nhumans are able to recognize objects with a high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crowder_D/0/1/0/all/0/1\">Dakarai Crowder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_G/0/1/0/all/0/1\">Girik Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Teaching Independent Parts Separately\" (TIPSy-GAN) : Improving Accuracy and Stability in Unsupervised Adversarial 2D to 3D Pose Estimation. (arXiv:2205.05980v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05980","description":"<p>We present TIPSy-GAN, a new approach to improve the accuracy and stability in\nunsupervised adversarial 2D to 3D human pose estimation. In our work we\ndemonstrate that the human kinematic skeleton should not be assumed as a single\nspatially codependent structure; in fact, we posit when a full 2D pose is\nprovided during training, there is an inherent bias learned where the 3D\ncoordinate of a keypoint is spatially codependent on the 2D coordinates of all\nother keypoints. To investigate our hypothesis we follow previous adversarial\napproaches but train two generators on spatially independent parts of the\nkinematic skeleton, the torso and the legs. We find that improving the\nself-consistency cycle is key to lowering the evaluation error and therefore\nintroduce new consistency constraints during training. A TIPSy model is\nproduced via knowledge distillation from these generators which can predict the\n3D ordinates for the entire 2D pose with improved results. Furthermore, we\naddress an unanswered question in prior work of how long to train in a truly\nunsupervised scenario. We show that for two independent generators training\nadversarially has improved stability than that of a solo generator which\ncollapses. TIPSy decreases the average error by 17\\% when compared to that of a\nbaseline solo generator on the Human3.6M dataset. TIPSy improves upon other\nunsupervised approaches while also performing strongly against supervised and\nweakly-supervised approaches during evaluation on both the Human3.6M and\nMPI-INF-3DHP datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardy_P/0/1/0/all/0/1\">Peter Hardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasmahapatra_S/0/1/0/all/0/1\">Srinandan Dasmahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hansung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Attention Composition for Temporal Action Localization. (arXiv:2205.09956v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09956","description":"<p>Temporal action localization aims at localizing action instances from\nuntrimmed videos. Existing works have designed various effective modules to\nprecisely localize action instances based on appearance and motion features.\nHowever, by treating these two kinds of features with equal importance,\nprevious works cannot take full advantage of each modality feature, making the\nlearned model still sub-optimal. To tackle this issue, we make an early effort\nto study temporal action localization from the perspective of multi-modality\nfeature learning, based on the observation that different actions exhibit\nspecific preferences to appearance or motion modality. Specifically, we build a\nnovel structured attention composition module. Unlike conventional attention,\nthe proposed module would not infer frame attention and modality attention\nindependently. Instead, by casting the relationship between the modality\nattention and the frame attention as an attention assignment process, the\nstructured attention composition module learns to encode the frame-modality\nstructure and uses it to regularize the inferred frame attention and modality\nattention, respectively, upon the optimal transport theory. The final\nframe-modality attention is obtained by the composition of the two individual\nattentions. The proposed structured attention composition module can be\ndeployed as a plug-and-play module into existing action localization\nframeworks. Extensive experiments on two widely used benchmarks show that the\nproposed structured attention composition consistently improves four\nstate-of-the-art temporal action localization methods and builds new\nstate-of-the-art performance on THUMOS14. Code is availabel at\nhttps://github.com/VividLe/Structured-Attention-Composition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Le Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes. (arXiv:2205.10337v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10337","description":"<p>We introduce UViM, a unified approach capable of modeling a wide range of\ncomputer vision tasks. In contrast to previous models, UViM has the same\nfunctional form for all tasks; it requires no task-specific modifications which\nrequire extensive human expertise. The approach involves two components: (I) a\nbase model (feed-forward) which is trained to directly predict raw vision\noutputs, guided by a learned discrete code and (II) a language model\n(autoregressive) that is trained to generate the guiding code. These components\ncomplement each other: the language model is well-suited to modeling structured\ninterdependent data, while the base model is efficient at dealing with\nhigh-dimensional outputs. We demonstrate the effectiveness of UViM on three\ndiverse and challenging vision tasks: panoptic segmentation, depth prediction\nand image colorization, where we achieve competitive and near state-of-the-art\nresults. Our experimental results suggest that UViM is a promising candidate\nfor a unified modeling approach in computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1\">Alexander Kolesnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_A/0/1/0/all/0/1\">Andr&#xe9; Susano Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmsen_J/0/1/0/all/0/1\">Jeremiah Harmsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1\">Neil Houlsby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13326","description":"<p>This paper describes the methods submitted for evaluation to the SHREC 2022\ntrack on pothole and crack detection in the road pavement. A total of 7\ndifferent runs for the semantic segmentation of the road surface are compared,\n6 from the participants plus a baseline method. All methods exploit Deep\nLearning techniques and their performance is tested using the same environment\n(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic\nsegmentation image/mask pairs and 797 RGB-D video clips collected with the\nlatest depth cameras was made available to the participants. The methods are\nthen evaluated on the 496 image/mask pairs in the validation set, on the 504\npairs in the test set and finally on 8 video clips. The analysis of the results\nis based on quantitative metrics for image segmentation and qualitative\nanalysis of the video clips. The participation and the results show that the\nscenario is of great interest and that the use of RGB-D data is still\nchallenging in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_E/0/1/0/all/0/1\">Elia Moscoso Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranieri_A/0/1/0/all/0/1\">Andrea Ranieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biasotti_S/0/1/0/all/0/1\">Silvia Biasotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chicchon_M/0/1/0/all/0/1\">Miguel Chicchon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sipiran_I/0/1/0/all/0/1\">Ivan Sipiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh-Khoi Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Ho_T/0/1/0/all/0/1\">Thang-Long Nguyen-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai-Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13331","description":"<p>This paper deals with deep transductive learning, and proposes TransBoost as\na procedure for fine-tuning any deep neural model to improve its performance on\nany (unlabeled) test set provided at training time. TransBoost is inspired by a\nlarge margin principle and is efficient and simple to use. The ImageNet\nclassification performance is consistently and significantly improved with\nTransBoost on many architectures such as ResNets, MobileNetV3-L,\nEfficientNetB0, ViT-S, and ConvNext-T. Additionally we show that TransBoost is\neffective on a wide variety of image classification datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belhasin_O/0/1/0/all/0/1\">Omer Belhasin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bar_Shalom_G/0/1/0/all/0/1\">Guy Bar-Shalom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Yaniv_R/0/1/0/all/0/1\">Ran El-Yaniv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing the Dark Secrets of Masked Image Modeling. (arXiv:2205.13543v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13543","description":"<p>Masked image modeling (MIM) as pre-training is shown to be effective for\nnumerous vision downstream tasks, but how and where MIM works remain unclear.\nIn this paper, we compare MIM with the long-dominant supervised pre-trained\nmodels from two perspectives, the visualizations and the experiments, to\nuncover their key representational differences. From the visualizations, we\nfind that MIM brings locality inductive bias to all layers of the trained\nmodels, but supervised models tend to focus locally at lower layers but more\nglobally at higher layers. That may be the reason why MIM helps Vision\nTransformers that have a very large receptive field to optimize. Using MIM, the\nmodel can maintain a large diversity on attention heads in all layers. But for\nsupervised models, the diversity on attention heads almost disappears from the\nlast three layers and less diversity harms the fine-tuning performance. From\nthe experiments, we find that MIM models can perform significantly better on\ngeometric and motion tasks with weak semantics or fine-grained classification\ntasks, than their supervised counterparts. Without bells and whistles, a\nstandard MIM pre-trained SwinV2-L could achieve state-of-the-art performance on\npose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth\nestimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object\ntracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the\ncategories are sufficiently covered by the supervised pre-training, MIM models\ncan still achieve highly competitive transfer performance. With a deeper\nunderstanding of MIM, we hope that our work can inspire new and solid research\nin this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zigang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingcheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}