{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05598","description":"<p>The study of the applicability of the BERTScore metric was conducted to\ntranslation quality assessment at the sentence level for English -&gt; Russian\ndirection. Experiments were performed with a pre-trained multilingual BERT as\nwell as with a pair of monolingual BERT models. To align the monolingual\nembeddings, an orthogonal transformation based on anchor tokens was used. It\nwas demonstrated that such transformation helps to prevent mismatching issue\nand shown that this approach gives better results than using embeddings of the\nmultilingual model. To improve the token matching process it is proposed to\ncombine all incomplete WorkPiece tokens into meaningful words and use simple\naveraging of corresponding vectors and to calculate BERTScore based on anchor\ntokens only. Such modifications allowed us to achieve a better correlation of\nthe model predictions with human estimates. In addition to evaluating machine\ntranslation, several versions of human translation were evaluated as well, the\nproblems of this approach were listed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_A/0/1/0/all/0/1\">A.A. Vetrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorn_E/0/1/0/all/0/1\">E.A. Gorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Free Attentive Scoring for Speaker Verification. (arXiv:2203.05642v1 [cs.SD])","link":"http://arxiv.org/abs/2203.05642","description":"<p>This paper presents a novel study of parameter-free attentive scoring for\nspeaker verification. Parameter-free scoring provides the flexibility of\ncomparing speaker representations without the need of an accompanying\nparametric scoring model. Inspired by the attention component in Transformer\nneural networks, we propose a variant of the scaled dot product attention\nmechanism to compare enrollment and test segment representations. In addition,\nthis work explores the effect on performance of (i) different types of\nnormalization, (ii) independent versus tied query/key estimation, (iii) varying\nthe number of key-value pairs and (iv) pooling multiple enrollment utterance\nstatistics. Experimental results for a 4 task average show that a simple\nparameter-free attentive scoring mechanism can improve the average EER by 10%\nover the best cosine similarity baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiling Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Sensorimotor Norms: multi-dimensional measures of sensorimotor strength for ambiguous English words, in context. (arXiv:2203.05648v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05648","description":"<p>Most large language models are trained on linguistic input alone, yet humans\nappear to ground their understanding of words in sensorimotor experience. A\nnatural solution is to augment LM representations with human judgments of a\nword's sensorimotor associations (e.g., the Lancaster Sensorimotor Norms), but\nthis raises another challenge: most words are ambiguous, and judgments of words\nin isolation fail to account for this multiplicity of meaning (e.g., \"wooden\ntable\" vs. \"data table\"). We attempted to address this problem by building a\nnew lexical resource of contextualized sensorimotor judgments for 112 English\nwords, each rated in four different contexts (448 sentences total). We show\nthat these ratings encode overlapping but distinct information from the\nLancaster Sensorimotor Norms, and that they also predict other measures of\ninterest (e.g., relatedness), above and beyond measures derived from BERT.\nBeyond shedding light on theoretical questions, we suggest that these ratings\ncould be of use as a \"challenge set\" for researchers building grounded language\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trott_S/0/1/0/all/0/1\">Sean Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NELA-GT-2021: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles. (arXiv:2203.05659v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05659","description":"<p>In this paper, we present the fourth installment of the NELA-GT datasets,\nNELA-GT-2021. The dataset contains 1.8M articles from 367 outlets between\nJanuary 1st, 2021 and December 31st, 2021. Just as in past releases of the\ndataset, NELA-GT-2021 includes outlet-level veracity labels from Media\nBias/Fact Check and tweets embedded in collected news articles. The\nNELA-GT-2021 dataset can be found at: https://doi.org/10.7910/DVN/RBKVBM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gruppi_M/0/1/0/all/0/1\">Maur&#xed;cio Gruppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horne_B/0/1/0/all/0/1\">Benjamin D. Horne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adali_S/0/1/0/all/0/1\">Sibel Adal&#x131;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05711","description":"<p>Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives(SyMoN), containing\n5,193 video summaries of popular movies and TV series. SyMoN captures\nnaturalistic storytelling videos for human audience made by human creators, and\nhas higher story coverage and more frequent mental-state references than\nsimilar video-language story datasets. Differing from most existing video-text\ndatasets, SyMoN features large semantic gaps between the visual and the textual\nmodalities due to the prevalence of reporting bias and mental state\ndescriptions. We establish benchmarks on video-text retrieval and zero-shot\nalignment on movie summary videos. With SyMoN, we hope to lay the groundwork\nfor progress in multimodal story understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yidan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Q/0/1/0/all/0/1\">Qin Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-constraint Optimal Transport for Entity Alignment with Dangling Cases. (arXiv:2203.05744v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05744","description":"<p>Entity alignment (EA) merges knowledge graphs (KGs) by identifying the\nequivalent entities in different graphs, which can effectively enrich knowledge\nrepresentations of KGs. However, in practice, different KGs often include\ndangling entities whose counterparts cannot be found in the other graph, which\nlimits the performance of EA methods. To improve EA with dangling entities, we\npropose an unsupervised method called Semi-constraint Optimal Transport for\nEntity Alignment in Dangling cases (SoTead). Our main idea is to model the\nentity alignment between two KGs as an optimal transport problem from one KG's\nentities to the others. First, we set pseudo entity pairs between KGs based on\npretrained word embeddings. Then, we conduct contrastive metric learning to\nobtain the transport cost between each entity pair. Finally, we introduce a\nvirtual entity for each KG to \"align\" the dangling entities from the other KGs,\nwhich relaxes the optimization constraints and leads to a semi-constraint\noptimal transport. In the experimental part, we first show the superiority of\nSoTead on a commonly-used entity alignment dataset. Besides, to analyze the\nability for dangling entity detection with other baselines, we construct a\nmedical cross-lingual knowledge graph dataset, MedED, where our SoTead also\nreaches state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pengyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval. (arXiv:2203.05765v1 [cs.IR])","link":"http://arxiv.org/abs/2203.05765","description":"<p>Recent rapid advancements in deep pre-trained language models and the\nintroductions of large datasets have powered research in embedding-based dense\nretrieval. While several good research papers have emerged, many of them come\nwith their own software stacks. These stacks are typically optimized for some\nparticular research goals instead of efficiency or code structure. In this\npaper, we present Tevatron, a dense retrieval toolkit optimized for efficiency,\nflexibility, and code simplicity. Tevatron provides a standardized pipeline for\ndense retrieval including text processing, model training, corpus/query\nencoding, and search. This paper presents an overview of Tevatron and\ndemonstrates its effectiveness and efficiency across several IR and QA data\nsets. We also show how Tevatron's flexible design enables easy generalization\nacross datasets, model architectures, and accelerator platforms(GPU/TPU). We\nbelieve Tevatron can serve as an effective software foundation for dense\nretrieval system research including design, modeling, and optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTopic: Neural topic modeling with a class-based TF-IDF procedure. (arXiv:2203.05794v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05794","description":"<p>Topic models can be useful tools to discover latent topics in collections of\ndocuments. Recent studies have shown the feasibility of approach topic modeling\nas a clustering task. We present BERTopic, a topic model that extends this\nprocess by extracting coherent topic representation through the development of\na class-based variation of TF-IDF. More specifically, BERTopic generates\ndocument embedding with pre-trained transformer-based language models, clusters\nthese embeddings, and finally, generates topic representations with the\nclass-based TF-IDF procedure. BERTopic generates coherent topics and remains\ncompetitive across a variety of benchmarks involving classical models and those\nthat follow the more recent clustering approach of topic modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grootendorst_M/0/1/0/all/0/1\">Maarten Grootendorst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. (arXiv:2203.05797v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05797","description":"<p>Most of the open-domain dialogue models tend to perform poorly in the setting\nof long-term human-bot conversations. The possible reason is that they lack the\ncapability of understanding and memorizing long-term dialogue history\ninformation. To address this issue, we present a novel task of Long-term Memory\nConversation (LeMon) and then build a new dialogue dataset DuLeMon and a\ndialogue generation framework with Long-Term Memory (LTM) mechanism (called\nPLATO-LTM). This LTM mechanism enables our system to accurately extract and\ncontinuously update long-term persona memory without requiring multiple-session\ndialogue datasets for model training. To our knowledge, this is the first\nattempt to conduct real-time dynamic management of persona information of both\nparties, including the user and the bot. Results on DuLeMon indicate that\nPLATO-LTM can significantly outperform baselines in terms of long-term dialogue\nconsistency, leading to better dialogue engagingness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_Z/0/1/0/all/0/1\">Zhibin Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenquan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zheng-Yu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shihang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Open Intent Detection. (arXiv:2203.05823v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05823","description":"<p>The open intent detection problem is presented in this paper, which aims to\nidentify known intents and detect open intent in natural language\nunderstanding. Current methods have two core challenges. On the one hand, the\nexisting methods have limitations in learning robust representations to detect\nthe open intent without any prior knowledge. On the other hand, there lacks an\neffective approach to learning the specific and compact decision boundary to\ndistinguish the known intents and the open intent. This paper introduces an\noriginal pipeline framework, DA-ADB, to address these issues, which\nsuccessively learns discriminative intent features with distance-aware strategy\nand appropriate decision boundaries adaptive to the feature space for open\nintent detection. The proposed method first leverages distance information to\nenhance the distinguishing capability of the intent representations. Then, it\nobtains discriminative decision boundaries adaptive to the known intent feature\nspace by balancing both the empirical and open space risks. Extensive\nexperiments show the effectiveness of distance-aware and boundary learning\nstrategies. Compared with the state-of-the-art methods, our method achieves\nsubstantial improvements on three benchmark intent datasets. It also yields\nrobust performance with different proportions of labeled data and known\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shaojie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianrui Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Identification and Classification of Bragging in Social Media. (arXiv:2203.05840v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05840","description":"<p>Bragging is a speech act employed with the goal of constructing a favorable\nself-image through positive statements about oneself. It is widespread in daily\ncommunication and especially popular in social media, where users aim to build\na positive image of their persona directly or indirectly. In this paper, we\npresent the first large scale study of bragging in computational linguistics,\nbuilding on previous research in linguistics and pragmatics. To facilitate\nthis, we introduce a new publicly available data set of tweets annotated for\nbragging and their types. We empirically evaluate different transformer-based\nmodels injected with linguistic information in (a) binary bragging\nclassification, i.e., if tweets contain bragging statements or not; and (b)\nmulti-class bragging type prediction including not bragging. Our results show\nthat our models can predict bragging with macro F1 up to 72.42 and 35.95 in the\nbinary and multi-class classification tasks respectively. Finally, we present\nan extensive linguistic and error analysis of bragging prediction to guide\nfuture research on this topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1\">Mali Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preotiuc_Pietro_D/0/1/0/all/0/1\">Daniel Preo&#x163;iuc-Pietro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogruoz_A/0/1/0/all/0/1\">A. Seza Do&#x11f;ru&#xf6;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation. (arXiv:2203.05843v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05843","description":"<p>We study the interpretability issue of task-oriented dialogue systems in this\npaper. Previously, most neural-based task-oriented dialogue systems employ an\nimplicit reasoning strategy that makes the model predictions uninterpretable to\nhumans. To obtain a transparent reasoning process, we introduce neuro-symbolic\nto perform explicit reasoning that justifies model decisions by reasoning\nchains. Since deriving reasoning chains requires multi-hop reasoning for\ntask-oriented dialogues, existing neuro-symbolic approaches would induce error\npropagation due to the one-phase design. To overcome this, we propose a\ntwo-phase approach that consists of a hypothesis generator and a reasoner. We\nfirst obtain multiple hypotheses, i.e., potential operations to perform the\ndesired task, through the hypothesis generator. Each hypothesis is then\nverified by the reasoner, and the valid one is selected to conduct the final\nprediction. The whole system is trained by exploiting raw textual dialogues\nwithout using any reasoning chain annotations. Experimental studies on two\npublic benchmark datasets demonstrate that the proposed approach not only\nachieves better results, but also introduces an interpretable decision process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shiquan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1\">Sarah Erfani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Word Embeddings to Analyze Protests News. (arXiv:2203.05875v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05875","description":"<p>The first two tasks of the CLEF 2019 ProtestNews events focused on\ndistinguishing between protest and non-protest related news articles and\nsentences in a binary classification task. Among the submissions, two well\nperforming models have been chosen in order to replace the existing word\nembeddings word2vec and FastTest with ELMo and DistilBERT. Unlike bag of words\nor earlier vector approaches, ELMo and DistilBERT represent words as a sequence\nof vectors by capturing the meaning based on contextual information in the\ntext. Without changing the architecture of the original models other than the\nword embeddings, the implementation of DistilBERT improved the performance\nmeasured on the F1-Score of 0.66 compared to the FastText implementation.\nDistilBERT also outperformed ELMo in both tasks and models. Cleaning the\ndatasets by removing stopwords and lemmatizing the words has been shown to make\nthe models more generalizable across different contexts when training on a\ndataset with Indian news articles and evaluating the models on a dataset with\nnews articles from China.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceron_M/0/1/0/all/0/1\">Maria Alejandra Cardoza Ceron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings. (arXiv:2203.05877v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05877","description":"<p>Contrastive learning has shown great potential in unsupervised sentence\nembedding tasks, e.g., SimCSE. However, We find that these existing solutions\nare heavily affected by superficial features like the length of sentences or\nsyntactic structures. In this paper, we propose a semantics-aware contrastive\nlearning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT),\nwhich is able to exploit the pseudo-token space (i.e., latent semantic space)\nrepresentation of a sentence while eliminating the impact of superficial\nfeatures such as sentence length and syntax. Specifically, we introduce an\nadditional pseudo token embedding layer independent of the BERT encoder to map\neach sentence into a sequence of pseudo tokens in a fixed length. Leveraging\nthese pseudo sequences, we are able to construct same-length positive and\nnegative pairs based on the attention mechanism to perform contrastive\nlearning. In addition, we utilize both the gradient-updating and\nmomentum-updating encoders to encode instances while dynamically maintaining an\nadditional queue to store the representation of sentence embeddings, enhancing\nthe encoder's learning performance for negative examples. Experiments show that\nour model outperforms the state-of-the-art baselines on six standard semantic\ntextual similarity (STS) tasks. Furthermore, experiments on alignments and\nuniformity losses, as well as hard examples with different sentence lengths and\nsyntax, consistently verify the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1\">Haochen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Ke Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Achieving Reliable Human Assessment of Open-Domain Dialogue Systems. (arXiv:2203.05899v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05899","description":"<p>Evaluation of open-domain dialogue systems is highly challenging and\ndevelopment of better techniques is highlighted time and again as desperately\nneeded. Despite substantial efforts to carry out reliable live evaluation of\nsystems in recent competitions, annotations have been abandoned and reported as\ntoo unreliable to yield sensible results. This is a serious problem since\nautomatic metrics are not known to provide a good indication of what may or may\nnot be a high-quality conversation. Answering the distress call of competitions\nthat have emphasized the urgent need for better evaluation techniques in\ndialogue, we present the successful development of human evaluation that is\nhighly reliable while still remaining feasible and low cost. Self-replication\nexperiments reveal almost perfectly repeatable results with a correlation of\n$r=0.969$. Furthermore, due to the lack of appropriate methods of statistical\nsignificance testing, the likelihood of potential improvements to systems\noccurring due to chance is rarely taken into account in dialogue evaluation,\nand the evaluation we propose facilitates application of standard tests. Since\nwe have developed a highly reliable evaluation method, new insights into system\nperformance can be revealed. We therefore include a comparison of\nstate-of-the-art models (i) with and without personas, to measure the\ncontribution of personas to conversation quality, as well as (ii) prescribed\nversus freely chosen topics. Interestingly with respect to personas, results\nindicate that personas do not positively contribute to conversation quality as\nexpected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Tianbo Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_G/0/1/0/all/0/1\">Gareth J. F. Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1\">Chenyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Dependency Tree Into Self-attention for Sentence Representation. (arXiv:2203.05918v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05918","description":"<p>Recent progress on parse tree encoder for sentence representation learning is\nnotable. However, these works mainly encode tree structures recursively, which\nis not conducive to parallelization. On the other hand, these works rarely take\ninto account the labels of arcs in dependency trees. To address both issues, we\npropose Dependency-Transformer, which applies a relation-attention mechanism\nthat works in concert with the self-attention mechanism. This mechanism aims to\nencode the dependency and the spatial positional relations between nodes in the\ndependency tree of sentences. By a score-based method, we successfully inject\nthe syntax information without affecting Transformer's parallelizability. Our\nmodel outperforms or is comparable to the state-of-the-art methods on four\ntasks for sentence representation and has obvious advantages in computational\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Junhua Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiajun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuxuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shangbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xue Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are discrete units necessary for Spoken Language Modeling?. (arXiv:2203.05936v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05936","description":"<p>Recent work in spoken language modeling shows the possibility of learning a\nlanguage unsupervisedly from raw audio without any text labels. The approach\nrelies first on transforming the audio into a sequence of discrete units (or\npseudo-text) and then training a language model directly on such pseudo-text.\nIs such a discrete bottleneck necessary, potentially introducing irreversible\nerrors in the encoding of the speech signal, or could we learn a language model\nwithout discrete units at all? In this work, show that discretization is indeed\nessential for good results in spoken language modeling, but that can omit the\ndiscrete bottleneck if we use using discrete target features from a higher\nlevel than the input features. We also show that an end-to-end model trained\nwith discrete target like HuBERT achieves similar results as the best language\nmodel trained on pseudo-text on a set of zero-shot spoken language modeling\nmetrics from the Zero Resource Speech Challenge 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Benoit Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block-Sparse Adversarial Attack to Fool Transformer-Based Text Classifiers. (arXiv:2203.05948v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05948","description":"<p>Recently, it has been shown that, in spite of the significant performance of\ndeep neural networks in different fields, those are vulnerable to adversarial\nexamples. In this paper, we propose a gradient-based adversarial attack against\ntransformer-based text classifiers. The adversarial perturbation in our method\nis imposed to be block-sparse so that the resultant adversarial example differs\nfrom the original sentence in only a few words. Due to the discrete nature of\ntextual data, we perform gradient projection to find the minimizer of our\nproposed optimization problem. Experimental results demonstrate that, while our\nadversarial attack maintains the semantics of the sentence, it can reduce the\naccuracy of GPT-2 to less than 5% on different datasets (AG News, MNLI, and\nYelp Reviews). Furthermore, the block-sparsity constraint of the proposed\noptimization problem results in small perturbations in the adversarial example.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadrizadeh_S/0/1/0/all/0/1\">Sahar Sadrizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolamic_L/0/1/0/all/0/1\">Ljiljana Dolamic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weibo Dataset for the 2022 Russo-Ukrainian Crisis. (arXiv:2203.05967v1 [cs.SI])","link":"http://arxiv.org/abs/2203.05967","description":"<p>Online social networks such as Twitter and Weibo play an important role in\nhow people stay informed and exchange reactions. Each crisis encompasses a new\nopportunity to study the portability of models for various tasks (e.g.,\ninformation extraction, complex event understanding, misinformation detection,\netc.), due to differences in domain, entities, and event types. We present the\nRussia-Ukraine Crisis Weibo (RUW) dataset, with over 3.5M user posts and\ncomments in the first release. Our data is available at\nhttps://github.com/yrf1/RussiaUkraine_weibo_dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fung_Y/0/1/0/all/0/1\">Yi R. Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons. (arXiv:2203.06063v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06063","description":"<p>Recent studies have shown the advantages of evaluating NLG systems using\npairwise comparisons as opposed to direct assessment. Given $k$ systems, a\nnaive approach for identifying the top-ranked system would be to uniformly\nobtain pairwise comparisons from all ${k \\choose 2}$ pairs of systems. However,\nthis can be very expensive as the number of human annotations required would\ngrow quadratically with $k$. In this work, we introduce Active Evaluation, a\nframework to efficiently identify the top-ranked system by actively choosing\nsystem pairs for comparison using dueling bandit algorithms. We perform\nextensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation\ndatasets spanning 5 tasks and show that the number of human annotations can be\nreduced by 80%. To further reduce the number of human annotations, we propose\nmodel-based dueling bandit algorithms which combine automatic evaluation\nmetrics with human evaluations. Specifically, we eliminate sub-optimal systems\neven before the human annotation process and perform human evaluations only on\ntest examples where the automatic metric is highly uncertain. This reduces the\nnumber of human annotations required further by 89%. In effect, we show that\nidentifying the top-ranked system requires only a few hundred human\nannotations, which grow linearly with $k$. Lastly, we provide practical\nrecommendations and best practices to identify the top-ranked system\nefficiently. Our code has been made publicly available at\nhttps://github.com/akashkm99/duelnlg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohankumar_A/0/1/0/all/0/1\">Akash Kumar Mohankumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language. (arXiv:2203.06096v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06096","description":"<p>Signed Language Processing (SLP) concerns the automated processing of signed\nlanguages, the main means of communication of Deaf and hearing impaired\nindividuals. SLP features many different tasks, ranging from sign recognition\nto translation and production of signed speech, but has been overlooked by the\nNLP community thus far. In this paper, we bring to attention the task of\nmodelling the phonology of sign languages. We leverage existing resources to\nconstruct a large-scale dataset of American Sign Language signs annotated with\nsix different phonological properties. We then conduct an extensive empirical\nstudy to investigate whether data-driven end-to-end and feature-based\napproaches can be optimised to automatically recognise these properties. We\nfind that, despite the inherent challenges of the task, graph-based neural\nnetworks that operate over skeleton features extracted from raw videos are able\nto succeed at the task to a varying degree. Most importantly, we show that this\nperformance pertains even on signs unobserved during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavella_F/0/1/0/all/0/1\">Federico Tavella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1\">Viktor Schlegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_M/0/1/0/all/0/1\">Marta Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galata_A/0/1/0/all/0/1\">Aphrodite Galata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1\">Angelo Cangelosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval. (arXiv:2203.06169v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06169","description":"<p>In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever\nthat does not require any supervised data for training. Specifically, we first\npresent Iterative Contrastive Learning (ICoL) that iteratively trains the query\nand document encoders with a cache mechanism. ICoL not only enlarges the number\nof negative instances but also keeps representations of cached examples in the\nsame hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a\nsimple yet effective way to enhance dense retrieval with lexical matching. We\nevaluate LaPraDoR on the recently proposed BEIR benchmark, including 18\ndatasets of 9 zero-shot text retrieval tasks. Experimental results show that\nLaPraDoR achieves state-of-the-art performance compared with supervised dense\nretrieval models, and further analysis reveals the effectiveness of our\ntraining strategy and objectives. Compared to re-ranking, our lexicon-enhanced\napproach can be run in milliseconds (22.5x faster) while achieving superior\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer. (arXiv:2005.02049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.02049","description":"<p>Unsupervised style transfer aims to change the style of an input sentence\nwhile preserving its original content without using parallel training data. In\ncurrent dominant approaches, owing to the lack of fine-grained control on the\ninfluence from the target style,they are unable to yield desirable output\nsentences. In this paper, we propose a novel attentional sequence-to-sequence\n(Seq2seq) model that dynamically exploits the relevance of each output word to\nthe target style for unsupervised style transfer. Specifically, we first\npretrain a style classifier, where the relevance of each input word to the\noriginal style can be quantified via layer-wise relevance propagation. In a\ndenoising auto-encoding manner, we train an attentional Seq2seq model to\nreconstruct input sentences and repredict word-level previously-quantified\nstyle relevance simultaneously. In this way, this model is endowed with the\nability to automatically predict the style relevance of each output word. Then,\nwe equip the decoder of this model with a neural style component to exploit the\npredicted wordlevel style relevance for better style transfer. Particularly, we\nfine-tune this model using a carefully-designed objective function involving\nstyle transfer, style relevance consistency, content preservation and fluency\nmodeling loss terms. Experimental results show that our proposed model achieves\nstate-of-the-art performance in terms of both transfer accuracy and content\npreservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition. (arXiv:2104.01989v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01989","description":"<p>Many neural network speaker recognition systems model each speaker using a\nfixed-dimensional embedding vector. These embeddings are generally compared\nusing either linear or 2nd-order scoring and, until recently, do not handle\nutterance-specific uncertainty. In this work we propose scoring these\nrepresentations in a way that can capture uncertainty, enroll/test asymmetry\nand additional non-linear information. This is achieved by incorporating a\n2nd-stage neural network (known as a decision network) as part of an end-to-end\ntraining regimen. In particular, we propose the concept of decision residual\nnetworks which involves the use of a compact decision network to leverage\ncosine scores and to model the residual signal that's needed. Additionally, we\npresent a modification to the generalized end-to-end softmax loss function to\ntarget the separation of same/different speaker scores. We observed significant\nperformance gains for the two techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelecanos_J/0/1/0/all/0/1\">Jason Pelecanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1\">Ignacio Lopez Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Heterogeneous Features in Sequence to Sequence Tasks: Latent Enhanced Multi-filter Seq2Seq Model. (arXiv:2105.08840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08840","description":"<p>In language processing, training data with extremely large variance may lead\nto difficulty of language model's convergence. It is difficult for the network\nparameters to adapt sentences with largely varied semantics or grammatical\nstructures. To resolve this problem, we introduce a model that concentrates the\neach of the heterogeneous features in the input sentences. Build upon the\nencoder-decoder architecture, we design a latent-enhanced multi-filter seq2seq\nmodel (LEMS) that analyzes the input representations by introducing a latent\nspace transformation and clustering. The representations are extracted from the\nfinal hidden state of the encoder and lie in the latent space. A latent space\ntransformation is applied for enhancing the quality of the representations.\nThus the clustering algorithm can easily separate samples based on the features\nof these representations. Multiple filters are trained by the features from\ntheir corresponding clusters, the heterogeneity of the training data can be\nresolved accordingly. We conduct two sets of comparative experiments on\nsemantic parsing and machine translation, using the Geo-query dataset and\nMulti30k English-French to demonstrate the enhancement our model has made\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhaokun Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-shot Cross-lingual Transfer between Closely Related Languages by injecting Character-level Noise. (arXiv:2109.06772v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06772","description":"<p>Cross-lingual transfer between a high-resource language and its dialects or\nclosely related language varieties should be facilitated by their similarity.\nHowever, current approaches that operate in the embedding space do not take\nsurface similarity into account. This work presents a simple yet effective\nstrategy to imrove cross-lingual transfer between closely related varieties. We\npropose to augment the data of the high-resource source language with\ncharacter-level noise to make the model more robust towards spelling\nvariations. Our strategy shows consistent improvements over several languages\nand tasks: Zero-shot transfer of POS tagging and topic identification between\nlanguage varieties from the Finnic, West and North Germanic, and Western\nRomance language branches. Our work provides evidence for the usefulness of\nsimple surface-level noise in improving transfer between language varieties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aepli_N/0/1/0/all/0/1\">No&#xeb;mi Aepli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactively Providing Explanations for Transformer Language Models. (arXiv:2110.02058v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02058","description":"<p>Transformer language models are state of the art in a multitude of NLP tasks.\nDespite these successes, their opaqueness remains problematic. Recent methods\naiming to provide interpretability and explainability to black-box models\nprimarily focus on post-hoc explanations of (sometimes spurious) input-output\ncorrelations. Instead, we emphasize using prototype networks directly\nincorporated into the model architecture and hence explain the reasoning\nprocess behind the network's decisions. Our architecture performs on par with\nseveral language models and, moreover, enables learning from user interactions.\nThis not only offers a better understanding of language models but uses human\ncapabilities to incorporate knowledge outside of the rigid range of purely\ndata-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tauchmann_C/0/1/0/all/0/1\">Christopher Tauchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding. (arXiv:2110.08518v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08518","description":"<p>Multimodal pre-training with text, layout, and image has made significant\nprogress for Visually Rich Document Understanding (VRDU), especially the\nfixed-layout documents such as scanned document images. While, there are still\na large number of digital documents where the layout information is not fixed\nand needs to be interactively and dynamically rendered for visualization,\nmaking existing layout-based pre-training approaches not easy to apply. In this\npaper, we propose MarkupLM for document understanding tasks with markup\nlanguages as the backbone, such as HTML/XML-based documents, where text and\nmarkup information is jointly pre-trained. Experiment results show that the\npre-trained MarkupLM significantly outperforms the existing strong baseline\nmodels on several document understanding tasks. The pre-trained model and code\nwill be publicly available at https://aka.ms/markuplm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Choice of Knowledge Base in Automated Claim Checking. (arXiv:2111.07795v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07795","description":"<p>Automated claim checking is the task of determining the veracity of a claim\ngiven evidence found in a knowledge base of trustworthy facts. While previous\nwork has taken the knowledge base as given and optimized the claim-checking\npipeline, we take the opposite approach - taking the pipeline as given, we\nexplore the choice of knowledge base. Our first insight is that a\nclaim-checking pipeline can be transferred to a new domain of claims with\naccess to a knowledge base from the new domain. Second, we do not find a\n\"universally best\" knowledge base - higher domain overlap of a task dataset and\na knowledge base tends to produce better label accuracy. Third, combining\nmultiple knowledge bases does not tend to improve performance beyond using the\nclosest-domain knowledge base. Finally, we show that the claim-checking\npipeline's confidence score for selecting evidence can be used to assess\nwhether a knowledge base will perform well for a new set of claims, even in the\nabsence of ground-truth labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stammbach_D/0/1/0/all/0/1\">Dominik Stammbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ash_E/0/1/0/all/0/1\">Elliott Ash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking. (arXiv:2202.13404v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13404","description":"<p>Entity linking (EL) is the task of linking entity mentions in a document to\nreferent entities in a knowledge base (KB). Many previous studies focus on\nWikipedia-derived KBs. There is little work on EL over Wikidata, even though it\nis the most extensive crowdsourced KB. The scale of Wikidata can open up many\nnew real-world applications, but its massive number of entities also makes EL\nchallenging. To effectively narrow down the search space, we propose a novel\ncandidate retrieval paradigm based on entity profiling. Wikidata entities and\ntheir textual fields are first indexed into a text search engine (e.g.,\nElasticsearch). During inference, given a mention and its context, we use a\nsequence-to-sequence (seq2seq) model to generate the profile of the target\nentity, which consists of its title and description. We use the profile to\nquery the indexed search engine to retrieve candidate entities. Our approach\ncomplements the traditional approach of using a Wikipedia anchor-text\ndictionary, enabling us to further design a highly effective hybrid method for\ncandidate retrieval. Combined with a simple cross-attention reranker, our\ncomplete EL framework achieves state-of-the-art results on three Wikidata-based\ndatasets and strong performance on TACKBP-2010.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation. (arXiv:2203.02889v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02889","description":"<p>Label smoothing and vocabulary sharing are two widely used techniques in\nneural machine translation models. However, we argue that simply applying both\ntechniques can be conflicting and even leads to sub-optimal performance. When\nallocating smoothed probability, original label smoothing treats the\nsource-side words that would never appear in the target language equally to the\nreal target-side words, which could bias the translation model. To address this\nissue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the\nsoft label probability of source-side words to zero. Simple yet effective, MLS\nmanages to better integrate label smoothing with vocabulary sharing. Our\nextensive experiments show that MLS consistently yields improvement over\noriginal label smoothing on different datasets, including bilingual and\nmultilingual translation from both translation quality and model's calibration.\nOur code is released at https://github.com/PKUnlp-icler/MLS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Evaluation of Answer-Agnostic Paragraph-level Multi-Question Generation. (arXiv:2203.04464v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04464","description":"<p>We study the task of predicting a set of salient questions from a given\nparagraph without any prior knowledge of the precise answer. We make two main\ncontributions. First, we propose a new method to evaluate a set of predicted\nquestions against the set of references by using the Hungarian algorithm to\nassign predicted questions to references before scoring the assigned pairs. We\nshow that our proposed evaluation strategy has better theoretical and practical\nproperties compared to prior methods because it can properly account for the\ncoverage of references. Second, we compare different strategies to utilize a\npre-trained seq2seq model to generate and select a set of questions related to\na given paragraph. The code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahata_D/0/1/0/all/0/1\">Debanjan Mahata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Backward and Forward: Self-Knowledge Distillation with Bidirectional Decoder for Neural Machine Translation. (arXiv:2203.05248v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.05248","description":"<p>Neural Machine Translation(NMT) models are usually trained via unidirectional\ndecoder which corresponds to optimizing one-step-ahead prediction. However,\nthis kind of unidirectional decoding framework may incline to focus on local\nstructure rather than global coherence. To alleviate this problem, we propose a\nnovel method, Self-Knowledge Distillation with Bidirectional Decoder for Neural\nMachine Translation(SBD-NMT). We deploy a backward decoder which can act as an\neffective regularization method to the forward decoder. By leveraging the\nbackward decoder's information about the longer-term future, distilling\nknowledge learned in the backward decoder can encourage auto-regressive NMT\nmodels to plan ahead. Experiments show that our method is significantly better\nthan the strong Transformer baselines on multiple machine translation data\nsets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Libin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">Disheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yanjun Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05297","description":"<p>Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1\">Naoya Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yichen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1\">Elif Bozkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05321","description":"<p>We present StyleBabel, a unique open access dataset of natural language\ncaptions and free-form tags describing the artistic style of over 135K digital\nartworks, collected via a novel participatory method from experts studying at\nspecialist art and design schools. StyleBabel was collected via an iterative\nmethod, inspired by `Grounded Theory': a qualitative approach that enables\nannotation while co-evolving a shared language for fine-grained artistic style\nattribute description. We demonstrate several downstream tasks for StyleBabel,\nadapting the recent ALADIN architecture for fine-grained style similarity, to\ntrain cross-modal embeddings for: 1) free-form tag generation; 2) natural\nlanguage description of artistic style; 3) fine-grained text search of style.\nTo do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and\ncross-modal representation learning, achieving a state of the art accuracy in\nfine-grained style retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruta_D/0/1/0/all/0/1\">Dan Ruta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marri_N/0/1/0/all/0/1\">Naveen Marri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_A/0/1/0/all/0/1\">Ajinkya Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briggs_J/0/1/0/all/0/1\">Jo Briggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speed_C/0/1/0/all/0/1\">Chris Speed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faieta_B/0/1/0/all/0/1\">Baldo Faieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipkowski_A/0/1/0/all/0/1\">Alex Filipkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Artificial Intelligence Solution for Effective Treatment Planning for Glioblastoma Patients. (arXiv:2203.05563v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05563","description":"<p>Glioblastomas are the most common malignant brain tumors in adults.\nApproximately 200000 people die each year from Glioblastoma in the world.\nGlioblastoma patients have a median survival of 12 months with optimal therapy\nand about 4 months without treatment. Glioblastomas appear as heterogeneous\nnecrotic masses with irregular peripheral enhancement, surrounded by vasogenic\nedema. The current standard of care includes surgical resection, radiotherapy\nand chemotherapy, which require accurate segmentation of brain tumor\nsubregions. For effective treatment planning, it is vital to identify the\nmethylation status of the promoter of Methylguanine Methyltransferase (MGMT), a\npositive prognostic factor for chemotherapy. However, current methods for brain\ntumor segmentation are tedious, subjective and not scalable, and current\ntechniques to determine the methylation status of MGMT promoter involve\nsurgically invasive procedures, which are expensive and time consuming. Hence\nthere is a pressing need to develop automated tools to segment brain tumors and\nnon-invasive methods to predict methylation status of MGMT promoter, to\nfacilitate better treatment planning and improve survival rate. I created an\nintegrated diagnostics solution powered by Artificial Intelligence to\nautomatically segment brain tumor subregions and predict MGMT promoter\nmethylation status, using brain MRI scans. My AI solution is proven on large\ndatasets with performance exceeding current standards and field tested with\ndata from teaching files of local neuroradiologists. With my solution,\nphysicians can submit brain MRI images, and get segmentation and methylation\npredictions in minutes, and guide brain tumor patients with effective treatment\nplanning and ultimately improve survival time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goddla_V/0/1/0/all/0/1\">Vikram Goddla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDL: Hybrid Deep Learning for the Synthesis of Myocardial Velocity Maps in Digital Twins for Cardiac Analysis. (arXiv:2203.05564v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05564","description":"<p>Synthetic digital twins based on medical data accelerate the acquisition,\nlabelling and decision making procedure in digital healthcare. A core part of\ndigital healthcare twins is model-based data synthesis, which permits the\ngeneration of realistic medical signals without requiring to cope with the\nmodelling complexity of anatomical and biochemical phenomena producing them in\nreality. Unfortunately, algorithms for cardiac data synthesis have been so far\nscarcely studied in the literature. An important imaging modality in the\ncardiac examination is three-directional CINE multi-slice myocardial velocity\nmapping (3Dir MVM), which provides a quantitative assessment of cardiac motion\nin three orthogonal directions of the left ventricle. The long acquisition time\nand complex acquisition produce make it more urgent to produce synthetic\ndigital twins of this imaging modality. In this study, we propose a hybrid deep\nlearning (HDL) network, especially for synthetic 3Dir MVM data. Our algorithm\nis featured by a hybrid UNet and a Generative Adversarial Network with a\nforeground-background generation scheme. The experimental results show that\nfrom temporally down-sampled magnitude CINE images (six times), our proposed\nalgorithm can still successfully synthesise high temporal resolution 3Dir MVM\nCMR data (PSNR=42.32) with precise left ventricle segmentation (DICE=0.92).\nThese performance scores indicate that our proposed HDL algorithm can be\nimplemented in real-world digital twins for myocardial velocity mapping data\nsimulation. To the best of our knowledge, this work is the first one in the\nliterature investigating digital twins of the 3Dir MVM CMR, which has shown\ngreat potential for improving the efficiency of clinical studies via\nsynthesised cardiac data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1\">Xiaodan Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1\">David Firmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gatehouse_P/0/1/0/all/0/1\">Peter Gatehouse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiftReg: Limited Angle 2D/3D Deformable Registration. (arXiv:2203.05565v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05565","description":"<p>We propose LiftReg, a 2D/3D deformable registration approach. LiftReg is a\ndeep registration framework which is trained using sets of digitally\nreconstructed radiographs (DRR) and computed tomography (CT) image pairs. By\nusing simulated training data, LiftReg can use a high-quality CT-CT image\nsimilarity measure, which helps the network to learn a high-quality deformation\nspace. To further improve registration quality and to address the inherent\ndepth ambiguities of very limited angle acquisitions, we propose to use\nfeatures extracted from the backprojected 2D images and a statistical\ndeformation model. We test our approach on the DirLab lung registration dataset\nand show that it outperforms an existing learning-based pairwise registration\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1\">Lin Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1\">Yueh Z. Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Estepar_R/0/1/0/all/0/1\">Ra&#xfa;l San Jos&#xe9; Est&#xe9;par</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recovering medical images from CT film photos. (arXiv:2203.05567v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05567","description":"<p>While medical images such as computed tomography (CT) are stored in DICOM\nformat in hospital PACS, it is still quite routine in many countries to print a\nfilm as a transferable medium for the purposes of self-storage and secondary\nconsultation. Also, with the ubiquitousness of mobile phone cameras, it is\nquite common to take pictures of CT films, which unfortunately suffer from\ngeometric deformation and illumination variation. In this work, we study the\nproblem of recovering a CT film, which marks \\textbf{the first attempt} in the\nliterature, to the best of our knowledge. We start with building a large-scale\nhead CT film database CTFilm20K, consisting of approximately 20,000 pictures,\nusing the widely used computer graphics software Blender. We also record all\naccompanying information related to the geometric deformation (such as 3D\ncoordinate, depth, normal, and UV maps) and illumination variation (such as\nalbedo map). Then we propose a deep framework called \\textbf{F}ilm\n\\textbf{I}mage \\textbf{Re}covery \\textbf{Net}work (\\textbf{FIReNet}) to tackle\ngeometric deformation and illumination variation using the multiple maps\nextracted from the CT films to collaboratively guide the recovery process.\nFinally, we convert the dewarped images to DICOM files with our cascade model\nfor further analysis such as radiomics feature extraction. Extensive\nexperiments demonstrate the superiority of our approach over the previous\napproaches. We plan to open source the simulated images and deep models for\npromoting the research on CT film image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qiyuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Liu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unfolded Deep Kernel Estimation for Blind Image Super-resolution. (arXiv:2203.05568v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05568","description":"<p>Blind image super-resolution (BISR) aims to reconstruct a high-resolution\nimage from its low-resolution counterpart degraded by unknown blur kernel and\nnoise. Many deep neural network based methods have been proposed to tackle this\nchallenging problem without considering the image degradation model. However,\nthey largely rely on the training sets and often fail to handle images with\nunseen blur kernels during inference. Deep unfolding methods have also been\nproposed to perform BISR by utilizing the degradation model. Nonetheless, the\nexisting deep unfolding methods cannot explicitly solve the data term of the\nunfolding objective function, limiting their capability in blur kernel\nestimation. In this work, we propose a novel unfolded deep kernel estimation\n(UDKE) method, which, for the first time to our best knowledge, explicitly\nsolves the data term with high efficiency. The UDKE based BISR method can\njointly learn image and kernel priors in an end-to-end manner, and it can\neffectively exploit the information in both training data and image degradation\nmodel. Experiments on benchmark datasets and real-world data demonstrate that\nthe proposed UDKE method could well predict complex unseen non-Gaussian blur\nkernels in inference, achieving significantly better BISR performance than\nstate-of-the-art. The source code of UDKE is available at:\nhttps://github.com/natezhenghy/UDKE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hongyi Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yong_H/0/1/0/all/0/1\">Hongwei Yong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autofocusing+: Noise-Resilient Motion Correction in Magnetic Resonance Imaging. (arXiv:2203.05569v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05569","description":"<p>Image corruption by motion artifacts is an ingrained problem in Magnetic\nResonance Imaging (MRI). In this work, we propose a neural network-based\nregularization term to enhance Autofocusing, a classic optimization-based\nmethod to remove motion artifacts. The method takes the best of both worlds:\nthe optimization-based routine iteratively executes the blind demotion and deep\nlearning-based prior penalizes for unrealistic restorations and speeds up the\nconvergence. We validate the method on three models of motion trajectories,\nusing synthetic and real noisy data. The method proves resilient to noise and\nanatomic structure variation, outperforming the state-of-the-art demotion\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kuzmina_E/0/1/0/all/0/1\">Ekaterina Kuzmina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razumov_A/0/1/0/all/0/1\">Artem Razumov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rogov_O/0/1/0/all/0/1\">Oleg Y. Rogov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+White_J/0/1/0/all/0/1\">Jacob White</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Convolutional Neural Networks for Molecular Subtyping of Gliomas Using Magnetic Resonance Imaging. (arXiv:2203.05571v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05571","description":"<p>Knowledge of molecular subtypes of gliomas can provide valuable information\nfor tailored therapies. This study aimed to investigate the use of deep\nconvolutional neural networks (DCNNs) for noninvasive glioma subtyping with\nradiological imaging data according to the new taxonomy announced by the World\nHealth Organization in 2016. Methods: A DCNN model was developed for the\nprediction of the five glioma subtypes based on a hierarchical classification\nparadigm. This model used three parallel, weight-sharing, deep residual\nlearning networks to process 2.5-dimensional input of trimodal MRI data,\nincluding T1-weighted, T1-weighted with contrast enhancement, and T2-weighted\nimages. A data set comprising 1,016 real patients was collected for evaluation\nof the developed DCNN model. The predictive performance was evaluated via the\narea under the curve (AUC) from the receiver operating characteristic analysis.\nFor comparison, the performance of a radiomics-based approach was also\nevaluated. Results: The AUCs of the DCNN model for the four classification\ntasks in the hierarchical classification paradigm were 0.89, 0.89, 0.85, and\n0.66, respectively, as compared to 0.85, 0.75, 0.67, and 0.59 of the radiomics\napproach. Conclusion: The results showed that the developed DCNN model can\npredict glioma subtypes with promising performance, given sufficient,\nnon-ill-balanced training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yinyan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_T/0/1/0/all/0/1\">Tianyi Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self Pre-training with Masked Autoencoders for Medical Image Analysis. (arXiv:2203.05573v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05573","description":"<p>Masked Autoencoder (MAE) has recently been shown to be effective in\npre-training Vision Transformers (ViT) for natural image analysis. By\nperforming the pretext task of reconstructing the original image from only\npartial observations, the encoder, which is a ViT, is encouraged to aggregate\ncontextual information to infer content in masked image regions. We believe\nthat this context aggregation ability is also essential to the medical image\ndomain where each anatomical structure is functionally and mechanically\nconnected to other structures and regions. However, there is no ImageNet-scale\nmedical image dataset for pre-training. Thus, in this paper, we investigate a\nself pre-training paradigm with MAE for medical images, i.e., models are\npre-trained on the same target dataset. To validate the MAE self pre-training,\nwe consider three diverse medical image tasks including chest X-ray disease\nclassification, CT abdomen multi-organ segmentation and MRI brain tumor\nsegmentation. It turns out MAE self pre-training benefits all the tasks\nmarkedly. Specifically, the mAUC on lung disease classification is increased by\n9.4%. The average DSC on brain tumor segmentation is improved from 77.4% to\n78.9%. Most interestingly, on the small-scale multi-organ segmentation dataset\n(N=30), the average DSC improves from 78.8% to 83.5% and the HD95 is reduced by\n60%, indicating its effectiveness in limited data scenarios. The segmentation\nand classification results reveal the promising potential of MAE self\npre-training for medical image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Huidong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-the-Fly Test-time Adaptation for Medical Image Segmentation. (arXiv:2203.05574v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05574","description":"<p>One major problem in deep learning-based solutions for medical imaging is the\ndrop in performance when a model is tested on a data distribution different\nfrom the one that it is trained on. Adapting the source model to target data\ndistribution at test-time is an efficient solution for the data-shift problem.\nPrevious methods solve this by adapting the model to target distribution by\nusing techniques like entropy minimization or regularization. In these methods,\nthe models are still updated by back-propagation using an unsupervised loss on\ncomplete test data distribution. In real-world clinical settings, it makes more\nsense to adapt a model to a new test image on-the-fly and avoid model update\nduring inference due to privacy concerns and lack of computing resource at\ndeployment. To this end, we propose a new setting - On-the-Fly Adaptation which\nis zero-shot and episodic (i.e., the model is adapted to a single image at a\ntime and also does not perform any back-propagation during test-time). To\nachieve this, we propose a new framework called Adaptive UNet where each\nconvolutional block is equipped with an adaptive batch normalization layer to\nadapt the features with respect to a domain code. The domain code is generated\nusing a pre-trained encoder trained on a large corpus of medical images. During\ntest-time, the model takes in just the new test image and generates a domain\ncode to adapt the features of source model according to the test data. We\nvalidate the performance on both 2D and 3D data distribution shifts where we\nget a better performance compared to previous test-time adaptation methods.\nCode is available at https://github.com/jeya-maria-jose/On-The-Fly-Adaptation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Face Recognition from Part of a Facial Image based on Image Stitching. (arXiv:2203.05601v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05601","description":"<p>Most of the current techniques for face recognition require the presence of a\nfull face of the person to be recognized, and this situation is difficult to\nachieve in practice, the required person may appear with a part of his face,\nwhich requires prediction of the part that did not appear. Most of the current\nforecasting processes are done by what is known as image interpolation, which\ndoes not give reliable results, especially if the missing part is large. In\nthis work, we adopted the process of stitching the face by completing the\nmissing part with the flipping of the part shown in the picture, depending on\nthe fact that the human face is characterized by symmetry in most cases. To\ncreate a complete model, two facial recognition methods were used to prove the\nefficiency of the algorithm. The selected face recognition algorithms that are\napplied here are Eigenfaces and geometrical methods. Image stitching is the\nprocess during which distinctive photographic images are combined to make a\ncomplete scene or a high-resolution image. Several images are integrated to\nform a wide-angle panoramic image. The quality of the image stitching is\ndetermined by calculating the similarity among the stitched image and original\nimages and by the presence of the seam lines through the stitched images. The\nEigenfaces approach utilizes PCA calculation to reduce the feature vector\ndimensions. It provides an effective approach for discovering the\nlower-dimensional space. In addition, to enable the proposed algorithm to\nrecognize the face, it also ensures a fast and effective way of classifying\nfaces. The phase of feature extraction is followed by the classifier phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahin_O/0/1/0/all/0/1\">Osama R. Shahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayedi_R/0/1/0/all/0/1\">Rami Ayedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayan_A/0/1/0/all/0/1\">Alanazi Rayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Aziz_R/0/1/0/all/0/1\">Rasha M. Abd El-Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taloba_A/0/1/0/all/0/1\">Ahmed I. Taloba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gesture based Arabic Sign Language Recognition for Impaired People based on Convolution Neural Network. (arXiv:2203.05602v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05602","description":"<p>The Arabic Sign Language has endorsed outstanding research achievements for\nidentifying gestures and hand signs using the deep learning methodology. The\nterm \"forms of communication\" refers to the actions used by hearing-impaired\npeople to communicate. These actions are difficult for ordinary people to\ncomprehend. The recognition of Arabic Sign Language (ArSL) has become a\ndifficult study subject due to variations in Arabic Sign Language (ArSL) from\none territory to another and then within states. The Convolution Neural Network\nhas been encapsulated in the proposed system which is based on the machine\nlearning technique. For the recognition of the Arabic Sign Language, the\nwearable sensor is utilized. This approach has been used a different system\nthat could suit all Arabic gestures. This could be used by the impaired people\nof the local Arabic community. The research method has been used with\nreasonable and moderate accuracy. A deep Convolutional network is initially\ndeveloped for feature extraction from the data gathered by the sensing devices.\nThese sensors can reliably recognize the Arabic sign language's 30 hand sign\nletters. The hand movements in the dataset were captured using DG5-V hand\ngloves with wearable sensors. For categorization purposes, the CNN technique is\nused. The suggested system takes Arabic sign language hand gestures as input\nand outputs vocalized speech as output. The results were recognized by 90% of\nthe people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rwelli_R/0/1/0/all/0/1\">Rady El Rwelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahin_O/0/1/0/all/0/1\">Osama R. Shahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taloba_A/0/1/0/all/0/1\">Ahmed I. Taloba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-Based Perceptual Stimulus Encoder for Bionic Vision. (arXiv:2203.05604v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05604","description":"<p>Retinal implants have the potential to treat incurable blindness, yet the\nquality of the artificial vision they produce is still rudimentary. An\noutstanding challenge is identifying electrode activation patterns that lead to\nintelligible visual percepts (phosphenes). Here we propose a PSE based on CNN\nthat is trained in an end-to-end fashion to predict the electrode activation\npatterns required to produce a desired visual percept. We demonstrate the\neffectiveness of the encoder on MNIST using a psychophysically validated\nphosphene model tailored to individual retinal implant users. The present work\nconstitutes an essential first step towards improving the quality of the\nartificial vision provided by retinal implants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Relic_L/0/1/0/all/0/1\">Lucas Relic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyeler_M/0/1/0/all/0/1\">Michael Beyeler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"City-wide Street-to-Satellite Image Geolocalization of a Mobile Ground Agent. (arXiv:2203.05612v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05612","description":"<p>Cross-view image geolocalization provides an estimate of an agent's global\nposition by matching a local ground image to an overhead satellite image\nwithout the need for GPS. It is challenging to reliably match a ground image to\nthe correct satellite image since the images have significant viewpoint\ndifferences. Existing works have demonstrated localization in constrained\nscenarios over small areas but have not demonstrated wider-scale localization.\nOur approach, called Wide-Area Geolocalization (WAG), combines a neural network\nwith a particle filter to achieve global position estimates for agents moving\nin GPS-denied environments, scaling efficiently to city-scale regions. WAG\nintroduces a trinomial loss function for a Siamese network to robustly match\nnon-centered image pairs and thus enables the generation of a smaller satellite\nimage database by coarsely discretizing the search area. A modified particle\nfilter weighting scheme is also presented to improve localization accuracy and\nconvergence. Taken together, WAG's network training and particle filter\nweighting approach achieves city-scale position estimation accuracies on the\norder of 20 meters, a 98% reduction compared to a baseline training and\nweighting approach. Applied to a smaller-scale testing area, WAG reduces the\nfinal position estimation error by 64% compared to a state-of-the-art baseline\nfrom the literature. WAG's search space discretization additionally\nsignificantly reduces storage and processing requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Downes_L/0/1/0/all/0/1\">Lena M. Downes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Ki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_T/0/1/0/all/0/1\">Ted J. Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1\">Jonathan P. How</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PETR: Position Embedding Transformation for Multi-View 3D Object Detection. (arXiv:2203.05625v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05625","description":"<p>In this paper, we develop position embedding transformation (PETR) for\nmulti-view 3D object detection. PETR encodes the position information of 3D\ncoordinates into image features, producing the 3D position-aware features.\nObject query can perceive the 3D position-aware features and perform end-to-end\nobject detection. PETR achieves state-of-the-art performance (50.4% NDS and\n44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.\nIt can serve as a simple yet strong baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tiancai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Definition, Inexpensive, Underwater Mapping. (arXiv:2203.05640v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05640","description":"<p>In this paper we present a complete framework for Underwater SLAM utilizing a\nsingle inexpensive sensor. Over the recent years, imaging technology of action\ncameras is producing stunning results even under the challenging conditions of\nthe underwater domain. The GoPro 9 camera provides high definition video in\nsynchronization with an Inertial Measurement Unit (IMU) data stream encoded in\na single mp4 file. The visual inertial SLAM framework is augmented to adjust\nthe map after each loop closure. Data collected at an artificial wreck of the\ncoast of South Carolina and in caverns and caves in Florida demonstrate the\nrobustness of the proposed approach in a variety of conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_B/0/1/0/all/0/1\">Bharat Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xanthidis_M/0/1/0/all/0/1\">Marios Xanthidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">Sharmin Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekleitis_I/0/1/0/all/0/1\">Ioannis Rekleitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attack Analysis of Face Recognition Authentication Systems Using Fast Gradient Sign Method. (arXiv:2203.05653v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05653","description":"<p>Biometric authentication methods, representing the \"something you are\"\nscheme, are considered the most secure approach for gaining access to protected\nresources. Recent attacks using Machine Learning techniques demand a serious\nsystematic reevaluation of biometric authentication. This paper analyzes and\npresents the Fast Gradient Sign Method (FGSM) attack using face recognition for\nbiometric authentication. Machine Learning techniques have been used to train\nand test the model, which can classify and identify different people's faces\nand which will be used as a target for carrying out the attack. Furthermore,\nthe case study will analyze the implementation of the FGSM and the level of\nperformance reduction that the model will have by applying this method in\nattacking. The test results were performed with the change of parameters both\nin terms of training and attacking the model, thus showing the efficiency of\napplying the FGSM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Musa_A/0/1/0/all/0/1\">Arbena Musa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishi_K/0/1/0/all/0/1\">Kamer Vishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rexha_B/0/1/0/all/0/1\">Blerim Rexha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Density-Aware Voxels for LiDAR 3D Object Detection. (arXiv:2203.05662v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05662","description":"<p>LiDAR has become one of the primary 3D object detection sensors in autonomous\ndriving. However, LiDAR's diverging point pattern with increasing distance\nresults in a non-uniform sampled point cloud ill-suited to discretized\nvolumetric feature extraction. Current methods either rely on voxelized point\nclouds or use inefficient farthest point sampling to mitigate detrimental\neffects caused by density variation but largely ignore point density as a\nfeature and its predictable relationship with distance from the LiDAR sensor.\nOur proposed solution, Point Density-Aware Voxel network (PDV), is an\nend-to-end two stage LiDAR 3D object detection architecture that is designed to\naccount for these point density variations. PDV efficiently localizes voxel\nfeatures from the 3D sparse convolution backbone through voxel point centroids.\nThe spatially localized voxel features are then aggregated through a\ndensity-aware RoI grid pooling module using kernel density estimation (KDE) and\nself-attention with point density positional encoding. Finally, we exploit\nLiDAR's point density to distance relationship to refine our final bounding box\nconfidences. PDV outperforms all state-of-the-art methods on the Waymo Open\nDataset and achieves competitive results on the KITTI dataset. We provide a\ncode release for PDV which is available at https://github.com/TRAILab/PDV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jordan S. K. Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuai_T/0/1/0/all/0/1\">Tianshu Kuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waslander_S/0/1/0/all/0/1\">Steven L. Waslander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Overlooked Classifier in Human-Object Interaction Recognition. (arXiv:2203.05676v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05676","description":"<p>Human-Object Interaction (HOI) recognition is challenging due to two factors:\n(1) significant imbalance across classes and (2) requiring multiple labels per\nimage. This paper shows that these two challenges can be effectively addressed\nby improving the classifier with the backbone architecture untouched. Firstly,\nwe encode the semantic correlation among classes into the classification head\nby initializing the weights with language embeddings of HOIs. As a result, the\nperformance is boosted significantly, especially for the few-shot subset.\nSecondly, we propose a new loss named LSE-Sign to enhance multi-label learning\non a long-tailed dataset. Our simple yet effective method enables\ndetection-free HOI classification, outperforming the state-of-the-arts that\nrequire object detection and human pose by a clear margin. Moreover, we\ntransfer the classification model to instance-level HOI detection by connecting\nit with an off-the-shelf object detector. We achieve state-of-the-art without\nadditional fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Ying Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Pei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Lin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Labeling Representations in Uncertainty-based Semi-supervised Segmentation. (arXiv:2203.05682v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05682","description":"<p>Semi-supervised segmentation tackles the scarcity of annotations by\nleveraging unlabeled data with a small amount of labeled data. A prominent way\nto utilize the unlabeled data is by consistency training which commonly uses a\nteacher-student network, where a teacher guides a student segmentation. The\npredictions of unlabeled data are not reliable, therefore, uncertainty-aware\nmethods have been proposed to gradually learn from meaningful and reliable\npredictions. Uncertainty estimation, however, relies on multiple inferences\nfrom model predictions that need to be computed for each training step, which\nis computationally expensive. This work proposes a novel method to estimate the\npixel-level uncertainty by leveraging the labeling representation of\nsegmentation masks. On the one hand, a labeling representation is learnt to\nrepresent the available segmentation masks. The learnt labeling representation\nis used to map the prediction of the segmentation into a set of plausible\nmasks. Such a reconstructed segmentation mask aids in estimating the\npixel-level uncertainty guiding the segmentation network. The proposed method\nestimates the uncertainty with a single inference from the labeling\nrepresentation, thereby reducing the total computation. We evaluate our method\non the 3D segmentation of left atrium in MRI, and we show that our uncertainty\nestimates from our labeling representation improve the segmentation accuracy\nover state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+V_S/0/1/0/all/0/1\">Sukesh Adiga V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herve Lombaert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multimodal Guidance for Medical Image Classification. (arXiv:2203.05683v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05683","description":"<p>Medical imaging is a cornerstone of therapy and diagnosis in modern medicine.\nHowever, the choice of imaging modality for a particular theranostic task\ntypically involves trade-offs between the feasibility of using a particular\nmodality (e.g., short wait times, low cost, fast acquisition, reduced\nradiation/invasiveness) and the expected performance on a clinical task (e.g.,\ndiagnostic accuracy, efficacy of treatment planning and guidance). In this\nwork, we aim to apply the knowledge learned from the less feasible but\nbetter-performing (superior) modality to guide the utilization of the\nmore-feasible yet under-performing (inferior) modality and steer it towards\nimproved performance. We focus on the application of deep learning for\nimage-based diagnosis. We develop a light-weight guidance model that leverages\nthe latent representation learned from the superior modality, when training a\nmodel that consumes only the inferior modality. We examine the advantages of\nour method in the context of two clinical applications: multi-task skin lesion\nclassification from clinical and dermoscopic images and brain tumor\nclassification from multi-sequence magnetic resonance imaging (MRI) and\nhistopathology images. For both these scenarios we show a boost in diagnostic\nperformance of the inferior modality without requiring the superior modality.\nFurthermore, in the case of brain tumor classification, our method outperforms\nthe model trained on the superior modality while producing comparable results\nto the model that uses both modalities during inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallya_M/0/1/0/all/0/1\">Mayur Mallya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1\">Ghassan Hamarneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PC-SwinMorph: Patch Representation for Unsupervised Medical Image Registration and Segmentation. (arXiv:2203.05684v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05684","description":"<p>Medical image registration and segmentation are critical tasks for several\nclinical procedures. Manual realisation of those tasks is time-consuming and\nthe quality is highly dependent on the level of expertise of the physician. To\nmitigate that laborious task, automatic tools have been developed where the\nmajority of solutions are supervised techniques. However, in medical domain,\nthe strong assumption of having a well-representative ground truth is far from\nbeing realistic. To overcome this challenge, unsupervised techniques have been\ninvestigated. However, they are still limited in performance and they fail to\nproduce plausible results. In this work, we propose a novel unified\nunsupervised framework for image registration and segmentation that we called\nPC-SwinMorph. The core of our framework is two patch-based strategies, where we\ndemonstrate that patch representation is key for performance gain. We first\nintroduce a patch-based contrastive strategy that enforces locality conditions\nand richer feature representation. Secondly, we utilise a 3D\nwindow/shifted-window multi-head self-attention module as a patch stitching\nstrategy to eliminate artifacts from the patch splitting. We demonstrate,\nthrough a set of numerical and visual results, that our technique outperforms\ncurrent state-of-the-art unsupervised techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhening Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1\">Angelica I. Aviles-Rivero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Efficient Hyperspectral Image Processing inside Camera Pixels. (arXiv:2203.05696v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05696","description":"<p>Hyperspectral cameras generate a large amount of data due to the presence of\nhundreds of spectral bands as opposed to only three channels (red, green, and\nblue) in traditional cameras. This requires a significant amount of data\ntransmission between the hyperspectral image sensor and a processor used to\nclassify/detect/track the images, frame by frame, expending high energy and\ncausing bandwidth and security bottlenecks. To mitigate this problem, we\npropose a form of processing-in-pixel (PIP) that leverages advanced CMOS\ntechnologies to enable the pixel array to perform a wide range of complex\noperations required by the modern convolutional neural networks (CNN) for\nhyperspectral image recognition (HSI). Consequently, our PIP-optimized custom\nCNN layers effectively compress the input data, significantly reducing the\nbandwidth required to transmit the data downstream to the HSI processing unit.\nThis reduces the average energy consumption associated with pixel array of\ncameras and the CNN processing unit by 25.06x and 3.90x respectively, compared\nto existing hardware implementations. Our custom models yield average test\naccuracies within 0.56% of the baseline models for the standard HSI benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1\">Gourav Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zihan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_A/0/1/0/all/0/1\">Ajey Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Akhilesh R. Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1\">Peter A. Beerel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-based Localizability Estimation for Robust LiDAR Localization. (arXiv:2203.05698v1 [cs.RO])","link":"http://arxiv.org/abs/2203.05698","description":"<p>LiDAR-based localization and mapping is one of the core components in many\nmodern robotic systems due to the direct integration of range and geometry,\nallowing for precise motion estimation and generation of high quality maps in\nreal-time. Yet, as a consequence of insufficient environmental constraints\npresent in the scene, this dependence on geometry can result in localization\nfailure, happening in self-symmetric surroundings such as tunnels. This work\naddresses precisely this issue by proposing a neural network-based estimation\napproach for detecting (non-)localizability during robot operation. Special\nattention is given to the localizability of scan-to-scan registration, as it is\na crucial component in many LiDAR odometry estimation pipelines. In contrast to\nprevious, mostly traditional detection approaches, the proposed method enables\nearly detection of failure by estimating the localizability on raw sensor\nmeasurements without evaluating the underlying registration optimization.\nMoreover, previous approaches remain limited in their ability to generalize\nacross environments and sensor types, as heuristic-tuning of degeneracy\ndetection thresholds is required. The proposed approach avoids this problem by\nlearning from a corpus of different environments, allowing the network to\nfunction over various scenarios. Furthermore, the network is trained\nexclusively on simulated data, avoiding arduous data collection in challenging\nand degenerate, often hard-to-access, environments. The presented method is\ntested during field experiments conducted across challenging environments and\non two different sensor types without any modifications. The observed detection\nperformance is on par with state-of-the-art methods after environment-specific\nthreshold tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nubert_J/0/1/0/all/0/1\">Julian Nubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walther_E/0/1/0/all/0/1\">Etienne Walther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattak_S/0/1/0/all/0/1\">Shehryar Khattak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1\">Marco Hutter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"6-DoF Pose Estimation of Household Objects for Robotic Manipulation: An Accessible Dataset and Benchmark. (arXiv:2203.05701v1 [cs.RO])","link":"http://arxiv.org/abs/2203.05701","description":"<p>We present a new dataset for 6-DoF pose estimation of known objects, with a\nfocus on robotic manipulation research. We propose a set of toy grocery\nobjects, whose physical instantiations are readily available for purchase and\nare appropriately sized for robotic grasping and manipulation. We provide 3D\nscanned textured models of these objects, suitable for generating synthetic\ntraining data, as well as RGBD images of the objects in challenging, cluttered\nscenes exhibiting partial occlusion, extreme lighting variations, multiple\ninstances per image, and a large variety of poses. Using semi-automated\nRGBD-to-model texture correspondences, the images are annotated with ground\ntruth poses that were verified empirically to be accurate to within a few\nmillimeters. We also propose a new pose evaluation metric called {ADD-H} based\nupon the Hungarian assignment algorithm that is robust to symmetries in object\ngeometry without requiring their explicit enumeration. We share pre-trained\npose estimators for all the toy grocery objects, along with their baseline\nperformance on both validation and test sets. We offer this dataset to the\ncommunity to help connect the efforts of computer vision researchers with the\nneeds of roboticists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyree_S/0/1/0/all/0/1\">Stephen Tyree</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1\">Jonathan Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+To_T/0/1/0/all/0/1\">Thang To</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jia Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosier_T/0/1/0/all/0/1\">Terry Mosier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jeffrey Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1\">Stan Birchfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Synthesis: A Free lunch for Large-scale Palmprint Recognition Model Pretraining. (arXiv:2203.05703v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05703","description":"<p>Palmprints are private and stable information for biometric recognition. In\nthe deep learning era, the development of palmprint recognition is limited by\nthe lack of sufficient training data. In this paper, by observing that palmar\ncreases are the key information to deep-learning-based palmprint recognition,\nwe propose to synthesize training data by manipulating palmar creases.\nConcretely, we introduce an intuitive geometric model which represents palmar\ncreases with parameterized B\\'ezier curves. By randomly sampling B\\'ezier\nparameters, we can synthesize massive training samples of diverse identities,\nwhich enables us to pretrain large-scale palmprint recognition models.\nExperimental results demonstrate that such synthetically pretrained models have\na very strong generalization ability: they can be efficiently transferred to\nreal datasets, leading to significant performance improvements on palmprint\nrecognition. For example, under the open-set protocol, our method improves the\nstrong ArcFace baseline by more than 10\\% in terms of TAR@1e-6. And under the\nclosed-set protocol, our method reduces the equal error rate (EER) by an order\nof magnitude.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Bi-directional Skip Connections in Encoder-Decoder Architectures and Beyond. (arXiv:2203.05709v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05709","description":"<p>U-Net, as an encoder-decoder architecture with forward skip connections, has\nachieved promising results in various medical image analysis tasks. Many recent\napproaches have also extended U-Net with more complex building blocks, which\ntypically increase the number of network parameters considerably. Such\ncomplexity makes the inference stage highly inefficient for clinical\napplications. Towards an effective yet economic segmentation network design, in\nthis work, we propose backward skip connections that bring decoded features\nback to the encoder. Our design can be jointly adopted with forward skip\nconnections in any encoder-decoder architecture forming a recurrence structure\nwithout introducing extra parameters. With the backward skip connections, we\npropose a U-Net based network family, namely Bi-directional O-shape networks,\nwhich set new benchmarks on multiple public medical imaging segmentation\ndatasets. On the other hand, with the most plain architecture (BiO-Net),\nnetwork computations inevitably increase along with the pre-set recurrence\ntime. We have thus studied the deficiency bottleneck of such recurrent design\nand propose a novel two-phase Neural Architecture Search (NAS) algorithm,\nnamely BiX-NAS, to search for the best multi-scale bi-directional skip\nconnections. The ineffective skip connections are then discarded to reduce\ncomputational costs and speed up network inference. The finally searched\nBiX-Net yields the least network complexity and outperforms other\nstate-of-the-art counterparts by large margins. We evaluate our methods on both\n2D and 3D segmentation tasks in a total of six datasets. Extensive ablation\nstudies have also been conducted to provide a comprehensive analysis for our\nproposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05711","description":"<p>Despite recent advances of AI, story understanding remains an open and\nunder-investigated problem. We collect, preprocess, and publicly release a\nvideo-language story dataset, Synopses of Movie Narratives(SyMoN), containing\n5,193 video summaries of popular movies and TV series. SyMoN captures\nnaturalistic storytelling videos for human audience made by human creators, and\nhas higher story coverage and more frequent mental-state references than\nsimilar video-language story datasets. Differing from most existing video-text\ndatasets, SyMoN features large semantic gaps between the visual and the textual\nmodalities due to the prevalence of reporting bias and mental state\ndescriptions. We establish benchmarks on video-text retrieval and zero-shot\nalignment on movie summary videos. With SyMoN, we hope to lay the groundwork\nfor progress in multimodal story understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yidan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_Q/0/1/0/all/0/1\">Qin Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Scale Consistent Monocular Visual Odometry by Learning from the Virtual World. (arXiv:2203.05712v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05712","description":"<p>Monocular visual odometry (VO) has attracted extensive research attention by\nproviding real-time vehicle motion from cost-effective camera images. However,\nstate-of-the-art optimization-based monocular VO methods suffer from the scale\ninconsistency problem for long-term predictions. Deep learning has recently\nbeen introduced to address this issue by leveraging stereo sequences or\nground-truth motions in the training dataset. However, it comes at an\nadditional cost for data collection, and such training data may not be\navailable in all datasets. In this work, we propose VRVO, a novel framework for\nretrieving the absolute scale from virtual data that can be easily obtained\nfrom modern simulation environments, whereas in the real domain no stereo or\nground-truth data are required in either the training or inference phases.\nSpecifically, we first train a scale-aware disparity network using both\nmonocular real images and stereo virtual data. The virtual-to-real domain gap\nis bridged by using an adversarial training strategy to map images from both\ndomains into a shared feature space. The resulting scale-consistent disparities\nare then integrated with a direct VO system by constructing a virtual stereo\nobjective that ensures the scale consistency over long trajectories.\nAdditionally, to address the suboptimality issue caused by the separate\noptimization backend and the learning process, we further propose a mutual\nreinforcement pipeline that allows bidirectional information flow between\nlearning and optimization, which boosts the robustness and accuracy of each\nother. We demonstrate the effectiveness of our framework on the KITTI and\nvKITTI2 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-based Stroke Assessment for Multi-site Preclinical Evaluation of Cerebroprotectants. (arXiv:2203.05714v1 [q-bio.QM])","link":"http://arxiv.org/abs/2203.05714","description":"<p>Ischemic stroke is a leading cause of death worldwide, but there has been\nlittle success translating putative cerebroprotectants from preclinical trials\nto patients. We investigated computational image-based assessment tools for\npractical improvement of the quality, scalability, and outlook for large scale\npreclinical screening for potential therapeutic interventions. We developed,\nevaluated, and deployed a pipeline for image-based stroke outcome\nquantification for the Stroke Prelinical Assessment Network (SPAN), which is a\nmulti-site, multi-arm, multi-stage study evaluating a suite of\ncerebroprotectant interventions. Our fully automated pipeline combines\nstate-of-the-art algorithmic and data analytic approaches to assess stroke\noutcomes from multi-parameter MRI data collected longitudinally from a rodent\nmodel of middle cerebral artery occlusion (MCAO), including measures of infarct\nvolume, brain atrophy, midline shift, and data quality. We tested our approach\nwith 1,368 scans and report population level results of lesion extent and\nlongitudinal changes from injury. We validated our system by comparison with\nmanual annotations of coronal MRI slices and tissue sections from the same\nbrain, using crowdsourcing from blinded stroke experts from the network. Our\nresults demonstrate the efficacy and robustness of our image-based stroke\nassessments. The pipeline may provide a promising resource for ongoing\npreclinical studies conducted by SPAN and other networks in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Cabeen_R/0/1/0/all/0/1\">Ryan P. Cabeen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mandeville_J/0/1/0/all/0/1\">Joseph Mandeville</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hyder_F/0/1/0/all/0/1\">Fahmeed Hyder</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sanganahalli_B/0/1/0/all/0/1\">Basavaraju G. Sanganahalli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Thedens_D/0/1/0/all/0/1\">Daniel R. Thedens</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Arbab_A/0/1/0/all/0/1\">Ali Arbab</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_S/0/1/0/all/0/1\">Shuning Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bibic_A/0/1/0/all/0/1\">Adnan Bibic</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tarakci_E/0/1/0/all/0/1\">Erendiz Tarakci</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mihailovic_J/0/1/0/all/0/1\">Jelena Mihailovic</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Morais_A/0/1/0/all/0/1\">Andreia Morais</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lamb_J/0/1/0/all/0/1\">Jessica Lamb</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nagarkatti_K/0/1/0/all/0/1\">Karisma Nagarkatti</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dinitz_M/0/1/0/all/0/1\">Marcio A. Dinitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rogatko_A/0/1/0/all/0/1\">Andre Rogatko</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Toga_A/0/1/0/all/0/1\">Arthur W. Toga</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lyden_P/0/1/0/all/0/1\">Patrick Lyden</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ayata_C/0/1/0/all/0/1\">Cenk Ayata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating U-net Brain Extraction for Multi-site and Longitudinal Preclinical Stroke Imaging. (arXiv:2203.05716v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05716","description":"<p>Rodent stroke models are important for evaluating treatments and\nunderstanding the pathophysiology and behavioral changes of brain ischemia, and\nmagnetic resonance imaging (MRI) is a valuable tool for measuring outcome in\npreclinical studies. Brain extraction is an essential first step in most\nneuroimaging pipelines; however, it can be challenging in the presence of\nsevere pathology and when dataset quality is highly variable. Convolutional\nneural networks (CNNs) can improve accuracy and reduce operator time,\nfacilitating high throughput preclinical studies. As part of an ongoing\npreclinical stroke imaging study, we developed a deep-learning mouse brain\nextraction tool by using a U-net CNN. While previous studies have evaluated\nU-net architectures, we sought to evaluate their practical performance across\ndata types. We ask how performance is affected with data across: six imaging\ncenters, two time points after experimental stroke, and across four MRI\ncontrasts. We trained, validated, and tested a typical U-net model on 240\nmultimodal MRI datasets including quantitative multi-echo T2 and apparent\ndiffusivity coefficient (ADC) maps, and performed qualitative evaluation with a\nlarge preclinical stroke database (N=1,368). We describe the design and\ndevelopment of this system, and report our findings linking data\ncharacteristics to segmentation performance. We consistently found high\naccuracy and ability of the U-net architecture to generalize performance in a\nrange of 95-97% accuracy, with only modest reductions in performance based on\nlower fidelity imaging hardware and brain pathology. This work can help inform\nthe design of future preclinical rodent imaging studies and improve their\nscalability and reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tarakci_E/0/1/0/all/0/1\">Erendiz Tarakci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandeville_J/0/1/0/all/0/1\">Joseph Mandeville</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hyder_F/0/1/0/all/0/1\">Fahmeed Hyder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanganahalli_B/0/1/0/all/0/1\">Basavaraju G. Sanganahalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thedens_D/0/1/0/all/0/1\">Daniel R. Thedens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbab_A/0/1/0/all/0/1\">Ali Arbab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1\">Shuning Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bibic_A/0/1/0/all/0/1\">Adnan Bibic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mihailovic_J/0/1/0/all/0/1\">Jelena Mihailovic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morais_A/0/1/0/all/0/1\">Andreia Morais</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lamb_J/0/1/0/all/0/1\">Jessica Lamb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nagarkatti_K/0/1/0/all/0/1\">Karisma Nagarkatti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dinitz_M/0/1/0/all/0/1\">Marcio A. Dinitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rogatko_A/0/1/0/all/0/1\">Andre Rogatko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toga_A/0/1/0/all/0/1\">Arthur W. Toga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyden_P/0/1/0/all/0/1\">Patrick Lyden</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayata_C/0/1/0/all/0/1\">Cenk Ayata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cabeen_R/0/1/0/all/0/1\">Ryan P. Cabeen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information-Theoretic Odometry Learning. (arXiv:2203.05724v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05724","description":"<p>In this paper, we propose a unified information theoretic framework for\nlearning-motivated methods aimed at odometry estimation, a crucial component of\nmany robotics and vision tasks such as navigation and virtual reality where\nrelative camera poses are required in real time. We formulate this problem as\noptimizing a variational information bottleneck objective function, which\neliminates pose-irrelevant information from the latent representation. The\nproposed framework provides an elegant tool for performance evaluation and\nunderstanding in information-theoretic language. Specifically, we bound the\ngeneralization errors of the deep information bottleneck framework and the\npredictability of the latent representation. These provide not only a\nperformance guarantee but also practical guidance for model design, sample\ncollection, and sensor selection. Furthermore, the stochastic latent\nrepresentation provides a natural uncertainty measure without the needs for\nextra structures or computations. Experiments on two well-known odometry\ndatasets demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Domain Reconstruction Networks with V-Net and K-Net for fast MRI. (arXiv:2203.05725v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05725","description":"<p>Partial scan is a common approach for accelerating Magnetic Resonance Imaging\n(MRI) data acquisition. However, it is challenging to accurately reconstruct\nimages from partial scan data (i.e., incomplete k-space matrices). Most\nstate-of-the-art reconstruction methods apply U-Net (a classical\nencoder-decoder form of convolutional neural network) or cascaded U-Nets in\nimage domain and/or k-space domain. These methods have great advantages over\ntraditional methods where deep learning is not involved in. Nevertheless, these\nmethods have following problems: (1) Directly applying U-Net in k-space domain\nis not optimal for extracting features in k-space domain; (2) Classical\nimage-domain oriented U-Net is heavy-weight and hence is inefficient to be\ncascaded many times for yielding good reconstruction accuracy; (3) Classical\nimage-domain oriented U-Net does not fully make use information of encoder\nnetwork for extracting features in decoder network; and (4) Existing methods\nare ineffective in simultaneously extracting and fusing features in image\ndomain and its dual k-space domain. To tackle these problems, we propose in\nthis paper (1) an image-domain encoder-decoder sub-network called V-Net which\nis more light-weight for cascading and effective in fully utilizing features in\nthe encoder for decoding, (2) a k-space domain sub-network called K-Net which\nis more suitable for extracting hierarchical features in k-space domain, and\n(3) a dual-domain reconstruction network where V-Nets and K-Nets are parallelly\nand effectively combined and cascaded. The effectiveness of KV-Net is\ndemonstrated on the challenging fastMRI dataset where large-scale raw k-space\ntraining data are available and ground truth of test data is not released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yanwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruiqi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenchang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Surface Defect Detection of Industrial Products Based on A Small Number of Labeled Data. (arXiv:2203.05733v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05733","description":"<p>The surface defect detection method based on visual perception has been\nwidely used in industrial quality inspection. Because defect data are not easy\nto obtain and the annotation of a large number of defect data will waste a lot\nof manpower and material resources. Therefore, this paper reviews the methods\nof surface defect detection of industrial products based on a small number of\nlabeled data, and this method is divided into traditional image\nprocessing-based industrial product surface defect detection methods and deep\nlearning-based industrial product surface defect detection methods suitable for\na small number of labeled data. The traditional image processing-based\nindustrial product surface defect detection methods are divided into\nstatistical methods, spectral methods and model methods. Deep learning-based\nindustrial product surface defect detection methods suitable for a small number\nof labeled data are divided into based on data augmentation, based on transfer\nlearning, model-based fine-tuning, semi-supervised, weak supervised and\nunsupervised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qifan Jin</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a> (1 and 2) ((1) College of Computer and Artificial Intelligence, Zhengzhou University, (2) Institute of Physical Education (Main Campus), Zhengzhou University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Distinctive Margin toward Active Domain Adaptation. (arXiv:2203.05738v1 [cs.LG])","link":"http://arxiv.org/abs/2203.05738","description":"<p>Despite plenty of efforts focusing on improving the domain adaptation ability\n(DA) under unsupervised or few-shot semi-supervised settings, recently the\nsolution of active learning started to attract more attention due to its\nsuitability in transferring model in a more practical way with limited\nannotation resource on target data. Nevertheless, most active learning methods\nare not inherently designed to handle domain gap between data distribution, on\nthe other hand, some active domain adaptation methods (ADA) usually requires\ncomplicated query functions, which is vulnerable to overfitting. In this work,\nwe propose a concise but effective ADA method called\nSelect-by-Distinctive-Margin (SDM), which consists of a maximum margin loss and\na margin sampling algorithm for data selection. We provide theoretical analysis\nto show that SDM works like a Support Vector Machine, storing hard examples\naround decision boundaries and exploiting them to find informative and\ntransferable data. In addition, we propose two variants of our method, one is\ndesigned to adaptively adjust the gradient from margin loss, the other boosts\nthe selectivity of margin sampling by taking the gradient direction into\naccount. We benchmark SDM with standard active learning setting, demonstrating\nour algorithm achieves competitive results with good data scalability. Code is\navailable at https://github.com/TencentYoutuResearch/ActiveLearning-SDM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Ming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zekun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhenye Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhongyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_M/0/1/0/all/0/1\">Mingmin Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization. (arXiv:2203.05740v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05740","description":"<p>Recently, post-training quantization (PTQ) has driven much attention to\nproduce efficient neural networks without long-time retraining. Despite its low\ncost, current PTQ works tend to fail under the extremely low-bit setting. In\nthis study, we pioneeringly confirm that properly incorporating activation\nquantization into the PTQ reconstruction benefits the final accuracy. To deeply\nunderstand the inherent reason, a theoretical framework is established,\nindicating that the flatness of the optimized low-bit model on calibration and\ntest data is crucial. Based on the conclusion, a simple yet effective approach\ndubbed as QDROP is proposed, which randomly drops the quantization of\nactivations during PTQ. Extensive experiments on various tasks including\ncomputer vision (image classification, object detection) and natural language\nprocessing (text classification and question answering) prove its superiority.\nWith QDROP, the limit of PTQ is pushed to the 2-bit activation for the first\ntime and the accuracy boost can be up to 51.49%. Without bells and whistles,\nQDROP establishes a new state of the art for PTQ. Our code is available at\nhttps://github.com/wimh966/QDrop and has been integrated into MQBench\n(https://github.com/ModelTC/MQBench)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiuying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Phase-Encode Selection for Slice-Specific Fast MR Scanning Using a Transformer-Based Deep Reinforcement Learning Framework. (arXiv:2203.05756v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05756","description":"<p>Purpose: Long scan time in phase encoding for forming complete K-space\nmatrices is a critical drawback of MRI, making patients uncomfortable and\nwasting important time for diagnosing emergent diseases. This paper aims to\nreducing the scan time by actively and sequentially selecting partial phases in\na short time so that a slice can be accurately reconstructed from the resultant\nslice-specific incomplete K-space matrix. Methods: A transformer based deep\nreinforcement learning framework is proposed for actively determining a\nsequence of partial phases according to reconstruction-quality based Q-value (a\nfunction of reward), where the reward is the improvement degree of\nreconstructed image quality. The Q-value is efficiently predicted from binary\nphase-indicator vectors, incomplete K-space matrices and their corresponding\nundersampled images with a light-weight transformer so that the sequential\ninformation of phases and global relationship in images can be used. The\ninverse Fourier transform is employed for efficiently computing the\nundersampled images and hence gaining the rewards of selecting phases. Results:\nExperimental results on the fastMRI dataset with original K-space data\naccessible demonstrate the efficiency and accuracy superiorities of proposed\nmethod. Compared with the state-of-the-art reinforcement learning based method\nproposed by Pineda et al., the proposed method is roughly 150 times faster and\nachieves significant improvement in reconstruction accuracy. Conclusions: We\nhave proposed a light-weight transformer based deep reinforcement learning\nframework for generating high-quality slice-specific trajectory consisting of a\nsmall number of phases. The proposed method, called TITLE (Transformer Involved\nTrajectory LEarning), has remarkable superiority in phase-encode selection\nefficiency and image reconstruction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yanwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Ruiqi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenchang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Remote Physiological Measurement with Imperfect Data. (arXiv:2203.05759v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05759","description":"<p>The growing need for technology that supports remote healthcare is being\nacutely highlighted by an aging population and the COVID-19 pandemic. In\nhealth-related machine learning applications the ability to learn predictive\nmodels without data leaving a private device is attractive, especially when\nthese data might contain features (e.g., photographs or videos of the body)\nthat make identifying a subject trivial and/or the training data volume is\nlarge (e.g., uncompressed video). Camera-based remote physiological sensing\nfacilitates scalable and low-cost measurement, but is a prime example of a task\nthat involves analysing high bit-rate videos containing identifiable images and\nsensitive health information. Federated learning enables privacy-preserving\ndecentralized training which has several properties beneficial for camera-based\nsensing. We develop the first mobile federated learning camera-based sensing\nsystem and show that it can perform competitively with traditional\nstate-of-the-art supervised approaches. However, in the presence of corrupted\ndata (e.g., video or label noise) from a few devices the performance of weight\naveraging quickly degrades. To address this, we leverage knowledge about the\nexpected noise profile within the video to intelligently adjust how the model\nweights are averaged on the server. Our results show that this significantly\nimproves upon the robustness of models even when the signal-to-noise ratio is\nlow\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shwetak Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-informed Reinforcement Learning for Perception and Reasoning about Fluids. (arXiv:2203.05775v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05775","description":"<p>Learning and reasoning about physical phenomena is still a challenge in\nrobotics development, and computational sciences play a capital role in the\nsearch for accurate methods able to provide explanations for past events and\nrigorous forecasts of future situations. We propose a physics-informed\nreinforcement learning strategy for fluid perception and reasoning from\nobservations. As a model problem, we take the sloshing phenomena of different\nfluids contained in a glass. Starting from full-field and high-resolution\nsynthetic data for a particular fluid, we develop a method for the tracking\n(perception) and analysis (reasoning) of any previously unseen liquid whose\nfree surface is observed with a commodity camera. This approach demonstrates\nthe importance of physics and knowledge not only in data-driven (grey box)\nmodeling but also in the correction for real physics adaptation in low data\nregimes and partial observations of the dynamics. The method here presented is\nextensible to other domains such as the development of cognitive digital twins,\nable to learn from observation of phenomena for which they have not been\ntrained explicitly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moya_B/0/1/0/all/0/1\">Beatriz Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_D/0/1/0/all/0/1\">David Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinesta_F/0/1/0/all/0/1\">Francisco Chinesta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cueto_E/0/1/0/all/0/1\">Elias Cueto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-enabled Automatic Multimodal Fusion of Cone-Beam CT and Intraoral Scans for Intelligent 3D Tooth-Bone Reconstruction and Clinical Applications. (arXiv:2203.05784v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05784","description":"<p>A critical step in virtual dental treatment planning is to accurately\ndelineate all tooth-bone structures from CBCT with high fidelity and accurate\nanatomical information. Previous studies have established several methods for\nCBCT segmentation using deep learning. However, the inherent resolution\ndiscrepancy of CBCT and the loss of occlusal and dentition information largely\nlimited its clinical applicability. Here, we present a Deep Dental Multimodal\nAnalysis (DDMA) framework consisting of a CBCT segmentation model, an intraoral\nscan (IOS) segmentation model (the most accurate digital dental model), and a\nfusion model to generate 3D fused crown-root-bone structures with high fidelity\nand accurate occlusal and dentition information. Our model was trained with a\nlarge-scale dataset with 503 CBCT and 28,559 IOS meshes manually annotated by\nexperienced human experts. For CBCT segmentation, we use a five-fold cross\nvalidation test, each with 50 CBCT, and our model achieves an average Dice\ncoefficient and IoU of 93.99% and 88.68%, respectively, significantly\noutperforming the baselines. For IOS segmentations, our model achieves an mIoU\nof 93.07% and 95.70% on the maxillary and mandible on a test set of 200 IOS\nmeshes, which are 1.77% and 3.52% higher than the state-of-art method. Our DDMA\nframework takes about 20 to 25 minutes to generate the fused 3D mesh model\nfollowing the sequential processing order, compared to over 5 hours by human\nexperts. Notably, our framework has been incorporated into a software by a\nclear aligner manufacturer, and real-world clinical cases demonstrate that our\nmodel can visualize crown-root-bone structures during the entire orthodontic\ntreatment and can predict risks like dehiscence and fenestration. These\nfindings demonstrate the potential of multi-modal deep learning to improve the\nquality of digital dental models and help dentists make better clinical\ndecisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hao_J/0/1/0/all/0/1\">Jin Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_W/0/1/0/all/0/1\">Wei Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_R/0/1/0/all/0/1\">Ruizhe Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_H/0/1/0/all/0/1\">Huimin Xiong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1\">Kaiwei Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Hangzheng Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wanlu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wanghui Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haoji Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yueling Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zeyu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huikai Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_B/0/1/0/all/0/1\">Bing Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zuozhu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhihe Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Democracy Does Matter: Comprehensive Feature Mining for Co-Salient Object Detection. (arXiv:2203.05787v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05787","description":"<p>Co-salient object detection, with the target of detecting co-existed salient\nobjects among a group of images, is gaining popularity. Recent works use the\nattention mechanism or extra information to aggregate common co-salient\nfeatures, leading to incomplete even incorrect responses for target objects. In\nthis paper, we aim to mine comprehensive co-salient features with democracy and\nreduce background interference without introducing any extra information. To\nachieve this, we design a democratic prototype generation module to generate\ndemocratic response maps, covering sufficient co-salient regions and thereby\ninvolving more shared attributes of co-salient objects. Then a comprehensive\nprototype based on the response maps can be generated as a guide for final\nprediction. To suppress the noisy background information in the prototype, we\npropose a self-contrastive learning module, where both positive and negative\npairs are formed without relying on additional classification information.\nBesides, we also design a democratic feature enhancement module to further\nstrengthen the co-salient features by readjusting attention values. Extensive\nexperiments show that our model obtains better performance than previous\nstate-of-the-art methods, especially on challenging real-world cases (e.g., for\nCoCA, we obtain a gain of 2.0% for MAE, 5.4% for maximum F-measure, 2.3% for\nmaximum E-measure, and 3.7% for S-measure) under the same settings. Code will\nbe released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Siyue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jimin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Eng Gee Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLAG: Flow-based 3D Avatar Generation from Sparse Observations. (arXiv:2203.05789v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05789","description":"<p>To represent people in mixed reality applications for collaboration and\ncommunication, we need to generate realistic and faithful avatar poses.\nHowever, the signal streams that can be applied for this task from head-mounted\ndevices (HMDs) are typically limited to head pose and hand pose estimates.\nWhile these signals are valuable, they are an incomplete representation of the\nhuman body, making it challenging to generate a faithful full-body avatar. We\naddress this challenge by developing a flow-based generative model of the 3D\nhuman body from sparse observations, wherein we learn not only a conditional\ndistribution of 3D human pose, but also a probabilistic mapping from\nobservations to the latent space from which we can generate a plausible pose\nalong with uncertainty estimates for the joints. We show that our approach is\nnot only a strong predictive model, but can also act as an efficient pose prior\nin different optimization settings where a good initial latent code plays a\nmajor role.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aliakbarian_S/0/1/0/all/0/1\">Sadegh Aliakbarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cameron_P/0/1/0/all/0/1\">Pashmina Cameron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogo_F/0/1/0/all/0/1\">Federica Bogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fitzgibbon_A/0/1/0/all/0/1\">Andrew Fitzgibbon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cashman_T/0/1/0/all/0/1\">Thomas J. Cashman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision. (arXiv:2203.05796v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05796","description":"<p>Contrastive Language-Image Pretraining (CLIP) has emerged as a novel paradigm\nto learn visual models from language supervision. While researchers continue to\npush the frontier of CLIP, reproducing these works remains challenging. This is\nbecause researchers do not choose consistent training recipes and even use\ndifferent data, hampering the fair comparison between different methods. In\nthis work, we propose CLIP-benchmark, a first attempt to evaluate, analyze, and\nbenchmark CLIP and its variants. We conduct a comprehensive analysis of three\nkey factors: data, supervision, and model architecture. We find considerable\nintuitive or counter-intuitive insights: (1). Data quality has a significant\nimpact on performance. (2). Certain supervision has different effects for\nConvolutional Networks (ConvNets) and Vision Transformers (ViT). Applying more\nproper supervision can effectively improve the performance of CLIP. (3).\nCurtailing the text encoder reduces the training cost but not much affect the\nfinal performance. Moreover, we further combine DeCLIP with FILIP, bringing us\nthe strongest variant DeFILIP. The CLIP-benchmark would be released at:\nhttps://github.com/Sense-GVT/DeCLIP for future CLIP research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yufeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Lichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Feng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Convolutional Neural Network Pruning by Maximizing Filter Variety. (arXiv:2203.05807v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05807","description":"<p>Neural network pruning is a widely used strategy for reducing model storage\nand computing requirements. It allows to lower the complexity of the network by\nintroducing sparsity in the weights. Because taking advantage of sparse\nmatrices is still challenging, pruning is often performed in a structured way,\ni.e. removing entire convolution filters in the case of ConvNets, according to\na chosen pruning criteria. Common pruning criteria, such as l1-norm or\nmovement, usually do not consider the individual utility of filters, which may\nlead to: (1) the removal of filters exhibiting rare, thus important and\ndiscriminative behaviour, and (2) the retaining of filters with redundant\ninformation. In this paper, we present a technique solving those two issues,\nand which can be appended to any pruning criteria. This technique ensures that\nthe criteria of selection focuses on redundant filters, while retaining the\nrare ones, thus maximizing the variety of remaining filters. The experimental\nresults, carried out on different datasets (CIFAR-10, CIFAR-100 and\nCALTECH-101) and using different architectures (VGG-16 and ResNet-18)\ndemonstrate that it is possible to achieve similar sparsity levels while\nmaintaining a higher performance when appending our filter selection technique\nto pruning criteria. Moreover, we assess the quality of the found sparse\nsub-networks by applying the Lottery Ticket Hypothesis and find that the\naddition of our method allows to discover better performing tickets in most\ncases\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hubens_N/0/1/0/all/0/1\">Nathan Hubens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancas_M/0/1/0/all/0/1\">Matei Mancas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_B/0/1/0/all/0/1\">Bernard Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preda_M/0/1/0/all/0/1\">Marius Preda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_T/0/1/0/all/0/1\">Titus Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Font Shape-to-Impression Translation. (arXiv:2203.05808v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05808","description":"<p>Different fonts have different impressions, such as elegant, scary, and cool.\nThis paper tackles part-based shape-impression analysis based on the\nTransformer architecture, which is able to handle the correlation among local\nparts by its self-attention mechanism. This ability will reveal how\ncombinations of local parts realize a specific impression of a font. The\nversatility of Transformer allows us to realize two very different approaches\nfor the analysis, i.e., multi-label classification and translation. A\nquantitative evaluation shows that our Transformer-based approaches estimate\nthe font impressions from a set of local parts more accurately than other\napproaches. A qualitative evaluation then indicates the important local parts\nfor a specific impression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masaya Ueda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"aiWave: Volumetric Image Compression with 3-D Trained Affine Wavelet-like Transform. (arXiv:2203.05822v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05822","description":"<p>Volumetric image compression has become an urgent task to effectively\ntransmit and store images produced in biological research and clinical\npractice. At present, the most commonly used volumetric image compression\nmethods are based on wavelet transform, such as JP3D. However, JP3D employs an\nideal, separable, global, and fixed wavelet basis to convert input images from\npixel domain to frequency domain, which seriously limits its performance. In\nthis paper, we first design a 3-D trained wavelet-like transform to enable\nsignal-dependent and non-separable transform. Then, an affine wavelet basis is\nintroduced to capture the various local correlations in different regions of\nvolumetric images. Furthermore, we embed the proposed wavelet-like transform to\nan end-to-end compression framework called aiWave to enable an adaptive\ncompression scheme for various datasets. Last but not least, we introduce the\nweight sharing strategies of the affine wavelet-like transform according to the\nvolumetric data characteristics in the axial direction to reduce the amount of\nparameters. The experimental results show that: 1) when cooperating our trained\n3-D affine wavelet-like transform with a simple factorized entropy module,\naiWave performs better than JP3D and is comparable in terms of encoding and\ndecoding complexities; 2) when adding a context module to further remove signal\nredundancy, aiWave can achieve a much better performance than HEVC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xue_D/0/1/0/all/0/1\">Dongmei Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_H/0/1/0/all/0/1\">Haichuan Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WiCV 2021: The Eighth Women In Computer Vision Workshop. (arXiv:2203.05825v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05825","description":"<p>In this paper, we present the details of Women in Computer Vision Workshop -\nWiCV 2021, organized alongside the virtual CVPR 2021. It provides a voice to a\nminority (female) group in the computer vision community and focuses on\nincreasing the visibility of these researchers, both in academia and industry.\nWiCV believes that such an event can play an important role in lowering the\ngender imbalance in the field of computer vision. WiCV is organized each year\nwhere it provides a)~opportunity for collaboration between researchers from\nminority groups, b)~mentorship to female junior researchers, c)~financial\nsupport to presenters to overcome monetary burden and d)~large and diverse\nchoice of role models, who can serve as examples to younger researchers at the\nbeginning of their careers. In this paper, we present a report on the workshop\nprogram, trends over the past years, a summary of statistics regarding\npresenters, attendees, and sponsorship for the WiCV 2021 workshop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_A/0/1/0/all/0/1\">Arushi Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalavakonda_N/0/1/0/all/0/1\">Niveditha Kalavakonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karessli_N/0/1/0/all/0/1\">Nour Karessli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasarla_T/0/1/0/all/0/1\">Tejaswi Kasarla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonard_K/0/1/0/all/0/1\">Kathryn Leonard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+and_N/0/1/0/all/0/1\">Nermin Samet and</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05845","description":"<p>Streamlined qBOLD acquisitions enable experimentally straightforward\nobservations of brain oxygen metabolism. $R_2^\\prime$ maps are easily inferred;\nhowever, the Oxygen extraction fraction (OEF) and deoxygenated blood volume\n(DBV) are more ambiguously determined from the data. As such, existing\ninference methods tend to yield very noisy and underestimated OEF maps, while\noverestimating DBV.\n</p>\n<p>This work describes a novel probabilistic machine learning approach that can\ninfer plausible distributions of OEF and DBV. Initially, we create a model that\nproduces informative voxelwise prior distribution based on synthetic training\ndata. Contrary to prior work, we model the joint distribution of OEF and DBV\nthrough a scaled multivariate logit-Normal distribution, which enables the\nvalues to be constrained within a plausible range. The prior distribution model\nis used to train an efficient amortized variational Bayesian inference model.\nThis model learns to infer OEF and DBV by predicting real image data, with few\ntraining data required, using the signal equations as a forward model.\n</p>\n<p>We demonstrate that our approach enables the inference of smooth OEF and DBV\nmaps, with a physiologically plausible distribution that can be adapted through\nspecification of an informative prior distribution. Other benefits include\nmodel comparison (via the evidence lower bound) and uncertainty quantification\nfor identifying image artefacts. Results are demonstrated on a small study\ncomparing subjects undergoing hyperventilation and at rest. We illustrate that\nthe proposed approach allows measurement of gray matter differences in OEF and\nDBV and enables voxelwise comparison between conditions, where we observe\nsignificant increases in OEF and $R_2^\\prime$ during hyperventilation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Simpson_I/0/1/0/all/0/1\">Ivor J.A. Simpson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McManamon_A/0/1/0/all/0/1\">Ashley McManamon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stone_A/0/1/0/all/0/1\">Alan J. Stone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blockley_N/0/1/0/all/0/1\">Nicholas P. Blockley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colasanti_A/0/1/0/all/0/1\">Alessandro Colasanti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cercignani_M/0/1/0/all/0/1\">Mara Cercignani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Fine-grained Glomerular Lesion Recognition in Kidney Pathology. (arXiv:2203.05847v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05847","description":"<p>Recognition of glomeruli lesions is the key for diagnosis and treatment\nplanning in kidney pathology; however, the coexisting glomerular structures\nsuch as mesangial regions exacerbate the difficulties of this task. In this\npaper, we introduce a scheme to recognize fine-grained glomeruli lesions from\nwhole slide images. First, a focal instance structural similarity loss is\nproposed to drive the model to locate all types of glomeruli precisely. Then an\nUncertainty Aided Apportionment Network is designed to carry out the\nfine-grained visual classification without bounding-box annotations. This\ndouble branch-shaped structure extracts common features of the child class from\nthe parent class and produces the uncertainty factor for reconstituting the\ntraining dataset. Results of slide-wise evaluation illustrate the effectiveness\nof the entire scheme, with an 8-22% improvement of the mean Average Precision\ncompared with remarkable detection methods. The comprehensive results clearly\ndemonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nan_Y/0/1/0/all/0/1\">Yang Nan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1\">Fengyi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_P/0/1/0/all/0/1\">Peng Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guyue Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_C/0/1/0/all/0/1\">Caihong Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Silhouette and Skeleton Video Synthesis through Wi-Fi signals. (arXiv:2203.05864v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05864","description":"<p>The increasing availability of wireless access points (APs) is leading\ntowards human sensing applications based on Wi-Fi signals as support or\nalternative tools to the widespread visual sensors, where the signals enable to\naddress well-known vision-related problems such as illumination changes or\nocclusions. Indeed, using image synthesis techniques to translate radio\nfrequencies to the visible spectrum can become essential to obtain otherwise\nunavailable visual data. This domain-to-domain translation is feasible because\nboth objects and people affect electromagnetic waves, causing radio and optical\nfrequencies variations. In literature, models capable of inferring\nradio-to-visual features mappings have gained momentum in the last few years\nsince frequency changes can be observed in the radio domain through the channel\nstate information (CSI) of Wi-Fi APs, enabling signal-based feature extraction,\ne.g., amplitude. On this account, this paper presents a novel two-branch\ngenerative neural network that effectively maps radio data into visual\nfeatures, following a teacher-student design that exploits a cross-modality\nsupervision strategy. The latter conditions signal-based features in the visual\ndomain to completely replace visual data. Once trained, the proposed method\nsynthesizes human silhouette and skeleton videos using exclusively Wi-Fi\nsignals. The approach is evaluated on publicly available data, where it obtains\nremarkable results for both silhouette and skeleton videos generation,\ndemonstrating the effectiveness of the proposed cross-modality supervision\nstrategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avola_D/0/1/0/all/0/1\">Danilo Avola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascio_M/0/1/0/all/0/1\">Marco Cascio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagioli_A/0/1/0/all/0/1\">Alessio Fagioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foresti_G/0/1/0/all/0/1\">Gian Luca Foresti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Graph Learning for Disease Prediction. (arXiv:2203.05880v1 [cs.LG])","link":"http://arxiv.org/abs/2203.05880","description":"<p>Benefiting from the powerful expressive capability of graphs, graph-based\napproaches have been popularly applied to handle multi-modal medical data and\nachieved impressive performance in various biomedical applications. For disease\nprediction tasks, most existing graph-based methods tend to define the graph\nmanually based on specified modality (e.g., demographic information), and then\nintegrated other modalities to obtain the patient representation by Graph\nRepresentation Learning (GRL). However, constructing an appropriate graph in\nadvance is not a simple matter for these methods. Meanwhile, the complex\ncorrelation between modalities is ignored. These factors inevitably yield the\ninadequacy of providing sufficient information about the patient's condition\nfor a reliable diagnosis. To this end, we propose an end-to-end Multi-modal\nGraph Learning framework (MMGL) for disease prediction with multi-modality. To\neffectively exploit the rich information across multi-modality associated with\nthe disease, modality-aware representation learning is proposed to aggregate\nthe features of each modality by leveraging the correlation and complementarity\nbetween the modalities. Furthermore, instead of defining the graph manually,\nthe latent graph structure is captured through an effective way of adaptive\ngraph learning. It could be jointly optimized with the prediction model, thus\nrevealing the intrinsic connections among samples. Our model is also applicable\nto the scenario of inductive learning for those unseen data. An extensive group\nof experiments on two disease prediction tasks demonstrates that the proposed\nMMGL achieves more favorable performance. The code of MMGL is available at\n\\url{https://github.com/SsGood/MMGL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuchen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Coding for Machines with Feature-Based Rate-Distortion Optimization. (arXiv:2203.05890v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05890","description":"<p>Common state-of-the-art video codecs are optimized to deliver a low bitrate\nby providing a certain quality for the final human observer, which is achieved\nby rate-distortion optimization (RDO). But, with the steady improvement of\nneural networks solving computer vision tasks, more and more multimedia data is\nnot observed by humans anymore, but directly analyzed by neural networks. In\nthis paper, we propose a standard-compliant feature-based RDO (FRDO) that is\ndesigned to increase the coding performance, when the decoded frame is analyzed\nby a neural network in a video coding for machine scenario. To that extent, we\nreplace the pixel-based distortion metrics in conventional RDO of VTM-8.0 with\ndistortion metrics calculated in the feature space created by the first layers\nof a neural network. Throughout several tests with the segmentation network\nMask R-CNN and single images from the Cityscapes dataset, we compare the\nproposed FRDO and its hybrid version HFRDO with different distortion measures\nin the feature space against the conventional RDO. With HFRDO, up to 5.49 %\nbitrate can be saved compared to the VTM-8.0 implementation in terms of\nBj{\\o}ntegaard Delta Rate and using the weighted average precision as quality\nmetric. Additionally, allowing the encoder to vary the quantization parameter\nresults in coding gains for the proposed HFRDO of up 9.95 % compared to\nconventional VTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fischer_K/0/1/0/all/0/1\">Kristian Fischer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brand_F/0/1/0/all/0/1\">Fabian Brand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herglotz_C/0/1/0/all/0/1\">Christian Herglotz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRTAM: Dual Rank-1 Tensor Attention Module. (arXiv:2203.05893v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05893","description":"<p>Recently, attention mechanisms have been extensively investigated in computer\nvision, but few of them show excellent performance on both large and mobile\nnetworks. This paper proposes Dual Rank-1 Tensor Attention Module (DRTAM), a\nnovel residual-attention-learning-guided attention module for feed-forward\nconvolutional neural networks. Given a 3D feature tensor map, DRTAM firstly\ngenerates three 2D feature descriptors along three axes. Then, using three\ndescriptors, DRTAM sequentially infers two rank-1 tensor attention maps, the\ninitial attention map and the complement attention map, combines and multiplied\nthem to the input feature map for adaptive feature refinement(see Fig.1(c)). To\ngenerate two attention maps, DRTAM introduces rank-1 tensor attention module\n(RTAM) and residual descriptors extraction module (RDEM): RTAM divides each 2D\nfeature descriptors into several chunks, and generate three factor vectors of a\nrank-1 tensor attention map by employing strip pooling on each chunk so that\nlocal and long-range contextual information can be captured along three\ndimension respectively; RDEM generates three 2D feature descriptors of the\nresidual feature to produce the complement attention map, using three factor\nvectors of the initial attention map and three descriptors of the input\nfeature. Extensive experimental results on ImageNet-1K, MS COCO and PASCAL VOC\ndemonstrate that DRTAM achieves competitive performance on both large and\nmobile networks compare with other state-of-the-art attention modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1\">Hanxing Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperbolic Image Segmentation. (arXiv:2203.05898v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05898","description":"<p>For image segmentation, the current standard is to perform pixel-level\noptimization and inference in Euclidean output embedding spaces through linear\nhyperplanes. In this work, we show that hyperbolic manifolds provide a valuable\nalternative for image segmentation and propose a tractable formulation of\nhierarchical pixel-level classification in hyperbolic space. Hyperbolic Image\nSegmentation opens up new possibilities and practical benefits for\nsegmentation, such as uncertainty estimation and boundary information for free,\nzero-label generalization, and increased performance in low-dimensional output\nembeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+GhadimiAtigh_M/0/1/0/all/0/1\">Mina GhadimiAtigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoep_J/0/1/0/all/0/1\">Julian Schoep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acar_E/0/1/0/all/0/1\">Erman Acar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noord_N/0/1/0/all/0/1\">Nanne van Noord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs. (arXiv:2203.05908v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05908","description":"<p>We present a 3D face reconstruction system that aims at recovering the 3D\nfacial geometry of babies from uncalibrated photographs, BabyNet. Since the 3D\nfacial geometry of babies differs substantially from that of adults,\nbaby-specific facial reconstruction systems are needed. BabyNet consists of two\nstages: 1) a 3D graph convolutional autoencoder learns a latent space of the\nbaby 3D facial shape; and 2) a 2D encoder that maps photographs to the 3D\nlatent space based on representative features extracted using transfer\nlearning. In this way, using the pre-trained 3D decoder, we can recover a 3D\nface from 2D images. We evaluate BabyNet and show that 1) methods based on\nadult datasets cannot model the 3D facial geometry of babies, which proves the\nneed for a baby-specific method, and 2) BabyNet outperforms classical\nmodel-fitting methods even when a baby-specific 3D morphable model, such as\nBabyFM, is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Araceli Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porras_A/0/1/0/all/0/1\">Antonio R. Porras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linguraru_M/0/1/0/all/0/1\">Marius George Linguraru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piella_G/0/1/0/all/0/1\">Gemma Piella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukno_F/0/1/0/all/0/1\">Federico M. Sukno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizing and Understanding Patch Interactions in Vision Transformer. (arXiv:2203.05922v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05922","description":"<p>Vision Transformer (ViT) has become a leading tool in various computer vision\ntasks, owing to its unique self-attention mechanism that learns visual\nrepresentations explicitly through cross-patch information interactions.\nDespite having good success, the literature seldom explores the explainability\nof vision transformer, and there is no clear picture of how the attention\nmechanism with respect to the correlation across comprehensive patches will\nimpact the performance and what is the further potential. In this work, we\npropose a novel explainable visualization approach to analyze and interpret the\ncrucial attention interactions among patches for vision transformer.\nSpecifically, we first introduce a quantification indicator to measure the\nimpact of patch interaction and verify such quantification on attention window\ndesign and indiscriminative patches removal. Then, we exploit the effective\nresponsive field of each patch in ViT and devise a window-free transformer\narchitecture accordingly. Extensive experiments on ImageNet demonstrate that\nthe exquisitely designed quantitative method is shown able to facilitate ViT\nmodel learning, leading the top-1 accuracy by 4.28% at most. Moreover, the\nresults on downstream fine-grained recognition tasks further validate the\ngeneralization of our proposal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jie Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yalong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_B/0/1/0/all/0/1\">Bineng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TFCNet: Temporal Fully Connected Networks for Static Unbiased Temporal Reasoning. (arXiv:2203.05928v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05928","description":"<p>Temporal Reasoning is one important functionality for vision intelligence. In\ncomputer vision research community, temporal reasoning is usually studied in\nthe form of video classification, for which many state-of-the-art Neural\nNetwork structures and dataset benchmarks are proposed in recent years,\nespecially 3D CNNs and Kinetics. However, some recent works found that current\nvideo classification benchmarks contain strong biases towards static features,\nthus cannot accurately reflect the temporal modeling ability. New video\nclassification benchmarks aiming to eliminate static biases are proposed, with\nexperiments on these new benchmarks showing that the current clip-based 3D CNNs\nare outperformed by RNN structures and recent video transformers.\n</p>\n<p>In this paper, we find that 3D CNNs and their efficient depthwise variants,\nwhen video-level sampling strategy is used, are actually able to beat RNNs and\nrecent vision transformers by significant margins on static-unbiased temporal\nreasoning benchmarks. Further, we propose Temporal Fully Connected Block (TFC\nBlock), an efficient and effective component, which approximates fully\nconnected layers along temporal dimension to obtain video-level receptive\nfield, enhancing the spatiotemporal reasoning ability. With TFC blocks inserted\ninto Video-level 3D CNNs (V3D), our proposed TFCNets establish new\nstate-of-the-art results on synthetic temporal reasoning benchmark, CATER, and\nreal world static-unbiased dataset, Diving48, surpassing all previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PD-Flow: A Point Cloud Denoising Framework with Normalizing Flows. (arXiv:2203.05940v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05940","description":"<p>Point cloud denoising aims to restore clean point clouds from raw\nobservations corrupted by noise and outliers while preserving the fine-grained\ndetails. We present a novel deep learning-based denoising model, that\nincorporates normalizing flows and noise disentanglement techniques to achieve\nhigh denoising accuracy. Unlike existing works that extract features of point\nclouds for point-wise correction, we formulate the denoising process from the\nperspective of distribution learning and feature disentanglement. By\nconsidering noisy point clouds as a joint distribution of clean points and\nnoise, the denoised results can be derived from disentangling the noise\ncounterpart from latent point representation, and the mapping between Euclidean\nand latent spaces is modeled by normalizing flows. We evaluate our method on\nsynthesized 3D models and real-world datasets with various noise settings.\nQualitative and quantitative results show that our method outperforms previous\nstate-of-the-art deep learning-based approaches. %in terms of detail\npreservation and distribution uniformity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Aihua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zihui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yu-Hui Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_J/0/1/0/all/0/1\">Jun Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency-Driven Versatile Video Coding for Neural Object Detection. (arXiv:2203.05944v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05944","description":"<p>Saliency-driven image and video coding for humans has gained importance in\nthe recent past. In this paper, we propose such a saliency-driven coding\nframework for the video coding for machines task using the latest video coding\nstandard Versatile Video Coding (VVC). To determine the salient regions before\nencoding, we employ the real-time-capable object detection network You Only\nLook Once~(YOLO) in combination with a novel decision criterion. To measure the\ncoding quality for a machine, the state-of-the-art object segmentation network\nMask R-CNN was applied to the decoded frame. From extensive simulations we find\nthat, compared to the reference VVC with a constant quality, up to 29 % of\nbitrate can be saved with the same detection accuracy at the decoder side by\napplying the proposed saliency-driven framework. Besides, we compare YOLO\nagainst other, more traditional saliency detection methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_K/0/1/0/all/0/1\">Kristian Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleckenstein_F/0/1/0/all/0/1\">Felix Fleckenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herglotz_C/0/1/0/all/0/1\">Christian Herglotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Peng Cheng Object Detection Benchmark for Smart City. (arXiv:2203.05949v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05949","description":"<p>Object detection is an algorithm that recognizes and locates the objects in\nthe image and has a wide range of applications in the visual understanding of\ncomplex urban scenes. Existing object detection benchmarks mainly focus on a\nsingle specific scenario and their annotation attributes are not rich enough,\nthese make the object detection model is not generalized for the smart city\nscenes. Considering the diversity and complexity of scenes in intelligent city\ngovernance, we build a large-scale object detection benchmark for the smart\ncity. Our benchmark contains about 500K images and includes three scenarios:\nintelligent transportation, intelligent security, and drones. For the\ncomplexity of the real scene in the smart city, the diversity of weather,\nocclusion, and other complex environment diversity attributes of the images in\nthe three scenes are annotated. The characteristics of the benchmark are\nanalyzed and extensive experiments of the current state-of-the-art target\ndetection algorithm are conducted based on our benchmark to show their\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhouxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuandu Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Leyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yahong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-efficient Hybrid-supervised Learning for Medical Image Segmentation. (arXiv:2203.05956v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05956","description":"<p>Due to the lack of expertise for medical image annotation, the investigation\nof label-efficient methodology for medical image segmentation becomes a heated\ntopic. Recent progresses focus on the efficient utilization of weak annotations\ntogether with few strongly-annotated labels so as to achieve comparable\nsegmentation performance in many unprofessional scenarios. However, these\napproaches only concentrate on the supervision inconsistency between strongly-\nand weakly-annotated instances but ignore the instance inconsistency inside the\nweakly-annotated instances, which inevitably leads to performance degradation.\nTo address this problem, we propose a novel label-efficient hybrid-supervised\nframework, which considers each weakly-annotated instance individually and\nlearns its weight guided by the gradient direction of the strongly-annotated\ninstances, so that the high-quality prior in the strongly-annotated instances\nis better exploited and the weakly-annotated instances are depicted more\nprecisely. Specially, our designed dynamic instance indicator (DII) realizes\nthe above objectives, and is adapted to our dynamic co-regularization (DCR)\nframework further to alleviate the erroneous accumulation from distortions of\nweak annotations. Extensive experiments on two hybrid-supervised medical\nsegmentation datasets demonstrate that with only 10% strong labels, the\nproposed framework can leverage the weak labels efficiently and achieve\ncompetitive performance against the 100% strong-label supervised scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Junwen Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bi_Q/0/1/0/all/0/1\">Qi Bi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yanzhan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bian_C/0/1/0/all/0/1\">Cheng Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice. (arXiv:2203.05962v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05962","description":"<p>Vision Transformer (ViT) has recently demonstrated promise in computer vision\nproblems. However, unlike Convolutional Neural Networks (CNN), it is known that\nthe performance of ViT saturates quickly with depth increasing, due to the\nobserved attention collapse or patch uniformity. Despite a couple of empirical\nsolutions, a rigorous framework studying on this scalability issue remains\nelusive. In this paper, we first establish a rigorous theory framework to\nanalyze ViT features from the Fourier spectrum domain. We show that the\nself-attention mechanism inherently amounts to a low-pass filter, which\nindicates when ViT scales up its depth, excessive low-pass filtering will cause\nfeature maps to only preserve their Direct-Current (DC) component. We then\npropose two straightforward yet effective techniques to mitigate the\nundesirable low-pass limitation. The first technique, termed AttnScale,\ndecomposes a self-attention block into low-pass and high-pass components, then\nrescales and combines these two filters to produce an all-pass self-attention\nmatrix. The second technique, termed FeatScale, re-weights feature maps on\nseparate frequency bands to amplify the high-frequency signals. Both techniques\nare efficient and hyperparameter-free, while effectively overcoming relevant\nViT training artifacts such as attention collapse and patch uniformity. By\nseamlessly plugging in our techniques to multiple ViT variants, we demonstrate\nthat they consistently help ViTs benefit from deeper architectures, bringing up\nto 1.1% performance gains \"for free\" (e.g., with little parameter overhead). We\npublicly release our codes and pre-trained models at\nhttps://github.com/VITA-Group/ViT-Anti-Oversmoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Channel Convolutional Analysis Operator Learning for Dual-Energy CT Reconstruction. (arXiv:2203.05968v1 [eess.IV])","link":"http://arxiv.org/abs/2203.05968","description":"<p>Objective. Dual-energy computed tomography (DECT) has the potential to\nimprove contrast, reduce artifacts and the ability to perform material\ndecomposition in advanced imaging applications. The increased number or\nmeasurements results with a higher radiation dose and it is therefore essential\nto reduce either number of projections per energy or the source X-ray\nintensity, but this makes tomographic reconstruction more ill-posed.\n</p>\n<p>Approach. We developed the multi-channel convolutional analysis operator\nlearning (MCAOL) method to exploit common spatial features within attenuation\nimages at different energies and we propose an optimization method which\njointly reconstructs the attenuation images at low and high energies with a\nmixed norm regularization on the sparse features obtained by pre-trained\nconvolutional filters through the convolutional analysis operator learning\n(CAOL) algorithm.\n</p>\n<p>Main results. Extensive experiments with simulated and real computed\ntomography (CT) data were performed to validate the effectiveness of the\nproposed methods and we reported increased reconstruction accuracy compared to\nCAOL and iterative methods with single and joint total-variation (TV)\nregularization.\n</p>\n<p>Significance. Qualitative and quantitative results on sparse-views and\nlow-dose DECT demonstrate that the proposed MCAOL method outperforms both CAOL\napplied on each energy independently and several existing state-of-the-art\nmodel-based iterative reconstruction (MBIR) techniques, thus paving the way for\ndose reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Perelli_A/0/1/0/all/0/1\">Alessandro Perelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_S/0/1/0/all/0/1\">Suxer Alfonso Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bousse_A/0/1/0/all/0/1\">Alexandre Bousse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tasu_J/0/1/0/all/0/1\">Jean-Pierre Tasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Efthimiadis_N/0/1/0/all/0/1\">Nikolaos Efthimiadis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visvikis_D/0/1/0/all/0/1\">Dimitris Visvikis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FExGAN-Meta: Facial Expression Generation with Meta Humans. (arXiv:2203.05975v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05975","description":"<p>The subtleness of human facial expressions and a large degree of variation in\nthe level of intensity to which a human expresses them is what makes it\nchallenging to robustly classify and generate images of facial expressions.\nLack of good quality data can hinder the performance of a deep learning model.\nIn this article, we have proposed a Facial Expression Generation method for\nMeta-Humans (FExGAN-Meta) that works robustly with the images of Meta-Humans.\nWe have prepared a large dataset of facial expressions exhibited by ten\nMeta-Humans when placed in a studio environment and then we have evaluated\nFExGAN-Meta on the collected images. The results show that FExGAN-Meta robustly\ngenerates and classifies the images of Meta-Humans for the simple as well as\nthe complex facial expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_J/0/1/0/all/0/1\">J. Rafid Siddiqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PseudoProp: Robust Pseudo-Label Generation for Semi-Supervised Object Detection in Autonomous Driving Systems. (arXiv:2203.05983v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05983","description":"<p>Semi-supervised object detection methods are widely used in autonomous\ndriving systems, where only a fraction of objects are labeled. To propagate\ninformation from the labeled objects to the unlabeled ones, pseudo-labels for\nunlabeled objects must be generated. Although pseudo-labels have proven to\nimprove the performance of semi-supervised object detection significantly, the\napplications of image-based methods to video frames result in numerous miss or\nfalse detections using such generated pseudo-labels. In this paper, we propose\na new approach, PseudoProp, to generate robust pseudo-labels by leveraging\nmotion continuity in video frames. Specifically, PseudoProp uses a novel\nbidirectional pseudo-label propagation approach to compensate for misdetection.\nA feature-based fusion technique is also used to suppress inference noise.\nExtensive experiments on the large-scale Cityscapes dataset demonstrate that\nour method outperforms the state-of-the-art semi-supervised object detection\nmethods by 7.4% on mAP75.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chun-Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_J/0/1/0/all/0/1\">Jayanta Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naveen Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Class Incremental Learning from Decentralized Data. (arXiv:2203.05984v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05984","description":"<p>In this paper, we focus on a new and challenging decentralized machine\nlearning paradigm in which there are continuous inflows of data to be addressed\nand the data are stored in multiple repositories. We initiate the study of data\ndecentralized class-incremental learning (DCIL) by making the following\ncontributions. Firstly, we formulate the DCIL problem and develop the\nexperimental protocol. Secondly, we introduce a paradigm to create a basic\ndecentralized counterpart of typical (centralized) class-incremental learning\napproaches, and as a result, establish a benchmark for the DCIL study. Thirdly,\nwe further propose a Decentralized Composite knowledge Incremental Distillation\nframework (DCID) to transfer knowledge from historical models and multiple\nlocal sites to the general model continually. DCID consists of three main\ncomponents namely local class-incremental learning, collaborated knowledge\ndistillation among local models, and aggregated knowledge distillation from\nlocal models to the general one. We comprehensively investigate our DCID\nframework by using different implementations of the three components. Extensive\nexperimental results demonstrate the effectiveness of our DCID framework. The\ncodes of the baseline methods and the proposed DCIL will be released at\nhttps://github.com/zxxxxh/DCIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Songlin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaopeng Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Neural Networks for Relational Inductive Bias in Vision-based Deep Reinforcement Learning of Robot Control. (arXiv:2203.05985v1 [cs.LG])","link":"http://arxiv.org/abs/2203.05985","description":"<p>State-of-the-art reinforcement learning algorithms predominantly learn a\npolicy from either a numerical state vector or images. Both approaches\ngenerally do not take structural knowledge of the task into account, which is\nespecially prevalent in robotic applications and can benefit learning if\nexploited. This work introduces a neural network architecture that combines\nrelational inductive bias and visual feedback to learn an efficient position\ncontrol policy for robotic manipulation. We derive a graph representation that\nmodels the physical structure of the manipulator and combines the robot's\ninternal state with a low-dimensional description of the visual scene generated\nby an image encoding network. On this basis, a graph neural network trained\nwith reinforcement learning predicts joint velocities to control the robot. We\nfurther introduce an asymmetric approach of training the image encoder\nseparately from the policy using supervised learning. Experimental results\ndemonstrate that, for a 2-DoF planar robot in a geometrically simplistic 2D\nenvironment, a learned representation of the visual scene can replace access to\nthe explicit coordinates of the reaching target without compromising on the\nquality and sample efficiency of the policy. We further show the ability of the\nmodel to improve sample efficiency for a 6-DoF robot arm in a visually\nrealistic 3D environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oliva_M/0/1/0/all/0/1\">Marco Oliva</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Banik_S/0/1/0/all/0/1\">Soubarna Banik</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Josifovski_J/0/1/0/all/0/1\">Josip Josifovski</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a> (1) ((1) Technical University of Munich, Germany)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Self-Supervised Learning of Global and Object-Centric Representations. (arXiv:2203.05997v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05997","description":"<p>Self-supervision allows learning meaningful representations of natural images\nwhich usually contain one central object. How well does it transfer to\nmulti-entity scenes? We discuss key aspects of learning structured\nobject-centric representations with self-supervision and validate our insights\nthrough several experiments on the CLEVR dataset. Regarding the architecture,\nwe confirm the importance of competition for attention-based object discovery,\nwhere each image patch is exclusively attended by one object. For training, we\nshow that contrastive losses equipped with matching can be applied directly in\na latent space, avoiding pixel-based reconstruction. However, such an\noptimization objective is sensitive to false negatives (recurring objects) and\nfalse positives (matching errors). Thus, careful consideration is required\naround data augmentation and negative sample selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baldassarre_F/0/1/0/all/0/1\">Federico Baldassarre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1\">Hossein Azizpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polar Transformation Based Multiple Instance Learning Assisting Weakly Supervised Image Segmentation With Loose Bounding Box Annotations. (arXiv:2203.06000v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06000","description":"<p>This study investigates weakly supervised image segmentation using loose\nbounding box supervision. It presents a multiple instance learning strategy\nbased on polar transformation to assist image segmentation when loose bounding\nboxes are employed as supervision. In this strategy, weighted smooth maximum\napproximation is introduced to incorporate the observation that pixels closer\nto the origin of the polar transformation are more likely to belong to the\nobject in the bounding box. The proposed approach was evaluated on a public\nmedical dataset using Dice coefficient. The results demonstrate its superior\nperformance. The codes are available at\n\\url{https://github.com/wangjuan313/wsis-polartransform}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Juan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An error correction scheme for improved air-tissue boundary in real-time MRI video for speech production. (arXiv:2203.06004v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06004","description":"<p>The best performance in Air-tissue boundary (ATB) segmentation of real-time\nMagnetic Resonance Imaging (rtMRI) videos in speech production is known to be\nachieved by a 3-dimensional convolutional neural network (3D-CNN) model.\nHowever, the evaluation of this model, as well as other ATB segmentation\ntechniques reported in the literature, is done using Dynamic Time Warping (DTW)\ndistance between the entire original and predicted contours. Such an evaluation\nmeasure may not capture local errors in the predicted contour. Careful analysis\nof predicted contours reveals errors in regions like the velum part of contour1\n(ATB comprising of upper lip, hard palate, and velum) and tongue base section\nof contour2 (ATB covering jawline, lower lip, tongue base, and epiglottis),\nwhich are not captured in a global evaluation metric like DTW distance. In this\nwork, we automatically detect such errors and propose a correction scheme for\nthe same. We also propose two new evaluation metrics for ATB segmentation\nseparately in contour1 and contour2 to explicitly capture two types of errors\nin these contours. The proposed detection and correction strategies result in\nan improvement of these two evaluation metrics by 61.8% and 61.4% for contour1\nand by 67.8% and 28.4% for contour2. Traditional DTW distance, on the other\nhand, improves by 44.6% for contour1 and 4.0% for contour2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Anwesha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagali_V/0/1/0/all/0/1\">Varun Belagali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Prasanta Kumar Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of ImageNet Classes in Fr\\'echet Inception Distance. (arXiv:2203.06026v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06026","description":"<p>Fr\\'echet Inception Distance (FID) is a metric for quantifying the distance\nbetween two distributions of images. Given its status as a standard yardstick\nfor ranking models in data-driven generative modeling research, it seems\nimportant that the distance is computed from general, \"vision-related\"\nfeatures. But is it? We observe that FID is essentially a distance between sets\nof ImageNet class probabilities. We trace the reason to the fact that the\nstandard feature space, the penultimate \"pre-logit\" layer of a particular\nInception-V3 classifier network, is only one affine transform away from the\nlogits, i.e., ImageNet classes, and thus, the features are necessarily highly\nspecialized to them. This has unintuitive consequences for the metric's\nsensitivity. For example, when evaluating a model for human faces, we observe\nthat, on average, FID is actually very insensitive to the facial region, and\nthat the probabilities of classes like \"bow tie\" or \"seat belt\" play a much\nlarger role. Further, we show that FID can be significantly reduced -- without\nactually improving the quality of results -- by an attack that first generates\na slightly larger set of candidates, and then chooses a subset that happens to\nmatch the histogram of such \"fringe features\" in the real data. We then\ndemonstrate that this observation has practical relevance in case of ImageNet\npre-training of GANs, where a part of the observed FID improvement turns out\nnot to be real. Our results suggest caution against over-interpreting FID\nimprovements, and underline the need for distribution metrics that are more\nperceptually uniform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kynkaanniemi_T/0/1/0/all/0/1\">Tuomas Kynk&#xe4;&#xe4;nniemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Earth: Self-supervised contrastive pre-training for dense land cover classification. (arXiv:2203.06041v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06041","description":"<p>In training machine learning models for land cover semantic segmentation\nthere is a stark contrast between the availability of satellite imagery to be\nused as inputs and ground truth data to enable supervised learning. While\nthousands of new satellite images become freely available on a daily basis,\ngetting ground truth data is still very challenging, time consuming and costly.\nIn this paper we present Embedding Earth a self-supervised contrastive\npre-training method for leveraging the large availability of satellite imagery\nto improve performance on downstream dense land cover classification tasks.\nPerforming an extensive experimental evaluation spanning four countries and two\ncontinents we use models pre-trained with our proposed method as initialization\npoints for supervised land cover semantic segmentation and observe significant\nimprovements up to 25% absolute mIoU. In every case tested we outperform random\ninitialization, especially so when ground truth data are scarse. Through a\nseries of ablation studies we explore the qualities of the proposed approach\nand find that learnt features can generalize between disparate regions opening\nup the possibility of using the proposed pre-training scheme as a replacement\nto random initialization for Earth observation tasks. Code will be uploaded\nsoon at https://github.com/michaeltrs/DeepSatModels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarasiou_M/0/1/0/all/0/1\">Michail Tarasiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROOD-MRI: Benchmarking the robustness of deep learning segmentation models to out-of-distribution and corrupted data in MRI. (arXiv:2203.06060v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06060","description":"<p>Deep artificial neural networks (DNNs) have moved to the forefront of medical\nimage analysis due to their success in classification, segmentation, and\ndetection challenges. A principal challenge in large-scale deployment of DNNs\nin neuroimage analysis is the potential for shifts in signal-to-noise ratio,\ncontrast, resolution, and presence of artifacts from site to site due to\nvariances in scanners and acquisition protocols. DNNs are famously susceptible\nto these distribution shifts in computer vision. Currently, there are no\nbenchmarking platforms or frameworks to assess the robustness of new and\nexisting models to specific distribution shifts in MRI, and accessible\nmulti-site benchmarking datasets are still scarce or task-specific. To address\nthese limitations, we propose ROOD-MRI: a platform for benchmarking the\nRobustness of DNNs to Out-Of-Distribution (OOD) data, corruptions, and\nartifacts in MRI. The platform provides modules for generating benchmarking\ndatasets using transforms that model distribution shifts in MRI,\nimplementations of newly derived benchmarking metrics for image segmentation,\nand examples for using the methodology with new models and tasks. We apply our\nmethodology to hippocampus, ventricle, and white matter hyperintensity\nsegmentation in several large studies, providing the hippocampus dataset as a\npublicly available benchmark. By evaluating modern DNNs on these datasets, we\ndemonstrate that they are highly susceptible to distribution shifts and\ncorruptions in MRI. We show that while data augmentation strategies can\nsubstantially improve robustness to OOD data for anatomical segmentation tasks,\nmodern DNNs using augmentation still lack robustness in more challenging\nlesion-based segmentation tasks. We finally benchmark U-Nets and\ntransformer-based models, finding consistent differences in robustness to\nparticular classes of transforms across architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boone_L/0/1/0/all/0/1\">Lyndon Boone</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biparva_M/0/1/0/all/0/1\">Mahdi Biparva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Forooshani_P/0/1/0/all/0/1\">Parisa Mojiri Forooshani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramirez_J/0/1/0/all/0/1\">Joel Ramirez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masellis_M/0/1/0/all/0/1\">Mario Masellis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bartha_R/0/1/0/all/0/1\">Robert Bartha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Symons_S/0/1/0/all/0/1\">Sean Symons</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strother_S/0/1/0/all/0/1\">Stephen Strother</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Black_S/0/1/0/all/0/1\">Sandra E. Black</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heyn_C/0/1/0/all/0/1\">Chris Heyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swartz_R/0/1/0/all/0/1\">Richard H. Swartz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goubran_M/0/1/0/all/0/1\">Maged Goubran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAPE: Task-Agnostic Prior Embedding for Image Restoration. (arXiv:2203.06074v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06074","description":"<p>Learning an generalized prior for natural image restoration is an important\nyet challenging task. Early methods mostly involved handcrafted priors\nincluding normalized sparsity, L0 gradients, dark channel priors, etc.\nRecently, deep neural networks have been used to learn various image priors but\ndo not guarantee to generalize. In this paper, we propose a novel approach that\nembeds a task-agnostic prior into a transformer. Our approach, named\nTask-Agnostic Prior Embedding (TAPE), consists of three stages, namely,\ntask-agnostic pre-training, task-agnostic fine-tuning, and task-specific\nfine-tuning, where the first one embeds prior knowledge about natural images\ninto the transformer and the latter two extracts the knowledge to assist\ndownstream image restoration. Experiments on various types of degradation\nvalidate the effectiveness of TAPE. The image restoration performance in terms\nof PSNR is improved by as much as 1.45 dB and even outperforms task-specific\nalgorithms. More importantly, TAPE shows the ability of disentangling\ngeneralized image priors from degraded images, which enjoys favorable transfer\nability to unknown downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shanxin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LFW-Beautified: A Dataset of Face Images with Beautification and Augmented Reality Filters. (arXiv:2203.06082v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06082","description":"<p>Selfie images enjoy huge popularity in social media. The same platforms\ncentered around sharing this type of images offer filters to beautify them or\nincorporate augmented reality effects. Studies suggests that filtered images\nattract more views and engagement. Selfie images are also in increasing use in\nsecurity applications due to mobiles becoming data hubs for many transactions.\nAlso, video conference applications, boomed during the pandemic, include such\nfilters.\n</p>\n<p>Such filters may destroy biometric features that would allow person\nrecognition or even detection of the face itself, even if such commodity\napplications are not necessarily used to compromise facial systems. This could\nalso affect subsequent investigations like crimes in social media, where\nautomatic analysis is usually necessary given the amount of information posted\nin social sites or stored in devices or cloud repositories.\n</p>\n<p>To help in counteracting such issues, we contribute with a database of facial\nimages that includes several manipulations. It includes image enhancement\nfilters (which mostly modify contrast and lightning) and augmented reality\nfilters that incorporate items like animal noses or glasses. Additionally,\nimages with sunglasses are processed with a reconstruction network trained to\nlearn to reverse such modifications. This is because obfuscating the eye region\nhas been observed in the literature to have the highest impact on the accuracy\nof face detection or recognition.\n</p>\n<p>We start from the popular Labeled Faces in the Wild (LFW) database, to which\nwe apply different modifications, generating 8 datasets. Each dataset contains\n4,324 images of size 64 x 64, with a total of 34,592 images. The use of a\npublic and widely employed face dataset allows for replication and comparison.\n</p>\n<p>The created database is available at\nhttps://github.com/HalmstadUniversityBiometrics/LFW-Beautified\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Pontus Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skepetzis_V/0/1/0/all/0/1\">Vasilios Skepetzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1\">Kevin Hernandez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1\">Josef Bigun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language. (arXiv:2203.06096v1 [cs.CL])","link":"http://arxiv.org/abs/2203.06096","description":"<p>Signed Language Processing (SLP) concerns the automated processing of signed\nlanguages, the main means of communication of Deaf and hearing impaired\nindividuals. SLP features many different tasks, ranging from sign recognition\nto translation and production of signed speech, but has been overlooked by the\nNLP community thus far. In this paper, we bring to attention the task of\nmodelling the phonology of sign languages. We leverage existing resources to\nconstruct a large-scale dataset of American Sign Language signs annotated with\nsix different phonological properties. We then conduct an extensive empirical\nstudy to investigate whether data-driven end-to-end and feature-based\napproaches can be optimised to automatically recognise these properties. We\nfind that, despite the inherent challenges of the task, graph-based neural\nnetworks that operate over skeleton features extracted from raw videos are able\nto succeed at the task to a varying degree. Most importantly, we show that this\nperformance pertains even on signs unobserved during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavella_F/0/1/0/all/0/1\">Federico Tavella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlegel_V/0/1/0/all/0/1\">Viktor Schlegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_M/0/1/0/all/0/1\">Marta Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galata_A/0/1/0/all/0/1\">Aphrodite Galata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1\">Angelo Cangelosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REX: Reasoning-aware and Grounded Explanation. (arXiv:2203.06107v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06107","description":"<p>Effectiveness and interpretability are two essential properties for\ntrustworthy AI systems. Most recent studies in visual reasoning are dedicated\nto improving the accuracy of predicted answers, and less attention is paid to\nexplaining the rationales behind the decisions. As a result, they commonly take\nadvantage of spurious biases instead of actually reasoning on the\nvisual-textual data, and have yet developed the capability to explain their\ndecision making by considering key information from both modalities. This paper\naims to close the gap from three distinct perspectives: first, we define a new\ntype of multi-modal explanations that explain the decisions by progressively\ntraversing the reasoning process and grounding keywords in the images. We\ndevelop a functional program to sequentially execute different reasoning steps\nand construct a new dataset with 1,040,830 multi-modal explanations. Second, we\nidentify the critical need to tightly couple important components across the\nvisual and textual modalities for explaining the decisions, and propose a novel\nexplanation generation method that explicitly models the pairwise\ncorrespondence between words and regions of interest. It improves the visual\ngrounding capability by a considerable margin, resulting in enhanced\ninterpretability and reasoning performance. Finally, with our new data and\nmethod, we perform extensive analyses to study the effectiveness of our\nexplanation under different settings, including multi-task learning and\ntransfer learning. Our code and data are available at\nhttps://github.com/szzexpoi/rex.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ActiveMLP: An MLP-like Architecture with Active Token Mixer. (arXiv:2203.06108v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06108","description":"<p>This paper presents ActiveMLP, a general MLP-like backbone for computer\nvision. The three existing dominant network families, i.e., CNNs, Transformers\nand MLPs, differ from each other mainly in the ways to fuse contextual\ninformation into a given token, leaving the design of more effective\ntoken-mixing mechanisms at the core of backbone architecture development. In\nActiveMLP, we propose an innovative token-mixer, dubbed Active Token Mixer\n(ATM), to actively incorporate contextual information from other tokens in the\nglobal scope into the given one. This fundamental operator actively predicts\nwhere to capture useful contexts and learns how to fuse the captured contexts\nwith the original information of the given token at channel levels. In this\nway, the spatial range of token-mixing is expanded and the way of token-mixing\nis reformed. With this design, ActiveMLP is endowed with the merits of global\nreceptive fields and more flexible content-adaptive information fusion.\nExtensive experiments demonstrate that ActiveMLP is generally applicable and\ncomprehensively surpasses different families of SOTA vision backbones by a\nclear margin on a broad range of vision tasks, including visual recognition and\ndense prediction tasks. The code and models will be available at\nhttps://github.com/microsoft/ActiveMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-sensor large-scale dataset for multi-view 3D reconstruction. (arXiv:2203.06111v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06111","description":"<p>We present a new multi-sensor dataset for 3D surface reconstruction. It\nincludes registered RGB and depth data from sensors of different resolutions\nand modalities: smartphones, Intel RealSense, Microsoft Kinect, industrial\ncameras, and structured-light scanner. The data for each scene is obtained\nunder a large number of lighting conditions, and the scenes are selected to\nemphasize a diverse set of material properties challenging for existing\nalgorithms. In the acquisition process, we aimed to maximize high-resolution\ndepth data quality for challenging cases, to provide reliable ground truth for\nlearning algorithms. Overall, we provide over 1.4 million images of 110\ndifferent scenes acquired at 14 lighting conditions from 100 viewing\ndirections. We expect our dataset will be useful for evaluation and training of\n3D reconstruction algorithms of different types and for other related tasks.\nOur dataset and accompanying software will be available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voynov_O/0/1/0/all/0/1\">Oleg Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobrovskikh_G/0/1/0/all/0/1\">Gleb Bobrovskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpyshev_P/0/1/0/all/0/1\">Pavel Karpyshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardelean_A/0/1/0/all/0/1\">Andrei-Timotei Ardelean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozhenko_A/0/1/0/all/0/1\">Arseniy Bozhenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galochkin_S/0/1/0/all/0/1\">Saveliy Galochkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmanova_E/0/1/0/all/0/1\">Ekaterina Karmanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1\">Pavel Kopanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labutin_Rymsho_Y/0/1/0/all/0/1\">Yaroslav Labutin-Rymsho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhimov_R/0/1/0/all/0/1\">Ruslan Rakhimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safin_A/0/1/0/all/0/1\">Aleksandr Safin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serpiva_V/0/1/0/all/0/1\">Valerii Serpiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1\">Dzmitry Tsetserukou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of multiple retinal diseases in ultra-widefield fundus images using deep learning: data-driven identification of relevant regions. (arXiv:2203.06113v1 [eess.IV])","link":"http://arxiv.org/abs/2203.06113","description":"<p>Ultra-widefield (UWF) imaging is a promising modality that captures a larger\nretinal field of view compared to traditional fundus photography. Previous\nstudies showed that deep learning (DL) models are effective for detecting\nretinal disease in UWF images, but primarily considered individual diseases\nunder less-than-realistic conditions (excluding images with other diseases,\nartefacts, comorbidities, or borderline cases; and balancing healthy and\ndiseased images) and did not systematically investigate which regions of the\nUWF images are relevant for disease detection. We first improve on the state of\nthe field by proposing a DL model that can recognise multiple retinal diseases\nunder more realistic conditions. We then use global explainability methods to\nidentify which regions of the UWF images the model generally attends to. Our\nmodel performs very well, separating between healthy and diseased retinas with\nan area under the curve (AUC) of 0.9206 on an internal test set, and an AUC of\n0.9841 on a challenging, external test set. When diagnosing specific diseases,\nthe model attends to regions where we would expect those diseases to occur. We\nfurther identify the posterior pole as the most important region in a purely\ndata-driven fashion. Surprisingly, 10% of the image around the posterior pole\nis sufficient for achieving comparable performance to having the full images\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Engelmann_J/0/1/0/all/0/1\">Justin Engelmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McTrusty_A/0/1/0/all/0/1\">Alice D. McTrusty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+MacCormick_I/0/1/0/all/0/1\">Ian J. C. MacCormick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pead_E/0/1/0/all/0/1\">Emma Pead</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bernabeu_M/0/1/0/all/0/1\">Miguel O. Bernabeu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning the Shape of the Brain Connectome. (arXiv:2203.06122v1 [q-bio.NC])","link":"http://arxiv.org/abs/2203.06122","description":"<p>To statistically study the variability and differences between normal and\nabnormal brain connectomes, a mathematical model of the neural connections is\nrequired. In this paper, we represent the brain connectome as a Riemannian\nmanifold, which allows us to model neural connections as geodesics. We show for\nthe first time how one can leverage deep neural networks to estimate a\nRiemannian metric of the brain that can accommodate fiber crossings and is a\nnatural modeling tool to infer the shape of the brain from DWMRI. Our method\nachieves excellent performance in geodesic-white-matter-pathway alignment and\ntackles the long-standing issue in previous methods: the inability to recover\nthe crossing fibers with high fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Dai_H/0/1/0/all/0/1\">Haocheng Dai</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bauer_M/0/1/0/all/0/1\">Martin Bauer</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Fletcher_P/0/1/0/all/0/1\">P. Thomas Fletcher</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Joshi_S/0/1/0/all/0/1\">Sarang C. Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Consistency Loss for Training Multi-Label Classifiers from Single-Label Annotations. (arXiv:2203.06127v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06127","description":"<p>As natural images usually contain multiple objects, multi-label image\nclassification is more applicable \"in the wild\" than single-label\nclassification. However, exhaustively annotating images with every object of\ninterest is costly and time-consuming. We aim to train multi-label classifiers\nfrom single-label annotations only. We show that adding a consistency loss,\nensuring that the predictions of the network are consistent over consecutive\ntraining epochs, is a simple yet effective method to train multi-label\nclassifiers in a weakly supervised setting. We further extend this approach\nspatially, by ensuring consistency of the spatial feature maps produced over\nconsecutive training epochs, maintaining per-class running-average heatmaps for\neach training image. We show that this spatial consistency loss further\nimproves the multi-label mAP of the classifiers. In addition, we show that this\nmethod overcomes shortcomings of the \"crop\" data-augmentation by recovering\ncorrect supervision signal even when most of the single ground truth object is\ncropped out of the input image by the data augmentation. We demonstrate gains\nof the consistency and spatial consistency losses over the binary cross-entropy\nbaseline, and over competing methods, on MS-COCO and Pascal VOC. We also\ndemonstrate improved multi-label classification mAP on ImageNet-1K using the\nReaL multi-label validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verelst_T/0/1/0/all/0/1\">Thomas Verelst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubenstein_P/0/1/0/all/0/1\">Paul K. Rubenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichner_M/0/1/0/all/0/1\">Marcin Eichner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berman_M/0/1/0/all/0/1\">Maxim Berman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuromorphic Data Augmentation for Training Spiking Neural Networks. (arXiv:2203.06145v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06145","description":"<p>Developing neuromorphic intelligence on event-based datasets with spiking\nneural networks (SNNs) has recently attracted much research attention. However,\nthe limited size of event-based datasets makes SNNs prone to overfitting and\nunstable convergence. This issue remains unexplored by previous academic works.\nIn an effort to minimize this generalization gap, we propose neuromorphic data\naugmentation (NDA), a family of geometric augmentations specifically designed\nfor event-based datasets with the goal of significantly stabilizing the SNN\ntraining and reducing the generalization gap between training and test\nperformance. The proposed method is simple and compatible with existing SNN\ntraining pipelines. Using the proposed augmentation, for the first time, we\ndemonstrate the feasibility of unsupervised contrastive learning for SNNs. We\nconduct comprehensive experiments on prevailing neuromorphic vision benchmarks\nand show that NDA yields substantial improvements over previous\nstate-of-the-art results. For example, NDA-based SNN achieves accuracy gain on\nCIFAR10-DVS and N-Caltech 101 by 10.1% and 13.7%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyoungseob Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geller_T/0/1/0/all/0/1\">Tamar Geller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep AutoAugment. (arXiv:2203.06172v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06172","description":"<p>While recent automated data augmentation methods lead to state-of-the-art\nresults, their design spaces and the derived data augmentation strategies still\nincorporate strong human priors. In this work, instead of fixing a set of\nhand-picked default augmentations alongside the searched data augmentations, we\npropose a fully automated approach for data augmentation search named Deep\nAutoAugment (DeepAA). DeepAA progressively builds a multi-layer data\naugmentation pipeline from scratch by stacking augmentation layers one at a\ntime until reaching convergence. For each augmentation layer, the policy is\noptimized to maximize the cosine similarity between the gradients of the\noriginal and augmented data along the direction with low variance. Our\nexperiments show that even without default augmentations, we can learn an\naugmentation policy that achieves strong performance with that of previous\nworks. Extensive ablation studies show that the regularized gradient matching\nis an effective search method for data augmentation policies. Our code is\navailable at: https://github.com/MSU-MLSys-Lab/DeepAA .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Visual Pre-training for Motor Control. (arXiv:2203.06173v1 [cs.CV])","link":"http://arxiv.org/abs/2203.06173","description":"<p>This paper shows that self-supervised visual pre-training from real-world\nimages is effective for learning motor control tasks from pixels. We first\ntrain the visual representations by masked modeling of natural images. We then\nfreeze the visual encoder and train neural network controllers on top with\nreinforcement learning. We do not perform any task-specific fine-tuning of the\nencoder; the same visual representations are used for all motor control tasks.\nTo the best of our knowledge, this is the first self-supervised model to\nexploit real-world images at scale for motor control. To accelerate progress in\nlearning from pixels, we contribute a benchmark suite of hand-designed tasks\nvarying in movements, scenes, and robots. Without relying on labels,\nstate-estimation, or expert demonstrations, we consistently outperform\nsupervised encoders by up to 80% absolute success rate, sometimes even matching\nthe oracle state performance. We also find that in-the-wild images, e.g., from\nYouTube or Egocentric videos, lead to better visual representations for various\nmanipulation tasks than ImageNet images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tete Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radosavovic_I/0/1/0/all/0/1\">Ilija Radosavovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Black-box Video Attack with Reinforcement Learning. (arXiv:2001.03754v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.03754","description":"<p>Adversarial attacks on video recognition models have been explored recently.\nHowever, most existing works treat each video frame equally and ignore their\ntemporal interactions. To overcome this drawback, a few methods try to select\nsome key frames and then perform attacks based on them. Unfortunately, their\nselection strategy is independent of the attacking step, therefore the\nresulting performance is limited. Instead, we argue the frame selection phase\nis closely relevant with the attacking phase. The key frames should be adjusted\naccording to the attacking results. For that, we formulate the black-box video\nattacks into a Reinforcement Learning (RL) framework. Specifically, the\nenvironment in RL is set as the recognition model, and the agent in RL plays\nthe role of frame selecting. By continuously querying the recognition models\nand receiving the attacking feedback, the agent gradually adjusts its frame\nselection strategy and adversarial perturbations become smaller and smaller. We\nconduct a series of experiments with two mainstream video recognition models:\nC3D and LRCN on the public UCF-101 and HMDB-51 datasets. The results\ndemonstrate that the proposed method can significantly reduce the adversarial\nperturbations with efficient query times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huanqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data. (arXiv:2003.09572v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.09572","description":"<p>We present a novel method for monocular hand shape and pose estimation at\nunprecedented runtime performance of 100fps and at state-of-the-art accuracy.\nThis is enabled by a new learning based architecture designed such that it can\nmake use of all the sources of available hand training data: image data with\neither 2D or 3D annotations, as well as stand-alone 3D animations without\ncorresponding image data. It features a 3D hand joint detection module and an\ninverse kinematics module which regresses not only 3D joint positions but also\nmaps them to joint rotations in a single feed-forward pass. This output makes\nthe method more directly usable for applications in computer vision and\ngraphics compared to only regressing 3D joint positions. We demonstrate that\nour architectural design leads to a significant quantitative and qualitative\nimprovement over the state of the art on several challenging benchmarks. Our\nmodel is publicly available for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habibie_I/0/1/0/all/0/1\">Ikhsanul Habibie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training for Class-Incremental Semantic Segmentation. (arXiv:2012.03362v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03362","description":"<p>In class-incremental semantic segmentation, we have no access to the labeled\ndata of previous tasks. Therefore, when incrementally learning new classes,\ndeep neural networks suffer from catastrophic forgetting of previously learned\nknowledge. To address this problem, we propose to apply a self-training\napproach that leverages unlabeled data, which is used for the rehearsal of\nprevious knowledge. Specifically, we first learn a temporary model for the\ncurrent task, and then pseudo labels for the unlabeled data are computed by\nfusing information from the old model of the previous task and the current\ntemporary model. Additionally, conflict reduction is proposed to resolve the\nconflicts of pseudo labels generated from both the old and temporary models. We\nshow that maximizing self-entropy can further improve results by smoothing the\noverconfident predictions. Interestingly, in the experiments we show that the\nauxiliary data can be different from the training data and that even\ngeneral-purpose but diverse auxiliary data can lead to large performance gains.\nThe experiments demonstrate state-of-the-art results: obtaining a relative gain\nof up to 114% on Pascal-VOC 2012 and 8.5% on the more challenging ADE20K\ncompared to previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Normalization and Restitution for Domain Generalization and Adaptation. (arXiv:2101.00588v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.00588","description":"<p>For many practical computer vision applications, the learned models usually\nhave high performance on the datasets used for training but suffer from\nsignificant performance degradation when deployed in new environments, where\nthere are usually style differences between the training images and the testing\nimages. An effective domain generalizable model is expected to be able to learn\nfeature representations that are both generalizable and discriminative. In this\npaper, we design a novel Style Normalization and Restitution module (SNR) to\nsimultaneously ensure both high generalization and discrimination capability of\nthe networks. In the SNR module, particularly, we filter out the style\nvariations (e.g, illumination, color contrast) by performing Instance\nNormalization (IN) to obtain style normalized features, where the discrepancy\namong different samples and domains is reduced. However, such a process is\ntask-ignorant and inevitably removes some task-relevant discriminative\ninformation, which could hurt the performance. To remedy this, we propose to\ndistill task-relevant discriminative features from the residual (i.e, the\ndifference between the original feature and the style normalized feature) and\nadd them back to the network to ensure high discrimination. Moreover, for\nbetter disentanglement, we enforce a dual causality loss constraint in the\nrestitution step to encourage the better separation of task-relevant and\ntask-irrelevant features. We validate the effectiveness of our SNR on different\ncomputer vision tasks, including classification, semantic segmentation, and\nobject detection. Experiments demonstrate that our SNR module is capable of\nimproving the performance of networks for domain generalization (DG) and\nunsupervised domain adaptation (UDA) on many tasks. Code are available at\nhttps://github.com/microsoft/SNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Action Quality Assessment using Weighted Aggregation. (arXiv:2102.10555v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10555","description":"<p>Action quality assessment (AQA) aims at automatically judging human action\nbased on a video of the said action and assigning a performance score to it.\nThe majority of works in the existing literature on AQA divide RGB videos into\nshort clips, transform these clips to higher-level representations using\nConvolutional 3D (C3D) networks, and aggregate them through averaging. These\nhigher-level representations are used to perform AQA. We find that the current\nclip level feature aggregation technique of averaging is insufficient to\ncapture the relative importance of clip level features. In this work, we\npropose a learning-based weighted-averaging technique. Using this technique,\nbetter performance can be obtained without sacrificing too much computational\nresources. We call this technique Weight-Decider(WD). We also experiment with\nResNets for learning better representations for action quality assessment. We\nassess the effects of the depth and input clip size of the convolutional neural\nnetwork on the quality of action score predictions. We achieve a new\nstate-of-the-art Spearman's rank correlation of 0.9315 (an increase of 0.45%)\non the MTL-AQA dataset using a 34 layer (2+1)D ResNet with the capability of\nprocessing 32 frame clips, with WD aggregation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farabi_S/0/1/0/all/0/1\">Shafkat Farabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Himel_H/0/1/0/all/0/1\">Hasibul Himel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gazzali_F/0/1/0/all/0/1\">Fakhruddin Gazzali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Bakhtiar Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Md. Hasanul Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farazi_M/0/1/0/all/0/1\">Moshiur Farazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-Aware Detection of Temporal Metadata Manipulation. (arXiv:2103.04736v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04736","description":"<p>Most pictures shared online are accompanied by temporal metadata (i.e., the\nday and time they were taken), which makes it possible to associate an image\ncontent with real-world events. Maliciously manipulating this metadata can\nconvey a distorted version of reality. In this work, we present the emerging\nproblem of detecting timestamp manipulation. We propose an end-to-end approach\nto verify whether the purported time of capture of an outdoor image is\nconsistent with its content and geographic location. We consider manipulations\ndone in the hour and/or month of capture of a photograph. The central idea is\nthe use of supervised consistency verification, in which we predict the\nprobability that the image content, capture time, and geographical location are\nconsistent. We also include a pair of auxiliary tasks, which can be used to\nexplain the network decision. Our approach improves upon previous work on a\nlarge benchmark dataset, increasing the classification accuracy from 59.0% to\n81.1%. We perform an ablation study that highlights the importance of various\ncomponents of the method, showing what types of tampering are detectable using\nour approach. Finally, we demonstrate how the proposed method can be employed\nto estimate a possible time-of-capture in scenarios in which the timestamp is\nmissing from the metadata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padilha_R/0/1/0/all/0/1\">Rafael Padilha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salem_T/0/1/0/all/0/1\">Tawfiq Salem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Workman_S/0/1/0/all/0/1\">Scott Workman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andalo_F/0/1/0/all/0/1\">Fernanda A. Andal&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfinityGAN: Towards Infinite-Pixel Image Synthesis. (arXiv:2104.03963v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03963","description":"<p>We present a novel framework, InfinityGAN, for arbitrary-sized image\ngeneration. The task is associated with several key challenges. First, scaling\nexisting models to an arbitrarily large image size is resource-constrained, in\nterms of both computation and availability of large-field-of-view training\ndata. InfinityGAN trains and infers in a seamless patch-by-patch manner with\nlow computational resources. Second, large images should be locally and\nglobally consistent, avoid repetitive patterns, and look realistic. To address\nthese, InfinityGAN disentangles global appearances, local structures, and\ntextures. With this formulation, we can generate images with spatial size and\nlevel of details not attainable before. Experimental evaluation validates that\nInfinityGAN generates images with superior realism compared to baselines and\nfeatures parallelizable inference. Finally, we show several applications\nunlocked by our approach, such as spatial style fusion, multi-modal\noutpainting, and image inbetweening. All applications can be operated with\narbitrary input and output sizes. Please find the full version of the paper at\nhttps://openreview.net/forum?id=ufGMqIM0a4b .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chieh Hubert Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yen-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FLEX: Extrinsic Parameter-free Multi-view 3D Human Motion Reconstruction. (arXiv:2105.01937v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.01937","description":"<p>The increasing availability of video recordings made by multiple cameras has\noffered new means for mitigating occlusion and depth ambiguities in pose and\nmotion reconstruction methods. Yet, multi-view algorithms strongly depend on\ncamera parameters; particularly, the relative transformations between the\ncameras. Such a dependency becomes a hurdle once shifting to dynamic capture in\nuncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an\nend-to-end extrinsic parameter-free multi-view model. FLEX is extrinsic\nparameter-free (dubbed ep-free) in the sense that it does not require extrinsic\ncamera parameters. Our key idea is that the 3D angles between skeletal parts,\nas well as bone lengths, are invariant to the camera position. Hence, learning\n3D rotations and bone lengths rather than locations allows predicting common\nvalues for all camera views. Our network takes multiple video streams, learns\nfused deep features through a novel multi-view fusion layer, and reconstructs a\nsingle consistent skeleton with temporally coherent joint rotations. We\ndemonstrate quantitative and qualitative results on three public datasets, and\non synthetic multi-person video streams captured by dynamic cameras. We compare\nour model to state-of-the-art methods that are not ep-free and show that in the\nabsence of camera parameters, we outperform them by a large margin while\nobtaining comparable results when camera parameters are available. Code,\ntrained models, and other materials are available on our project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gordon_B/0/1/0/all/0/1\">Brian Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raab_S/0/1/0/all/0/1\">Sigal Raab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azov_G/0/1/0/all/0/1\">Guy Azov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RLCorrector: Reinforced Proofreading for Cell-level Microscopy Image Segmentation. (arXiv:2106.05487v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05487","description":"<p>Segmentation of nanoscale electron microscopy (EM) images is crucial but\nstill challenging in connectomics research. One reason for this is that none of\nthe existing segmentation methods are error-free, so they require proofreading,\nwhich is typically implemented as an interactive, semi-automatic process via\nmanual intervention. Herein, we propose a fully automatic proofreading method\nbased on reinforcement learning that mimics the human decision process of\ndetection, classification, and correction of segmentation errors. We\nsystematically design the proposed system by combining multiple reinforcement\nlearning agents in a hierarchical manner, where each agent focuses only on a\nspecific task while preserving dependency between agents. Furthermore, we\ndemonstrate that the episodic task setting of reinforcement learning can\nefficiently manage a combination of merge and split errors concurrently\npresented in the input. We demonstrate the efficacy of the proposed system by\ncomparing it with conventional proofreading methods over various testing cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoa Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_G/0/1/0/all/0/1\">Ganghee Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_T/0/1/0/all/0/1\">Tran Anh Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1\">Won-ki Jeong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To fit or not to fit: Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision. (arXiv:2106.09614v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09614","description":"<p>3D face reconstruction under occlusions is highly challenging due to the\nlarge variability of occluders. Currently, the most successful methods fit a 3D\nface model through inverse rendering and assume a given segmentation of the\noccluder to avoid fitting the occluder. However, training an occlusion\nsegmentation model requires large amounts of annotated data. In this work, we\nintroduce a model-based approach for 3D face reconstruction that is highly\nrobust to occlusions but does not require any occlusion annotations for\ntraining. In our approach, we exploit the fact that generative face models can\nonly synthesize human faces, but not the occluders. We use this property to\nguide the decision-making process of an occlusion segmentation network and\nresulting in unsupervised training. The main challenge is that the model\nfitting and the occlusion segmentation are mutually dependent on each other,\nand need to be inferred jointly. We resolve this chicken-and-egg problem with\nan EM-type training strategy. This leads to a synergistic effect, in which the\nsegmentation network prevents the face encoder from fitting to the occlusion,\nenhancing the reconstruction quality. The improved 3D face reconstruction, in\nturn, enables the segmentation network to better predict the occlusion.\nQualitative and quantitative experiments on the CelebA-HQ, the AR databases,\nand the NoW challenge demonstrate that the proposed pipeline achieves the\nstate-of-the-art 3D face reconstruction under occlusion. Moreover, the\nsegmentation network localizes occlusions accurately despite being trained\nwithout any occlusion annotation. The code is available at\nhttps://github.com/unibas-gravis/Occlusion-Robust-MoFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunlu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morel_Forster_A/0/1/0/all/0/1\">Andreas Morel-Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetter_T/0/1/0/all/0/1\">Thomas Vetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo-LiDAR Based Road Detection. (arXiv:2107.13279v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13279","description":"<p>Road detection is a critically important task for self-driving cars. By\nemploying LiDAR data, recent works have significantly improved the accuracy of\nroad detection. Relying on LiDAR sensors limits the wide application of those\nmethods when only cameras are available. In this paper, we propose a novel road\ndetection approach with RGB being the only input during inference.\nSpecifically, we exploit pseudo-LiDAR using depth estimation, and propose a\nfeature fusion network where RGB and learned depth information are fused for\nimproved road detection. To further optimize the network structure and improve\nthe efficiency of the network. we search for the network structure of the\nfeature fusion module using NAS techniques. Finally, be aware of that\ngenerating pseudo-LiDAR from RGB via depth estimation introduces extra\ncomputational costs and relies on depth estimation networks, we design a\nmodality distillation strategy and leverage it to further free our network from\nthese extra computational cost and dependencies during inference. The proposed\nmethod achieves state-of-the-art performance on two challenging benchmarks,\nKITTI and R2D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Libo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v3 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2109.01838","description":"<p>We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.\ncorrelation clustering) problem, a classical graph clustering problem widely\nused in machine learning and computer vision. Our algorithm consists of three\nsteps executed recursively: (1) Finding conflicted cycles that correspond to\nviolated inequalities of the underlying multicut relaxation, (2) Performing\nmessage passing between the edges and cycles to optimize the Lagrange\nrelaxation coming from the found violated cycles producing reduced costs and\n(3) Contracting edges with high reduced costs through matrix-matrix\nmultiplications. Our algorithm produces primal solutions and lower bounds that\nestimate the distance to optimum. We implement our algorithm on GPUs and show\nresulting one to two orders-of-magnitudes improvements in execution speed\nwithout sacrificing solution quality compared to traditional sequential\nalgorithms that run on CPUs. We can solve very large scale benchmark problems\nwith up to $\\mathcal{O}(10^8)$ variables in a few seconds with small\nprimal-dual gaps. Our code is available at\nhttps://github.com/pawelswoboda/RAMA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAFNe: A One-Stage Anchor-Free Approach for Oriented Object Detection. (arXiv:2109.06148v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06148","description":"<p>We present DAFNe, a Dense one-stage Anchor-Free deep Network for oriented\nobject detection. As a one-stage model, it performs bounding box predictions on\na dense grid over the input image, being architecturally simpler in design, as\nwell as easier to optimize than its two-stage counterparts. Furthermore, as an\nanchor-free model, it reduces the prediction complexity by refraining from\nemploying bounding box anchors. With DAFNe we introduce an orientation-aware\ngeneralization of the center-ness function for arbitrarily oriented bounding\nboxes to down-weight low-quality predictions and a center-to-corner bounding\nbox prediction strategy that improves object localization performance. Our\nexperiments show that DAFNe outperforms all previous one-stage anchor-free\nmodels on DOTA 1.0, DOTA 1.5, and UCAS-AOD and is on par with the best models\non HRSC2016.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_S/0/1/0/all/0/1\">Steven Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventola_F/0/1/0/all/0/1\">Fabrizio Ventola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FathomNet: A global image database for enabling artificial intelligence in the ocean. (arXiv:2109.14646v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14646","description":"<p>The ocean is experiencing unprecedented rapid change, and visually monitoring\nmarine biota at the spatiotemporal scales needed for responsible stewardship is\na formidable task. As baselines are sought by the research community, the\nvolume and rate of this required data collection rapidly outpaces our abilities\nto process and analyze them. Recent advances in machine learning enables fast,\nsophisticated analysis of visual data, but have had limited success in the\nocean due to lack of data standardization, insufficient formatting, and demand\nfor large, labeled datasets. To address this need, we built FathomNet, an\nopen-source image database that standardizes and aggregates expertly curated\nlabeled data. FathomNet has been seeded with existing iconic and non-iconic\nimagery of marine animals, underwater equipment, debris, and other concepts,\nand allows for future contributions from distributed data sources. We\ndemonstrate how FathomNet data can be used to train and deploy models on other\ninstitutional video to reduce annotation effort, and enable automated tracking\nof underwater concepts when integrated with robotic vehicles. As FathomNet\ncontinues to grow and incorporate more labeled data from the community, we can\naccelerate the processing of visual data to achieve a healthy and sustainable\nglobal ocean.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katija_K/0/1/0/all/0/1\">Kakani Katija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orenstein_E/0/1/0/all/0/1\">Eric Orenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlining_B/0/1/0/all/0/1\">Brian Schlining</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundsten_L/0/1/0/all/0/1\">Lonny Lundsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnard_K/0/1/0/all/0/1\">Kevin Barnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainz_G/0/1/0/all/0/1\">Giovanna Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulais_O/0/1/0/all/0/1\">Oceane Boulais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cromwell_M/0/1/0/all/0/1\">Megan Cromwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butler_E/0/1/0/all/0/1\">Erin Butler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodward_B/0/1/0/all/0/1\">Benjamin Woodward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_K/0/1/0/all/0/1\">Katy Croff Bell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Instance Segmentation with Automotive Radar Detection Points. (arXiv:2110.01775v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01775","description":"<p>Automotive radar provides reliable environmental perception in all-weather\nconditions with affordable cost, but it hardly supplies semantic and geometry\ninformation due to the sparsity of radar detection points. With the development\nof automotive radar technologies in recent years, instance segmentation becomes\npossible by using automotive radar. Its data contain contexts such as radar\ncross section and micro-Doppler effects, and sometimes can provide detection\nwhen the field of view is obscured. The outcome from instance segmentation\ncould be potentially used as the input of trackers for tracking targets. The\nexisting methods often utilize a clustering based classification framework,\nwhich fits the need of real-time processing but has limited performance due to\nminimum information provided by sparse radar detection points. In this paper,\nwe propose an efficient method based on clustering of estimated semantic\ninformation to achieve instance segmentation for the sparse radar detection\npoints. In addition, we show that the performance of the proposed approach can\nbe further enhanced by incorporating the visual multi-layer perceptron. The\neffectiveness of the proposed method is verified by experimental results on the\npopular RadarScenes dataset, achieving 89.53% mCov and 86.97% mAP0.5, which is\nthe best comparing to other approaches in the literature. More significantly,\nthe proposed algorithm consumes memory around 1MB, and the inference time is\nless than 40ms. These two criteria ensure the practicality of the proposed\nmethod in real-world systems\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Weiyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Liping Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuxuan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bing Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Effect of Selfie Beautification Filters on Face Detection and Recognition. (arXiv:2110.08934v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08934","description":"<p>Beautification and augmented reality filters are very popular in applications\nthat use selfie images captured with smartphones or personal devices. However,\nthey can distort or modify biometric features, severely affecting the\ncapability of recognizing individuals' identity or even detecting the face.\nAccordingly, we address the effect of such filters on the accuracy of automated\nface detection and recognition. The social media image filters studied either\nmodify the image contrast or illumination or occlude parts of the face with for\nexample artificial glasses or animal noses. We observe that the effect of some\nof these filters is harmful both to face detection and identity recognition,\nspecially if they obfuscate the eye or (to a lesser extent) the nose. To\ncounteract such effect, we develop a method to reconstruct the applied\nmanipulation with a modified version of the U-NET segmentation network. This is\nobserved to contribute to a better face detection and recognition accuracy.\nFrom a recognition perspective, we employ distance measures and trained machine\nlearning algorithms applied to features extracted using a ResNet-34 network\ntrained to recognize faces. We also evaluate if incorporating filtered images\nto the training set of machine learning approaches are beneficial for identity\nrecognition. Our results show good recognition when filters do not occlude\nimportant landmarks, specially the eyes (identification accuracy &gt;99%, EER&lt;2%).\nThe combined effect of the proposed approaches also allow to mitigate the\neffect produced by filters that occlude parts of the face, achieving an\nidentification accuracy of &gt;92% with the majority of perturbations evaluated,\nand an EER &lt;8%. Although there is room for improvement, when neither U-NET\nreconstruction nor training with filtered images is applied, the accuracy with\nfilters that severely occlude the eye is &lt;72% (identification) and &gt;12% (EER)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Pontus Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skepetzis_V/0/1/0/all/0/1\">Vasilios Skepetzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Diaz_K/0/1/0/all/0/1\">Kevin Hernandez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1\">Josef Bigun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1\">Fernando Alonso-Fernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Restormer: Efficient Transformer for High-Resolution Image Restoration. (arXiv:2111.09881v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09881","description":"<p>Since convolutional neural networks (CNNs) perform well at learning\ngeneralizable image priors from large-scale data, these models have been\nextensively applied to image restoration and related tasks. Recently, another\nclass of neural architectures, Transformers, have shown significant performance\ngains on natural language and high-level vision tasks. While the Transformer\nmodel mitigates the shortcomings of CNNs (i.e., limited receptive field and\ninadaptability to input content), its computational complexity grows\nquadratically with the spatial resolution, therefore making it infeasible to\napply to most image restoration tasks involving high-resolution images. In this\nwork, we propose an efficient Transformer model by making several key designs\nin the building blocks (multi-head attention and feed-forward network) such\nthat it can capture long-range pixel interactions, while still remaining\napplicable to large images. Our model, named Restoration Transformer\n(Restormer), achieves state-of-the-art results on several image restoration\ntasks, including image deraining, single-image motion deblurring, defocus\ndeblurring (single-image and dual-pixel data), and image denoising (Gaussian\ngrayscale/color denoising, and real image denoising). The source code and\npre-trained models are available at https://github.com/swz30/Restormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aditya Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastDOG: Fast Discrete Optimization on GPU. (arXiv:2111.10270v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2111.10270","description":"<p>We present a massively parallel Lagrange decomposition method for solving\n0--1 integer linear programs occurring in structured prediction. We propose a\nnew iterative update scheme for solving the Lagrangean dual and a perturbation\ntechnique for decoding primal solutions. For representing subproblems we follow\nLange et al. (2021) and use binary decision diagrams (BDDs). Our primal and\ndual algorithms require little synchronization between subproblems and\noptimization over BDDs needs only elementary operations without complicated\ncontrol flow. This allows us to exploit the parallelism offered by GPUs for all\ncomponents of our method. We present experimental results on combinatorial\nproblems from MAP inference for Markov Random Fields, quadratic assignment and\ncell tracking for developmental biology. Our highly parallel GPU implementation\nimproves upon the running times of the algorithms from Lange et al. (2021) by\nup to an order of magnitude. In particular, we come close to or outperform some\nstate-of-the-art specialized heuristics while being problem agnostic. Our\nimplementation is available at https://github.com/LPMP/BDD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Image Patch is a Wave: Quantum Inspired Vision MLP. (arXiv:2111.12294v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12294","description":"<p>In the field of computer vision, recent works show that a pure MLP\narchitecture mainly stacked by fully-connected layers can achieve competing\nperformance with CNN and transformer. An input image of vision MLP is usually\nsplit into multiple tokens (patches), while the existing MLP models directly\naggregate them with fixed weights, neglecting the varying semantic information\nof tokens from different images. To dynamically aggregate tokens, we propose to\nrepresent each token as a wave function with two parts, amplitude and phase.\nAmplitude is the original feature and the phase term is a complex value\nchanging according to the semantic contents of input images. Introducing the\nphase term can dynamically modulate the relationship between tokens and fixed\nweights in MLP. Based on the wave-like token representation, we establish a\nnovel Wave-MLP architecture for vision tasks. Extensive experiments demonstrate\nthat the proposed Wave-MLP is superior to the state-of-the-art MLP\narchitectures on various vision tasks such as image classification, object\ndetection and semantic segmentation. The source code will be available at\nhttps://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch and\nhttps://gitee.com/mindspore/models/tree/master/research/cv/wave_mlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep MAGSAC++. (arXiv:2111.14093v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14093","description":"<p>We propose Deep MAGSAC++ combining the advantages of traditional and deep\nrobust estimators. We introduce a novel loss function that exploits the\norientation and scale from partially affine covariant features, e.g., SIFT, in\na geometrically justifiable manner. The new loss helps in learning higher-order\ninformation about the underlying scene geometry. Moreover, we propose a new\nsampler for RANSAC that always selects the sample with the highest probability\nof consisting only of inliers. After every unsuccessful iteration, the\nprobabilities are updated in a principled way via a Bayesian approach. The\nprediction of the deep network is exploited as prior inside the sampler.\nBenefiting from the new loss, the proposed sampler and a number of technical\nadvancements, Deep MAGSAC++ is superior to the state-of-the-art both in terms\nof accuracy and run-time on thousands of image pairs from publicly available\nreal-world datasets for essential and fundamental matrix estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_W/0/1/0/all/0/1\">Wei Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No-Reference Point Cloud Quality Assessment via Domain Adaptation. (arXiv:2112.02851v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02851","description":"<p>We present a novel no-reference quality assessment metric, the image\ntransferred point cloud quality assessment (IT-PCQA), for 3D point clouds. For\nquality assessment, deep neural network (DNN) has shown compelling performance\non no-reference metric design. However, the most challenging issue for\nno-reference PCQA is that we lack large-scale subjective databases to drive\nrobust networks. Our motivation is that the human visual system (HVS) is the\ndecision-maker regardless of the type of media for quality assessment.\nLeveraging the rich subjective scores of the natural images, we can quest the\nevaluation criteria of human perception via DNN and transfer the capability of\nprediction to 3D point clouds. In particular, we treat natural images as the\nsource domain and point clouds as the target domain, and infer point cloud\nquality via unsupervised adversarial domain adaptation. To extract effective\nlatent features and minimize the domain discrepancy, we propose a hierarchical\nfeature encoder and a conditional-discriminative network. Considering that the\nultimate purpose is regressing objective score, we introduce a novel\nconditional cross entropy loss in the conditional-discriminative network to\npenalize the negative samples which hinder the convergence of the quality\nregression network. Experimental results show that the proposed method can\nachieve higher performance than traditional no-reference metrics, even\ncomparable results with full-reference metrics. The proposed method also\nsuggests the feasibility of assessing the quality of specific media content\nwithout the expensive and cumbersome subjective evaluations. Code is available\nat https://github.com/Qi-Yangsjtu/IT-PCQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yipeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Visualization and Representation Analysis Applied to Glacier Segmentation. (arXiv:2112.08184v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08184","description":"<p>Interpretability has attracted increasing attention in earth observation\nproblems. We apply interactive visualization and representation analysis to\nguide interpretation of glacier segmentation models. We visualize the\nactivations from a U-Net to understand and evaluate the model performance. We\nbuild an online interface using the Shiny R package to provide comprehensive\nerror analysis of the predictions. Users can interact with the panels and\ndiscover model failure modes. Further, we discuss how visualization can provide\nsanity checks during data preprocessing and model training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Minxing Zheng</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xinran Miao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sankaran_K/0/1/0/all/0/1\">Kris Sankaran</a> (1) ((1) Department of Statistics, University of Wisconsin - Madison)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Based Workflow for Detection of Lung Nodules With Chest Radiograph. (arXiv:2112.10184v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10184","description":"<p>PURPOSE: This study aimed to develop a deep learning-based tool to detect and\nlocalize lung nodules with chest radiographs(CXRs). We expected it to enhance\nthe efficiency of interpreting CXRs and reduce the possibilities of delayed\ndiagnosis of lung cancer.\n</p>\n<p>MATERIALS AND METHODS: We collected CXRs from NCKUH database and VBD, an\nopen-source medical image dataset, as our training and validation data. A\nnumber of CXRs from the Ministry of Health and Welfare(MOHW) database served as\nour test data. We built a segmentation model to identify lung areas from CXRs,\nand sliced them into 16 patches. Physicians labeled the CXRs by clicking the\npatches. These labeled patches were then used to train and fine-tune a deep\nneural network(DNN) model, classifying the patches as positive or negative.\nFinally, we test the DNN model with the lung patches of CXRs from MOHW.\n</p>\n<p>RESULTS: Our segmentation model identified the lung regions well from the\nwhole CXR. The Intersection over Union(IoU) between the ground truth and the\nsegmentation result was 0.9228. In addition, our DNN model achieved a\nsensitivity of 0.81, specificity of 0.82, and AUROC of 0.869 in 98 of 125\ncases. For the other 27 difficult cases, the sensitivity was 0.54, specificity\n0.494, and AUROC 0.682. Overall, we obtained a sensitivity of 0.78, specificity\nof 0.79, and AUROC 0.837.\n</p>\n<p>CONCLUSIONS: Our two-step workflow is comparable to state-of-the-art\nalgorithms in the sensitivity and specificity of localizing lung nodules from\nCXRs. Notably, our workflow provides an efficient way for specialists to label\nthe data, which is valuable for relevant researches because of the relative\nrarity of labeled medical image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tai_Y/0/1/0/all/0/1\">Yang Tai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yu-Wen Fang</a> (Same contribution), <a href=\"http://arxiv.org/find/eess/1/au:+Su_F/0/1/0/all/0/1\">Fang-Yi Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiang_J/0/1/0/all/0/1\">Jung-Hsien Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation. (arXiv:2201.01501v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.01501","description":"<p>Depth estimation is solved as a regression or classification problem in\nexisting learning-based multi-view stereo methods. Although these two\nrepresentations have recently demonstrated their excellent performance, they\nstill have apparent shortcomings, e.g., regression methods tend to overfit due\nto the indirect learning cost volume, and classification methods cannot\ndirectly infer the exact depth due to its discrete prediction. In this paper,\nwe propose a novel representation, termed Unification, to unify the advantages\nof regression and classification. It can directly constrain the cost volume\nlike classification methods, but also realize the sub-pixel depth prediction\nlike regression methods. To excavate the potential of unification, we design a\nnew loss function named Unified Focal Loss, which is more uniform and\nreasonable to combat the challenge of sample imbalance. Combining these two\nunburdened modules, we present a coarse-to-fine framework, that we call\nUniMVSNet. The results of ranking first on both DTU and Tanks and Temples\nbenchmarks verify that our model not only performs the best but also has the\nbest generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Rui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yawen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ronggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles. (arXiv:2201.05057v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05057","description":"<p>Trajectory prediction is a critical component for autonomous vehicles (AVs)\nto perform safe planning and navigation. However, few studies have analyzed the\nadversarial robustness of trajectory prediction or investigated whether the\nworst-case prediction can still lead to safe planning. To bridge this gap, we\nstudy the adversarial robustness of trajectory prediction models by proposing a\nnew adversarial attack that perturbs normal vehicle trajectories to maximize\nthe prediction error. Our experiments on three models and three datasets show\nthat the adversarial prediction increases the prediction error by more than\n150%. Our case studies show that if an adversary drives a vehicle close to the\ntarget AV following the adversarial trajectory, the AV may make an inaccurate\nprediction and even make unsafe driving decisions. We also explore possible\nmitigation techniques via data augmentation and trajectory smoothing. The\nimplementation is open source at\nhttps://github.com/zqzqz/AdvTrajectoryPrediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengtuo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiachen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Alfred Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Z. Morley Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04595","description":"<p>Neural image compression have reached or out-performed traditional methods\n(such as JPEG, BPG, WebP). However,their sophisticated network structures with\ncascaded convolution layers bring heavy computational burden for practical\ndeployment. In this paper, we explore the structural sparsity in neural image\ncompression network to obtain real-time acceleration without any specialized\nhardware design or algorithm. We propose a simple plug-in adaptive binary\nchannel masking(ABCM) to judge the importance of each convolution channel and\nintroduce sparsity during training. During inference, the unimportant channels\nare pruned to obtain slimmer network and less computation. We implement our\nmethod into three neural image compression networks with different entropy\nmodels to verify its effectiveness and generalization, the experiment results\nshow that up to 7x computation reduction and 3x acceleration can be achieved\nwith negligible performance drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yin_S/0/1/0/all/0/1\">Shanzhi Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Wen Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Youneng Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Y/0/1/0/all/0/1\">Yongsheng Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bench-Marking And Improving Arabic Automatic Image Captioning Through The Use Of Multi-Task Learning Paradigm. (arXiv:2202.05474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.05474","description":"<p>The continuous increase in the use of social media and the visual content on\nthe internet have accelerated the research in computer vision field in general\nand the image captioning task in specific. The process of generating a caption\nthat best describes an image is a useful task for various applications such as\nit can be used in image indexing and as a hearing aid for the visually\nimpaired. In recent years, the image captioning task has witnessed remarkable\nadvances regarding both datasets and architectures, and as a result, the\ncaptioning quality has reached an astounding performance. However, the majority\nof these advances especially in datasets are targeted for English, which left\nother languages such as Arabic lagging behind. Although Arabic language, being\nspoken by more than 450 million people and being the most growing language on\nthe internet, lacks the fundamental pillars it needs to advance its image\ncaptioning research, such as benchmarks or unified datasets. This works is an\nattempt to expedite the synergy in this task by providing unified datasets and\nbenchmarks, while also exploring methods and techniques that could enhance the\nperformance of Arabic image captioning. The use of multi-task learning is\nexplored, alongside exploring various word representations and different\nfeatures. The results showed that the use of multi-task learning and\npre-trained word embeddings noticeably enhanced the quality of image\ncaptioning, however the presented results shows that Arabic captioning still\nlags behind when compared to the English language. The used dataset and code\nare available at this link.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zater_M/0/1/0/all/0/1\">Muhy Eddin Za&#x27;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talafha_B/0/1/0/all/0/1\">Bashar Talafha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Deterministic Translation for Unsupervised Domain Adaptation. (arXiv:2202.07778v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07778","description":"<p>In this work we challenge the common approach of using a one-to-one mapping\n('translation') between the source and target domains in unsupervised domain\nadaptation (UDA). Instead, we rely on stochastic translation to capture\ninherent translation ambiguities. This allows us to (i) train more accurate\ntarget networks by generating multiple outputs conditioned on the same source\nimage, leveraging both accurate translation and data augmentation for\nappearance variability, (ii) impute robust pseudo-labels for the target data by\naveraging the predictions of a source network on multiple translated versions\nof a single target image and (iii) train and ensemble diverse networks in the\ntarget domain by modulating the degree of stochasticity in the translations. We\nreport improvements over strong recent baselines, leading to state-of-the-art\nUDA results on two challenging semantic segmentation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiou_E/0/1/0/all/0/1\">Eleni Chiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagiotaki_E/0/1/0/all/0/1\">Eleftheria Panagiotaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokkinos_I/0/1/0/all/0/1\">Iasonas Kokkinos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TableFormer: Table Structure Understanding with Transformers. (arXiv:2203.01017v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01017","description":"<p>Tables organize valuable content in a concise and compact representation.\nThis content is extremely valuable for systems such as search engines,\nKnowledge Graph's, etc, since they enhance their predictive capabilities.\nUnfortunately, tables come in a large variety of shapes and sizes. Furthermore,\nthey can have complex column/row-header configurations, multiline rows,\ndifferent variety of separation lines, missing entries, etc. As such, the\ncorrect identification of the table-structure from an image is a non-trivial\ntask. In this paper, we present a new table-structure identification model. The\nlatter improves the latest end-to-end deep learning model (i.e.\nencoder-dual-decoder from PubTabNet) in two significant ways. First, we\nintroduce a new object detection decoder for table-cells. In this way, we can\nobtain the content of the table-cells from programmatic PDF's directly from the\nPDF source and avoid the training of the custom OCR decoders. This\narchitectural change leads to more accurate table-content extraction and allows\nus to tackle non-english tables. Second, we replace the LSTM decoders with\ntransformer based decoders. This upgrade improves significantly the previous\nstate-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple\ntables and from 88.7% to 95% on complex tables.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Ahmed Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livathinos_N/0/1/0/all/0/1\">Nikolaos Livathinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lysak_M/0/1/0/all/0/1\">Maksym Lysak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staar_P/0/1/0/all/0/1\">Peter Staar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02925","description":"<p>Weakly supervised temporal action localization aims to localize temporal\nboundaries of actions and simultaneously identify their categories with only\nvideo-level category labels. Many existing methods seek to generate pseudo\nlabels for bridging the discrepancy between classification and localization,\nbut usually only make use of limited contextual information for pseudo label\ngeneration. To alleviate this problem, we propose a representative snippet\nsummarization and propagation framework. Our method seeks to mine the\nrepresentative snippets in each video for propagating information between video\nsnippets to generate better pseudo labels. For each video, its own\nrepresentative snippets and the representative snippets from a memory bank are\npropagated to update the input features in an intra- and inter-video manner.\nThe pseudo labels are generated from the temporal class activation maps of the\nupdated features to rectify the predictions of the main branch. Our method\nobtains superior performance in comparison to the existing methods on two\nbenchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in\nterms of average mAP on THUMOS14.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linjiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Federated Learning with Local Regularization and Sparsification. (arXiv:2203.03106v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.03106","description":"<p>User-level differential privacy (DP) provides certifiable privacy guarantees\nto the information that is specific to any user's data in federated learning.\nExisting methods that ensure user-level DP come at the cost of severe accuracy\ndecrease. In this paper, we study the cause of model performance degradation in\nfederated learning under user-level DP guarantee. We find the key to solving\nthis issue is to naturally restrict the norm of local updates before executing\noperations that guarantee DP. To this end, we propose two techniques, Bounded\nLocal Update Regularization and Local Update Sparsification, to increase model\nquality without sacrificing privacy. We provide theoretical analysis on the\nconvergence of our framework and give rigorous privacy guarantees. Extensive\nexperiments show that our framework significantly improves the privacy-utility\ntrade-off over the state-of-the-arts for federated learning with user-level DP\nguarantee.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">Anda Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peisong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Sheryl Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biometric recognition: why not massively adopted yet?. (arXiv:2203.03719v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2203.03719","description":"<p>Although there has been a dramatically reduction on the prices of capturing\ndevices and an increase on computing power in the last decade, it seems that\nbiometric systems are still far from massive adoption for civilian\napplications. This paper deals with the causes of this phenomenon, as well as\nsome misconceptions regarding biometric identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards performant and reliable undersampled MR reconstruction via diffusion model sampling. (arXiv:2203.04292v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04292","description":"<p>Magnetic Resonance (MR) image reconstruction from under-sampled acquisition\npromises faster scanning time. To this end, current State-of-The-Art (SoTA)\napproaches leverage deep neural networks and supervised training to learn a\nrecovery model. While these approaches achieve impressive performances, the\nlearned model can be fragile on unseen degradation, e.g. when given a different\nacceleration factor. These methods are also generally deterministic and provide\na single solution to an ill-posed problem; as such, it can be difficult for\npractitioners to understand the reliability of the reconstruction. We introduce\nDiffuseRecon, a novel diffusion model-based MR reconstruction method.\nDiffuseRecon guides the generation process based on the observed signals and a\npre-trained diffusion model, and does not require additional training on\nspecific acceleration factors. DiffuseRecon is stochastic in nature and\ngenerates results from a distribution of fully-sampled MR images; as such, it\nallows us to explicitly visualize different potential reconstruction solutions.\nLastly, DiffuseRecon proposes an accelerated, coarse-to-fine Monte-Carlo\nsampling scheme to approximate the most likely reconstruction candidate. The\nproposed DiffuseRecon achieves SoTA performances reconstructing from raw\nacquisition signals in fastMRI and SKM-TEA. Code will be open-sourced at\nwww.github.com/cpeng93/DiffuseRecon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Cheng Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-free Domain Adaptation for Multi-site and Lifespan Brain Skull Stripping. (arXiv:2203.04299v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04299","description":"<p>Skull stripping is a crucial prerequisite step in the analysis of brain\nmagnetic resonance (MR) images. Although many excellent works or tools have\nbeen proposed, they suffer from low generalization capability. For instance,\nthe model trained on a dataset with specific imaging parameters (source domain)\ncannot be well applied to other datasets with different imaging parameters\n(target domain). Especially, for the lifespan datasets, the model trained on an\nadult dataset is not applicable to an infant dataset due to the large domain\ndifference. To address this issue, numerous domain adaptation (DA) methods have\nbeen proposed to align the extracted features between the source and target\ndomains, requiring concurrent access to the input images of both domains.\nUnfortunately, it is problematic to share the images due to privacy. In this\npaper, we design a source-free domain adaptation framework (SDAF) for\nmulti-site and lifespan skull stripping that can accomplish domain adaptation\nwithout access to source domain images. Our method only needs to share the\nsource labels as shape dictionaries and the weights trained on the source data,\nwithout disclosing private information from source domain subjects. To deal\nwith the domain shift between multi-site lifespan datasets, we take advantage\nof the brain shape prior which is invariant to imaging parameters and ages.\nExperiments demonstrate that our framework can significantly outperform the\nstate-of-the-art methods on multi-site lifespan datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yunxiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dan_R/0/1/0/all/0/1\">Ruilong Dan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1\">Yifan Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1\">Xiangde Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_C/0/1/0/all/0/1\">Chenghao Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_G/0/1/0/all/0/1\">Gangyong Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structure-Aware Flow Generation for Human Body Reshaping. (arXiv:2203.04670v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04670","description":"<p>Body reshaping is an important procedure in portrait photo retouching. Due to\nthe complicated structure and multifarious appearance of human bodies, existing\nmethods either fall back on the 3D domain via body morphable model or resort to\nkeypoint-based image deformation, leading to inefficiency and unsatisfied\nvisual quality. In this paper, we address these limitations by formulating an\nend-to-end flow generation architecture under the guidance of body structural\npriors, including skeletons and Part Affinity Fields, and achieve\nunprecedentedly controllable performance under arbitrary poses and garments. A\ncompositional attention mechanism is introduced for capturing both visual\nperceptual correlations and structural associations of the human body to\nreinforce the manipulation consistency among related parts. For a comprehensive\nevaluation, we construct the first large-scale body reshaping dataset, namely\nBR-5K, which contains 5,000 portrait photos as well as professionally retouched\ntargets. Extensive experiments demonstrate that our approach significantly\noutperforms existing state-of-the-art methods in terms of visual performance,\ncontrollability, and efficiency. The dataset is available at our website:\nhttps://github.com/JianqiangRen/FlowBasedBodyReshaping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jianqiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1\">Biwen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Transformer Framework for Group-based Segmentation: Co-Segmentation, Co-Saliency Detection and Video Salient Object Detection. (arXiv:2203.04708v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04708","description":"<p>Humans tend to mine objects by learning from a group of images or several\nframes of video since we live in a dynamic world. In the computer vision area,\nmany researches focus on co-segmentation (CoS), co-saliency detection (CoSD)\nand video salient object detection (VSOD) to discover the co-occurrent objects.\nHowever, previous approaches design different networks on these similar tasks\nseparately, and they are difficult to apply to each other, which lowers the\nupper bound of the transferability of deep learning frameworks. Besides, they\nfail to take full advantage of the cues among inter- and intra-feature within a\ngroup of images. In this paper, we introduce a unified framework to tackle\nthese issues, term as UFO (Unified Framework for Co-Object Segmentation).\nSpecifically, we first introduce a transformer block, which views the image\nfeature as a patch token and then captures their long-range dependencies\nthrough the self-attention mechanism. This can help the network to excavate the\npatch structured similarities among the relevant objects. Furthermore, we\npropose an intra-MLP learning module to produce self-mask to enhance the\nnetwork to avoid partial activation. Extensive experiments on four CoS\nbenchmarks (PASCAL, iCoseg, Internet and MSRC), three CoSD benchmarks\n(Cosal2015, CoSOD3k, and CocA) and four VSOD benchmarks (DAVIS16, FBMS, ViSal\nand SegV2) show that our method outperforms other state-of-the-arts on three\ndifferent tasks in both accuracy and speed by using the same network\narchitecture , which can reach 140 FPS in real-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yukun Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jingliang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingyao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Boundary Learning for Point Cloud Segmentation. (arXiv:2203.05272v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05272","description":"<p>Point cloud segmentation is fundamental in understanding 3D environments.\nHowever, current 3D point cloud segmentation methods usually perform poorly on\nscene boundaries, which degenerates the overall segmentation performance. In\nthis paper, we focus on the segmentation of scene boundaries. Accordingly, we\nfirst explore metrics to evaluate the segmentation performance on scene\nboundaries. To address the unsatisfactory performance on boundaries, we then\npropose a novel contrastive boundary learning (CBL) framework for point cloud\nsegmentation. Specifically, the proposed CBL enhances feature discrimination\nbetween points across boundaries by contrasting their representations with the\nassistance of scene contexts at multiple scales. By applying CBL on three\ndifferent baseline methods, we experimentally show that CBL consistently\nimproves different baselines and assists them to achieve compelling performance\non boundaries, as well as the overall performance, eg in mIoU. The experimental\nresults demonstrate the effectiveness of our method and the importance of\nboundaries for 3D point cloud segmentation. Code and model will be made\npublicly available at https://github.com/LiyaoTang/contrastBoundary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05297","description":"<p>Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1\">Naoya Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yichen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1\">Elif Bozkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05321","description":"<p>We present StyleBabel, a unique open access dataset of natural language\ncaptions and free-form tags describing the artistic style of over 135K digital\nartworks, collected via a novel participatory method from experts studying at\nspecialist art and design schools. StyleBabel was collected via an iterative\nmethod, inspired by `Grounded Theory': a qualitative approach that enables\nannotation while co-evolving a shared language for fine-grained artistic style\nattribute description. We demonstrate several downstream tasks for StyleBabel,\nadapting the recent ALADIN architecture for fine-grained style similarity, to\ntrain cross-modal embeddings for: 1) free-form tag generation; 2) natural\nlanguage description of artistic style; 3) fine-grained text search of style.\nTo do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and\ncross-modal representation learning, achieving a state of the art accuracy in\nfine-grained style retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruta_D/0/1/0/all/0/1\">Dan Ruta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marri_N/0/1/0/all/0/1\">Naveen Marri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_A/0/1/0/all/0/1\">Ajinkya Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briggs_J/0/1/0/all/0/1\">Jo Briggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speed_C/0/1/0/all/0/1\">Chris Speed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faieta_B/0/1/0/all/0/1\">Baldo Faieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipkowski_A/0/1/0/all/0/1\">Alex Filipkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrueType Transformer: Character and Font Style Recognition in Outline Format. (arXiv:2203.05338v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05338","description":"<p>We propose TrueType Transformer (T3), which can perform character and font\nstyle recognition in an outline format. The outline format, such as TrueType,\nrepresents each character as a sequence of control points of stroke contours\nand is frequently used in born-digital documents. T3 is organized by a deep\nneural network, so-called Transformer. Transformer is originally proposed for\nsequential data, such as text, and therefore appropriate for handling the\noutline data. In other words, T3 directly accepts the outline data without\nconverting it into a bitmap image. Consequently, T3 realizes a\nresolution-independent classification. Moreover, since the locations of the\ncontrol points represent the fine and local structures of the font style, T3 is\nsuitable for font style classification, where such structures are very\nimportant. In this paper, we experimentally show the applicability of T3 in\ncharacter and font style recognition tasks, while observing how the individual\ncontrol points contribute to classification results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagata_Y/0/1/0/all/0/1\">Yusuke Nagata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otao_J/0/1/0/all/0/1\">Jinki Otao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_D/0/1/0/all/0/1\">Daichi Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05340","description":"<p>With diverse presentation attacks emerging continually, generalizable face\nanti-spoofing (FAS) has drawn growing attention. Most existing methods\nimplement domain generalization (DG) on the complete representations. However,\ndifferent image statistics may have unique properties for the FAS tasks. In\nthis work, we separate the complete representation into content and style ones.\nA novel Shuffled Style Assembly Network (SSAN) is proposed to extract and\nreassemble different content and style features for a stylized feature space.\nThen, to obtain a generalized representation, a contrastive learning strategy\nis developed to emphasize liveness-related style information while suppress the\ndomain-specific one. Finally, the representations of the correct assemblies are\nused to distinguish between living and spoofing during the inferring. On the\nother hand, despite the decent performance, there still exists a gap between\nacademia and industry, due to the difference in data quantity and distribution.\nThus, a new large-scale benchmark for FAS is built up to further evaluate the\nperformance of algorithms in reality. Both qualitative and quantitative results\non existing and proposed benchmarks demonstrate the effectiveness of our\nmethods. The codes will be available at https://github.com/wangzhuo2019/SSAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Size Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGCN: Augmented Graph Convolutional Network for Lifelong Multi-label Image Recognition. (arXiv:2203.05534v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05534","description":"<p>The Lifelong Multi-Label (LML) image recognition builds an online\nclass-incremental classifier in a sequential multi-label image recognition data\nstream. The key challenges of LML image recognition are the construction of\nlabel relationships on Partial Labels of training data and the Catastrophic\nForgetting on old classes, resulting in poor generalization. To solve the\nproblems, the study proposes an Augmented Graph Convolutional Network (AGCN)\nmodel that can construct the label relationships across the sequential\nrecognition tasks and sustain the catastrophic forgetting. First, we build an\nAugmented Correlation Matrix (ACM) across all seen classes, where the\nintra-task relationships derive from the hard label statistics while the\ninter-task relationships leverage both hard and soft labels from data and a\nconstructed expert network. Then, based on the ACM, the proposed AGCN captures\nlabel dependencies with dynamic augmented structure and yields effective class\nrepresentations. Last, to suppress the forgetting of label dependencies across\nold tasks, we propose a relationship-preserving loss as a constraint to the\nconstruction of label relationships. The proposed method is evaluated using two\nmulti-label image benchmarks and the experimental results show that the\nproposed method is effective for LML image recognition and can build convincing\ncorrelation across tasks even if the labels of previous tasks are missing. Our\ncode is available at https://github.com/Kaile-Du/AGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1\">Kaile Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1\">Fan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fuyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fenglei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiming Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}