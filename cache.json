{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Product Market Demand Analysis Using NLP in Banglish Text with Sentiment Analysis and Named Entity Recognition. (arXiv:2204.01827v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01827","description":"<p>Product market demand analysis plays a significant role for originating\nbusiness strategies due to its noticeable impact on the competitive business\nfield. Furthermore, there are roughly 228 million native Bengali speakers, the\nmajority of whom use Banglish text to interact with one another on social\nmedia. Consumers are buying and evaluating items on social media with Banglish\ntext as social media emerges as an online marketplace for entrepreneurs. People\nuse social media to find preferred smartphone brands and models by sharing\ntheir positive and bad experiences with them. For this reason, our goal is to\ngather Banglish text data and use sentiment analysis and named entity\nidentification to assess Bangladeshi market demand for smartphones in order to\ndetermine the most popular smartphones by gender. We scraped product related\ndata from social media with instant data scrapers and crawled data from\nWikipedia and other sites for product information with python web scrapers.\nUsing Python's Pandas and Seaborn libraries, the raw data is filtered using NLP\nmethods. To train our datasets for named entity recognition, we utilized\nSpacey's custom NER model, Amazon Comprehend Custom NER. A tensorflow\nsequential model was deployed with parameter tweaking for sentiment analysis.\nMeanwhile, we used the Google Cloud Translation API to estimate the gender of\nthe reviewers using the BanglaLinga library. In this article, we use natural\nlanguage processing (NLP) approaches and several machine learning models to\nidentify the most in-demand items and services in the Bangladeshi market. Our\nmodel has an accuracy of 87.99% in Spacy Custom Named Entity recognition,\n95.51% in Amazon Comprehend Custom NER, and 87.02% in the Sequential model for\ndemand analysis. After Spacy's study, we were able to manage 80% of mistakes\nrelated to misspelled words using a mix of Levenshtein distance and ratio\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Sabbir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayla_N/0/1/0/all/0/1\">Nishat Nayla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasel_A/0/1/0/all/0/1\">Annajiat Alim Rasel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Automatic Text Summarization for Fake News Detection. (arXiv:2204.01841v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01841","description":"<p>The distribution of fake news is not a new but a rapidly growing problem. The\nshift to news consumption via social media has been one of the drivers for the\nspread of misleading and deliberately wrong information, as in addition to it\nof easy use there is rarely any veracity monitoring. Due to the harmful effects\nof such fake news on society, the detection of these has become increasingly\nimportant. We present an approach to the problem that combines the power of\ntransformer-based language models while simultaneously addressing one of their\ninherent problems. Our framework, CMTR-BERT, combines multiple text\nrepresentations, with the goal of circumventing sequential limits and related\nloss of information the underlying transformer architecture typically suffers\nfrom. Additionally, it enables the incorporation of contextual information.\nExtensive experiments on two very different, publicly available datasets\ndemonstrates that our approach is able to set new state-of-the-art performance\nbenchmarks. Apart from the benefit of using automatic text summarization\ntechniques we also find that the incorporation of contextual information\ncontributes to performance gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartl_P/0/1/0/all/0/1\">Philipp Hartl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compliance Checking with NLI: Privacy Policies vs. Regulations. (arXiv:2204.01845v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01845","description":"<p>A privacy policy is a document that states how a company intends to handle\nand manage their customers' personal data. One of the problems that arises with\nthese privacy policies is that their content might violate data privacy\nregulations. Because of the enormous number of privacy policies that exist, the\nonly realistic way to check for legal inconsistencies in all of them is through\nan automated method. In this work, we use Natural Language Inference (NLI)\ntechniques to compare privacy regulations against sections of privacy policies\nfrom a selection of large companies. Our NLI model uses pre-trained embeddings,\nalong with BiLSTM in its attention mechanism. We tried two versions of our\nmodel: one that was trained on the Stanford Natural Language Inference (SNLI)\nand the second on the Multi-Genre Natural Language Inference (MNLI) dataset. We\nfound that our test accuracy was higher on our model trained on the SNLI, but\nwhen actually doing NLI tasks on real world privacy policies, the model trained\non MNLI generalized and performed much better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabinia_A/0/1/0/all/0/1\">Amin Rabinia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nygaard_Z/0/1/0/all/0/1\">Zane Nygaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Embeddings with Laplacian Graph Priors. (arXiv:2204.01846v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01846","description":"<p>We introduce probabilistic embeddings using Laplacian priors (PELP). The\nproposed model enables incorporating graph side-information into static word\nembeddings. We theoretically show that the model unifies several previously\nproposed embedding methods under one umbrella. PELP generalises graph-enhanced,\ngroup, dynamic, and cross-lingual static word embeddings. PELP also enables any\ncombination of these previous models in a straightforward fashion. Furthermore,\nwe empirically show that our model matches the performance of previous models\nas special cases. In addition, we demonstrate its flexibility by applying it to\nthe comparison of political sociolects over time. Finally, we provide code as a\nTensorFlow implementation enabling flexible estimation in different settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yrjanainen_V/0/1/0/all/0/1\">V&#xe4;in&#xf6; Yrj&#xe4;n&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_M/0/1/0/all/0/1\">M&#xe5;ns Magnusson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Abusiveness Identification on Code-Mixed Social Media Text. (arXiv:2204.01848v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01848","description":"<p>Social Media platforms have been seeing adoption and growth in their usage\nover time. This growth has been further accelerated with the lockdown in the\npast year when people's interaction, conversation, and expression were limited\nphysically. It is becoming increasingly important to keep the platform safe\nfrom abusive content for better user experience. Much work has been done on\nEnglish social media content but text analysis on non-English social media is\nrelatively underexplored. Non-English social media content have the additional\nchallenges of code-mixing, transliteration and using different scripture in\nsame sentence. In this work, we propose an approach for abusiveness\nidentification on the multilingual Moj dataset which comprises of Indic\nlanguages. Our approach tackles the common challenges of non-English social\nmedia content and can be extended to other languages as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_E/0/1/0/all/0/1\">Ekagra Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_N/0/1/0/all/0/1\">Naman Poddar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Text Summarization Methods: A Comprehensive Review. (arXiv:2204.01849v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01849","description":"<p>One of the most pressing issues that have arisen due to the rapid growth of\nthe Internet is known as information overloading. Simplifying the relevant\ninformation in the form of a summary will assist many people because the\nmaterial on any topic is plentiful on the Internet. Manually summarising\nmassive amounts of text is quite challenging for humans. So, it has increased\nthe need for more complex and powerful summarizers. Researchers have been\ntrying to improve approaches for creating summaries since the 1950s, such that\nthe machine-generated summary matches the human-created summary. This study\nprovides a detailed state-of-the-art analysis of text summarization concepts\nsuch as summarization approaches, techniques used, standard datasets,\nevaluation metrics and future scopes for research. The most commonly accepted\napproaches are extractive and abstractive, studied in detail in this work.\nEvaluating the summary and increasing the development of reusable resources and\ninfrastructure aids in comparing and replicating findings, adding competition\nto improve the outcomes. Different evaluation methods of generated summaries\nare also discussed in this study. Finally, at the end of this study, several\nchallenges and research opportunities related to text summarization research\nare mentioned that may be useful for potential researchers working in this\narea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_D/0/1/0/all/0/1\">Divakar Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_J/0/1/0/all/0/1\">Jalpa Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1\">Arun Kumar Yadav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deliberation Model for On-Device Spoken Language Understanding. (arXiv:2204.01893v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01893","description":"<p>We propose a novel deliberation-based approach to end-to-end (E2E) spoken\nlanguage understanding (SLU), where a streaming automatic speech recognition\n(ASR) model produces the first-pass hypothesis and a second-pass natural\nlanguage understanding (NLU) component generates the semantic parse by\nconditioning on both ASR's text and audio embeddings. By formulating E2E SLU as\na generalized decoder, our system is able to support complex compositional\nsemantic structures. Furthermore, the sharing of parameters between ASR and NLU\nmakes the system especially suitable for resource-constrained (on-device)\nenvironments; our proposed approach consistently outperforms strong pipeline\nNLU baselines by 0.82% to 1.34% across various operating points on the spoken\nversion of the TOPv2 dataset. We demonstrate that the fusion of text and audio\nfeatures, coupled with the system's ability to rewrite the first-pass\nhypothesis, makes our approach more robust to ASR errors. Finally, we show that\nour approach can significantly reduce the degradation when moving from natural\nspeech to synthetic speech training, but more work is required to make\ntext-to-speech (TTS) a viable solution for scaling up E2E SLU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasello_P/0/1/0/all/0/1\">Paden Tomasello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Suyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livshits_A/0/1/0/all/0/1\">Aleksandr Livshits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks. (arXiv:2204.01906v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01906","description":"<p>We introduce Dynatask: an open source system for setting up custom NLP tasks\nthat aims to greatly lower the technical knowledge and effort required for\nhosting and evaluating state-of-the-art NLP models, as well as for conducting\nmodel in the loop data collection with crowdworkers. Dynatask is integrated\nwith Dynabench, a research platform for rethinking benchmarking in AI that\nfacilitates human and model in the loop data collection and evaluation. To\ncreate a task, users only need to write a short task configuration file from\nwhich the relevant web interfaces and model hosting infrastructure are\nautomatically generated. The system is available at https://dynabench.org/ and\nthe full library can be found at https://github.com/facebookresearch/dynabench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirumala_K/0/1/0/all/0/1\">Kushal Tirumala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pedro Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kane_T/0/1/0/all/0/1\">Tariq Kane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rojas_W/0/1/0/all/0/1\">William Gaviria Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattson_P/0/1/0/all/0/1\">Peter Mattson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Intent Classification with Off-the-shelf Large Language Models. (arXiv:2204.01959v1 [cs.CL])","link":"http://arxiv.org/abs/2204.01959","description":"<p>Data augmentation is a widely employed technique to alleviate the problem of\ndata scarcity. In this work, we propose a prompting-based approach to generate\nlabelled training data for intent classification with off-the-shelf language\nmodels (LMs) such as GPT-3. An advantage of this method is that no\ntask-specific LM-fine-tuning for data generation is required; hence the method\nrequires no hyper-parameter tuning and is applicable even when the available\ntraining data is very scarce. We evaluate the proposed method in a few-shot\nsetting on four diverse intent classification tasks. We find that GPT-generated\ndata significantly boosts the performance of intent classifiers when intents in\nconsideration are sufficiently distinct from each other. In tasks with\nsemantically close intents, we observe that the generated data is less helpful.\nOur analysis shows that this is because GPT often generates utterances that\nbelong to a closely-related intent instead of the desired one. We present\npreliminary evidence that a prompting-based GPT classifier could be helpful in\nfiltering the generated data to enhance its quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1\">Gaurav Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1\">Pau Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1\">Issam H. Laradji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atighehchian_P/0/1/0/all/0/1\">Parmida Atighehchian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The COVMis-Stance dataset: Stance Detection on Twitter for COVID-19 Misinformation. (arXiv:2204.02000v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02000","description":"<p>During the COVID-19 pandemic, large amounts of COVID-19 misinformation are\nspreading on social media. We are interested in the stance of Twitter users\ntowards COVID-19 misinformation. However, due to the relative recent nature of\nthe pandemic, only a few stance detection datasets fit our task. We have\nconstructed a new stance dataset consisting of 2631 tweets annotated with the\nstance towards COVID-19 misinformation. In contexts with limited labeled data,\nwe fine-tune our models by leveraging the MNLI dataset and two existing stance\ndetection datasets (RumourEval and COVIDLies), and evaluate the model\nperformance on our dataset. Our experimental results show that the model\nperforms the best when fine-tuned sequentially on the MNLI dataset and the\ncombination of the undersampled RumourEval and COVIDLies datasets. Our code and\ndataset are publicly available at\nhttps://github.com/yanfangh/covid-rumor-stance\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yanfang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Putten_P/0/1/0/all/0/1\">Peter van der Putten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fact Checking with Insufficient Evidence. (arXiv:2204.02007v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02007","description":"<p>Automating the fact checking (FC) process relies on information obtained from\nexternal sources. In this work, we posit that it is crucial for FC models to\nmake veracity predictions only when there is sufficient evidence and otherwise\nindicate when it is not enough. To this end, we are the first to study what\ninformation FC models consider sufficient by introducing a novel task and\nadvancing it with three main contributions. First, we conduct an in-depth\nempirical analysis of the task with a new fluency-preserving method for\nomitting information from the evidence at the constituent and sentence level.\nWe identify when models consider the remaining evidence (in)sufficient for FC,\nbased on three trained models with different Transformer architectures and\nthree FC datasets. Second, we ask annotators whether the omitted evidence was\nimportant for FC, resulting in a novel diagnostic dataset, SufficientFacts, for\nFC with omitted evidence. We find that models are least successful in detecting\nmissing evidence when adverbial modifiers are omitted (21% accuracy), whereas\nit is easiest for omitted date modifiers (63% accuracy). Finally, we propose a\nnovel data augmentation strategy for contrastive self-learning of missing\nevidence by employing the proposed omission method combined with tri-training.\nIt improves performance for Evidence Sufficiency Prediction by up to 17.8 F1\nscore, which in turn improves FC performance by up to 2.6 F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonsen_J/0/1/0/all/0/1\">Jakob Grue Simonsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioma_C/0/1/0/all/0/1\">Christina Lioma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Complementary Joint Training Approach Using Unpaired Speech and Text for Low-Resource Automatic Speech Recognition. (arXiv:2204.02023v1 [cs.SD])","link":"http://arxiv.org/abs/2204.02023","description":"<p>Unpaired data has shown to be beneficial for low-resource automatic speech\nrecognition~(ASR), which can be involved in the design of hybrid models with\nmulti-task training or language model dependent pre-training. In this work, we\nleverage unpaired data to train a general sequence-to-sequence model. Unpaired\nspeech and text are used in the form of data pairs by generating the\ncorresponding missing parts in prior to model training. Inspired by the\ncomplementarity of speech-PseudoLabel pair and SynthesizedAudio-text pair in\nboth acoustic features and linguistic features, we propose a complementary\njoint training~(CJT) method that trains a model alternatively with two data\npairs. Furthermore, label masking for pseudo-labels and gradient restriction\nfor synthesized audio are proposed to further cope with the deviations from\nreal data, termed as CJT++. Experimental results show that compared to\nspeech-only training, the proposed basic CJT achieves great performance\nimprovements on clean/other test sets, and the CJT++ re-training yields further\nperformance enhancements. It is also apparent that the proposed method\noutperforms the wav2vec2.0 model with the same model size and beam size,\nparticularly in extreme low-resource cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Ye-Qian Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiu-Shi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Li-Rong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming-Hui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1\">Xin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhou-Wang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\textit{latent}$-GLAT: Glancing at Latent Variables for Parallel Text Generation. (arXiv:2204.02030v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02030","description":"<p>Recently, parallel text generation has received widespread attention due to\nits success in generation efficiency. Although many advanced techniques are\nproposed to improve its generation quality, they still need the help of an\nautoregressive model for training to overcome the one-to-many multi-modal\nphenomenon in the dataset, limiting their applications. In this paper, we\npropose $\\textit{latent}$-GLAT, which employs the discrete latent variables to\ncapture word categorical information and invoke an advanced curriculum learning\ntechnique, alleviating the multi-modality problem. Experiment results show that\nour method outperforms strong baselines without the help of an autoregressive\nmodel, which further broadens the application scenarios of the parallel\ndecoding paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yu Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lihua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xinyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperBox: A Supervised Approach for Hypernym Discovery using Box Embeddings. (arXiv:2204.02058v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02058","description":"<p>Hypernymy plays a fundamental role in many AI tasks like taxonomy learning,\nontology learning, etc. This has motivated the development of many automatic\nidentification methods for extracting this relation, most of which rely on word\ndistribution. We present a novel model HyperBox to learn box embeddings for\nhypernym discovery. Given an input term, HyperBox retrieves its suitable\nhypernym from a target corpus. For this task, we use the dataset published for\nSemEval 2018 Shared Task on Hypernym Discovery. We compare the performance of\nour model on two specific domains of knowledge: medical and music.\nExperimentally, we show that our model outperforms existing methods on the\nmajority of the evaluation metrics. Moreover, our model generalize well over\nunseen hypernymy pairs using only a small set of training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Maulik Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_D/0/1/0/all/0/1\">Dr. Apurva Narayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design considerations for a hierarchical semantic compositional framework for medical natural language understanding. (arXiv:2204.02067v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02067","description":"<p>Medical natural language processing (NLP) systems are a key enabling\ntechnology for transforming Big Data from clinical report repositories to\ninformation used to support disease models and validate intervention methods.\nHowever, current medical NLP systems fall considerably short when faced with\nthe task of logically interpreting clinical text. In this paper, we describe a\nframework inspired by mechanisms of human cognition in an attempt to jump the\nNLP performance curve. The design centers about a hierarchical semantic\ncompositional model (HSCM) which provides an internal substrate for guiding the\ninterpretation process. The paper describes insights from four key cognitive\naspects including semantic memory, semantic composition, semantic activation,\nand hierarchical predictive coding. We discuss the design of a generative\nsemantic model and an associated semantic parser used to transform a free-text\nsentence into a logical representation of its meaning. The paper discusses\nsupportive and antagonistic arguments for the key features of the architecture\nas a long-term foundational framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taira_R/0/1/0/all/0/1\">Ricky K. Taira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garlid_A/0/1/0/all/0/1\">Anders O. Garlid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speier_W/0/1/0/all/0/1\">William Speier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How do media talk about the Covid-19 pandemic? Metaphorical thematic clustering in Italian online newspapers. (arXiv:2204.02106v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02106","description":"<p>The contribution presents a study on figurative language of the first months\nof the COVID-19 crisis in Italian online newspapers. Particularly, we contrast\ntopics and metaphorical language used by journalists in the first and second\nphase of the government response to the pandemic in Spring 2020. The analysis\nis conducted on a journalistic corpus collected between February 24th and June\n3rd, 2020. The analysis is performed using both quantitative and qualitative\napproaches, combining Structural Topic Modelling (Roberts et al. 2016),\nConceptual Metaphor Theory (Lakoff &amp; Johnson, 1980), and qualitative-corpus\nbased metaphor analysis (Charteris-Black, 2004). We find a significant shift in\ntopics discussed across Phase 1 and Phase 2, and interesting overlaps in\ntopic-specific metaphors. Using qualitative corpus analysis, we present a more\nin-depth case study discussing metaphorical collocations of the topics of\nEconomy and Society\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Busso_L/0/1/0/all/0/1\">Lucia Busso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tordini_O/0/1/0/all/0/1\">Ottavia Tordini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved and Efficient Conversational Slot Labeling through Question Answering. (arXiv:2204.02123v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02123","description":"<p>Transformer-based pretrained language models (PLMs) offer unmatched\nperformance across the majority of natural language understanding (NLU) tasks,\nincluding a body of question answering (QA) tasks. We hypothesize that\nimprovements in QA methodology can also be directly exploited in dialog NLU;\nhowever, dialog tasks must be \\textit{reformatted} into QA tasks. In\nparticular, we focus on modeling and studying \\textit{slot labeling} (SL), a\ncrucial component of NLU for dialog, through the QA optics, aiming to improve\nboth its performance and efficiency, and make it more effective and resilient\nto working with limited task data. To this end, we make a series of\ncontributions: 1) We demonstrate how QA-tuned PLMs can be applied to the SL\ntask, reaching new state-of-the-art performance, with large gains especially\npronounced in such low-data regimes. 2) We propose to leverage contextual\ninformation, required to tackle ambiguous values, simply through natural\nlanguage. 3) Efficiency and compactness of QA-oriented fine-tuning are boosted\nthrough the use of lightweight yet effective adapter modules. 4) Trading-off\nsome of the quality of QA datasets for their size, we experiment with larger\nautomatically generated QA datasets for QA-tuning, arriving at even higher\nperformance. Finally, our analysis suggests that our novel QA-based slot\nlabeling models, supported by the PLMs, reach a performance ceiling in\nhigh-data regimes, calling for more challenging and more nuanced benchmarks in\nfuture work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuisz_G/0/1/0/all/0/1\">Gabor Fuisz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibbons_S/0/1/0/all/0/1\">Samuel Gibbons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1\">Inigo Casanueva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1\">Pawe&#x142; Budzianowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Anchors' Opinion in Hinghlish News Delivery. (arXiv:2204.02155v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02155","description":"<p>Humans like to express their opinions and crave the opinions of others.\nMining and detecting opinions from various sources are beneficial to\nindividuals, organisations, and even governments. One such organisation is news\nmedia, where a general norm is not to showcase opinions from their side.\nAnchors are the face of the digital media, and it is required for them not to\nbe opinionated. However, at times, they diverge from the accepted norm and\ninsert their opinions into otherwise straightforward news reports, either\npurposefully or unintentionally. This is primarily seen in debates as it\nrequires the anchors to be spontaneous, thus making them vulnerable to add\ntheir opinions. The consequence of such mishappening might lead to biased news\nor even supporting a certain agenda at the worst. To this end, we propose a\nnovel task of anchors' opinion detection in debates. We curate code-mixed news\ndebates and develop the ODIN dataset. A total of 2054 anchors' utterances in\nthe dataset are marked as opinionated or non-opinionated. Lastly, we propose\nDetONADe, an interactive attention-based framework for classifying anchors'\nutterances and obtain the best weighted-F1 score of 0.703. A thorough analysis\nand evaluation show many interesting patterns in the dataset and predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadhwani_S/0/1/0/all/0/1\">Siddharth Sadhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_N/0/1/0/all/0/1\">Nishant Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilinguals at SemEval-2022 Task 11: Transformer Based Architecture for Complex NER. (arXiv:2204.02173v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02173","description":"<p>We investigate the task of complex NER for the English language. The task is\nnon-trivial due to the semantic ambiguity of the textual structure and the\nrarity of occurrence of such entities in the prevalent literature. Using\npre-trained language models such as BERT, we obtain a competitive performance\non this task. We qualitatively analyze the performance of multiple\narchitectures for this task. All our models are able to outperform the baseline\nby a significant margin. Our best performing model beats the baseline F1-score\nby over 9%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Amit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daw_S/0/1/0/all/0/1\">Swayatta Daw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pudi_V/0/1/0/all/0/1\">Vikram Pudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Transformer for 3D Visual Grounding. (arXiv:2204.02174v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02174","description":"<p>The 3D visual grounding task aims to ground a natural language description to\nthe targeted object in a 3D scene, which is usually represented in 3D point\nclouds. Previous works studied visual grounding under specific views. The\nvision-language correspondence learned by this way can easily fail once the\nview changes. In this paper, we propose a Multi-View Transformer (MVT) for 3D\nvisual grounding. We project the 3D scene to a multi-view space, in which the\nposition information of the 3D scene under different views are modeled\nsimultaneously and aggregated together. The multi-view space enables the\nnetwork to learn a more robust multi-modal representation for 3D visual\ngrounding and eliminates the dependence on specific views. Extensive\nexperiments show that our approach significantly outperforms all\nstate-of-the-art methods. Specifically, on Nr3D and Sr3D datasets, our method\noutperforms the best competitor by 11.2% and 7.1% and even surpasses recent\nwork with extra 2D assistance by 5.9% and 6.6%. Our code is available at\nhttps://github.com/sega-hsj/MVT-3DVG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstractive summarization of hospitalisation histories with transformer networks. (arXiv:2204.02208v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02208","description":"<p>In this paper we present a novel approach to abstractive summarization of\npatient hospitalisation histories. We applied an encoder-decoder framework with\nLongformer neural network as an encoder and BERT as a decoder. Our experiments\nshow improved quality on some summarization tasks compared with\npointer-generator networks. We also conducted a study with experienced\nphysicians evaluating the results of our model in comparison with PGN baseline\nand human-generated abstracts, which showed the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yalunin_A/0/1/0/all/0/1\">Alexander Yalunin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umerenkov_D/0/1/0/all/0/1\">Dmitriy Umerenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EntSUM: A Data Set for Entity-Centric Summarization. (arXiv:2204.02213v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02213","description":"<p>Controllable summarization aims to provide summaries that take into account\nuser-specified aspects and preferences to better assist them with their\ninformation need, as opposed to the standard summarization setup which build a\nsingle generic summary of a document. We introduce a human-annotated data set\nEntSUM for controllable summarization with a focus on named entities as the\naspects to control. We conduct an extensive quantitative analysis to motivate\nthe task of entity-centric summarization and show that existing methods for\ncontrollable summarization fail to generate entity-centric summaries. We\npropose extensions to state-of-the-art summarization approaches that achieve\nsubstantially better results on our data set. Our analysis and results show the\nchallenging nature of this task and of the proposed data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_M/0/1/0/all/0/1\">Mayank Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preotiuc_Pietro_D/0/1/0/all/0/1\">Daniel Preotiuc-Pietro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors. (arXiv:2204.02261v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02261","description":"<p>Robustness of machine learning models on ever-changing real-world data is\ncritical, especially for applications affecting human well-being such as\ncontent moderation. New kinds of abusive language continually emerge in online\ndiscussions in response to current events (e.g., COVID-19), and the deployed\nabuse detection systems should be updated regularly to remain accurate. In this\npaper, we show that general abusive language classifiers tend to be fairly\nreliable in detecting out-of-domain explicitly abusive utterances but fail to\ndetect new types of more subtle, implicit abuse. Next, we propose an\ninterpretability technique, based on the Testing Concept Activation Vector\n(TCAV) method from computer vision, to quantify the sensitivity of a trained\nmodel to the human-defined concepts of explicit and implicit abusive language,\nand use that to explain the generalizability of the model on new data, in this\ncase, COVID-related anti-Asian hate speech. Extending this technique, we\nintroduce a novel metric, Degree of Explicitness, for a single instance and\nshow that the new metric is beneficial in suggesting out-of-domain unlabeled\nexamples to effectively enrich the training data with informative, implicitly\nabusive texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1\">Isar Nejadgholi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1\">Kathleen C. Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1\">Svetlana Kiritchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual and Multimodal Abuse Detection. (arXiv:2204.02263v1 [eess.AS])","link":"http://arxiv.org/abs/2204.02263","description":"<p>The presence of abusive content on social media platforms is undesirable as\nit severely impedes healthy and safe social media interactions. While automatic\nabuse detection has been widely explored in textual domain, audio abuse\ndetection still remains unexplored. In this paper, we attempt abuse detection\nin conversational audio from a multimodal perspective in a multilingual social\nmedia setting. Our key hypothesis is that along with the modelling of audio,\nincorporating discriminative information from other modalities can be highly\nbeneficial for this task. Our proposed method, MADA, explicitly focuses on two\nmodalities other than the audio itself, namely, the underlying emotions\nexpressed in the abusive audio and the semantic information encapsulated in the\ncorresponding textual form. Observations prove that MADA demonstrates gains\nover audio-only approaches on the ADIMA dataset. We test the proposed approach\non 10 different languages and observe consistent gains in the range 0.6%-5.2%\nby leveraging multiple modalities. We also perform extensive ablation\nexperiments for studying the contributions of every modality and observe the\nbest results while leveraging all the modalities together. Additionally, we\nperform experiments to empirically confirm that there is a strong correlation\nbetween underlying emotions and abusive behaviour.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sharon_R/0/1/0/all/0/1\">Rini Sharon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_H/0/1/0/all/0/1\">Heet Shah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukherjee_D/0/1/0/all/0/1\">Debdoot Mukherjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_V/0/1/0/all/0/1\">Vikram Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Repeat after me: Self-supervised learning of acoustic-to-articulatory mapping by vocal imitation. (arXiv:2204.02269v1 [cs.SD])","link":"http://arxiv.org/abs/2204.02269","description":"<p>We propose a computational model of speech production combining a pre-trained\nneural articulatory synthesizer able to reproduce complex speech stimuli from a\nlimited set of interpretable articulatory parameters, a DNN-based internal\nforward model predicting the sensory consequences of articulatory commands, and\nan internal inverse model based on a recurrent neural network recovering\narticulatory commands from the acoustic speech input. Both forward and inverse\nmodels are jointly trained in a self-supervised way from raw acoustic-only\nspeech data from different speakers. The imitation simulations are evaluated\nobjectively and subjectively and display quite encouraging performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georges_M/0/1/0/all/0/1\">Marc-Antoine Georges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diard_J/0/1/0/all/0/1\">Julien Diard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1\">Laurent Girin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_J/0/1/0/all/0/1\">Jean-Luc Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hueber_T/0/1/0/all/0/1\">Thomas Hueber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering. (arXiv:2204.02285v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02285","description":"<p>While Visual Question Answering (VQA) has progressed rapidly, previous works\nraise concerns about robustness of current VQA models. In this work, we study\nthe robustness of VQA models from a novel perspective: visual context. We\nsuggest that the models over-rely on the visual context, i.e., irrelevant\nobjects in the image, to make predictions. To diagnose the model's reliance on\nvisual context and measure their robustness, we propose a simple yet effective\nperturbation technique, SwapMix. SwapMix perturbs the visual context by\nswapping features of irrelevant context objects with features from other\nobjects in the dataset. Using SwapMix we are able to change answers to more\nthan 45 % of the questions for a representative VQA model. Additionally, we\ntrain the models with perfect sight and find that the context over-reliance\nhighly depends on the quality of visual representations. In addition to\ndiagnosing, SwapMix can also be applied as a data augmentation strategy during\ntraining in order to regularize the context over-reliance. By swapping the\ncontext object features, the model reliance on context can be suppressed\neffectively. Two representative VQA models are studied using SwapMix: a\nco-attention model MCAN and a large-scale pretrained model LXMERT. Our\nexperiments on the popular GQA dataset show the effectiveness of SwapMix for\nboth diagnosing model robustness and regularizing the over-reliance on visual\ncontext. The code for our method is available at\nhttps://github.com/vipulgupta1011/swapmix\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vipul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval. (arXiv:2204.02292v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02292","description":"<p>State-of-the-art neural (re)rankers are notoriously data hungry which - given\nthe lack of large-scale training data in languages other than English - makes\nthem rarely used in multilingual and cross-lingual retrieval settings. Current\napproaches therefore typically transfer rankers trained on English data to\nother languages and cross-lingual setups by means of multilingual encoders:\nthey fine-tune all the parameters of a pretrained massively multilingual\nTransformer (MMT, e.g., multilingual BERT) on English relevance judgments and\nthen deploy it in the target language. In this work, we show that two\nparameter-efficient approaches to cross-lingual transfer, namely Sparse\nFine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more\neffective zero-shot transfer to multilingual and cross-lingual retrieval tasks.\nWe first train language adapters (or SFTMs) via Masked Language Modelling and\nthen train retrieval (i.e., reranking) adapters (SFTMs) on top while keeping\nall other parameters fixed. At inference, this modular design allows us to\ncompose the ranker by applying the task adapter (or SFTM) trained with source\nlanguage data together with the language adapter (or SFTM) of a target\nlanguage. Besides improved transfer performance, these two approaches offer\nfaster ranker training, with only a fraction of parameters being updated\ncompared to full MMT fine-tuning. We benchmark our models on the CLEF-2003\nbenchmark, showing that our parameter-efficient methods outperform standard\nzero-shot transfer with full MMT fine-tuning, while enabling modularity and\nreducing training times. Further, we show on the example of Swahili and Somali\nthat, for low(er)-resource languages, our parameter-efficient neural re-rankers\ncan improve the ranking of the competitive machine translation-based ranker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Litschko_R/0/1/0/all/0/1\">Robert Litschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaLM: Scaling Language Modeling with Pathways. (arXiv:2204.02311v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02311","description":"<p>Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barham_P/0/1/0/all/0/1\">Paul Barham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuh_P/0/1/0/all/0/1\">Parker Schuh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kensen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvyashchenko_S/0/1/0/all/0/1\">Sasha Tsvyashchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Abhishek Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_P/0/1/0/all/0/1\">Parker Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1\">Ben Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pope_R/0/1/0/all/0/1\">Reiner Pope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1\">James Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isard_M/0/1/0/all/0/1\">Michael Isard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_Ari_G/0/1/0/all/0/1\">Guy Gur-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_T/0/1/0/all/0/1\">Toju Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levskaya_A/0/1/0/all/0/1\">Anselm Levskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghemawat_S/0/1/0/all/0/1\">Sanjay Ghemawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_L/0/1/0/all/0/1\">Liam Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_D/0/1/0/all/0/1\">David Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyeontaek Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiridonov_A/0/1/0/all/0/1\">Alexander Spiridonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sepassi_R/0/1/0/all/0/1\">Ryan Sepassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shivani Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omernick_M/0/1/0/all/0/1\">Mark Omernick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_T/0/1/0/all/0/1\">Thanumalayan Sankaranarayana Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellat_M/0/1/0/all/0/1\">Marie Pellat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_E/0/1/0/all/0/1\">Erica Moreira</a>, et al. (15 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can language models learn from explanations in context?. (arXiv:2204.02329v1 [cs.CL])","link":"http://arxiv.org/abs/2204.02329","description":"<p>Large language models can perform new tasks by adapting to a few in-context\nexamples. For humans, rapid learning from examples can benefit from\nexplanations that connect examples to task principles. We therefore investigate\nwhether explanations of few-shot examples can allow language models to adapt\nmore effectively. We annotate a set of 40 challenging tasks from BIG-Bench with\nexplanations of answers to a small subset of questions, as well as a variety of\nmatched control explanations. We evaluate the effects of various zero-shot and\nfew-shot prompts that include different types of explanations, instructions,\nand controls on the performance of a range of large language models. We analyze\nthese results using statistical multilevel modeling techniques that account for\nthe nested dependencies among conditions, tasks, prompts, and models. We find\nthat explanations of examples can improve performance. Adding untuned\nexplanations to a few-shot prompt offers a modest improvement in performance;\nabout 1/3 the effect size of adding few-shot examples, but twice the effect\nsize of task instructions. We then show that explanations tuned for performance\non a small validation set offer substantially larger benefits; building a\nprompt by selecting examples and explanations together substantially improves\nperformance over selecting examples alone. Hand-tuning explanations can\nsubstantially improve performance on challenging tasks. Furthermore, even\nuntuned explanations outperform carefully matched controls, suggesting that the\nbenefits are due to the link between an example and its explanation, rather\nthan lower-level features of the language used. However, only large models can\nbenefit from explanations. In summary, explanations can support the in-context\nlearning abilities of large language models on\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew K. Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">Stephanie C. Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthewson_K/0/1/0/all/0/1\">Kory Matthewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessler_M/0/1/0/all/0/1\">Michael Henry Tessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creswell_A/0/1/0/all/0/1\">Antonia Creswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McClelland_J/0/1/0/all/0/1\">James L. McClelland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Best Practices for Training Multilingual Dense Retrieval Models. (arXiv:2204.02363v1 [cs.IR])","link":"http://arxiv.org/abs/2204.02363","description":"<p>Dense retrieval models using a transformer-based bi-encoder design have\nemerged as an active area of research. In this work, we focus on the task of\nmonolingual retrieval in a variety of typologically diverse languages using one\nsuch design. Although recent work with multilingual transformers demonstrates\nthat they exhibit strong cross-lingual generalization capabilities, there\nremain many open research questions, which we tackle here. Our study is\norganized as a \"best practices\" guide for training multilingual dense retrieval\nmodels, broken down into three main scenarios: where a multilingual transformer\nis available, but relevance judgments are not available in the language of\ninterest; where both models and training data are available; and, where\ntraining data are available not but models. In considering these scenarios, we\ngain a better understanding of the role of multi-stage fine-tuning, the\nstrength of cross-lingual transfer under various conditions, the usefulness of\nout-of-language data, and the advantages of multilingual vs. monolingual\ntransformers. Our recommendations offer a guide for practitioners building\nsearch applications, particularly for low-resource languages, and while our\nwork leaves open a number of research questions, we provide a solid foundation\nfor future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1\">Kelechi Ogueji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xueguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jimmy Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations. (arXiv:2204.02380v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02380","description":"<p>Providing explanations in the context of Visual Question Answering (VQA)\npresents a fundamental problem in machine learning. To obtain detailed insights\ninto the process of generating natural language explanations for VQA, we\nintroduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with\nnatural language explanations. For each image-question pair in the CLEVR\ndataset, CLEVR-X contains multiple structured textual explanations which are\nderived from the original scene graphs. By construction, the CLEVR-X\nexplanations are correct and describe the reasoning and visual information that\nis necessary to answer a given question. We conducted a user study to confirm\nthat the ground-truth explanations in our proposed dataset are indeed complete\nand relevant. We present baseline results for generating natural language\nexplanations in the context of VQA using two state-of-the-art frameworks on the\nCLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation\ngeneration quality for different question and answer types. Additionally, we\nstudy the influence of using different numbers of ground-truth explanations on\nthe convergence of natural language generation (NLG) metrics. The CLEVR-X\ndataset is publicly available at\n\\url{https://explainableml.github.io/CLEVR-X/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1\">A. Sophia Koepke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory limitations are hidden in grammar. (arXiv:1908.06629v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1908.06629","description":"<p>The ability to produce and understand an unlimited number of different\nsentences is a hallmark of human language. Linguists have sought to define the\nessence of this generative capacity using formal grammars that describe the\nsyntactic dependencies between constituents, independent of the computational\nlimitations of the human brain. Here, we evaluate this independence assumption\nby sampling sentences uniformly from the space of possible syntactic\nstructures. We find that the average dependency distance between syntactically\nrelated words, a proxy for memory limitations, is less than expected by chance\nin a collection of state-of-the-art classes of dependency grammars. Our\nfindings indicate that memory limitations have permeated grammatical\ndescriptions, suggesting that it may be impossible to build a parsimonious\ntheory of human linguistic productivity independent of non-linguistic cognitive\nconstraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiansen_M/0/1/0/all/0/1\">Morten H. Christiansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the grammar of drug prescription: recurrent neural network grammars for medication information extraction in clinical texts. (arXiv:2004.11622v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.11622","description":"<p>In this study, we evaluated the RNNG, a neural top-down transition based\nparser, for medication information extraction in clinical texts. We evaluated\nthis model on a French clinical corpus. The task was to extract the name of a\ndrug (or a drug class), as well as attributes informing its administration:\nfrequency, dosage, duration, condition and route of administration. We compared\nthe RNNG model that jointly identifies entities, events and their relations\nwith separate BiLSTMs models for entities, events and relations as baselines.\nWe call seq-BiLSTMs the baseline models for relations extraction that takes as\nextra-input the output of the BiLSTMs for entities and events. Similarly, we\nevaluated seq-RNNG, a hybrid RNNG model that takes as extra-input the output of\nthe BiLSTMs for entities and events. RNNG outperforms seq-BiLSTM for\nidentifying complex relations, with on average 88.1 [84.4-91.6] % versus 69.9\n[64.0-75.4] F-measure. However, RNNG tends to be weaker than the baseline\nBiLSTM on detecting entities, with on average 82.4 [80.8-83.8] versus 84.1\n[82.7-85.6] % F- measure. RNNG trained only for detecting relations tends to be\nweaker than RNNG with the joint modelling objective, 87.4% [85.8-88.8] versus\n88.5% [87.2-89.8]. Seq-RNNG is on par with BiLSTM for entities (84.0\n[82.6-85.4] % F-measure) and with RNNG for relations (88.7 [87.4-90.0] %\nF-measure). The performance of RNNG on relations can be explained both by the\nmodel architecture, which provides inductive bias to capture the hierarchy in\nthe targets, and the joint modeling objective which allows the RNNG to learn\nricher representations. RNNG is efficient for modeling relations between\nentities or/and events in medical texts and its performances are close to those\nof a BiLSTM for entity and event detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lerner_I/0/1/0/all/0/1\">Ivan Lerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouffroy_J/0/1/0/all/0/1\">Jordan Jouffroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgun_A/0/1/0/all/0/1\">Anita Burgun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuraz_A/0/1/0/all/0/1\">Antoine Neuraz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.06876","description":"<p>The current best practice for computing optimal transport (OT) is via entropy\nregularization and Sinkhorn iterations. This algorithm runs in quadratic time\nas it requires the full pairwise cost matrix, which is prohibitively expensive\nfor large sets of objects. In this work we propose two effective log-linear\ntime approximations of the cost matrix: First, a sparse approximation based on\nlocality-sensitive hashing (LSH) and, second, a Nystr\\\"om approximation with\nLSH-based sparse corrections, which we call locally corrected Nystr\\\"om (LCN).\nThese approximations enable general log-linear time algorithms for\nentropy-regularized OT that perform well even for the complex, high-dimensional\nspaces common in deep learning. We analyse these approximations theoretically\nand evaluate them experimentally both directly and end-to-end as a component\nfor real-world applications. Using our approximations for unsupervised word\nembedding alignment enables us to speed up a state-of-the-art method by a\nfactor of 3 while also improving the accuracy by 3.1 percentage points without\nany additional model changes. For graph distance regression we propose the\ngraph transport network (GTN), which combines graph neural networks (GNNs) with\nenhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales\nlog-linearly in the number of nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gasteiger_J/0/1/0/all/0/1\">Johannes Gasteiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienen_M/0/1/0/all/0/1\">Marten Lienen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MarIA: Spanish Language Models. (arXiv:2107.07253v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07253","description":"<p>This work presents MarIA, a family of Spanish language models and associated\nresources made available to the industry and the research community. Currently,\nMarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large Spanish\nlanguage models, which can arguably be presented as the largest and most\nproficient language models in Spanish. The models were pretrained using a\nmassive corpus of 570GB of clean and deduplicated texts with 135 billion words\nextracted from the Spanish Web Archive crawled by the National Library of Spain\nbetween 2009 and 2019. We assessed the performance of the models with nine\nexisting evaluation datasets and with a novel extractive Question Answering\ndataset created ex novo. Overall, MarIA models outperform the existing Spanish\nmodels across a variety of NLU tasks and training settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. (arXiv:2108.04539v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04539","description":"<p>Key information extraction (KIE) from document images requires understanding\nthe contextual and spatial semantics of texts in two-dimensional (2D) space.\nMany recent studies try to solve the task by developing pre-trained language\nmodels focusing on combining visual features from document images with texts\nand their layout. On the other hand, this paper tackles the problem by going\nback to the basic: effective combination of text and layout. Specifically, we\npropose a pre-trained language model, named BROS (BERT Relying On Spatiality),\nthat encodes relative positions of texts in 2D space and learns from unlabeled\ndocuments with area-masking strategy. With this optimized training scheme for\nunderstanding texts in 2D space, BROS shows comparable or better performance\ncompared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and\nSciTSR) without relying on visual features. This paper also reveals two\nreal-world challenges in KIE tasks-(1) minimizing the error from incorrect text\nordering and (2) efficient learning from fewer downstream examples-and\ndemonstrates the superiority of BROS over previous methods. Code is available\nat https://github.com/clovaai/bros.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing task-oriented and open-domain dialogues in conversational agents. (arXiv:2109.04137v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04137","description":"<p>The goal of building intelligent dialogue systems has largely been separately\npursued under two paradigms: task-oriented dialogue (TOD) systems, which\nperform goal-oriented functions, and open-domain dialogue (ODD) systems, which\nfocus on non-goal-oriented chitchat. The two dialogue modes can potentially be\nintertwined together seamlessly in the same conversation, as easily done by a\nfriendly human assistant. Such ability is desirable in conversational agents,\nas the integration makes them more accessible and useful. Our paper addresses\nthis problem of fusing TODs and ODDs in multi-turn dialogues. Based on the\npopular TOD dataset MultiWOZ, we build a new dataset FusedChat, by rewriting\nthe existing TOD turns and adding new ODD turns. This procedure constructs\nconversation sessions containing exchanges from both dialogue modes. It\nfeatures inter-mode contextual dependency, i.e., the dialogue turns from the\ntwo modes depend on each other. Rich dependency patterns including co-reference\nand ellipsis are features. The new dataset, with 60k new human-written ODD\nturns and 5k re-written TOD turns, offers a benchmark to test a dialogue\nmodel's ability to perform inter-mode conversations. This is a more challenging\ntask since the model has to determine the appropriate dialogue mode and\ngenerate the response based on the inter-mode context. But such models would\nbetter mimic human-level conversation capabilities. We evaluate baseline models\non this task, including classification-based two-stage models and two-in-one\nfused models. We publicly release FusedChat and the baselines to propel future\nwork on inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1\">Tom Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Frank Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandelea_V/0/1/0/all/0/1\">Vlad Pandelea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jinjie Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoEfication: Transformer Feed-forward Layers are Mixtures of Experts. (arXiv:2110.01786v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01786","description":"<p>Recent work has shown that feed-forward networks (FFNs) in pre-trained\nTransformers are a key component, storing various linguistic and factual\nknowledge. However, the computational patterns of FFNs are still unclear. In\nthis work, we study the computational patterns of FFNs and observe that most\ninputs only activate a tiny ratio of neurons of FFNs. This phenomenon is\nsimilar to the sparsity of the human brain, which drives research on functional\npartitions of the human brain. To verify whether functional partitions also\nemerge in FFNs, we propose to convert a model into its MoE version with the\nsame parameters, namely MoEfication. Specifically, MoEfication consists of two\nphases: (1) splitting the parameters of FFNs into multiple functional\npartitions as experts, and (2) building expert routers to decide which experts\nwill be used for each input. Experimental results show that MoEfication can\nconditionally use 10% to 30% of FFN parameters while maintaining over 95%\noriginal performance for different models on various downstream tasks. Besides,\nMoEfication brings two advantages: (1) it significantly reduces the FLOPS of\ninference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a\nfine-grained perspective to study the inner mechanism of FFNs. The source code\nof this paper can be obtained from https://github.com/thunlp/MoEfication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking Down Multilingual Machine Translation. (arXiv:2110.08130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08130","description":"<p>While multilingual training is now an essential ingredient in machine\ntranslation (MT) systems, recent work has demonstrated that it has different\neffects in different multilingual settings, such as many-to-one, one-to-many,\nand many-to-many learning. These training settings expose the encoder and the\ndecoder in a machine translation model with different data distributions. In\nthis paper, we examine how different varieties of multilingual training\ncontribute to learning these two components of the MT model. Specifically, we\ncompare bilingual models with encoders and/or decoders initialized by\nmultilingual training. We show that multilingual training is beneficial to\nencoders in general, while it only benefits decoders for low-resource languages\n(LRLs). We further find the important attention heads for each language pair\nand compare their correlations during inference. Our analysis sheds light on\nhow multilingual translation models work and enables us to propose methods to\nimprove performance by training with highly related languages. Our many-to-one\nmodels for high-resource languages and one-to-many models for LRL outperform\nthe best results reported by Aharoni et al. (2019)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm. (arXiv:2110.08190v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08190","description":"<p>Conventional wisdom in pruning Transformer-based language models is that\npruning reduces the model expressiveness and thus is more likely to underfit\nrather than overfit. However, under the trending pretrain-and-finetune\nparadigm, we postulate a counter-traditional hypothesis, that is: pruning\nincreases the risk of overfitting when performed at the fine-tuning phase. In\nthis paper, we aim to address the overfitting problem and improve pruning\nperformance via progressive knowledge distillation with error-bound properties.\nWe show for the first time that reducing the risk of overfitting can help the\neffectiveness of pruning under the pretrain-and-finetune paradigm. Ablation\nstudies and experiments on the GLUE benchmark show that our method outperforms\nthe leading competitors across different tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1\">Ian E.H. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sung-en Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mimi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_S/0/1/0/all/0/1\">Sanguthevar Rajasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Generating Grounded Navigation Instructions from Landmarks. (arXiv:2111.12872v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12872","description":"<p>We study the automatic generation of navigation instructions from 360-degree\nimages captured on indoor routes. Existing generators suffer from poor visual\ngrounding, causing them to rely on language priors and hallucinate objects. Our\nMARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a\nfirst stage landmark detector and a second stage generator -- a multimodal,\nmultilingual, multitask encoder-decoder. To train it, we bootstrap grounded\nlandmark annotations on top of the Room-across-Room (RxR) dataset. Using text\nparsers, weak supervision from RxR's pose traces, and a multilingual image-text\nencoder trained on 1.8b images, we identify 971k English, Hindi and Telugu\nlandmark descriptions and ground them to specific regions in panoramas. On\nRoom-to-Room, human wayfinders obtain success rates (SR) of 71% following\nMARKY-MT5's instructions, just shy of their 75% SR following human instructions\n-- and well above SRs with other generators. Evaluations on RxR's longer,\ndiverse paths obtain 61-64% SRs on three languages. Generating such\nhigh-quality navigation instructions in novel environments is a step towards\nconversational navigation tools and could facilitate larger-scale training of\ninstruction-following agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_C/0/1/0/all/0/1\">Ceslee Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbay_J/0/1/0/all/0/1\">Jordi Orbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1\">Vighnesh Birodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1\">Izzeddin Gur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Directed Speech Separation for Automatic Speech Recognition of Long Form Conversational Speech. (arXiv:2112.05863v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2112.05863","description":"<p>Many of the recent advances in speech separation are primarily aimed at\nsynthetic mixtures of short audio utterances with high degrees of overlap. Most\nof these approaches need an additional stitching step to stitch the separated\nspeech chunks for long form audio. Since most of the approaches involve\nPermutation Invariant training (PIT), the order of separated speech chunks is\nnondeterministic and leads to difficulty in accurately stitching homogenous\nspeaker chunks for downstream tasks like Automatic Speech Recognition (ASR).\nAlso, most of these models are trained with synthetic mixtures and do not\ngeneralize to real conversational data. In this paper, we propose a speaker\nconditioned separator trained on speaker embeddings extracted directly from the\nmixed signal using an over-clustering based approach. This model naturally\nregulates the order of the separated chunks without the need for an additional\nstitching step. We also introduce a data sampling strategy with real and\nsynthetic mixtures which generalizes well to real conversation speech. With\nthis model and data sampling technique, we show significant improvements in\nspeaker-attributed word error rate (SA-WER) on Hub5 data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Paturi_R/0/1/0/all/0/1\">Rohit Paturi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sundararajan Srinivasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_Romero_D/0/1/0/all/0/1\">Daniel Garcia-Romero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis. (arXiv:2201.02026v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02026","description":"<p>In recent years, pretrained language models have revolutionized the NLP\nworld, while achieving state of the art performance in various downstream\ntasks. However, in many cases, these models do not perform well when labeled\ndata is scarce and the model is expected to perform in the zero or few shot\nsetting. Recently, several works have shown that continual pretraining or\nperforming a second phase of pretraining (inter-training) which is better\naligned with the downstream task, can lead to improved results, especially in\nthe scarce data setting. Here, we propose to leverage sentiment-carrying\ndiscourse markers to generate large-scale weakly-labeled data, which in turn\ncan be used to adapt language models for sentiment analysis. Extensive\nexperimental results show the value of our approach on various benchmark\ndatasets, including the finance domain. Code, models and data are available at\nhttps://github.com/ibm/tslm-discourse-markers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ein_Dor_L/0/1/0/all/0/1\">Liat Ein-Dor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnayderman_I/0/1/0/all/0/1\">Ilya Shnayderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_A/0/1/0/all/0/1\">Artem Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankin_L/0/1/0/all/0/1\">Lena Dankin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Agnostic Website Embedding and Classification. (arXiv:2201.03677v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.03677","description":"<p>Currently, publicly available models for website classification do not offer\nan embedding method and have limited support for languages beyond English. We\nrelease a dataset of more than two million category-labeled websites in 92\nlanguages collected from Curlie, the largest multilingual human-edited Web\ndirectory. The dataset contains 14 website categories aligned across languages.\nAlongside it, we introduce Homepage2Vec, a machine-learned pre-trained model\nfor classifying and embedding websites based on their homepage in a\nlanguage-agnostic way. Homepage2Vec, thanks to its feature set (textual\ncontent, metadata tags, and visual attributes) and recent progress in natural\nlanguage representation, is language-independent by design and generates\nembedding-based representations. We show that Homepage2Vec correctly classifies\nwebsites with a macro-averaged F1-score of 0.90, with stable performance across\nlow- as well as high-resource languages. Feature analysis shows that a small\nsubset of efficiently computable features suffices to achieve high performance\neven with limited computational resources. We make publicly available the\ncurated Curlie dataset aligned across languages, the pre-trained Homepage2Vec\nmodel, and libraries\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugeon_S/0/1/0/all/0/1\">Sylvain Lugeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccardi_T/0/1/0/all/0/1\">Tiziano Piccardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Context-Free Ambiguity of Emoji. (arXiv:2201.06302v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06302","description":"<p>Emojis come with prepacked semantics making them great candidates to create\nnew forms of more accessible communications. Yet, little is known about how\nmuch of this emojis semantic is agreed upon by humans, outside of textual\ncontexts. Thus, we collected a crowdsourced dataset of one-word emoji\ndescriptions for 1,289 emojis presented to participants with no surrounding\ntext. The emojis and their interpretations were then examined for ambiguity. We\nfind that with 30 annotations per emoji, 16 emojis (1.2%) are completely\nunambiguous, whereas 55 emojis (4.3%) are so ambiguous that their descriptions\nare indistinguishable from randomly chosen descriptions. Most of studied emojis\nare spread out between the two extremes. Furthermore, investigating the\nambiguity of different types of emojis, we find that an important factor is the\nextent to which an emoji has an embedded symbolical meaning drawn from an\nestablished code-book of symbols. We conclude by discussing design\nimplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Czestochowska_J/0/1/0/all/0/1\">Justyna Czestochowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1\">Kristina Gligoric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mentha_Y/0/1/0/all/0/1\">Yann Mentha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bien_M/0/1/0/all/0/1\">Michal Bien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grutter_A/0/1/0/all/0/1\">Andrea Grutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_A/0/1/0/all/0/1\">Anita Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xanthos_A/0/1/0/all/0/1\">Aris Xanthos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Hallucination in Natural Language Generation. (arXiv:2202.03629v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03629","description":"<p>Natural Language Generation (NLG) has improved exponentially in recent years\nthanks to the development of sequence-to-sequence deep learning technologies\nsuch as Transformer-based language models. This advancement has led to more\nfluent and coherent NLG, leading to improved development in downstream tasks\nsuch as abstractive summarization, dialogue generation and data-to-text\ngeneration. However, it is also apparent that deep learning based generation is\nprone to hallucinate unintended text, which degrades the system performance and\nfails to meet user expectations in many real-world scenarios. To address this\nissue, many studies have been presented in measuring and mitigating\nhallucinated texts, but these have never been reviewed in a comprehensive\nmanner before. In this survey, we thus provide a broad overview of the research\nprogress and challenges in the hallucination problem in NLG. The survey is\norganized into two parts: (1) a general overview of metrics, mitigation\nmethods, and future directions; and (2) an overview of task-specific research\nprogress on hallucinations in the following downstream tasks, namely\nabstractive summarization, dialogue generation, generative question answering,\ndata-to-text generation, and machine translation. This survey serves to\nfacilitate collaborative efforts among researchers in tackling the challenge of\nhallucinated texts in NLG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1\">Rita Frieske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bang_Y/0/1/0/all/0/1\">Yejin Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07427","description":"<p>Authors of posts in social media communicate their emotions and what causes\nthem with text and images. While there is work on emotion and stimulus\ndetection for each modality separately, it is yet unknown if the modalities\ncontain complementary emotion information in social media. We aim at filling\nthis research gap and contribute a novel, annotated corpus of English\nmultimodal Reddit posts. On this resource, we develop models to automatically\ndetect the relation between image and text, an emotion stimulus category and\nthe emotion class. We evaluate if these tasks require both modalities and find\nfor the image-text relations, that text alone is sufficient for most categories\n(complementary, illustrative, opposing): the information in the text allows to\npredict if an image is required for emotion understanding. The emotions of\nanger and sadness are best predicted with a multimodal model, while text alone\nis sufficient for disgust, joy, and surprise. Stimuli depicted by objects,\nanimals, food, or a person are best predicted by image-only models, while\nmultimodal models are most effective on art, events, memes, places, or\nscreenshots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khlyzova_A/0/1/0/all/0/1\">Anna Khlyzova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1\">Carina Silberer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"splink\" is happy and \"phrouth\" is scary: Emotion Intensity Analysis for Nonsense Words. (arXiv:2202.12132v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12132","description":"<p>People associate affective meanings to words - \"death\" is scary and sad while\n\"party\" is connotated with surprise and joy. This raises the question if the\nassociation is purely a product of the learned affective imports inherent to\nsemantic meanings, or is also an effect of other features of words, e.g.,\nmorphological and phonological patterns. We approach this question with an\nannotation-based analysis leveraging nonsense words. Specifically, we conduct a\nbest-worst scaling crowdsourcing study in which participants assign intensity\nscores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense\nwords and, for comparison of the results to previous work, to 68 real words.\nBased on this resource, we develop character-level and phonology-based\nintensity regressors. We evaluate them on both nonsense words and real words\n(making use of the NRC emotion intensity lexicon of 7493 words), across six\nemotion categories. The analysis of our data reveals that some phonetic\npatterns show clear differences between emotion intensities. For instance, s as\na first phoneme contributes to joy, sh to surprise, p as last phoneme more to\ndisgust than to anger and fear. In the modelling experiments, a regressor\ntrained on real words from the NRC emotion intensity lexicon shows a higher\nperformance (r = 0.17) than regressors that aim at learning the emotion\nconnotation purely from nonsense words. We conclude that humans do associate\naffective meaning to words based on surface patterns, but also based on\nsimilarities to existing words (\"juy\" to \"joy\", or \"flike\" to \"like\").\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabbatino_V/0/1/0/all/0/1\">Valentino Sabbatino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweitzer_A/0/1/0/all/0/1\">Antje Schweitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Sequence Generation with Adaptive Compositional Modules. (arXiv:2203.10652v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10652","description":"<p>Continual learning is essential for real-world deployment when there is a\nneed to quickly adapt the model to new tasks without forgetting knowledge of\nold tasks. Existing work on continual sequence generation either always reuses\nexisting parameters to learn new tasks, which is vulnerable to catastrophic\nforgetting on dissimilar tasks, or blindly adds new parameters for every new\ntask, which could prevent knowledge sharing between similar tasks. To get the\nbest of both worlds, in this work, we propose continual sequence generation\nwith adaptive compositional modules to adaptively add modules in transformer\narchitectures and compose both old and new modules for new tasks. We also\nincorporate pseudo experience replay to facilitate knowledge transfer in those\nshared modules. Experiment results on various sequences of generation tasks\nshow that our framework can adaptively add modules or reuse modules based on\ntask similarity, outperforming state-of-the-art baselines in terms of both\nperformance and parameter efficiency. We make our code public at\nhttps://github.com/GT-SALT/Adaptive-Compositional-Modules.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Speech Recognition Decoding via Layer Aggregation. (arXiv:2203.11325v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.11325","description":"<p>Recently proposed speech recognition systems are designed to predict using\nrepresentations generated by their top layers, employing greedy decoding which\nisolates each timestep from the rest of the sequence. Aiming for improved\nperformance, a beam search algorithm is frequently utilized and a language\nmodel is incorporated to assist with ranking the top candidates. In this work,\nwe experiment with several speech recognition models and find that logits\npredicted using the top layers may hamper beam search from achieving optimal\nresults. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield\nhighly confident predictions, and hypothesize that the predictions are based on\nlocal information and may not take full advantage of the information encoded in\nintermediate layers. To this end, we perform a layer analysis to reveal and\nvisualize how predictions evolve throughout the inference flow. We then propose\na prediction method that aggregates the top M layers, potentially leveraging\nuseful information encoded in intermediate layers and relaxing model\nconfidence. We showcase the effectiveness of our approach via beam search\ndecoding, conducting our experiments on Librispeech test and dev sets and\nachieving WER, and CER reduction of up to 10% and 22%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1\">Tomer Wullach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chazan_S/0/1/0/all/0/1\">Shlomo E. Chazan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues. (arXiv:2203.13926v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13926","description":"<p>This paper addresses the problem of dialogue reasoning with contextualized\ncommonsense inference. We curate CICERO, a dataset of dyadic conversations with\nfive types of utterance-level reasoning-based inferences: cause, subsequent\nevent, prerequisite, motivation, and emotional reaction. The dataset contains\n53,105 of such inferences from 5,672 dialogues. We use this dataset to solve\nrelevant generative and discriminative tasks: generation of cause and\nsubsequent event; generation of prerequisite, motivation, and listener's\nemotional reaction; and selection of plausible alternatives. Our results\nascertain the value of such dialogue-centric commonsense knowledge datasets. It\nis our hope that CICERO will open new research avenues into commonsense-based\ndialogue reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15125","description":"<p>Natural language-based communication with mobile devices and home appliances\nis becoming increasingly popular and has the potential to become natural for\ncommunicating with mobile robots in the future. Towards this goal, we\ninvestigate cross-modal text-to-point-cloud localization that will allow us to\nspecify, for example, a vehicle pick-up or goods delivery location. In\nparticular, we propose Text2Pos, a cross-modal localization module that learns\nto align textual descriptions with localization cues in a coarse- to-fine\nmanner. Given a point cloud of the environment, Text2Pos locates a position\nthat is specified via a natural language-based description of the immediate\nsurroundings. To train Text2Pos and study its performance, we construct\nKITTI360Pose, the first dataset for this task based on the recently introduced\nKITTI360 dataset. Our experiments show that we can localize 65% of textual\nqueries within 15m distance to query locations for top-10 retrieved locations.\nThis is a starting point that we hope will spark future developments towards\nlanguage-based navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolmet_M/0/1/0/all/0/1\">Manuel Kolmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qunjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljosa Osep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Domain Adaptation for ASR with Full Self-Supervision. (arXiv:2203.15966v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.15966","description":"<p>Cross-device federated learning (FL) protects user privacy by collaboratively\ntraining a model on user devices, therefore eliminating the need for\ncollecting, storing, and manually labeling user data. While important topics\nsuch as the FL training algorithm, non-IID-ness, and Differential Privacy have\nbeen well studied in the literature, this paper focuses on two challenges of\npractical importance for improving on-device ASR: the lack of ground-truth\ntranscriptions and the scarcity of compute resource and network bandwidth on\nedge devices. First, we propose a FL system for on-device ASR domain adaptation\nwith full self-supervision, which uses self-labeling together with data\naugmentation and filtering techniques. The system can improve a strong\nEmformer-Transducer based ASR model pretrained on out-of-domain data, using\nin-domain audio without any ground-truth transcriptions. Second, to reduce the\ntraining cost, we propose a self-restricted RNN Transducer (SR-RNN-T) loss, a\nvariant of alignment-restricted RNN-T that uses Viterbi alignments from\nself-supervision. To further reduce the compute and network cost, we\nsystematically explore adapting only a subset of weights in the\nEmformer-Transducer. Our best training recipe achieves a $12.9\\%$ relative WER\nreduction over the strong out-of-domain baseline, which equals $70\\%$ of the\nreduction achievable with full human supervision and centralized training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Junteng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weiyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seide_F/0/1/0/all/0/1\">Frank Seide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00763","description":"<p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline\nsetting or through human evaluation. The evaluation is often limited to\nsingle-turn or very time-intensive. As an alternative, user simulators that\nmimic user behavior allow us to consider a broad set of user goals to generate\nhuman-like conversations for simulated evaluation. Employing existing user\nsimulators to evaluate TDSs is challenging as user simulators are primarily\ndesigned to optimize dialogue policies for TDSs and have limited evaluation\ncapability. Moreover, the evaluation of user simulators is an open challenge.\nIn this work, we proposes a metaphorical user simulator for endto-end TDS\nevaluation. We also propose a tester-based evaluation framework to generate\nvariants, i.e., dialogue systems with different capabilities. Our user\nsimulator constructs a metaphorical user model that assists the simulator in\nreasoning by referring to prior knowledge when encountering new items. We\nestimate the quality of simulators by checking the simulated interactions\nbetween simulators and variants. Our experiments are conducted using three TDS\ndatasets. The metaphorical user simulator demonstrates better consistency with\nmanual evaluation than Agenda-based simulator and Seq2seq model on three\ndatasets; our tester framework demonstrates efficiency, and our approach\ndemonstrates better generalization and scalability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pragmatic constraints and pronoun reference disambiguation: the possible and the impossible. (arXiv:2204.01166v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01166","description":"<p>Pronoun disambiguation in understanding text and discourse often requires the\napplication of both general pragmatic knowledge and context-specific\ninformation. In AI and linguistics research, this has mostly been studied in\ncases where the referent is explicitly stated in the preceding text nearby.\nHowever, pronouns in natural text often refer to entities, collections, or\nevents that are only implicitly mentioned previously; in those cases the need\nto use pragmatic knowledge to disambiguate becomes much more acute and the\ncharacterization of the knowledge becomes much more difficult. Extended\nliterary texts at times employ both extremely complex patterns of reference and\nextremely rich and subtle forms of knowledge. Indeed, it is occasionally\npossible to have a pronoun that is far separated from its referent in a text.\nIn the opposite direction, pronoun use is affected by considerations of focus\nof attention and by formal constraints such as a preference for parallel\nsyntactic structures; these can be so strong that no pragmatic knowledge\nsuffices to overrule them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1\">Ernest Davis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligned Weight Regularizers for Pruning Pretrained Neural Networks. (arXiv:2204.01385v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01385","description":"<p>While various avenues of research have been explored for iterative pruning,\nlittle is known what effect pruning has on zero-shot test performance and its\npotential implications on the choice of pruning criteria. This pruning setup is\nparticularly important for cross-lingual models that implicitly learn alignment\nbetween language representations during pretraining, which if distorted via\npruning, not only leads to poorer performance on language data used for\nretraining but also on zero-shot languages that are evaluated.\n</p>\n<p>In this work, we show that there is a clear performance discrepancy in\nmagnitude-based pruning when comparing standard supervised learning to the\nzero-shot setting. From this finding, we propose two weight regularizers that\naim to maximize the alignment between units of pruned and unpruned networks to\nmitigate alignment distortion in pruned cross-lingual models and perform well\nfor both non zero-shot and zero-shot settings.\n</p>\n<p>We provide experimental results on cross-lingual tasks for the zero-shot\nsetting using XLM-RoBERTa$_{\\mathrm{Base}}$, where we also find that pruning\nhas varying degrees of representational degradation depending on the language\ncorresponding to the zero-shot test set. This is also the first study that\nfocuses on cross-lingual language model compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neill_J/0/1/0/all/0/1\">James O&#x27; Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assem_H/0/1/0/all/0/1\">Haytham Assem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Entropy of Linguistic Distributions. (arXiv:2204.01469v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01469","description":"<p>Shannon entropy is often a quantity of interest to linguists studying the\ncommunicative capacity of human language. However, entropy must typically be\nestimated from observed data because researchers do not have access to the\nunderlying probability distribution that gives rise to these data. While\nentropy estimation is a well-studied problem in other fields, there is not yet\na comprehensive exploration of the efficacy of entropy estimators for use with\nlinguistic data. In this work, we fill this void, studying the empirical\neffectiveness of different entropy estimators for linguistic distributions. In\na replication of two recent information-theoretic linguistic studies, we find\nevidence that the reported effect size is over-estimated due to over-reliance\non poor entropy estimators. Finally, we end our paper with concrete\nrecommendations for entropy estimation depending on distribution type and data\navailability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aryaman Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"QuadraLib: A Performant Quadratic Neural Network Library for Architecture Optimization and Design Exploration. (arXiv:2204.01701v1 [cs.LG])","link":"http://arxiv.org/abs/2204.01701","description":"<p>The significant success of Deep Neural Networks (DNNs) is highly promoted by\nthe multiple sophisticated DNN libraries. On the contrary, although some work\nhave proved that Quadratic Deep Neuron Networks (QDNNs) show better\nnon-linearity and learning capability than the first-order DNNs, their neuron\ndesign suffers certain drawbacks from theoretical performance to practical\ndeployment. In this paper, we first proposed a new QDNN neuron architecture\ndesign, and further developed QuadraLib, a QDNN library to provide architecture\noptimization and design exploration for QDNNs. Extensive experiments show that\nour design has good performance regarding prediction accuracy and computation\nconsumption on multiple learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zirui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fuxun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01702","description":"<p>Precision medicine for chronic diseases such as multiple sclerosis (MS)\ninvolves choosing a treatment which best balances efficacy and side\neffects/preferences for individual patients. Making this choice as early as\npossible is important, as delays in finding an effective therapy can lead to\nirreversible disability accrual. To this end, we present the first deep neural\nnetwork model for individualized treatment decisions from baseline magnetic\nresonance imaging (MRI) (with clinical information if available) for MS\npatients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)\nlesion counts on follow-up MRI on multiple treatments and (b) estimates the\nconditional average treatment effect (CATE), as defined by the predicted future\nsuppression of NE-T2 lesions, between different treatment options relative to\nplacebo. Our model is validated on a proprietary federated dataset of 1817\nmulti-sequence MRIs acquired from MS patients during four multi-centre\nrandomized clinical trials. Our framework achieves high average precision in\nthe binarized regression of future NE-T2 lesions on five different treatments,\nidentifies heterogeneous treatment effects, and provides a personalized\ntreatment recommendation that accounts for treatment-associated risk (e.g. side\neffects, patient preference, administration difficulties).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Durso_Finley_J/0/1/0/all/0/1\">Joshua Durso-Finley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Falet_J/0/1/0/all/0/1\">Jean-Pierre R. Falet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data and Physics Driven Learning Models for Fast MRI -- Fundamentals and Methodologies from CNN, GAN to Attention and Transformers. (arXiv:2204.01706v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01706","description":"<p>Research studies have shown no qualms about using data driven deep learning\nmodels for downstream tasks in medical image analysis, e.g., anatomy\nsegmentation and lesion detection, disease diagnosis and prognosis, and\ntreatment planning. However, deep learning models are not the sovereign remedy\nfor medical image analysis when the upstream imaging is not being conducted\nproperly (with artefacts). This has been manifested in MRI studies, where the\nscanning is typically slow, prone to motion artefacts, with a relatively low\nsignal to noise ratio, and poor spatial and/or temporal resolution. Recent\nstudies have witnessed substantial growth in the development of deep learning\ntechniques for propelling fast MRI. This article aims to (1) introduce the deep\nlearning based data driven techniques for fast MRI including convolutional\nneural network and generative adversarial network based methods, (2) survey the\nattention and transformer based models for speeding up MRI reconstruction, and\n(3) detail the research in coupling physics and data driven models for MRI\nacceleration. Finally, we will demonstrate through a few clinical applications,\nexplain the importance of data harmonisation and explainable models for such\nfast MRI techniques in multicentre and multi-scanner studies, and discuss\ncommon pitfalls in current research and recommendations for future research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yingying Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nan_Y/0/1/0/all/0/1\">Yang Nan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huanjun Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zidong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Lio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRI-based Multi-task Decoupling Learning for Alzheimer's Disease Detection and MMSE Score Prediction: A Multi-site Validation. (arXiv:2204.01708v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01708","description":"<p>Accurately detecting Alzheimer's disease (AD) and predicting mini-mental\nstate examination (MMSE) score are important tasks in elderly health by\nmagnetic resonance imaging (MRI). Most of the previous methods on these two\ntasks are based on single-task learning and rarely consider the correlation\nbetween them. Since the MMSE score, which is an important basis for AD\ndiagnosis, can also reflect the progress of cognitive impairment, some studies\nhave begun to apply multi-task learning methods to these two tasks. However,\nhow to exploit feature correlation remains a challenging problem for these\nmethods. To comprehensively address this challenge, we propose a MRI-based\nmulti-task decoupled learning method for AD detection and MMSE score\nprediction. First, a multi-task learning network is proposed to implement AD\ndetection and MMSE score prediction, which exploits feature correlation by\nadding three multi-task interaction layers between the backbones of the two\ntasks. Each multi-task interaction layer contains two feature decoupling\nmodules and one feature interaction module. Furthermore, to enhance the\ngeneralization between tasks of the features selected by the feature decoupling\nmodule, we propose the feature consistency loss constrained feature decoupling\nmodule. Finally, in order to exploit the specific distribution information of\nMMSE score in different groups, a distribution loss is proposed to further\nenhance the model performance. We evaluate our proposed method on multi-site\ndatasets. Experimental results show that our proposed multi-task decoupled\nrepresentation learning method achieves good performance, outperforming\nsingle-task learning and other existing state-of-the-art methods. The source\ncode of our proposed method is available at https://github.com/miacsu/MTDL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_X/0/1/0/all/0/1\">Xu Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_H/0/1/0/all/0/1\">Hulin Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_Y/0/1/0/all/0/1\">Yu Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jianxin Wang</a>, The <a href=\"http://arxiv.org/find/eess/1/au:+Initiative_A/0/1/0/all/0/1\">Alzheimer&#x27;s Disease Neuroimaging Initiative</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forestry digital twin with machine learning in Landsat 7 data. (arXiv:2204.01709v1 [cs.LG])","link":"http://arxiv.org/abs/2204.01709","description":"<p>Modeling forests using historical data allows for more accurately evolution\nanalysis, thus providing an important basis for other studies. As a recognized\nand effective tool, remote sensing plays an important role in forestry\nanalysis. We can use it to derive information about the forest, including tree\ntype, coverage and canopy density. There are many forest time series modeling\nstudies using statistic values, but few using remote sensing images. Image\nprediction digital twin is an implementation of digital twin, which aims to\npredict future images bases on historical data. In this paper, we propose an\nLSTM-based digital twin approach for forest modeling, using Landsat 7 remote\nsensing image within 20 years. The experimental results show that the\nprediction twin method in this paper can effectively predict the future images\nof study area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuetao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meiyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1\">YuChun Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingguo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Networks for Image Spam Detection. (arXiv:2204.01710v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01710","description":"<p>Spam can be defined as unsolicited bulk email. In an effort to evade\ntext-based filters, spammers sometimes embed spam text in an image, which is\nreferred to as image spam. In this research, we consider the problem of image\nspam detection, based on image analysis. We apply convolutional neural networks\n(CNN) to this problem, we compare the results obtained using CNNs to other\nmachine learning techniques, and we compare our results to previous related\nwork. We consider both real-world image spam and challenging image spam-like\ndatasets. Our results improve on previous work by employing CNNs based on a\nnovel feature set consisting of a combination of the raw image and Canny edges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharmin_T/0/1/0/all/0/1\">Tazmina Sharmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troia_F/0/1/0/all/0/1\">Fabio Di Troia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potika_K/0/1/0/all/0/1\">Katerina Potika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Image Internal Distribution Measurement Using Non-Local Variational Autoencoder. (arXiv:2204.01711v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01711","description":"<p>Deep learning-based super-resolution methods have shown great promise,\nespecially for single image super-resolution (SISR) tasks. Despite the\nperformance gain, these methods are limited due to their reliance on copious\ndata for model training. In addition, supervised SISR solutions rely on local\nneighbourhood information focusing only on the feature learning processes for\nthe reconstruction of low-dimensional images. Moreover, they fail to capitalize\non global context due to their constrained receptive field. To combat these\nchallenges, this paper proposes a novel image-specific solution, namely\nnon-local variational autoencoder (\\texttt{NLVAE}), to reconstruct a\nhigh-resolution (HR) image from a single low-resolution (LR) image without the\nneed for any prior training. To harvest maximum details for various receptive\nregions and high-quality synthetic images, \\texttt{NLVAE} is introduced as a\nself-supervised strategy that reconstructs high-resolution images using\ndisentangled information from the non-local neighbourhood. Experimental results\nfrom seven benchmark datasets demonstrate the effectiveness of the\n\\texttt{NLVAE} model. Moreover, our proposed model outperforms a number of\nbaseline and state-of-the-art methods as confirmed through extensive\nqualitative and quantitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sarker_Y/0/1/0/all/0/1\">Yeahia Sarker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Imran_A/0/1/0/all/0/1\">Abdullah-Al-Zubaer Imran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahamed_M/0/1/0/all/0/1\">Md Hafiz Ahamed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chakrabortty_R/0/1/0/all/0/1\">Ripon K. Chakrabortty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ryan_M/0/1/0/all/0/1\">Michael J. Ryan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sajal K. Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Histogram of Oriented Gradients Meet Deep Learning: A Novel Multi-task Deep Network for Medical Image Semantic Segmentation. (arXiv:2204.01712v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01712","description":"<p>We present our novel deep multi-task learning method for medical image\nsegmentation. Existing multi-task methods demand ground truth annotations for\nboth the primary and auxiliary tasks. Contrary to it, we propose to generate\nthe pseudo-labels of an auxiliary task in an unsupervised manner. To generate\nthe pseudo-labels, we leverage Histogram of Oriented Gradients (HOGs), one of\nthe most widely used and powerful hand-crafted features for detection. Together\nwith the ground truth semantic segmentation masks for the primary task and\npseudo-labels for the auxiliary task, we learn the parameters of the deep\nnetwork to minimise the loss of both the primary task and the auxiliary task\njointly. We employed our method on two powerful and widely used semantic\nsegmentation networks: UNet and U2Net to train in a multi-task setup. To\nvalidate our hypothesis, we performed experiments on two different medical\nimage segmentation data sets. From the extensive quantitative and qualitative\nresults, we observe that our method consistently improves the performance\ncompared to the counter-part method. Moreover, our method is the winner of\nFetReg Endovis Sub-challenge on Semantic Segmentation organised in conjunction\nwith MICCAI 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subedi_R/0/1/0/all/0/1\">Ronast Subedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaire_R/0/1/0/all/0/1\">Rebati Raman Gaire</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vazquez_E/0/1/0/all/0/1\">Eduard Vazquez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exemplar Learning for Medical Image Segmentation. (arXiv:2204.01713v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01713","description":"<p>Medical image annotation typically requires expert knowledge and hence incurs\ntime-consuming and expensive data annotation costs. To reduce this burden, we\npropose a novel learning scenario, Exemplar Learning (EL), to explore automated\nlearning processes for medical image segmentation from a single annotated image\nexample. This innovative learning task is particularly suitable for medical\nimage segmentation, where all categories of organs can be presented in one\nsingle image for annotation all at once. To address this challenging EL task,\nwe propose an Exemplar Learning-based Synthesis Net (ELSNet) framework for\nmedical image segmentation that enables innovative exemplar-based data\nsynthesis, pixel-prototype based contrastive embedding learning, and\npseudo-label based exploitation of the unlabeled data. Specifically, ELSNet\nintroduces two new modules for image segmentation: an exemplar-guided synthesis\nmodule, which enriches and diversifies the training set by synthesizing\nannotated samples from the given exemplar, and a pixel-prototype based\ncontrastive embedding module, which enhances the discriminative capacity of the\nbase segmentation model via contrastive self-supervised learning. Moreover, we\ndeploy a two-stage process for segmentation model training, which exploits the\nunlabeled data with predicted pseudo segmentation labels. To evaluate this new\nlearning framework, we conduct extensive experiments on several organ\nsegmentation datasets and present an in-depth analysis. The empirical results\nshow that the proposed exemplar learning framework produces effective\nsegmentation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+En_Q/0/1/0/all/0/1\">Qing En</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Fine-Grained Noise Model via Contrastive Learning. (arXiv:2204.01716v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01716","description":"<p>Image denoising has achieved unprecedented progress as great efforts have\nbeen made to exploit effective deep denoisers. To improve the denoising\nperformance in realworld, two typical solutions are used in recent trends:\ndevising better noise models for the synthesis of more realistic training data,\nand estimating noise level function to guide non-blind denoisers. In this work,\nwe combine both noise modeling and estimation, and propose an innovative noise\nmodel estimation and noise synthesis pipeline for realistic noisy image\ngeneration. Specifically, our model learns a noise estimation model with\nfine-grained statistical noise model in a contrastive manner. Then, we use the\nestimated noise parameters to model camera-specific noise distribution, and\nsynthesize realistic noisy training data. The most striking thing for our work\nis that by calibrating noise models of several sensors, our model can be\nextended to predict other cameras. In other words, we can estimate\ncameraspecific noise models for unknown sensors with only testing images,\nwithout laborious calibration frames or paired noisy/clean data. The proposed\npipeline endows deep denoisers with competitive performances with\nstate-of-the-art real noise modeling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zou_Y/0/1/0/all/0/1\">Yunhao Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RestoreX-AI: A Contrastive Approach towards Guiding Image Restoration via Explainable AI Systems. (arXiv:2204.01719v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01719","description":"<p>Modern applications such as self-driving cars and drones rely heavily upon\nrobust object detection techniques. However, weather corruptions can hinder the\nobject detectability and pose a serious threat to their navigation and\nreliability. Thus, there is a need for efficient denoising, deraining, and\nrestoration techniques. Generative adversarial networks and transformers have\nbeen widely adopted for image restoration. However, the training of these\nmethods is often unstable and time-consuming. Furthermore, when used for object\ndetection (OD), the output images generated by these methods may provide\nunsatisfactory results despite image clarity. In this work, we propose a\ncontrastive approach towards mitigating this problem, by evaluating images\ngenerated by restoration models during and post training. This approach\nleverages OD scores combined with attention maps for predicting the usefulness\nof restored images for the OD task. We conduct experiments using two novel\nuse-cases of conditional GANs and two transformer methods that probe the\nrobustness of the proposed approach on multi-weather corruptions in the OD\ntask. Our approach achieves an averaged 178 percent increase in mAP between the\ninput and restored images under adverse weather conditions like dust tornadoes\nand snowfall. We report unique cases where greater denoising does not improve\nOD performance and conversely where noisy generated images demonstrate good\nresults. We conclude the need for explainability frameworks to bridge the gap\nbetween human and machine perception, especially in the context of robust\nobject detection for autonomous vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Marathe_A/0/1/0/all/0/1\">Aboli Marathe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jain_P/0/1/0/all/0/1\">Pushkar Jain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walambe_R/0/1/0/all/0/1\">Rahee Walambe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kotecha_K/0/1/0/all/0/1\">Ketan Kotecha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishing Homophenes Using Multi-Head Visual-Audio Memory for Lip Reading. (arXiv:2204.01725v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01725","description":"<p>Recognizing speech from silent lip movement, which is called lip reading, is\na challenging task due to 1) the inherent information insufficiency of lip\nmovement to fully represent the speech, and 2) the existence of homophenes that\nhave similar lip movement with different pronunciations. In this paper, we try\nto alleviate the aforementioned two challenges in lip reading by proposing a\nMulti-head Visual-audio Memory (MVM). Firstly, MVM is trained with audio-visual\ndatasets and remembers audio representations by modelling the\ninter-relationships of paired audio-visual representations. At the inference\nstage, visual input alone can extract the saved audio representation from the\nmemory by examining the learned inter-relationships. Therefore, the lip reading\nmodel can complement the insufficient visual information with the extracted\naudio representations. Secondly, MVM is composed of multi-head key memories for\nsaving visual features and one value memory for saving audio knowledge, which\nis designed to distinguish the homophenes. With the multi-head key memories,\nMVM extracts possible candidate audio features from the memory, which allows\nthe lip reading model to consider the possibility of which pronunciations can\nbe represented from the input lip movement. This also can be viewed as an\nexplicit implementation of the one-to-many mapping of viseme-to-phoneme.\nMoreover, MVM is employed in multi-temporal levels to consider the context when\nretrieving the memory and distinguish the homophenes. Extensive experimental\nresults verify the effectiveness of the proposed method in lip reading and in\ndistinguishing the homophenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1\">Jeong Hun Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lip to Speech Synthesis with Visual Context Attentional GAN. (arXiv:2204.01726v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01726","description":"<p>In this paper, we propose a novel lip-to-speech generative adversarial\nnetwork, Visual Context Attentional GAN (VCA-GAN), which can jointly model\nlocal and global lip movements during speech synthesis. Specifically, the\nproposed VCA-GAN synthesizes the speech from local lip visual features by\nfinding a mapping function of viseme-to-phoneme, while global visual context is\nembedded into the intermediate layers of the generator to clarify the ambiguity\nin the mapping induced by homophene. To achieve this, a visual context\nattention module is proposed where it encodes global representations from the\nlocal visual features, and provides the desired global visual context\ncorresponding to the given coarse speech representation to the generator\nthrough audio-visual attention. In addition to the explicit modelling of local\nand global visual representations, synchronization learning is introduced as a\nform of contrastive learning that guides the generator to synthesize a speech\nin sync with the given input lip movements. Extensive experiments demonstrate\nthat the proposed VCA-GAN outperforms existing state-of-the-art and is able to\neffectively synthesize the speech from multi-speaker that has been barely\nhandled in the previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joanna Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Zero Shot Learning For Medical Image Classification. (arXiv:2204.01728v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01728","description":"<p>In many real world medical image classification settings we do not have\naccess to samples of all possible disease classes, while a robust system is\nexpected to give high performance in recognizing novel test data. We propose a\ngeneralized zero shot learning (GZSL) method that uses self supervised learning\n(SSL) for: 1) selecting anchor vectors of different disease classes; and 2)\ntraining a feature generator. Our approach does not require class attribute\nvectors which are available for natural images but not for medical images. SSL\nensures that the anchor vectors are representative of each class. SSL is also\nused to generate synthetic features of unseen classes. Using a simpler\narchitecture, our method matches a state of the art SSL based GZSL method for\nnatural images and outperforms all methods for medical images. Our method is\nadaptable enough to accommodate class attribute vectors when they are available\nfor natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Effects of Handling Data Imbalance on Learned Features from Medical Images by Looking Into the Models. (arXiv:2204.01729v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01729","description":"<p>One challenging property lurking in medical datasets is the imbalanced data\ndistribution, where the frequency of the samples between the different classes\nis not balanced. Training a model on an imbalanced dataset can introduce unique\nchallenges to the learning problem where a model is biased towards the highly\nfrequent class. Many methods are proposed to tackle the distributional\ndifferences and the imbalanced problem. However, the impact of these approaches\non the learned features is not well studied. In this paper, we look deeper into\nthe internal units of neural networks to observe how handling data imbalance\naffects the learned features. We study several popular cost-sensitive\napproaches for handling data imbalance and analyze the feature maps of the\nconvolutional neural networks from multiple perspectives: analyzing the\nalignment of salient features with pathologies and analyzing the\npathology-related concepts encoded by the networks. Our study reveals\ndifferences and insights regarding the trained models that are not reflected by\nquantitative metrics such as AUROC and AP and show up only by looking at the\nmodels through a lens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1\">Ashkan Khakzar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanisoglu_M/0/1/0/all/0/1\">Mirac Sanisoglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Seong Tae Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rezaei_M/0/1/0/all/0/1\">Mina Rezaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transient motion classification through turbid volumes via parallelized single-photon detection and deep contrastive embedding. (arXiv:2204.01733v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01733","description":"<p>Fast noninvasive probing of spatially varying decorrelating events, such as\ncerebral blood flow beneath the human skull, is an essential task in various\nscientific and clinical settings. One of the primary optical techniques used is\ndiffuse correlation spectroscopy (DCS), whose classical implementation uses a\nsingle or few single-photon detectors, resulting in poor spatial localization\naccuracy and relatively low temporal resolution. Here, we propose a technique\ntermed Classifying Rapid decorrelation Events via Parallelized single photon\ndEtection (CREPE)}, a new form of DCS that can probe and classify different\ndecorrelating movements hidden underneath turbid volume with high sensitivity\nusing parallelized speckle detection from a $32\\times32$ pixel SPAD array. We\nevaluate our setup by classifying different spatiotemporal-decorrelating\npatterns hidden beneath a 5mm tissue-like phantom made with rapidly\ndecorrelating dynamic scattering media. Twelve multi-mode fibers are used to\ncollect scattered light from different positions on the surface of the tissue\nphantom. To validate our setup, we generate perturbed decorrelation patterns by\nboth a digital micromirror device (DMD) modulated at multi-kilo-hertz rates, as\nwell as a vessel phantom containing flowing fluid. Along with a deep\ncontrastive learning algorithm that outperforms classic unsupervised learning\nmethods, we demonstrate our approach can accurately detect and classify\ndifferent transient decorrelation events (happening in 0.1-0.4s) underneath\nturbid scattering media, without any data labeling. This has the potential to\nbe applied to noninvasively monitor deep tissue motion patterns, for example\nidentifying normal or abnormal cerebral blood flow events, at multi-Hertz rates\nwithin a compact and static detection probe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wenhui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim J&#xf6;nsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_R/0/1/0/all/0/1\">Ruobing Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McKee_P/0/1/0/all/0/1\">Paul McKee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kanghyun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konda_P/0/1/0/all/0/1\">Pavan Chandra Konda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreiss_L/0/1/0/all/0/1\">Lucas Krei&#xdf;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berrocal_E/0/1/0/all/0/1\">Edouard Berrocal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huettel_S/0/1/0/all/0/1\">Scott Huettel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Explaining Multimodal Hateful Meme Detection Models. (arXiv:2204.01734v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01734","description":"<p>Hateful meme detection is a new multimodal task that has gained significant\ntraction in academic and industry research communities. Recently, researchers\nhave applied pre-trained visual-linguistic models to perform the multimodal\nclassification task, and some of these solutions have yielded promising\nresults. However, what these visual-linguistic models learn for the hateful\nmeme classification task remains unclear. For instance, it is unclear if these\nmodels are able to capture the derogatory or slurs references in multimodality\n(i.e., image and text) of the hateful memes. To fill this research gap, this\npaper propose three research questions to improve our understanding of these\nvisual-linguistic models performing the hateful meme classification task. We\nfound that the image modality contributes more to the hateful meme\nclassification task, and the visual-linguistic models are able to perform\nvisual-text slurs grounding to a certain extent. Our error analysis also shows\nthat the visual-linguistic models have acquired biases, which resulted in\nfalse-positive predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hee_M/0/1/0/all/0/1\">Ming Shan Hee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking Urbanization in Developing Regions with Remote Sensing Spatial-Temporal Super-Resolution. (arXiv:2204.01736v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01736","description":"<p>Automated tracking of urban development in areas where construction\ninformation is not available became possible with recent advancements in\nmachine learning and remote sensing. Unfortunately, these solutions perform\nbest on high-resolution imagery, which is expensive to acquire and infrequently\navailable, making it difficult to scale over long time spans and across large\ngeographies. In this work, we propose a pipeline that leverages a single\nhigh-resolution image and a time series of publicly available low-resolution\nimages to generate accurate high-resolution time series for object tracking in\nurban construction. Our method achieves significant improvement in comparison\nto baselines using single image super-resolution, and can assist in extending\nthe accessibility and scalability of building construction tracking across the\ndeveloping world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yutong He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">William Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lobell_D/0/1/0/all/0/1\">David B. Lobell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection. (arXiv:2204.01737v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01737","description":"<p>Convolutional neural networks have enabled significant improvements in\nmedical image-based disease classification. It has, however, become\nincreasingly clear that these models are susceptible to performance degradation\ndue to spurious correlations and dataset shifts, which may lead to\nunderperformance on underrepresented patient groups, among other problems. In\nthis paper, we compare two classification schemes on the ADNI MRI dataset: a\nvery simple logistic regression model that uses manually selected volumetric\nfeatures as inputs, and a convolutional neural network trained on 3D MRI data.\nWe assess the robustness of the trained models in the face of varying dataset\nsplits, training set sex composition, and stage of disease. In contrast to\nearlier work on diagnosing lung diseases based on chest x-ray data, we do not\nfind a strong dependence of model performance for male and female test subjects\non the sex composition of the training dataset. Moreover, in our analysis, the\nlow-dimensional model with manually selected features outperforms the 3D CNN,\nthus emphasizing the need for automatic robust feature extraction methods and\nthe value of manual feature specification (based on prior knowledge) for\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Petersen_E/0/1/0/all/0/1\">Eike Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feragen_A/0/1/0/all/0/1\">Aasa Feragen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zemsch_L/0/1/0/all/0/1\">Luise da Costa Zemsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henriksen_A/0/1/0/all/0/1\">Anders Henriksen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christensen_O/0/1/0/all/0/1\">Oskar Eiler Wiese Christensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ganz_M/0/1/0/all/0/1\">Melanie Ganz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Recognition In Children: A Longitudinal Study. (arXiv:2204.01760v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01760","description":"<p>The lack of high fidelity and publicly available longitudinal children face\ndatasets is one of the main limiting factors in the development of face\nrecognition systems for children. In this work, we introduce the Young Face\nAging (YFA) dataset for analyzing the performance of face recognition systems\nover short age-gaps in children. We expand previous work by comparing YFA with\nseveral publicly available cross-age adult datasets to quantify the effects of\nshort age-gap in adults and children. Our analysis confirms a statistically\nsignificant and matcher independent decaying relationship between the match\nscores of ArcFace-Focal, MagFace, and Facenet matchers and the age-gap between\nthe gallery and probe images in children, even at the short age-gap of 6\nmonths. However, our result indicates that the low verification performance\nreported in previous work might be due to the intra-class structure of the\nmatcher and the lower quality of the samples. Our experiment using YFA and a\nstate-of-the-art, quality-aware face matcher (MagFace) indicates 98.3% and\n94.9% TAR at 0.1% FAR over 6 and 36 Months age-gaps, respectively, suggesting\nthat face recognition may be feasible for children for age-gaps of up to three\nyears.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_K/0/1/0/all/0/1\">Keivan Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuckers_S/0/1/0/all/0/1\">Stephanie Schuckers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The First Principles of Deep Learning and Compression. (arXiv:2204.01782v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01782","description":"<p>The deep learning revolution incited by the 2012 Alexnet paper has been\ntransformative for the field of computer vision. Many problems which were\nseverely limited using classical solutions are now seeing unprecedented\nsuccess. The rapid proliferation of deep learning methods has led to a sharp\nincrease in their use in consumer and embedded applications. One consequence of\nconsumer and embedded applications is lossy multimedia compression which is\nrequired to engineer the efficient storage and transmission of data in these\nreal-world scenarios. As such, there has been increased interest in a deep\nlearning solution for multimedia compression which would allow for higher\ncompression ratios and increased visual quality.\n</p>\n<p>The deep learning approach to multimedia compression, so called Learned\nMultimedia Compression, involves computing a compressed representation of an\nimage or video using a deep network for the encoder and the decoder. While\nthese techniques have enjoyed impressive academic success, their industry\nadoption has been essentially non-existent. Classical compression techniques\nlike JPEG and MPEG are too entrenched in modern computing to be easily\nreplaced. This dissertation takes an orthogonal approach and leverages deep\nlearning to improve the compression fidelity of these classical algorithms.\nThis allows the incredible advances in deep learning to be used for multimedia\ncompression without threatening the ubiquity of the classical methods.\n</p>\n<p>The key insight of this work is that methods which are motivated by first\nprinciples, i.e., the underlying engineering decisions that were made when the\ncompression algorithms were developed, are more effective than general methods.\nBy encoding prior knowledge into the design of the algorithm, the flexibility,\nperformance, and/or accuracy are improved at the cost of generality...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ehrlich_M/0/1/0/all/0/1\">Max Ehrlich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Permanence Emerges in a Random Walk along Memory. (arXiv:2204.01784v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01784","description":"<p>This paper proposes a self-supervised objective for learning representations\nthat localize objects under occlusion - a property known as object permanence.\nA central question is the choice of learning signal in cases of total\nocclusion. Rather than directly supervising the locations of invisible objects,\nwe propose a self-supervised objective that requires neither human annotation,\nnor assumptions about object dynamics. We show that object permanence can\nemerge by optimizing for temporal coherence of memory: we fit a Markov walk\nalong a space-time graph of memories, where the states in each time step are\nnon-Markovian features from a sequence encoder. This leads to a memory\nrepresentation that stores occluded objects and predicts their motion, to\nbetter localize them. The resulting model outperforms existing approaches on\nseveral datasets of increasing complexity and realism, despite requiring\nminimal supervision and assumptions, and hence being broadly applicable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1\">Pavel Tokmakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabri_A/0/1/0/all/0/1\">Allan Jabri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight HDR Camera ISP for Robust Perception in Dynamic Illumination Conditions via Fourier Adversarial Networks. (arXiv:2204.01795v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01795","description":"<p>The limited dynamic range of commercial compact camera sensors results in an\ninaccurate representation of scenes with varying illumination conditions,\nadversely affecting image quality and subsequently limiting the performance of\nunderlying image processing algorithms. Current state-of-the-art (SoTA)\nconvolutional neural networks (CNN) are developed as post-processing techniques\nto independently recover under-/over-exposed images. However, when applied to\nimages containing real-world degradations such as glare, high-beam, color\nbleeding with varying noise intensity, these algorithms amplify the\ndegradations, further degrading image quality. We propose a lightweight\ntwo-stage image enhancement algorithm sequentially balancing illumination and\nnoise removal using frequency priors for structural guidance to overcome these\nlimitations. Furthermore, to ensure realistic image quality, we leverage the\nrelationship between frequency and spatial domain properties of an image and\npropose a Fourier spectrum-based adversarial framework (AFNet) for consistent\nimage enhancement under varying illumination conditions. While current\nformulations of image enhancement are envisioned as post-processing techniques,\nwe examine if such an algorithm could be extended to integrate the\nfunctionality of the Image Signal Processing (ISP) pipeline within the camera\nsensor benefiting from RAW sensor data and lightweight CNN architecture. Based\non quantitative and qualitative evaluations, we also examine the practicality\nand effects of image enhancement techniques on the performance of common\nperception tasks such as object detection and semantic segmentation in varying\nillumination conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shyam_P/0/1/0/all/0/1\">Pranjay Shyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengar_S/0/1/0/all/0/1\">Sandeep Singh Sengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Soo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Near/Remote Sensing with Geospatial Attention. (arXiv:2204.01807v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01807","description":"<p>This work addresses the task of overhead image segmentation when auxiliary\nground-level images are available. Recent work has shown that performing joint\ninference over these two modalities, often called near/remote sensing, can\nyield significant accuracy improvements. Extending this line of work, we\nintroduce the concept of geospatial attention, a geometry-aware attention\nmechanism that explicitly considers the geospatial relationship between the\npixels in a ground-level image and a geographic location. We propose an\napproach for computing geospatial attention that incorporates geometric\nfeatures and the appearance of the overhead and ground-level imagery. We\nintroduce a novel architecture for near/remote sensing that is based on\ngeospatial attention and demonstrate its use for five segmentation tasks. The\nresults demonstrate that our method significantly outperforms the previous\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Workman_S/0/1/0/all/0/1\">Scott Workman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_M/0/1/0/all/0/1\">M. Usman Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanton_H/0/1/0/all/0/1\">Hunter Blanton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Infield Navigation: leveraging simulated data for crop row detection. (arXiv:2204.01811v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01811","description":"<p>Agricultural datasets for crop row detection are often bound by their limited\nnumber of images. This restricts the researchers from developing deep learning\nbased models for precision agricultural tasks involving crop row detection. We\nsuggest the utilization of small real-world datasets along with additional data\ngenerated by simulations to yield similar crop row detection performance as\nthat of a model trained with a large real world dataset. Our method could reach\nthe performance of a deep learning based crop row detection model trained with\nreal-world data by using 60% less labelled real-world data. Our model performed\nwell against field variations such as shadows, sunlight and grow stages. We\nintroduce an automated pipeline to generate labelled images for crop row\ndetection in simulation domain. An extensive comparison is done to analyze the\ncontribution of simulated data towards reaching robust crop row detection in\nvarious real-world field scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Rajitha de Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1\">Grzegorz Cielniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Efficiency Pedestrian Crossing Prediction. (arXiv:2204.01862v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01862","description":"<p>Predicting pedestrian crossing intention is an indispensable aspect of\ndeploying advanced driving systems (ADS) or advanced driver-assistance systems\n(ADAS) to real life. State-of-the-art methods in predicting pedestrian crossing\nintention often rely on multiple streams of information as inputs, each of\nwhich requires massive computational resources and heavy network architectures\nto generate. However, such reliance limits the practical application of the\nsystems. In this paper, driven the the real-world demands of pedestrian\ncrossing intention prediction models with both high efficiency and accuracy, we\nintroduce a network with only frames of pedestrians as the input. Every\ncomponent in the introduced network is driven by the goal of light weight.\nSpecifically, we reduce the multi-source input dependency and employ light\nneural networks that are tailored for mobile devices. These smaller neural\nnetworks can fit into computer memory and can be transmitted over a computer\nnetwork more easily, thus making them more suitable for real-life deployment\nand real-time prediction. To compensate the removal of the multi-source input,\nwe enhance the network effectiveness by adopting a multi-task learning\ntraining, named \"side task learning\", to include multiple auxiliary tasks to\njointly learn the feature extractor for improved robustness. Each head handles\na specific task that potentially shares knowledge with other heads. In the\nmeantime, the feature extractor is shared across all tasks to ensure the\nsharing of basic knowledge across all layers. The light weight but high\nefficiency characteristics of our model endow it the potential of being\ndeployed on vehicle-based systems. Experiments validate that our model\nconsistently delivers outstanding performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhuoran Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truck Axle Detection with Convolutional Neural Networks. (arXiv:2204.01868v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01868","description":"<p>Axle count in trucks is important to the classification of vehicles and to\nthe operation of road systems, and is used in the determination of service fees\nand the impact on the pavement. Although axle count can be achieved with\ntraditional methods, such as manual labor, it is increasingly possible to count\naxles using deep learning and computer vision methods. This paper aims to\ncompare three deep learning object detection algorithms, YOLO, Faster R-CNN and\nSSD, for the detection of truck axles. A dataset was built to provide training\nand testing examples for the neural networks. Training was done on different\nbase models, to increase training time efficiency and to compare results. We\nevaluated results based on three metrics: mAP, F1-score, and FPS count. Results\nindicate that YOLO and SSD have similar accuracy and performance, with more\nthan 96% mAP for both models. Dataset and codes are publicly available for\ndownload.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marcomini_L/0/1/0/all/0/1\">Leandro Arab Marcomini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunha_A/0/1/0/all/0/1\">Andr&#xe9; Luiz Cunha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoTrack: Shuttle trajectory reconstruction from monocular badminton video. (arXiv:2204.01899v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01899","description":"<p>Trajectory estimation is a fundamental component of racket sport analytics,\nas the trajectory contains information not only about the winning and losing of\neach point, but also how it was won or lost. In sports such as badminton,\nplayers benefit from knowing the full 3D trajectory, as the height of\nshuttlecock or ball provides valuable tactical information. Unfortunately, 3D\nreconstruction is a notoriously hard problem, and standard trajectory\nestimators can only track 2D pixel coordinates. In this work, we present the\nfirst complete end-to-end system for the extraction and segmentation of 3D\nshuttle trajectories from monocular badminton videos. Our system integrates\nbadminton domain knowledge such as court dimension, shot placement, physical\nlaws of motion, along with vision-based features such as player poses and\nshuttle tracking. We find that significant engineering efforts and model\nimprovements are needed to make the overall system robust, and as a by-product\nof our work, improve state-of-the-art results on court recognition, 2D\ntrajectory estimation, and hit recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Paul Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jui-Hsien Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Active Learning for Affective Digital Phenotyping. (arXiv:2204.01915v1 [cs.LG])","link":"http://arxiv.org/abs/2204.01915","description":"<p>Some of the most severe bottlenecks preventing widespread development of\nmachine learning models for human behavior include a dearth of labeled training\ndata and difficulty of acquiring high quality labels. Active learning is a\nparadigm for using algorithms to computationally select a useful subset of data\npoints to label using metrics for model uncertainty and data similarity. We\nexplore active learning for naturalistic computer vision emotion data, a\nparticularly heterogeneous and complex data space due to inherently subjective\nlabels. Using frames collected from gameplay acquired from a therapeutic\nsmartphone game for children with autism, we run a simulation of active\nlearning using gameplay prompts as metadata to aid in the active learning\nprocess. We find that active learning using information generated during\ngameplay slightly outperforms random selection of the same number of labeled\nframes. We next investigate a method to conduct active learning with subjective\ndata, such as in affective computing, and where multiple crowdsourced labels\ncan be acquired for each image. Using the Child Affective Facial Expression\n(CAFE) dataset, we simulate an active learning process for crowdsourcing many\nlabels and find that prioritizing frames using the entropy of the crowdsourced\nlabel distribution results in lower categorical cross-entropy loss compared to\nrandom frame selection. Collectively, these results demonstrate pilot\nevaluations of two novel active learning approaches for subjective affective\ndata collected in noisy settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_C/0/1/0/all/0/1\">Cezmi Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Cathy Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunlap_K/0/1/0/all/0/1\">Kaitlyn Dunlap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kent_J/0/1/0/all/0/1\">Jack Kent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husic_A/0/1/0/all/0/1\">Arman Husic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1\">Nate Stockham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1\">Brianna Chrisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1\">Kelley Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jae-Yoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis P. Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Spotting Transformers. (arXiv:2204.01918v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01918","description":"<p>In this paper, we present TExt Spotting TRansformers (TESTR), a generic\nend-to-end text spotting framework using Transformers for text detection and\nrecognition in the wild. TESTR builds upon a single encoder and dual decoders\nfor the joint text-box control point regression and character recognition.\nOther than most existing literature, our method is free from Region-of-Interest\noperations and heuristics-driven post-processing procedures; TESTR is\nparticularly effective when dealing with curved text-boxes where special cares\nare needed for the adaptation of the traditional bounding-box representations.\nWe show our canonical representation of control points suitable for text\ninstances in both Bezier curve and polygon annotations. In addition, we design\na bounding-box guided polygon detection (box-to-polygon) process. Experiments\non curved and arbitrarily shaped datasets demonstrate state-of-the-art\nperformances of the proposed TESTR algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yongwen Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Quality Pluralistic Image Completion via Code Shared VQGAN. (arXiv:2204.01931v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01931","description":"<p>PICNet pioneered the generation of multiple and diverse results for image\ncompletion task, but it required a careful balance between $\\mathcal{KL}$ loss\n(diversity) and reconstruction loss (quality), resulting in a limited diversity\nand quality . Separately, iGPT-based architecture has been employed to infer\ndistributions in a discrete space derived from a pixel-level pre-clustered\npalette, which however cannot generate high-quality results directly. In this\nwork, we present a novel framework for pluralistic image completion that can\nachieve both high quality and diversity at much faster inference speed. The\ncore of our design lies in a simple yet effective code sharing mechanism that\nleads to a very compact yet expressive image representation in a discrete\nlatent domain. The compactness and the richness of the representation further\nfacilitate the subsequent deployment of a transformer to effectively learn how\nto composite and complete a masked image at the discrete code domain. Based on\nthe global context well-captured by the transformer and the available visual\nregions, we are able to sample all tokens simultaneously, which is completely\ndifferent from the prevailing autoregressive approach of iGPT-based works, and\nleads to more than 100$\\times$ faster inference speed. Experiments show that\nour framework is able to learn semantically-rich discrete codes efficiently and\nrobustly, resulting in much better image reconstruction quality. Our diverse\nimage completion framework significantly outperforms the state-of-the-art both\nquantitatively and qualitatively on multiple benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chuanxia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guoxian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_T/0/1/0/all/0/1\">Tat-Jen Cham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Linjie Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Distraction: Watermark Removal Through Continual Learning with Selective Forgetting. (arXiv:2204.01934v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01934","description":"<p>Fine-tuning attacks are effective in removing the embedded watermarks in deep\nlearning models. However, when the source data is unavailable, it is\nchallenging to just erase the watermark without jeopardizing the model\nperformance. In this context, we introduce Attention Distraction (AD), a novel\nsource data-free watermark removal attack, to make the model selectively forget\nthe embedded watermarks by customizing continual learning. In particular, AD\nfirst anchors the model's attention on the main task using some unlabeled data.\nThen, through continual learning, a small number of \\textit{lures} (randomly\nselected natural images) that are assigned a new label distract the model's\nattention away from the watermarks. Experimental results from different\ndatasets and networks corroborate that AD can thoroughly remove the watermark\nwith a small resource budget without compromising the model's performance on\nthe main task, which outperforms the state-of-the-art works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1\">Qi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leo Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengshan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yong Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Implicit Neural Stylization. (arXiv:2204.01943v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01943","description":"<p>Representing visual signals by implicit representation (e.g., a coordinate\nbased deep network) has prevailed among many vision tasks. This work explores a\nnew intriguing direction: training a stylized implicit representation, using a\ngeneralized approach that can apply to various 2D and 3D scenarios. We conduct\na pilot study on a variety of implicit functions, including 2D coordinate-based\nrepresentation, neural radiance field, and signed distance function. Our\nsolution is a Unified Implicit Neural Stylization framework, dubbed INS. In\ncontrary to vanilla implicit representation, INS decouples the ordinary\nimplicit function into a style implicit module and a content implicit module,\nin order to separately encode the representations from the style image and\ninput scenes. An amalgamation module is then applied to aggregate these\ninformation and synthesize the stylized output. To regularize the geometry in\n3D scenes, we propose a novel self-distillation geometry consistency loss which\npreserves the geometry fidelity of the stylized scenes. Comprehensive\nexperiments are conducted on multiple task settings, including novel view\nsynthesis of complex scenes, stylization for implicit surfaces, and fitting\nimages using MLPs. We further demonstrate that the learned representation is\ncontinuous not only spatially but also style-wise, leading to effortlessly\ninterpolating between different styles and generating images with new mixed\nstyles. Please refer to the video on our project page for more view synthesis\nresults: https://zhiwenfan.github.io/INS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhiwen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dejia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards On-Board Panoptic Segmentation of Multispectral Satellite Images. (arXiv:2204.01952v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01952","description":"<p>With tremendous advancements in low-power embedded computing devices and\nremote sensing instruments, the traditional satellite image processing pipeline\nwhich includes an expensive data transfer step prior to processing data on the\nground is being replaced by on-board processing of captured data. This paradigm\nshift enables critical and time-sensitive analytic intelligence to be acquired\nin a timely manner on-board the satellite itself. However, at present, the\non-board processing of multi-spectral satellite images is limited to\nclassification and segmentation tasks. Extending this processing to its next\nlogical level, in this paper we propose a lightweight pipeline for on-board\npanoptic segmentation of multi-spectral satellite images. Panoptic segmentation\noffers major economic and environmental insights, ranging from yield estimation\nfrom agricultural lands to intelligence for complex military applications.\nNevertheless, the on-board intelligence extraction raises several challenges\ndue to the loss of temporal observations and the need to generate predictions\nfrom a single image sample. To address this challenge, we propose a multimodal\nteacher network based on a cross-modality attention-based fusion strategy to\nimprove the segmentation accuracy by exploiting data from multiple modes. We\nalso propose an online knowledge distillation framework to transfer the\nknowledge learned by this multi-modal teacher network to a uni-modal student\nwhich receives only a single frame input, and is more appropriate for an\non-board environment. We benchmark our approach against existing\nstate-of-the-art panoptic segmentation models using the PASTIS multi-spectral\npanoptic segmentation dataset considering an on-board processing setting. Our\nevaluations demonstrate a substantial increase in accuracy metrics compared to\nthe existing state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1\">Tharindu Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gammulle_H/0/1/0/all/0/1\">Harshala Gammulle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive 3D Shape Generation via Canonical Mapping. (arXiv:2204.01955v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01955","description":"<p>With the capacity of modeling long-range dependencies in sequential data,\ntransformers have shown remarkable performances in a variety of generative\ntasks such as image, audio, and text generation. Yet, taming them in generating\nless structured and voluminous data formats such as high-resolution point\nclouds have seldom been explored due to ambiguous sequentialization processes\nand infeasible computation burden. In this paper, we aim to further exploit the\npower of transformers and employ them for the task of 3D point cloud\ngeneration. The key idea is to decompose point clouds of one category into\nsemantically aligned sequences of shape compositions, via a learned canonical\nspace. These shape compositions can then be quantized and used to learn a\ncontext-rich composition codebook for point cloud generation. Experimental\nresults on point cloud reconstruction and unconditional generation show that\nour model performs favorably against state-of-the-art approaches. Furthermore,\nour model can be easily extended to multi-modal shape completion as an\napplication for conditional shape generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">An-Chieh Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xueting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceSigns: Semi-Fragile Neural Watermarks for Media Authentication and Countering Deepfakes. (arXiv:2204.01960v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01960","description":"<p>Deepfakes and manipulated media are becoming a prominent threat due to the\nrecent advances in realistic image and video synthesis techniques. There have\nbeen several attempts at combating Deepfakes using machine learning\nclassifiers. However, such classifiers do not generalize well to black-box\nimage synthesis techniques and have been shown to be vulnerable to adversarial\nexamples. To address these challenges, we introduce a deep learning based\nsemi-fragile watermarking technique that allows media authentication by\nverifying an invisible secret message embedded in the image pixels. Instead of\nidentifying and detecting fake media using visual artifacts, we propose to\nproactively embed a semi-fragile watermark into a real image so that we can\nprove its authenticity when needed. Our watermarking framework is designed to\nbe fragile to facial manipulations or tampering while being robust to benign\nimage-processing operations such as image compression, scaling, saturation,\ncontrast adjustments etc. This allows images shared over the internet to retain\nthe verifiable watermark as long as face-swapping or any other Deepfake\nmodification technique is not applied. We demonstrate that FaceSigns can embed\na 128 bit secret as an imperceptible image watermark that can be recovered with\na high bit recovery accuracy at several compression levels, while being\nnon-recoverable when unseen Deepfake manipulations are applied. For a set of\nunseen benign and Deepfake manipulations studied in our work, FaceSigns can\nreliably detect manipulated content with an AUC score of 0.996 which is\nsignificantly higher than prior image watermarking and steganography\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neekhara_P/0/1/0/all/0/1\">Paarth Neekhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1\">Shehzeen Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Ke Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Garment Transfer. (arXiv:2204.01965v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01965","description":"<p>Image-based garment transfer replaces the garment on the target human with\nthe desired garment; this enables users to virtually view themselves in the\ndesired garment. To this end, many approaches have been proposed using the\ngenerative model and have shown promising results. However, most fail to\nprovide the user with on the fly garment modification functionality. We aim to\nadd this customizable option of \"garment tweaking\" to our model to control\ngarment attributes, such as sleeve length, waist width, and garment texture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_J/0/1/0/all/0/1\">Jooeun Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedroso_T/0/1/0/all/0/1\">Tomas Cabezon Pedroso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siga_C/0/1/0/all/0/1\">Carolene Siga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinsung Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSDoodle: Searching for App Screens via Interactive Sketching. (arXiv:2204.01968v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01968","description":"<p>Keyword-based mobile screen search does not account for screen content and\nfails to operate as a universal tool for all levels of users. Visual searching\n(e.g., image, sketch) is structured and easy to adopt. Current visual search\napproaches count on a complete screen and are therefore slow and tedious.\nPSDoodle employs a deep neural network to recognize partial screen element\ndrawings instantly on a digital drawing interface and shows results in\nreal-time. PSDoodle is the first tool that utilizes partial sketches and\nsearches for screens in an interactive iterative way. PSDoodle supports\ndifferent drawing styles and retrieves search results that are relevant to the\nuser's sketch query. A short video demonstration is available online at:\nhttps://youtu.be/3cVLHFm5pY4\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohian_S/0/1/0/all/0/1\">Soumik Mohian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csallner_C/0/1/0/all/0/1\">Christoph Csallner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region Rebalance for Long-Tailed Semantic Segmentation. (arXiv:2204.01969v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01969","description":"<p>In this paper, we study the problem of class imbalance in semantic\nsegmentation. We first investigate and identify the main challenges of\naddressing this issue through pixel rebalance. Then a simple and yet effective\nregion rebalance scheme is derived based on our analysis. In our solution,\npixel features belonging to the same class are grouped into region features,\nand a rebalanced region classifier is applied via an auxiliary region rebalance\nbranch during training. To verify the flexibility and effectiveness of our\nmethod, we apply the region rebalance module into various semantic segmentation\nmethods, such as Deeplabv3+, OCRNet, and Swin. Our strategy achieves consistent\nimprovement on the challenging ADE20K and COCO-Stuff benchmark. In particular,\nwith the proposed region rebalance scheme, state-of-the-art BEiT receives +0.7%\ngain in terms of mIoU on the ADE20K val set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhisheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation. (arXiv:2204.01971v1 [cs.CV])","link":"http://arxiv.org/abs/2204.01971","description":"<p>Available 3D human pose estimation approaches leverage different forms of\nstrong (2D/3D pose) or weak (multi-view or depth) paired supervision. Barring\nsynthetic or in-studio domains, acquiring such supervision for each new target\nenvironment is highly inconvenient. To this end, we cast 3D pose learning as a\nself-supervised adaptation problem that aims to transfer the task knowledge\nfrom a labeled source domain to a completely unpaired target. We propose to\ninfer image-to-pose via two explicit mappings viz. image-to-latent and\nlatent-to-pose where the latter is a pre-learned decoder obtained from a\nprior-enforcing generative adversarial auto-encoder. Next, we introduce\nrelation distillation as a means to align the unpaired cross-modal samples i.e.\nthe unpaired target videos and unpaired 3D pose sequences. To this end, we\npropose a new set of non-local relations in order to characterize long-range\nlatent pose interactions unlike general contrastive relations where positive\ncouplings are limited to a local neighborhood structure. Further, we provide an\nobjective way to quantify non-localness in order to select the most effective\nrelation set. We evaluate different self-adaptation settings and demonstrate\nstate-of-the-art 3D human pose estimation performance on standard benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_J/0/1/0/all/0/1\">Jogendra Nath Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_S/0/1/0/all/0/1\">Siddharth Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamkhandi_A/0/1/0/all/0/1\">Anirudh Jamkhandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YM_P/0/1/0/all/0/1\">Pradyumna YM</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-visual multi-channel speech separation, dereverberation and recognition. (arXiv:2204.01977v1 [cs.SD])","link":"http://arxiv.org/abs/2204.01977","description":"<p>Despite the rapid advance of automatic speech recognition (ASR) technologies,\naccurate recognition of cocktail party speech characterised by the interference\nfrom overlapping speakers, background noise and room reverberation remains a\nhighly challenging task to date. Motivated by the invariance of visual modality\nto acoustic signal corruption, audio-visual speech enhancement techniques have\nbeen developed, although predominantly targeting overlapping speech separation\nand recognition tasks. In this paper, an audio-visual multi-channel speech\nseparation, dereverberation and recognition approach featuring a full\nincorporation of visual information into all three stages of the system is\nproposed. The advantage of the additional visual modality over using audio only\nis demonstrated on two neural dereverberation approaches based on DNN-WPE and\nspectral mapping respectively. The learning cost function mismatch between the\nseparation and dereverberation models and their integration with the back-end\nrecognition system is minimised using fine-tuning on the MSE and LF-MMI\ncriteria. Experiments conducted on the LRS2 dataset suggest that the proposed\naudio-visual multi-channel speech separation, dereverberation and recognition\nsystem outperforms the baseline audio-visual multi-channel speech separation\nand recognition system containing no dereverberation module by a statistically\nsignificant word error rate (WER) reduction of 2.06% absolute (8.77% relative).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guinan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Weight Respecification of Scan-specific Learning for Parallel Imaging. (arXiv:2204.01979v1 [eess.IV])","link":"http://arxiv.org/abs/2204.01979","description":"<p>Parallel imaging is widely used in magnetic resonance imaging as an\nacceleration technology. Traditional linear reconstruction methods in parallel\nimaging often suffer from noise amplification. Recently, a non-linear robust\nartificial-neural-network for k-space interpolation (RAKI) exhibits superior\nnoise resilience over other linear methods. However, RAKI performs poorly at\nhigh acceleration rates, and needs a large amount of autocalibration signals as\nthe training samples. In order to tackle these issues, we propose a\nmulti-weight method that implements multiple weighting matrices on the\nundersampled data, named as MW-RAKI. Enforcing multiple weighted matrices on\nthe measurements can effectively reduce the influence of noise and increase the\ndata constraints. Furthermore, we incorporate the strategy of multiple\nweighting matrixes into a residual version of RAKI, and form\nMW-rRAKI.Experimental compari-sons with the alternative methods demonstrated\nnoticeably better reconstruction performances, particularly at high\nacceleration rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tao_H/0/1/0/all/0/1\">Hui Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaoling Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bimodal Distributed Binarized Neural Networks. (arXiv:2204.02004v1 [cs.LG])","link":"http://arxiv.org/abs/2204.02004","description":"<p>Binary Neural Networks (BNNs) are an extremely promising method to reduce\ndeep neural networks' complexity and power consumption massively. Binarization\ntechniques, however, suffer from ineligible performance degradation compared to\ntheir full-precision counterparts.\n</p>\n<p>Prior work mainly focused on strategies for sign function approximation\nduring forward and backward phases to reduce the quantization error during the\nbinarization process. In this work, we propose a Bi-Modal Distributed\nbinarization method (\\methodname{}). That imposes bi-modal distribution of the\nnetwork weights by kurtosis regularization. The proposed method consists of a\ntraining scheme that we call Weight Distribution Mimicking (WDM), which\nefficiently imitates the full-precision network weight distribution to their\nbinary counterpart. Preserving this distribution during binarization-aware\ntraining creates robust and informative binary feature maps and significantly\nreduces the generalization error of the BNN. Extensive evaluations on CIFAR-10\nand ImageNet demonstrate the superiority of our method over current\nstate-of-the-art schemes. Our source code, experimental settings, training\nlogs, and binary models are available at\n\\url{https://github.com/BlueAnon/BD-BNN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozen_T/0/1/0/all/0/1\">Tal Rozen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimhi_M/0/1/0/all/0/1\">Moshe Kimhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chmiel_B/0/1/0/all/0/1\">Brian Chmiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendelson_A/0/1/0/all/0/1\">Avi Mendelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baskin_C/0/1/0/all/0/1\">Chaim Baskin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Video Salient Object Detection Progressively from Unlabeled Videos. (arXiv:2204.02008v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02008","description":"<p>Recent deep learning-based video salient object detection (VSOD) has achieved\nsome breakthrough, but these methods rely on expensive annotated videos with\npixel-wise annotations, weak annotations, or part of the pixel-wise\nannotations. In this paper, based on the similarities and the differences\nbetween VSOD and image salient object detection (SOD), we propose a novel VSOD\nmethod via a progressive framework that locates and segments salient objects in\nsequence without utilizing any video annotation. To use the knowledge learned\nin the SOD dataset for VSOD efficiently, we introduce dynamic saliency to\ncompensate for the lack of motion information of SOD during the locating\nprocess but retain the same fine segmenting process. Specifically, an algorithm\nfor generating spatiotemporal location labels, which consists of generating\nhigh-saliency location labels and tracking salient objects in adjacent frames,\nis proposed. Based on these location labels, a two-stream locating network that\nintroduces an optical flow branch for video salient object locating is\npresented. Although our method does not require labeled video at all, the\nexperimental results on five public benchmarks of DAVIS, FBMS, ViSal, VOS, and\nDAVSOD demonstrate that our proposed method is competitive with fully\nsupervised methods and outperforms the state-of-the-art weakly and unsupervised\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Binwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haoran Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_W/0/1/0/all/0/1\">Wentian Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1\">Weihua Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ronghua Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatentGAN Autoencoder: Learning Disentangled Latent Distribution. (arXiv:2204.02010v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02010","description":"<p>In autoencoder, the encoder generally approximates the latent distribution\nover the dataset, and the decoder generates samples using this learned latent\ndistribution. There is very little control over the latent vector as using the\nrandom latent vector for generation will lead to trivial outputs. This work\ntries to address this issue by using the LatentGAN generator to directly learn\nto approximate the latent distribution of the autoencoder and show meaningful\nresults on MNIST, 3D Chair, and CelebA datasets, an additional\ninformation-theoretic constrain is used which successfully learns to control\nautoencoder latent distribution. With this, our model also achieves an error\nrate of 2.38 on MNIST unsupervised image classification, which is better as\ncompared to InfoGAN and AAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalwar_S/0/1/0/all/0/1\">Sanket Kalwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aich_A/0/1/0/all/0/1\">Animikh Aich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dixit_T/0/1/0/all/0/1\">Tanay Dixit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts. (arXiv:2204.02028v1 [physics.ao-ph])","link":"http://arxiv.org/abs/2204.02028","description":"<p>Despite continuous improvements, precipitation forecasts are still not as\naccurate and reliable as those of other meteorological variables. A major\ncontributing factor to this is that several key processes affecting\nprecipitation distribution and intensity occur below the resolved scale of\nglobal weather models. Generative adversarial networks (GANs) have been\ndemonstrated by the computer vision community to be successful at\nsuper-resolution problems, i.e., learning to add fine-scale structure to coarse\nimages. Leinonen et al. (2020) previously applied a GAN to produce ensembles of\nreconstructed high-resolution atmospheric fields, given coarsened input data.\nIn this paper, we demonstrate this approach can be extended to the more\nchallenging problem of increasing the accuracy and resolution of comparatively\nlow-resolution input from a weather forecasting model, using high-resolution\nradar measurements as a \"ground truth\". The neural network must learn to add\nresolution and structure whilst accounting for non-negligible forecast error.\nWe show that GANs and VAE-GANs can match the statistical properties of\nstate-of-the-art pointwise post-processing methods whilst creating\nhigh-resolution, spatially coherent precipitation maps. Our model compares\nfavourably to the best existing downscaling methods in both pixel-wise and\npooled CRPS scores, power spectrum information and rank histograms (used to\nassess calibration). We test our models and show that they perform in a range\nof scenarios, including heavy rainfall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Harris_L/0/1/0/all/0/1\">Lucy Harris</a>, <a href=\"http://arxiv.org/find/physics/1/au:+McRae_A/0/1/0/all/0/1\">Andrew T. T. McRae</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chantry_M/0/1/0/all/0/1\">Matthew Chantry</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dueben_P/0/1/0/all/0/1\">Peter D. Dueben</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Palmer_T/0/1/0/all/0/1\">Tim N. Palmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reduce Information Bottleneck for Object Detection in Aerial Images. (arXiv:2204.02033v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02033","description":"<p>Object detection in aerial images is a fundamental research topic in the\ndomain of geoscience and remote sensing. However, advanced progresses on this\ntopic are mainly focused on the designment of backbone networks or header\nnetworks, but surprisingly ignored the neck ones. In this letter, we first\nanalyse the importance of the neck network in object detection frameworks from\nthe theory of information bottleneck. Then, to alleviate the information loss\nproblem in the current neck network, we propose a global semantic network,\nwhich acts as a bridge from the backbone to the head network in a bidirectional\nglobal convolution manner. Compared to the existing neck networks, our method\nhas advantages of capturing rich detailed information and less computational\ncosts. Moreover, we further propose a fusion refinement module, which is used\nfor feature fusion with rich details from different scales. To demonstrate the\neffectiveness and efficiency of our method, experiments are carried out on two\nchallenging datasets (i.e., DOTA and HRSC2016). Results in terms of accuracy\nand computational complexity both can verify the superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuchen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1\">Zhihao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1\">Liyong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuesong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qiaolin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DT2I: Dense Text-to-Image Generation from Region Descriptions. (arXiv:2204.02035v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02035","description":"<p>Despite astonishing progress, generating realistic images of complex scenes\nremains a challenging problem. Recently, layout-to-image synthesis approaches\nhave attracted much interest by conditioning the generator on a list of\nbounding boxes and corresponding class labels. However, previous approaches are\nvery restrictive because the set of labels is fixed a priori. Meanwhile,\ntext-to-image synthesis methods have substantially improved and provide a\nflexible way for conditional image generation. In this work, we introduce dense\ntext-to-image (DT2I) synthesis as a new task to pave the way toward more\nintuitive image generation. Furthermore, we propose DTC-GAN, a novel method to\ngenerate images from semantically rich region descriptions, and a multi-modal\nregion feature matching loss to encourage semantic image-text matching. Our\nresults demonstrate the capability of our approach to generate plausible images\nof complex scenes using region captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1\">Stanislav Frolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_P/0/1/0/all/0/1\">Prateek Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">J&#xf6;rn Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An efficient real-time target tracking algorithm using adaptive feature fusion. (arXiv:2204.02054v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02054","description":"<p>Visual-based target tracking is easily influenced by multiple factors, such\nas background clutter, targets fast-moving, illumination variation, object\nshape change, occlusion, etc. These factors influence the tracking accuracy of\na target tracking task. To address this issue, an efficient real-time target\ntracking method based on a low-dimension adaptive feature fusion is proposed to\nallow us the simultaneous implementation of the high-accuracy and real-time\ntarget tracking. First, the adaptive fusion of a histogram of oriented gradient\n(HOG) feature and color feature is utilized to improve the tracking accuracy.\nSecond, a convolution dimension reduction method applies to the fusion between\nthe HOG feature and color feature to reduce the over-fitting caused by their\nhigh-dimension fusions. Third, an average correlation energy estimation method\nis used to extract the relative confidence adaptive coefficients to ensure\ntracking accuracy. We experimentally confirm the proposed method on an OTB100\ndata set. Compared with nine popular target tracking algorithms, the proposed\nalgorithm gains the highest tracking accuracy and success tracking rate.\nCompared with the traditional Sum of Template and Pixel-wise LEarners (STAPLE)\nalgorithm, the proposed algorithm can obtain a higher success rate and\naccuracy, improving by 0.023 and 0.019, respectively. The experimental results\nalso demonstrate that the proposed algorithm can reach the real-time target\ntracking with 50 fps. The proposed method paves a more promising way for\nreal-time target tracking tasks under a complex environment, such as appearance\ndeformation, illumination change, motion blur, background, similarity, scale\nchange, and occlusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Changcheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_M/0/1/0/all/0/1\">Minglin Bie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation. (arXiv:2204.02070v1 [cs.LG])","link":"http://arxiv.org/abs/2204.02070","description":"<p>The paradigm of worst-group loss minimization has shown its promise in\navoiding to learn spurious correlations, but requires costly additional\nsupervision on spurious attributes. To resolve this, recent works focus on\ndeveloping weaker forms of supervision -- e.g., hyperparameters discovered with\na small number of validation samples with spurious attribute annotation -- but\nnone of the methods retain comparable performance to methods using full\nsupervision on the spurious attribute. In this paper, instead of searching for\nweaker supervisions, we ask: Given access to a fixed number of samples with\nspurious attribute annotations, what is the best achievable worst-group loss if\nwe \"fully exploit\" them? To this end, we propose a pseudo-attribute-based\nalgorithm, coined Spread Spurious Attribute (SSA), for improving the\nworst-group accuracy. In particular, we leverage samples both with and without\nspurious attribute annotations to train a model to predict the spurious\nattribute, then use the pseudo-attribute predicted by the trained model as\nsupervision on the spurious attribute to train a new robust model having\nminimal worst-group loss. Our experiments on various benchmark datasets show\nthat our algorithm consistently outperforms the baseline methods using the same\nnumber of validation samples with spurious attribute annotations. We also\ndemonstrate that the proposed SSA can achieve comparable performances to\nmethods using full (100%) spurious attribute supervision, by using a much\nsmaller number of annotated samples -- from 0.6% and up to 1.5%, depending on\nthe dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Junhyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaeho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Split Hierarchical Variational Compression. (arXiv:2204.02071v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02071","description":"<p>Variational autoencoders (VAEs) have witnessed great success in performing\nthe compression of image datasets. This success, made possible by the bits-back\ncoding framework, has produced competitive compression performance across many\nbenchmarks. However, despite this, VAE architectures are currently limited by a\ncombination of coding practicalities and compression ratios. That is, not only\ndo state-of-the-art methods, such as normalizing flows, often demonstrate\nout-performance, but the initial bits required in coding makes single and\nparallel image compression challenging. To remedy this, we introduce Split\nHierarchical Variational Compression (SHVC). SHVC introduces two novelties.\nFirstly, we propose an efficient autoregressive prior, the autoregressive\nsub-pixel convolution, that allows a generalisation between per-pixel\nautoregressions and fully factorised probability models. Secondly, we define\nour coding framework, the autoregressive initial bits, that flexibly supports\nparallel coding and avoids -- for the first time -- many of the practicalities\ncommonly associated with bits-back coding. In our experiments, we demonstrate\nSHVC is able to achieve state-of-the-art compression performance across\nfull-resolution lossless image compression tasks, with up to 100x fewer model\nparameters than competing VAE approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ryder_T/0/1/0/all/0/1\">Tom Ryder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_N/0/1/0/all/0/1\">Ning Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shifeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex-Valued Autoencoders for Object Discovery. (arXiv:2204.02075v1 [cs.LG])","link":"http://arxiv.org/abs/2204.02075","description":"<p>Object-centric representations form the basis of human perception and enable\nus to reason about the world and to systematically generalize to new settings.\nCurrently, most machine learning work on unsupervised object discovery focuses\non slot-based approaches, which explicitly separate the latent representations\nof individual objects. While the result is easily interpretable, it usually\nrequires the design of involved architectures. In contrast to this, we propose\na distributed approach to object-centric representations: the Complex\nAutoEncoder. Following a coding scheme theorized to underlie object\nrepresentations in biological neurons, its complex-valued activations represent\ntwo messages: their magnitudes express the presence of a feature, while the\nrelative phase differences between neurons express which features should be\nbound together to create joint object representations. We show that this simple\nand efficient approach achieves better reconstruction performance than an\nequivalent real-valued autoencoder on simple multi-object datasets.\nAdditionally, we show that it achieves competitive unsupervised object\ndiscovery performance to a SlotAttention model on two datasets, and manages to\ndisentangle objects in a third dataset where SlotAttention fails - all while\nbeing 7-70 times faster to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lowe_S/0/1/0/all/0/1\">Sindy L&#xf6;we</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1\">Maja Rudolph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1\">Max Welling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Semantic Segmentation with Error Localization Network. (arXiv:2204.02078v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02078","description":"<p>This paper studies semi-supervised learning of semantic segmentation, which\nassumes that only a small portion of training images are labeled and the others\nremain unlabeled. The unlabeled images are usually assigned pseudo labels to be\nused in training, which however often causes the risk of performance\ndegradation due to the confirmation bias towards errors on the pseudo labels.\nWe present a novel method that resolves this chronic issue of pseudo labeling.\nAt the heart of our method lies error localization network (ELN), an auxiliary\nmodule that takes an image and its segmentation prediction as input and\nidentifies pixels whose pseudo labels are likely to be wrong. ELN enables\nsemi-supervised learning to be robust against inaccurate pseudo labels by\ndisregarding label noises during training and can be naturally integrated with\nself-training and contrastive learning. Moreover, we introduce a new learning\nstrategy for ELN that simulates plausible and diverse segmentation errors\nduring training of ELN to enhance its generalization. Our method is evaluated\non PASCAL VOC 2012 and Cityscapes, where it outperforms all existing methods in\nevery evaluation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1\">Donghyeon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Online Multi-Object Tracking in Compressed Domain. (arXiv:2204.02081v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02081","description":"<p>Recent online Multi-Object Tracking (MOT) methods have achieved desirable\ntracking performance. However, the tracking speed of most existing methods is\nrather slow. Inspired from the fact that the adjacent frames are highly\nrelevant and redundant, we divide the frames into key and non-key frames\nrespectively and track objects in the compressed domain. For the key frames,\nthe RGB images are restored for detection and data association. To make data\nassociation more reliable, an appearance Convolutional Neural Network (CNN)\nwhich can be jointly trained with the detector is proposed. For the non-key\nframes, the objects are directly propagated by a tracking CNN based on the\nmotion information provided in the compressed domain. Compared with the\nstate-of-the-art online MOT methods,our tracker is about 6x faster while\nmaintaining a comparable tracking performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiankun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Hyperspectral Imaging in Hardware via Trained Metasurface Encoders. (arXiv:2204.02084v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02084","description":"<p>Hyperspectral imaging has attracted significant attention to identify\nspectral signatures for image classification and automated pattern recognition\nin computer vision. State-of-the-art implementations of snapshot hyperspectral\nimaging rely on bulky, non-integrated, and expensive optical elements,\nincluding lenses, spectrometers, and filters. These macroscopic components do\nnot allow fast data processing for, e.g real-time and high-resolution videos.\nThis work introduces Hyplex, a new integrated architecture addressing the\nlimitations discussed above. Hyplex is a CMOS-compatible, fast hyperspectral\ncamera that replaces bulk optics with nanoscale metasurfaces inversely designed\nthrough artificial intelligence. Hyplex does not require spectrometers but\nmakes use of conventional monochrome cameras, opening up the possibility for\nreal-time and high-resolution hyperspectral imaging at inexpensive costs.\nHyplex exploits a model-driven optimization, which connects the physical\nmetasurfaces layer with modern visual computing approaches based on end-to-end\ntraining. We design and implement a prototype version of Hyplex and compare its\nperformance against the state-of-the-art for typical imaging tasks such as\nspectral reconstruction and semantic segmentation. In all benchmarks, Hyplex\nreports the smallest reconstruction error. We additionally present what is, to\nthe best of our knowledge, the largest publicly available labeled hyperspectral\ndataset for semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makarenko_M/0/1/0/all/0/1\">Maksim Makarenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burguete_Lopez_A/0/1/0/all/0/1\">Arturo Burguete-Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getman_F/0/1/0/all/0/1\">Fedor Getman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fratalocchi_A/0/1/0/all/0/1\">Andrea Fratalocchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices. (arXiv:2204.02090v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02090","description":"<p>In this paper, we address the problem of lip-voice synchronisation in videos\ncontaining human face and voice. Our approach is based on determining if the\nlips motion and the voice in a video are synchronised or not, depending on\ntheir audio-visual correspondence score. We propose an audio-visual cross-modal\ntransformer-based model that outperforms several baseline models in the\naudio-visual synchronisation task on the standard lip-reading speech benchmark\ndataset LRS2. While the existing methods focus mainly on the lip\nsynchronisation in speech videos, we also consider the special case of singing\nvoice. Singing voice is a more challenging use case for synchronisation due to\nsustained vowel sounds. We also investigate the relevance of lip\nsynchronisation models trained on speech datasets in the context of singing\nvoice. Finally, we use the frozen visual features learned by our lip\nsynchronisation model in the singing voice separation task to outperform a\nbaseline audio-visual model which was trained end-to-end. The demos, source\ncode, and the pre-trained model will be made available on\nhttps://ipcv.github.io/VocaLiST/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadandale_V/0/1/0/all/0/1\">Venkatesh S. Kadandale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesinos_J/0/1/0/all/0/1\">Juan F. Montesinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haro_G/0/1/0/all/0/1\">Gloria Haro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior. (arXiv:2204.02091v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02091","description":"<p>Monocular depth estimation is vital for scene understanding and downstream\ntasks. We focus on the supervised setup, in which ground-truth depth is\navailable only at training time. Based on knowledge about the high regularity\nof real 3D scenes, we propose a method that learns to selectively leverage\ninformation from coplanar pixels to improve the predicted depth. In particular,\nwe introduce a piecewise planarity prior which states that for each pixel,\nthere is a seed pixel which shares the same planar 3D surface with the former.\nMotivated by this prior, we design a network with two heads. The first head\noutputs pixel-level plane coefficients, while the second one outputs a dense\noffset vector field that identifies the positions of seed pixels. The plane\ncoefficients of seed pixels are then used to predict depth at each position.\nThe resulting prediction is adaptively fused with the initial prediction from\nthe first head via a learned confidence to account for potential deviations\nfrom precise local planarity. The entire architecture is trained end-to-end\nthanks to the differentiability of the proposed modules and it learns to\npredict regular depth maps, with sharp edges at occlusion boundaries. An\nextensive evaluation of our method shows that we set the new state of the art\nin supervised monocular depth estimation, surpassing prior methods on NYU\nDepth-v2 and on the Garg split of KITTI. Our method delivers depth maps that\nyield plausible 3D reconstructions of the input scenes. Code is available at:\nhttps://github.com/SysCV/P3Depth\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_V/0/1/0/all/0/1\">Vaishakh Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Birds of A Feather Flock Together: Category-Divergence Guidance for Domain Adaptive Segmentation. (arXiv:2204.02111v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02111","description":"<p>Unsupervised domain adaptation (UDA) aims to enhance the generalization\ncapability of a certain model from a source domain to a target domain. Present\nUDA models focus on alleviating the domain shift by minimizing the feature\ndiscrepancy between the source domain and the target domain but usually ignore\nthe class confusion problem. In this work, we propose an Inter-class Separation\nand Intra-class Aggregation (ISIA) mechanism. It encourages the cross-domain\nrepresentative consistency between the same categories and differentiation\namong diverse categories. In this way, the features belonging to the same\ncategories are aligned together and the confusable categories are separated. By\nmeasuring the align complexity of each category, we design an Adaptive-weighted\nInstance Matching (AIM) strategy to further optimize the instance-level\nadaptation. Based on our proposed methods, we also raise a hierarchical\nunsupervised domain adaptation framework for cross-domain semantic segmentation\ntask. Through performing the image-level, feature-level, category-level and\ninstance-level alignment, our method achieves a stronger generalization\nperformance of the model from the source domain to the target domain. In two\ntypical cross-domain semantic segmentation tasks, i.e., GTA5 to Cityscapes and\nSYNTHIA to Cityscapes, our method achieves the state-of-the-art segmentation\naccuracy. We also build two cross-domain semantic segmentation datasets based\non the publicly available data, i.e., remote sensing building segmentation and\nroad segmentation, for domain adaptive segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Bo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Danpei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Catastrophic Forgetting in Incremental Object Detection via Elastic Response Distillation. (arXiv:2204.02136v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02136","description":"<p>Traditional object detectors are ill-equipped for incremental learning.\nHowever, fine-tuning directly on a well-trained detection model with only new\ndata will lead to catastrophic forgetting. Knowledge distillation is a flexible\nway to mitigate catastrophic forgetting. In Incremental Object Detection (IOD),\nprevious work mainly focuses on distilling for the combination of features and\nresponses. However, they under-explore the information that contains in\nresponses. In this paper, we propose a response-based incremental distillation\nmethod, dubbed Elastic Response Distillation (ERD), which focuses on\nelastically learning responses from the classification head and the regression\nhead. Firstly, our method transfers category knowledge while equipping student\ndetector with the ability to retain localization information during incremental\nlearning. In addition, we further evaluate the quality of all locations and\nprovide valuable responses by the Elastic Response Selection (ERS) strategy.\nFinally, we elucidate that the knowledge from different responses should be\nassigned with different importance during incremental distillation. Extensive\nexperiments conducted on MS COCO demonstrate our method achieves\nstate-of-the-art result, which substantially narrows the performance gap\ntowards full training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hangjie Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detector-Free Weakly Supervised Group Activity Recognition. (arXiv:2204.02139v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02139","description":"<p>Group activity recognition is the task of understanding the activity\nconducted by a group of people as a whole in a multi-person video. Existing\nmodels for this task are often impractical in that they demand ground-truth\nbounding box labels of actors even in testing or rely on off-the-shelf object\ndetectors. Motivated by this, we propose a novel model for group activity\nrecognition that depends neither on bounding box labels nor on object detector.\nOur model based on Transformer localizes and encodes partial contexts of a\ngroup activity by leveraging the attention mechanism, and represents a video\nclip as a set of partial context embeddings. The embedding vectors are then\naggregated to form a single group representation that reflects the entire\ncontext of an activity while capturing temporal evolution of each partial\ncontext. Our method achieves outstanding performance on two benchmarks,\nVolleyball and NBA datasets, surpassing not only the state of the art trained\nwith the same level of supervision, but also some of existing models relying on\nstronger supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongkeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinsung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-AI: Dual-path Action Interaction Learning for Group Activity Recognition. (arXiv:2204.02148v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02148","description":"<p>Learning spatial-temporal relation among multiple actors is crucial for group\nactivity recognition. Different group activities often show the diversified\ninteractions between actors in the video. Hence, it is often difficult to model\ncomplex group activities from a single view of spatial-temporal actor\nevolution. To tackle this problem, we propose a distinct Dual-path Actor\nInteraction (DualAI) framework, which flexibly arranges spatial and temporal\ntransformers in two complementary orders, enhancing actor relations by\nintegrating merits from different spatiotemporal paths. Moreover, we introduce\na novel Multi-scale Actor Contrastive Loss (MAC-Loss) between two interactive\npaths of Dual-AI. Via self-supervised actor consistency in both frame and video\nlevels, MAC-Loss can effectively distinguish individual actor representations\nto reduce action confusion among different actors. Consequently, our Dual-AI\ncan boost group activity recognition by fusing such discriminative features of\ndifferent actors. To evaluate the proposed approach, we conduct extensive\nexperiments on the widely used benchmarks, including Volleyball, Collective\nActivity, and NBA datasets. The proposed Dual-AI achieves state-of-the-art\nperformance on all these datasets. It is worth noting the proposed Dual-AI with\n50% training data outperforms a number of recent approaches with 100% training\ndata. This confirms the generalization power of Dual-AI for group activity\nrecognition, even under the challenging scenarios of limited supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mingfei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lina Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Image Content Extraction: Operationalizing Machine Learning in Humanistic Photographic Studies of Large Visual Archives. (arXiv:2204.02149v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02149","description":"<p>Applying machine learning tools to digitized image archives has a potential\nto revolutionize quantitative research of visual studies in humanities and\nsocial sciences. The ability to process a hundredfold greater number of photos\nthan has been traditionally possible and to analyze them with an extensive set\nof variables will contribute to deeper insight into the material. Overall,\nthese changes will help to shift the workflow from simple manual tasks to more\ndemanding stages.\n</p>\n<p>In this paper, we introduce Automatic Image Content Extraction (AICE)\nframework for machine learning-based search and analysis of large image\narchives. We developed the framework in a multidisciplinary research project as\nframework for future photographic studies by reformulating and expanding the\ntraditional visual content analysis methodologies to be compatible with the\ncurrent and emerging state-of-the-art machine learning tools and to cover the\nnovel machine learning opportunities for automatic content analysis. The\nproposed framework can be applied in several domains in humanities and social\nsciences, and it can be adjusted and scaled into various research settings. We\nalso provide information on the current state of different machine learning\ntechniques and show that there are already various publicly available methods\nthat are suitable to a wide-scale of visual content analysis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mannisto_A/0/1/0/all/0/1\">Anssi M&#xe4;nnist&#xf6;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seker_M/0/1/0/all/0/1\">Mert Seker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raitoharju_J/0/1/0/all/0/1\">Jenni Raitoharju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Equivariant Features for Absolute Pose Regression. (arXiv:2204.02163v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02163","description":"<p>While end-to-end approaches have achieved state-of-the-art performance in\nmany perception tasks, they are not yet able to compete with 3D geometry-based\nmethods in pose estimation. Moreover, absolute pose regression has been shown\nto be more related to image retrieval. As a result, we hypothesize that the\nstatistical features learned by classical Convolutional Neural Networks do not\ncarry enough geometric information to reliably solve this inherently geometric\ntask. In this paper, we demonstrate how a translation and rotation equivariant\nConvolutional Neural Network directly induces representations of camera motions\ninto the feature space. We then show that this geometric property allows for\nimplicitly augmenting the training data under a whole group of image\nplane-preserving transformations. Therefore, we argue that directly learning\nequivariant features is preferable than learning data-intensive intermediate\nrepresentations. Comprehensive experimental validation demonstrates that our\nlightweight model outperforms existing ones on standard datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Musallam_M/0/1/0/all/0/1\">Mohamed Adel Musallam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaudilliere_V/0/1/0/all/0/1\">Vincent Gaudilliere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_M/0/1/0/all/0/1\">Miguel Ortiz del Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ismaeil_K/0/1/0/all/0/1\">Kassem Al Ismaeil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1\">Djamila Aouada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Learning of Feature Extraction and Cost Aggregation for Semantic Correspondence. (arXiv:2204.02164v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02164","description":"<p>Establishing dense correspondences across semantically similar images is one\nof the challenging tasks due to the significant intra-class variations and\nbackground clutters. To solve these problems, numerous methods have been\nproposed, focused on learning feature extractor or cost aggregation\nindependently, which yields sub-optimal performance. In this paper, we propose\na novel framework for jointly learning feature extraction and cost aggregation\nfor semantic correspondence. By exploiting the pseudo labels from each module,\nthe networks consisting of feature extraction and cost aggregation modules are\nsimultaneously learned in a boosting fashion. Moreover, to ignore unreliable\npseudo labels, we present a confidence-aware contrastive loss function for\nlearning the networks in a weakly-supervised manner. We demonstrate our\ncompetitive results on standard benchmarks for semantic correspondence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Youngjo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Mira Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Transformer for 3D Visual Grounding. (arXiv:2204.02174v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02174","description":"<p>The 3D visual grounding task aims to ground a natural language description to\nthe targeted object in a 3D scene, which is usually represented in 3D point\nclouds. Previous works studied visual grounding under specific views. The\nvision-language correspondence learned by this way can easily fail once the\nview changes. In this paper, we propose a Multi-View Transformer (MVT) for 3D\nvisual grounding. We project the 3D scene to a multi-view space, in which the\nposition information of the 3D scene under different views are modeled\nsimultaneously and aggregated together. The multi-view space enables the\nnetwork to learn a more robust multi-modal representation for 3D visual\ngrounding and eliminates the dependence on specific views. Extensive\nexperiments show that our approach significantly outperforms all\nstate-of-the-art methods. Specifically, on Nr3D and Sr3D datasets, our method\noutperforms the best competitor by 11.2% and 7.1% and even surpasses recent\nwork with extra 2D assistance by 5.9% and 6.6%. Our code is available at\nhttps://github.com/sega-hsj/MVT-3DVG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Equipped with Neural Resizer on Facial Expression Recognition Task. (arXiv:2204.02181v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02181","description":"<p>When it comes to wild conditions, Facial Expression Recognition is often\nchallenged with low-quality data and imbalanced, ambiguous labels. This field\nhas much benefited from CNN based approaches; however, CNN models have\nstructural limitation to see the facial regions in distant. As a remedy,\nTransformer has been introduced to vision fields with global receptive field,\nbut requires adjusting input spatial size to the pretrained models to enjoy\ntheir strong inductive bias at hands. We herein raise a question whether using\nthe deterministic interpolation method is enough to feed low-resolution data to\nTransformer. In this work, we propose a novel training framework, Neural\nResizer, to support Transformer by compensating information and downscaling in\na data-driven manner trained with loss function balancing the noisiness and\nimbalance. Experiments show our Neural Resizer with F-PDLS loss function\nimproves the performance with Transformer variants in general and nearly\nachieves the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1\">Hyeonbin Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Wei-Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jiho Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_K/0/1/0/all/0/1\">Kyungtae Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hyeon Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNUG: Self-Supervised Neural Dynamic Garments. (arXiv:2204.02219v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02219","description":"<p>We present a self-supervised method to learn dynamic 3D deformations of\ngarments worn by parametric human bodies. State-of-the-art data-driven\napproaches to model 3D garment deformations are trained using supervised\nstrategies that require large datasets, usually obtained by expensive\nphysics-based simulation methods or professional multi-camera capture setups.\nIn contrast, we propose a new training scheme that removes the need for\nground-truth samples, enabling self-supervised training of dynamic 3D garment\ndeformations. Our key contribution is to realize that physics-based deformation\nmodels, traditionally solved in a frame-by-frame basis by implicit integrators,\ncan be recasted as an optimization problem. We leverage such optimization-based\nscheme to formulate a set of physics-based loss terms that can be used to train\nneural networks without precomputing ground-truth data. This allows us to learn\nmodels for interactive garments, including dynamic deformations and fine\nwrinkles, with two orders of magnitude speed up in training time compared to\nstate-of-the-art supervised methods\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santesteban_I/0/1/0/all/0/1\">Igor Santesteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otaduy_M/0/1/0/all/0/1\">Miguel A. Otaduy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Dan Casas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Sparsity Meets Dynamic Convolution. (arXiv:2204.02227v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02227","description":"<p>Dynamic convolution achieves a substantial performance boost for efficient\nCNNs at a cost of increased convolutional weights. Contrastively, mask-based\nunstructured pruning obtains a lightweight network by removing redundancy in\nthe heavy network at risk of performance drop. In this paper, we propose a new\nframework to coherently integrate these two paths so that they can complement\neach other compensate for the disadvantages. We first design a binary mask\nderived from a learnable threshold to prune static kernels, significantly\nreducing the parameters and computational cost but achieving higher performance\nin Imagenet-1K(0.6\\% increase in top-1 accuracy with 0.67G fewer FLOPs). Based\non this learnable mask, we further propose a novel dynamic sparse network\nincorporating the dynamic routine mechanism, which exerts much higher accuracy\nthan baselines ($2.63\\%$ increase in top-1 accuracy for MobileNetV1 with $90\\%$\nsparsity). As a result, our method demonstrates a more efficient dynamic\nconvolution with sparsity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenbo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images. (arXiv:2204.02232v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02232","description":"<p>We propose a neural inverse rendering pipeline called IRON that operates on\nphotometric images and outputs high-quality 3D content in the format of\ntriangle meshes and material textures readily deployable in existing graphics\npipelines. Our method adopts neural representations for geometry as signed\ndistance fields (SDFs) and materials during optimization to enjoy their\nflexibility and compactness, and features a hybrid optimization scheme for\nneural SDFs: first, optimize using a volumetric radiance field approach to\nrecover correct topology, then optimize further using edgeaware physics-based\nsurface rendering for geometry refinement and disentanglement of materials and\nlighting. In the second stage, we also draw inspiration from mesh-based\ndifferentiable rendering, and design a novel edge sampling algorithm for neural\nSDFs to further improve performance. We show that our IRON achieves\nsignificantly better inverse rendering quality compared to prior works. Our\nproject page is here: https://kai-46.github.io/IRON-website/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1\">Fujun Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RBGNet: Ray-based Grouping for 3D Object Detection. (arXiv:2204.02251v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02251","description":"<p>As a fundamental problem in computer vision, 3D object detection is\nexperiencing rapid growth. To extract the point-wise features from the\nirregularly and sparsely distributed points, previous methods usually take a\nfeature grouping module to aggregate the point features to an object candidate.\nHowever, these methods have not yet leveraged the surface geometry of\nforeground objects to enhance grouping and 3D box generation. In this paper, we\npropose the RBGNet framework, a voting-based 3D detector for accurate 3D object\ndetection from point clouds. In order to learn better representations of object\nshape to enhance cluster features for predicting 3D boxes, we propose a\nray-based feature grouping module, which aggregates the point-wise features on\nobject surfaces using a group of determined rays uniformly emitted from cluster\ncenters. Considering the fact that foreground points are more meaningful for\nbox estimation, we design a novel foreground biased sampling strategy in\ndownsample process to sample more points on object surfaces and further boost\nthe detection performance. Our model achieves state-of-the-art 3D detection\nperformance on ScanNet V2 and SUN RGB-D with remarkable performance gains. Code\nwill be available at https://github.com/Haiyang-W/RBGNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaoshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ze Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rongyao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Probabilistic Normal Epipolar Constraint for Frame-To-Frame Rotation Optimization under Uncertain Feature Positions. (arXiv:2204.02256v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02256","description":"<p>The estimation of the relative pose of two camera views is a fundamental\nproblem in computer vision. Kneip et al. proposed to solve this problem by\nintroducing the normal epipolar constraint (NEC). However, their approach does\nnot take into account uncertainties, so that the accuracy of the estimated\nrelative pose is highly dependent on accurate feature positions in the target\nframe. In this work, we introduce the probabilistic normal epipolar constraint\n(PNEC) that overcomes this limitation by accounting for anisotropic and\ninhomogeneous uncertainties in the feature positions. To this end, we propose a\nnovel objective function, along with an efficient optimization scheme that\neffectively minimizes our objective while maintaining real-time performance. In\nexperiments on synthetic data, we demonstrate that the novel PNEC yields more\naccurate rotation estimates than the original NEC and several popular relative\nrotation estimation algorithms. Furthermore, we integrate the proposed method\ninto a state-of-the-art monocular rotation-only odometry system and achieve\nconsistently improved results for the real-world KITTI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muhle_D/0/1/0/all/0/1\">Dominik Muhle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koestler_L/0/1/0/all/0/1\">Lukas Koestler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arbitrary-Scale Image Synthesis. (arXiv:2204.02273v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02273","description":"<p>Positional encodings have enabled recent works to train a single adversarial\nnetwork that can generate images of different scales. However, these approaches\nare either limited to a set of discrete scales or struggle to maintain good\nperceptual quality at the scales for which the model is not trained explicitly.\nWe propose the design of scale-consistent positional encodings invariant to our\ngenerator's layers transformations. This enables the generation of\narbitrary-scale images even at scales unseen during training. Moreover, we\nincorporate novel inter-scale augmentations into our pipeline and partial\ngeneration training to facilitate the synthesis of consistent images at\narbitrary scales. Lastly, we show competitive results for a continuum of scales\non various commonly used datasets for image synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ntavelis_E/0/1/0/all/0/1\">Evangelos Ntavelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahbazi_M/0/1/0/all/0/1\">Mohamad Shahbazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kastanis_I/0/1/0/all/0/1\">Iason Kastanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding of the Functional Object-Oriented Network in Industrial Tasks. (arXiv:2204.02274v1 [cs.RO])","link":"http://arxiv.org/abs/2204.02274","description":"<p>In this preliminary work, we propose to design an activity recognition system\nthat is suitable for Industrie 4.0 (I4.0) applications, especially focusing on\nLearning from Demonstration (LfD) in collaborative robot tasks. More precisely,\nwe focus on the issue of data exchange between an activity recognition system\nand a collaborative robotic system. We propose an activity recognition system\nwith linked data using functional object-oriented network (FOON) to facilitate\nindustrial use cases. Initially, we drafted a FOON for our use case.\nAfterwards, an action is estimated by using object and hand recognition systems\ncoupled with a recurrent neural network, which refers to FOON objects and\nstates. Finally, the detected action is shared via a context broker using an\nexisting linked data model, thus enabling the robotic system to interpret the\naction and execute it afterwards. Our initial results show that FOON can be\nused for an industrial use case and that we can use existing linked data models\nin LfD applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayari_R/0/1/0/all/0/1\">Rafik Ayari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantano_M/0/1/0/all/0/1\">Matteo Pantano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulius_D/0/1/0/all/0/1\">David Paulius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Clustering via Center-Oriented Margin Free-Triplet Loss for Skin Lesion Detection in Highly Imbalanced Datasets. (arXiv:2204.02275v1 [eess.IV])","link":"http://arxiv.org/abs/2204.02275","description":"<p>Melanoma is a fatal skin cancer that is curable and has dramatically\nincreasing survival rate when diagnosed at early stages. Learning-based methods\nhold significant promise for the detection of melanoma from dermoscopic images.\nHowever, since melanoma is a rare disease, existing databases of skin lesions\npredominantly contain highly imbalanced numbers of benign versus malignant\nsamples. In turn, this imbalance introduces substantial bias in classification\nmodels due to the statistical dominance of the majority class. To address this\nissue, we introduce a deep clustering approach based on the latent-space\nembedding of dermoscopic images. Clustering is achieved using a novel\ncenter-oriented margin-free triplet loss (COM-Triplet) enforced on image\nembeddings from a convolutional neural network backbone. The proposed method\naims to form maximally-separated cluster centers as opposed to minimizing\nclassification error, so it is less sensitive to class imbalance. To avoid the\nneed for labeled data, we further propose to implement COM-Triplet based on\npseudo-labels generated by a Gaussian mixture model. Comprehensive experiments\nshow that deep clustering with COM-Triplet loss outperforms clustering with\ntriplet loss, and competing classifiers in both supervised and unsupervised\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ozturk_S/0/1/0/all/0/1\">Saban Ozturk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga Cukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lost in Latent Space: Disentangled Models and the Challenge of Combinatorial Generalisation. (arXiv:2204.02283v1 [cs.LG])","link":"http://arxiv.org/abs/2204.02283","description":"<p>Recent research has shown that generative models with highly disentangled\nrepresentations fail to generalise to unseen combination of generative factor\nvalues. These findings contradict earlier research which showed improved\nperformance in out-of-training distribution settings when compared to entangled\nrepresentations. Additionally, it is not clear if the reported failures are due\nto (a) encoders failing to map novel combinations to the proper regions of the\nlatent space or (b) novel combinations being mapped correctly but the\ndecoder/downstream process is unable to render the correct output for the\nunseen combinations. We investigate these alternatives by testing several\nmodels on a range of datasets and training settings. We find that (i) when\nmodels fail, their encoders also fail to map unseen combinations to correct\nregions of the latent space and (ii) when models succeed, it is either because\nthe test conditions do not exclude enough examples, or because excluded\ngenerative factors determine independent parts of the output image. Based on\nthese results, we argue that to generalise properly, models not only need to\ncapture factors of variation, but also understand how to invert the generative\nprocess that was used to generate the data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montero_M/0/1/0/all/0/1\">Milton L. Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowers_J/0/1/0/all/0/1\">Jeffrey S. Bowers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_R/0/1/0/all/0/1\">Rui Ponte Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_C/0/1/0/all/0/1\">Casimir J.H. Ludwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malhotra_G/0/1/0/all/0/1\">Gaurav Malhotra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering. (arXiv:2204.02285v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02285","description":"<p>While Visual Question Answering (VQA) has progressed rapidly, previous works\nraise concerns about robustness of current VQA models. In this work, we study\nthe robustness of VQA models from a novel perspective: visual context. We\nsuggest that the models over-rely on the visual context, i.e., irrelevant\nobjects in the image, to make predictions. To diagnose the model's reliance on\nvisual context and measure their robustness, we propose a simple yet effective\nperturbation technique, SwapMix. SwapMix perturbs the visual context by\nswapping features of irrelevant context objects with features from other\nobjects in the dataset. Using SwapMix we are able to change answers to more\nthan 45 % of the questions for a representative VQA model. Additionally, we\ntrain the models with perfect sight and find that the context over-reliance\nhighly depends on the quality of visual representations. In addition to\ndiagnosing, SwapMix can also be applied as a data augmentation strategy during\ntraining in order to regularize the context over-reliance. By swapping the\ncontext object features, the model reliance on context can be suppressed\neffectively. Two representative VQA models are studied using SwapMix: a\nco-attention model MCAN and a large-scale pretrained model LXMERT. Our\nexperiments on the popular GQA dataset show the effectiveness of SwapMix for\nboth diagnosing model robustness and regularizing the over-reliance on visual\ncontext. The code for our method is available at\nhttps://github.com/vipulgupta1011/swapmix\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vipul Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Visual Geo-localization for Large-Scale Applications. (arXiv:2204.02287v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02287","description":"<p>Visual Geo-localization (VG) is the task of estimating the position where a\ngiven photo was taken by comparing it with a large database of images of known\nlocations. To investigate how existing techniques would perform on a real-world\ncity-wide VG application, we build San Francisco eXtra Large, a new dataset\ncovering a whole city and providing a wide range of challenging cases, with a\nsize 30x bigger than the previous largest dataset for visual geo-localization.\nWe find that current methods fail to scale to such large datasets, therefore we\ndesign a new highly scalable training technique, called CosPlace, which casts\nthe training as a classification problem avoiding the expensive mining needed\nby the commonly used contrastive learning. We achieve state-of-the-art\nperformance on a wide range of datasets and find that CosPlace is robust to\nheavy domain changes. Moreover, we show that, compared to the previous\nstate-of-the-art, CosPlace requires roughly 80% less GPU memory at train time,\nand it achieves better results with 8x smaller descriptors, paving the way for\ncity-wide real-world visual geo-localization. Dataset, code and trained models\nare available for research purposes at https://github.com/gmberton/CosPlace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berton_G/0/1/0/all/0/1\">Gabriele Berton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masone_C/0/1/0/all/0/1\">Carlo Masone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Convolutional Surfaces. (arXiv:2204.02289v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02289","description":"<p>This work is concerned with a representation of shapes that disentangles\nfine, local and possibly repeating geometry, from global, coarse structures.\nAchieving such disentanglement leads to two unrelated advantages: i) a\nsignificant compression in the number of parameters required to represent a\ngiven geometry; ii) the ability to manipulate either global geometry, or local\ndetails, without harming the other. At the core of our approach lies a novel\npipeline and neural architecture, which are optimized to represent one specific\natlas, representing one 3D surface. Our pipeline and architecture are designed\nso that disentanglement of global geometry from local details is accomplished\nthrough optimization, in a completely unsupervised manner. We show that this\napproach achieves better neural shape compression than the state of the art, as\nwell as enabling manipulation and transfer of shape details. Project page at\n<a href=\"http://geometry.cs.ucl.ac.uk/projects/2022/cnnmaps/\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morreale_L/0/1/0/all/0/1\">Luca Morreale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir G. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSDF: Real-Time Neural Signed Distance Fields for Robot Perception. (arXiv:2204.02296v1 [cs.RO])","link":"http://arxiv.org/abs/2204.02296","description":"<p>We present iSDF, a continual learning system for real-time signed distance\nfield (SDF) reconstruction. Given a stream of posed depth images from a moving\ncamera, it trains a randomly initialised neural network to map input 3D\ncoordinate to approximate signed distance. The model is self-supervised by\nminimising a loss that bounds the predicted signed distance using the distance\nto the closest sampled point in a batch of query points that are actively\nsampled. In contrast to prior work based on voxel grids, our neural method is\nable to provide adaptive levels of detail with plausible filling in of\npartially observed regions and denoising of observations, all while having a\nmore compact representation. In evaluations against alternative methods on real\nand synthetic datasets of indoor environments, we find that iSDF produces more\naccurate reconstructions, and better approximations of collision costs and\ngradients useful for downstream planners in domains from navigation to\nmanipulation. Code and video results can be found at our project page:\nhttps://joeaortiz.github.io/iSDF/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1\">Alexander Clegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucar_E/0/1/0/all/0/1\">Edgar Sucar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Generalizable Dexterous Manipulation from Human Grasp Affordance. (arXiv:2204.02320v1 [cs.RO])","link":"http://arxiv.org/abs/2204.02320","description":"<p>Dexterous manipulation with a multi-finger hand is one of the most\nchallenging problems in robotics. While recent progress in imitation learning\nhas largely improved the sample efficiency compared to Reinforcement Learning,\nthe learned policy can hardly generalize to manipulate novel objects, given\nlimited expert demonstrations. In this paper, we propose to learn dexterous\nmanipulation using large-scale demonstrations with diverse 3D objects in a\ncategory, which are generated from a human grasp affordance model. This\ngeneralizes the policy to novel object instances within the same category. To\ntrain the policy, we propose a novel imitation learning objective jointly with\na geometric representation learning objective using our demonstrations. By\nexperimenting with relocating diverse objects in simulation, we show that our\napproach outperforms baselines with a large margin when manipulating novel\nobjects. We also ablate the importance on 3D object representation learning for\nmanipulation. We include videos, code, and additional information on the\nproject website - https://kristery.github.io/ILAD/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueh-Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A lightweight and accurate YOLO-like network for small target detection in Aerial Imagery. (arXiv:2204.02325v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02325","description":"<p>Despite the breakthrough deep learning performances achieved for automatic\nobject detection, small target detection is still a challenging problem,\nespecially when looking at fast and accurate solutions suitable for mobile or\nedge applications. In this work we present YOLO-S, a simple, fast and efficient\nnetwork for small target detection. The architecture exploits a small feature\nextractor based on Darknet20, as well as skip connection, via both bypass and\nconcatenation, and reshape-passthrough layer to alleviate the vanishing\ngradient problem, promote feature reuse across network and combine low-level\npositional information with more meaningful high-level information. To verify\nthe performances of YOLO-S, we build \"AIRES\", a novel dataset for cAr detectIon\nfRom hElicopter imageS acquired in Europe, and set up experiments on both AIRES\nand VEDAI datasets, benchmarking this architecture with four baseline\ndetectors. Furthermore, in order to handle efficiently the issue of data\ninsufficiency and domain gap when dealing with a transfer learning strategy, we\nintroduce a transitional learning task over a combined dataset based on DOTAv2\nand VEDAI and demonstrate that can enhance the overall accuracy with respect to\nmore general features transferred from COCO data. YOLO-S is from 25% to 50%\nfaster than YOLOv3 and only 15-25% slower than Tiny-YOLOv3, outperforming also\nYOLOv3 in terms of accuracy in a wide range of experiments. Further simulations\nperformed on SARD dataset demonstrate also its applicability to different\nscenarios such as for search and rescue operations. Besides, YOLO-S has an 87%\ndecrease of parameter size and almost one half FLOPs of YOLOv3, making\npractical the deployment for low-power industrial applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betti_A/0/1/0/all/0/1\">Alessandro Betti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations. (arXiv:2204.02380v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02380","description":"<p>Providing explanations in the context of Visual Question Answering (VQA)\npresents a fundamental problem in machine learning. To obtain detailed insights\ninto the process of generating natural language explanations for VQA, we\nintroduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with\nnatural language explanations. For each image-question pair in the CLEVR\ndataset, CLEVR-X contains multiple structured textual explanations which are\nderived from the original scene graphs. By construction, the CLEVR-X\nexplanations are correct and describe the reasoning and visual information that\nis necessary to answer a given question. We conducted a user study to confirm\nthat the ground-truth explanations in our proposed dataset are indeed complete\nand relevant. We present baseline results for generating natural language\nexplanations in the context of VQA using two state-of-the-art frameworks on the\nCLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation\ngeneration quality for different question and answer types. Additionally, we\nstudy the influence of using different numbers of ground-truth explanations on\nthe convergence of natural language generation (NLG) metrics. The CLEVR-X\ndataset is publicly available at\n\\url{https://explainableml.github.io/CLEVR-X/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koepke_A/0/1/0/all/0/1\">A. Sophia Koepke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Frequency Network with Spatial Attention Residual Refinement Module for Monocular Depth Estimation. (arXiv:2204.02386v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02386","description":"<p>Deep-learning-based approaches to depth estimation are rapidly advancing,\noffering superior performance over existing methods. To estimate the depth in\nreal-world scenarios, depth estimation models require the robustness of various\nnoise environments. In this work, a Pyramid Frequency Network(PFN) with Spatial\nAttention Residual Refinement Module(SARRM) is proposed to deal with the weak\nrobustness of existing deep-learning methods. To reconstruct depth maps with\naccurate details, the SARRM constructs a residual fusion method with an\nattention mechanism to refine the blur depth. The frequency division strategy\nis designed, and the frequency pyramid network is developed to extract features\nfrom multiple frequency bands. With the frequency strategy, PFN achieves better\nvisual accuracy than state-of-the-art methods in both indoor and outdoor scenes\non Make3D, KITTI depth, and NYUv2 datasets. Additional experiments on the noisy\nNYUv2 dataset demonstrate that PFN is more reliable than existing deep-learning\nmethods in high-noise scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhengyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer. (arXiv:2204.02389v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02389","description":"<p>Objects play a crucial role in our everyday activities. Though multisensory\nobject-centric learning has shown great potential lately, the modeling of\nobjects in prior work is rather unrealistic. ObjectFolder 1.0 is a recent\ndataset that introduces 100 virtualized objects with visual, acoustic, and\ntactile sensory data. However, the dataset is small in scale and the\nmultisensory data is of limited quality, hampering generalization to real-world\nscenarios. We present ObjectFolder 2.0, a large-scale, multisensory dataset of\ncommon household objects in the form of implicit neural representations that\nsignificantly enhances ObjectFolder 1.0 in three aspects. First, our dataset is\n10 times larger in the amount of objects and orders of magnitude faster in\nrendering time. Second, we significantly improve the multisensory rendering\nquality for all three modalities. Third, we show that models learned from\nvirtual objects in our dataset successfully transfer to their real-world\ncounterparts in three challenging tasks: object scale estimation, contact\nlocalization, and shape reconstruction. ObjectFolder 2.0 offers a new path and\ntestbed for multisensory learning in computer vision and robotics. The dataset\nis available at https://github.com/rhgao/ObjectFolder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruohan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_Z/0/1/0/all/0/1\">Zilin Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Yu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarke_S/0/1/0/all/0/1\">Samuel Clarke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1\">Jeannette Bohg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wenzhen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower. (arXiv:2204.02390v1 [cs.RO])","link":"http://arxiv.org/abs/2204.02390","description":"<p>We investigate pneumatic non-prehensile manipulation (i.e., blowing) as a\nmeans of efficiently moving scattered objects into a target receptacle. Due to\nthe chaotic nature of aerodynamic forces, a blowing controller must (i)\ncontinually adapt to unexpected changes from its actions, (ii) maintain\nfine-grained control, since the slightest misstep can result in large\nunintended consequences (e.g., scatter objects already in a pile), and (iii)\ninfer long-range plans (e.g., move the robot to strategic blowing locations).\nWe tackle these challenges in the context of deep reinforcement learning,\nintroducing a multi-frequency version of the spatial action maps framework.\nThis allows for efficient learning of vision-based policies that effectively\ncombine high-level planning and low-level closed-loop control for dynamic\nmobile manipulation. Experiments show that our system learns efficient\nbehaviors for the task, demonstrating in particular that blowing achieves\nbetter downstream performance than pushing, and that our policies improve\nperformance over baselines. Moreover, we show that our system naturally\nencourages emergent specialization between the different subpolicies spanning\nlow-level fine-grained control and high-level planning. On a real mobile robot\nequipped with a miniature air blower, we show that our simulation-trained\npolicies transfer well to a real environment and can generalize to novel\nobjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jimmy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xingyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusinkiewicz_S/0/1/0/all/0/1\">Szymon Rusinkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action-Conditioned Contrastive Policy Pretraining. (arXiv:2204.02393v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02393","description":"<p>Deep visuomotor policy learning achieves promising results in control tasks\nsuch as robotic manipulation and autonomous driving, where the action is\ngenerated from the visual input by the neural policy. However, it requires a\nhuge number of online interactions with the training environment, which limits\nits real-world application. Compared to the popular unsupervised feature\nlearning for visual recognition, feature pretraining for visuomotor control\ntasks is much less explored. In this work, we aim to pretrain policy\nrepresentations for driving tasks using hours-long uncurated YouTube videos. A\nnew contrastive policy pretraining method is developed to learn\naction-conditioned features from video frames with action pseudo labels.\nExperiments show that the resulting action-conditioned features bring\nsubstantial improvements to the downstream reinforcement learning and imitation\nlearning tasks, outperforming the weights pretrained from previous unsupervised\nlearning methods. Code and models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qihang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhenghao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SE(3)-Equivariant Attention Networks for Shape Reconstruction in Function Space. (arXiv:2204.02394v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02394","description":"<p>We propose the first SE(3)-equivariant coordinate-based network for learning\noccupancy fields from point clouds. In contrast to previous shape\nreconstruction methods that align the input to a regular grid, we operate\ndirectly on the irregular, unoriented point cloud. We leverage attention\nmechanisms in order to preserve the set structure (permutation equivariance and\nvariable length) of the input. At the same time, attention layers enable local\nshape modelling, a crucial property for scalability to large scenes. In\ncontrast to architectures that create a global signature for the shape, we\noperate on local tokens. Given an unoriented, sparse, noisy point cloud as\ninput, we produce equivariant features for each point. These serve as keys and\nvalues for the subsequent equivariant cross-attention blocks that parametrize\nthe occupancy field. By querying an arbitrary point in space, we predict its\noccupancy score. We show that our method outperforms previous SO(3)-equivariant\nmethods, as well as non-equivariant methods trained on SO(3)-augmented\ndatasets. More importantly, local modelling together with SE(3)-equivariance\ncreate an ideal setting for SE(3) scene reconstruction. We show that by\ntraining only on single objects and without any pre-segmentation, we can\nreconstruct a novel scene with single-object performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chatzipantazis_E/0/1/0/all/0/1\">Evangelos Chatzipantazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pertigkiozoglou_S/0/1/0/all/0/1\">Stefanos Pertigkiozoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SALISA: Saliency-based Input Sampling for Efficient Video Object Detection. (arXiv:2204.02397v1 [cs.CV])","link":"http://arxiv.org/abs/2204.02397","description":"<p>High-resolution images are widely adopted for high-performance object\ndetection in videos. However, processing high-resolution inputs comes with high\ncomputation costs, and naive down-sampling of the input to reduce the\ncomputation costs quickly degrades the detection performance. In this paper, we\npropose SALISA, a novel non-uniform SALiency-based Input SAmpling technique for\nvideo object detection that allows for heavy down-sampling of unimportant\nbackground regions while preserving the fine-grained details of a\nhigh-resolution image. The resulting image is spatially smaller, leading to\nreduced computational costs while enabling a performance comparable to a\nhigh-resolution input. To achieve this, we propose a differentiable resampling\nmodule based on a thin plate spline spatial transformer network (TPS-STN). This\nmodule is regularized by a novel loss to provide an explicit supervision signal\nto learn to \"magnify\" salient regions. We report state-of-the-art results in\nthe low compute regime on the ImageNet-VID and UA-DETRAC video object detection\ndatasets. We demonstrate that on both datasets, the mAP of an EfficientDet-D1\n(EfficientDet-D2) gets on par with EfficientDet-D2 (EfficientDet-D3) at a much\nlower computational cost. We also show that SALISA significantly improves the\ndetection of small objects. In particular, SALISA with an EfficientDet-D1\ndetector improves the detection of small objects by $77\\%$, and remarkably also\noutperforms EfficientDetD3 baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bejnordi_B/0/1/0/all/0/1\">Babak Ehteshami Bejnordi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habibian_A/0/1/0/all/0/1\">Amirhossein Habibian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodrati_A/0/1/0/all/0/1\">Amir Ghodrati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images. (arXiv:2001.09193v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.09193","description":"<p>Vertebral labelling and segmentation are two fundamental tasks in an\nautomated spine processing pipeline. Reliable and accurate processing of spine\nimages is expected to benefit clinical decision-support systems for diagnosis,\nsurgery planning, and population-based analysis on spine and bone health.\nHowever, designing automated algorithms for spine processing is challenging\npredominantly due to considerable variations in anatomy and acquisition\nprotocols and due to a severe shortage of publicly available data. Addressing\nthese limitations, the Large Scale Vertebrae Segmentation Challenge (VerSe) was\norganised in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI) in 2019 and 2020, with a\ncall for algorithms towards labelling and segmentation of vertebrae. Two\ndatasets containing a total of 374 multi-detector CT scans from 355 patients\nwere prepared and 4505 vertebrae have individually been annotated at\nvoxel-level by a human-machine hybrid algorithm (https://osf.io/nqjyw/,\nhttps://osf.io/t98fz/). A total of 25 algorithms were benchmarked on these\ndatasets. In this work, we present the the results of this evaluation and\nfurther investigate the performance-variation at vertebra-level, scan-level,\nand at different fields-of-view. We also evaluate the generalisability of the\napproaches to an implicit domain shift in data by evaluating the top performing\nalgorithms of one challenge iteration on data from the other iteration. The\nprincipal takeaway from VerSe: the performance of an algorithm in labelling and\nsegmenting a spine scan hinges on its ability to correctly identify vertebrae\nin cases of rare anatomical variations. The content and code concerning VerSe\ncan be accessed at: https://github.com/anjany/verse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekuboyina_A/0/1/0/all/0/1\">Anjany Sekuboyina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Husseini_M/0/1/0/all/0/1\">Malek E. Husseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_A/0/1/0/all/0/1\">Amirhossein Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loffler_M/0/1/0/all/0/1\">Maximilian L&#xf6;ffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebl_H/0/1/0/all/0/1\">Hans Liebl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetteh_G/0/1/0/all/0/1\">Giles Tetteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Payer_C/0/1/0/all/0/1\">Christian Payer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_D/0/1/0/all/0/1\">Darko &#x160;tern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urschler_M/0/1/0/all/0/1\">Martin Urschler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maodong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dalong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lessmann_N/0/1/0/all/0/1\">Nikolas Lessmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yujin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambellan_F/0/1/0/all/0/1\">Felix Ambellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiranashvili_T/0/1/0/all/0/1\">Tamaz Amiranashvili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehlke_M/0/1/0/all/0/1\">Moritz Ehlke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamecker_H/0/1/0/all/0/1\">Hans Lamecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehnert_S/0/1/0/all/0/1\">Sebastian Lehnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lirio_M/0/1/0/all/0/1\">Marilia Lirio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olaguer_N/0/1/0/all/0/1\">Nicol&#xe1;s P&#xe9;rez de Olaguer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramm_H/0/1/0/all/0/1\">Heiko Ramm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_M/0/1/0/all/0/1\">Manish Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tack_A/0/1/0/all/0/1\">Alexander Tack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zachow_S/0/1/0/all/0/1\">Stefan Zachow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angerman_C/0/1/0/all/0/1\">Christoph Angerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_K/0/1/0/all/0/1\">Kevin Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirszenberg_A/0/1/0/all/0/1\">Alexandre Kirszenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puybareau_E/0/1/0/all/0/1\">&#xc9;lodie Puybareau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Di Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yiwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rapazzo_B/0/1/0/all/0/1\">Brandon H. Rapazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeah_T/0/1/0/all/0/1\">Timyoas Yeah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Amber Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shangliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feng Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiangshang_Z/0/1/0/all/0/1\">Zheng Xiangshang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liming_X/0/1/0/all/0/1\">Xu Liming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netherton_T/0/1/0/all/0/1\">Tucker J. Netherton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mumme_R/0/1/0/all/0/1\">Raymond P. Mumme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Court_L/0/1/0/all/0/1\">Laurence E. Court</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixun Huang</a>, et al. (18 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization via Optimal Transport with Metric Similarity Learning. (arXiv:2007.10573v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.10573","description":"<p>Generalizing knowledge to unseen domains, where data and labels are\nunavailable, is crucial for machine learning models. We tackle the domain\ngeneralization problem to learn from multiple source domains and generalize to\na target domain with unknown statistics. The crucial idea is to extract the\nunderlying invariant features across all the domains. Previous domain\ngeneralization approaches mainly focused on learning invariant features and\nstacking the learned features from each source domain to generalize to a new\ntarget domain while ignoring the label information, which will lead to\nindistinguishable features with an ambiguous classification boundary. For this,\none possible solution is to constrain the label-similarity when extracting the\ninvariant features and to take advantage of the label similarities for\nclass-specific cohesion and separation of features across domains. Therefore we\nadopt optimal transport with Wasserstein distance, which could constrain the\nclass label similarity, for adversarial training and also further deploy a\nmetric learning objective to leverage the label information for achieving\ndistinguishable classification boundary. Empirical results show that our\nproposed method could outperform most of the baselines. Furthermore, ablation\nstudies also demonstrate the effectiveness of each component of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuqing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shui_C/0/1/0/all/0/1\">Changjian Shui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaib_draa_B/0/1/0/all/0/1\">Brahim Chaib-draa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification and Segmentation of Pulmonary Lesions in CT Images Using a Combined VGG-XGBoost Method, and an Integrated Fuzzy Clustering-Level Set Technique. (arXiv:2101.00948v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.00948","description":"<p>Given that lung cancer is one of the deadliest illnesses, early\nidentification and diagnosis are critical to preserving a patient's life.\nHowever, lung illness diagnosis is time-intensive and requires the expertise of\na pulmonary disease specialist, subject to a significant rate of inaccuracy.\nOur objective is to design a system capable of accurately detecting and\nclassifying lung lesions and segmenting them in CT-scan images. The suggested\ntechnique extracts features automatically from the CT-scan image and then\nclassifies them using Ensemble Gradient Boosting methods. Finally, if a lesion\nis detected in the CT-scan image, it is segmented using a hybrid approach based\non Fuzzy Clustering and Level Set. To train and test our models we gathered a\ndataset that included CT images of patients residing in Mashhad, Iran. Finally,\nthe results indicate 96% accuracy within this dataset. This approach may assist\nclinicians in diagnosing lung abnormalities and avoiding potential errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Javan_N/0/1/0/all/0/1\">Niloofar Akhavan Javan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jebreili_A/0/1/0/all/0/1\">Ali Jebreili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozafari_B/0/1/0/all/0/1\">Babak Mozafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinioun_M/0/1/0/all/0/1\">Morteza Hosseinioun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghahramani_S/0/1/0/all/0/1\">S. AmirAli Gh. Ghahramani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05642","description":"<p>While the importance of automatic image analysis is continuously increasing,\nrecent meta-research revealed major flaws with respect to algorithm validation.\nPerformance metrics are particularly key for meaningful, objective, and\ntransparent performance assessment and validation of the used automatic\nalgorithms, but relatively little attention has been given to the practical\npitfalls when using specific metrics for a given image analysis task. These are\ntypically related to (1) the disregard of inherent metric properties, such as\nthe behaviour in the presence of class imbalance or small target structures,\n(2) the disregard of inherent data set properties, such as the non-independence\nof the test cases, and (3) the disregard of the actual biomedical domain\ninterest that the metrics should reflect. This living dynamically document has\nthe purpose to illustrate important limitations of performance metrics commonly\napplied in the field of image analysis. In this context, it focuses on\nbiomedical image analysis problems that can be phrased as image-level\nclassification, semantic segmentation, instance segmentation, or object\ndetection task. The current version is based on a Delphi process on metrics\nconducted by an international consortium of image analysis experts from more\nthan 60 institutions worldwide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reinke_A/0/1/0/all/0/1\">Annika Reinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1\">Minu D. Tizabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudre_C/0/1/0/all/0/1\">Carole H. Sudre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1\">Matthias Eisenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radsch_T/0/1/0/all/0/1\">Tim R&#xe4;dsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_M/0/1/0/all/0/1\">Michael Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acion_L/0/1/0/all/0/1\">Laura Acion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonelli_M/0/1/0/all/0/1\">Michela Antonelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bankhead_P/0/1/0/all/0/1\">Peter Bankhead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benis_A/0/1/0/all/0/1\">Arriel Benis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1\">M. Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimini_B/0/1/0/all/0/1\">Beth Cimini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_G/0/1/0/all/0/1\">Gary S. Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamprecht_F/0/1/0/all/0/1\">Fred Hamprecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1\">Daniel A. Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1\">Doreen Heckmann-N&#xf6;tzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_M/0/1/0/all/0/1\">Michael M. Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huisman_M/0/1/0/all/0/1\">Merel Huisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannin_P/0/1/0/all/0/1\">Pierre Jannin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1\">Charles E. Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karargyris_A/0/1/0/all/0/1\">Alexandros Karargyris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1\">Alan Karthikesalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavur_E/0/1/0/all/0/1\">Emre Kavur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1\">Hannes Kenngott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooi_T/0/1/0/all/0/1\">Thijs Kooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozubek_M/0/1/0/all/0/1\">Michal Kozubek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreshuk_A/0/1/0/all/0/1\">Anna Kreshuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litjens_G/0/1/0/all/0/1\">Geert Litjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1\">Amin Madani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus Maier-Hein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattson_P/0/1/0/all/0/1\">Peter Mattson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meijering_E/0/1/0/all/0/1\">Erik Meijering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1\">Bjoern Menze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moher_D/0/1/0/all/0/1\">David Moher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moons_K/0/1/0/all/0/1\">Karel G.M. Moons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1\">Henning M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_F/0/1/0/all/0/1\">Felix Nickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, et al. (22 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Isotropy Maximization Loss: Seamless and High-Performance Out-of-Distribution Detection Simply Replacing the SoftMax Loss. (arXiv:2105.14399v10 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14399","description":"<p>Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (e.g., classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all previously mentioned\ndrawbacks). The entropic out-of-distribution detection solution uses the IsoMax\nloss for training and the entropic score for out-of-distribution detection. The\nIsoMax loss works as a drop-in replacement of the SoftMax loss (i.e., the\ncombination of the output linear layer, the SoftMax activation, and the\ncross-entropy loss) because swapping the SoftMax loss with the IsoMax loss\nrequires no changes in the model's architecture or training\nprocedures/hyperparameters. In this paper, we perform what we call an\nisometrization of the distances used in the IsoMax loss. Additionally, we\npropose replacing the entropic score with the minimum distance score.\nExperiments showed that these modifications significantly increase\nout-of-distribution detection performance while keeping the solution seamless.\nBesides being competitive with or outperforming all major current approaches,\nthe proposed solution avoids all their current limitations, in addition to\nbeing much easier to use because only a simple loss replacement for training\nthe neural network is required. The code to replace the SoftMax loss with the\nIsoMax+ loss and reproduce the results is available at\nhttps://github.com/dlmacedo/entropic-out-of-distribution-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Styleformer: Transformer based Generative Adversarial Networks with Style Vector. (arXiv:2106.07023v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07023","description":"<p>We propose Styleformer, which is a style-based generator for GAN\narchitecture, but a convolution-free transformer-based generator. In our paper,\nwe explain how a transformer can generate high-quality images, overcoming the\ndisadvantage that convolution operations are difficult to capture global\nfeatures in an image. Furthermore, we change the demodulation of StyleGAN2 and\nmodify the existing transformer structure (e.g., residual connection, layer\nnormalization) to create a strong style-based generator with a convolution-free\nstructure. We also make Styleformer lighter by applying Linformer, enabling\nStyleformer to generate higher resolution images and result in improvements in\nterms of speed and memory. We experiment with the low-resolution image dataset\nsuch as CIFAR-10, as well as the high-resolution image dataset like\nLSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark\ndataset, which is comparable performance to the current state-of-the-art and\noutperforms all GAN-based generative models, including StyleGAN2-ADA with fewer\nparameters on the unconditional setting. We also both achieve new\nstate-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10\nand CelebA. We release our code at\nhttps://github.com/Jeeseung-Park/Styleformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeeseung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younggeun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training of deep cross-modality conversion models with a small dataset, and their application in megavoltage CT to kilovoltage CT conversion. (arXiv:2107.05238v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05238","description":"<p>In recent years, deep-learning-based image processing has emerged as a\nvaluable tool for medical imaging owing to its high performance. However, the\nquality of deep-learning-based methods heavily relies on the amount of training\ndata; the high cost of acquiring a large dataset is a limitation to their\nutilization in medical fields. Herein, based on deep learning, we developed a\ncomputed tomography (CT) modality conversion method requiring only a few\nunsupervised images. The proposed method is based on CycleGAN with several\nextensions tailored for CT images, which aims at preserving the structure in\nthe processed images and reducing the amount of training data. This method was\napplied to realize the conversion of megavoltage computed tomography (MVCT) to\nkilovoltage computed tomography (kVCT) images. Training was conducted using\nseveral datasets acquired from patients with head and neck cancer. The size of\nthe datasets ranged from 16 slices (two patients) to 2745 slices (137 patients)\nfor MVCT and 2824 slices (98 patients) for kVCT. The required size of the\ntraining data was found to be as small as a few hundred slices. By statistical\nand visual evaluations, the quality improvement and structure preservation of\nthe MVCT images converted by the proposed model were investigated. As a\nclinical benefit, it was observed by medical doctors that the converted images\nenhanced the precision of contouring. We developed an MVCT to kVCT conversion\nmodel based on deep learning, which can be trained using only a few hundred\nunpaired images. The stability of the model against changes in data size was\ndemonstrated. This study promotes the reliable use of deep learning in clinical\nmedicine by partially answering commonly asked questions, such as \"Is our data\nsufficient?\" and \"How much data should we acquire?\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_S/0/1/0/all/0/1\">Sho Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaji_S/0/1/0/all/0/1\">Shizuo Kaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawa_K/0/1/0/all/0/1\">Kanabu Nawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imae_T/0/1/0/all/0/1\">Toshikazu Imae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoki_A/0/1/0/all/0/1\">Atsushi Aoki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamoto_T/0/1/0/all/0/1\">Takahiro Nakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohta_T/0/1/0/all/0/1\">Takeshi Ohta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozawa_Y/0/1/0/all/0/1\">Yuki Nozawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_H/0/1/0/all/0/1\">Hideomi Yamashita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haga_A/0/1/0/all/0/1\">Akihiro Haga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_K/0/1/0/all/0/1\">Keiichi Nakagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Image-based Illumination Harmonization. (arXiv:2108.00150v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00150","description":"<p>Integrating a foreground object into a background scene with illumination\nharmonization is an important but challenging task in computer vision and\naugmented reality community. Existing methods mainly focus on foreground and\nbackground appearance consistency or the foreground object shadow generation,\nwhich rarely consider global appearance and illumination harmonization. In this\npaper, we formulate seamless illumination harmonization as an illumination\nexchange and aggregation problem. Specifically, we firstly apply a\nphysically-based rendering method to construct a large-scale, high-quality\ndataset (named IH) for our task, which contains various types of foreground\nobjects and background scenes with different lighting conditions. Then, we\npropose a deep image-based illumination harmonization GAN framework named\nDIH-GAN, which makes full use of a multi-scale attention mechanism and\nillumination exchange strategy to directly infer mapping relationship between\nthe inserted foreground object and the corresponding background scene.\nMeanwhile, we also use adversarial learning strategy to further refine the\nillumination harmonization result. Our method can not only achieve harmonious\nappearance and illumination for the foreground object but also can generate\ncompelling shadow cast by the foreground object. Comprehensive experiments on\nboth our IH dataset and real-world images show that our proposed DIH-GAN\nprovides a practical and effective solution for image-based object illumination\nharmonization editing, and validate the superiority of our method against\nstate-of-the-art methods. Our IH dataset is available at\nhttps://github.com/zhongyunbao/Dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhongyun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1\">Chengjiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Gang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chunxia Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception. (arXiv:2108.08505v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.08505","description":"<p>Perceptual quality assessment of the videos acquired in the wilds is of vital\nimportance for quality assurance of video services. The inaccessibility of\nreference videos with pristine quality and the complexity of authentic\ndistortions pose great challenges for this kind of blind video quality\nassessment (BVQA) task. Although model-based transfer learning is an effective\nand efficient paradigm for the BVQA task, it remains to be a challenge to\nexplore what and how to bridge the domain shifts for better video\nrepresentation. In this work, we propose to transfer knowledge from image\nquality assessment (IQA) databases with authentic distortions and large-scale\naction recognition with rich motion patterns. We rely on both groups of data to\nlearn the feature extractor. We train the proposed model on the target VQA\ndatabases using a mixed list-wise ranking loss function. Extensive experiments\non six databases demonstrate that our method performs very competitively under\nboth individual database and mixed database training settings. We also verify\nthe rationality of each component of the proposed method and explore a simple\nmanner for further improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weixia Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_M/0/1/0/all/0/1\">Meng Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xianpei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Field-Based Underwater 3D Reconstruction Via Angular Resampling. (arXiv:2109.02116v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02116","description":"<p>Recovering 3D geometry of underwater scenes is challenging because of\nnon-linear refraction of light at the water-air interface caused by the camera\nhousing. We present a light field-based approach that leverages properties of\nangular samples for high-quality underwater 3D reconstruction from a single\nviewpoint. Specifically, we resample the light field image to angular patches.\nAs underwater scenes exhibit weak view-dependent specularity, an angular patch\ntends to have uniform intensity when sampled at the correct depth. We thus\nimpose this angular uniformity as a constraint for depth estimation. For\nefficient angular resampling, we design a fast approximation algorithm based on\nmultivariate polynomial regression to approximate nonlinear refraction paths.\nWe further develop a light field calibration algorithm that estimates the\nwater-air interface geometry along with the camera parameters. Comprehensive\nexperiments on synthetic and real data show our method produces\nstate-of-the-art reconstruction on static and dynamic underwater scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuqi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jinwei Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation. (arXiv:2109.04871v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04871","description":"<p>Event camera has offered promising alternative for visual perception,\nespecially in high speed and high dynamic range scenes. Recently, many deep\nlearning methods have shown great success in providing promising solutions to\nmany event-based problems, such as optical flow estimation. However, existing\ndeep learning methods did not address the importance of temporal information\nwell from the perspective of architecture design and cannot effectively extract\nspatio-temporal features. Another line of research that utilizes Spiking Neural\nNetwork suffers from training issues for deeper architecture.To address these\npoints, a novel input representation is proposed that captures the events'\ntemporal distribution for signal enhancement. Moreover, we introduce a\nspatio-temporal recurrent encoding-decoding neural network architecture for\nevent-based optical flow estimation, which utilizes Convolutional Gated\nRecurrent Units to extract feature maps from a series of event images. Besides,\nour architecture allows some traditional frame-based core modules, such as\ncorrelation layer and iterative residual refine scheme, to be incorporated. The\nnetwork is end-to-end trained with self-supervised learning on the\nMulti-Vehicle Stereo Event Camera dataset. We have shown that it outperforms\nall the existing state-of-the-art methods by a large margin. The code link is\nhttps://github.com/ruizhao26/STE-FlowNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Ziluo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianxiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Abstraction in Distributed Probabilistic SLAM Graphs. (arXiv:2109.06241v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06241","description":"<p>Scene graphs represent the key components of a scene in a compact and\nsemantically rich way, but are difficult to build during incremental SLAM\noperation because of the challenges of robustly identifying abstract scene\nelements and optimising continually changing, complex graphs. We present a\ndistributed, graph-based SLAM framework for incrementally building scene graphs\nbased on two novel components. First, we propose an incremental abstraction\nframework in which a neural network proposes abstract scene elements that are\nincorporated into the factor graph of a feature-based monocular SLAM system.\nScene elements are confirmed or rejected through optimisation and incrementally\nreplace the points yielding a more dense, semantic and compact representation.\nSecond, enabled by our novel routing procedure, we use Gaussian Belief\nPropagation (GBP) for distributed inference on a graph processor. The time per\niteration of GBP is structure-agnostic and we demonstrate the speed advantages\nover direct methods for inference of heterogeneous factor graphs. We run our\nsystem on real indoor datasets using planar abstractions and recover the major\nplanes with significant compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_T/0/1/0/all/0/1\">Talfan Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucar_E/0/1/0/all/0/1\">Edgar Sucar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition. (arXiv:2109.07270v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07270","description":"<p>We present a novel facial expression recognition network, called Distract\nyour Attention Network (DAN). Our method is based on two key observations.\nFirstly, multiple classes share inherently similar underlying facial\nappearance, and their differences could be subtle. Secondly, facial expressions\nexhibit themselves through multiple facial regions simultaneously, and the\nrecognition requires a holistic approach by encoding high-order interactions\namong local features. To address these issues, we propose our DAN with three\nkey components: Feature Clustering Network (FCN), Multi-head cross Attention\nNetwork (MAN), and Attention Fusion Network (AFN). The FCN extracts robust\nfeatures by adopting a large-margin learning objective to maximize class\nseparability. In addition, the MAN instantiates a number of attention heads to\nsimultaneously attend to multiple facial areas and build attention maps on\nthese regions. Further, the AFN distracts these attentions to multiple\nlocations before fusing the attention maps to a comprehensive one. Extensive\nexperiments on three public datasets (including AffectNet, RAF-DB, and SFEW\n2.0) verified that the proposed method consistently achieves state-of-the-art\nfacial expression recognition performance. Code will be made available at\nhttps://github.com/yaoing/DAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhengyao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wenzhong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Ge Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Markerless Suture Needle 6D Pose Tracking with Robust Uncertainty Estimation for Autonomous Minimally Invasive Robotic Surgery. (arXiv:2109.12722v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.12722","description":"<p>Suture needle localization is necessary for autonomous suturing. Previous\napproaches in autonomous suturing often relied on fiducial markers rather than\nmarkerless detection schemes for localizing a suture needle due to the\ninconsistency of markerless detections. However, fiducial markers are not\npractical for real-world applications and can often be occluded from\nenvironmental factors in surgery (e.g., blood). Therefore in this work, we\npresent a robust tracking approach for estimating the 6D pose of a suture\nneedle when using inconsistent detections. We define observation models based\non suture needles' geometry that captures the uncertainty of the detections and\nfuse them temporally in a probabilistic fashion. In our experiments, we compare\ndifferent permutations of the observation models in the suture needle\nlocalization task to show their effectiveness. Our proposed method outperforms\nprevious approaches in localizing a suture needle. We also demonstrate the\nproposed tracking method in an autonomous suture needle regrasping task and ex\nvivo environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_Z/0/1/0/all/0/1\">Zih-Yun Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_A/0/1/0/all/0/1\">Albert Z Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_F/0/1/0/all/0/1\">Florian Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_B/0/1/0/all/0/1\">Bjorn Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1\">Michael C. Yip</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02711","description":"<p>Recently, GAN inversion methods combined with Contrastive Language-Image\nPretraining (CLIP) enables zero-shot image manipulation guided by text prompts.\nHowever, their applications to diverse real images are still difficult due to\nthe limited GAN inversion capability. Specifically, these approaches often have\ndifficulties in reconstructing images with novel poses, views, and highly\nvariable contents compared to the training data, altering object identity, or\nproducing unwanted image artifacts. To mitigate these problems and enable\nfaithful manipulation of real images, we propose a novel method, dubbed\nDiffusionCLIP, that performs text-driven image manipulation using diffusion\nmodels. Based on full inversion capability and high-quality image generation\npower of recent diffusion models, our method performs zero-shot image\nmanipulation successfully even between unseen domains and takes another step\ntowards general application by manipulating images from a widely varying\nImageNet dataset. Furthermore, we propose a novel noise combination method that\nallows straightforward multi-attribute manipulation. Extensive experiments and\nhuman evaluation confirmed robust and superior manipulation performance of our\nmethods compared to the existing baselines. Code is available at\nhttps://github.com/gwang-kim/DiffusionCLIP.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gwanghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1\">Taesung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design. (arXiv:2110.03659v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03659","description":"<p>An agent's functionality is largely determined by its design, i.e., skeletal\nstructure and joint attributes (e.g., length, size, strength). However, finding\nthe optimal agent design for a given function is extremely challenging since\nthe problem is inherently combinatorial and the design space is prohibitively\nlarge. Additionally, it can be costly to evaluate each candidate design which\nrequires solving for its optimal controller. To tackle these problems, our key\nidea is to incorporate the design procedure of an agent into its\ndecision-making process. Specifically, we learn a conditional policy that, in\nan episode, first applies a sequence of transform actions to modify an agent's\nskeletal structure and joint attributes, and then applies control actions under\nthe new design. To handle a variable number of joints across designs, we use a\ngraph-based policy where each graph node represents a joint and uses message\npassing with its neighbors to output joint-specific actions. Using policy\ngradient methods, our approach enables joint optimization of agent design and\ncontrol as well as experience sharing across different designs, which improves\nsample efficiency substantially. Experiments show that our approach,\nTransform2Act, outperforms prior methods significantly in terms of convergence\nspeed and final performance. Notably, Transform2Act can automatically discover\nplausible designs similar to giraffes, squids, and spiders. Code and videos are\navailable at https://sites.google.com/view/transform2act.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuda Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengyi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representational Continuity for Unsupervised Continual Learning. (arXiv:2110.06976v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06976","description":"<p>Continual learning (CL) aims to learn a sequence of tasks without forgetting\nthe previously acquired knowledge. However, recent CL advances are restricted\nto supervised continual learning (SCL) scenarios. Consequently, they are not\nscalable to real-world applications where the data distribution is often biased\nand unannotated. In this work, we focus on unsupervised continual learning\n(UCL), where we learn the feature representations on an unlabelled sequence of\ntasks and show that reliance on annotated data is not necessary for continual\nlearning. We conduct a systematic study analyzing the learned feature\nrepresentations and show that unsupervised visual representations are\nsurprisingly more robust to catastrophic forgetting, consistently achieve\nbetter performance, and generalize better to out-of-distribution tasks than\nSCL. Furthermore, we find that UCL achieves a smoother loss landscape through\nqualitative analysis of the learned representations and learns meaningful\nfeature representations. Additionally, we propose Lifelong Unsupervised Mixup\n(LUMP), a simple yet effective technique that interpolates between the current\ntask and previous tasks' instances to alleviate catastrophic forgetting for\nunsupervised representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1\">Divyam Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunxin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Attention-guided Graph Clustering with Dual Self-supervision. (arXiv:2111.05548v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05548","description":"<p>Existing deep embedding clustering works only consider the deepest layer to\nlearn a feature embedding and thus fail to well utilize the available\ndiscriminative information from cluster assignments, resulting performance\nlimitation. To this end, we propose a novel method, namely deep\nattention-guided graph clustering with dual self-supervision (DAGC).\nSpecifically, DAGC first utilizes a heterogeneity-wise fusion module to\nadaptively integrate the features of an auto-encoder and a graph convolutional\nnetwork in each layer and then uses a scale-wise fusion module to dynamically\nconcatenate the multi-scale features in different layers. Such modules are\ncapable of learning a discriminative feature embedding via an attention-based\nmechanism. In addition, we design a distribution-wise fusion module that\nleverages cluster assignments to acquire clustering results directly. To better\nexplore the discriminative information from the cluster assignments, we develop\na dual self-supervision solution consisting of a soft self-supervision strategy\nwith a triplet Kullback-Leibler divergence loss and a hard self-supervision\nstrategy with a pseudo supervision loss. Extensive experiments validate that\nour method consistently outperforms state-of-the-art methods on six benchmark\ndatasets. Especially, our method improves the ARI by more than 18.14% over the\nbest baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhihao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Stage model based on YOLOv3 for defect detection in PV panels based on IR and Visible Imaging by Unmanned Aerial Vehicle. (arXiv:2111.11709v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11709","description":"<p>As solar capacity installed worldwide continues to grow, there is an\nincreasing awareness that advanced inspection systems are becoming of utmost\nimportance to schedule smart interventions and minimize downtime likelihood. In\nthis work we propose a novel automatic multi-stage model to detect panel\ndefects on aerial images captured by unmanned aerial vehicle by using the\nYOLOv3 network and Computer Vision techniques. The model combines detections of\npanels and defects to refine its accuracy and exhibits an average inference\ntime per image of 0.98 s. The main novelties are represented by its versatility\nto process either thermographic or visible images and detect a large variety of\ndefects, to prescript recommended actions to O&amp;M crew to give a more efficient\ndata-driven maintenance strategy and its portability to both rooftop and\nground-mounted PV systems and different panel types. The proposed model has\nbeen validated on two big PV plants in the south of Italy with an outstanding\nAP@0.5 exceeding 98% for panel detection, a remarkable AP@0.4 (AP@0.5) of\nroughly 88.3% (66.9%) for hotspots by means of infrared thermography and a\nmAP@0.5 of almost 70% in the visible spectrum for detection of anomalies\nincluding panel shading induced by soiling and bird dropping, delamination,\npresence of puddles and raised rooftop panels. The model predicts also the\nseverity of hotspot areas based on the estimated temperature gradients, as well\nas it computes the soiling coverage based on visual images. Finally an analysis\nof the influence of the different YOLOv3's output scales on the detection is\ndiscussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tommaso_A/0/1/0/all/0/1\">Antonio Di Tommaso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betti_A/0/1/0/all/0/1\">Alessandro Betti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontanelli_G/0/1/0/all/0/1\">Giacomo Fontanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michelozzi_B/0/1/0/all/0/1\">Benedetto Michelozzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Generating Grounded Navigation Instructions from Landmarks. (arXiv:2111.12872v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12872","description":"<p>We study the automatic generation of navigation instructions from 360-degree\nimages captured on indoor routes. Existing generators suffer from poor visual\ngrounding, causing them to rely on language priors and hallucinate objects. Our\nMARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a\nfirst stage landmark detector and a second stage generator -- a multimodal,\nmultilingual, multitask encoder-decoder. To train it, we bootstrap grounded\nlandmark annotations on top of the Room-across-Room (RxR) dataset. Using text\nparsers, weak supervision from RxR's pose traces, and a multilingual image-text\nencoder trained on 1.8b images, we identify 971k English, Hindi and Telugu\nlandmark descriptions and ground them to specific regions in panoramas. On\nRoom-to-Room, human wayfinders obtain success rates (SR) of 71% following\nMARKY-MT5's instructions, just shy of their 75% SR following human instructions\n-- and well above SRs with other generators. Evaluations on RxR's longer,\ndiverse paths obtain 61-64% SRs on three languages. Generating such\nhigh-quality navigation instructions in novel environments is a step towards\nconversational navigation tools and could facilitate larger-scale training of\ninstruction-following agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_C/0/1/0/all/0/1\">Ceslee Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orbay_J/0/1/0/all/0/1\">Jordi Orbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1\">Vighnesh Birodkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1\">Izzeddin Gur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1\">Natasha Jaques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waters_A/0/1/0/all/0/1\">Austin Waters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1\">Peter Anderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.14820","description":"<p>Learning behavioral patterns from observational data has been a de-facto\napproach to motion forecasting. Yet, the current paradigm suffers from two\nshortcomings: brittle under distribution shifts and inefficient for knowledge\ntransfer. In this work, we propose to address these challenges from a causal\nrepresentation perspective. We first introduce a causal formalism of motion\nforecasting, which casts the problem as a dynamic process with three groups of\nlatent variables, namely invariant variables, style confounders, and spurious\nfeatures. We then introduce a learning framework that treats each group\nseparately: (i) unlike the common practice mixing datasets collected from\ndifferent locations, we exploit their subtle distinctions by means of an\ninvariance loss encouraging the model to suppress spurious correlations; (ii)\nwe devise a modular architecture that factorizes the representations of\ninvariant mechanisms and style confounders to approximate a sparse causal\ngraph; (iii) we introduce a style contrastive loss that not only enforces the\nstructure of style representations but also serves as a self-supervisory signal\nfor test-time refinement on the fly. Experiments on synthetic and real datasets\nshow that our proposed method improves the robustness and reusability of\nlearned motion representations, significantly outperforming prior\nstate-of-the-art motion forecasting models for out-of-distribution\ngeneralization and low-shot transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuejiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadei_R/0/1/0/all/0/1\">Riccardo Cadei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweizer_J/0/1/0/all/0/1\">Jonas Schweizer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1\">Sherwin Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Norm Must Go On: Dynamic Unsupervised Domain Adaptation by Normalization. (arXiv:2112.00463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00463","description":"<p>Domain adaptation is crucial to adapt a learned model to new scenarios, such\nas domain shifts or changing data distributions. Current approaches usually\nrequire a large amount of labeled or unlabeled data from the shifted domain.\nThis can be a hurdle in fields which require continuous dynamic adaptation or\nsuffer from scarcity of data, e.g. autonomous driving in challenging weather\nconditions. To address this problem of continuous adaptation to distribution\nshifts, we propose Dynamic Unsupervised Adaptation (DUA). By continuously\nadapting the statistics of the batch normalization layers we modify the feature\nrepresentations of the model. We show that by sequentially adapting a model\nwith only a fraction of unlabeled data, a strong performance gain can be\nachieved. With even less than 1% of unlabeled data from the target domain, DUA\nalready achieves competitive results to strong baselines. In addition, the\ncomputational overhead is minimal in contrast to previous approaches. Our\napproach is simple, yet effective and can be applied to any architecture which\nuses batch normalization as one of its components. We show the utility of DUA\nby evaluating it on a variety of domain adaptation datasets and tasks including\nobject recognition, digit recognition and object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirza_M/0/1/0/all/0/1\">M. Jehanzeb Mirza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Micorek_J/0/1/0/all/0/1\">Jakub Micorek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1\">Horst Possegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GCA-Net : Utilizing Gated Context Attention for Improving Image Forgery Localization and Detection. (arXiv:2112.04298v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04298","description":"<p>Forensic analysis of manipulated pixels requires the identification of\nvarious hidden and subtle features from images. Conventional image recognition\nmodels generally fail at this task because they are biased and more attentive\ntoward the dominant local and spatial features. In this paper, we propose a\nnovel Gated Context Attention Network (GCA-Net) that utilizes non-local\nattention in conjunction with a gating mechanism in order to capture the finer\nimage discrepancies and better identify forged regions. The proposed framework\nuses high dimensional embeddings to filter and aggregate the relevant context\nfrom coarse feature maps at various stages of the decoding process. This\nimproves the network's understanding of global differences and reduces\nfalse-positive localizations. Our evaluation on standard image forensic\nbenchmarks shows that GCA-Net can both compete against and improve over\nstate-of-the-art networks by an average of 4.7% AUC. Additional ablation\nstudies also demonstrate the method's robustness against attributions and\nresilience to false-positive predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sowmen Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1\">Md. Ruhul Amin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-temporal Relation Modeling for Few-shot Action Recognition. (arXiv:2112.05132v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05132","description":"<p>We propose a novel few-shot action recognition framework, STRM, which\nenhances class-specific feature discriminability while simultaneously learning\nhigher-order temporal representations. The focus of our approach is a novel\nspatio-temporal enrichment module that aggregates spatial and temporal contexts\nwith dedicated local patch-level and global frame-level feature enrichment\nsub-modules. Local patch-level enrichment captures the appearance-based\ncharacteristics of actions. On the other hand, global frame-level enrichment\nexplicitly encodes the broad temporal context, thereby capturing the relevant\nobject features over time. The resulting spatio-temporally enriched\nrepresentations are then utilized to learn the relational matching between\nquery and support action sub-sequences. We further introduce a query-class\nsimilarity classifier on the patch-level enriched features to enhance\nclass-specific feature discriminability by reinforcing the feature learning at\ndifferent stages in the proposed framework. Experiments are performed on four\nfew-shot action recognition benchmarks: Kinetics, SSv2, HMDB51 and UCF101. Our\nextensive ablation study reveals the benefits of the proposed contributions.\nFurthermore, our approach sets a new state-of-the-art on all four benchmarks.\nOn the challenging SSv2 benchmark, our approach achieves an absolute gain of\n$3.5\\%$ in classification accuracy, as compared to the best existing method in\nthe literature. Our code and models are available at\nhttps://github.com/Anirudh257/strm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thatipelli_A/0/1/0/all/0/1\">Anirudh Thatipelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Sanath Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN-Supervised Dense Visual Alignment. (arXiv:2112.05143v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05143","description":"<p>We propose GAN-Supervised Learning, a framework for learning discriminative\nmodels and their GAN-generated training data jointly end-to-end. We apply our\nframework to the dense visual alignment problem. Inspired by the classic\nCongealing method, our GANgealing algorithm trains a Spatial Transformer to map\nrandom samples from a GAN trained on unaligned data to a common,\njointly-learned target mode. We show results on eight datasets, all of which\ndemonstrate our method successfully aligns complex data and discovers dense\ncorrespondences. GANgealing significantly outperforms past self-supervised\ncorrespondence algorithms and performs on-par with (and sometimes exceeds)\nstate-of-the-art supervised correspondence algorithms on several datasets --\nwithout making use of any correspondence supervision or data augmentation and\ndespite being trained exclusively on GAN-generated data. For precise\ncorrespondence, we improve upon state-of-the-art supervised methods by as much\nas $3\\times$. We show applications of our method for augmented reality, image\nediting and automated pre-processing of image datasets for downstream GAN\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peebles_W/0/1/0/all/0/1\">William Peebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The CLEAR Benchmark: Continual LEArning on Real-World Imagery. (arXiv:2201.06289v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06289","description":"<p>Continual learning (CL) is widely regarded as crucial challenge for lifelong\nAI. However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make\nuse of artificial temporal variation and do not align with or generalize to the\nreal-world. In this paper, we introduce CLEAR, the first continual image\nclassification benchmark dataset with a natural temporal evolution of visual\nconcepts in the real world that spans a decade (2004-2014). We build CLEAR from\nexisting large-scale image collections (YFCC100M) through a novel and scalable\nlow-cost approach to visio-linguistic dataset curation. Our pipeline makes use\nof pretrained vision-language models (e.g. CLIP) to interactively build labeled\ndatasets, which are further validated with crowd-sourcing to remove errors and\neven inappropriate images (hidden in original YFCC100M). The major strength of\nCLEAR over prior CL benchmarks is the smooth temporal evolution of visual\nconcepts with real-world imagery, including both high-quality labeled data\nalong with abundant unlabeled samples per time period for continual\nsemi-supervised learning. We find that a simple unsupervised pre-training step\ncan already boost state-of-the-art CL algorithms that only utilize\nfully-supervised data. Our analysis also reveals that mainstream CL evaluation\nprotocols that train and test on iid data artificially inflate performance of\nCL system. To address this, we propose novel \"streaming\" protocols for CL that\nalways test on the (near) future. Interestingly, streaming protocols (a) can\nsimplify dataset curation since today's testset can be repurposed for\ntomorrow's trainset and (b) can produce more generalizable models with more\naccurate estimates of performance since all labeled data from each time-period\nis used for both training and testing (unlike classic iid train-test splits).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiqiu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's All in the Head: Representation Knowledge Distillation through Classifier Sharing. (arXiv:2201.06945v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06945","description":"<p>Representation knowledge distillation aims at transferring rich information\nfrom one model to another. Common approaches for representation distillation\nmainly focus on the direct minimization of distance metrics between the models'\nembedding vectors. Such direct methods may be limited in transferring\nhigh-order dependencies embedded in the representation vectors, or in handling\nthe capacity gap between the teacher and student models. Moreover, in standard\nknowledge distillation, the teacher is trained without awareness of the\nstudent's characteristics and capacity. In this paper, we explore two\nmechanisms for enhancing representation distillation using classifier sharing\nbetween the teacher and student. We first investigate a simple scheme where the\nteacher's classifier is connected to the student backbone, acting as an\nadditional classification head. Then, we propose a student-aware mechanism that\nasks to tailor the teacher model to a student with limited capacity by training\nthe teacher with a temporary student's head. We analyze and compare these two\nmechanisms and show their effectiveness on various datasets and tasks,\nincluding image classification, fine-grained classification, and face\nverification. In particular, we achieve state-of-the-art results for face\nverification on the IJB-C dataset for a MobileFaceNet model:\nTAR@(FAR=1e-5)=93.7\\%. Code is available at\nhttps://github.com/Alibaba-MIIL/HeadSharingKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1\">Emanuel Ben-Baruch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karklinsky_M/0/1/0/all/0/1\">Matan Karklinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biton_Y/0/1/0/all/0/1\">Yossi Biton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Cohen_A/0/1/0/all/0/1\">Avi Ben-Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawen_H/0/1/0/all/0/1\">Hussam Lawen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_N/0/1/0/all/0/1\">Nadav Zamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Detection of Doppelg\\\"angers based on Deep Face Representations. (arXiv:2201.08831v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08831","description":"<p>Doppelg\\\"angers (or lookalikes) usually yield an increased probability of\nfalse matches in a facial recognition system, as opposed to random face image\npairs selected for non-mated comparison trials. In this work, we assess the\nimpact of doppelg\\\"angers on the HDA Doppelg\\\"anger and Disguised Faces in The\nWild databases using a state-of-the-art face recognition system. It is found\nthat doppelg\\\"anger image pairs yield very high similarity scores resulting in\na significant increase of false match rates. Further, we propose a\ndoppelg\\\"anger detection method which distinguishes doppelg\\\"angers from mated\ncomparison trials by analysing differences in deep representations obtained\nfrom face image pairs. The proposed detection system employs a machine\nlearning-based classifier, which is trained with generated doppelg\\\"anger image\npairs utilising face morphing techniques. Experimental evaluations conducted on\nthe HDA Doppelg\\\"anger and Look-Alike Face databases reveal a detection equal\nerror rate of approximately 2.7% for the task of separating mated\nauthentication attempts from doppelg\\\"angers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_D/0/1/0/all/0/1\">Daniel Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video. (arXiv:2201.12792v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12792","description":"<p>We propose SelfRecon, a clothed human body reconstruction method that\ncombines implicit and explicit representations to recover space-time coherent\ngeometries from a monocular self-rotating human video. Explicit methods require\na predefined template mesh for a given sequence, while the template is hard to\nacquire for a specific subject. Meanwhile, the fixed topology limits the\nreconstruction accuracy and clothing types. Implicit representation supports\narbitrary topology and can represent high-fidelity geometry shapes due to its\ncontinuous nature. However, it is difficult to integrate multi-frame\ninformation to produce a consistent registration sequence for downstream\napplications. We propose to combine the advantages of both representations. We\nutilize differential mask loss of the explicit mesh to obtain the coherent\noverall shape, while the details on the implicit surface are refined with the\ndifferentiable neural rendering. Meanwhile, the explicit mesh is updated\nperiodically to adjust its topology changes, and a consistency loss is designed\nto match both representations. Compared with existing methods, SelfRecon can\nproduce high-fidelity surfaces for arbitrary clothed humans with\nself-supervised optimization. Extensive experimental results demonstrate its\neffectiveness on real captured monocular videos. The source code is available\nat https://github.com/jby1993/SelfReconCode.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Boyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray. (arXiv:2202.01020v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.01020","description":"<p>Computed tomography (CT) is an effective medical imaging modality, widely\nused in the field of clinical medicine for the diagnosis of various\npathologies. Advances in Multidetector CT imaging technology have enabled\nadditional functionalities, including generation of thin slice multiplanar\ncross-sectional body imaging and 3D reconstructions. However, this involves\npatients being exposed to a considerable dose of ionising radiation. Excessive\nionising radiation can lead to deterministic and harmful effects on the body.\nThis paper proposes a Deep Learning model that learns to reconstruct CT\nprojections from a few or even a single-view X-ray. This is based on a novel\narchitecture that builds from neural radiance fields, which learns a continuous\nrepresentation of CT scans by disentangling the shape and volumetric depth of\nsurface and internal anatomical structures from 2D images. Our model is trained\non chest and knee datasets, and we demonstrate qualitative and quantitative\nhigh-fidelity renderings and compare our approach to other recent radiance\nfield-based methods. Our code and link to our datasets is available at\nhttps://github.com/jonathanfrawley/mednerf\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Corona_Figueroa_A/0/1/0/all/0/1\">Abril Corona-Figueroa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frawley_J/0/1/0/all/0/1\">Jonathan Frawley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bond_Taylor_S/0/1/0/all/0/1\">Sam Bond-Taylor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bethapudi_S/0/1/0/all/0/1\">Sarath Bethapudi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Willcocks_C/0/1/0/all/0/1\">Chris G. Willcocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Detection without Model Information. (arXiv:2202.04271v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04271","description":"<p>Prior state-of-the-art adversarial detection works are classifier model\ndependent, i.e., they require classifier model outputs and parameters for\ntraining the detector or during adversarial detection. This makes their\ndetection approach classifier model specific. Furthermore, classifier model\noutputs and parameters might not always be accessible. To this end, we propose\na classifier model independent adversarial detection method using a simple\nenergy function to distinguish between adversarial and natural inputs. We train\na standalone detector independent of the classifier model, with a layer-wise\nenergy separation (LES) training to increase the separation between natural and\nadversarial energies. With this, we perform energy distribution-based\nadversarial detection. Our method achieves comparable performance with\nstate-of-the-art detection works (ROC-AUC &gt; 0.9) across a wide range of\ngradient, score and gaussian noise attacks on CIFAR10, CIFAR100 and\nTinyImagenet datasets. Furthermore, compared to prior works, our detection\napproach is light-weight, requires less amount of training data (40% of the\nactual dataset) and is transferable across different datasets. For\nreproducibility, we provide layer-wise energy separation training code at\nhttps://github.com/Intelligent-Computing-Lab-Yale/Energy-Separation-Training\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1\">Abhishek Moitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07427","description":"<p>Authors of posts in social media communicate their emotions and what causes\nthem with text and images. While there is work on emotion and stimulus\ndetection for each modality separately, it is yet unknown if the modalities\ncontain complementary emotion information in social media. We aim at filling\nthis research gap and contribute a novel, annotated corpus of English\nmultimodal Reddit posts. On this resource, we develop models to automatically\ndetect the relation between image and text, an emotion stimulus category and\nthe emotion class. We evaluate if these tasks require both modalities and find\nfor the image-text relations, that text alone is sufficient for most categories\n(complementary, illustrative, opposing): the information in the text allows to\npredict if an image is required for emotion understanding. The emotions of\nanger and sadness are best predicted with a multimodal model, while text alone\nis sufficient for disgust, joy, and surprise. Stimuli depicted by objects,\nanimals, food, or a person are best predicted by image-only models, while\nmultimodal models are most effective on art, events, memes, places, or\nscreenshots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khlyzova_A/0/1/0/all/0/1\">Anna Khlyzova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1\">Carina Silberer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSSNet: Multi-Scale-Stage Network for Single Image Deblurring. (arXiv:2202.09652v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.09652","description":"<p>Most of traditional single image deblurring methods before deep learning\nadopt a coarse-to-fine scheme that estimates a sharp image at a coarse scale\nand progressively refines it at finer scales. While this scheme has also been\nadopted to several deep learning-based approaches, recently a number of\nsingle-scale approaches have been introduced showing superior performance to\nprevious coarse-to-fine approaches both in quality and computation time. In\nthis paper, we revisit the coarse-to-fine scheme, and analyze defects of\nprevious coarse-to-fine approaches that degrade their performance. Based on the\nanalysis, we propose Multi-Scale-Stage Network (MSSNet), a novel deep\nlearning-based approach to single image deblurring that adopts our remedies to\nthe defects. Specifically, MSSNet adopts three novel technical components:\nstage configuration reflecting blur scales, an inter-scale information\npropagation scheme, and a pixel-shuffle-based multi-scale scheme. Our\nexperiments show that MSSNet achieves the state-of-the-art performance in terms\nof quality, network size, and computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protecting Celebrities from DeepFake with Identity Consistency Transformer. (arXiv:2203.01318v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01318","description":"<p>In this work we propose Identity Consistency Transformer, a novel face\nforgery detection method that focuses on high-level semantics, specifically\nidentity information, and detecting a suspect face by finding identity\ninconsistency in inner and outer face regions. The Identity Consistency\nTransformer incorporates a consistency loss for identity consistency\ndetermination. We show that Identity Consistency Transformer exhibits superior\ngeneralization ability not only across different datasets but also across\nvarious types of image degradation forms found in real-world applications\nincluding deepfake videos. The Identity Consistency Transformer can be easily\nenhanced with additional identity information when such information is\navailable, and for this reason it is especially well-suited for detecting face\nforgeries involving celebrities. Code will be released at\n\\url{https://github.com/LightDXY/ICT_DeepFake}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S2F2: Self-Supervised High Fidelity Face Reconstruction from Monocular Image. (arXiv:2203.07732v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07732","description":"<p>We present a novel face reconstruction method capable of reconstructing\ndetailed face geometry, spatially varying face reflectance from a single\nmonocular image. We build our work upon the recent advances of DNN-based\nauto-encoders with differentiable ray tracing image formation, trained in\nself-supervised manner. While providing the advantage of learning-based\napproaches and real-time reconstruction, the latter methods lacked fidelity. In\nthis work, we achieve, for the first time, high fidelity face reconstruction\nusing self-supervised learning only. Our novel coarse-to-fine deep architecture\nallows us to solve the challenging problem of decoupling face reflectance from\ngeometry using a single image, at high computational speed. Compared to\nstate-of-the-art methods, our method achieves more visually appealing\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dib_A/0/1/0/all/0/1\">Abdallah Dib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Junghyun Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thebault_C/0/1/0/all/0/1\">Cedric Thebault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_P/0/1/0/all/0/1\">Philippe-Henri Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chevallier_L/0/1/0/all/0/1\">Louis Chevallier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageNet Challenging Classification with the Raspberry Pi: An Incremental Local Stochastic Gradient Descent Algorithm. (arXiv:2203.11853v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11853","description":"<p>With rising powerful, low-cost embedded devices, the edge computing has\nbecome an increasingly popular choice. In this paper, we propose a new\nincremental local stochastic gradient descent (SGD) tailored on the Raspberry\nPi to deal with large ImageNet ILSVRC 2010 dataset having 1,261,405 images with\n1,000 classes. The local SGD splits the data block into $k$ partitions using\n$k$means algorithm and then it learns in the parallel way SGD models in each\ndata partition to classify the data locally. The incremental local SGD\nsequentially loads small data blocks of the training dataset to learn local SGD\nmodels. The numerical test results on Imagenet dataset show that our\nincremental local SGD algorithm with the Raspberry Pi 4 is faster and more\naccurate than the state-of-the-art linear SVM run on a PC Intel(R) Core i7-4790\nCPU, 3.6 GHz, 4 cores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thanh-Nghi Do</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Representation Forgetting in Supervised and Unsupervised Continual Learning. (arXiv:2203.13381v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13381","description":"<p>Continual Learning research typically focuses on tackling the phenomenon of\ncatastrophic forgetting in neural networks. Catastrophic forgetting is\nassociated with an abrupt loss of knowledge previously learned by a model when\nthe task, or more broadly the data distribution, being trained on changes. In\nsupervised learning problems this forgetting, resulting from a change in the\nmodel's representation, is typically measured or observed by evaluating the\ndecrease in old task performance. However, a model's representation can change\nwithout losing knowledge about prior tasks. In this work we consider the\nconcept of representation forgetting, observed by using the difference in\nperformance of an optimal linear classifier before and after a new task is\nintroduced. Using this tool we revisit a number of standard continual learning\nbenchmarks and observe that, through this lens, model representations trained\nwithout any explicit control for forgetting often experience small\nrepresentation forgetting and can sometimes be comparable to methods which\nexplicitly control for forgetting, especially in longer task sequences. We also\nshow that representation forgetting can lead to new insights on the effect of\nmodel capacity and loss function used in continual learning. Based on our\nresults, we show that a simple yet competitive approach is to learn\nrepresentations continually with standard supervised contrastive learning while\nconstructing prototypes of class samples when queried on old samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davari_M/0/1/0/all/0/1\">MohammadReza Davari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadi_N/0/1/0/all/0/1\">Nader Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudur_S/0/1/0/all/0/1\">Sudhir Mudur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1\">Rahaf Aljundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent Masking: Deep Q-Learning for Context Encoding in Medical Image Analysis. (arXiv:2203.13865v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13865","description":"<p>The need for a large amount of labeled data in the supervised setting has led\nrecent studies to utilize self-supervised learning to pre-train deep neural\nnetworks using unlabeled data. Many self-supervised training strategies have\nbeen investigated especially for medical datasets to leverage the information\navailable in the much fewer unlabeled data. One of the fundamental strategies\nin image-based self-supervision is context prediction. In this approach, a\nmodel is trained to reconstruct the contents of an arbitrary missing region of\nan image based on its surroundings. However, the existing methods adopt a\nrandom and blind masking approach by focusing uniformly on all regions of the\nimages. This approach results in a lot of unnecessary network updates that\ncause the model to forget the rich extracted features. In this work, we develop\na novel self-supervised approach that occludes targeted regions to improve the\npre-training procedure. To this end, we propose a reinforcement learning-based\nagent which learns to intelligently mask input images through deep Q-learning.\nWe show that training the agent against the prediction model can significantly\nimprove the semantic features extracted for downstream classification tasks. We\nperform our experiments on two public datasets for diagnosing breast cancer in\nthe ultrasound images and detecting lower-grade glioma with MR images. In our\nexperiments, we show that our novel masking strategy advances the learned\nfeatures according to the performance on the classification task in terms of\naccuracy, macro F1, and AUROC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahrami_M/0/1/0/all/0/1\">Mojtaba Bahrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_M/0/1/0/all/0/1\">Mahsa Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sylph: A Hypernetwork Framework for Incremental Few-shot Object Detection. (arXiv:2203.13903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13903","description":"<p>We study the challenging incremental few-shot object detection (iFSD)\nsetting. Recently, hypernetwork-based approaches have been studied in the\ncontext of continuous and finetune-free iFSD with limited success. We take a\ncloser look at important design choices of such methods, leading to several key\nimprovements and resulting in a more accurate and flexible framework, which we\ncall Sylph. In particular, we demonstrate the effectiveness of decoupling\nobject classification from localization by leveraging a base detector that is\npretrained for class-agnostic localization on a large-scale dataset. Contrary\nto what previous results have suggested, we show that with a carefully designed\nclass-conditional hypernetwork, finetune-free iFSD can be highly effective,\nespecially when a large number of base categories with abundant data are\navailable for meta-training, almost approaching alternatives that undergo\ntest-time-training. This result is even more significant considering its many\npractical advantages: (1) incrementally learning new classes in sequence\nwithout additional training, (2) detecting both novel and seen classes in a\nsingle pass, and (3) no forgetting of previously seen classes. We benchmark our\nmodel on both COCO and LVIS, reporting as high as 17% AP on the long-tail rare\nclasses on LVIS, indicating the promise of hypernetwork-based iFSD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Li Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rua_J/0/1/0/all/0/1\">Juan M Perez-Rua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kevin J Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Answer Questions in Dynamic Audio-Visual Scenarios. (arXiv:2203.14072v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14072","description":"<p>In this paper, we focus on the Audio-Visual Question Answering (AVQA) task,\nwhich aims to answer questions regarding different visual objects, sounds, and\ntheir associations in videos. The problem requires comprehensive multimodal\nunderstanding and spatio-temporal reasoning over audio-visual scenes. To\nbenchmark this task and facilitate our study, we introduce a large-scale\nMUSIC-AVQA dataset, which contains more than 45K question-answer pairs covering\n33 different question templates spanning over different modalities and question\ntypes. We develop several baselines and introduce a spatio-temporal grounded\naudio-visual network for the AVQA problem. Our results demonstrate that AVQA\nbenefits from multisensory perception and our model outperforms recent A-, V-,\nand AVQA approaches. We believe that our built dataset has the potential to\nserve as testbed for evaluating and promoting progress in audio-visual scene\nunderstanding and spatio-temporal reasoning. Code and dataset:\n<a href=\"http://gewu-lab.github.io/MUSIC-AVQA/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yake Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Discriminative Representation: Multi-view Trajectory Contrastive Learning for Online Multi-object Tracking. (arXiv:2203.14208v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14208","description":"<p>Discriminative representation is crucial for the association step in\nmulti-object tracking. Recent work mainly utilizes features in single or\nneighboring frames for constructing metric loss and empowering networks to\nextract representation of targets. Although this strategy is effective, it\nfails to fully exploit the information contained in a whole trajectory. To this\nend, we propose a strategy, namely multi-view trajectory contrastive learning,\nin which each trajectory is represented as a center vector. By maintaining all\nthe vectors in a dynamically updated memory bank, a trajectory-level\ncontrastive loss is devised to explore the inter-frame information in the whole\ntrajectories. Besides, in this strategy, each target is represented as multiple\nadaptively selected keypoints rather than a pre-defined anchor or center. This\ndesign allows the network to generate richer representation from multiple views\nof the same target, which can better characterize occluded objects.\nAdditionally, in the inference stage, a similarity-guided feature fusion\nstrategy is developed for further boosting the quality of the trajectory\nrepresentation. Extensive experiments have been conducted on MOTChallenge to\nverify the effectiveness of the proposed techniques. The experimental results\nindicate that our method has surpassed preceding trackers and established new\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1\">En Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shoudong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni6D: A Unified CNN Framework without Projection Breakdown for 6D Pose Estimation. (arXiv:2203.14531v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14531","description":"<p>As RGB-D sensors become more affordable, using RGB-D images to obtain\nhigh-accuracy 6D pose estimation results becomes a better option.\nState-of-the-art approaches typically use different backbones to extract\nfeatures for RGB and depth images. They use a 2D CNN for RGB images and a\nper-pixel point cloud network for depth data, as well as a fusion network for\nfeature fusion. We find that the essential reason for using two independent\nbackbones is the \"projection breakdown\" problem. In the depth image plane, the\nprojected 3D structure of the physical world is preserved by the 1D depth value\nand its built-in 2D pixel coordinate (UV). Any spatial transformation that\nmodifies UV, such as resize, flip, crop, or pooling operations in the CNN\npipeline, breaks the binding between the pixel value and UV coordinate. As a\nconsequence, the 3D structure is no longer preserved by a modified depth image\nor feature. To address this issue, we propose a simple yet effective method\ndenoted as Uni6D that explicitly takes the extra UV data along with RGB-D\nimages as input. Our method has a Unified CNN framework for 6D pose estimation\nwith a single CNN backbone. In particular, the architecture of our method is\nbased on Mask R-CNN with two extra heads, one named RT head for directly\npredicting 6D pose and the other named abc head for guiding the network to map\nthe visible points to their coordinates in the 3D model as an auxiliary module.\nThis end-to-end approach balances simplicity and accuracy, achieving comparable\naccuracy with state of the arts and 7.2x faster inference speed on the\nYCB-Video dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoke Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Donghai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Ye Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14542","description":"<p>Supervised deep learning methods require a large repository of annotated\ndata; hence, label noise is inevitable. Training with such noisy data\nnegatively impacts the generalization performance of deep neural networks. To\ncombat label noise, recent state-of-the-art methods employ some sort of sample\nselection mechanism to select a possibly clean subset of data. Next, an\noff-the-shelf semi-supervised learning method is used for training where\nrejected samples are treated as unlabeled data. Our comprehensive analysis\nshows that current selection methods disproportionately select samples from\neasy (fast learnable) classes while rejecting those from relatively harder\nones. This creates class imbalance in the selected clean set and in turn,\ndeteriorates performance under high label noise. In this work, we propose\nUNICON, a simple yet effective sample selection method which is robust to high\nlabel noise. To address the disproportionate selection of easy and hard\nsamples, we introduce a Jensen-Shannon divergence based uniform selection\nmechanism which does not require any probabilistic modeling and hyperparameter\ntuning. We complement our selection method with contrastive learning to further\ncombat the memorization of noisy labels. Extensive experimentation on multiple\nbenchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%\nimprovement over the current state-of-the-art on CIFAR100 dataset with a 90%\nnoise rate. Our code is publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1\">Mamshad Nayeem Rizve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15125","description":"<p>Natural language-based communication with mobile devices and home appliances\nis becoming increasingly popular and has the potential to become natural for\ncommunicating with mobile robots in the future. Towards this goal, we\ninvestigate cross-modal text-to-point-cloud localization that will allow us to\nspecify, for example, a vehicle pick-up or goods delivery location. In\nparticular, we propose Text2Pos, a cross-modal localization module that learns\nto align textual descriptions with localization cues in a coarse- to-fine\nmanner. Given a point cloud of the environment, Text2Pos locates a position\nthat is specified via a natural language-based description of the immediate\nsurroundings. To train Text2Pos and study its performance, we construct\nKITTI360Pose, the first dataset for this task based on the recently introduced\nKITTI360 dataset. Our experiments show that we can localize 65% of textual\nqueries within 15m distance to query locations for top-10 retrieved locations.\nThis is a starting point that we hope will spark future developments towards\nlanguage-based navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolmet_M/0/1/0/all/0/1\">Manuel Kolmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qunjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljosa Osep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Few-Shot NAS with Gradient Matching. (arXiv:2203.15207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15207","description":"<p>Efficient performance estimation of architectures drawn from large search\nspaces is essential to Neural Architecture Search. One-Shot methods tackle this\nchallenge by training one supernet to approximate the performance of every\narchitecture in the search space via weight-sharing, thereby drastically\nreducing the search cost. However, due to coupled optimization between child\narchitectures caused by weight-sharing, One-Shot supernet's performance\nestimation could be inaccurate, leading to degraded search outcomes. To address\nthis issue, Few-Shot NAS reduces the level of weight-sharing by splitting the\nOne-Shot supernet into multiple separated sub-supernets via edge-wise\n(layer-wise) exhaustive partitioning. Since each partition of the supernet is\nnot equally important, it necessitates the design of a more effective splitting\ncriterion. In this work, we propose a gradient matching score (GM) that\nleverages gradient information at the shared weight for making informed\nsplitting decisions. Intuitively, gradients from different child models can be\nused to identify whether they agree on how to update the shared modules, and\nsubsequently to decide if they should share the same weight. Compared with\nexhaustive partitioning, the proposed criterion significantly reduces the\nbranching factor per edge. This allows us to split more edges (layers) for a\ngiven budget, resulting in substantially improved performance as NAS search\nspaces usually include dozens of edges (layers). Extensive empirical\nevaluations of the proposed method on a wide range of search spaces\n(NASBench-201, DARTS, MobileNet Space), datasets (cifar10, cifar100, ImageNet)\nand search algorithms (DARTS, SNAS, RSPS, ProxylessNAS, OFA) demonstrate that\nit significantly outperforms its Few-Shot counterparts while surpassing\nprevious comparable methods in terms of the accuracy of derived architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shoukang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning of Semantic Correspondence with Pseudo-Labels. (arXiv:2203.16038v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16038","description":"<p>Establishing dense correspondences across semantically similar images remains\na challenging task due to the significant intra-class variations and background\nclutters. Traditionally, a supervised learning was used for training the\nmodels, which required tremendous manually-labeled data, while some methods\nsuggested a self-supervised or weakly-supervised learning to mitigate the\nreliance on the labeled data, but with limited performance. In this paper, we\npresent a simple, but effective solution for semantic correspondence that\nlearns the networks in a semi-supervised manner by supplementing few\nground-truth correspondences via utilization of a large amount of confident\ncorrespondences as pseudo-labels, called SemiMatch. Specifically, our framework\ngenerates the pseudo-labels using the model's prediction itself between source\nand weakly-augmented target, and uses pseudo-labels to learn the model again\nbetween source and strongly-augmented target, which improves the robustness of\nthe model. We also present a novel confidence measure for pseudo-labels and\ndata augmentation tailored for semantic correspondence. In experiments,\nSemiMatch achieves state-of-the-art performance on various benchmarks,\nespecially on PF-Willow by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_K/0/1/0/all/0/1\">Kwangrok Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Junyoung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyuseong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daehwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hansang Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Pre-training for Person Re-identification with Noisy Labels. (arXiv:2203.16533v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16533","description":"<p>This paper aims to address the problem of pre-training for person\nre-identification (Re-ID) with noisy labels. To setup the pre-training task, we\napply a simple online multi-object tracking system on raw videos of an existing\nunlabeled Re-ID dataset \"LUPerson\" nd build the Noisy Labeled variant called\n\"LUPerson-NL\". Since theses ID labels automatically derived from tracklets\ninevitably contain noises, we develop a large-scale Pre-training framework\nutilizing Noisy Labels (PNL), which consists of three learning modules:\nsupervised Re-ID learning, prototype-based contrastive learning, and\nlabel-guided contrastive learning. In principle, joint learning of these three\nmodules not only clusters similar examples to one prototype, but also rectifies\nnoisy labels based on the prototype assignment. We demonstrate that learning\ndirectly from raw videos is a promising alternative for pre-training, which\nutilizes spatial and temporal correlations as weak supervision. This simple\npre-training task provides a scalable way to learn SOTA Re-ID representations\nfrom scratch on \"LUPerson-NL\" without bells and whistles. For example, by\napplying on the same supervised Re-ID method MGN, our pre-trained model\nimproves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%,\n2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or\nfew-shot setting, the performance gain is even more significant, suggesting a\nbetter transferability of the learned representation. Code is available at\nhttps://github.com/DengpanFu/LUPerson-NL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Dengpan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.00631","description":"<p>Vision Transformers (ViT)s have recently become popular due to their\noutstanding modeling capabilities, in particular for capturing long-range\ninformation, and scalability to dataset and model sizes which has led to\nstate-of-the-art performance in various computer vision and medical image\nanalysis tasks. In this work, we introduce a unified framework consisting of\ntwo architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder\nand Convolutional Neural Network (CNN) and transformer-based decoders. In the\nproposed model, the encoder is linked to the decoder via skip connections at\nfive different resolutions with deep supervision. The design of proposed\narchitecture allows for meeting a wide range of trade-off requirements between\naccuracy and computational cost. In addition, we present a methodology for\nself-supervised pre-training of the encoder backbone via learning to predict\nrandomly masked volumetric tokens using contextual information of visible\ntokens. We pre-train our framework on a cohort of $5050$ CT images, gathered\nfrom publicly available CT datasets, and present a systematic investigation of\nvarious components such as masking ratio and patch size that affect the\nrepresentation learning capability and performance of downstream tasks. We\nvalidate the effectiveness of our pre-training approach by fine-tuning and\ntesting our model on liver and liver tumor segmentation task using the Medical\nSegmentation Decathlon (MSD) dataset and achieve state-of-the-art performance\nin terms of various segmentation metrics. To demonstrate its generalizability,\nwe train and test the model on BraTS 21 dataset for brain tumor segmentation\nusing MRI images and outperform other methods in terms of Dice score. Code:\nhttps://github.com/Project-MONAI/research-contributions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyue Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Minimal Path Method with Embedded CNN. (arXiv:2204.00944v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00944","description":"<p>We propose Path-CNN, a method for the segmentation of centerlines of tubular\nstructures by embedding convolutional neural networks (CNNs) into the\nprogressive minimal path method. Minimal path methods are widely used for\ntopology-aware centerline segmentation, but usually these methods rely on weak,\nhand-tuned image features. In contrast, CNNs use strong image features which\nare learned automatically from images. But CNNs usually do not take the\ntopology of the results into account, and often require a large amount of\nannotations for training. We integrate CNNs into the minimal path method, so\nthat both techniques benefit from each other: CNNs employ learned image\nfeatures to improve the determination of minimal paths, while the minimal path\nmethod ensures the correct topology of the segmented centerlines, provides\nstrong geometric priors to increase the performance of CNNs, and reduces the\namount of annotations for the training of CNNs significantly. Our method has\nlower hardware requirements than many recent methods. Qualitative and\nquantitative comparison with other methods shows that Path-CNN achieves better\nperformance, especially when dealing with tubular structures with complex\nshapes in challenging environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wei Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adjusting for Bias with Procedural Data. (arXiv:2204.01108v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01108","description":"<p>3D softwares are now capable of producing highly realistic images that look\nnearly indistinguishable from the real images. This raises the question: can\nreal datasets be enhanced with 3D rendered data? We investigate this question.\nIn this paper we demonstrate the use of 3D rendered data, procedural, data for\nthe adjustment of bias in image datasets. We perform error analysis of images\nof animals which shows that the misclassification of some animal breeds is\nlargely a data issue. We then create procedural images of the poorly classified\nbreeds and that model further trained on procedural data can better classify\npoorly performing breeds on real data. We believe that this approach can be\nused for the enhancement of visual data for any underrepresented group,\nincluding rare diseases, or any data bias potentially improving the accuracy\nand fairness of models. We find that the resulting representations rival or\neven out-perform those learned directly from real data, but that good\nperformance requires care in the 3D rendered procedural data generation. 3D\nimage dataset can be viewed as a compressed and organized copy of a real\ndataset, and we envision a future where more and more procedural data\nproliferate while datasets become increasingly unwieldy, missing, or private.\nThis paper suggests several techniques for dealing with visual representation\nlearning in such a future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shesh Narayan Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Nicholas Bear Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Change Detection Based on Image Reconstruction Loss. (arXiv:2204.01200v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01200","description":"<p>To train the change detector, bi-temporal images taken at different times in\nthe same area are used. However, collecting labeled bi-temporal images is\nexpensive and time consuming. To solve this problem, various unsupervised\nchange detection methods have been proposed, but they still require unlabeled\nbi-temporal images. In this paper, we propose unsupervised change detection\nbased on image reconstruction loss using only unlabeled single temporal single\nimage. The image reconstruction model is trained to reconstruct the original\nsource image by receiving the source image and the photometrically transformed\nsource image as a pair. During inference, the model receives bi-temporal images\nas the input, and tries to reconstruct one of the inputs. The changed region\nbetween bi-temporal images shows high reconstruction loss. Our change detector\nshowed significant performance in various change detection benchmark datasets\neven though only a single temporal single source image was used. The code and\ntrained models will be publicly available for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">Hyeoncheol Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1\">Jingi Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Document Image Dewarping by Grid Regularization. (arXiv:2203.16850v1 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2203.16850","description":"<p>This paper addresses the problem of document image dewarping, which aims at\neliminating the geometric distortion in document images for document\ndigitization. Instead of designing a better neural network to approximate the\noptical flow fields between the inputs and outputs, we pursue the best\nreadability by taking the text lines and the document boundaries into account\nfrom a constrained optimization perspective. Specifically, our proposed method\nfirst learns the boundary points and the pixels in the text lines and then\nfollows the most simple observation that the boundaries and text lines in both\nhorizontal and vertical directions should be kept after dewarping to introduce\na novel grid regularization scheme. To obtain the final forward mapping for\ndewarping, we solve an optimization problem with our proposed grid\nregularization. The experiments comprehensively demonstrate that our proposed\napproach outperforms the prior arts by large margins in terms of readability\n(with the metrics of Character Errors Rate and the Edit Distance) while\nmaintaining the best image quality on the publicly-available DocUNet benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1\">Xiangwei Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_R/0/1/0/all/0/1\">Rujiao Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_C/0/1/0/all/0/1\">Cong Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}