{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-17T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and Their Implications. (arXiv:2205.06828v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06828","description":"<p>There are many ways to express similar things in text, which makes evaluating\nnatural language generation (NLG) systems difficult. Compounding this\ndifficulty is the need to assess varying quality criteria depending on the\ndeployment setting. While the landscape of NLG evaluation has been well-mapped,\npractitioners' goals, assumptions, and constraints -- which inform decisions\nabout what, when, and how to evaluate -- are often partially or implicitly\nstated, or not stated at all. Combining a formative semi-structured interview\nstudy of NLG practitioners (N=18) with a survey study of a broader sample of\npractitioners (N=61), we surface goals, community practices, assumptions, and\nconstraints that shape NLG evaluations, examining their implications and how\nthey embody ethical considerations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaitlyn Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blodgett_S/0/1/0/all/0/1\">Su Lin Blodgett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trischler_A/0/1/0/all/0/1\">Adam Trischler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1\">Hal Daum&#xe9; III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleman_K/0/1/0/all/0/1\">Kaheer Suleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olteanu_A/0/1/0/all/0/1\">Alexandra Olteanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IRB-NLP at SemEval-2022 Task 1: Exploring the Relationship Between Words and Their Semantic Representations. (arXiv:2205.06840v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06840","description":"<p>What is the relation between a word and its description, or a word and its\nembedding? Both descriptions and embeddings are semantic representations of\nwords. But, what information from the original word remains in these\nrepresentations? Or more importantly, which information about a word do these\ntwo representations share? Definition Modeling and Reverse Dictionary are two\nopposite learning tasks that address these questions. The goal of the\nDefinition Modeling task is to investigate the power of information laying\ninside a word embedding to express the meaning of the word in a humanly\nunderstandable way -- as a dictionary definition. Conversely, the Reverse\nDictionary task explores the ability to predict word embeddings directly from\nits definition. In this paper, by tackling these two tasks, we are exploring\nthe relationship between words and their semantic representations. We present\nour findings based on the descriptive, exploratory, and predictive data\nanalysis conducted on the CODWOE dataset. We give a detailed overview of the\nsystems that we designed for Definition Modeling and Reverse Dictionary tasks,\nand that achieved top scores on SemEval-2022 CODWOE challenge in several\nsubtasks. We hope that our experimental results concerning the predictive\nmodels and the data analyses we provide will prove useful in future\nexplorations of word representations and their relationships.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1\">Damir Koren&#x10d;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grubisic_I/0/1/0/all/0/1\">Ivan Grubi&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Approach for Automatic Construction of an Algorithmic Knowledge Graph from Textual Resources. (arXiv:2205.06854v1 [cs.AI])","link":"http://arxiv.org/abs/2205.06854","description":"<p>There is enormous growth in various fields of research. This development is\naccompanied by new problems. To solve these problems efficiently and in an\noptimized manner, algorithms are created and described by researchers in the\nscientific literature. Scientific algorithms are vital for understanding and\nreusing existing work in numerous domains. However, algorithms are generally\nchallenging to find. Also, the comparison among similar algorithms is difficult\nbecause of the disconnected documentation. Information about algorithms is\nmostly present in websites, code comments, and so on. There is an absence of\nstructured metadata to portray algorithms. As a result, sometimes redundant or\nsimilar algorithms are published, and the researchers build them from scratch\ninstead of reusing or expanding upon the already existing algorithm. In this\npaper, we introduce an approach for automatically developing a knowledge graph\n(KG) for algorithmic problems from unstructured data. Because it captures\ninformation more clearly and extensively, an algorithm KG will give additional\ncontext and explainability to the algorithm metadata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1\">Jyotima Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_B/0/1/0/all/0/1\">Biswanath Dutta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis of Covid-related Reddits. (arXiv:2205.06863v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06863","description":"<p>This paper focuses on Sentiment Analysis of Covid-19 related messages from\nthe r/Canada and r/Unitedkingdom subreddits of Reddit. We apply manual\nannotation and three Machine Learning algorithms to analyze sentiments conveyed\nin those messages. We use VADER and TextBlob to label messages for Machine\nLearning experiments. Our results show that removal of shortest and longest\nmessages improves VADER and TextBlob agreement on positive sentiments and\nF-score of sentiment classification by all the three algorithms\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fieg_T/0/1/0/all/0/1\">Tomas Fieg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1\">Marina Sokolova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets. (arXiv:2205.06871v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06871","description":"<p>Precisely assessing the progress in natural language generation (NLG) tasks\nis challenging, and human evaluation to establish preference in a model's\noutput over another is often necessary. However, human evaluation is usually\ncostly, difficult to reproduce, and non-reusable. In this paper, we propose a\nnew and simple automatic evaluation method for NLG called Near-Negative\nDistinction (NND) that repurposes prior human annotations into NND tests. In an\nNND test, an NLG model must place higher likelihood on a high-quality output\ncandidate than on a near-negative candidate with a known error. Model\nperformance is established by the number of NND tests a model passes, as well\nas the distribution over task-specific errors the model fails on. Through\nexperiments on three NLG tasks (question generation, question answering, and\nsummarization), we show that NND achieves higher correlation with human\njudgments than standard NLG evaluation metrics. We then illustrate NND\nevaluation in four practical scenarios, for example performing fine-grain model\nanalysis, or studying model training dynamics. Our findings suggest NND can\ngive a second life to human annotations and provide low-cost NLG evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1\">Philippe Laban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain. (arXiv:2205.06885v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06885","description":"<p>Pathology text mining is a challenging task given the reporting variability\nand constant new findings in cancer sub-type definitions. However, successful\ntext mining of a large pathology database can play a critical role to advance\n'big data' cancer research like similarity-based treatment selection, case\nidentification, prognostication, surveillance, clinical trial screening, risk\nstratification, and many others. While there is a growing interest in\ndeveloping language models for more specific clinical domains, no\npathology-specific language space exist to support the rapid data-mining\ndevelopment in pathology space. In literature, a few approaches fine-tuned\ngeneral transformer models on specialized corpora while maintaining the\noriginal tokenizer, but in fields requiring specialized terminology, these\nmodels often fail to perform adequately. We propose PathologyBERT - a\npre-trained masked language model which was trained on 347,173 histopathology\nspecimen reports and publicly released in the Huggingface repository. Our\ncomprehensive experiments demonstrate that pre-training of transformer model on\npathology corpora yields performance improvements on Natural Language\nUnderstanding (NLU) and Breast Cancer Diagnose Classification when compared to\nnonspecific language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_T/0/1/0/all/0/1\">Thiago Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_A/0/1/0/all/0/1\">Amara Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Susmita Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vayalpati_K/0/1/0/all/0/1\">Kavyasree Vayalpati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_G/0/1/0/all/0/1\">Geoffrey H. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Hari Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1\">Imon Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapping Text Anonymization Models with Distant Supervision. (arXiv:2205.06895v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06895","description":"<p>We propose a novel method to bootstrap text anonymization models based on\ndistant supervision. Instead of requiring manually labeled training data, the\napproach relies on a knowledge graph expressing the background information\nassumed to be publicly available about various individuals. This knowledge\ngraph is employed to automatically annotate text documents including personal\ndata about a subset of those individuals. More precisely, the method determines\nwhich text spans ought to be masked in order to guarantee $k$-anonymity,\nassuming an adversary with access to both the text documents and the background\ninformation expressed in the knowledge graph. The resulting collection of\nlabeled documents is then used as training data to fine-tune a pre-trained\nlanguage model for text anonymization. We illustrate this approach using a\nknowledge graph extracted from Wikidata and short biographical texts from\nWikipedia. Evaluation results with a RoBERTa-based model and a manually\nannotated collection of 553 summaries showcase the potential of the approach,\nbut also unveil a number of issues that may arise if the knowledge graph is\nnoisy or incomplete. The results also illustrate that, contrary to most\nsequence labeling problems, the text anonymization task may admit several\nalternative solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulou_A/0/1/0/all/0/1\">Anthi Papadopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lison_P/0/1/0/all/0/1\">Pierre Lison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilan_I/0/1/0/all/0/1\">Ildik&#xf3; Pil&#xe1;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing a Production System for Purpose of Call Detection in Business Phone Conversations. (arXiv:2205.06904v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06904","description":"<p>For agents at a contact centre receiving calls, the most important piece of\ninformation is the reason for a given call. An agent cannot provide support on\na call if they do not know why a customer is calling. In this paper we describe\nour implementation of a commercial system to detect Purpose of Call statements\nin English business call transcripts in real time. We present a detailed\nanalysis of types of Purpose of Call statements and language patterns related\nto them, discuss an approach to collect rich training data by bootstrapping\nfrom a set of rules to a neural model, and describe a hybrid model which\nconsists of a transformer-based classifier and a set of rules by leveraging\ninsights from the analysis of call transcripts. The model achieved 88.6 F1 on\naverage in various types of business calls when tested on real life data and\nhas low inference time. We reflect on the challenges and design decisions when\ndeveloping and deploying the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khasanova_E/0/1/0/all/0/1\">Elena Khasanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hiranandani_P/0/1/0/all/0/1\">Pooja Hiranandani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardiner_S/0/1/0/all/0/1\">Shayna Gardiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xue-Yong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corston_Oliver_S/0/1/0/all/0/1\">Simon Corston-Oliver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Property Induction Framework for Neural Language Models. (arXiv:2205.06910v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06910","description":"<p>To what extent can experience from language contribute to our conceptual\nknowledge? Computational explorations of this question have shed light on the\nability of powerful neural language models (LMs) -- informed solely through\ntext input -- to encode and elicit information about concepts and properties.\nTo extend this line of research, we present a framework that uses\nneural-network language models (LMs) to perform property induction -- a task in\nwhich humans generalize novel property knowledge (has sesamoid bones) from one\nor more concepts (robins) to others (sparrows, canaries). Patterns of property\ninduction observed in humans have shed considerable light on the nature and\norganization of human conceptual knowledge. Inspired by this insight, we use\nour framework to explore the property inductions of LMs, and find that they\nshow an inductive preference to generalize novel properties on the basis of\ncategory membership, suggesting the presence of a taxonomic bias in their\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1\">Julia Taylor Rayz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Literal and Implied Subquestions to Fact-check Complex Claims. (arXiv:2205.06938v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06938","description":"<p>Verifying complex political claims is a challenging task, especially when\npoliticians use various tactics to subtly misrepresent the facts. Automatic\nfact-checking systems fall short here, and their predictions like \"half-true\"\nare not very useful in isolation, since we have no idea which parts of the\nclaim are true and which are not. In this work, we focus on decomposing a\ncomplex claim into a comprehensive set of yes-no subquestions whose answers\ninfluence the veracity of the claim. We present ClaimDecomp, a dataset of\ndecompositions for over 1000 claims. Given a claim and its verification\nparagraph written by fact-checkers, our trained annotators write subquestions\ncovering both explicit propositions of the original claim and its implicit\nfacets, such as asking about additional political context that changes our view\nof the claim's veracity. We study whether state-of-the-art models can generate\nsuch subquestions, showing that these models generate reasonable questions to\nask, but predicting the comprehensive set of subquestions from the original\nclaim without evidence remains challenging. We further show that these\nsubquestions can help identify relevant evidence to fact-check the full claim\nand derive the veracity through their answers, suggesting that they can be\nuseful pieces of a fact-checking pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Aniruddh Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Select Reading Passages in English Assessment Tests?. (arXiv:2205.06961v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06961","description":"<p>We show a method to auto-select reading passages in English assessment tests\nand share some key insights that can be helpful in related fields. In\nspecifics, we prove that finding a similar passage (to a passage that already\nappeared in the test) can give a suitable passage for test development. In the\nprocess, we create a simple database-tagger-filter algorithm and perform a\nhuman evaluation. However, 1. the textual features, that we analyzed, lack\ncoverage, and 2. we fail to find meaningful correlations between each feature\nand suitability score. Lastly, we describe the future developments to improve\nautomated reading passage selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bruce W. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason H. Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Consistency Training for Semi-Supervised Sequence-to-Sequence ASR via Speech Chain Reconstruction and Self-Transcribing. (arXiv:2205.06963v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06963","description":"<p>Consistency regularization has recently been applied to semi-supervised\nsequence-to-sequence (S2S) automatic speech recognition (ASR). This principle\nencourages an ASR model to output similar predictions for the same input speech\nwith different perturbations. The existing paradigm of semi-supervised S2S ASR\nutilizes SpecAugment as data augmentation and requires a static teacher model\nto produce pseudo transcripts for untranscribed speech. However, this paradigm\nfails to take full advantage of consistency regularization. First, the masking\noperations of SpecAugment may damage the linguistic contents of the speech,\nthus influencing the quality of pseudo labels. Second, S2S ASR requires both\ninput speech and prefix tokens to make the next prediction. The static prefix\ntokens made by the offline teacher model cannot match dynamic pseudo labels\nduring consistency training. In this work, we propose an improved consistency\ntraining paradigm of semi-supervised S2S ASR. We utilize speech chain\nreconstruction as the weak augmentation to generate high-quality pseudo labels.\nMoreover, we demonstrate that dynamic pseudo transcripts produced by the\nstudent ASR model benefit the consistency training. Experiments on LJSpeech and\nLibriSpeech corpora show that compared to supervised baselines, our improved\nparadigm achieves a 12.2% CER improvement in the single-speaker setting and\n38.6% in the multi-speaker setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Heli Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novitasari_S/0/1/0/all/0/1\">Sashi Novitasari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakti_S/0/1/0/all/0/1\">Sakriani Sakti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts. (arXiv:2205.06982v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06982","description":"<p>Systems that can automatically define unfamiliar terms hold the promise of\nimproving the accessibility of scientific texts, especially for readers who may\nlack prerequisite background knowledge. However, current systems assume a\nsingle \"best\" description per concept, which fails to account for the many\npotentially useful ways a concept can be described. We present ACCoRD, an\nend-to-end system tackling the novel task of generating sets of descriptions of\nscientific concepts. Our system takes advantage of the myriad ways a concept is\nmentioned across the scientific literature to produce distinct, diverse\ndescriptions of target scientific concepts in terms of different reference\nconcepts. To support research on the task, we release an expert-annotated\nresource, the ACCoRD corpus, which includes 1,275 labeled contexts and 1,787\nhand-authored concept descriptions. We conduct a user study demonstrating that\n(1) users prefer descriptions produced by our end-to-end system, and (2) users\nprefer multiple descriptions to a single \"best\" description.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murthy_S/0/1/0/all/0/1\">Sonia K. Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_D/0/1/0/all/0/1\">Daniel King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchardt_J/0/1/0/all/0/1\">Jonathan Borchardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL. (arXiv:2205.06983v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06983","description":"<p>Relational structures such as schema linking and schema encoding have been\nvalidated as a key component to qualitatively translating natural language into\nSQL queries. However, introducing these structural relations comes with prices:\nthey often result in a specialized model structure, which largely prohibits the\nuse of large pretrained models in text-to-SQL. To address this problem, we\npropose RASAT: a Transformer seq2seq architecture augmented with relation-aware\nself-attention that could leverage a variety of relational structures while at\nthe meantime being able to effectively inherit the pretrained parameters from\nthe T5 model. Our model is able to incorporate almost all types of existing\nrelations in the literature, and in addition, we propose to introduce\nco-reference relations for the multi-turn scenario. Experimental results on\nthree widely used text-to-SQL datasets, covering both single-turn and\nmulti-turn scenarios, have shown that RASAT could achieve competitive results\nin all three benchmarks, achieving state-of-the-art performance in execution\naccuracy (80.5\\% EX on Spider, 53.1\\% IEX on SParC, and 37.5\\% IEX on CoSQL).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jiexing Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jingyao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiangpeng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chenghu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review-Based Tip Generation for Music Songs. (arXiv:2205.06985v1 [cs.IR])","link":"http://arxiv.org/abs/2205.06985","description":"<p>Reviews of songs play an important role in online music service platforms.\nPrior research shows that users can make quicker and more informed decisions\nwhen presented with meaningful song reviews. However, reviews of music songs\nare generally long in length and most of them are non-informative for users. It\nis difficult for users to efficiently grasp meaningful messages for making\ndecisions. To solve this problem, one practical strategy is to provide tips,\ni.e., short, concise, empathetic, and self-contained descriptions about songs.\nTips are produced from song reviews and should express non-trivial insight\nabout the songs. To the best of our knowledge, no prior studies have explored\nthe tip generation task in music domain. In this paper, we create a dataset\nnamed MTips for the task and propose a framework named GenTMS for automatically\ngenerating tips from song reviews. The dataset involves 8,003 Chinese\ntips/non-tips from 128 songs which are distributed in five different song\ngenres. Experimental results show that GenTMS achieves top-10 precision at\n85.56%, outperforming the baseline models by at least 3.34%. Besides, to\nsimulate the practical usage of our proposed framework, we also experiment with\npreviously-unseen songs, during which GenTMS also achieves the best performance\nwith top-10 precision at 78.89% on average. The results demonstrate the\neffectiveness of the proposed framework in tip generation of the music domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_J/0/1/0/all/0/1\">Jingya Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Cuiyun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yupan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruifeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lanjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Machine Translation of Indigenous Languages with Multilingual Transfer Learning. (arXiv:2205.06993v1 [cs.CL])","link":"http://arxiv.org/abs/2205.06993","description":"<p>Machine translation (MT) involving Indigenous languages, including those\npossibly endangered, is challenging due to lack of sufficient parallel data. We\ndescribe an approach exploiting bilingual and multilingual pretrained MT models\nin a transfer learning setting to translate from Spanish to ten South American\nIndigenous languages. Our models set new SOTA on five out of the ten language\npairs we consider, even doubling performance on one of these five pairs. Unlike\nprevious SOTA that perform data augmentation to enlarge the train sets, we\nretain the low-resource setting to test the effectiveness of our models under\nsuch a constraint. In spite of the rarity of linguistic information available\nabout the Indigenous languages, we offer a number of quantitative and\nqualitative analyses (e.g., as to morphology, tokenization, and orthography) to\ncontextualize our results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integration of Text and Graph-based Features for Detecting Mental Health Disorders from Voice. (arXiv:2205.07006v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07006","description":"<p>With the availability of voice-enabled devices such as smart phones, mental\nhealth disorders could be detected and treated earlier, particularly\npost-pandemic. The current methods involve extracting features directly from\naudio signals. In this paper, two methods are used to enrich voice analysis for\ndepression detection: graph transformation of voice signals, and natural\nlanguage processing of the transcript based on representational learning, fused\ntogether to produce final class labels. The results of experiments with the\nDAIC-WOZ dataset suggest that integration of text-based voice classification\nand learning from low level and graph-based voice signal features can improve\nthe detection of mental disorders like depression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_N/0/1/0/all/0/1\">Nasser Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samani_R/0/1/0/all/0/1\">Rasoul Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahrokh_F/0/1/0/all/0/1\">Fahime Shahrokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Naturalistic Causal Probing for Morpho-Syntax. (arXiv:2205.07043v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07043","description":"<p>Probing has become a go-to methodology for interpreting and analyzing deep\nneural models in natural language processing. Yet recently, there has been much\ndebate around the limitations and weaknesses of probes. In this work, we\nsuggest a naturalistic strategy for input-level intervention on real world data\nin Spanish, which is a language with gender marking. Using our approach, we\nisolate morpho-syntactic features from counfounders in sentences, e.g. topic,\nwhich will then allow us to causally probe pre-trained models. We apply this\nmethodology to analyze causal effects of gender and number on contextualized\nrepresentations extracted from pre-trained models -- BERT, RoBERTa and GPT-2.\nOur experiments suggest that naturalistic intervention can give us stable\nestimates of causal effects, which varies across different words in a sentence.\nWe further show the utility of our estimator in investigating gender bias in\nadjectives, and answering counterfactual questions in masked prediction. Our\nprobing experiments highlights the importance of conducting causal probing in\ndetermining if a particular property is encoded in representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge. (arXiv:2205.07065v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07065","description":"<p>There are limitations in learning language from text alone. Therefore, recent\nfocus has been on developing multimodal models. However, few benchmarks exist\nthat can measure what language models learn about language from multimodal\ntraining. We hypothesize that training on a visual modality should improve on\nthe visual commonsense knowledge in language models. Therefore, we introduce\ntwo evaluation tasks for measuring visual commonsense knowledge in language\nmodels and use them to evaluate different multimodal models and unimodal\nbaselines. Primarily, we find that the visual commonsense knowledge is not\nsignificantly different between the multimodal models and unimodal baseline\nmodels trained on visual text data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_L/0/1/0/all/0/1\">Lovisa Hagstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1\">Richard Johansson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining Approaches for Spoken Language Recognition: TalTech Submission to the OLR 2021 Challenge. (arXiv:2205.07083v1 [eess.AS])","link":"http://arxiv.org/abs/2205.07083","description":"<p>This paper investigates different pretraining approaches to spoken language\nidentification. The paper is based on our submission to the Oriental Language\nRecognition 2021 Challenge. We participated in two tracks of the challenge:\nconstrained and unconstrained language recognition. For the constrained track,\nwe first trained a Conformer-based encoder-decoder model for multilingual\nautomatic speech recognition (ASR), using the provided training data that had\ntranscripts available. The shared encoder of the multilingual ASR model was\nthen finetuned for the language identification task. For the unconstrained\ntask, we relied on both externally available pretrained models as well as\nexternal data: the multilingual XLSR-53 wav2vec2.0 model was finetuned on the\nVoxLingua107 corpus for the language recognition task, and finally finetuned on\nthe provided target language training data, augmented with CommonVoice data.\nOur primary metric $C_{\\rm avg}$ values on the Test set are 0.0079 for the\nconstrained task and 0.0119 for the unconstrained task which resulted in the\nsecond place in both rankings. In post-evaluation experiments, we study the\namount of target language data needed for training an accurate backend model,\nthe importance of multilingual pretraining data, and compare different models\nas finetuning starting points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alumae_T/0/1/0/all/0/1\">Tanel Alum&#xe4;e</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kukk_K/0/1/0/all/0/1\">Kunnar Kukk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collar-aware Training for Streaming Speaker Change Detection in Broadcast Speech. (arXiv:2205.07086v1 [eess.AS])","link":"http://arxiv.org/abs/2205.07086","description":"<p>In this paper, we present a novel training method for speaker change\ndetection models. Speaker change detection is often viewed as a binary sequence\nlabelling problem. The main challenges with this approach are the vagueness of\nannotated change points caused by the silences between speaker turns and\nimbalanced data due to the majority of frames not including a speaker change.\nConventional training methods tackle these by artificially increasing the\nproportion of positive labels in the training data. Instead, the proposed\nmethod uses an objective function which encourages the model to predict a\nsingle positive label within a specified collar. This is done by marginalizing\nover all possible subsequences that have exactly one positive label within the\ncollar. Experiments on English and Estonian datasets show large improvements\nover the conventional training method. Additionally, the model outputs have\npeaks concentrated to a single frame, removing the need for post-processing to\nfind the exact predicted change point which is particularly useful for\nstreaming applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kalda_J/0/1/0/all/0/1\">Joonas Kalda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alumae_T/0/1/0/all/0/1\">Tanel Alum&#xe4;e</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiformer: A Head-Configurable Transformer-Based Model for Direct Speech Translation. (arXiv:2205.07100v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07100","description":"<p>Transformer-based models have been achieving state-of-the-art results in\nseveral fields of Natural Language Processing. However, its direct application\nto speech tasks is not trivial. The nature of this sequences carries problems\nsuch as long sequence lengths and redundancy between adjacent tokens.\nTherefore, we believe that regular self-attention mechanism might not be well\nsuited for it.\n</p>\n<p>Different approaches have been proposed to overcome these problems, such as\nthe use of efficient attention mechanisms. However, the use of these methods\nusually comes with a cost, which is a performance reduction caused by\ninformation loss. In this study, we present the Multiformer, a\nTransformer-based model which allows the use of different attention mechanisms\non each head. By doing this, the model is able to bias the self-attention\ntowards the extraction of more diverse token interactions, and the information\nloss is reduced. Finally, we perform an analysis of the head contributions, and\nwe observe that those architectures where all heads relevance is uniformly\ndistributed obtain better results. Our results show that mixing attention\npatterns along the different heads and layers outperforms our baseline by up to\n0.7 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sant_G/0/1/0/all/0/1\">Gerard Sant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1\">Belen Alastruey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_Jussa_M/0/1/0/all/0/1\">Marta R. Costa-Juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VoicePrivacy 2020 Challenge Evaluation Plan. (arXiv:2205.07123v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07123","description":"<p>The VoicePrivacy Challenge aims to promote the development of privacy\npreservation tools for speech technology by gathering a new community to define\nthe tasks of interest and the evaluation methodology, and benchmarking\nsolutions through a series of challenges. In this document, we formulate the\nvoice anonymization task selected for the VoicePrivacy 2020 Challenge and\ndescribe the datasets used for system development and evaluation. We also\npresent the attack models and the associated objective and subjective\nevaluation metrics. We introduce two anonymization baselines and report\nobjective evaluation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Brij Mohan Lal Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1\">Andreas Nautsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonastre_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bonastre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noe_P/0/1/0/all/0/1\">Paul-Gauthier No&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Generalizability of Fine-Tuned Models for Fake News Detection. (arXiv:2205.07154v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07154","description":"<p>The Covid-19 pandemic has caused a dramatic and parallel rise in dangerous\nmisinformation, denoted an `infodemic' by the CDC and WHO. Misinformation tied\nto the Covid-19 infodemic changes continuously; this can lead to performance\ndegradation of fine-tuned models due to concept drift. Degredation can be\nmitigated if models generalize well-enough to capture some cyclical aspects of\ndrifted data. In this paper, we explore generalizability of pre-trained and\nfine-tuned fake news detectors across 9 fake news datasets. We show that\nexisting models often overfit on their training dataset and have poor\nperformance on unseen data. However, on some subsets of unseen data that\noverlap with training data, models have higher accuracy. Based on this\nobservation, we also present KMeans-Proxy, a fast and effective method based on\nK-Means clustering for quickly identifying these overlapping subsets of unseen\ndata. KMeans-Proxy improves generalizability on unseen fake news datasets by\n0.1-0.2 f1-points across datasets. We present both our generalizability\nexperiments as well as KMeans-Proxy to further research in tackling the fake\nnews problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suprem_A/0/1/0/all/0/1\">Abhijit Suprem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_C/0/1/0/all/0/1\">Calton Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Cognitive to Computational Modeling: Text-based Risky Decision-Making Guided by Fuzzy Trace Theory. (arXiv:2205.07164v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07164","description":"<p>Understanding, modelling and predicting human risky decision-making is\nchallenging due to intrinsic individual differences and irrationality. Fuzzy\ntrace theory (FTT) is a powerful paradigm that explains human decision-making\nby incorporating gists, i.e., fuzzy representations of information which\ncapture only its quintessential meaning. Inspired by Broniatowski and Reyna's\nFTT cognitive model, we propose a computational framework which combines the\neffects of the underlying semantics and sentiments on text-based\ndecision-making. In particular, we introduce Category-2-Vector to learn\ncategorical gists and categorical sentiments, and demonstrate how our\ncomputational model can be optimised to predict risky decision-making in groups\nand individuals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mar_J/0/1/0/all/0/1\">Jaron Mar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiamou Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hero-Gang Neural Model For Named Entity Recognition. (arXiv:2205.07177v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07177","description":"<p>Named entity recognition (NER) is a fundamental and important task in NLP,\naiming at identifying named entities (NEs) from free text. Recently, since the\nmulti-head attention mechanism applied in the Transformer model can effectively\ncapture longer contextual information, Transformer-based models have become the\nmainstream methods and have achieved significant performance in this task.\nUnfortunately, although these models can capture effective global context\ninformation, they are still limited in the local feature and position\ninformation extraction, which is critical in NER. In this paper, to address\nthis limitation, we propose a novel Hero-Gang Neural structure (HGN), including\nthe Hero and Gang module, to leverage both global and local information to\npromote NER. Specifically, the Hero module is composed of a Transformer-based\nencoder to maintain the advantage of the self-attention mechanism, and the Gang\nmodule utilizes a multi-window recurrent module to extract local features and\nposition information under the guidance of the Hero module. Afterward, the\nproposed multi-window attention effectively combines global information and\nmultiple local features for predicting entity labels. Experimental results on\nseveral benchmark datasets demonstrate the effectiveness of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaling Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization. (arXiv:2205.07208v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07208","description":"<p>It is challenging to train a good intent classifier for a task-oriented\ndialogue system with only a few annotations. Recent studies have shown that\nfine-tuning pre-trained language models with a small amount of labeled\nutterances from public benchmarks in a supervised manner is extremely helpful.\nHowever, we find that supervised pre-training yields an anisotropic feature\nspace, which may suppress the expressive power of the semantic representations.\nInspired by recent research in isotropization, we propose to improve supervised\npre-training by regularizing the feature space towards isotropy. We propose two\nregularizers based on contrastive learning and correlation matrix respectively,\nand demonstrate their effectiveness through extensive experiments. Our main\nfinding is that it is promising to regularize supervised pre-training with\nisotropization to further improve the performance of few-shot intent detection.\nThe source code can be found at https://github.com/fanolabs/isoIntentBert-main.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haode Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haowen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liming Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaolei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_A/0/1/0/all/0/1\">Albert Y.S. Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech Synthesis. (arXiv:2205.07211v1 [eess.AS])","link":"http://arxiv.org/abs/2205.07211","description":"<p>Style transfer for out-of-domain (OOD) speech synthesis aims to generate\nspeech samples with unseen style (e.g., speaker identity, emotion, and prosody)\nderived from an acoustic reference, while facing the following challenges: 1)\nThe highly dynamic style features in expressive voice are difficult to model\nand transfer; and 2) the TTS models should be robust enough to handle diverse\nOOD conditions that differ from the source data. This paper proposes\nGenerSpeech, a text-to-speech model towards high-fidelity zero-shot style\ntransfer of OOD custom voice. GenerSpeech decomposes the speech variation into\nthe style-agnostic and style-specific parts by introducing two components: 1) a\nmulti-level style adaptor to efficiently model a large range of style\nconditions, including global speaker and emotion characteristics, and the local\n(utterance, phoneme, and word-level) fine-grained prosodic representations; and\n2) a generalizable content adaptor with Mix-Style Layer Normalization to\neliminate style information in the linguistic content representation and thus\nimprove model generalization. Our evaluations on zero-shot style transfer\ndemonstrate that GenerSpeech surpasses the state-of-the-art models in terms of\naudio quality and style similarity. The extension studies to adaptive style\ntransfer further show that GenerSpeech performs robustly in the few-shot data\nsetting. Audio samples are available at \\url{https://GenerSpeech.github.io/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_R/0/1/0/all/0/1\">Rongjie Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Chenye Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Prompt Learning-based Few-Shot Sentiment Analysis. (arXiv:2205.07220v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07220","description":"<p>In the field of natural language processing, sentiment analysis via deep\nlearning has a excellent performance by using large labeled datasets.\nMeanwhile, labeled data are insufficient in many sentiment analysis, and\nobtaining these data is time-consuming and laborious. Prompt learning devotes\nto resolving the data deficiency by reformulating downstream tasks with the\nhelp of prompt. In this way, the appropriate prompt is very important for the\nperformance of the model. This paper proposes an adaptive prompting(AP)\nconstruction strategy using seq2seq-attention structure to acquire the semantic\ninformation of the input sequence. Then dynamically construct adaptive prompt\nwhich can not only improve the quality of the prompt, but also can effectively\ngeneralize to other fields by pre-trained prompt which is constructed by\nexisting public labeled data. The experimental results on FewCLUE datasets\ndemonstrate that the proposed method AP can effectively construct appropriate\nadaptive prompt regardless of the quality of hand-crafted prompt and outperform\nthe state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengfei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1\">Tingting Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongdong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Toxic Degeneration with Empathetic Data: Exploring the Relationship Between Toxicity and Empathy. (arXiv:2205.07233v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07233","description":"<p>Large pre-trained neural language models have supported the effectiveness of\nmany NLP tasks, yet are still prone to generating toxic language hindering the\nsafety of their use. Using empathetic data, we improve over recent work on\ncontrollable text generation that aims to reduce the toxicity of generated\ntext. We find we are able to dramatically reduce the size of fine-tuning data\nto 7.5-30k samples while at the same time making significant improvements over\nstate-of-the-art toxicity mitigation of up to 3.4% absolute reduction (26%\nrelative) from the original work on 2.3m samples, by strategically sampling\ndata based on empathy scores. We observe that the degree of improvement is\nsubject to specific communication components of empathy. In particular, the\ncognitive components of empathy significantly beat the original dataset in\nalmost all experiments, while emotional empathy was tied to less improvement\nand even underperforming random samples of the original data. This is a\nparticularly implicative insight for NLP work concerning empathy as until\nrecently the research and resources built for it have exclusively considered\nempathy as an emotional concept.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1\">Allison Lahnala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuendorf_B/0/1/0/all/0/1\">B&#xe9;la Neuendorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Latent Concepts Learned in BERT. (arXiv:2205.07237v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07237","description":"<p>A large number of studies that analyze deep neural network models and their\nability to encode various linguistic and non-linguistic concepts provide an\ninterpretation of the inner mechanics of these models. The scope of the\nanalyses is limited to pre-defined concepts that reinforce the traditional\nlinguistic knowledge and do not reflect on how novel concepts are learned by\nthe model. We address this limitation by discovering and analyzing latent\nconcepts learned in neural network models in an unsupervised fashion and\nprovide interpretations from the model's perspective. In this work, we study:\ni) what latent concepts exist in the pre-trained BERT model, ii) how the\ndiscovered latent concepts align or diverge from classical linguistic hierarchy\nand iii) how the latent concepts evolve across layers. Our findings show: i) a\nmodel learns novel concepts (e.g. animal categories and demographic groups),\nwhich do not strictly adhere to any pre-defined categorization (e.g. POS,\nsemantic tags), ii) several latent concepts are based on multiple properties\nwhich may include semantics, syntax, and morphology, iii) the lower layers in\nthe model dominate in learning shallow lexical concepts while the higher layers\nlearn semantic relations and iv) the discovered latent concepts highlight\npotential biases learned in the model. We also release a novel BERT ConceptNet\ndataset (BCN) consisting of 174 concept labels and 1M annotated instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdul Rafae Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not to Overfit or Underfit? A Study of Domain Generalization in Question Answering. (arXiv:2205.07257v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07257","description":"<p>Machine learning models are prone to overfitting their source (training)\ndistributions, which is commonly believed to be why they falter in novel target\ndomains. Here we examine the contrasting view that multi-source domain\ngeneralization (DG) is in fact a problem of mitigating source domain\nunderfitting: models not adequately learning the signal in their multi-domain\ntraining data. Experiments on a reading comprehension DG benchmark show that as\na model gradually learns its source domains better -- using known methods such\nas knowledge distillation from a larger model -- its zero-shot out-of-domain\naccuracy improves at an even faster rate. Improved source domain learning also\ndemonstrates superior generalization over three popular domain-invariant\nlearning methods that aim to counter overfitting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sultan_M/0/1/0/all/0/1\">Md Arafat Sultan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1\">Avirup Sil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1\">Radu Florian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Modelling on Consumer Financial Protection Bureau Data: An Approach Using BERT Based Embeddings. (arXiv:2205.07259v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07259","description":"<p>Customers' reviews and comments are important for businesses to understand\nusers' sentiment about the products and services. However, this data needs to\nbe analyzed to assess the sentiment associated with topics/aspects to provide\nefficient customer assistance. LDA and LSA fail to capture the semantic\nrelationship and are not specific to any domain. In this study, we evaluate\nBERTopic, a novel method that generates topics using sentence embeddings on\nConsumer Financial Protection Bureau (CFPB) data. Our work shows that BERTopic\nis flexible and yet provides meaningful and diverse topics compared to LDA and\nLSA. Furthermore, domain-specific pre-trained embeddings (FinBERT) yield even\nbetter topics. We evaluated the topics on coherence score (c_v) and UMass.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sangaraju_V/0/1/0/all/0/1\">Vasudeva Raju Sangaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolla_B/0/1/0/all/0/1\">Bharath Kumar Bolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_D/0/1/0/all/0/1\">Deepak Kumar Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kh_J/0/1/0/all/0/1\">Jyothsna Kh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Explanations and Critiques in Recommendation Systems. (arXiv:2205.07268v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07268","description":"<p>Artificial intelligence and machine learning algorithms have become\nubiquitous. Although they offer a wide range of benefits, their adoption in\ndecision-critical fields is limited by their lack of interpretability,\nparticularly with textual data. Moreover, with more data available than ever\nbefore, it has become increasingly important to explain automated predictions.\n</p>\n<p>Generally, users find it difficult to understand the underlying computational\nprocesses and interact with the models, especially when the models fail to\ngenerate the outcomes or explanations, or both, correctly. This problem\nhighlights the growing need for users to better understand the models' inner\nworkings and gain control over their actions. This dissertation focuses on two\nfundamental challenges of addressing this need. The first involves explanation\ngeneration: inferring high-quality explanations from text documents in a\nscalable and data-driven manner. The second challenge consists in making\nexplanations actionable, and we refer to it as critiquing. This dissertation\nexamines two important applications in natural language processing and\nrecommendation tasks.\n</p>\n<p>Overall, we demonstrate that interpretability does not come at the cost of\nreduced performance in two consequential applications. Our framework is\napplicable to other fields as well. This dissertation presents an effective\nmeans of closing the gap between promise and practice in artificial\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifiers are Better Experts for Controllable Text Generation. (arXiv:2205.07276v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07276","description":"<p>This paper proposes a simple method for controllable text generation based on\nweighting logits produced, namely CAIF sampling. Using an arbitrary third-party\ntext classifier, we adjust a small part of a language model's logits and guide\ntext generation towards or away from classifier prediction. We show that the\nproposed method significantly outperforms recent PPLM, GeDi, and DExperts on\nPPL and sentiment accuracy based on the external classifier of generated texts.\nA the same time, it is also easier to implement and tune, and has significantly\nfewer restrictions and requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sitdikov_A/0/1/0/all/0/1\">Askhat Sitdikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balagansky_N/0/1/0/all/0/1\">Nikita Balagansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavrilov_D/0/1/0/all/0/1\">Daniil Gavrilov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markov_A/0/1/0/all/0/1\">Alexander Markov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification. (arXiv:2205.07283v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07283","description":"<p>Complex word identification (CWI) is a cornerstone process towards proper\ntext simplification. CWI is highly dependent on context, whereas its difficulty\nis augmented by the scarcity of available datasets which vary greatly in terms\nof domains and languages. As such, it becomes increasingly more difficult to\ndevelop a robust model that generalizes across a wide array of input examples.\nIn this paper, we propose a novel training technique for the CWI task based on\ndomain adaptation to improve the target character and context representations.\nThis technique addresses the problem of working with multiple domains, inasmuch\nas it creates a way of smoothing the differences between the explored datasets.\nMoreover, we also propose a similar auxiliary task, namely text simplification,\nthat can be used to complement lexical complexity prediction. Our model obtains\na boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast\nto vanilla training techniques, when considering the CompLex from the Lexical\nComplexity Prediction 2021 dataset. At the same time, we obtain an increase of\n3% in Pearson scores, while considering a cross-lingual setup relying on the\nComplex Word Identification 2018 dataset. In addition, our model yields\nstate-of-the-art results in terms of Mean Absolute Error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_G/0/1/0/all/0/1\">George-Eduard Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smadu_R/0/1/0/all/0/1\">R&#x103;zvan-Alexandru Sm&#x103;du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1\">Dumitru-Clementin Cercel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dascalu_M/0/1/0/all/0/1\">Mihai Dascalu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Self-Refinement for Robust Learning with Weak Supervision. (arXiv:2205.07290v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07290","description":"<p>Training deep neural networks (DNNs) with weak supervision has been a hot\ntopic as it can significantly reduce the annotation cost. However, labels from\nweak supervision can be rather noisy and the high capacity of DNNs makes them\neasy to overfit the noisy labels. Recent methods leverage self-training\ntechniques to train noise-robust models, where a teacher trained on noisy\nlabels is used to teach a student. However, the teacher from such models might\nfit a substantial amount of noise and produce wrong pseudo-labels with high\nconfidence, leading to error propagation. In this work, we propose Meta\nSelf-Refinement (MSR), a noise-resistant learning framework, to effectively\ncombat noisy labels from weak supervision sources. Instead of purely relying on\na fixed teacher trained on noisy labels, we keep updating the teacher to refine\nits pseudo-labels. At each training step, it performs a meta gradient descent\non the current mini-batch to maximize the student performance on a clean\nvalidation set. Extensive experimentation on eight NLP benchmarks demonstrates\nthat MSR is robust against noise in all settings and outperforms the\nstate-of-the-art up to 11.4% in accuracy and 9.26% in F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedderich_M/0/1/0/all/0/1\">Michael A. Hedderich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TiBERT: Tibetan Pre-trained Language Model. (arXiv:2205.07303v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07303","description":"<p>The pre-trained language model is trained on large-scale unlabeled text and\ncan achieve state-of-the-art results in many different downstream tasks.\nHowever, the current pre-trained language model is mainly concentrated in the\nChinese and English fields. For low resource language such as Tibetan, there is\nlack of a monolingual pre-trained model. To promote the development of Tibetan\nnatural language processing tasks, this paper collects the large-scale training\ndata from Tibetan websites and constructs a vocabulary that can cover 99.95$\\%$\nof the words in the corpus by using Sentencepiece. Then, we train the Tibetan\nmonolingual pre-trained language model named TiBERT on the data and vocabulary.\nFinally, we apply TiBERT to the downstream tasks of text classification and\nquestion generation, and compare it with classic models and multilingual\npre-trained models, the experimental results show that TiBERT can achieve the\nbest performance. Our model is published in <a href=\"http://tibert.cmli-nlp.com/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sisi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Junjie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaobing Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transkimmer: Transformer Learns to Layer-wise Skim. (arXiv:2205.07324v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07324","description":"<p>Transformer architecture has become the de-facto model for many machine\nlearning tasks from natural language processing and computer vision. As such,\nimproving its computational efficiency becomes paramount. One of the major\ncomputational inefficiency of Transformer-based models is that they spend the\nidentical amount of computation throughout all layers. Prior works have\nproposed to augment the Transformer model with the capability of skimming\ntokens to improve its computational efficiency. However, they suffer from not\nhaving effectual and end-to-end optimization of the discrete skimming\npredictor. To address the above limitations, we propose the Transkimmer\narchitecture, which learns to identify hidden state tokens that are not\nrequired by each layer. The skimmed tokens are then forwarded directly to the\nfinal output, thus reducing the computation of the successive layers. The key\nidea in Transkimmer is to add a parameterized predictor before each layer that\nlearns to make the skimming decision. We also propose to adopt\nreparameterization trick and add skim loss for the end-to-end training of\nTranskimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark\ncompared with vanilla BERT-base baseline with less than 1% accuracy\ndegradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jingwen Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Minyi Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-term Control for Dialogue Generation: Methods and Evaluation. (arXiv:2205.07352v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07352","description":"<p>Current approaches for controlling dialogue response generation are primarily\nfocused on high-level attributes like style, sentiment, or topic. In this work,\nwe focus on constrained long-term dialogue generation, which involves more\nfine-grained control and requires a given set of control words to appear in\ngenerated responses. This setting requires a model to not only consider the\ngeneration of these control words in the immediate context, but also produce\nutterances that will encourage the generation of the words at some time in the\n(possibly distant) future. We define the problem of constrained long-term\ncontrol for dialogue generation, identify gaps in current methods for\nevaluation, and propose new metrics that better measure long-term control. We\nalso propose a retrieval-augmented method that improves performance of\nlong-term controlled generation via logit modification techniques. We show\nthrough experiments on three task-oriented dialogue datasets that our metrics\nbetter assess dialogue control relative to current alternatives and that our\nmethod outperforms state-of-the-art constrained generation baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_R/0/1/0/all/0/1\">Ramya Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narangodage_H/0/1/0/all/0/1\">Hashan Buddhika Narangodage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilman_M/0/1/0/all/0/1\">Mauro Schilman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Ryan McDonald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models. (arXiv:2205.07381v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07381","description":"<p>Recent research showed promising results on combining pretrained language\nmodels (LMs) with canonical utterance for few-shot semantic parsing. The\ncanonical utterance is often lengthy and complex due to the compositional\nstructure of formal languages. Learning to generate such canonical utterance\nrequires significant amount of data to reach high performance. Fine-tuning with\nonly few-shot samples, the LMs can easily forget pretrained knowledge, overfit\nspurious biases, and suffer from compositionally out-of-distribution\ngeneralization errors. To tackle these issues, we propose a novel few-shot\nsemantic parsing method -- SeqZero. SeqZero decomposes the problem into a\nsequence of sub-problems, which correspond to the sub-clauses of the formal\nlanguage. Based on the decomposition, the LMs only need to generate short\nanswers using prompts for predicting sub-clauses. Thus, SeqZero avoids\ngenerating a long canonical utterance at once. Moreover, SeqZero employs not\nonly a few-shot model but also a zero-shot model to alleviate the overfitting.\nIn particular, SeqZero brings out the merits from both models via ensemble\nequipped with our proposed constrained rescaling. SeqZero achieves SOTA\nperformance of BART-based models on GeoQuery and EcommerceQuery, which are two\nfew-shot datasets with compositional data split.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Downstream Transformer Generation of Question-Answer Pairs with Preprocessing and Postprocessing Pipelines. (arXiv:2205.07387v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07387","description":"<p>We present a system called TP3 to perform a downstream task of transformers\non generating question-answer pairs (QAPs) from a given article. TP3 first\nfinetunes pretrained transformers on QAP datasets, then uses a preprocessing\npipeline to select appropriate answers, feeds the relevant sentences and the\nanswer to the finetuned transformer to generate candidate QAPs, and finally\nuses a postprocessing pipeline to filter inadequate QAPs. In particular, using\npretrained T5 models as transformers and the SQuAD dataset as the finetruning\ndataset, we show that TP3 generates satisfactory number of QAPs with high\nqualities on the Gaokao-EN dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What GPT Knows About Who is Who. (arXiv:2205.07407v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07407","description":"<p>Coreference resolution -- which is a crucial task for understanding discourse\nand language at large -- has yet to witness widespread benefits from large\nlanguage models (LLMs). Moreover, coreference resolution systems largely rely\non supervised labels, which are highly expensive and difficult to annotate,\nthus making it ripe for prompt engineering. In this paper, we introduce a\nQA-based prompt-engineering method and discern \\textit{generative}, pre-trained\nLLMs' abilities and limitations toward the task of coreference resolution. Our\nexperiments show that GPT-2 and GPT-Neo can return valid answers, but that\ntheir capabilities to identify coreferent mentions are limited and\nprompt-sensitive, leading to inconsistent results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaohan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peynetti_E/0/1/0/all/0/1\">Eduardo Peynetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meerman_V/0/1/0/all/0/1\">Vasco Meerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1\">Chris Tanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Miutsu: NTU's TaskBot for the Alexa Prize. (arXiv:2205.07446v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07446","description":"<p>This paper introduces Miutsu, National Taiwan University's Alexa Prize\nTaskBot, which is designed to assist users in completing tasks requiring\nmultiple steps and decisions in two different domains -- home improvement and\ncooking. We overview our system design and architectural goals, and detail the\nproposed core elements, including question answering, task retrieval, social\nchatting, and various conversational modules. A dialogue flow is proposed to\nprovide a robust and engaging conversation when handling complex tasks. We\ndiscuss the faced challenges during the competition and potential future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hui-Chi Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ze-Song Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Ssu Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_C/0/1/0/all/0/1\">Chieh-Chi Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao-Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning about Procedures with Natural Language Processing: A Tutorial. (arXiv:2205.07455v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07455","description":"<p>This tutorial provides a comprehensive and in-depth view of the research on\nprocedures, primarily in Natural Language Processing. A procedure is a sequence\nof steps intended to achieve some goal. Understanding procedures in natural\nlanguage has a long history, with recent breakthroughs made possible by\nadvances in technology. First, we discuss established approaches to collect\nprocedures, by human annotation or extraction from web resources. Then, we\nexamine different angles from which procedures can be reasoned about, as well\nas ways to represent them. Finally, we enumerate scenarios where procedural\nknowledge can be applied to the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Directed Acyclic Transformer for Non-Autoregressive Machine Translation. (arXiv:2205.07459v1 [cs.CL])","link":"http://arxiv.org/abs/2205.07459","description":"<p>Non-autoregressive Transformers (NATs) significantly reduce the decoding\nlatency by generating all tokens in parallel. However, such independent\npredictions prevent NATs from capturing the dependencies between the tokens for\ngenerating multiple possible translations. In this paper, we propose Directed\nAcyclic Transfomer (DA-Transformer), which represents the hidden states in a\nDirected Acyclic Graph (DAG), where each path of the DAG corresponds to a\nspecific translation. The whole DAG simultaneously captures multiple\ntranslations and facilitates fast predictions in a non-autoregressive fashion.\nExperiments on the raw training data of WMT benchmark show that DA-Transformer\nsubstantially outperforms previous NATs by about 3 BLEU on average, which is\nthe first NAT model that achieves competitive results with autoregressive\nTransformers without relying on knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Distributed Representation of News (DRNews) for Stock Market Predictions. (arXiv:2005.11706v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.11706","description":"<p>In this study, a novel Distributed Representation of News (DRNews) model is\ndeveloped and applied in deep learning-based stock market predictions. With the\nmerit of integrating contextual information and cross-documental knowledge, the\nDRNews model creates news vectors that describe both the semantic information\nand potential linkages among news events through an attributed news network.\nTwo stock market prediction tasks, namely the short-term stock movement\nprediction and stock crises early warning, are implemented in the framework of\nthe attention-based Long Short Term-Memory (LSTM) network. It is suggested that\nDRNews substantially enhances the results of both tasks comparing with five\nbaselines of news embedding models. Further, the attention mechanism suggests\nthat short-term stock trend and stock market crises both receive influences\nfrom daily news with the former demonstrates more critical responses on the\ninformation related to the stock market {\\em per se}, whilst the latter draws\nmore concerns on the banking sector and economic policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Ye Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_L/0/1/0/all/0/1\">Lu Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiwan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Gender Bias in Speech Translation. (arXiv:2010.14465v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.14465","description":"<p>The scientific community is increasingly aware of the necessity to embrace\npluralism and consistently represent major and minor social groups. Currently,\nthere are no standard evaluation techniques for different types of biases.\nAccordingly, there is an urgent need to provide evaluation sets and protocols\nto measure existing biases in our automatic systems. Evaluating the biases\nshould be an essential step towards mitigating them in the systems.\n</p>\n<p>This paper introduces WinoST, a new freely available challenge set for\nevaluating gender bias in speech translation. WinoST is the speech version of\nWinoMT which is a MT challenge set and both follow an evaluation protocol to\nmeasure gender accuracy. Using a state-of-the-art end-to-end speech translation\nsystem, we report the gender bias evaluation on four language pairs and we show\nthat gender accuracy in speech translation is more than 23% lower than in MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basta_C/0/1/0/all/0/1\">Christine Basta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing the Transformer Decoder with Transition-based Syntax. (arXiv:2101.12640v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.12640","description":"<p>Notwithstanding recent advances, syntactic generalization remains a challenge\nfor text decoders. While some studies showed gains from incorporating\nsource-side symbolic syntactic and semantic structure into text generation\nTransformers, very little work addressed the decoding of such structure. We\npropose a general approach for tree decoding using a transition-based approach.\nExamining the challenging test case of incorporating Universal Dependencies\nsyntax into machine translation, we present substantial improvements on test\nsets that focus on syntactic generalization, while presenting improved or\ncomparable performance on standard MT benchmarks. Further qualitative analysis\naddresses cases where syntactic generalization in the vanilla Transformer\ndecoder is inadequate and demonstrates the advantages afforded by integrating\nsyntactic information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From Human Correction For Data-Centric Deep Learning. (arXiv:2102.00225v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% accuracy is trained on the corrected dataset, which improve the baseline\nfrom 83.3% to 91.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chess as a Testbed for Language Model State Tracking. (arXiv:2102.13249v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.13249","description":"<p>Transformer language models have made tremendous strides in natural language\nunderstanding tasks. However, the complexity of natural language makes it\nchallenging to ascertain how accurately these models are tracking the world\nstate underlying the text. Motivated by this issue, we consider the task of\nlanguage modeling for the game of chess. Unlike natural language, chess\nnotations describe a simple, constrained, and deterministic domain. Moreover,\nwe observe that the appropriate choice of chess notation allows for directly\nprobing the world state, without requiring any additional probing-related\nmachinery. We find that: (a) With enough training data, transformer language\nmodels can learn to track pieces and predict legal moves with high accuracy\nwhen trained solely on move sequences. (b) For small training sets providing\naccess to board state information during training can yield significant\nimprovements. (c) The success of transformer language models is dependent on\naccess to the entire game history i.e. \"full attention\". Approximating this\nfull attention results in a significant performance drop. We propose this\ntestbed as a benchmark for future work on the development and analysis of\ntransformer language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toshniwal_S/0/1/0/all/0/1\">Shubham Toshniwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning. (arXiv:2105.01311v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01311","description":"<p>Transformer-based language model approaches to automated story generation\ncurrently provide state-of-the-art results. However, they still suffer from\nplot incoherence when generating narratives over time, and critically lack\nbasic commonsense reasoning. Furthermore, existing methods generally focus only\non single-character stories, or fail to track characters at all. To improve the\ncoherence of generated narratives and to expand the scope of character-centric\nnarrative generation, we introduce Commonsense-inference Augmented neural\nStoryTelling (CAST), a framework for introducing commonsense reasoning into the\ngeneration process with the option to model the interaction between multiple\ncharacters. We find that our CAST method produces significantly more coherent,\non-topic, enjoyable, and fluent stories than existing models in both the\nsingle-character and two-character settings in three storytelling domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Diversity and Limits of Human Explanations. (arXiv:2106.11988v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11988","description":"<p>A growing effort in NLP aims to build datasets of human explanations.\nHowever, the term explanation encompasses a broad range of notions, each with\ndifferent properties and ramifications. Our goal is to provide an overview of\ndiverse types of explanations and human limitations, and discuss implications\nfor collecting and using explanations in NLP. Inspired by prior work in\npsychology and cognitive sciences, we group existing human explanations in NLP\ninto three categories: proximal mechanism, evidence, and procedure. These three\ntypes differ in nature and have implications for the resultant explanations.\nFor instance, procedure is not considered explanations in psychology and\nconnects with a rich body of work on learning from instructions. The diversity\nof explanations is further evidenced by proxy questions that are needed for\nannotators to interpret and answer open-ended why questions. Finally,\nexplanations may require different, often deeper, understandings than\npredictions, which casts doubt on whether humans can provide useful\nexplanations in some tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Recognition under Consideration of the Emotion Component Process Model. (arXiv:2107.12895v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12895","description":"<p>Emotion classification in text is typically performed with neural network\nmodels which learn to associate linguistic units with emotions. While this\noften leads to good predictive performance, it does only help to a limited\ndegree to understand how emotions are communicated in various domains. The\nemotion component process model (CPM) by Scherer (2005) is an interesting\napproach to explain emotion communication. It states that emotions are a\ncoordinated process of various subcomponents, in reaction to an event, namely\nthe subjective feeling, the cognitive appraisal, the expression, a\nphysiological bodily reaction, and a motivational action tendency. We\nhypothesize that these components are associated with linguistic realizations:\nan emotion can be expressed by describing a physiological bodily reaction (\"he\nwas trembling\"), or the expression (\"she smiled\"), etc. We annotate existing\nliterature and Twitter emotion corpora with emotion component classes and find\nthat emotions on Twitter are predominantly expressed by event descriptions or\nsubjective reports of the feeling, while in literature, authors prefer to\ndescribe what characters do, and leave the interpretation to the reader. We\nfurther include the CPM in a multitask learning model and find that this\nsupports the emotion categorization. The annotated corpora are available at\nhttps://www.ims.uni-stuttgart.de/data/emotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casel_F/0/1/0/all/0/1\">Felix Casel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindl_A/0/1/0/all/0/1\">Amelie Heindl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Stimulus Detection in German News Headlines. (arXiv:2107.12920v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12920","description":"<p>Emotion stimulus extraction is a fine-grained subtask of emotion analysis\nthat focuses on identifying the description of the cause behind an emotion\nexpression from a text passage (e.g., in the sentence \"I am happy that I passed\nmy exam\" the phrase \"passed my exam\" corresponds to the stimulus.). Previous\nwork mainly focused on Mandarin and English, with no resources or models for\nGerman. We fill this research gap by developing a corpus of 2006 German news\nheadlines annotated with emotions and 811 instances with annotations of\nstimulus phrases. Given that such corpus creation efforts are time-consuming\nand expensive, we additionally work on an approach for projecting the existing\nEnglish GoodNewsEveryone (GNE) corpus to a machine-translated German version.\nWe compare the performance of a conditional random field (CRF) model (trained\nmonolingually on German and cross-lingually via projection) with a multilingual\nXLM-RoBERTa (XLM-R) model. Our results show that training with the German\ncorpus achieves higher F1 scores than projection. Experiments with XLM-R\noutperform their respective CRF counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Bao Minh Doan Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10904","description":"<p>With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Differential Privacy for Language Modeling. (arXiv:2108.12944v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12944","description":"<p>With the increasing applications of language models, it has become crucial to\nprotect these models from leaking private information. Previous work has\nattempted to tackle this challenge by training RNN-based language models with\ndifferential privacy guarantees. However, applying classical differential\nprivacy to language models leads to poor model performance as the underlying\nprivacy notion is over-pessimistic and provides undifferentiated protection for\nall tokens in the data. Given that the private information in natural language\nis sparse (for example, the bulk of an email might not carry personally\nidentifiable information), we propose a new privacy notion, selective\ndifferential privacy, to provide rigorous privacy guarantees on the sensitive\nportion of the data to improve model utility. To realize such a new notion, we\ndevelop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based\nlanguage models. Besides language modeling, we also apply the method to a more\nconcrete application--dialog systems. Experiments on both language modeling and\ndialog system building show that the proposed privacy-preserving mechanism\nachieves better utilities while remaining safe under various privacy attacks\ncompared to the baselines. The data and code are released at\nhttps://github.com/wyshi/lm_privacy to facilitate future research .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Weiyan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_A/0/1/0/all/0/1\">Aiqi Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Evan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges in Generalization in Open Domain Question Answering. (arXiv:2109.01156v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01156","description":"<p>Recent work on Open Domain Question Answering has shown that there is a large\ndiscrepancy in model performance between novel test questions and those that\nlargely overlap with training questions. However, it is unclear which aspects\nof novel questions make them challenging. Drawing upon studies on systematic\ngeneralization, we introduce and annotate questions according to three\ncategories that measure different levels and kinds of generalization: training\nset overlap, compositional generalization (comp-gen), and novel-entity\ngeneralization (novel-entity). When evaluating six popular parametric and\nnon-parametric models, we find that for the established Natural Questions and\nTriviaQA datasets, even the strongest model performance for\ncomp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the\nfull test set -- indicating the challenge posed by these types of questions.\nFurthermore, we show that whilst non-parametric models can handle questions\ncontaining novel entities relatively well, they struggle with those requiring\ncompositional generalization. Lastly, we find that key question difficulty\nfactors are: cascading errors from the retrieval component, frequency of\nquestion pattern, and frequency of the entity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing out Bias: Achieving Fairness Through Balanced Training. (arXiv:2109.08253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08253","description":"<p>Group bias in natural language processing tasks manifests as disparities in\nsystem error rates across texts authorized by different demographic groups,\ntypically disadvantaging minority groups. Dataset balancing has been shown to\nbe effective at mitigating bias, however existing approaches do not directly\naccount for correlations between author demographics and linguistic variables,\nlimiting their effectiveness. To achieve Equal Opportunity fairness, such as\nequal job opportunity without regard to demographics, this paper introduces a\nsimple, but highly effective, objective for countering bias using balanced\ntraining. We extend the method in the form of a gated model, which incorporates\nprotected attributes as input, and show that it is effective at reducing bias\nin predictions through demographic input perturbation, outperforming all other\nbias mitigation techniques when combined with balanced training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying the Suicidal Tendency on Social Media: A Survey. (arXiv:2110.03663v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2110.03663","description":"<p>Amid lockdown period more people express their feelings over social media\nplatforms due to closed third-place and academic researchers have witnessed\nstrong associations between the mental healthcare and social media posts. The\nstress for a brief period may lead to clinical depressions and the long-lasting\ntraits of prevailing depressions can be life threatening with suicidal ideation\nas the possible outcome. The increasing concern towards the rise in number of\nsuicide cases is because it is one of the leading cause of premature but\npreventable death. Recent studies have shown that mining social media data has\nhelped in quantifying the suicidal tendency of users at risk. This potential\nmanuscript elucidates the taxonomy of mental healthcare and highlights some\nrecent attempts in examining the potential of quantifying suicidal tendency on\nsocial media data. This manuscript presents the classification of heterogeneous\nfeatures from social media data and handling feature vector representation.\nAiming to identify the new research directions and advances in the development\nof Machine Learning (ML) and Deep Learning (DL) based models, a quantitative\nsynthesis and a qualitative review was carried out with corpus of over 77\npotential research articles related to stress, depression and suicide risk from\n2013 to 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Case Study on the Independence of Speech Emotion Recognition in Bangla and English Languages using Language-Independent Prosodic Features. (arXiv:2111.10776v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10776","description":"<p>A language agnostic approach to recognizing emotions from speech remains an\nincomplete and challenging task. In this paper, we performed a step-by-step\ncomparative analysis of Speech Emotion Recognition (SER) using Bangla and\nEnglish languages to assess whether distinguishing emotions from speech is\nindependent of language. Six emotions were categorized for this study, such as\n- happy, angry, neutral, sad, disgust, and fear. We employed three Emotional\nSpeech Sets (ESS), of which the first two were developed by native Bengali\nspeakers in Bangla and English languages separately. The third was a subset of\nthe Toronto Emotional Speech Set (TESS), which was developed by native English\nspeakers from Canada. We carefully selected language-independent prosodic\nfeatures, adopted a Support Vector Machine (SVM) model, and conducted three\nexperiments to carry out our proposition. In the first experiment, we measured\nthe performance of the three speech sets individually, followed by the second\nexperiment, where different ESS pairs were integrated to analyze the impact on\nSER. Finally, we measured the recognition rate by training and testing the\nmodel with different speech sets in the third experiment. Although this study\nreveals that SER in Bangla and English languages is mostly\nlanguage-independent, some disparities were observed while recognizing\nemotional states like disgust and fear in these two languages. Moreover, our\ninvestigations revealed that non-native speakers convey emotions through\nspeech, much like expressing themselves in their native tongue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saad_F/0/1/0/all/0/1\">Fardin Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_H/0/1/0/all/0/1\">Hasan Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Mohammad Ridwan Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaheen_M/0/1/0/all/0/1\">Md. Alamin Shaheen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farastu_P/0/1/0/all/0/1\">Paresha Farastu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Kamrul Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Numerical Reasoning Skills into Knowledge Base Question Answering Models. (arXiv:2112.06109v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06109","description":"<p>Embedding-based methods are popular for Knowledge Base Question Answering\n(KBQA), but few current models have numerical reasoning skills and thus\nstruggle to answer ordinal constrained questions. This paper proposes a new\nembedding-based KBQA framework which particularly takes numerical reasoning\ninto account. We present NumericalTransformer on top of NSM, a state-of-the-art\nembedding-based KBQA model, to create NT-NSM. To enable better training, we\npropose two pre-training tasks with explicit numerical-oriented loss functions\non two generated training datasets and a template-based data augmentation\nmethod for enriching ordinal constrained QA dataset. Extensive experiments on\nKBQA benchmarks demonstrate that with the help of our training algorithm,\nNT-NSM is empowered with numerical reasoning skills and substantially\noutperforms the baselines in answering ordinal constrained questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaokang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cuiping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block-Skim: Efficient Question Answering for Transformer. (arXiv:2112.08560v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08560","description":"<p>Transformer models have achieved promising results on natural language\nprocessing (NLP) tasks including extractive question answering (QA). Common\nTransformer encoders used in NLP tasks process the hidden states of all input\ntokens in the context paragraph throughout all layers. However, different from\nother tasks such as sequence classification, answering the raised question does\nnot necessarily need all the tokens in the context paragraph. Following this\nmotivation, we propose Block-skim, which learns to skim unnecessary context in\nhigher hidden layers to improve and accelerate the Transformer performance. The\nkey idea of Block-Skim is to identify the context that must be further\nprocessed and those that could be safely discarded early on during inference.\nCritically, we find that such information could be sufficiently derived from\nthe self-attention weights inside the Transformer model. We further prune the\nhidden states corresponding to the unnecessary positions early in lower layers,\nachieving significant inference-time speedup. To our surprise, we observe that\nmodels pruned in this way outperform their full-size counterparts. Block-Skim\nimproves QA models' accuracy on different datasets and achieves 3 times speedup\non BERT-base model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jingwen Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Minyi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuhao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Neural Story Generation with Reader Models. (arXiv:2112.08596v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08596","description":"<p>Automated storytelling has long captured the attention of researchers for the\nubiquity of narratives in everyday life. However, it is challenging to maintain\ncoherence and stay on-topic toward a specific ending when generating narratives\nwith neural language models. In this paper, we introduce Story generation with\nReader Models (StoRM), a framework in which a reader model is used to reason\nabout the story should progress. A reader model infers what a human reader\nbelieves about the concepts, entities, and relations about the fictional story\nworld. We show how an explicit reader model represented as a knowledge graph\naffords story coherence and provides controllability in the form of achieving a\ngiven story world state goal. Experiments show that our model produces\nsignificantly more coherent and on-topic stories, outperforming baselines in\ndimensions including plot plausibility and staying on topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kaige Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabdulkarim_A/0/1/0/all/0/1\">Amal Alabdulkarim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayam_H/0/1/0/all/0/1\">Harshith Kayam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dani_S/0/1/0/all/0/1\">Samihan Dani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge. (arXiv:2112.08619v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08619","description":"<p>Humans usually have conversations by making use of prior knowledge about a\ntopic and background information of the people whom they are talking to.\nHowever, existing conversational agents and datasets do not consider such\ncomprehensive information, and thus they have a limitation in generating the\nutterances where the knowledge and persona are fused properly. To address this\nissue, we introduce a call For Customized conversation (FoCus) dataset where\nthe customized answers are built with the user's persona and Wikipedia\nknowledge. To evaluate the abilities to make informative and customized\nutterances of pre-trained language models, we utilize BART and GPT-2 as well as\ntransformer-based models. We assess their generation abilities with automatic\nscores and conduct human evaluations for qualitative results. We examine\nwhether the model reflects adequate persona and knowledge with our proposed two\nsub-tasks, persona grounding (PG) and knowledge grounding (KG). Moreover, we\nshow that the utterances of our data are constructed with the proper knowledge\nand persona through grounding quality assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoonna Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Jungwoo Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_Y/0/1/0/all/0/1\">Yuna Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_D/0/1/0/all/0/1\">Dongsuk Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Suhyune Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Donghoon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks. (arXiv:2112.08688v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08688","description":"<p>Retrieval-augmented generation models have shown state-of-the-art performance\nacross many knowledge-intensive NLP tasks such as open question answering and\nfact verification. These models are trained to generate the final output given\nthe retrieved passages, which can be irrelevant to the original query, leading\nto learning spurious cues or answer memorization. This work introduces a method\nto incorporate the evidentiality of passages -- whether a passage contains\ncorrect evidence to support the output -- into training the generator. We\nintroduce a multi-task learning framework to jointly generate the final output\nand predict the evidentiality of each passage, leveraging a new task-agnostic\nmethod to obtain silver evidentiality labels for supervision. Our experiments\non five datasets across three knowledge-intensive tasks show that our new\nevidentiality-guided generator significantly outperforms its direct counterpart\nwith the same-size model and advances the state of the art on FaVIQ-Ambig. We\nattribute these improvements to both the auxiliary multi-task learning and\nsilver evidentiality mining techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Data Corruption Affect Natural Language Understanding Models? A Study on GLUE datasets. (arXiv:2201.04467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.04467","description":"<p>A central question in natural language understanding (NLU) research is\nwhether high performance demonstrates the models' strong reasoning\ncapabilities. We present an extensive series of controlled experiments where\npre-trained language models are exposed to data that have undergone specific\ncorruption transformations. These involve removing instances of specific word\nclasses and often lead to non-sensical sentences. Our results show that\nperformance remains high on most GLUE tasks when the models are fine-tuned or\ntested on corrupted data, suggesting that they leverage other cues for\nprediction even in non-sensical contexts. Our proposed data transformations can\nbe used to assess the extent to which a specific dataset constitutes a proper\ntestbed for evaluating models' language understanding capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talman_A/0/1/0/all/0/1\">Aarne Talman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzikyriakidis_S/0/1/0/all/0/1\">Stergios Chatzikyriakidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1\">J&#xf6;rg Tiedemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretrained Language Models for Text Generation: A Survey. (arXiv:2201.05273v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05273","description":"<p>Text Generation aims to produce plausible and readable text in a human\nlanguage from input data. The resurgence of deep learning has greatly advanced\nthis field, in particular, with the help of neural generation models based on\npre-trained language models (PLMs). Text generation based on PLMs is viewed as\na promising approach in both academia and industry. In this paper, we provide a\nsurvey on the utilization of PLMs in text generation. We begin with introducing\nthree key aspects of applying PLMs to text generation: 1) how to encode the\ninput into representations preserving input semantics which can be fused into\nPLMs; 2) how to design an effective PLM to serve as the generation model; and\n3) how to effectively optimize PLMs given the reference text and to ensure that\nthe generated texts satisfy special text properties. Then, we show the major\nchallenges arisen in these aspects, as well as possible solutions for them. We\nalso include a summary of various useful resources and typical text generation\napplications based on PLMs. Finally, we highlight the future research\ndirections which will further improve these PLMs for text generation. This\ncomprehensive survey is intended to help researchers interested in text\ngeneration problems to learn the core concepts, the main techniques and the\nlatest developments in this area based on PLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05729","description":"<p>Contrastive language-image pretraining (CLIP) links vision and language\nmodalities into a unified embedding space, yielding the tremendous potential\nfor vision-language (VL) tasks. While early concurrent works have begun to\nstudy this potential on a subset of tasks, important questions remain: 1) What\nis the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in\nlow-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches\nwithout impacting inference or pretraining complexity? In this work, we seek to\nanswer these questions through two key contributions. First, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata availability constraints and conditions of domain shift. Second, we\npropose an approach, named CLIP Targeted Distillation (CLIP-TD), to\nintelligently distill knowledge from CLIP into existing architectures using a\ndynamically weighted objective applied to adaptively selected tokens per\ninstance. Experiments demonstrate that our proposed CLIP-TD leads to\nexceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to\n71.3%) conditions of VCR, while simultaneously improving performance under\nstandard fully-supervised conditions (up to 2%), achieving state-of-art\nperformance on VCR compared to other single models that are pretrained with\nimage-text data only. On SNLI-VE, CLIP-TD produces significant gains in\nlow-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On\nVQA, CLIP-TD provides improvement in low-shot (up to 9%), and in\nfully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works\nutilizing CLIP for finetuning, as well as baseline naive distillation\napproaches. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Learning for Aspect and Polarity Classification in Persian Reviews Using Multi-Task Deep Learning. (arXiv:2201.06313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.06313","description":"<p>The purpose of this paper focuses on two sub-tasks related to aspect-based\nsentiment analysis, namely, aspect category detection (ACD) and aspect category\npolarity (ACP) in the Persian language. Its ability to identify all aspects\ndiscussed in the text is what makes aspect-based sentiment analysis so\nimportant and useful. While aspect-based sentiment analysis analyses all\naspects of the text, it will be most useful when it is able to identify their\npolarity along with their identification. Most of the previous methods only\nfocus on solving one of these sub-tasks separately or use two separate models.\nThus, the process is pipelined, that is, the aspects are identified before the\npolarities are identified. In practice, these methods lead to model errors that\nare unsuitable for practical applications. In other words, ACD mistakes are\nsent to ACP. In this paper, we propose a multi-task learning model based on\ndeep neural networks, which can concurrently detect aspect category and detect\naspect category polarity. We evaluated the proposed method using a Persian\nlanguage dataset in the movie domain on different deep learning-based models.\nFinal experiments show that the CNN model has better results than other models.\nThe reason is CNN's capability to extract local features. Since sentiment is\nexpressed using specific words and phrases, CNN has been able to be more\nefficient in identifying these in this dataset.iments show that the CNN model\nhas better results than other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vazan_M/0/1/0/all/0/1\">Milad Vazan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02394","description":"<p>Large Language Models have been successful in a wide variety of Natural\nLanguage Processing tasks by capturing the compositionality of the text\nrepresentations. In spite of their great success, these vector representations\nfail to capture meaning of idiomatic multi-word expressions (MWEs). In this\npaper, we focus on the detection of idiomatic expressions by using binary\nclassification. We use a dataset consisting of the literal and idiomatic usage\nof MWEs in English and Portuguese. Thereafter, we perform the classification in\ntwo different settings: zero shot and one shot, to determine if a given\nsentence contains an idiom or not. N shot classification for this task is\ndefined by N number of common idioms between the training and testing sets. In\nthis paper, we train multiple Large Language Models in both the settings and\nachieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score\n(macro) of 0.85 for the one shot setting. An implementation of our work can be\nfound at\nhttps://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakhotiya_Y/0/1/0/all/0/1\">Yash Jakhotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1\">Ashwin Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Raj Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07427","description":"<p>Authors of posts in social media communicate their emotions and what causes\nthem with text and images. While there is work on emotion and stimulus\ndetection for each modality separately, it is yet unknown if the modalities\ncontain complementary emotion information in social media. We aim at filling\nthis research gap and contribute a novel, annotated corpus of English\nmultimodal Reddit posts. On this resource, we develop models to automatically\ndetect the relation between image and text, an emotion stimulus category and\nthe emotion class. We evaluate if these tasks require both modalities and find\nfor the image-text relations, that text alone is sufficient for most categories\n(complementary, illustrative, opposing): the information in the text allows to\npredict if an image is required for emotion understanding. The emotions of\nanger and sadness are best predicted with a multimodal model, while text alone\nis sufficient for disgust, joy, and surprise. Stimuli depicted by objects,\nanimals, food, or a person are best predicted by image-only models, while\nmultimodal models are most effective on art, events, memes, places, or\nscreenshots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khlyzova_A/0/1/0/all/0/1\">Anna Khlyzova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1\">Carina Silberer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users. (arXiv:2202.10415v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10415","description":"<p>Machine-learned models for author profiling in social media often rely on\ndata acquired via self-reporting-based psychometric tests (questionnaires)\nfilled out by social media users. This is an expensive but accurate data\ncollection strategy. Another, less costly alternative, which leads to\npotentially more noisy and biased data, is to rely on labels inferred from\npublicly available information in the profiles of the users, for instance\nself-reported diagnoses or test results. In this paper, we explore a third\nstrategy, namely to directly use a corpus of items from validated psychometric\ntests as training data. Items from psychometric tests often consist of\nsentences from an I-perspective (e.g., \"I make friends easily.\"). Such corpora\nof test items constitute 'small data', but their availability for many concepts\nis a rich resource. We investigate this approach for personality profiling, and\nevaluate BERT classifiers fine-tuned on such psychometric test items for the\nbig five personality traits (openness, conscientiousness, extraversion,\nagreeableness, neuroticism) and analyze various augmentation strategies\nregarding their potential to address the challenges coming with such a small\ncorpus. Our evaluation on a publicly available Twitter corpus shows a\ncomparable performance to in-domain training for 4/5 personality traits with\nT5-based data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreuter_A/0/1/0/all/0/1\">Anne Kreuter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassenberg_K/0/1/0/all/0/1\">Kai Sassenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"splink\" is happy and \"phrouth\" is scary: Emotion Intensity Analysis for Nonsense Words. (arXiv:2202.12132v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12132","description":"<p>People associate affective meanings to words - \"death\" is scary and sad while\n\"party\" is connotated with surprise and joy. This raises the question if the\nassociation is purely a product of the learned affective imports inherent to\nsemantic meanings, or is also an effect of other features of words, e.g.,\nmorphological and phonological patterns. We approach this question with an\nannotation-based analysis leveraging nonsense words. Specifically, we conduct a\nbest-worst scaling crowdsourcing study in which participants assign intensity\nscores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense\nwords and, for comparison of the results to previous work, to 68 real words.\nBased on this resource, we develop character-level and phonology-based\nintensity regressors. We evaluate them on both nonsense words and real words\n(making use of the NRC emotion intensity lexicon of 7493 words), across six\nemotion categories. The analysis of our data reveals that some phonetic\npatterns show clear differences between emotion intensities. For instance, s as\na first phoneme contributes to joy, sh to surprise, p as last phoneme more to\ndisgust than to anger and fear. In the modelling experiments, a regressor\ntrained on real words from the NRC emotion intensity lexicon shows a higher\nperformance (r = 0.17) than regressors that aim at learning the emotion\nconnotation purely from nonsense words. We conclude that humans do associate\naffective meaning to words based on surface patterns, but also based on\nsimilarities to existing words (\"juy\" to \"joy\", or \"flike\" to \"like\").\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabbatino_V/0/1/0/all/0/1\">Valentino Sabbatino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweitzer_A/0/1/0/all/0/1\">Antje Schweitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Equal Opportunity Fairness through Adversarial Learning. (arXiv:2203.06317v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06317","description":"<p>Adversarial training is a common approach for bias mitigation in natural\nlanguage processing. Although most work on debiasing is motivated by equal\nopportunity, it is not explicitly captured in standard adversarial training. In\nthis paper, we propose an augmented discriminator for adversarial training,\nwhich takes the target class as input to create richer features and more\nexplicitly model equal opportunity. Experimental results over two datasets show\nthat our method substantially improves over standard adversarial debiasing\nmethods, in terms of the performance--fairness trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11406","description":"<p>Self-augmentation has received increasing research interest recently to\nimprove named entity recognition (NER) performance in low-resource scenarios.\nToken substitution and mixup are two feasible heterogeneous self-augmentation\ntechniques for NER that can achieve effective performance with certain\nspecialized efforts. Noticeably, self-augmentation may introduce potentially\nnoisy augmented data. Prior research has mainly resorted to heuristic\nrule-based constraints to reduce the noise for specific self-augmentation\nmethods individually. In this paper, we revisit these two typical\nself-augmentation methods for NER, and propose a unified meta-reweighting\nstrategy for them to achieve a natural integration. Our method is easily\nextensible, imposing little effort on a specific self-augmentation method.\nExperiments on different Chinese and English NER benchmarks show that our token\nsubstitution and mixup method, as well as their integration, can achieve\neffective performance improvement. Based on the meta-reweighting mechanism, we\ncan enhance the advantages of the self-augmentation techniques without much\nextra effort.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Linzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chunping Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Ability of Language Models to Interpret Figurative Language. (arXiv:2204.12632v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12632","description":"<p>Figurative and metaphorical language are commonplace in discourse, and\nfigurative expressions play an important role in communication and cognition.\nHowever, figurative language has been a relatively under-studied area in NLP,\nand it remains an open question to what extent modern language models can\ninterpret nonliteral phrases. To address this question, we introduce Fig-QA, a\nWinograd-style nonliteral language understanding task consisting of correctly\ninterpreting paired figurative phrases with divergent meanings. We evaluate the\nperformance of several state-of-the-art language models on this task, and find\nthat although language models achieve performance significantly over chance,\nthey still fall short of human performance, particularly in zero- or few-shot\nsettings. This suggests that further work is needed to improve the nonliteral\nreasoning capabilities of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emmy Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kenneth Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeBERT: Enhancing Pre-Trained Language Representations with Temporal Information. (arXiv:2204.13032v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13032","description":"<p>Time is an important aspect of text documents, which has been widely\nexploited in natural language processing and has strong influence, for example,\nin temporal information retrieval, where the temporal information of queries or\ndocuments needs to be identified for relevance estimation. Event-related tasks\nlike event ordering, which aims to order events by their occurrence time, needs\nto determine the temporal information of events, too. In this work, we\ninvestigate methods for incorporating temporal information during pre-training\nto further improve the performance on time-related tasks. Compared with BERT\nwhich utilizes synchronic document collections (BooksCorpus and English\nWikipedia) as the training corpora, we use long-span temporal news collection\nfor building word representations. We introduce TimeBERT, a novel language\nrepresentation model trained on a temporal collection of news articles via two\nnew pre-training tasks, which harness two distinct temporal signals to\nconstruct time-aware language representation. The experimental results show\nthat TimeBERT consistently outperforms BERT and other existing pre-trained\nmodels, with substantial gains on different downstream NLP tasks or\napplications for which time is of importance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas. (arXiv:2205.00872v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00872","description":"<p>Maintaining a consistent persona is essential for building a human-like\nconversational model. However, the lack of attention to the partner makes the\nmodel more egocentric: they tend to show their persona by all means such as\ntwisting the topic stiffly, pulling the conversation to their own interests\nregardless, and rambling their persona with little curiosity to the partner. In\nthis work, we propose COSPLAY(COncept Set guided PersonaLized dialogue\ngeneration Across both partY personas) that considers both parties as a \"team\":\nexpressing self-persona while keeping curiosity toward the partner, leading\nresponses around mutual personas, and finding the common ground. Specifically,\nwe first represent self-persona, partner persona and mutual dialogue all in the\nconcept sets. Then, we propose the Concept Set framework with a suite of\nknowledge-enhanced operations to process them such as set algebras, set\nexpansion, and set distance. Based on these operations as medium, we train the\nmodel by utilizing 1) concepts of both party personas, 2) concept relationship\nbetween them, and 3) their relationship to the future dialogue. Extensive\nexperiments on a large public dataset, Persona-Chat, demonstrate that our model\noutperforms state-of-the-art baselines for generating less egocentric, more\nhuman-like, and higher quality responses in both automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chuangbai Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemAttack: Natural Textual Attacks via Different Semantic Spaces. (arXiv:2205.01287v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01287","description":"<p>Recent studies show that pre-trained language models (LMs) are vulnerable to\ntextual adversarial attacks. However, existing attack methods either suffer\nfrom low attack success rates or fail to search efficiently in the\nexponentially large perturbation space. We propose an efficient and effective\nframework SemAttack to generate natural adversarial text by constructing\ndifferent semantic perturbation functions. In particular, SemAttack optimizes\nthe generated perturbations constrained on generic semantic spaces, including\ntypo space, knowledge space (e.g., WordNet), contextualized semantic space\n(e.g., the embedding space of BERT clusterings), or the combination of these\nspaces. Thus, the generated adversarial texts are more semantically close to\nthe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)\nlarge-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are\nstill vulnerable to SemAttack. We further demonstrate that SemAttack is general\nand able to generate natural adversarial texts for different languages (e.g.,\nEnglish and Chinese) with high attack success rates. Human evaluations also\nconfirm that our generated adversarial texts are natural and barely affect\nhuman performance. Our code is publicly available at\nhttps://github.com/AI-secure/SemAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chejian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Transfer Prompts for Text Generation. (arXiv:2205.01543v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01543","description":"<p>Pretrained language models (PLMs) have made remarkable progress in text\ngeneration tasks via fine-tuning. While, it is challenging to fine-tune PLMs in\na data-scarce situation. Therefore, it is non-trivial to develop a general and\nlightweight model that can adapt to various text generation tasks based on\nPLMs. To fulfill this purpose, the recent prompt-based learning offers a\npotential solution. In this paper, we improve this technique and propose a\nnovel prompt-based method (PTG) for text generation in a transferable setting.\nFirst, PTG learns a set of source prompts for various source generation tasks\nand then transfers these prompts as target prompts to perform target generation\ntasks. To consider both task- and instance-level information, we design an\nadaptive attention mechanism to derive the target prompts. For each data\ninstance, PTG learns a specific target prompt by attending to highly relevant\nsource prompts. In extensive experiments, PTG yields competitive or better\nresults than fine-tuning methods. We release our source prompts as an open\nresource, where users can add or reuse them to improve new text generation\ntasks for future research. Code and data can be available at\nhttps://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tianyi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jian-Yun Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CogIntAc: Modeling the Relationships between Intention, Emotion and Action in Interactive Process from Cognitive Perspective. (arXiv:2205.03540v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2205.03540","description":"<p>Intention, emotion and action are important psychological factors in human\nactivities, which play an important role in the interaction between\nindividuals. How to model the interaction process between individuals by\nanalyzing the relationship of their intentions, emotions, and actions at the\ncognitive level is challenging. In this paper, we propose a novel cognitive\nframework of individual interaction. The core of the framework is that\nindividuals achieve interaction through external action driven by their inner\nintention. Based on this idea, the interactions between individuals can be\nconstructed by establishing relationships between the intention, emotion and\naction. Furthermore, we conduct analysis on the interaction between individuals\nand give a reasonable explanation for the predicting results. To verify the\neffectiveness of the framework, we reconstruct a dataset and propose three\ntasks as well as the corresponding baseline models, including action abduction,\nemotion prediction and action generation. The novel framework shows an\ninteresting perspective on mimicking the mental state of human beings in\ncognitive science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Machine Translation Systems for the Next Thousand Languages. (arXiv:2205.03983v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.03983","description":"<p>In this paper we share findings from our effort to build practical machine\ntranslation (MT) systems capable of translating across over one thousand\nlanguages. We describe results in three research domains: (i) Building clean,\nweb-mined datasets for 1500+ languages by leveraging semi-supervised\npre-training for language identification and developing data-driven filtering\ntechniques; (ii) Developing practical MT models for under-served languages by\nleveraging massively multilingual models trained with supervised parallel data\nfor over 100 high-resource languages and monolingual datasets for an additional\n1000+ languages; and (iii) Studying the limitations of evaluation metrics for\nthese languages and conducting qualitative analysis of the outputs from our MT\nmodels, highlighting several frequent error modes of these types of models. We\nhope that our work provides useful insights to practitioners working towards\nbuilding MT systems for currently understudied languages, and highlights\nresearch directions that can complement the weaknesses of massively\nmultilingual models in data-sparse settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caswell_I/0/1/0/all/0/1\">Isaac Caswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esch_D/0/1/0/all/0/1\">Daan van Esch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Mengmeng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baljekar_P/0/1/0/all/0/1\">Pallavi Baljekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_W/0/1/0/all/0/1\">Wolfgang Macherey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breiner_T/0/1/0/all/0/1\">Theresa Breiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Axelrod_V/0/1/0/all/0/1\">Vera Axelrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riesa_J/0/1/0/all/0/1\">Jason Riesa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mia Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macherey_K/0/1/0/all/0/1\">Klaus Macherey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutkin_A/0/1/0/all/0/1\">Alexander Gutkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Apurva Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Macduff Hughes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Climate Awareness in NLP Research. (arXiv:2205.05071v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05071","description":"<p>The climate impact of AI, and NLP research in particular, has become a\nserious issue given the enormous amount of energy that is increasingly being\nused for training and running computational models. Consequently, increasing\nfocus is placed on efficient NLP. However, this important initiative lacks\nsimple guidelines that would allow for systematic climate reporting of NLP\nresearch. We argue that this deficiency is one of the reasons why very few\npublications in NLP report key figures that would allow a more thorough\nexamination of environmental impact. As a remedy, we propose a climate\nperformance model card with the primary purpose of being practically usable\nwith only limited information about experiments and the underlying computer\nhardware. We describe why this step is essential to increase awareness about\nthe environmental impact of NLP research and, thereby, paving the way for more\nthorough discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Framework for Event-based Computer Vision on a Mobile Device. (arXiv:2205.06836v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06836","description":"<p>We present the first publicly available Android framework to stream data from\nan event camera directly to a mobile phone. Today's mobile devices handle a\nwider range of workloads than ever before and they incorporate a growing gamut\nof sensors that make devices smarter, more user friendly and secure.\nConventional cameras in particular play a central role in such tasks, but they\ncannot record continuously, as the amount of redundant information recorded is\ncostly to process. Bio-inspired event cameras on the other hand only record\nchanges in a visual scene and have shown promising low-power applications that\nspecifically suit mobile tasks such as face detection, gesture recognition or\ngaze tracking. Our prototype device is the first step towards embedding such an\nevent camera into a battery-powered handheld device. The mobile framework\nallows us to stream events in real-time and opens up the possibilities for\nalways-on and on-demand sensing on mobile phones. To liaise the asynchronous\nevent camera output with synchronous von Neumann hardware, we look at how\nbuffering events and processing them in batches can benefit mobile\napplications. We evaluate our framework in terms of latency and throughput and\nshow examples of computer vision tasks that involve both event-by-event and\npre-trained neural network methods for gesture recognition, aperture robust\noptical flow and grey-level image reconstruction from events. The code is\navailable at https://github.com/neuromorphic-paris/frog\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lenz_G/0/1/0/all/0/1\">Gregor Lenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picaud_S/0/1/0/all/0/1\">Serge Picaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ieng_S/0/1/0/all/0/1\">Sio-Hoi Ieng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Images to Probabilistic Anatomical Shapes: A Deep Variational Bottleneck Approach. (arXiv:2205.06862v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06862","description":"<p>Statistical shape modeling (SSM) directly from 3D medical images is an\nunderutilized tool for detecting pathology, diagnosing disease, and conducting\npopulation-level morphology analysis. Deep learning frameworks have increased\nthe feasibility of adopting SSM in medical practice by reducing the\nexpert-driven manual and computational overhead in traditional SSM workflows.\nHowever, translating such frameworks to clinical practice requires calibrated\nuncertainty measures as neural networks can produce over-confident predictions\nthat cannot be trusted in sensitive clinical decision-making. Existing\ntechniques for predicting shape with aleatoric (data-dependent) uncertainty\nutilize a principal component analysis (PCA) based shape representation\ncomputed in isolation from the model training. This constraint restricts the\nlearning task to solely estimating pre-defined shape descriptors from 3D images\nand imposes a linear relationship between this shape representation and the\noutput (i.e., shape) space. In this paper, we propose a principled framework\nbased on the variational information bottleneck theory to relax these\nassumptions while predicting probabilistic shapes of anatomy directly from\nimages without supervised encoding of shape descriptors. Here, the latent\nrepresentation is learned in the context of the learning task, resulting in a\nmore scalable, flexible model that better captures data non-linearity.\nAdditionally, this model is self-regularized and generalizes better given\nlimited training data. Our experiments demonstrate that the proposed method\nprovides improved accuracy and better calibrated aleatoric uncertainty\nestimates than state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jadie Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1\">Shireen Elhabian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Augmented Face Images to Improve Facial Recognition Tasks. (arXiv:2205.06873v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06873","description":"<p>We present a framework that uses GAN-augmented images to complement certain\nspecific attributes, usually underrepresented, for machine learning model\ntraining. This allows us to improve inference quality over those attributes for\nthe facial recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shuo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guoxian Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wan-Chun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Linjie Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVCAffe: A Large Scale Audio-Visual Dataset of Cognitive Load and Affect for Remote Work. (arXiv:2205.06887v1 [cs.HC])","link":"http://arxiv.org/abs/2205.06887","description":"<p>We introduce AVCAffe, the first Audio-Visual dataset consisting of Cognitive\nload and Affect attributes. We record AVCAffe by simulating remote work\nscenarios over a video-conferencing platform, where subjects collaborate to\ncomplete a number of cognitively engaging tasks. AVCAffe is the largest\noriginally collected (not collected from the Internet) affective dataset in\nEnglish language. We recruit 106 participants from 18 different countries of\norigin, spanning an age range of 18 to 57 years old, with a balanced\nmale-female ratio. AVCAffe comprises a total of 108 hours of video, equivalent\nto more than 58,000 clips along with task-based self-reported ground truth\nlabels for arousal, valence, and cognitive load attributes such as mental\ndemand, temporal demand, effort, and a few others. We believe AVCAffe would be\na challenging benchmark for the deep learning research community given the\ninherent difficulty of classifying affect and cognitive load in particular.\nMoreover, our dataset fills an existing timely gap by facilitating the creation\nof learning systems for better self-management of remote work meetings, and\nfurther study of hypotheses regarding the impact of remote work on cognitive\nload and affective states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_P/0/1/0/all/0/1\">Pritam Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posen_A/0/1/0/all/0/1\">Aaron Posen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Representation Learning for 3D MRI Super Resolution with Degradation Adaptation. (arXiv:2205.06891v1 [eess.IV])","link":"http://arxiv.org/abs/2205.06891","description":"<p>High-resolution (HR) MRI is critical in assisting the doctor's diagnosis and\nimage-guided treatment, but is hard to obtain in a clinical setting due to long\nacquisition time. Therefore, the research community investigated deep\nlearning-based super-resolution (SR) technology to reconstruct HR MRI images\nwith shortened acquisition time. However, training such neural networks usually\nrequires paired HR and low-resolution (LR) in-vivo images, which are difficult\nto acquire due to patient movement during and between the image acquisition.\nRigid movements of hard tissues can be corrected with image-registration,\nwhereas the alignment of deformed soft tissues is challenging, making it\nimpractical to train the neural network with such authentic HR and LR image\npairs. Therefore, most of the previous studies proposed SR reconstruction by\nemploying authentic HR images and synthetic LR images downsampled from the HR\nimages, yet the difference in degradation representations between synthetic and\nauthentic LR images suppresses the performance of SR reconstruction from\nauthentic LR images. To mitigate the aforementioned problems, we propose a\nnovel Unsupervised DEgradation Adaptation Network (UDEAN). Our model consists\nof two components: the degradation learning network and the SR reconstruction\nnetwork. The degradation learning network downsamples the HR images by\naddressing the degradation representation of the misaligned or unpaired LR\nimages, and the SR reconstruction network learns the mapping from the\ndownsampled HR images to their original HR images. As a result, the SR\nreconstruction network can generate SR images from the LR images and achieve\ncomparable quality to the HR images. Experimental results show that our method\noutperforms the state-of-the-art models and can potentially be applied in\nreal-world clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahn_E/0/1/0/all/0/1\">Euijoon Ahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razi_A/0/1/0/all/0/1\">Adeel Razi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageSig: A signature transform for ultra-lightweight image recognition. (arXiv:2205.06929v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06929","description":"<p>This paper introduces a new lightweight method for image recognition.\nImageSig is based on computing signatures and does not require a convolutional\nstructure or an attention-based encoder. It is striking to the authors that it\nachieves: a) an accuracy for 64 X 64 RGB images that exceeds many of the\nstate-of-the-art methods and simultaneously b) requires orders of magnitude\nless FLOPS, power and memory footprint. The pretrained model can be as small as\n44.2 KB in size. ImageSig shows unprecedented performance on hardware such as\nRaspberry Pi and Jetson-nano. ImageSig treats images as streams with multiple\nchannels. These streams are parameterized by spatial directions. We contribute\nto the functionality of signature and rough path theory to stream-like data and\nvision tasks on static images beyond temporal streams. With very few parameters\nand small size models, the key advantage is that one could have many of these\n\"detectors\" assembled on the same chip; moreover, the feature acquisition can\nbe performed once and shared between different models of different tasks -\nfurther accelerating the process. This contributes to energy efficiency and the\nadvancements of embedded AI at the edge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1\">Mohamed R. Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyons_T/0/1/0/all/0/1\">Terry Lyons</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Saliency-Guided Street View Image Inpainting Framework for Efficient Last-Meters Wayfinding. (arXiv:2205.06934v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06934","description":"<p>Global Positioning Systems (GPS) have played a crucial role in various\nnavigation applications. Nevertheless, localizing the perfect destination\nwithin the last few meters remains an important but unresolved problem. Limited\nby the GPS positioning accuracy, navigation systems always show users a\nvicinity of a destination, but not its exact location. Street view images (SVI)\nin maps as an immersive media technology have served as an aid to provide the\nphysical environment for human last-meters wayfinding. However, due to the\nlarge diversity of geographic context and acquisition conditions, the captured\nSVI always contains various distracting objects (e.g., pedestrians and\nvehicles), which will distract human visual attention from efficiently finding\nthe destination in the last few meters. To address this problem, we highlight\nthe importance of reducing visual distraction in image-based wayfinding by\nproposing a saliency-guided image inpainting framework. It aims at redirecting\nhuman visual attention from distracting objects to destination-related objects\nfor more efficient and accurate wayfinding in the last meters. Specifically, a\ncontext-aware distracting object detection method driven by deep salient object\ndetection has been designed to extract distracting objects from three semantic\nlevels in SVI. Then we employ a large-mask inpainting method with fast Fourier\nconvolutions to remove the detected distracting objects. Experimental results\nwith both qualitative and quantitative analysis show that our saliency-guided\ninpainting method can not only achieve great perceptual quality in street view\nimages but also redirect the human's visual attention to focus more on static\nlocation-related objects than distracting ones. The human-based evaluation also\njustified the effectiveness of our method in improving the efficiency of\nlocating the target destination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1\">Shan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense residual Transformer for image denoising. (arXiv:2205.06944v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06944","description":"<p>Image denoising is an important low-level computer vision task, which aims to\nreconstruct a noise-free and high-quality image from a noisy image. With the\ndevelopment of deep learning, convolutional neural network (CNN) has been\ngradually applied and achieved great success in image denoising, image\ncompression, image enhancement, etc. Recently, Transformer has been a hot\ntechnique, which is widely used to tackle computer vision tasks. However, few\nTransformer-based methods have been proposed for low-level vision tasks. In\nthis paper, we proposed an image denoising network structure based on\nTransformer, which is named DenSformer. DenSformer consists of three modules,\nincluding a preprocessing module, a local-global feature extraction module, and\na reconstruction module. Specifically, the local-global feature extraction\nmodule consists of several Sformer groups, each of which has several\nETransformer layers and a convolution layer, together with a residual\nconnection. These Sformer groups are densely skip-connected to fuse the feature\nof different layers, and they jointly capture the local and global information\nfrom the given noisy images. We conduct our model on comprehensive experiments.\nExperimental results prove that our DenSformer achieves improvement compared to\nsome state-of-the-art methods, both for the synthetic noise data and real noise\ndata, in the objective and subjective evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meiqin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_X/0/1/0/all/0/1\">Xiaojuan Ban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BronchusNet: Region and Structure Prior Embedded Representation Learning for Bronchus Segmentation and Classification. (arXiv:2205.06947v1 [eess.IV])","link":"http://arxiv.org/abs/2205.06947","description":"<p>CT-based bronchial tree analysis plays an important role in the\ncomputer-aided diagnosis for respiratory diseases, as it could provide\nstructured information for clinicians. The basis of airway analysis is\nbronchial tree reconstruction, which consists of bronchus segmentation and\nclassification. However, there remains a challenge for accurate bronchial\nanalysis due to the individual variations and the severe class imbalance. In\nthis paper, we propose a region and structure prior embedded framework named\nBronchusNet to achieve accurate segmentation and classification of bronchial\nregions in CT images. For bronchus segmentation, we propose an adaptive hard\nregion-aware UNet that incorporates multi-level prior guidance of hard\npixel-wise samples in the general Unet segmentation network to achieve better\nhierarchical feature learning. For the classification of bronchial branches, we\npropose a hybrid point-voxel graph learning module to fully exploit bronchial\nstructure priors and to support simultaneous feature interactions across\ndifferent branches. To facilitate the study of bronchial analysis, we\ncontribute~\\textbf{BRSC}: an open-access benchmark of \\textbf{BR}onchus imaging\nanalysis with high-quality pixel-wise \\textbf{S}egmentation masks and the\n\\textbf{C}lass of bronchial segments. Experimental results on BRSC show that\nour proposed method not only achieves the state-of-the-art performance for\nbinary segmentation of bronchial region but also exceeds the best existing\nmethod on bronchial branches classification by 6.9\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Wenhao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_H/0/1/0/all/0/1\">Haifan Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haofeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Hong Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RiCS: A 2D Self-Occlusion Map for Harmonizing Volumetric Objects. (arXiv:2205.06975v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06975","description":"<p>There have been remarkable successes in computer vision with deep learning.\nWhile such breakthroughs show robust performance, there have still been many\nchallenges in learning in-depth knowledge, like occlusion or predicting\nphysical interactions. Although some recent works show the potential of 3D data\nin serving such context, it is unclear how we efficiently provide 3D input to\nthe 2D models due to the misalignment in dimensionality between 2D and 3D. To\nleverage the successes of 2D models in predicting self-occlusions, we design\nRay-marching in Camera Space (RiCS), a new method to represent the\nself-occlusions of foreground objects in 3D into a 2D self-occlusion map. We\ntest the effectiveness of our representation on the human image harmonization\ntask by predicting shading that is coherent with a given background image. Our\nexperiments demonstrate that our representation map not only allows us to\nenhance the image quality but also to model temporally coherent complex shadow\neffects compared with the simulation-to-real and harmonization methods, both\nquantitatively and qualitatively. We further show that we can significantly\nimprove the performance of human parts segmentation networks trained on\nexisting synthetic datasets by enhancing the harmonization quality with our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yunseok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1\">Ruben Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Gesture Recognition for the Assistance of Visually Impaired People using Multi-Head Neural Networks. (arXiv:2205.06980v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06980","description":"<p>This paper proposes an interactive system for mobile devices controlled by\nhand gestures aimed at helping people with visual impairments. This system\nallows the user to interact with the device by making simple static and dynamic\nhand gestures. Each gesture triggers a different action in the system, such as\nobject recognition, scene description or image scaling (e.g., pointing a finger\nat an object will show a description of it). The system is based on a\nmulti-head neural network architecture, which initially detects and classifies\nthe gestures, and subsequently, depending on the gesture detected, performs a\nsecond stage that carries out the corresponding action. This multi-head\narchitecture optimizes the resources required to perform different tasks\nsimultaneously, and takes advantage of the information obtained from an initial\nbackbone to perform different processes in a second stage. To train and\nevaluate the system, a dataset with about 40k images was manually compiled and\nlabeled including different types of hand gestures, backgrounds (indoors and\noutdoors), lighting conditions, etc. This dataset contains synthetic gestures\n(whose objective is to pre-train the system in order to improve the results)\nand real images captured using different mobile phones. The results obtained\nand the comparison made with the state of the art show competitive results as\nregards the different actions performed by the system, such as the accuracy of\nclassification and localization of gestures, or the generation of descriptions\nfor objects and scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alashhab_S/0/1/0/all/0/1\">Samer Alashhab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_A/0/1/0/all/0/1\">Antonio Javier Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_M/0/1/0/all/0/1\">Miguel &#xc1;ngel Lozano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voxel-wise Adversarial Semi-supervised Learning for Medical Image Segmentation. (arXiv:2205.06987v1 [cs.CV])","link":"http://arxiv.org/abs/2205.06987","description":"<p>Semi-supervised learning for medical image segmentation is an important area\nof research for alleviating the huge cost associated with the construction of\nreliable large-scale annotations in the medical domain. Recent semi-supervised\napproaches have demonstrated promising results by employing consistency\nregularization, pseudo-labeling techniques, and adversarial learning. These\nmethods primarily attempt to learn the distribution of labeled and unlabeled\ndata by enforcing consistency in the predictions or embedding context. However,\nprevious approaches have focused only on local discrepancy minimization or\ncontext relations across single classes. In this paper, we introduce a novel\nadversarial learning-based semi-supervised segmentation method that effectively\nembeds both local and global features from multiple hidden layers and learns\ncontext relations between multiple classes. Our voxel-wise adversarial learning\nmethod utilizes a voxel-wise feature discriminator, which considers multilayer\nvoxel-wise features (involving both local and global features) as an input by\nembedding class-specific voxel-wise feature distribution. Furthermore, we\nimprove our previous representation learning method by overcoming information\nloss and learning stability problems, which enables rich representations of\nlabeled data. Our method outperforms current best-performing state-of-the-art\nsemi-supervised learning approaches on the image segmentation of the left\natrium (single class) and multiorgan datasets (multiclass). Moreover, our\nvisual interpretation of the feature space demonstrates that our proposed\nmethod enables a well-distributed and separated feature space from both labeled\nand unlabeled data, which improves the overall prediction results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chae Eun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyelim Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yeong-Gil Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1\">Minyoung Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap. (arXiv:2205.07002v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07002","description":"<p>As a rising task, panoptic segmentation is faced with challenges in both\nsemantic segmentation and instance segmentation. However, in terms of speed and\naccuracy, existing LiDAR methods in the field are still limited. In this paper,\nwe propose a fast and high-performance LiDAR-based framework, referred to as\nPanoptic-PHNet, with three attractive aspects: 1) We introduce a clustering\npseudo heatmap as a new paradigm, which, followed by a center grouping module,\nyields instance centers for efficient clustering without object-level learning\ntasks. 2) A knn-transformer module is proposed to model the interaction among\nforeground points for accurate offset regression. 3) For backbone design, we\nfuse the fine-grained voxel features and the 2D Bird's Eye View (BEV) features\nwith different receptive fields to utilize both detailed and global\ninformation. Extensive experiments on both SemanticKITTI dataset and nuScenes\ndataset show that our Panoptic-PHNet surpasses state-of-the-art methods by\nremarkable margins with a real-time speed. We achieve the 1st place on the\npublic leaderboard of SemanticKITTI and leading performance on the recently\nreleased leaderboard of nuScenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaoqiang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SaiNet: Stereo aware inpainting behind objects with generative networks. (arXiv:2205.07014v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07014","description":"<p>In this work, we present an end-to-end network for stereo-consistent image\ninpainting with the objective of inpainting large missing regions behind\nobjects. The proposed model consists of an edge-guided UNet-like network using\nPartial Convolutions. We enforce multi-view stereo consistency by introducing a\ndisparity loss. More importantly, we develop a training scheme where the model\nis learned from realistic stereo masks representing object occlusions, instead\nof the more common random masks. The technique is trained in a supervised way.\nOur evaluation shows competitive results compared to previous state-of-the-art\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_V/0/1/0/all/0/1\">Violeta Men&#xe9;ndez Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillipson_G/0/1/0/all/0/1\">Graeme Phillipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jolly_S/0/1/0/all/0/1\">Stephen Jolly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1\">Simon Hadfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Importance Weighted Structure Learning for Scene Graph Generation. (arXiv:2205.07017v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07017","description":"<p>Scene graph generation is a structured prediction task aiming to explicitly\nmodel objects and their relationships via constructing a visually-grounded\nscene graph for an input image. Currently, the message passing neural network\nbased mean field variational Bayesian methodology is the ubiquitous solution\nfor such a task, in which the variational inference objective is often assumed\nto be the classical evidence lower bound. However, the variational\napproximation inferred from such loose objective generally underestimates the\nunderlying posterior, which often leads to inferior generation performance. In\nthis paper, we propose a novel importance weighted structure learning method\naiming to approximate the underlying log-partition function with a tighter\nimportance weighted lower bound, which is computed from multiple samples drawn\nfrom a reparameterizable Gumbel-Softmax sampler. A generic entropic mirror\ndescent algorithm is applied to solve the resulting constrained variational\ninference task. The proposed method achieves the state-of-the-art performance\non various popular scene graph generation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittler_J/0/1/0/all/0/1\">Josef Kittler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Generalization Ability of Super-Resolution Networks. (arXiv:2205.07019v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07019","description":"<p>Performance and generalization ability are two important aspects to evaluate\ndeep learning models. However, research on the generalization ability of\nSuper-Resolution (SR) networks is currently absent. We make the first attempt\nto propose a Generalization Assessment Index for SR networks, namely SRGA. SRGA\nexploits the statistical characteristics of internal features of deep networks,\nnot output images to measure the generalization ability. Specially, it is a\nnon-parametric and non-learning metric. To better validate our method, we\ncollect a patch-based image evaluation set (PIES) that includes both synthetic\nand real-world images, covering a wide range of degradations. With SRGA and\nPIES dataset, we benchmark existing SR models on the generalization ability.\nThis work could lay the foundation for future research on model generalization\nin low-level vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Assisted Active Learning for Skin Lesion Segmentation. (arXiv:2205.07021v1 [eess.IV])","link":"http://arxiv.org/abs/2205.07021","description":"<p>Label scarcity has been a long-standing issue for biomedical image\nsegmentation, due to high annotation costs and professional requirements.\nRecently, active learning (AL) strategies strive to reduce annotation costs by\nquerying a small portion of data for annotation, receiving much traction in the\nfield of medical imaging. However, most of the existing AL methods have to\ninitialize models with some randomly selected samples followed by active\nselection based on various criteria, such as uncertainty and diversity. Such\nrandom-start initialization methods inevitably introduce under-value redundant\nsamples and unnecessary annotation costs. For the purpose of addressing the\nissue, we propose a novel self-supervised assisted active learning framework in\nthe cold-start setting, in which the segmentation model is first warmed up with\nself-supervised learning (SSL), and then SSL features are used for sample\nselection via latent feature clustering without accessing labels. We assess our\nproposed methodology on skin lesions segmentation task. Extensive experiments\ndemonstrate that our approach is capable of achieving promising performance\nwith substantial improvements over existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Ziyuan Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Wenjing Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kaixin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Veeravalli_B/0/1/0/all/0/1\">Bharadwaj Veeravalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-Aware Self-supervised Multi-Label Learning. (arXiv:2205.07028v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07028","description":"<p>Multi-label Learning on Image data has been widely exploited with deep\nlearning models. However, supervised training on deep CNN models often cannot\ndiscover sufficient discriminative features for classification. As a result,\nnumerous self-supervision methods are proposed to learn more robust image\nrepresentations. However, most self-supervised approaches focus on\nsingle-instance single-label data and fall short on more complex images with\nmultiple objects. Therefore, we propose an Object-Aware Self-Supervision (OASS)\nmethod to obtain more fine-grained representations for multi-label learning,\ndynamically generating auxiliary tasks based on object locations. Secondly, the\nrobust representation learned by OASS can be leveraged to efficiently generate\nClass-Specific Instances (CSI) in a proposal-free fashion to better guide\nmulti-label supervision signal transfer to instances. Extensive experiments on\nthe VOC2012 dataset for multi-label classification demonstrate the\neffectiveness of the proposed method against the state-of-the-art counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaixin_X/0/1/0/all/0/1\">Xu Kaixin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liyang_L/0/1/0/all/0/1\">Liu Liyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziyuan_Z/0/1/0/all/0/1\">Zhao Ziyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veeravalli_B/0/1/0/all/0/1\">Bharadwaj Veeravalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Defocus Blur for Multiplane Computer-Generated Holography. (arXiv:2205.07030v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07030","description":"<p>This paper introduces a new multiplane CGH computation method to reconstruct\nartefact-free high-quality holograms with natural-looking defocus blur. Our\nmethod introduces a new targeting scheme and a new loss function. While the\ntargeting scheme accounts for defocused parts of the scene at each depth plane,\nthe new loss function analyzes focused and defocused parts separately in\nreconstructed images. Our method support phase-only CGH calculations using\nvarious iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative\n(e.g., Double Phase) CGH techniques. We achieve our best image quality using a\nmodified gradient descent-based optimization recipe where we introduce a\nconstraint inspired by the double phase method. We validate our method\nexperimentally using our proof-of-concept holographic display, comparing\nvarious algorithms, including multi-depth scenes with sparse and dense\ncontents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kavakli_K/0/1/0/all/0/1\">Koray Kavakl&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itoh_Y/0/1/0/all/0/1\">Yuta Itoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urey_H/0/1/0/all/0/1\">Hakan Urey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksit_K/0/1/0/all/0/1\">Kaan Ak&#x15f;it</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Scale Gate for Semantic Segmentation. (arXiv:2205.07056v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07056","description":"<p>Effectively encoding multi-scale contextual information is crucial for\naccurate semantic segmentation. Existing transformer-based segmentation models\ncombine features across scales without any selection, where features on\nsub-optimal scales may degrade segmentation outcomes. Leveraging from the\ninherent properties of Vision Transformers, we propose a simple yet effective\nmodule, Transformer Scale Gate (TSG), to optimally combine multi-scale\nfeatures.TSG exploits cues in self and cross attentions in Vision Transformers\nfor the scale selection. TSG is a highly flexible plug-and-play module, and can\neasily be incorporated with any encoder-decoder-based hierarchical vision\nTransformer architecture. Extensive experiments on the Pascal Context and\nADE20K datasets demonstrate that our feature selection strategy achieves\nconsistent gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hengcan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis. (arXiv:2205.07058v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07058","description":"<p>We present a large-scale synthetic dataset for novel view synthesis\nconsisting of ~300k images rendered from nearly 2000 complex scenes using\nhigh-quality ray tracing at high resolution (1600 x 1600 pixels). The dataset\nis orders of magnitude larger than existing synthetic datasets for novel view\nsynthesis, thus providing a large unified benchmark for both training and\nevaluation. Using 4 distinct sources of high-quality 3D meshes, the scenes of\nour dataset exhibit challenging variations in camera views, lighting, shape,\nmaterials, and textures. Because our dataset is too large for existing methods\nto process, we propose Sparse Voxel Light Field (SVLF), an efficient\nvoxel-based light field approach for novel view synthesis that achieves\ncomparable performance to NeRF on synthetic data, while being an order of\nmagnitude faster to train and two orders of magnitude faster to render. SVLF\nachieves this speed by relying on a sparse voxel octree, careful voxel sampling\n(requiring only a handful of queries per ray), and reduced network structure;\nas well as ground truth depth maps at training time. Our dataset is generated\nby NViSII, a Python-based ray tracing renderer, which is designed to be simple\nfor non-experts to use and share, flexible and powerful through its use of\nscripting, and able to create high-quality and physically-based rendered\nimages. Experiments with a subset of our dataset allow us to compare standard\nmethods like NeRF and mip-NeRF for single-scene modeling, and pixelNeRF for\ncategory-level modeling, pointing toward the need for future improvements in\nthis area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1\">Jonathan Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meshry_M/0/1/0/all/0/1\">Moustafa Meshry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_A/0/1/0/all/0/1\">Alex Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_A/0/1/0/all/0/1\">Alexander Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khamis_S/0/1/0/all/0/1\">Sameh Khamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loop_C/0/1/0/all/0/1\">Charles Loop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrical_N/0/1/0/all/0/1\">Nathan Morrical</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagano_K/0/1/0/all/0/1\">Koki Nagano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takikawa_T/0/1/0/all/0/1\">Towaki Takikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1\">Stan Birchfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unifying Multi-sampling-ratio CS-MRI Framework With Two-grid-cycle Correction and Geometric Prior Distillation. (arXiv:2205.07062v1 [eess.IV])","link":"http://arxiv.org/abs/2205.07062","description":"<p>CS is an efficient method to accelerate the acquisition of MR images from\nunder-sampled k-space data. Although existing deep learning CS-MRI methods have\nachieved considerably impressive performance, explainability and\ngeneralizability continue to be challenging for such methods since most of them\nare not flexible enough to handle multi-sampling-ratio reconstruction\nassignments, often the transition from mathematical analysis to network design\nnot always natural enough. In this work, to tackle explainability and\ngeneralizability, we propose a unifying deep unfolding multi-sampling-ratio\nCS-MRI framework, by merging advantages of model-based and deep learning-based\nmethods. The combined approach offers more generalizability than previous works\nwhereas deep learning gains explainability through a geometric prior module.\nInspired by multigrid algorithm, we first embed the CS-MRI-based optimization\nalgorithm into correction-distillation scheme that consists of three\ningredients: pre-relaxation module, correction module and geometric prior\ndistillation module. Furthermore, we employ a condition module to learn\nadaptively step-length and noise level from compressive sampling ratio in every\nstage, which enables the proposed framework to jointly train multi-ratio tasks\nthrough a single model. The proposed model can not only compensate the lost\ncontextual information of reconstructed image which is refined from low\nfrequency error in geometric characteristic k-space, but also integrate the\ntheoretical guarantee of model-based methods and the superior reconstruction\nperformances of deep learning-based methods. All physical-model parameters are\nlearnable, and numerical experiments show that our framework outperforms\nstate-of-the-art methods in terms of qualitative and quantitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1\">Xiaohong Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Ke Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_K/0/1/0/all/0/1\">Ke Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Architecture for the detection of GAN-generated Flood Images with Localization Capabilities. (arXiv:2205.07073v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07073","description":"<p>In this paper, we address a new image forensics task, namely the detection of\nfake flood images generated by ClimateGAN architecture. We do so by proposing a\nhybrid deep learning architecture including both a detection and a localization\nbranch, the latter being devoted to the identification of the image regions\nmanipulated by ClimateGAN. Even if our goal is the detection of fake flood\nimages, in fact, we found that adding a localization branch helps the network\nto focus on the most relevant image regions with significant improvements in\nterms of generalization capabilities and robustness against image processing\noperations. The good performance of the proposed architecture is validated on\ntwo datasets of pristine flood images downloaded from the internet and three\ndatasets of fake flood images generated by ClimateGAN starting from a large set\nof diverse street images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamayreh_O/0/1/0/all/0/1\">Omran Alamayreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1\">Benedetta Tondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1\">Mauro Barni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corrosion Detection for Industrial Objects: From Multi-Sensor System to 5D Feature Space. (arXiv:2205.07075v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07075","description":"<p>Corrosion is a form of damage that often appears on the surface of metal-made\nobjects used in industrial applications. Those damages can be critical\ndepending on the purpose of the used object. Optical-based testing systems\nprovide a form of non-contact data acquisition, where the acquired data can\nthen be used to analyse the surface of an object. In the field of industrial\nimage processing, this is called surface inspection. We provide a testing setup\nconsisting of a rotary table which rotates the object by 360 degrees, as well\nas industrial RGB cameras and laser triangulation sensors for the acquisition\nof 2D and 3D data as our multi-sensor system. These sensors acquire data while\nthe object to be tested takes a full rotation. Further on, data augmentation is\napplied to prepare new data or enhance already acquired data. In order to\nevaluate the impact of a laser triangulation sensor for corrosion detection,\none challenge is to at first fuse the data of both domains. After the data\nfusion process, 5 different channels can be utilized to create a 5D feature\nspace. Besides the red, green and blue channels of the image (1-3), additional\nrange data from the laser triangulation sensor is incorporated (4). As a fifth\nchannel, said sensor provides additional intensity data (5). With a\nmulti-channel image classification, a 5D feature space will lead to slightly\nsuperior results opposed to a 3D feature space, composed of only the RGB\nchannels of the image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haitz_D/0/1/0/all/0/1\">Dennis Haitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jutzi_B/0/1/0/all/0/1\">Boris Jutzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huebner_P/0/1/0/all/0/1\">Patrick Huebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_M/0/1/0/all/0/1\">Markus Ulrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spiking Approximations of the MaxPooling Operation in Deep SNNs. (arXiv:2205.07076v1 [cs.NE])","link":"http://arxiv.org/abs/2205.07076","description":"<p>Spiking Neural Networks (SNNs) are an emerging domain of biologically\ninspired neural networks that have shown promise for low-power AI. A number of\nmethods exist for building deep SNNs, with Artificial Neural Network\n(ANN)-to-SNN conversion being highly successful. MaxPooling layers in\nConvolutional Neural Networks (CNNs) are an integral component to downsample\nthe intermediate feature maps and introduce translational invariance, but the\nabsence of their hardware-friendly spiking equivalents limits such CNNs'\nconversion to deep SNNs. In this paper, we present two hardware-friendly\nmethods to implement Max-Pooling in deep SNNs, thus facilitating easy\nconversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute\nSNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware\n(with MNIST, FMNIST, &amp; CIFAR10 dataset); thus, showing the feasibility of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaurav_R/0/1/0/all/0/1\">Ramashish Gaurav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripp_B/0/1/0/all/0/1\">Bryan Tripp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1\">Apurva Narayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging. (arXiv:2205.07085v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07085","description":"<p>Modern data-driven machine learning research that enables revolutionary\nadvances in image analysis has now become a critical tool to redefine how skin\nlesions are documented, mapped, and tracked. We propose a 3D whole body imaging\nprototype to enable rapid evaluation and mapping of skin lesions. A modular\ncamera rig arranged in a cylindrical configuration is designed to automatically\ncapture synchronised images from multiple angles for entire body scanning. We\ndevelop algorithms for 3D body image reconstruction, data processing and skin\nlesion detection based on deep convolutional neural networks. We also propose a\ncustomised, intuitive and flexible interface that allows the user to interact\nand collaborate with the machine to understand the data. The hybrid of the\nhuman and computer is represented by the analysis of 2D lesion detection, 3D\nmapping and data management. The experimental results using synthetic and real\nimages demonstrate the effectiveness of the proposed solution by providing\nmultiple views of the target skin lesion, enabling further 3D geometry\nanalysis. Skin lesions are identified as outliers which deserve more attention\nfrom a skin cancer physician. Our detector identifies lesions at a comparable\nperformance level as a physician. The proposed 3D whole body imaging system can\nbe used by dermatological clinics, allowing for fast documentation of lesions,\nquick and accurate analysis of the entire body to detect suspicious lesions.\nBecause of its fast examination, the method might be used for screening or\nepidemiological investigations. 3D data analysis has the potential to change\nthe paradigm of total-body photography with many applications in skin diseases,\nincluding inflammatory and pigmentary disorders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tychsen_Smith_L/0/1/0/all/0/1\">Lachlan Tychsen-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stacey_A/0/1/0/all/0/1\">Ashley Stacey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenghong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathikulangara_J/0/1/0/all/0/1\">Joseph Pathikulangara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dadong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal curb detection and filtering. (arXiv:2205.07096v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07096","description":"<p>Reliable knowledge of road boundaries is critical for autonomous vehicle\nnavigation. We propose a robust curb detection and filtering technique based on\nthe fusion of camera semantics and dense lidar point clouds. The lidar point\nclouds are collected by fusing multiple lidars for robust feature detection.\nThe camera semantics are based on a modified EfficientNet architecture which is\ntrained with labeled data collected from onboard fisheye cameras. The point\nclouds are associated with the closest curb segment with $L_2$-norm analysis\nafter projecting into the image space with the fisheye model projection. Next,\nthe selected points are clustered using unsupervised density-based spatial\nclustering to detect different curb regions. As new curb points are detected in\nconsecutive frames they are associated with the existing curb clusters using\ntemporal reachability constraints. If no reachability constraints are found a\nnew curb cluster is formed from these new points. This ensures we can detect\nmultiple curbs present in road segments consisting of multiple lanes if they\nare in the sensors' field of view. Finally, Delaunay filtering is applied for\noutlier removal and its performance is compared to traditional RANSAC-based\nfiltering. An objective evaluation of the proposed solution is done using a\nhigh-definition map containing ground truth curb points obtained from a\ncommercial map supplier. The proposed system has proven capable of detecting\ncurbs of any orientation in complex urban road scenarios comprising straight\nroads, curved roads, and intersections with traffic isles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sandipan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahabadi_N/0/1/0/all/0/1\">Navid Mahabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Saikat Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1\">Maurice Fallon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable SAR Renderer and SAR Target Reconstruction. (arXiv:2205.07099v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07099","description":"<p>Forward modeling of wave scattering and radar imaging mechanisms is the key\nto information extraction from synthetic aperture radar (SAR) images. Like\ninverse graphics in optical domain, an inherently-integrated forward-inverse\napproach would be promising for SAR advanced information retrieval and target\nreconstruction. This paper presents such an attempt to the inverse graphics for\nSAR imagery. A differentiable SAR renderer (DSR) is developed which\nreformulates the mapping and projection algorithm of SAR imaging mechanism in\nthe differentiable form of probability maps. First-order gradients of the\nproposed DSR are then analytically derived which can be back-propagated from\nrendered image/silhouette to the target geometry and scattering attributes. A\n3D inverse target reconstruction algorithm from SAR images is devised. Several\nsimulation and reconstruction experiments are conducted, including targets with\nand without background, using both synthesized data or real measured inverse\nSAR (ISAR) data by ground radar. Results demonstrate the efficacy of the\nproposed DSR and its inverse approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Shilei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Deep Learning Methods for Identification of Defective Casting Products. (arXiv:2205.07118v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07118","description":"<p>Quality inspection has become crucial in any large-scale manufacturing\nindustry recently. In order to reduce human error, it has become imperative to\nuse efficient and low computational AI algorithms to identify such defective\nproducts. In this paper, we have compared and contrasted various pre-trained\nand custom-built architectures using model size, performance and CPU latency in\nthe detection of defective casting products. Our results show that custom\narchitectures are efficient than pre-trained mobile architectures. Moreover,\ncustom models perform 6 to 9 times faster than lightweight models such as\nMobileNetV2 and NasNet. The number of training parameters and the model size of\nthe custom architectures is significantly lower (~386 times &amp; ~119 times\nrespectively) than the best performing models such as MobileNetV2 and NasNet.\nAugmentation experimentations have also been carried out on the custom\narchitectures to make the models more robust and generalizable. Our work sheds\nlight on the efficiency of these custom-built architectures for deployment on\nEdge and IoT devices and that transfer learning models may not always be ideal.\nInstead, they should be specific to the kind of dataset and the classification\nproblem at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bolla_B/0/1/0/all/0/1\">Bharath Kumar Bolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingam_M/0/1/0/all/0/1\">Mohan Kingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethiraj_S/0/1/0/all/0/1\">Sabeesh Ethiraj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Facial Key Point Detection: An Efficient Approach Using Deep Neural Networks. (arXiv:2205.07121v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07121","description":"<p>Facial landmark detection is a widely researched field of deep learning as\nthis has a wide range of applications in many fields. These key points are\ndistinguishing characteristic points on the face, such as the eyes center, the\neye's inner and outer corners, the mouth center, and the nose tip from which\nhuman emotions and intent can be explained. The focus of our work has been\nevaluating transfer learning models such as MobileNetV2 and NasNetMobile,\nincluding custom CNN architectures. The objective of the research has been to\ndevelop efficient deep learning models in terms of model size, parameters, and\ninference time and to study the effect of augmentation imputation and\nfine-tuning on these models. It was found that while augmentation techniques\nproduced lower RMSE scores than imputation techniques, they did not affect the\ninference time. MobileNetV2 architecture produced the lowest RMSE and inference\ntime. Moreover, our results indicate that manually optimized CNN architectures\nperformed similarly to Auto Keras tuned architecture. However, manually\noptimized architectures yielded better inference time and training curves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dileep_P/0/1/0/all/0/1\">Prathima Dileep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolla_B/0/1/0/all/0/1\">Bharath Kumar Bolla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethiraj_S/0/1/0/all/0/1\">Sabeesh Ethiraj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Astronomical Bodies by Efficient Layer Fine-Tuning of Deep Neural Networks. (arXiv:2205.07124v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07124","description":"<p>The SDSS-IV dataset contains information about various astronomical bodies\nsuch as Galaxies, Stars, and Quasars captured by observatories. Inspired by our\nwork on deep multimodal learning, which utilized transfer learning to classify\nthe SDSS-IV dataset, we further extended our research in the fine tuning of\nthese architectures to study the effect in the classification scenario.\nArchitectures such as Resnet-50, DenseNet-121 VGG-16, Xception, EfficientNetB2,\nMobileNetV2 and NasnetMobile have been built using layer wise fine tuning at\ndifferent levels. Our findings suggest that freezing all layers with Imagenet\nweights and adding a final trainable layer may not be the optimal solution.\nFurther, baseline models and models that have higher number of trainable layers\nperformed similarly in certain architectures. Model need to be fine tuned at\ndifferent levels and a specific training ratio is required for a model to be\ntermed ideal. Different architectures had different responses to the change in\nthe number of trainable layers w.r.t accuracies. While models such as\nDenseNet-121, Xception, EfficientNetB2 achieved peak accuracies that were\nrelatively consistent with near perfect training curves, models such as\nResnet-50,VGG-16, MobileNetV2 and NasnetMobile had lower, delayed peak\naccuracies with poorly fitting training curves. It was also found that though\nmobile neural networks have lesser parameters and model size, they may not\nalways be ideal for deployment on a low computational device as they had\nconsistently lower validation accuracies. Customized evaluation metrics such as\nTuning Parameter Ratio and Tuning Layer Ratio are used for model evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ethiraj_S/0/1/0/all/0/1\">Sabeesh Ethiraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolla_B/0/1/0/all/0/1\">Bharath Kumar Bolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ETAD: A Unified Framework for Efficient Temporal Action Detection. (arXiv:2205.07134v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07134","description":"<p>Untrimmed video understanding such as temporal action detection (TAD) often\nsuffers from the pain of huge demand for computing resources. Because of long\nvideo durations and limited GPU memory, most action detectors can only operate\non pre-extracted features rather than the original videos, and they still\nrequire a lot of computation to achieve high detection performance. To\nalleviate the heavy computation problem in TAD, in this work, we first propose\nan efficient action detector with detector proposal sampling, based on the\nobservation that performance saturates at a small number of proposals. This\ndetector is designed with several important techniques, such as LSTM-boosted\ntemporal aggregation and cascaded proposal refinement to achieve high detection\nquality as well as low computational cost. To enable joint optimization of this\naction detector and the feature encoder, we also propose encoder gradient\nsampling, which selectively back-propagates through video snippets and\ntremendously reduces GPU memory consumption. With the two sampling strategies\nand the effective detector, we build a unified framework for efficient\nend-to-end temporal action detection (ETAD), making real-world untrimmed video\nunderstanding tractable. ETAD achieves state-of-the-art performance on both\nTHUMOS-14 and ActivityNet-1.3. Interestingly, on ActivityNet-1.3, it reaches\n37.78% average mAP, while only requiring 6 mins of training time and 1.23 GB\nmemory based on pre-extracted features. With end-to-end training, it reduces\nthe GPU memory footprint by more than 70% with even higher performance (38.21%\naverage mAP), as compared with traditional end-to-end methods. The code is\navailable at https://github.com/sming256/ETAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training. (arXiv:2205.07139v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07139","description":"<p>When reading images, radiologists generate text reports describing the\nfindings therein. Current state-of-the-art computer-aided diagnosis tools\nutilize a fixed set of predefined categories automatically extracted from these\nmedical reports for training. This form of supervision limits the potential\nusage of models as they are unable to pick up on anomalies outside of their\npredefined set, thus, making it a necessity to retrain the classifier with\nadditional data when faced with novel classes. In contrast, we investigate\ndirect text supervision to break away from this closed set assumption. By doing\nso, we avoid noisy label extraction via text classifiers and incorporate more\ncontextual information.\n</p>\n<p>We employ a contrastive global-local dual-encoder architecture to learn\nconcepts directly from unstructured medical reports while maintaining its\nability to perform free form classification.\n</p>\n<p>We investigate relevant properties of open set recognition for radiological\ndata and propose a method to employ currently weakly annotated data into\ntraining.\n</p>\n<p>We evaluate our approach on the large-scale chest X-Ray datasets MIMIC-CXR,\nCheXpert, and ChestX-Ray14 for disease classification. We show that despite\nusing unstructured medical report supervision, we perform on par with direct\nlabel supervision through a sophisticated inference setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiss_S/0/1/0/all/0/1\">Simon Rei&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfraz_M/0/1/0/all/0/1\">M. Saquib Sarfraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Uncertainty Calibration for Open-Set Recognition. (arXiv:2205.07160v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07160","description":"<p>Despite achieving enormous success in predictive accuracy for visual\nclassification problems, deep neural networks (DNNs) suffer from providing\noverconfident probabilities on out-of-distribution (OOD) data. Yet, accurate\nuncertainty estimation is crucial for safe and reliable robot autonomy. In this\npaper, we evaluate popular calibration techniques for open-set conditions in a\nway that is distinctly different from the conventional evaluation of\ncalibration methods on OOD data. Our results show that closed-set DNN\ncalibration approaches are much less effective for open-set recognition, which\nhighlights the need to develop new DNN calibration methods to address this\nproblem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zongyao Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_N/0/1/0/all/0/1\">Nolan B. Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beksi_W/0/1/0/all/0/1\">William J. Beksi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLaMa: Joint Spatial and Frequency Loss for General Image Inpainting. (arXiv:2205.07162v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07162","description":"<p>The purpose of image inpainting is to recover scratches and damaged areas\nusing context information from remaining parts. In recent years, thanks to the\nresurgence of convolutional neural networks (CNNs), image inpainting task has\nmade great breakthroughs. However, most of the work consider insufficient types\nof mask, and their performance will drop dramatically when encountering unseen\nmasks. To combat these challenges, we propose a simple yet general method to\nsolve this problem based on the LaMa image inpainting framework, dubbed GLaMa.\nOur proposed GLaMa can better capture different types of missing information by\nusing more types of masks. By incorporating more degraded images in the\ntraining phase, we can expect to enhance the robustness of the model with\nrespect to various masks. In order to yield more reasonable results, we further\nintroduce a frequency-based loss in addition to the traditional spatial\nreconstruction loss and adversarial loss. In particular, we introduce an\neffective reconstruction loss both in the spatial and frequency domain to\nreduce the chessboard effect and ripples in the reconstructed image. Extensive\nexperiments demonstrate that our method can boost the performance over the\noriginal LaMa method for each type of mask on FFHQ, ImageNet, Places2 and\nWikiArt dataset. The proposed GLaMa was ranked first in terms of PSNR, LPIPS\nand SSIM in the NTIRE 2022 Image Inpainting Challenge Track 1 Unsupervised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zeyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proxyless Neural Architecture Adaptation for Supervised Learning and Self-Supervised Learning. (arXiv:2205.07168v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07168","description":"<p>Recently, Neural Architecture Search (NAS) methods have been introduced and\nshow impressive performance on many benchmarks. Among those NAS studies, Neural\nArchitecture Transformer (NAT) aims to adapt the given neural architecture to\nimprove performance while maintaining computational costs. However, NAT lacks\nreproducibility and it requires an additional architecture adaptation process\nbefore network weight training. In this paper, we propose proxyless neural\narchitecture adaptation that is reproducible and efficient. Our method can be\napplied to both supervised learning and self-supervised learning. The proposed\nmethod shows stable performance on various architectures. Extensive\nreproducibility experiments on two datasets, i.e., CIFAR-10 and Tiny Imagenet,\npresent that the proposed method definitely outperforms NAT and is applicable\nto other models and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Do-Guk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heung-Chang Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection. (arXiv:2205.07179v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07179","description":"<p>Growing interests in RGB-D salient object detection (RGB-D SOD) have been\nwitnessed in recent years, owing partly to the popularity of depth sensors and\nthe rapid progress of deep learning techniques. Unfortunately, existing RGB-D\nSOD methods typically demand large quantity of training images being thoroughly\nannotated at pixel-level. The laborious and time-consuming manual annotation\nhas become a real bottleneck in various practical scenarios. On the other hand,\ncurrent unsupervised RGB-D SOD methods still heavily rely on handcrafted\nfeature representations. This inspires us to propose in this paper a deep\nunsupervised RGB-D saliency detection approach, which requires no manual\npixel-level annotation during training. It is realized by two key ingredients\nin our training pipeline. First, a depth-disentangled saliency update (DSU)\nframework is designed to automatically produce pseudo-labels with iterative\nfollow-up refinements, which provides more trustworthy supervision signals for\ntraining the saliency network. Second, an attentive training strategy is\nintroduced to tackle the issue of noisy pseudo-labels, by properly re-weighting\nto highlight the more reliable pseudo-labels. Extensive experiments demonstrate\nthe superior efficiency and effectiveness of our approach in tackling the\nchallenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also\nbe adapted to work in fully-supervised situation. Empirical studies show the\nincorporation of our approach gives rise to notably performance improvement in\nexisting supervised RGB-D SOD models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Q/0/1/0/all/0/1\">Qi Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT. (arXiv:2205.07180v1 [eess.AS])","link":"http://arxiv.org/abs/2205.07180","description":"<p>This paper investigates self-supervised pre-training for audio-visual speaker\nrepresentation learning where a visual stream showing the speaker's mouth area\nis used alongside speech as inputs. Our study focuses on the Audio-Visual\nHidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose\naudio-visual speech pre-training framework. We conducted extensive experiments\nprobing the effectiveness of pre-training and visual modality. Experimental\nresults suggest that AV-HuBERT generalizes decently to speaker related\ndownstream tasks, improving label efficiency by roughly ten fold for both\naudio-only and audio-visual speaker verification. We also show that\nincorporating visual information, even just the lip area, greatly improves the\nperformance and noise robustness, reducing EER by 38% in the clean condition\nand 75% in noisy conditions. Our code and models will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonconvex ${{L_ {{1/2}}}} $-Regularized Nonlocal Self-similarity Denoiser for Compressive Sensing based CT Reconstruction. (arXiv:2205.07185v1 [eess.IV])","link":"http://arxiv.org/abs/2205.07185","description":"<p>Compressive sensing (CS) based computed tomography (CT) image reconstruction\naims at reducing the radiation risk through sparse-view projection data. It is\nusually challenging to achieve satisfying image quality from incomplete\nprojections. Recently, the nonconvex ${{L_ {{1/2}}}} $-norm has achieved\npromising performance in sparse recovery, while the applications on imaging are\nunsatisfactory due to its nonconvexity. In this paper, we develop a ${{L_\n{{1/2}}}} $-regularized nonlocal self-similarity (NSS) denoiser for CT\nreconstruction problem, which integrates low-rank approximation with group\nsparse coding (GSC) framework. Concretely, we first split the CT reconstruction\nproblem into two subproblems, and then improve the CT image quality furtherly\nusing our ${{L_ {{1/2}}}} $-regularized NSS denoiser. Instead of optimizing the\nnonconvex problem under the perspective of GSC, we particularly reconstruct CT\nimage via low-rank minimization based on two simple yet essential schemes,\nwhich build the equivalent relationship between GSC based denoiser and low-rank\nminimization. Furtherly, the weighted singular value thresholding (WSVT)\noperator is utilized to optimize the resulting nonconvex ${{L_ {{1/2}}}} $\nminimization problem. Following this, our proposed denoiser is integrated with\nthe CT reconstruction problem by alternating direction method of multipliers\n(ADMM) framework. Extensive experimental results on typical clinical CT images\nhave demonstrated that our approach can further achieve better performance than\npopular approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yunyi Li</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqiu Jiang</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hengmin Zhang</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jianxun Liu</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xiangling Ding</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Gui_G/0/1/0/all/0/1\">Guan Gui</a> (4) ((1) School of Computer Science and Engineering, Hunan University of Science and Technology (2) Department of Sports Medicine and Joint Surgery, Nanjing First Hospital, Nanjing Medical University (3) Department of Computer and Information Science, University of Macau (4) College of Telecommunications and Information Engineering, Nanjing University of Posts and Telecommunications)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-centric Consistency Learning for Deepfake Detection. (arXiv:2205.07201v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07201","description":"<p>Most of previous deepfake detection researches bent their efforts to describe\nand discriminate artifacts in human perceptible ways, which leave a bias in the\nlearned networks of ignoring some critical invariance features intra-class and\nunderperforming the robustness of internet interference. Essentially, the\ntarget of deepfake detection problem is to represent natural faces and fake\nfaces at the representation space discriminatively, and it reminds us whether\nwe could optimize the feature extraction procedure at the representation space\nthrough constraining intra-class consistence and inter-class inconsistence to\nbring the intra-class representations close and push the inter-class\nrepresentations apart? Therefore, inspired by contrastive representation\nlearning, we tackle the deepfake detection problem through learning the\ninvariant representations of both classes and propose a novel real-centric\nconsistency learning method. We constraint the representation from both the\nsample level and the feature level. At the sample level, we take the procedure\nof deepfake synthesis into consideration and propose a novel forgery\nsemantical-based pairing strategy to mine latent generation-related features.\nAt the feature level, based on the centers of natural faces at the\nrepresentation space, we design a hard positive mining and synthesizing method\nto simulate the potential marginal features. Besides, a hard negative fusion\nmethod is designed to improve the discrimination of negative marginal features\nwith the help of supervised contrastive margin loss we developed. The\neffectiveness and robustness of the proposed method has been demonstrated\nthrough extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_R/0/1/0/all/0/1\">Ruiqi Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhichao Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qianmu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Siqi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fused Deep Neural Network based Transfer Learning in Occluded Face Classification and Person re-Identification. (arXiv:2205.07203v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07203","description":"<p>Recent period of pandemic has brought person identification even with\noccluded face image a great importance with increased number of mask usage.\nThis paper aims to recognize the occlusion of one of four types in face images.\nVarious transfer learning methods were tested, and the results show that\nMobileNet V2 with Gated Recurrent Unit(GRU) performs better than any other\nTransfer Learning methods, with a perfect accuracy of 99% in classification of\nimages as with or without occlusion and if with occlusion, then the type of\nocclusion. In parallel, identifying the Region of interest from the device\ncaptured image is done. This extracted Region of interest is utilised in face\nidentification. Such a face identification process is done using the ResNet\nmodel with its Caffe implementation. To reduce the execution time, after the\nface occlusion type was recognized the person was searched to confirm their\nface image in the registered database. The face label of the person obtained\nfrom both simultaneous processes was verified for their matching score. If the\nmatching score was above 90, the recognized label of the person was logged into\na file with their name, type of mask, date, and time of recognition.\nMobileNetV2 is a lightweight framework which can also be used in embedded or\nIoT devices to perform real time detection and identification in suspicious\nareas of investigations using CCTV footages. When MobileNetV2 was combined with\nGRU, a reliable accuracy was obtained. The data provided in the paper belong to\ntwo categories, being either collected from Google Images for occlusion\nclassification, face recognition, and facial landmarks, or collected in\nfieldwork. The motive behind this research is to identify and log person\ndetails which could serve surveillance activities in society-based\ne-governance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohana_M/0/1/0/all/0/1\">Mohamed Mohana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+B_P/0/1/0/all/0/1\">Prasanalakshmi B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alelyani_S/0/1/0/all/0/1\">Salem Alelyani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alsaqer_M/0/1/0/all/0/1\">Mohammed Saleh Alsaqer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Frame Interpolation with Transformer. (arXiv:2205.07230v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07230","description":"<p>Video frame interpolation (VFI), which aims to synthesize intermediate frames\nof a video, has made remarkable progress with development of deep convolutional\nnetworks over past years. Existing methods built upon convolutional networks\ngenerally face challenges of handling large motion due to the locality of\nconvolution operations. To overcome this limitation, we introduce a novel\nframework, which takes advantage of Transformer to model long-range pixel\ncorrelation among video frames. Further, our network is equipped with a novel\ncross-scale window-based attention mechanism, where cross-scale windows\ninteract with each other. This design effectively enlarges the receptive field\nand aggregates multi-scale information. Extensive quantitative and qualitative\nexperiments demonstrate that our method achieves new state-of-the-art results\non various benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruizheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huaijia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangbo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combating COVID-19 using Generative Adversarial Networks and Artificial Intelligence for Medical Images: A Scoping Review. (arXiv:2205.07236v1 [eess.IV])","link":"http://arxiv.org/abs/2205.07236","description":"<p>This review presents a comprehensive study on the role of GANs in addressing\nthe challenges related to COVID-19 data scarcity and diagnosis. It is the first\nreview that summarizes the different GANs methods and the lungs images datasets\nfor COVID-19. It attempts to answer the questions related to applications of\nGANs, popular GAN architectures, frequently used image modalities, and the\navailability of source code. This review included 57 full-text studies that\nreported the use of GANs for different applications in COVID-19 lungs images\ndata. Most of the studies (n=42) used GANs for data augmentation to enhance the\nperformance of AI techniques for COVID-19 diagnosis. Other popular applications\nof GANs were segmentation of lungs and super-resolution of the lungs images.\nThe cycleGAN and the conditional GAN were the most commonly used architectures\nused in nine studies each. 29 studies used chest X-Ray images while 21 studies\nused CT images for the training of GANs. For majority of the studies (n=47),\nthe experiments were done and results were reported using publicly available\ndata. A secondary evaluation of the results by radiologists/clinicians was\nreported by only two studies. Conclusion: Studies have shown that GANs have\ngreat potential to address the data scarcity challenge for lungs images of\nCOVID-19. Data synthesized with GANs have been helpful to improve the training\nof the Convolutional Neural Network (CNN) models trained for the diagnosis of\nCOVID-19. Besides, GANs have also contributed to enhancing the CNNs performance\nthrough the super-resolution of the images and segmentation. This review also\nidentified key limitations of the potential transformation of GANs based\nmethods in clinical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ali_H/0/1/0/all/0/1\">Hazrat Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_Z/0/1/0/all/0/1\">Zubair Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning. (arXiv:2205.07246v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07246","description":"<p>Pseudo labeling and consistency regularization approaches with\nconfidence-based thresholding have made great progress in semi-supervised\nlearning (SSL). In this paper, we theoretically and empirically analyze the\nrelationship between the unlabeled data distribution and the desirable\nconfidence threshold. Our analysis shows that previous methods might fail to\ndefine favorable threshold since they either require a pre-defined / fixed\nthreshold or an ad-hoc threshold adjusting scheme that does not reflect the\nlearning effect well, resulting in inferior performance and slow convergence,\nespecially for complicated unlabeled data distributions. We hence propose\n\\emph{FreeMatch} to define and adjust the confidence threshold in a\nself-adaptive manner according to the model's learning status. To handle\ncomplicated unlabeled data distributions more effectively, we further propose a\nself-adaptive class fairness regularization method that encourages the model to\nproduce diverse predictions during training. Extensive experimental results\nindicate the superiority of FreeMatch especially when the labeled data are\nextremely rare. FreeMatch achieves \\textbf{5.78}\\%, \\textbf{13.59}\\%, and\n\\textbf{1.28}\\% error rate reduction over the latest state-of-the-art method\nFlexMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class,\nand ImageNet with 100k labels respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_Q/0/1/0/all/0/1\">Qiang Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guidelines for the Regularization of Gammas in Batch Normalization for Deep Residual Networks. (arXiv:2205.07260v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07260","description":"<p>L2 regularization for weights in neural networks is widely used as a standard\ntraining trick. However, L2 regularization for gamma, a trainable parameter of\nbatch normalization, remains an undiscussed mystery and is applied in different\nways depending on the library and practitioner. In this paper, we study whether\nL2 regularization for gamma is valid. To explore this issue, we consider two\napproaches: 1) variance control to make the residual network behave like\nidentity mapping and 2) stable optimization through the improvement of\neffective learning rate. Through two analyses, we specify the desirable and\nundesirable gamma to apply L2 regularization and propose four guidelines for\nmanaging them. In several experiments, we observed the increase and decrease in\nperformance caused by applying L2 regularization to gamma of four categories,\nwhich is consistent with our four guidelines. Our proposed guidelines were\nvalidated through various tasks and architectures, including variants of\nresidual networks and transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Bum Jun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyeyeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyeonah Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong Gu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1\">Wonseok Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sang Woo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regulating Facial Processing Technologies: Tensions Between Legal and Technical Considerations in the Application of Illinois BIPA. (arXiv:2205.07299v1 [cs.CY])","link":"http://arxiv.org/abs/2205.07299","description":"<p>Harms resulting from the development and deployment of facial processing\ntechnologies (FPT) have been met with increasing controversy. Several states\nand cities in the U.S. have banned the use of facial recognition by law\nenforcement and governments, but FPT are still being developed and used in a\nwide variety of contexts where they primarily are regulated by state biometric\ninformation privacy laws. Among these laws, the 2008 Illinois Biometric\nInformation Privacy Act (BIPA) has generated a significant amount of\nlitigation. Yet, with most BIPA lawsuits reaching settlements before there have\nbeen meaningful clarifications of relevant technical intricacies and legal\ndefinitions, there remains a great degree of uncertainty as to how exactly this\nlaw applies to FPT. What we have found through applications of BIPA in FPT\nlitigation so far, however, points to potential disconnects between technical\nand legal communities. This paper analyzes what we know based on BIPA court\nproceedings and highlights these points of tension: areas where the technical\noperationalization of BIPA may create unintended and undesirable incentives for\nFPT development, as well as areas where BIPA litigation can bring to light the\nlimitations of solely technical methods in achieving legal privacy values.\nThese factors are relevant for (i) reasoning about biometric information\nprivacy laws as a governing mechanism for FPT, (ii) assessing the potential\nharms of FPT, and (iii) providing incentives for the mitigation of these harms.\nBy illuminating these considerations, we hope to empower courts and lawmakers\nto take a more nuanced approach to regulating FPT and developers to better\nunderstand privacy values in the current U.S. legal landscape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yew_R/0/1/0/all/0/1\">Rui-Jie Yew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1\">Alice Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Vector Graphics Generation for Music Cover Images. (arXiv:2205.07301v1 [cs.GR])","link":"http://arxiv.org/abs/2205.07301","description":"<p>Generative Adversarial Networks (GAN) have motivated a rapid growth of the\ndomain of computer image synthesis. As almost all the existing image synthesis\nalgorithms consider an image as a pixel matrix, the high-resolution image\nsynthesis is complicated.A good alternative can be vector images. However, they\nbelong to the highly sophisticated parametric space, which is a restriction for\nsolving the task of synthesizing vector graphics by GANs. In this paper, we\nconsider a specific application domain that softens this restriction\ndramatically allowing the usage of vector image synthesis.\n</p>\n<p>Music cover images should meet the requirements of Internet streaming\nservices and printing standards, which imply high resolution of graphic\nmaterials without any additional requirements on the content of such images.\nExisting music cover image generation services do not analyze tracks\nthemselves; however, some services mostly consider only genre tags. To generate\nmusic covers as vector images that reflect the music and consist of simple\ngeometric objects, we suggest a GAN-based algorithm called CoverGAN. The\nassessment of resulting images is based on their correspondence to the music\ncompared with AttnGAN and DALL-E text-to-image generation according to title or\nlyrics. Moreover, the significance of the patterns found by CoverGAN has been\nevaluated in terms of the correspondence of the generated cover images to the\nmusical tracks. Listeners evaluate the music covers generated by the proposed\nalgorithm as quite satisfactory and corresponding to the tracks. Music cover\nimages generation code and demo are available at\nhttps://github.com/IzhanVarsky/CoverGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Efimova_V/0/1/0/all/0/1\">Valeria Efimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarsky_I/0/1/0/all/0/1\">Ivan Jarsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizyaev_I/0/1/0/all/0/1\">Ilya Bizyaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty estimation for Cross-dataset performance in Trajectory prediction. (arXiv:2205.07310v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07310","description":"<p>While a lot of work has been done on developing trajectory prediction\nmethods, and various datasets have been proposed for benchmarking this task,\nlittle study has been done so far on the generalizability and the\ntransferability of these methods across dataset. In this paper, we study the\nperformance of a state-of-the-art trajectory prediction method across four\ndifferent datasets (Argoverse, NuScenes, Interaction, Shifts). We first check\nhow a similar method can be applied and trained on all these datasets with\nsimilar hyperparameters. Then we highlight which datasets work best on others,\nand study how uncertainty estimation allows for a better transferable\nperformance; proposing a novel way to estimate uncertainty and to directly use\nit in prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trucks Don't Mean Trump: Diagnosing Human Error in Image Analysis. (arXiv:2205.07333v1 [cs.HC])","link":"http://arxiv.org/abs/2205.07333","description":"<p>Algorithms provide powerful tools for detecting and dissecting human bias and\nerror. Here, we develop machine learning methods to to analyze how humans err\nin a particular high-stakes task: image interpretation. We leverage a unique\ndataset of 16,135,392 human predictions of whether a neighborhood voted for\nDonald Trump or Joe Biden in the 2020 US election, based on a Google Street\nView image. We show that by training a machine learning estimator of the Bayes\noptimal decision for each image, we can provide an actionable decomposition of\nhuman error into bias, variance, and noise terms, and further identify specific\nfeatures (like pickup trucks) which lead humans astray. Our methods can be\napplied to ensure that human-in-the-loop decision-making is accurate and fair\nand are also applicable to black-box algorithmic systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamfirescu_Pereira_J/0/1/0/all/0/1\">J.D. Zamfirescu-Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jerry Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_E/0/1/0/all/0/1\">Emily Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1\">Allison Koenecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1\">Nikhil Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1\">Emma Pierson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel Multicolumn Kernel Extreme Learning Machine for Food Detection via Optimal Features from CNN. (arXiv:2205.07348v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07348","description":"<p>Automatic food detection is an emerging topic of interest due to its wide\narray of applications ranging from detecting food images on social media\nplatforms to filtering non-food photos from the users in dietary assessment\napps. Recently, during the COVID-19 pandemic, it has facilitated enforcing an\neating ban by automatically detecting eating activities from cameras in public\nplaces. Therefore, to tackle the challenge of recognizing food images with high\naccuracy, we proposed the idea of a hybrid framework for extracting and\nselecting optimal features from an efficient neural network. There on, a\nnonlinear classifier is employed to discriminate between linearly inseparable\nfeature vectors with great precision. In line with this idea, our method\nextracts features from MobileNetV3, selects an optimal subset of attributes by\nusing Shapley Additive exPlanations (SHAP) values, and exploits kernel extreme\nlearning machine (KELM) due to its nonlinear decision boundary and good\ngeneralization ability. However, KELM suffers from the 'curse of dimensionality\nproblem' for large datasets due to the complex computation of kernel matrix\nwith large numbers of hidden nodes. We solved this problem by proposing a novel\nmulticolumn kernel extreme learning machine (MCKELM) which exploited the k-d\ntree algorithm to divide data into N subsets and trains separate KELM on each\nsubset of data. Then, the method incorporates KELM classifiers into parallel\nstructures and selects the top k nearest subsets during testing by using the\nk-d tree search for classifying input instead of the whole network. For\nevaluating a proposed framework large food/non-food dataset is prepared using\nnine publically available datasets. Experimental results showed the superiority\nof our method on an integrated set of measures while solving the problem of\n'curse of dimensionality in KELM for large datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_G/0/1/0/all/0/1\">Ghalib Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1\">Tahir Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_K/0/1/0/all/0/1\">Kiong Loo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution CMB Lensing Reconstruction with Deep Learning. (arXiv:2205.07368v1 [astro-ph.CO])","link":"http://arxiv.org/abs/2205.07368","description":"<p>Next-generation cosmic microwave background (CMB) surveys are expected to\nprovide valuable information about the primordial universe by creating maps of\nthe mass along the line of sight. Traditional tools for creating these lensing\nconvergence maps include the quadratic estimator and the maximum likelihood\nbased iterative estimator. Here, we apply a generative adversarial network\n(GAN) to reconstruct the lensing convergence field. We compare our results with\na previous deep learning approach -- Residual-UNet -- and discuss the pros and\ncons of each. In the process, we use training sets generated by a variety of\npower spectra, rather than the one used in testing the methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Li_P/0/1/0/all/0/1\">Peikai Li</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Onur_I/0/1/0/all/0/1\">Ipek Ilayda Onur</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dodelson_S/0/1/0/all/0/1\">Scott Dodelson</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Chaudhari_S/0/1/0/all/0/1\">Shreyas Chaudhari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperWarp: Supervised Learning and Warping on U-Net for Invariant Subvoxel-Precise Registration. (arXiv:2205.07399v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07399","description":"<p>In recent years, learning-based image registration methods have gradually\nmoved away from direct supervision with target warps to instead use\nself-supervision, with excellent results in several registration benchmarks.\nThese approaches utilize a loss function that penalizes the intensity\ndifferences between the fixed and moving images, along with a suitable\nregularizer on the deformation. In this paper, we argue that the relative\nfailure of supervised registration approaches can in part be blamed on the use\nof regular U-Nets, which are jointly tasked with feature extraction, feature\nmatching, and estimation of deformation. We introduce one simple but crucial\nmodification to the U-Net that disentangles feature extraction and matching\nfrom deformation prediction, allowing the U-Net to warp the features, across\nlevels, as the deformation field is evolved. With this modification, direct\nsupervision using target warps begins to outperform self-supervision approaches\nthat require segmentations, presenting new directions for registration when\nimages do not have segmentations. We hope that our findings in this preliminary\nworkshop paper will re-ignite research interest in supervised image\nregistration techniques. Our code is publicly available from\nhttps://github.com/balbasty/superwarp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Young_S/0/1/0/all/0/1\">Sean I. Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balbastre_Y/0/1/0/all/0/1\">Ya&#xeb;l Balbastre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William M. Wells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PillarNet: High-Performance Pillar-based 3D Object Detection. (arXiv:2205.07403v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07403","description":"<p>Real-time and high-performance 3D object detection is of critical importance\nfor autonomous driving. Recent top-performing 3D object detectors mainly rely\non point-based or 3D voxel-based convolutions, which are both computationally\ninefficient for onboard deployment. In contrast, pillar-based methods use\nmerely 2D convolutions, which consume less computation resources, but they lag\nfar behind their voxel-based counterparts in detection accuracy. In this paper,\nby examining the primary performance gap between pillar- and voxel-based\ndetectors, we develop a real-time and high-performance pillar-based detector,\ndubbed PillarNet. The proposed PillarNet consists of a powerful encoder network\nfor effective pillar feature learning, a neck network for spatial-semantic\nfeature fusion and the commonly used detect head. Using only 2D convolutions,\nPillarNet is flexible to an optional pillar size and compatible with classical\n2D CNN backbones, such as VGGNet and ResNet. Additionally, PillarNet benefits\nfrom an orientation-decoupled IoU regression loss along with the IoU-aware\nprediction branch. Extensive experimental results on the large-scale nuScenes\nDataset and Waymo Open Dataset demonstrate that the proposed PillarNet performs\nwell over the state-of-the-art 3D detectors in terms of effectiveness and\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1\">Guangsheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Outlier Removal Strategy Based on Reliability of Correspondence Graph for Fast Point Cloud Registration. (arXiv:2205.07404v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07404","description":"<p>Registration is a basic yet crucial task in point cloud processing. In\ncorrespondence-based point cloud registration, matching correspondences by\npoint feature techniques may lead to an extremely high outlier ratio. Current\nmethods still suffer from low efficiency, accuracy, and recall rate. We use a\nsimple and intuitive method to describe the 6-DOF (degree of freedom)\ncurtailment process in point cloud registration and propose an outlier removal\nstrategy based on the reliability of the correspondence graph. The method\nconstructs the corresponding graph according to the given correspondences and\ndesigns the concept of the reliability degree of the graph node for optimal\ncandidate selection and the reliability degree of the graph edge to obtain the\nglobal maximum consensus set. The presented method could achieve fast and\naccurate outliers removal along with gradual aligning parameters estimation.\nExtensive experiments on simulations and challenging real-world datasets\ndemonstrate that the proposed method can still perform effective point cloud\nregistration even the correspondence outlier ratio is over 99%, and the\nefficiency is better than the state-of-the-art. Code is available at\nhttps://github.com/WPC-WHU/GROR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Li Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Pengcheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jicheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Ming Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in 3D Point Clouds: A Survey. (arXiv:2205.07417v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07417","description":"<p>In recent years, Transformer models have been proven to have the remarkable\nability of long-range dependencies modeling. They have achieved satisfactory\nresults both in Natural Language Processing (NLP) and image processing. This\nsignificant achievement sparks great interest among researchers in 3D point\ncloud processing to apply them to various 3D tasks. Due to the inherent\npermutation invariance and strong global feature learning ability, 3D\nTransformers are well suited for point cloud processing and analysis. They have\nachieved competitive or even better performance compared to the\nstate-of-the-art non-Transformer algorithms. This survey aims to provide a\ncomprehensive overview of 3D Transformers designed for various tasks (e.g.\npoint cloud classification, segmentation, object detection, and so on). We\nstart by introducing the fundamental components of the general Transformer and\nproviding a brief description of its application in 2D and 3D fields. Then, we\npresent three different taxonomies (i.e., Transformer implementation-based\ntaxonomy, data representation-based taxonomy, and task-based taxonomy) for\nmethod classification, which allows us to analyze involved methods from\nmultiple perspectives. Furthermore, we also conduct an investigation of 3D\nself-attention mechanism variants designed for performance improvement. To\ndemonstrate the superiority of 3D Transformers, we compare the performance of\nTransformer-based algorithms in terms of point cloud classification,\nsegmentation, and object detection. Finally, we point out three potential\nfuture research directions, expecting to provide some benefit references for\nthe development of 3D Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dening Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linlin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binarizing by Classification: Is soft function really necessary?. (arXiv:2205.07433v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07433","description":"<p>Binary neural network leverages the $Sign$ function to binarize real values,\nand its non-derivative property inevitably brings huge gradient errors during\nbackpropagation. Although many hand-designed soft functions have been proposed\nto approximate gradients, their mechanism is not clear and there are still huge\nperformance gaps between binary models and their full-precision counterparts.\nTo address this, we propose to tackle network binarization as a binary\nclassification problem and use a multi-layer perceptron (MLP) as the\nclassifier. The MLP-based classifier can fit any continuous function\ntheoretically and is adaptively learned to binarize networks and backpropagate\ngradients without any specific soft function. With this view, we further prove\nexperimentally that even a simple linear function can outperform previous\ncomplex soft functions. Extensive experiments demonstrate that the proposed\nmethod yields surprising performance both in image classification and human\npose estimation tasks. Specifically, we achieve 65.7% top-1 accuracy of\nResNet-34 on ImageNet dataset, with an absolute improvement of 2.8%. When\nevaluating on the challenging Microsoft COCO keypoint dataset, the proposed\nmethod enables binary networks to achieve a mAP of 60.6 for the first time, on\npar with some full-precision methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yefei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Luoming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReDFeat: Recoupling Detection and Description for Multimodal Feature Learning. (arXiv:2205.07439v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07439","description":"<p>Deep-learning-based local feature extraction algorithms that combine\ndetection and description have made significant progress in visible image\nmatching. However, the end-to-end training of such frameworks is notoriously\nunstable due to the lack of strong supervision of detection and the\ninappropriate coupling between detection and description. The problem is\nmagnified in cross-modal scenarios, in which most methods heavily rely on the\npre-training. In this paper, we recouple independent constraints of detection\nand description of multimodal feature learning with a mutual weighting\nstrategy, in which the detected probabilities of robust features are forced to\npeak and repeat, while features with high detection scores are emphasized\nduring optimization. Different from previous works, those weights are detached\nfrom back propagation so that the detected probability of indistinct features\nwould not be directly suppressed and the training would be more stable.\nMoreover, we propose the Super Detector, a detector that possesses a large\nreceptive field and is equipped with learnable non-maximum suppression layers,\nto fulfill the harsh terms of detection. Finally, we build a benchmark that\ncontains cross visible, infrared, near-infrared and synthetic aperture radar\nimage pairs for evaluating the performance of features in feature matching and\nimage registration tasks. Extensive experiments demonstrate that features\ntrained with the recoulped detection and description, named ReDFeat, surpass\nprevious state-of-the-arts in the benchmark, while the model can be readily\ntrained from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuxin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Models for Adversarial Purification. (arXiv:2205.07460v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07460","description":"<p>Adversarial purification refers to a class of defense methods that remove\nadversarial perturbations using a generative model. These methods do not make\nassumptions on the form of attack and the classification model, and thus can\ndefend pre-existing classifiers against unseen threats. However, their\nperformance currently falls behind adversarial training methods. In this work,\nwe propose DiffPure that uses diffusion models for adversarial purification:\nGiven an adversarial example, we first diffuse it with a small amount of noise\nfollowing a forward diffusion process, and then recover the clean image through\na reverse generative process. To evaluate our method against strong adaptive\nattacks in an efficient and scalable way, we propose to use the adjoint method\nto compute full gradients of the reverse generative process. Extensive\nexperiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQ\nwith three classifier architectures including ResNet, WideResNet and ViT\ndemonstrate that our method achieves the state-of-the-art results,\noutperforming current adversarial training and adversarial purification\nmethods, often by a large margin. Project page: https://diffpure.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weili Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Brandon Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yujia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1\">Arash Vahdat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Representation via Dynamic Feature Aggregation. (arXiv:2205.07466v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07466","description":"<p>Deep convolutional neural network (CNN) based models are vulnerable to the\nadversarial attacks. One of the possible reasons is that the embedding space of\nCNN based model is sparse, resulting in a large space for the generation of\nadversarial samples. In this study, we propose a method, denoted as Dynamic\nFeature Aggregation, to compress the embedding space with a novel\nregularization. Particularly, the convex combination between two samples are\nregarded as the pivot for aggregation. In the embedding space, the selected\nsamples are guided to be similar to the representation of the pivot. On the\nother side, to mitigate the trivial solution of such regularization, the last\nfully-connected layer of the model is replaced by an orthogonal classifier, in\nwhich the embedding codes for different classes are processed orthogonally and\nseparately. With the regularization and orthogonal classifier, a more compact\nembedding space can be obtained, which accordingly improves the model\nrobustness against adversarial attacks. An averaging accuracy of 56.91% is\nachieved by our method on CIFAR-10 against various attack methods, which\nsignificantly surpasses a solid baseline (Mixup) by a margin of 37.31%. More\nsurprisingly, empirical results show that, the proposed method can also achieve\nthe state-of-the-art performance for out-of-distribution (OOD) detection, due\nto the learned compact feature space. An F1 score of 0.937 is achieved by the\nproposed method, when adopting CIFAR-10 as in-distribution (ID) dataset and\nLSUN as OOD dataset. Code is available at\nhttps://github.com/HaozheLiu-ST/DynamicFeatureAggregation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haoqin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1\">Nanjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoqian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Convolutional Dictionary Network for CT Metal Artifact Reduction. (arXiv:2205.07471v1 [eess.IV])","link":"http://arxiv.org/abs/2205.07471","description":"<p>Inspired by the great success of deep neural networks, learning-based methods\nhave gained promising performances for metal artifact reduction (MAR) in\ncomputed tomography (CT) images. However, most of the existing approaches put\nless emphasis on modelling and embedding the intrinsic prior knowledge\nunderlying this specific MAR task into their network designs. Against this\nissue, we propose an adaptive convolutional dictionary network (ACDNet), which\nleverages both model-based and learning-based methods. Specifically, we explore\nthe prior structures of metal artifacts, e.g., non-local repetitive streaking\npatterns, and encode them as an explicit weighted convolutional dictionary\nmodel. Then, a simple-yet-effective algorithm is carefully designed to solve\nthe model. By unfolding every iterative substep of the proposed algorithm into\na network module, we explicitly embed the prior structure into a deep network,\n\\emph{i.e.,} a clear interpretability for the MAR task. Furthermore, our ACDNet\ncan automatically learn the prior for artifact-free CT images via training data\nand adaptively adjust the representation kernels for each input CT image based\non its content. Hence, our method inherits the clear interpretability of\nmodel-based methods and maintains the powerful representation ability of\nlearning-based methods. Comprehensive experiments executed on synthetic and\nclinical datasets show the superiority of our ACDNet in terms of effectiveness\nand model generalization. {\\color{blue}{{\\textit{Code is available at\n{\\url{https://github.com/hongwang01/ACDNet}}.}}}}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency selective extrapolation with residual filtering for image error concealment. (arXiv:2205.07476v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07476","description":"<p>The purpose of signal extrapolation is to estimate unknown signal parts from\nknown samples. This task is especially important for error concealment in image\nand video communication. For obtaining a high quality reconstruction,\nassumptions have to be made about the underlying signal in order to solve this\nunderdetermined problem. Among existent reconstruction algorithms, frequency\nselective extrapolation (FSE) achieves high performance by assuming that image\nsignals can be sparsely represented in the frequency domain. However, FSE does\nnot take into account the low-pass behaviour of natural images. In this paper,\nwe propose a modified FSE that takes this prior knowledge into account for the\nmodelling, yielding significant PSNR gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koloda_J/0/1/0/all/0/1\">J&#xe1;n Koloda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seiler_J/0/1/0/all/0/1\">J&#xfc;rgen Seiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victoria S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peinado_A/0/1/0/all/0/1\">Antonio M. Peinado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Characteristics That Predict Downstream Task Performance. (arXiv:2205.07477v1 [cs.LG])","link":"http://arxiv.org/abs/2205.07477","description":"<p>Pretraining methods are typically compared by evaluating the accuracy of\nlinear classifiers, transfer learning performance, or visually inspecting the\nrepresentation manifold's (RM) lower-dimensional projections. We show that the\ndifferences between methods can be understood more clearly by investigating the\nRM directly, which allows for a more detailed comparison. To this end, we\npropose a framework and new metric to measure and compare different RMs. We\nalso investigate and report on the RM characteristics for various pretraining\nmethods. These characteristics are measured by applying sequentially larger\nlocal alterations to the input data, using white noise injections and Projected\nGradient Descent (PGD) adversarial attacks, and then tracking each datapoint.\nWe calculate the total distance moved for each datapoint and the relative\nchange in distance between successive alterations. We show that self-supervised\nmethods learn an RM where alterations lead to large but constant size changes,\nindicating a smoother RM than fully supervised methods. We then combine these\nmeasurements into one metric, the Representation Manifold Quality Metric\n(RMQM), where larger values indicate larger and less variable step sizes, and\nshow that RMQM correlates positively with performance on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merwe_R/0/1/0/all/0/1\">Ruan van der Merwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_G/0/1/0/all/0/1\">Gregory Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnard_E/0/1/0/all/0/1\">Etienne Barnard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topologically Persistent Features-based Object Recognition in Cluttered Indoor Environments. (arXiv:2205.07479v1 [cs.CV])","link":"http://arxiv.org/abs/2205.07479","description":"<p>Recognition of occluded objects in unseen indoor environments is a\nchallenging problem for mobile robots. This work proposes a new slicing-based\ntopological descriptor that captures the 3D shape of object point clouds to\naddress this challenge. It yields similarities between the descriptors of the\noccluded and the corresponding unoccluded objects, enabling object unity-based\nrecognition using a library of trained models. The descriptor is obtained by\npartitioning an object's point cloud into multiple 2D slices and constructing\nfiltrations (nested sequences of simplicial complexes) on the slices to mimic\nfurther slicing of the slices, thereby capturing detailed shapes through\npersistent homology-generated features. We use nine different sequences of\ncluttered scenes from a benchmark dataset for performance evaluation. Our\nmethod outperforms two state-of-the-art deep learning-based point cloud\nclassification methods, namely, DGCNN and SimpleView.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1\">Ekta U. Samani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis G. Banerjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Out-of-Distribution Detection for Real-World Settings. (arXiv:1911.11132v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.11132","description":"<p>Detecting out-of-distribution examples is important for safety-critical\nmachine learning applications such as detecting novel biological phenomena and\nself-driving cars. However, existing research mainly focuses on simple\nsmall-scale settings. To set the stage for more realistic out-of-distribution\ndetection, we depart from small-scale settings and explore large-scale\nmulticlass and multi-label settings with high-resolution images and thousands\nof classes. To make future work in real-world settings possible, we create new\nbenchmarks for three large-scale settings. To test ImageNet multiclass anomaly\ndetectors, we introduce the Species dataset containing over 700,000 images and\nover a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL\nVOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark\nfor anomaly segmentation by introducing a segmentation benchmark with road\nanomalies. We conduct extensive experiments in these more realistic settings\nfor out-of-distribution detection and find that a surprisingly simple detector\nbased on the maximum logit outperforms prior methods in all the large-scale\nmulti-class, multi-label, and segmentation tasks, establishing a simple new\nbaseline for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1\">Steven Basart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joe Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostajabi_M/0/1/0/all/0/1\">Mohammadreza Mostajabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-light Image Enhancement Using the Cell Vibration Model. (arXiv:2006.02271v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2006.02271","description":"<p>Low light very likely leads to the degradation of an image's quality and even\ncauses visual task failures. Existing image enhancement technologies are prone\nto overenhancement, color distortion or time consumption, and their\nadaptability is fairly limited. Therefore, we propose a new single low-light\nimage lightness enhancement method. First, an energy model is presented based\non the analysis of membrane vibrations induced by photon stimulations. Then,\nbased on the unique mathematical properties of the energy model and combined\nwith the gamma correction model, a new global lightness enhancement model is\nproposed. Furthermore, a special relationship between image lightness and gamma\nintensity is found. Finally, a local fusion strategy, including segmentation,\nfiltering and fusion, is proposed to optimize the local details of the global\nlightness enhancement images. Experimental results show that the proposed\nalgorithm is superior to nine state-of-the-art methods in avoiding color\ndistortion, restoring the textures of dark areas, reproducing natural colors\nand reducing time cost. The image source and code will be released at\nhttps://github.com/leixiaozhou/CDEFmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lei_X/0/1/0/all/0/1\">Xiaozhou Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_Z/0/1/0/all/0/1\">Zixiang Fei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wenju Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_M/0/1/0/all/0/1\">Minrui Fei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural Networks Acceleration. (arXiv:2006.12963v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.12963","description":"<p>The existence of a lot of redundant information in convolutional neural\nnetworks leads to the slow deployment of its equipment on the edge. To solve\nthis issue, we proposed a novel deep learning model compression acceleration\nmethod based on data distribution characteristics, namely Pruning Filter via\nGaussian Distribution Feature(PFGDF) which was to found the smaller interval of\nthe convolution layer of a certain layer to describe the original on the\ngrounds of distribution characteristics . Compared with revious advanced\nmethods, PFGDF compressed the model by filters with insignificance in\ndistribution regardless of the contribution and sensitivity information of the\nconvolution filter. The pruning process of the model was automated, and always\nensured that the compressed model could restore the performance of original\nmodel. Notably, on CIFAR-10, PFGDF compressed the convolution filter on VGG-16\nby 66:62%, the parameter reducing more than 90%, and FLOPs achieved 70:27%. On\nResNet-32, PFGDF reduced the convolution filter by 21:92%. The parameter was\nreduced to 54:64%, and the FLOPs exceeded 42%\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianrong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bifeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weight-dependent Gates for Network Pruning. (arXiv:2007.02066v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.02066","description":"<p>In this paper, a simple yet effective network pruning framework is proposed\nto simultaneously address the problems of pruning indicator, pruning ratio, and\nefficiency constraint. This paper argues that the pruning decision should\ndepend on the convolutional weights, and thus proposes novel weight-dependent\ngates (W-Gates) to learn the information from filter weights and obtain binary\ngates to prune or keep the filters automatically. To prune the network under\nefficiency constraints, a switchable Efficiency Module is constructed to\npredict the hardware latency or FLOPs of candidate pruned networks. Combined\nwith the proposed Efficiency Module, W-Gates can perform filter pruning in an\nefficiency-aware manner and achieve a compact network with a better\naccuracy-efficiency trade-off. We have demonstrated the effectiveness of the\nproposed method on ResNet34, ResNet50, and MobileNet V2, respectively achieving\nup to 1.33/1.28/1.1 higher Top-1 accuracy with lower hardware latency on\nImageNet. Compared with state-of-the-art methods, W-Gates also achieves\nsuperior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiqun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Haotian Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baoqun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex Polytope. (arXiv:2011.14035v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14035","description":"<p>During the last years, the emerging field of Augmented &amp; Virtual Reality\n(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop\nlow cost high-quality AR systems where computing poweris in demand. Feature\npoints are extensively used in these real-time frame-rate and 3D applications,\nthereforeefficient high-speed feature detectors are necessary. Corners are such\nspecial features and often are used as thefirst step in the marker alignment in\nAugmented Reality (AR). Corners are also used in image registration\nandrecognition, tracking, SLAM, robot path finding and 2D or 3D object\ndetection and retrieval. Therefore thereis a large number of corner detection\nalgorithms but most of them are too computationally intensive for use\ninreal-time applications of any complexity. Many times the border of the image\nis a convex polygon. For thisspecial, but quite common case, we have developed\na specific algorithm, cMinMax. The proposed algorithmis faster, approximately\nby a factor of 5 compared to the widely used Harris Corner Detection algorithm.\nInaddition is highly parallelizable. The algorithm is suitable for the fast\nregistration of markers in augmentedreality systems and in applications where a\ncomputationally efficient real time feature detector is necessary.The algorithm\ncan also be extended to N-dimensional polyhedrons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chamzas_D/0/1/0/all/0/1\">Dimitrios Chamzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chamzas_C/0/1/0/all/0/1\">Constantinos Chamzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustakas_K/0/1/0/all/0/1\">Konstantinos Moustakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Based Guidance for Tracking Dynamic Objects. (arXiv:2104.09301v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09301","description":"<p>In this paper, we present a novel vision-based framework for tracking dynamic\nobjects using guidance laws based on a rendezvous cone approach. These guidance\nlaws enable an unmanned aircraft system equipped with a monocular camera to\ncontinuously follow a moving object within the sensor's field of view. We\nidentify and classify feature point estimators for managing the occurrence of\nocclusions during the tracking process in an exclusive manner. Furthermore, we\ndevelop an open-source simulation environment and perform a series of\nsimulations to show the efficacy of our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karmokar_P/0/1/0/all/0/1\">Pritam Karmokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhal_K/0/1/0/all/0/1\">Kashish Dhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beksi_W/0/1/0/all/0/1\">William J. Beksi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthy_A/0/1/0/all/0/1\">Animesh Chakravarthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Spatial Reasoning on Multi-View Line Drawings. (arXiv:2104.13433v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13433","description":"<p>Spatial reasoning on multi-view line drawings by state-of-the-art supervised\ndeep networks is recently shown with puzzling low performances on the SPARE3D\ndataset. Based on the fact that self-supervised learning is helpful when a\nlarge number of data are available, we propose two self-supervised learning\napproaches to improve the baseline performance for view consistency reasoning\nand camera pose reasoning tasks on the SPARE3D dataset. For the first task, we\nuse a self-supervised binary classification network to contrast the line\ndrawing differences between various views of any two similar 3D objects,\nenabling the trained networks to effectively learn detail-sensitive yet\nview-invariant line drawing representations of 3D objects. For the second type\nof task, we propose a self-supervised multi-class classification framework to\ntrain a model to select the correct corresponding view from which a line\ndrawing is rendered. Our method is even helpful for the downstream tasks with\nunseen camera poses. Experiments show that our method could significantly\nincrease the baseline performance in SPARE3D, while some popular\nself-supervised learning methods cannot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Siyuan Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Anbang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yanfei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaoqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZeroWaste Dataset: Towards Deformable Object Segmentation in Cluttered Scenes. (arXiv:2106.02740v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02740","description":"<p>Less than 35% of recyclable waste is being actually recycled in the US, which\nleads to increased soil and sea pollution and is one of the major concerns of\nenvironmental researchers as well as the common public. At the heart of the\nproblem are the inefficiencies of the waste sorting process (separating paper,\nplastic, metal, glass, etc.) due to the extremely complex and cluttered nature\nof the waste stream. Recyclable waste detection poses a unique computer vision\nchallenge as it requires detection of highly deformable and often translucent\nobjects in cluttered scenes without the kind of context information usually\npresent in human-centric datasets. This challenging computer vision task\ncurrently lacks suitable datasets or methods in the available literature. In\nthis paper, we take a step towards computer-aided waste detection and present\nthe first in-the-wild industrial-grade waste detection and segmentation\ndataset, ZeroWaste. We believe that ZeroWaste will catalyze research in object\ndetection and semantic segmentation in extreme clutter as well as applications\nin the recycling domain. Our project page can be found at\n<a href=\"http://ai.bu.edu/zerowaste/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1\">Dina Bashkirova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1\">Mohamed Abdelfattah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Ziliang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akl_J/0/1/0/all/0/1\">James Akl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alladkani_F/0/1/0/all/0/1\">Fadi Alladkani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Ping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calli_B/0/1/0/all/0/1\">Berk Calli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1\">Sarah Adel Bargal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applications of knowledge graphs for food science and industry. (arXiv:2107.05869v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05869","description":"<p>The deployment of various networks (e.g., Internet of Things [IoT] and mobile\nnetworks), databases (e.g., nutrition tables and food compositional databases),\nand social media (e.g., Instagram and Twitter) generates huge amounts of food\ndata, which present researchers with an unprecedented opportunity to study\nvarious problems and applications in food science and industry via data-driven\ncomputational methods. However, these multi-source heterogeneous food data\nappear as information silos, leading to difficulty in fully exploiting these\nfood data. The knowledge graph provides a unified and standardized conceptual\nterminology in a structured form, and thus can effectively organize these food\ndata to benefit various applications. In this review, we provide a brief\nintroduction to knowledge graphs and the evolution of food knowledge\norganization mainly from food ontology to food knowledge graphs. We then\nsummarize seven representative applications of food knowledge graphs, such as\nnew recipe development, diet-disease correlation discovery, and personalized\ndietary recommendation. We also discuss future directions in this field, such\nas multimodal food knowledge graph construction and food knowledge graphs for\nhuman health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Leyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifting the Convex Conjugate in Lagrangian Relaxations: A Tractable Approach for Continuous Markov Random Fields. (arXiv:2107.06028v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2107.06028","description":"<p>Dual decomposition approaches in nonconvex optimization may suffer from a\nduality gap. This poses a challenge when applying them directly to nonconvex\nproblems such as MAP-inference in a Markov random field (MRF) with continuous\nstate spaces. To eliminate such gaps, this paper considers a reformulation of\nthe original nonconvex task in the space of measures. This infinite-dimensional\nreformulation is then approximated by a semi-infinite one, which is obtained\nvia a piecewise polynomial discretization in the dual. We provide a geometric\nintuition behind the primal problem induced by the dual discretization and draw\nconnections to optimization over moment spaces. In contrast to existing\ndiscretizations which suffer from a grid bias, we show that a piecewise\npolynomial discretization better preserves the continuous nature of our\nproblem. Invoking results from optimal transport theory and convex algebraic\ngeometry we reduce the semi-infinite program to a finite one and provide a\npractical implementation based on semidefinite programming. We show,\nexperimentally and in theory, that the approach successfully reduces the\nduality gap. To showcase the scalability of our approach, we apply it to the\nstereo matching problem between two images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Bauermeister_H/0/1/0/all/0/1\">Hartmut Bauermeister</a>, <a href=\"http://arxiv.org/find/math/1/au:+Laude_E/0/1/0/all/0/1\">Emanuel Laude</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mollenhoff_T/0/1/0/all/0/1\">Thomas M&#xf6;llenhoff</a>, <a href=\"http://arxiv.org/find/math/1/au:+Moeller_M/0/1/0/all/0/1\">Michael Moeller</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clipped Hyperbolic Classifiers Are Super-Hyperbolic Classifiers. (arXiv:2107.11472v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.11472","description":"<p>Hyperbolic space can naturally embed hierarchies, unlike Euclidean space.\nHyperbolic Neural Networks (HNNs) exploit such representational power by\nlifting Euclidean features into hyperbolic space for classification,\noutperforming Euclidean neural networks (ENNs) on datasets with known semantic\nhierarchies. However, HNNs underperform ENNs on standard benchmarks without\nclear hierarchies, greatly restricting HNNs' applicability in practice.\n</p>\n<p>Our key insight is that HNNs' poorer general classification performance\nresults from vanishing gradients during backpropagation, caused by their hybrid\narchitecture connecting Euclidean features to a hyperbolic classifier. We\npropose an effective solution by simply clipping the Euclidean feature\nmagnitude while training HNNs.\n</p>\n<p>Our experiments demonstrate that clipped HNNs become super-hyperbolic\nclassifiers: They are not only consistently better than HNNs which already\noutperform ENNs on hierarchical data, but also on-par with ENNs on MNIST,\nCIFAR10, CIFAR100 and ImageNet benchmarks, with better adversarial robustness\nand out-of-distribution detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yunhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yubei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Operator-Splitting Method for the Gaussian Curvature Regularization Model with Applications to Surface Smoothing and Imaging. (arXiv:2108.01914v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01914","description":"<p>Gaussian curvature is an important geometric property of surfaces, which has\nbeen used broadly in mathematical modeling. Due to the full nonlinearity of the\nGaussian curvature, efficient numerical methods for models based on it are\nuncommon in literature. In this article, we propose an operator-splitting\nmethod for a general Gaussian curvature model. In our method, we decouple the\nfull nonlinearity of Gaussian curvature from differential operators by\nintroducing two matrix- and vector-valued functions. The optimization problem\nis then converted into the search for the steady state solution of a time\ndependent PDE system. The above PDE system is well-suited to time\ndiscretization by operator splitting, the sub-problems encountered at each\nfractional step having either a closed form solution or being solvable by\nefficient algorithms. The proposed method is not sensitive to the choice of\nparameters, its efficiency and performances being demonstrated via systematic\nexperiments on surface smoothing and image denoising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1\">Xue-Cheng Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glowinski_R/0/1/0/all/0/1\">Roland Glowinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04349","description":"<p>In this paper, we present a new network named Attention Aware Network (AASeg)\nfor real time semantic image segmentation. Our network incorporates spatial and\nchannel information using Spatial Attention (SA) and Channel Attention (CA)\nmodules respectively. It also uses dense local multi-scale context information\nusing Multi Scale Context (MSC) module. The feature maps are concatenated\nindividually to produce the final segmentation map. We demonstrate the\neffectiveness of our method using a comprehensive analysis, quantitative\nexperimental results and ablation study using Cityscapes, ADE20K and Camvid\ndatasets. Our network performs better than most previous architectures with a\n74.4\\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10904","description":"<p>With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Coated Adversarial Camouflages for Object Detectors. (arXiv:2109.00124v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00124","description":"<p>An adversary can fool deep neural network object detectors by generating\nadversarial noises. Most of the existing works focus on learning local visible\nnoises in an adversarial \"patch\" fashion. However, the 2D patch attached to a\n3D object tends to suffer from an inevitable reduction in attack performance as\nthe viewpoint changes. To remedy this issue, this work proposes the Coated\nAdversarial Camouflage (CAC) to attack the detectors in arbitrary viewpoints.\nUnlike the patch trained in the 2D space, our camouflage generated by a\nconceptually different training framework consists of 3D rendering and dense\nproposals attack. Specifically, we make the camouflage perform 3D spatial\ntransformations according to the pose changes of the object. Based on the\nmulti-view rendering results, the top-n proposals of the region proposal\nnetwork are fixed, and all the classifications in the fixed dense proposals are\nattacked simultaneously to output errors. In addition, we build a virtual 3D\nscene to fairly and reproducibly evaluate different attacks. Extensive\nexperiments demonstrate the superiority of CAC over the existing attacks, and\nit shows impressive performance both in the virtual scene and the real world.\nThis poses a potential threat to the security-critical computer vision systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yexin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jialin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junhua Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengyun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhisong Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cross-Scale Visual Representations for Real-Time Image Geo-Localization. (arXiv:2109.04087v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04087","description":"<p>Robot localization remains a challenging task in GPS denied environments.\nState estimation approaches based on local sensors, e.g. cameras or IMUs, are\ndrifting-prone for long-range missions as error accumulates. In this study, we\naim to address this problem by localizing image observations in a 2D\nmulti-modal geospatial map. We introduce the cross-scale dataset and a\nmethodology to produce additional data from cross-modality sources. We propose\na framework that learns cross-scale visual representations without supervision.\nExperiments are conducted on data from two different domains, underwater and\naerial. In contrast to existing studies in cross-view image geo-localization,\nour approach a) performs better on smaller-scale multi-modal maps; b) is more\ncomputationally efficient for real-time applications; c) can serve directly in\nconcert with state estimation pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1\">Matthew Johnson-Roberson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.02636","description":"<p>Diffusion-based inpainting is a powerful tool for the reconstruction of\nimages from sparse data. Its quality strongly depends on the choice of known\ndata. Optimising their spatial location -- the inpainting mask -- is\nchallenging. A commonly used tool for this task are stochastic optimisation\nstrategies. However, they are slow as they compute multiple inpainting results.\nWe provide a remedy in terms of a learned mask generation model. By emulating\nthe complete inpainting pipeline with two networks for mask generation and\nneural surrogate inpainting, we obtain a model for highly efficient adaptive\nmask generation. Experiments indicate that our model can achieve competitive\nquality with an acceleration by as much as four orders of magnitude. Our\nfindings serve as a basis for making diffusion-based inpainting more attractive\nfor applications such as image compression, where fast encoding is highly\ndesirable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alt_T/0/1/0/all/0/1\">Tobias Alt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peter_P/0/1/0/all/0/1\">Pascal Peter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weickert_J/0/1/0/all/0/1\">Joachim Weickert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13285","description":"<p>Due to the success of generative flows to model data distributions, they have\nbeen explored in inverse problems. Given a pre-trained generative flow,\nprevious work proposed to minimize the 2-norm of the latent variables as a\nregularization term. The intuition behind it was to ensure high likelihood\nlatent variables that produce the closest restoration. However, high-likelihood\nlatent variables may generate unrealistic samples as we show in our\nexperiments. We therefore propose a solver to directly produce high-likelihood\nreconstructions. We hypothesize that our approach could make generative flows a\ngeneral purpose solver for inverse problems. Furthermore, we propose 1 x 1\ncoupling functions to introduce permutations in a generative flow. It has the\nadvantage that its inverse does not require to be calculated in the generation\nprocess. Finally, we evaluate our method for denoising, deblurring, inpainting,\nand colorization. We observe a compelling improvement of our method over prior\nworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chavez_J/0/1/0/all/0/1\">Jos&#xe9; A. Ch&#xe1;vez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic 3D Scene Reconstruction From a Single RGB Image. (arXiv:2111.02444v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.02444","description":"<p>Understanding 3D scenes from a single image is fundamental to a wide variety\nof tasks, such as for robotics, motion planning, or augmented reality. Existing\nworks in 3D perception from a single RGB image tend to focus on geometric\nreconstruction only, or geometric reconstruction with semantic segmentation or\ninstance segmentation. Inspired by 2D panoptic segmentation, we propose to\nunify the tasks of geometric reconstruction, 3D semantic segmentation, and 3D\ninstance segmentation into the task of panoptic 3D scene reconstruction - from\na single RGB image, predicting the complete geometric reconstruction of the\nscene in the camera frustum of the image, along with semantic and instance\nsegmentations. We thus propose a new approach for holistic 3D scene\nunderstanding from a single RGB image which learns to lift and propagate 2D\nfeatures from an input image to a 3D volumetric scene representation. We\ndemonstrate that this holistic view of joint scene reconstruction, semantic,\nand instance segmentation is beneficial over treating the tasks independently,\nthus outperforming alternative approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahnert_M/0/1/0/all/0/1\">Manuel Dahnert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Ji Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion. (arXiv:2111.10332v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10332","description":"<p>Point cloud processing is a challenging task due to its sparsity and\nirregularity. Prior works introduce delicate designs on either local feature\naggregator or global geometric architecture, but few combine both advantages.\nWe propose Dual-Scale Point Cloud Recognition with High-frequency Fusion\n(DSPoint) to extract local-global features by concurrently operating on voxels\nand points. We reverse the conventional design of applying convolution on\nvoxels and attention to points. Specifically, we disentangle point features\nthrough channel dimension for dual-scale processing: one by point-wise\nconvolution for fine-grained geometry parsing, the other by voxel-wise global\nattention for long-range structural exploration. We design a co-attention\nfusion module for feature alignment to blend local-global modalities, which\nconducts inter-scale cross-modality interaction by communicating high-frequency\ncoordinates information. Experiments and ablations on widely-adopted\nModelNet40, ShapeNet, and S3DIS demonstrate the state-of-the-art performance of\nour DSPoint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Ziyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinben Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kexue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianbo Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TridentAdapt: Learning Domain-invariance via Source-Target Confrontation and Self-induced Cross-domain Augmentation. (arXiv:2111.15300v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15300","description":"<p>Due to the difficulty of obtaining ground-truth labels, learning from\nvirtual-world datasets is of great interest for real-world applications like\nsemantic segmentation. From domain adaptation perspective, the key challenge is\nto learn domain-agnostic representation of the inputs in order to benefit from\nvirtual data. In this paper, we propose a novel trident-like architecture that\nenforces a shared feature encoder to satisfy confrontational source and target\nconstraints simultaneously, thus learning a domain-invariant feature space.\nMoreover, we also introduce a novel training pipeline enabling self-induced\ncross-domain data augmentation during the forward pass. This contributes to a\nfurther reduction of the domain gap. Combined with a self-training process, we\nobtain state-of-the-art results on benchmark datasets (e.g. GTA5 or Synthia to\nCityscapes adaptation). Code and pre-trained models are available at\nhttps://github.com/HMRC-AEL/TridentAdapt\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Fengyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurram_A/0/1/0/all/0/1\">Akhil Gurram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuna_A/0/1/0/all/0/1\">Ahmet Faruk Tuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urfalioglu_O/0/1/0/all/0/1\">Onay Urfalioglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSSL: Co-Learning of Representation and Classifier for Imbalanced Semi-Supervised Learning. (arXiv:2112.04564v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04564","description":"<p>In this paper, we propose a novel co-learning framework (CoSSL) with\ndecoupled representation learning and classifier learning for imbalanced SSL.\nTo handle the data imbalance, we devise Tail-class Feature Enhancement (TFE)\nfor classifier learning. Furthermore, the current evaluation protocol for\nimbalanced SSL focuses only on balanced test sets, which has limited\npracticality in real-world scenarios. Therefore, we further conduct a\ncomprehensive evaluation under various shifted test distributions. In\nexperiments, we show that our approach outperforms other methods over a large\nrange of shifted distributions, achieving state-of-the-art performance on\nbenchmark datasets ranging from CIFAR-10, CIFAR-100, ImageNet, to Food-101. Our\ncode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yue Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Transformers with Primal Object Queries for Multi-Label Image Classification. (arXiv:2112.05485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05485","description":"<p>Multi-label image classification is about predicting a set of class labels\nthat can be considered as orderless sequential data. Transformers process the\nsequential data as a whole, therefore they are inherently good at set\nprediction. The first vision-based transformer model, which was proposed for\nthe object detection task introduced the concept of object queries. Object\nqueries are learnable positional encodings that are used by attention modules\nin decoder layers to decode the object classes or bounding boxes using the\nregion of interests in an image. However, inputting the same set of object\nqueries to different decoder layers hinders the training: it results in lower\nperformance and delays convergence. In this paper, we propose the usage of\nprimal object queries that are only provided at the start of the transformer\ndecoder stack. In addition, we improve the mixup technique proposed for\nmulti-label classification. The proposed transformer model with primal object\nqueries improves the state-of-the-art class wise F1 metric by 2.1% and 1.8%;\nand speeds up the convergence by 79.0% and 38.6% on MS-COCO and NUS-WIDE\ndatasets respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yazici_V/0/1/0/all/0/1\">Vacit Oguz Yazici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Longlong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HerosNet: Hyperspectral Explicable Reconstruction and Optimal Sampling Deep Network for Snapshot Compressive Imaging. (arXiv:2112.06238v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06238","description":"<p>Hyperspectral imaging is an essential imaging modality for a wide range of\napplications, especially in remote sensing, agriculture, and medicine. Inspired\nby existing hyperspectral cameras that are either slow, expensive, or bulky,\nreconstructing hyperspectral images (HSIs) from a low-budget snapshot\nmeasurement has drawn wide attention. By mapping a truncated numerical\noptimization algorithm into a network with a fixed number of phases, recent\ndeep unfolding networks (DUNs) for spectral snapshot compressive sensing (SCI)\nhave achieved remarkable success. However, DUNs are far from reaching the scope\nof industrial applications limited by the lack of cross-phase feature\ninteraction and adaptive parameter adjustment. In this paper, we propose a\nnovel Hyperspectral Explicable Reconstruction and Optimal Sampling deep Network\nfor SCI, dubbed HerosNet, which includes several phases under the\nISTA-unfolding framework. Each phase can flexibly simulate the sensing matrix\nand contextually adjust the step size in the gradient descent step, and\nhierarchically fuse and interact the hidden states of previous phases to\neffectively recover current HSI frames in the proximal mapping step.\nSimultaneously, a hardware-friendly optimal binary mask is learned end-to-end\nto further improve the reconstruction performance. Finally, our HerosNet is\nvalidated to outperform the state-of-the-art methods on both simulation and\nreal datasets by large margins. The source code is available at\nhttps://github.com/jianzhangcs/HerosNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongbing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qilin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIgLib & SmokeyNet: Dataset and Deep Learning Model for Real-Time Wildland Fire Smoke Detection. (arXiv:2112.08598v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08598","description":"<p>The size and frequency of wildland fires in the western United States have\ndramatically increased in recent years. On high-fire-risk days, a small fire\nignition can rapidly grow and become out of control. Early detection of fire\nignitions from initial smoke can assist the response to such fires before they\nbecome difficult to manage. Past deep learning approaches for wildfire smoke\ndetection have suffered from small or unreliable datasets that make it\ndifficult to extrapolate performance to real-world scenarios. In this work, we\npresent the Fire Ignition Library (FIgLib), a publicly available dataset of\nnearly 25,000 labeled wildfire smoke images as seen from fixed-view cameras\ndeployed in Southern California. We also introduce SmokeyNet, a novel deep\nlearning architecture using spatiotemporal information from camera imagery for\nreal-time wildfire smoke detection. When trained on the FIgLib dataset,\nSmokeyNet outperforms comparable baselines and rivals human performance. We\nhope that the availability of the FIgLib dataset and the SmokeyNet architecture\nwill inspire further research into deep learning methods for wildfire smoke\ndetection, leading to automated notification systems that reduce the time to\nwildfire response.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dewangan_A/0/1/0/all/0/1\">Anshuman Dewangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_Y/0/1/0/all/0/1\">Yash Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_H/0/1/0/all/0/1\">Hans-Werner Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernon_F/0/1/0/all/0/1\">Frank Vernon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_I/0/1/0/all/0/1\">Ismael Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altintas_I/0/1/0/all/0/1\">Ilkay Altintas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cottrell_G/0/1/0/all/0/1\">Garrison W. Cottrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Mai H. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05729","description":"<p>Contrastive language-image pretraining (CLIP) links vision and language\nmodalities into a unified embedding space, yielding the tremendous potential\nfor vision-language (VL) tasks. While early concurrent works have begun to\nstudy this potential on a subset of tasks, important questions remain: 1) What\nis the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in\nlow-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches\nwithout impacting inference or pretraining complexity? In this work, we seek to\nanswer these questions through two key contributions. First, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata availability constraints and conditions of domain shift. Second, we\npropose an approach, named CLIP Targeted Distillation (CLIP-TD), to\nintelligently distill knowledge from CLIP into existing architectures using a\ndynamically weighted objective applied to adaptively selected tokens per\ninstance. Experiments demonstrate that our proposed CLIP-TD leads to\nexceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to\n71.3%) conditions of VCR, while simultaneously improving performance under\nstandard fully-supervised conditions (up to 2%), achieving state-of-art\nperformance on VCR compared to other single models that are pretrained with\nimage-text data only. On SNLI-VE, CLIP-TD produces significant gains in\nlow-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On\nVQA, CLIP-TD provides improvement in low-shot (up to 9%), and in\nfully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works\nutilizing CLIP for finetuning, as well as baseline naive distillation\napproaches. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of the Carotid Lumen and Vessel Wall using Deep Learning and Location Priors. (arXiv:2201.06259v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.06259","description":"<p>In this report we want to present our method and results for the Carotid\nArtery Vessel Wall Segmentation Challenge. We propose an image-based pipeline\nutilizing the U-Net architecture and location priors to solve the segmentation\nproblem at hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thamm_F/0/1/0/all/0/1\">Florian Thamm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1\">Felix Denzinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rist_L/0/1/0/all/0/1\">Leonhard Rist</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vicario_C/0/1/0/all/0/1\">Celia Martin Vicario</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kordon_F/0/1/0/all/0/1\">Florian Kordon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTHA: Video Captioning Evaluation Via Transfer-Learned Human Assessment. (arXiv:2201.10243v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10243","description":"<p>Evaluating video captioning systems is a challenging task as there are\nmultiple factors to consider; for instance: the fluency of the caption,\nmultiple actions happening in a single scene, and the human bias of what is\nconsidered important. Most metrics try to measure how similar the system\ngenerated captions are to a single or a set of human-annotated captions. This\npaper presents a new method based on a deep learning model to evaluate these\nsystems. The model is based on BERT, which is a language model that has been\nshown to work well in multiple NLP tasks. The aim is for the model to learn to\nperform an evaluation similar to that of a human. To do so, we use a dataset\nthat contains human evaluations of system generated captions. The dataset\nconsists of the human judgments of the captions produce by the system\nparticipating in various years of the TRECVid video to text task. These\nannotations will be made publicly available. BERTHA obtain favourable results,\noutperforming the commonly used metrics in some setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lebron_L/0/1/0/all/0/1\">Luis Lebron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_Y/0/1/0/all/0/1\">Yvette Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouramas_K/0/1/0/all/0/1\">Konstantinos Kouramas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized visual encoding model construction with small data. (arXiv:2202.02245v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2202.02245","description":"<p>Encoding models that predict brain response patterns to stimuli are one way\nto capture this relationship between variability in bottom-up neural systems\nand individual's behavior or pathological state. However, they generally need a\nlarge amount of training data to achieve optimal accuracy. Here, we propose and\ntest an alternative personalized ensemble encoding model approach to utilize\nexisting encoding models, to create encoding models for novel individuals with\nrelatively little stimuli-response data. We show that these personalized\nensemble encoding models trained with small amounts of data for a specific\nindividual, i.e. ~300 image-response pairs, achieve accuracy not different from\nmodels trained on ~20,000 image-response pairs for the same individual.\nImportantly, the personalized ensemble encoding models preserve patterns of\ninter-individual variability in the image-response relationship. Additionally,\nwe show the proposed approach is robust against domain shift by validating on a\nprospectively collected set of image-response data in novel individuals with a\ndifferent scanner and experimental setup. Finally, we use our personalized\nensemble encoding model within the recently developed NeuroGen framework to\ngenerate optimal stimuli designed to maximize specific regions' activations for\na specific individual. We show that the inter-individual differences in face\nareas responses to images of animal vs human faces observed previously is\nreplicated using NeuroGen with the ensemble encoding model. Our approach shows\nthe potential to use previously collected, deeply sampled data to efficiently\ncreate accurate, personalized encoding models and, subsequently, personalized\noptimal synthetic images for new individuals scanned under different\nexperimental conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Gu_Z/0/1/0/all/0/1\">Zijin Gu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jamison_K/0/1/0/all/0/1\">Keith Jamison</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert Sabuncu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kuceyeski_A/0/1/0/all/0/1\">Amy Kuceyeski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPasso: Semantically-Aware Object Sketching. (arXiv:2202.05822v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2202.05822","description":"<p>Abstraction is at the heart of sketching due to the simple and minimal nature\nof line drawings. Abstraction entails identifying the essential visual\nproperties of an object or scene, which requires semantic understanding and\nprior knowledge of high-level concepts. Abstract depictions are therefore\nchallenging for artists, and even more so for machines. We present CLIPasso, an\nobject sketching method that can achieve different levels of abstraction,\nguided by geometric and semantic simplifications. While sketch generation\nmethods often rely on explicit sketch datasets for training, we utilize the\nremarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill\nsemantic concepts from sketches and images alike. We define a sketch as a set\nof B\\'ezier curves and use a differentiable rasterizer to optimize the\nparameters of the curves directly with respect to a CLIP-based perceptual loss.\nThe abstraction degree is controlled by varying the number of strokes. The\ngenerated sketches demonstrate multiple levels of abstraction while maintaining\nrecognizability, underlying structure, and essential visual components of the\nsubject drawn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1\">Yael Vinker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajouheshgar_E/0/1/0/all/0/1\">Ehsan Pajouheshgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_J/0/1/0/all/0/1\">Jessica Y. Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachmann_R/0/1/0/all/0/1\">Roman Christian Bachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit Haim Bermano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1\">Amir Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07427","description":"<p>Authors of posts in social media communicate their emotions and what causes\nthem with text and images. While there is work on emotion and stimulus\ndetection for each modality separately, it is yet unknown if the modalities\ncontain complementary emotion information in social media. We aim at filling\nthis research gap and contribute a novel, annotated corpus of English\nmultimodal Reddit posts. On this resource, we develop models to automatically\ndetect the relation between image and text, an emotion stimulus category and\nthe emotion class. We evaluate if these tasks require both modalities and find\nfor the image-text relations, that text alone is sufficient for most categories\n(complementary, illustrative, opposing): the information in the text allows to\npredict if an image is required for emotion understanding. The emotions of\nanger and sadness are best predicted with a multimodal model, while text alone\nis sufficient for disgust, joy, and surprise. Stimuli depicted by objects,\nanimals, food, or a person are best predicted by image-only models, while\nmultimodal models are most effective on art, events, memes, places, or\nscreenshots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khlyzova_A/0/1/0/all/0/1\">Anna Khlyzova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silberer_C/0/1/0/all/0/1\">Carina Silberer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variable Rate Compression for Raw 3D Point Clouds. (arXiv:2202.13862v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13862","description":"<p>In this paper, we propose a novel variable rate deep compression architecture\nthat operates on raw 3D point cloud data. The majority of learning-based point\ncloud compression methods work on a downsampled representation of the data.\nMoreover, many existing techniques require training multiple networks for\ndifferent compression rates to generate consolidated point clouds of varying\nquality. In contrast, our network is capable of explicitly processing point\nclouds and generating a compressed description at a comprehensive range of\nbitrates. Furthermore, our approach ensures that there is no loss of\ninformation as a result of the voxelization process and the density of the\npoint cloud does not affect the encoder/decoder performance. An extensive\nexperimental evaluation shows that our model obtains state-of-the-art results,\nit is computationally efficient, and it can work directly with point cloud data\nthus avoiding an expensive voxelized representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muzaddid_M/0/1/0/all/0/1\">Md Ahmed Al Muzaddid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beksi_W/0/1/0/all/0/1\">William J. Beksi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PASS: Part-Aware Self-Supervised Pre-Training for Person Re-Identification. (arXiv:2203.03931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03931","description":"<p>In person re-identification (ReID), very recent researches have validated\npre-training the models on unlabelled person images is much better than on\nImageNet. However, these researches directly apply the existing self-supervised\nlearning (SSL) methods designed for image classification to ReID without any\nadaption in the framework. These SSL methods match the outputs of local views\n(e.g., red T-shirt, blue shorts) to those of the global views at the same time,\nlosing lots of details. In this paper, we propose a ReID-specific pre-training\nmethod, Part-Aware Self-Supervised pre-training (PASS), which can generate\npart-level features to offer fine-grained information and is more suitable for\nReID. PASS divides the images into several local areas, and the local views\nrandomly cropped from each area are assigned with a specific learnable [PART]\ntoken. On the other hand, the [PART]s of all local areas are also appended to\nthe global views. PASS learns to match the output of the local views and global\nviews on the same [PART]. That is, the learned [PART] of the local views from a\nlocal area is only matched with the corresponding [PART] learned from the\nglobal views. As a result, each [PART] can focus on a specific local area of\nthe image and extracts fine-grained information of this area. Experiments show\nPASS sets the new state-of-the-art performances on Market1501 and MSMT17 on\nvarious ReID tasks, e.g., vanilla ViT-S/16 pre-trained by PASS achieves\n92.2\\%/90.2\\%/88.5\\% mAP accuracy on Market1501 for supervised/UDA/USL ReID.\nOur codes are available at https://github.com/CASIA-IVA-Lab/PASS-reID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haiyun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tianyi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Temporal Consistency for Source-Free Video Domain Adaptation. (arXiv:2203.04559v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04559","description":"<p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the\nrobustness of video models, enabling them to be applied to action recognition\ntasks across different environments. However, these methods require constant\naccess to source data during the adaptation process. Yet in many real-world\napplications, subjects and scenes in the source video domain should be\nirrelevant to those in the target video domain. With the increasing emphasis on\ndata privacy, such methods that require source data access would raise serious\nprivacy issues. Therefore, to cope with such concern, a more practical domain\nadaptation scenario is formulated as the Source-Free Video-based Domain\nAdaptation (SFVDA). Though there are a few methods for Source-Free Domain\nAdaptation (SFDA) on image data, these methods yield degenerating performance\nin SFVDA due to the multi-modality nature of videos, with the existence of\nadditional temporal features. In this paper, we propose a novel Attentive\nTemporal Consistent Network (ATCoN) to address SFVDA by learning temporal\nconsistency, guaranteed by two novel consistency objectives, namely feature\nconsistency and source prediction consistency, performed across local temporal\nfeatures. ATCoN further constructs effective overall temporal features by\nattending to local temporal features based on prediction confidence. Empirical\nresults demonstrate the state-of-the-art performance of ATCoN across various\ncross-domain action recognition benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuecong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haozhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Keyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Wu Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity. (arXiv:2203.08101v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08101","description":"<p>An intuitive way to search for images is to use queries composed of an\nexample image and a complementary text. While the first provides rich and\nimplicit context for the search, the latter explicitly calls for new traits, or\nspecifies how some elements of the example image should be changed to retrieve\nthe desired target image. Current approaches typically combine the features of\neach of the two elements of the query into a single representation, which can\nthen be compared to the ones of the potential target images. Our work aims at\nshedding new light on the task by looking at it through the prism of two\nfamiliar and related frameworks: text-to-image and image-to-image retrieval.\nTaking inspiration from them, we exploit the specific relation of each query\nelement with the targeted image and derive light-weight attention mechanisms\nwhich enable to mediate between the two complementary modalities. We validate\nour approach on several retrieval benchmarks, querying with images and their\nassociated free-form text modifiers. Our method obtains state-of-the-art\nresults without resorting to side information, multi-level features, heavy\npre-training nor large architectures as in previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delmas_G/0/1/0/all/0/1\">Ginger Delmas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezende_R/0/1/0/all/0/1\">Rafael Sampaio de Rezende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csurka_G/0/1/0/all/0/1\">Gabriela Csurka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation with Active Semi-Supervised Learning. (arXiv:2203.10730v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10730","description":"<p>Using deep learning, we now have the ability to create exceptionally good\nsemantic segmentation systems; however, collecting the prerequisite pixel-wise\nannotations for training images remains expensive and time-consuming.\nTherefore, it would be ideal to minimize the number of human annotations needed\nwhen creating a new dataset. Here, we address this problem by proposing a novel\nalgorithm that combines active learning and semi-supervised learning. Active\nlearning is an approach for identifying the best unlabeled samples to annotate.\nWhile there has been work on active learning for segmentation, most methods\nrequire annotating all pixel objects in each image, rather than only the most\ninformative regions. We argue that this is inefficient. Instead, our active\nlearning approach aims to minimize the number of annotations per-image. Our\nmethod is enriched with semi-supervised learning, where we use pseudo labels\ngenerated with a teacher-student framework to identify image regions that help\ndisambiguate confused classes. We also integrate mechanisms that enable better\nperformance on imbalanced label distributions, which have not been studied\npreviously for active learning in semantic segmentation. In experiments on the\nCamVid and CityScapes datasets, our method obtains over 95% of the network's\nperformance on the full-training set using less than 17% of the training data,\nwhereas the previous state of the art required 40% of the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rangnekar_A/0/1/0/all/0/1\">Aneesh Rangnekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffman_M/0/1/0/all/0/1\">Matthew Hoffman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Multi-Object Tracking Using Graph Neural Networks with Cross-Edge Modality Attention. (arXiv:2203.10926v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10926","description":"<p>Online 3D multi-object tracking (MOT) has witnessed significant research\ninterest in recent years, largely driven by demand from the autonomous systems\ncommunity. However, 3D offline MOT is relatively less explored. Labeling 3D\ntrajectory scene data at a large scale while not relying on high-cost human\nexperts is still an open research question. In this work, we propose Batch3DMOT\nwhich follows the tracking-by-detection paradigm and represents real-world\nscenes as directed, acyclic, and category-disjoint tracking graphs that are\nattributed using various modalities such as camera, LiDAR, and radar. We\npresent a multi-modal graph neural network that uses a cross-edge attention\nmechanism mitigating modality intermittence, which translates into sparsity in\nthe graph domain. Additionally, we present attention-weighted convolutions over\nframe-wise k-NN neighborhoods as suitable means to allow information exchange\nacross disconnected graph components. We evaluate our approach using various\nsensor modalities and model configurations on the challenging nuScenes and\nKITTI datasets. Extensive experiments demonstrate that our proposed approach\nyields an overall improvement of 3.3% in the AMOTA score on nuScenes thereby\nsetting the new state-of-the-art for 3D tracking and further enhancing false\npositive filtering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buchner_M/0/1/0/all/0/1\">Martin Buchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Probability Sampling Network for Stochastic Human Trajectory Prediction. (arXiv:2203.13471v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13471","description":"<p>Capturing multimodal natures is essential for stochastic pedestrian\ntrajectory prediction, to infer a finite set of future trajectories. The\ninferred trajectories are based on observation paths and the latent vectors of\npotential decisions of pedestrians in the inference step. However, stochastic\napproaches provide varying results for the same data and parameter settings,\ndue to the random sampling of the latent vector. In this paper, we analyze the\nproblem by reconstructing and comparing probabilistic distributions from\nprediction samples and socially-acceptable paths, respectively. Through this\nanalysis, we observe that the inferences of all stochastic models are biased\ntoward the random sampling, and fail to generate a set of realistic paths from\nfinite samples. The problem cannot be resolved unless an infinite number of\nsamples is available, which is infeasible in practice. We introduce that the\nQuasi-Monte Carlo (QMC) method, ensuring uniform coverage on the sampling\nspace, as an alternative to the conventional random sampling. With the same\nfinite number of samples, the QMC improves all the multimodal prediction\nresults. We take an additional step ahead by incorporating a learnable sampling\nnetwork into the existing networks for trajectory prediction. For this purpose,\nwe propose the Non-Probability Sampling Network (NPSN), a very small network\n(~5K parameters) that generates purposive sample sequences using the past paths\nof pedestrians and their social interactions. Extensive experiments confirm\nthat NPSN can significantly improve both the prediction accuracy (up to 60%)\nand reliability of the public pedestrian trajectory prediction benchmark. Code\nis publicly available at https://github.com/inhwanbae/NPSN .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_I/0/1/0/all/0/1\">Inhwan Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jin-Hwi Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1\">Hae-Gon Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.01702","description":"<p>Precision medicine for chronic diseases such as multiple sclerosis (MS)\ninvolves choosing a treatment which best balances efficacy and side\neffects/preferences for individual patients. Making this choice as early as\npossible is important, as delays in finding an effective therapy can lead to\nirreversible disability accrual. To this end, we present the first deep neural\nnetwork model for individualized treatment decisions from baseline magnetic\nresonance imaging (MRI) (with clinical information if available) for MS\npatients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)\nlesion counts on follow-up MRI on multiple treatments and (b) estimates the\nconditional average treatment effect (CATE), as defined by the predicted future\nsuppression of NE-T2 lesions, between different treatment options relative to\nplacebo. Our model is validated on a proprietary federated dataset of 1817\nmulti-sequence MRIs acquired from MS patients during four multi-centre\nrandomized clinical trials. Our framework achieves high average precision in\nthe binarized regression of future NE-T2 lesions on five different treatments,\nidentifies heterogeneous treatment effects, and provides a personalized\ntreatment recommendation that accounts for treatment-associated risk (e.g. side\neffects, patient preference, administration difficulties).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Durso_Finley_J/0/1/0/all/0/1\">Joshua Durso-Finley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Falet_J/0/1/0/all/0/1\">Jean-Pierre R. Falet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight HDR Camera ISP for Robust Perception in Dynamic Illumination Conditions via Fourier Adversarial Networks. (arXiv:2204.01795v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01795","description":"<p>The limited dynamic range of commercial compact camera sensors results in an\ninaccurate representation of scenes with varying illumination conditions,\nadversely affecting image quality and subsequently limiting the performance of\nunderlying image processing algorithms. Current state-of-the-art (SoTA)\nconvolutional neural networks (CNN) are developed as post-processing techniques\nto independently recover under-/over-exposed images. However, when applied to\nimages containing real-world degradations such as glare, high-beam, color\nbleeding with varying noise intensity, these algorithms amplify the\ndegradations, further degrading image quality. We propose a lightweight\ntwo-stage image enhancement algorithm sequentially balancing illumination and\nnoise removal using frequency priors for structural guidance to overcome these\nlimitations. Furthermore, to ensure realistic image quality, we leverage the\nrelationship between frequency and spatial domain properties of an image and\npropose a Fourier spectrum-based adversarial framework (AFNet) for consistent\nimage enhancement under varying illumination conditions. While current\nformulations of image enhancement are envisioned as post-processing techniques,\nwe examine if such an algorithm could be extended to integrate the\nfunctionality of the Image Signal Processing (ISP) pipeline within the camera\nsensor benefiting from RAW sensor data and lightweight CNN architecture. Based\non quantitative and qualitative evaluations, we also examine the practicality\nand effects of image enhancement techniques on the performance of common\nperception tasks such as object detection and semantic segmentation in varying\nillumination conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shyam_P/0/1/0/all/0/1\">Pranjay Shyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengar_S/0/1/0/all/0/1\">Sandeep Singh Sengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Soo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Remote Sensing Pretraining. (arXiv:2204.02825v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02825","description":"<p>Deep learning has largely reshaped remote sensing (RS) research for aerial\nimage understanding and made a great success. Nevertheless, most of the\nexisting deep models are initialized with the ImageNet pretrained weights.\nSince natural images inevitably present a large domain gap relative to aerial\nimages, probably limiting the finetuning performance on downstream aerial scene\ntasks. This issue motivates us to conduct an empirical study of remote sensing\npretraining (RSP) on aerial images. To this end, we train different networks\nfrom scratch with the help of the largest RS scene recognition dataset up to\nnow -- MillionAID, to obtain a series of RS pretrained backbones, including\nboth convolutional neural networks (CNN) and vision transformers such as Swin\nand ViTAE, which have shown promising performance on computer vision tasks.\nThen, we investigate the impact of RSP on representative downstream tasks\nincluding scene recognition, semantic segmentation, object detection, and\nchange detection using these CNN and vision transformer backbones. Empirical\nstudy shows that RSP can help deliver distinctive performances in scene\nrecognition tasks and in perceiving RS related semantics such as \"Bridge\" and\n\"Airplane\". We also find that, although RSP mitigates the data discrepancies of\ntraditional ImageNet pretraining on RS images, it may still suffer from task\ndiscrepancies, where downstream tasks require different representations from\nscene recognition tasks. These findings call for further research efforts on\nboth large-scale pretraining datasets and effective pretraining methods. The\ncodes and pretrained models will be released at\nhttps://github.com/ViTAE-Transformer/ViTAE-Transformer-Remote-Sensing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels. (arXiv:2204.04905v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.04905","description":"<p>Vision Transformers (ViT) have recently demonstrated the significant\npotential of transformer architectures for computer vision. To what extent can\nimage-based deep reinforcement learning also benefit from ViT architectures, as\ncompared to standard convolutional neural network (CNN) architectures? To\nanswer this question, we evaluate ViT training methods for image-based\nreinforcement learning (RL) control tasks and compare these results to a\nleading convolutional-network architecture method, RAD. For training the ViT\nencoder, we consider several recently-proposed self-supervised losses that are\ntreated as auxiliary tasks, as well as a baseline with no additional loss\nterms. We find that the CNN architectures trained using RAD still generally\nprovide superior performance. For the ViT methods, all three types of auxiliary\ntasks that we consider provide a benefit over plain ViT training. Furthermore,\nViT reconstruction-based tasks are found to significantly outperform ViT\ncontrastive-learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianxin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reda_D/0/1/0/all/0/1\">Daniele Reda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1\">Michiel van de Panne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ONCE-3DLanes: Building Monocular 3D Lane Detection. (arXiv:2205.00301v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.00301","description":"<p>We present ONCE-3DLanes, a real-world autonomous driving dataset with lane\nlayout annotation in 3D space. Conventional 2D lane detection from a monocular\nimage yields poor performance of following planning and control tasks in\nautonomous driving due to the case of uneven road. Predicting the 3D lane\nlayout is thus necessary and enables effective and safe driving. However,\nexisting 3D lane detection datasets are either unpublished or synthesized from\na simulated environment, severely hampering the development of this field. In\nthis paper, we take steps towards addressing these issues. By exploiting the\nexplicit relationship between point clouds and image pixels, a dataset\nannotation pipeline is designed to automatically generate high-quality 3D lane\nlocations from 2D lane annotations in 211K road scenes. In addition, we present\nan extrinsic-free, anchor-free method, called SALAD, regressing the 3D\ncoordinates of lanes in image view without converting the feature map into the\nbird's-eye view (BEV). To facilitate future research on 3D lane detection, we\nbenchmark the dataset and provide a novel evaluation metric, performing\nextensive experiments of both existing approaches and our proposed method. The\naim of our work is to revive the interest of 3D lane detection in a real-world\nscenario. We believe our work can lead to the expected and unexpected\ninnovations in both academia and industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1\">Fan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1\">Ming Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xinyue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_M/0/1/0/all/0/1\">Michael Bi Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressive Ptychography using Deep Image and Generative Priors. (arXiv:2205.02397v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02397","description":"<p>Ptychography is a well-established coherent diffraction imaging technique\nthat enables non-invasive imaging of samples at a nanometer scale. It has been\nextensively used in various areas such as the defense industry or materials\nscience. One major limitation of ptychography is the long data acquisition time\ndue to mechanical scanning of the sample; therefore, approaches to reduce the\nscan points are highly desired. However, reconstructions with less number of\nscan points lead to imaging artifacts and significant distortions, hindering a\nquantitative evaluation of the results. To address this bottleneck, we propose\na generative model combining deep image priors with deep generative priors. The\nself-training approach optimizes the deep generative neural network to create a\nsolution for a given dataset. We complement our approach with a prior acquired\nfrom a previously trained discriminator network to avoid a possible divergence\nfrom the desired output caused by the noise in the measurements. We also\nsuggest using the total variation as a complementary before combat artifacts\ndue to measurement noise. We analyze our approach with numerical experiments\nthrough different probe overlap percentages and varying noise levels. We also\ndemonstrate improved reconstruction accuracy compared to the state-of-the-art\nmethod and discuss the advantages and disadvantages of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barutcu_S/0/1/0/all/0/1\">Semih Barutcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gursoy_D/0/1/0/all/0/1\">Do&#x11f;a G&#xfc;rsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsaggelos_A/0/1/0/all/0/1\">Aggelos K. Katsaggelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection. (arXiv:2205.02848v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.02848","description":"<p>Ischemic strokes are often caused by large vessel occlusions (LVOs), which\ncan be visualized and diagnosed with Computed Tomography Angiography scans. As\ntime is brain, a fast, accurate and automated diagnosis of these scans is\ndesirable. Human readers compare the left and right hemispheres in their\nassessment of strokes. A large training data set is required for a standard\ndeep learning-based model to learn this strategy from data. As labeled medical\ndata in this field is rare, other approaches need to be developed. To both\ninclude the prior knowledge of side comparison and increase the amount of\ntraining data, we propose an augmentation method that generates artificial\ntraining samples by recombining vessel tree segmentations of the hemispheres or\nhemisphere subregions from different patients. The subregions cover vessels\ncommonly affected by LVOs, namely the internal carotid artery (ICA) and middle\ncerebral artery (MCA). In line with the augmentation scheme, we use a\n3D-DenseNet fed with task-specific input, fostering a side-by-side comparison\nbetween the hemispheres. Furthermore, we propose an extension of that\narchitecture to process the individual hemisphere subregions. All\nconfigurations predict the presence of an LVO, its side, and the affected\nsubregion. We show the effect of recombination as an augmentation strategy in a\n5-fold cross validated ablation study. We enhanced the AUC for patient-wise\nclassification regarding the presence of an LVO of all investigated\narchitectures. For one variant, the proposed method improved the AUC from 0.73\nwithout augmentation to 0.89. The best configuration detects LVOs with an AUC\nof 0.91, LVOs in the ICA with an AUC of 0.96, and in the MCA with 0.91 while\naccurately predicting the affected side.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thamm_F/0/1/0/all/0/1\">Florian Thamm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taubmann_O/0/1/0/all/0/1\">Oliver Taubmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jurgens_M/0/1/0/all/0/1\">Markus J&#xfc;rgens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thamm_A/0/1/0/all/0/1\">Aleksandra Thamm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denzinger_F/0/1/0/all/0/1\">Felix Denzinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rist_L/0/1/0/all/0/1\">Leonhard Rist</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ditt_H/0/1/0/all/0/1\">Hendrik Ditt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HierAttn: Effectively Learn Representations from Stage Attention and Branch Attention for Skin Lesions Diagnosis. (arXiv:2205.04326v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.04326","description":"<p>Accurate and unbiased examinations of skin lesions are critical for early\ndiagnosis and treatment of skin conditions and disorders. Visual features of\nskin lesions vary significantly because the skin images are collected from\npatients with different skin colours by using dissimilar type of imaging\nequipment. Recent studies have reported ensembled convolutional neural networks\n(CNNs) to classify the images for early diagnosis of skin disorders. However,\nthe practical use of CNNs is limited because the majority of networks are\nheavyweight and inadequate to use the contextual information. Although\nlightweight networks (e.g., MobileNetV3 and EfficientNet) were developed to\nsave the computational cost for implementing deep neural networks on mobile\ndevices, not sufficient representation depth restricts their performance. To\naddress the limitations, we introduce a new light and effective neural network,\nnamely HierAttn network. The HierAttn applies a novel strategy to balance the\nlearning local and global features by using a multi-stage attention mechanism\nin a hierarchical architecture. The efficacy of HierAttn was evaluated by using\nthe dermoscopy images dataset ISIC2019 and smartphone photos dataset\nPAD-UFES-20. The experimental results show that HierAttn achieves the best\ntop-1 accuracy and AUC among the state-of-the-art light-weight networks. The\nnew light HierAttn network has the potential in promoting the use of deep\nlearning in clinics and allowing patients for early diagnosis of skin disorders\nwith personal devices. The code is available at\nhttps://github.com/anthonyweidai/HierAttn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activating More Pixels in Image Super-Resolution Transformer. (arXiv:2205.04437v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.04437","description":"<p>Transformer-based methods have shown impressive performance in low-level\nvision tasks, such as image super-resolution. However, we find that these\nnetworks can only utilize a limited spatial range of input information through\nattribution analysis. This implies that the potential of Transformer is still\nnot fully exploited in existing networks. In order to activate more input\npixels for reconstruction, we propose a novel Hybrid Attention Transformer\n(HAT). It combines channel attention and self-attention schemes, thus making\nuse of their complementary advantages. Moreover, to better aggregate the\ncross-window information, we introduce an overlapping cross-attention module to\nenhance the interaction between neighboring window features. In the training\nstage, we additionally propose a same-task pre-training strategy to bring\nfurther improvement. Extensive experiments show the effectiveness of the\nproposed modules, and the overall method significantly outperforms the\nstate-of-the-art methods by more than 1dB. Codes and models will be available\nat https://github.com/chxy95/HAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jiantao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reduce Information Loss in Transformers for Pluralistic Image Inpainting. (arXiv:2205.05076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05076","description":"<p>Transformers have achieved great success in pluralistic image inpainting\nrecently. However, we find existing transformer based solutions regard each\npixel as a token, thus suffer from information loss issue from two aspects: 1)\nThey downsample the input image into much lower resolutions for efficiency\nconsideration, incurring information loss and extra misalignment for the\nboundaries of masked regions. 2) They quantize $256^3$ RGB pixels to a small\nnumber (such as 512) of quantized pixels. The indices of quantized pixels are\nused as tokens for the inputs and prediction targets of transformer. Although\nan extra CNN network is used to upsample and refine the low-resolution results,\nit is difficult to retrieve the lost information back.To keep input information\nas much as possible, we propose a new transformer based framework \"PUT\".\nSpecifically, to avoid input downsampling while maintaining the computation\nefficiency, we design a patch-based auto-encoder P-VQVAE, where the encoder\nconverts the masked image into non-overlapped patch tokens and the decoder\nrecovers the masked regions from inpainted tokens while keeping the unmasked\nregions unchanged. To eliminate the information loss caused by quantization, an\nUn-Quantized Transformer (UQ-Transformer) is applied, which directly takes the\nfeatures from P-VQVAE encoder as input without quantization and regards the\nquantized tokens only as prediction targets. Extensive experiments show that\nPUT greatly outperforms state-of-the-art methods on image fidelity, especially\nfor large masked regions and complex large-scale datasets. Code is available at\nhttps://github.com/liuqk3/PUT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiankun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhentao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Continual Deepfake Detection Benchmark: Dataset, Methods, and Essentials. (arXiv:2205.05467v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05467","description":"<p>There have been emerging a number of benchmarks and techniques for the\ndetection of deepfakes. However, very few works study the detection of\nincrementally appearing deepfakes in the real-world scenarios. To simulate the\nwild scenes, this paper suggests a continual deepfake detection benchmark\n(CDDB) over a new collection of deepfakes from both known and unknown\ngenerative models. The suggested CDDB designs multiple evaluations on the\ndetection over easy, hard, and long sequence of deepfake tasks, with a set of\nappropriate measures. In addition, we exploit multiple approaches to adapt\nmulticlass incremental learning methods, commonly used in the continual visual\nrecognition, to the continual deepfake detection problem. We evaluate several\nmethods, including the adapted ones, on the proposed CDDB. Within the proposed\nbenchmark, we explore some commonly known essentials of standard continual\nlearning. Our study provides new insights on these essentials in the context of\ncontinual deepfake detection. The suggested CDDB is clearly more challenging\nthan the existing benchmarks, which thus offers a suitable evaluation avenue to\nthe future research. Our benchmark dataset and the source code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiwu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahbazi_M/0/1/0/all/0/1\">Mohamad Shahbazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_X/0/1/0/all/0/1\">Xiaopeng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Teaching Independent Parts Separately\"(TIPSy-GAN) : Improving Accuracy and Stability in Unsupervised Adversarial 2D to 3D Human Pose Estimation. (arXiv:2205.05980v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05980","description":"<p>We present TIPSy-GAN, a new approach to improve the accuracy and stability in\nunsupervised adversarial 2D to 3D human pose estimation. In our work we\ndemonstrate that the human kinematic skeleton should not be assumed as one\nspatially codependent structure. In fact, we believe when a full 2D pose is\nprovided during training, there is an inherent bias learned where the 3D\ncoordinate of a keypoint is spatially codependent on the 2D locations of all\nother keypoints. To investigate our theory we follow previous adversarial\napproaches but train two generators on spatially independent parts of the\nkinematic skeleton, the torso and the legs. We find that improving the 2D\nreprojection self-consistency cycle is key to lowering the evaluation error and\ntherefore introduce new consistency constraints during training. A TIPSy is\nproduced model via knowledge distillation from these generators which can\npredict the 3D coordinates for the entire 2D pose with improved results.\nFurthermore, we address the question left unanswered in prior work detailing\nhow long to train for a truly unsupervised scenario. We show that two\nindependent generators training adversarially has improved stability than that\nof a solo generator which will collapse due to the adversarial network becoming\nunstable. TIPSy decreases the average error by 18% when compared to that of a\nbaseline solo generator. TIPSy improves upon other unsupervised approaches\nwhile also performing strongly against supervised and weakly-supervised\napproaches during evaluation on both the Human3.6M and MPI-INF-3DHP dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardy_P/0/1/0/all/0/1\">Peter Hardy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasmahapatra_S/0/1/0/all/0/1\">Srinandan Dasmahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hansung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELODI: Ensemble Logit Difference Inhibition for Positive-Congruent Training. (arXiv:2205.06265v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.06265","description":"<p>Negative flips are errors introduced in a classification system when a legacy\nmodel is replaced with a new one. Existing methods to reduce the negative flip\nrate (NFR) either do so at the expense of overall accuracy using model\ndistillation, or use ensembles, which multiply inference cost prohibitively. We\npresent a method to train a classification system that achieves paragon\nperformance in both error rate and NFR, at the inference cost of a single\nmodel. Our method introduces a generalized distillation objective, Logit\nDifference Inhibition (LDI), that penalizes changes in the logits between the\nnew and old model, without forcing them to coincide as in ordinary\ndistillation. LDI affords the model flexibility to reduce error rate along with\nNFR. The method uses a homogeneous ensemble as the reference model for LDI,\nhence the name Ensemble LDI, or ELODI. The reference model can then be\nsubstituted with a single model at inference time. The method leverages the\nobservation that negative flips are typically not close to the decision\nboundary, but often exhibit large deviations in the distance among their\nlogits, which are reduced by ELODI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yantao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhuowen Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Human Digitization via Implicit Re-projection Networks. (arXiv:2205.06468v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06468","description":"<p>We present an approach to generating 3D human models from images. The key to\nour framework is that we predict double-sided orthographic depth maps and color\nimages from a single perspective projected image. Our framework consists of\nthree networks. The first network predicts normal maps to recover geometric\ndetails such as wrinkles in the clothes and facial regions. The second network\npredicts shade-removed images for the front and back views by utilizing the\npredicted normal maps. The last multi-headed network takes both normal maps and\nshade-free images and predicts depth maps while selectively fusing photometric\nand geometric information through multi-headed attention gates. Experimental\nresults demonstrate that our method shows visually plausible results and\ncompetitive performance in terms of various evaluation metrics over\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Min-Gyu Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Ju-Mi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Je Woo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Ju Hong Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-16T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}