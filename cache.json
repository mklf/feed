{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Linearizing Transformer with Key-Value Memory Bank. (arXiv:2203.12644v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12644","description":"<p>Transformer has brought great success to a wide range of natural language\nprocessing tasks. Nevertheless, the computational overhead of the vanilla\ntransformer scales quadratically with sequence length. Many efforts have been\nmade to develop more efficient transformer variants. A line of work (e.g.,\nLinformer) projects the input sequence into a low-rank space, achieving linear\ntime complexity. However, Linformer does not suit well for text generation\ntasks as the sequence length must be pre-specified. We propose MemSizer, an\napproach also projects the source sequence into lower dimension representation\nbut can take input with dynamic length, with a different perspective of the\nattention mechanism. MemSizer not only achieves the same linear time complexity\nbut also enjoys efficient recurrent-style autoregressive generation, which\nyields constant memory complexity and reduced computation at inference. We\ndemonstrate that MemSizer provides an improved tradeoff between efficiency and\naccuracy over the vanilla transformer and other linear variants in language\nmodeling and machine translation tasks, revealing a viable direction towards\nfurther inference efficiency improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12667","description":"<p>A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefani_E/0/1/0/all/0/1\">Eliana Stefani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Label Is Better Than Human Label. (arXiv:2203.12668v1 [cs.LG])","link":"http://arxiv.org/abs/2203.12668","description":"<p>State-of-the-art automatic speech recognition (ASR) systems are trained with\ntens of thousands of hours of labeled speech data. Human transcription is\nexpensive and time consuming. Factors such as the quality and consistency of\nthe transcription can greatly affect the performance of the ASR models trained\nwith these data. In this paper, we show that we can train a strong teacher\nmodel to produce high quality pseudo labels by utilizing recent self-supervised\nand semi-supervised learning techniques. Specifically, we use JUST (Joint\nUnsupervised/Supervised Training) and iterative noisy student teacher training\nto train a 600 million parameter bi-directional teacher model. This model\nachieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively\nbetter than a baseline. We further show that by using this strong teacher model\nto generate high-quality pseudo labels for training, we can achieve 13.6%\nrelative WER reduction (5.9% to 5.1%) for a streaming model compared to using\nhuman labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation. (arXiv:2203.12709v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12709","description":"<p>Neural language models show vulnerability to adversarial examples which are\nsemantically similar to their original counterparts with a few words replaced\nby their synonyms. A common way to improve model robustness is adversarial\ntraining which follows two steps-collecting adversarial examples by attacking a\ntarget model, and fine-tuning the model on the augmented dataset with these\nadversarial examples. The objective of traditional adversarial training is to\nmake a model produce the same correct predictions on an original/adversarial\nexample pair. However, the consistency between model decision-makings on two\nsimilar texts is ignored. We argue that a robust model should behave\nconsistently on original/adversarial example pairs, that is making the same\npredictions (what) based on the same reasons (how) which can be reflected by\nconsistent interpretations. In this work, we propose a novel feature-level\nadversarial training method named FLAT. FLAT aims at improving model robustness\nin terms of both predictions and interpretations. FLAT incorporates variational\nword masks in neural networks to learn global word importance and play as a\nbottleneck teaching the model to make predictions based on important words.\nFLAT explicitly shoots at the vulnerability problem caused by the mismatch\nbetween model understandings on the replaced words and their synonyms in\noriginal/adversarial example pairs by regularizing the corresponding global\nword importance scores. Experiments show the effectiveness of FLAT in improving\nthe robustness with respect to both predictions and interpretations of four\nneural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks\non four text classification tasks. The models trained via FLAT also show better\nrobustness than baseline models on unforeseen adversarial examples across\ndifferent attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThingTalk: An Extensible, Executable Representation Language for Task-Oriented Dialogues. (arXiv:2203.12751v1 [cs.PL])","link":"http://arxiv.org/abs/2203.12751","description":"<p>Task-oriented conversational agents rely on semantic parsers to translate\nnatural language to formal representations. In this paper, we propose the\ndesign and rationale of the ThingTalk formal representation, and how the design\nimproves the development of transactional task-oriented agents.\n</p>\n<p>ThingTalk is built on four core principles: (1) representing user requests\ndirectly as executable statements, covering all the functionality of the agent,\n(2) representing dialogues formally and succinctly to support accurate\ncontextual semantic parsing, (3) standardizing types and interfaces to maximize\nreuse between agents, and (4) allowing multiple, independently-developed agents\nto be composed in a single virtual assistant. ThingTalk is developed as part of\nthe Genie Framework that allows developers to quickly build transactional\nagents given a database and APIs.\n</p>\n<p>We compare ThingTalk to existing representations: SMCalFlow, SGD, TreeDST.\nCompared to the others, the ThingTalk design is both more general and more\ncost-effective. Evaluated on the MultiWOZ benchmark, using ThingTalk and\nassociated tools yields a new state of the art accuracy of 79% turn-by-turn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1\">Monica S. Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campagna_G/0/1/0/all/0/1\">Giovanni Campagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semnani_S/0/1/0/all/0/1\">Sina J. Semnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Silei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Cyber-Risky Clinical Notes by Employing Natural Language Processing. (arXiv:2203.12781v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12781","description":"<p>Clinical notes, which can be embedded into electronic medical records,\ndocument patient care delivery and summarize interactions between healthcare\nproviders and patients. These clinical notes directly inform patient care and\ncan also indirectly inform research and quality/safety metrics, among other\nindirect metrics. Recently, some states within the United States of America\nrequire patients to have open access to their clinical notes to improve the\nexchange of patient information for patient care. Thus, developing methods to\nassess the cyber risks of clinical notes before sharing and exchanging data is\ncritical. While existing natural language processing techniques are geared to\nde-identify clinical notes, to the best of our knowledge, few have focused on\nclassifying sensitive-information risk, which is a fundamental step toward\ndeveloping effective, widespread protection of patient health information. To\nbridge this gap, this research investigates methods for identifying\nsecurity/privacy risks within clinical notes. The classification either can be\nused upstream to identify areas within notes that likely contain sensitive\ninformation or downstream to improve the identification of clinical notes that\nhave not been entirely de-identified. We develop several models using unigram\nand word2vec features with different classifiers to categorize sentence risk.\nExperiments on i2b2 de-identification dataset show that the SVM classifier\nusing word2vec features obtained a maximum F1-score of 0.792. Future research\ninvolves articulation and differentiation of risk in terms of different global\nregulatory requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmeelk_S/0/1/0/all/0/1\">Suzanna Schmeelk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogo_M/0/1/0/all/0/1\">Martins Samuel Dogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_B/0/1/0/all/0/1\">Braja Gopal Patra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Distributional Distortion in Neural Language Modeling. (arXiv:2203.12788v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12788","description":"<p>A fundamental characteristic of natural language is the high rate at which\nspeakers produce novel expressions. Because of this novelty, a heavy-tail of\nrare events accounts for a significant amount of the total probability mass of\ndistributions in language (Baayen, 2001). Standard language modeling metrics\nsuch as perplexity quantify the performance of language models (LM) in\naggregate. As a result, we have relatively little understanding of whether\nneural LMs accurately estimate the probability of sequences in this heavy-tail\nof rare events. To address this gap, we develop a controlled evaluation scheme\nwhich uses generative models trained on natural data as artificial languages\nfrom which we can exactly compute sequence probabilities. Training LMs on\ngenerations from these artificial languages, we compare the sequence-level\nprobability estimates given by LMs to the true probabilities in the target\nlanguage. Our experiments reveal that LSTM and Transformer language models (i)\nsystematically underestimate the probability of sequences drawn from the target\nlanguage, and (ii) do so more severely for less-probable sequences.\nInvestigating where this probability mass went, (iii) we find that LMs tend to\noverestimate the probability of ill formed (perturbed) sequences. In addition,\nwe find that this underestimation behaviour (iv) is weakened, but not\neliminated by greater amounts of training data, and (v) is exacerbated for\ntarget distributions with lower entropy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+LeBrun_B/0/1/0/all/0/1\">Benjamin LeBrun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangleing Content and Fine-grained Prosody Information via Hybrid ASR Bottleneck Features for Voice Conversion. (arXiv:2203.12813v1 [cs.SD])","link":"http://arxiv.org/abs/2203.12813","description":"<p>Non-parallel data voice conversion (VC) have achieved considerable\nbreakthroughs recently through introducing bottleneck features (BNFs) extracted\nby the automatic speech recognition(ASR) model. However, selection of BNFs have\na significant impact on VC result. For example, when extracting BNFs from ASR\ntrained with Cross Entropy loss (CE-BNFs) and feeding into neural network to\ntrain a VC system, the timbre similarity of converted speech is significantly\ndegraded. If BNFs are extracted from ASR trained using Connectionist Temporal\nClassification loss (CTC-BNFs), the naturalness of the converted speech may\ndecrease. This phenomenon is caused by the difference of information contained\nin BNFs. In this paper, we proposed an any-to-one VC method using hybrid\nbottleneck features extracted from CTC-BNFs and CE-BNFs to complement each\nother advantages. Gradient reversal layer and instance normalization were used\nto extract prosody information from CE-BNFs and content information from\nCTC-BNFs. Auto-regressive decoder and Hifi-GAN vocoder were used to generate\nhigh-quality waveform. Experimental results show that our proposed method\nachieves higher similarity, naturalness, quality than baseline method and\nreveals the differences between the information contained in CE-BNFs and\nCTC-BNFs as well as the influence they have on the converted speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xintao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Changhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Shiyin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_D/0/1/0/all/0/1\">Deyi Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Effects of Leakage on Dependency Parsing. (arXiv:2203.12815v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12815","description":"<p>Recent work by S{\\o}gaard (2020) showed that, treebank size aside, overlap\nbetween training and test graphs (termed leakage) explains more of the observed\nvariation in dependency parsing performance than other explanations. In this\nwork we revisit this claim, testing it on more models and languages. We find\nthat it only holds for zero-shot cross-lingual settings. We then propose a more\nfine-grained measure of such leakage which, unlike the original measure, not\nonly explains but also correlates with observed performance variation. Code and\ndata are available here: https://github.com/miriamwanner/reu-nlp-project\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krasner_N/0/1/0/all/0/1\">Nathaniel Krasner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanner_M/0/1/0/all/0/1\">Miriam Wanner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12865","description":"<p>The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation\nof NLP systems has revealed high failure rates for basic capabilities for\nmultiple state-of-the-art and commercial models. However, the CheckList\ncreation process is manual which creates a bottleneck towards creation of\nmultilingual CheckLists catering 100s of languages. In this work, we explore\nmultiple approaches to generate and evaluate the quality of Multilingual\nCheckList. We device an algorithm -- Automated Multilingual Checklist\nGeneration (AMCG) for automatically transferring a CheckList from a source to a\ntarget language that relies on a reasonable machine translation system. We then\ncompare the CheckList generated by AMCG with CheckLists generated with\ndifferent levels of human intervention. Through in-depth crosslingual\nexperiments between English and Hindi, and broad multilingual experiments\nspanning 11 languages, we show that the automatic approach can provide accurate\nestimates of failure rates of a model across capabilities, as would a\nhuman-verified CheckList, and better than CheckLists generated by humans from\nscratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karthikeyan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pankaj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1\">Somak Aditya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_M/0/1/0/all/0/1\">Monojit Choudhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?. (arXiv:2203.12881v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12881","description":"<p>Identifying argument components from unstructured texts and predicting the\nrelationships expressed among them are two primary steps of argument mining.\nThe intrinsic complexity of these tasks demands powerful learning models. While\npretrained Transformer-based Language Models (LM) have been shown to provide\nstate-of-the-art results over different NLP tasks, the scarcity of manually\nannotated data and the highly domain-dependent nature of argumentation restrict\nthe capabilities of such models. In this work, we propose a novel transfer\nlearning strategy to overcome these challenges. We utilize argumentation-rich\nsocial discussions from the ChangeMyView subreddit as a source of unsupervised,\nargumentative discourse-aware knowledge by finetuning pretrained LMs on a\nselectively masked language modeling task. Furthermore, we introduce a novel\nprompt-based strategy for inter-component relation prediction that compliments\nour proposed finetuning method while leveraging on the discourse context.\nExhaustive experiments show the generalization capability of our method on\nthese two tasks over within-domain as well as out-of-domain datasets,\noutperforming several existing and employed strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Subhabrata Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juneja_J/0/1/0/all/0/1\">Jeevesh Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipankar Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Speech recognition for Speech Assessment of Preschool Children. (arXiv:2203.12886v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12886","description":"<p>The acoustic and linguistic features of preschool speech are investigated in\nthis study to design an automated speech recognition (ASR) system. Acoustic\nfluctuation has been highlighted as a significant barrier to developing\nhigh-performance ASR applications for youngsters. Because of the epidemic,\npreschool speech assessment should be conducted online. Accordingly, there is a\nneed for an automatic speech recognition system. We were confronted with new\nchallenges in our cognitive system, including converting meaningless words from\nspeech to text and recognizing word sequence. After testing and experimenting\nwith several models we obtained a 3.1\\% phoneme error rate in Persian. Wav2Vec\n2.0 is a paradigm that could be used to build a robust end-to-end speech\nrecognition system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_F/0/1/0/all/0/1\">Fatemeh Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some benchmarks. (arXiv:2203.12906v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12906","description":"<p>The Donate Speech campaign has so far succeeded in gathering approximately\n3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta\n(Donate Speech) corpus. The corpus includes over twenty thousand speakers from\nall the regions of Finland and from all age brackets. The primary goals of the\ncollection were to create a representative, large-scale resource to study\nspontaneous spoken Finnish and to accelerate the development of language\ntechnology and speech-based services. In this paper, we present the collection\nprocess and the collected corpus, and showcase its versatility through multiple\nuse cases. The evaluated use cases include: automatic speech recognition of\nspontaneous speech, detection of age, gender, dialect and topic and metadata\nanalysis. We provide benchmarks for the use cases, as well down loadable,\ntrained baseline systems with open-source code for reproducibility. One further\nuse case is to verify the metadata and transcripts given in this corpus itself,\nand to suggest artificial metadata and transcripts for the part of the corpus\nwhere it is missing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moisio_A/0/1/0/all/0/1\">Anssi Moisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porjazovski_D/0/1/0/all/0/1\">Dejan Porjazovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhe_A/0/1/0/all/0/1\">Aku Rouhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getman_Y/0/1/0/all/0/1\">Yaroslav Getman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virkkunen_A/0/1/0/all/0/1\">Anja Virkkunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosz_T/0/1/0/all/0/1\">Tam&#xe1;s Gr&#xf3;sz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linden_K/0/1/0/all/0/1\">Krister Lind&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurimo_M/0/1/0/all/0/1\">Mikko Kurimo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named Entity Recognition. (arXiv:2203.12907v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12907","description":"<p>Named entity recognition (NER) is the process of recognising and classifying\nimportant information (entities) in text. Proper nouns, such as a person's\nname, an organization's name, or a location's name, are examples of entities.\nThe NER is one of the important modules in applications like human resources,\ncustomer support, search engines, content classification, and academia. In this\nwork, we consider NER for low-resource Indian languages like Hindi and Marathi.\nThe transformer-based models have been widely used for NER tasks. We consider\ndifferent variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark\nthem on publicly available Hindi and Marathi NER datasets. We provide an\nexhaustive comparison of different monolingual and multilingual\ntransformer-based models and establish simple baselines currently missing in\nthe literature. We show that the monolingual MahaRoBERTa model performs the\nbest for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for\nHindi NER. We also perform cross-language evaluation and present mixed\nobservations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1\">Onkar Litake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabane_M/0/1/0/all/0/1\">Maithili Sabane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_P/0/1/0/all/0/1\">Parth Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranade_A/0/1/0/all/0/1\">Aparna Ranade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Rationale-Centric Framework for Human-in-the-loop Machine Learning. (arXiv:2203.12918v1 [cs.AI])","link":"http://arxiv.org/abs/2203.12918","description":"<p>We present a novel rationale-centric framework with human-in-the-loop --\nRationales-centric Double-robustness Learning (RDL) -- to boost model\nout-of-distribution performance in few-shot learning scenarios. By using static\nsemi-factual generation and dynamic human-intervened correction, RDL exploits\nrationales (i.e. phrases that cause the prediction), human interventions and\nsemi-factual augmentations to decouple spurious associations and bias models\ntowards generally applicable underlying distributions, which enables fast and\naccurate generalisation. Experimental results show that RDL leads to\nsignificant prediction benefits on both in-distribution and out-of-distribution\ntests compared to many state-of-the-art benchmarks -- especially for few-shot\nlearning scenarios. We also perform extensive ablation studies to support\nin-depth analyses of each component in our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinghui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1\">Brian Mac Namee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitasking Framework for Unsupervised Simple Definition Generation. (arXiv:2203.12926v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12926","description":"<p>The definition generation task can help language learners by providing\nexplanations for unfamiliar words. This task has attracted much attention in\nrecent years. We propose a novel task of Simple Definition Generation (SDG) to\nhelp language learners and low literacy readers. A significant challenge of\nthis task is the lack of learner's dictionaries in many languages, and\ntherefore the lack of data for supervised training. We explore this task and\npropose a multitasking framework SimpDefiner that only requires a standard\ndictionary with complex definitions and a corpus containing arbitrary simple\ntexts. We disentangle the complexity factors from the text by carefully\ndesigning a parameter sharing scheme between two decoders. By jointly training\nthese components, the framework can generate both complex and simple\ndefinitions simultaneously. We demonstrate that the framework can generate\nrelevant, simple definitions for the target words through automatic and manual\nevaluations on English and Chinese datasets. Our method outperforms the\nbaseline model by a 1.77 SARI score on the English dataset, and raises the\nproportion of the low level (HSK level 1-3) words in Chinese definitions by\n3.87%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Cunliang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hengyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liner Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling. (arXiv:2203.12940v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12940","description":"<p>Zero-shot slot filling has received considerable attention to cope with the\nproblem of limited available data for the target domain. One of the important\nfactors in zero-shot learning is to make the model learn generalized and\nreliable representations. For this purpose, we present mcBERT, which stands for\nmomentum contrastive learning with BERT, to develop a robust zero-shot slot\nfilling model. mcBERT uses BERT to initialize the two encoders, the query\nencoder and key encoder, and is trained by applying momentum contrastive\nlearning. Our experimental results on the SNIPS benchmark show that mcBERT\nsubstantially outperforms the previous models, recording a new\nstate-of-the-art. Besides, we also show that each component composing mcBERT\ncontributes to the performance improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1\">Seong-Hwan Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">WonKee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Hyeok Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets. (arXiv:2203.12942v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12942","description":"<p>Natural language processing models often exploit spurious correlations\nbetween task-independent features and labels in datasets to perform well only\nwithin the distributions they are trained on, while not generalising to\ndifferent task distributions. We propose to tackle this problem by generating a\ndebiased version of a dataset, which can then be used to train a debiased,\noff-the-shelf model, by simply replacing its training data. Our approach\nconsists of 1) a method for training data generators to generate high-quality,\nlabel-consistent data samples; and 2) a filtering mechanism for removing data\npoints that contribute to spurious correlations, measured in terms of\nz-statistics. We generate debiased versions of the SNLI and MNLI datasets, and\nwe evaluate on a large suite of debiased, out-of-distribution, and adversarial\ntest sets. Results show that models trained on our debiased datasets generalise\nbetter than those trained on the original datasets in all settings. On the\nmajority of the datasets, our method outperforms or performs comparably to\nprevious state-of-the-art debiasing strategies, and when combined with an\northogonal technique, product-of-experts, it improves further and outperforms\nprevious best results of SNLI-hard and MNLI-hard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duality-Induced Regularizer for Semantic Matching Knowledge Graph Embeddings. (arXiv:2203.12949v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12949","description":"<p>Semantic matching models -- which assume that entities with similar semantics\nhave similar embeddings -- have shown great power in knowledge graph embeddings\n(KGE). Many existing semantic matching models use inner products in embedding\nspaces to measure the plausibility of triples and quadruples in static and\ntemporal knowledge graphs. However, vectors that have the same inner products\nwith another vector can still be orthogonal to each other, which implies that\nentities with similar semantics may have dissimilar embeddings. This property\nof inner products significantly limits the performance of semantic matching\nmodels. To address this challenge, we propose a novel regularizer -- namely,\nDUality-induced RegulArizer (DURA) -- which effectively encourages the entities\nwith similar semantics to have similar embeddings. The major novelty of DURA is\nbased on the observation that, for an existing semantic matching KGE model\n(primal), there is often another distance based KGE model (dual) closely\nassociated with it, which can be used as effective constraints for entity\nembeddings. Experiments demonstrate that DURA consistently and significantly\nimproves the performance of state-of-the-art semantic matching models on both\nstatic and temporal knowledge graph benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhanqiu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhihao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing for Labeled Dependency Trees. (arXiv:2203.12971v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12971","description":"<p>Probing has become an important tool for analyzing representations in Natural\nLanguage Processing (NLP). For graphical NLP tasks such as dependency parsing,\nlinear probes are currently limited to extracting undirected or unlabeled parse\ntrees which do not capture the full task. This work introduces DepProbe, a\nlinear probe which can extract labeled and directed dependency parse trees from\nembeddings while using fewer parameters and compute than prior methods.\nLeveraging its full task coverage and lightweight parametrization, we\ninvestigate its predictive power for selecting the best transfer language for\ntraining a full biaffine attention parser. Across 13 languages, our proposed\nmethod identifies the best source treebank 94% of the time, outperforming\ncompetitive baselines and prior work. Finally, we analyze the informativeness\nof task-specific subspaces in contextual embeddings as well as which benefits a\nfull parser's non-linear parametrization provides.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Scientific Claims for Zero-Shot Scientific Fact Checking. (arXiv:2203.12990v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12990","description":"<p>Automated scientific fact checking is difficult due to the complexity of\nscientific language and a lack of significant amounts of training data, as\nannotation requires domain expertise. To address this challenge, we propose\nscientific claim generation, the task of generating one or more atomic and\nverifiable claims from scientific sentences, and demonstrate its usefulness in\nzero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new\nsupervised method for generating claims supported by the literature, as well as\nKBIN, a novel method for generating claim negations. Additionally, we adapt an\nexisting unsupervised entity-centric method of claim generation to biomedical\nclaims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking\ndemonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN,\nachieve up to 90% performance of fully supervised models trained on manually\nannotated claims and evidence. A rigorous evaluation study demonstrates\nsignificant improvement in generated claim and negation quality over existing\nbaselines\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1\">Dustin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kratt: Developing an Automatic Subject Indexing Tool for The National Library of Estonia. (arXiv:2203.12998v1 [cs.CL])","link":"http://arxiv.org/abs/2203.12998","description":"<p>Manual subject indexing in libraries is a time-consuming and costly process\nand the quality of the assigned subjects is affected by the cataloguer's\nknowledge on the specific topics contained in the book. Trying to solve these\nissues, we exploited the opportunities arising from artificial intelligence to\ndevelop Kratt: a prototype of an automatic subject indexing tool. Kratt is able\nto subject index a book independent of its extent and genre with a set of\nkeywords present in the Estonian Subject Thesaurus. It takes Kratt\napproximately 1 minute to subject index a book, outperforming humans 10-15\ntimes. Although the resulting keywords were not considered satisfactory by the\ncataloguers, the ratings of a small sample of regular library users showed more\npromise. We also argue that the results can be enhanced by including a bigger\ncorpus for training the model and applying more careful preprocessing\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asula_M/0/1/0/all/0/1\">Marit Asula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makke_J/0/1/0/all/0/1\">Jane Makke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freienthal_L/0/1/0/all/0/1\">Linda Freienthal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuulmets_H/0/1/0/all/0/1\">Hele-Andra Kuulmets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirel_R/0/1/0/all/0/1\">Raul Sirel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction. (arXiv:2203.13064v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13064","description":"<p>In this paper, we investigate improvements to the GEC sequence tagging\narchitecture with a focus on ensembling of recent cutting-edge\nTransformer-based encoders in Large configurations. We encourage ensembling\nmodels by majority votes on span-level edits because this approach is tolerant\nto the model architecture and vocabulary size. Our best ensemble achieves a new\nSOTA result with an $F_{0.5}$ score of 76.05 on BEA-2019 (test), even without\npre-training on synthetic datasets. In addition, we perform knowledge\ndistillation with a trained ensemble to generate new synthetic training\ndatasets, \"Troy-Blogs\" and \"Troy-1BW\". Our best single sequence tagging model\nthat is pretrained on the generated Troy-datasets in combination with the\npublicly available synthetic PIE dataset achieves a near-SOTA (To the best of\nour knowledge, our best single model gives way only to much heavier T5 model\nresult with an $F_{0.5}$ score of 73.21 on BEA-2019 (test). The code, datasets,\nand trained models are publicly available).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarnavskyi_M/0/1/0/all/0/1\">Maksym Tarnavskyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernodub_A/0/1/0/all/0/1\">Artem Chernodub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omelianchuk_K/0/1/0/all/0/1\">Kostiantyn Omelianchuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction. (arXiv:2203.13088v1 [cs.IR])","link":"http://arxiv.org/abs/2203.13088","description":"<p>Recent progress in neural information retrieval has demonstrated large gains\nin effectiveness, while often sacrificing the efficiency and interpretability\nof the neural model compared to classical approaches. This paper proposes\nColBERTer, a neural retrieval model using contextualized late interaction\n(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,\nColBERTer's reductions dramatically lower ColBERT's storage requirements while\nsimultaneously improving the interpretability of its token-matching scores. To\nthis end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and\noptional lexical matching components into one model. For its multi-vector\ncomponent, ColBERTer reduces the number of stored vectors per document by\nlearning unique whole-word representations for the terms in each document and\nlearning to identify and remove word representations that are not essential to\neffective scoring. We employ an explicit multi-task, multi-stage training to\nfacilitate using very small vector dimensions. Results on the MS MARCO and\nTREC-DL collection show that ColBERTer can reduce the storage footprint by up\nto 2.5x, while maintaining effectiveness. With just one dimension per token in\nits smallest setting, ColBERTer achieves index storage parity with the\nplaintext size, with very strong effectiveness results. Finally, we demonstrate\nColBERTer's robustness on seven high-quality out-of-domain collections,\nyielding statistically significant gains over traditional retrieval baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hofstatter_S/0/1/0/all/0/1\">Sebastian Hofst&#xe4;tter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1\">Sophia Althammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sertkan_M/0/1/0/all/0/1\">Mete Sertkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models. (arXiv:2203.13112v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13112","description":"<p>We present minicons, an open source library that provides a standard API for\nresearchers interested in conducting behavioral and representational analyses\nof transformer-based language models (LMs). Specifically, minicons enables\nresearchers to apply analysis methods at two levels: (1) at the prediction\nlevel -- by providing functions to efficiently extract word/sentence level\nprobabilities; and (2) at the representational level -- by also facilitating\nefficient extraction of word/phrase level vectors from one or more layers. In\nthis paper, we describe the library and apply it to two motivating case\nstudies: One focusing on the learning dynamics of the BERT architecture on\nrelative grammatical judgments, and the other on benchmarking 23 different LMs\non zero-shot abductive reasoning. minicons is available at\nhttps://github.com/kanishkamisra/minicons\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1\">Kanishka Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. (arXiv:2203.13131v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13131","description":"<p>Recent text-to-image generation methods provide a simple yet exciting\nconversion capability between text and image domains. While these methods have\nincrementally improved the generated image fidelity and text relevancy, several\npivotal gaps remain unanswered, limiting applicability and quality. We propose\na novel text-to-image method that addresses these gaps by (i) enabling a simple\ncontrol mechanism complementary to text in the form of a scene, (ii)\nintroducing elements that substantially improve the tokenization process by\nemploying domain-specific knowledge over key image regions (faces and salient\nobjects), and (iii) adapting classifier-free guidance for the transformer use\ncase. Our model achieves state-of-the-art FID and human evaluation results,\nunlocking the ability to generate high fidelity images in a resolution of\n512x512 pixels, significantly improving visual quality. Through scene\ncontrollability, we introduce several new capabilities: (i) Scene editing, (ii)\ntext editing with anchor scenes, (iii) overcoming out-of-distribution text\nprompts, and (iv) story illustration generation, as demonstrated in the story\nwe wrote.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gafni_O/0/1/0/all/0/1\">Oran Gafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashual_O/0/1/0/all/0/1\">Oron Ashual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheynin_S/0/1/0/all/0/1\">Shelly Sheynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taigman_Y/0/1/0/all/0/1\">Yaniv Taigman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13151","description":"<p>Transformer-based language models (TLMs) provide state-of-the-art performance\nin many modern natural language processing applications. TLM training is\nconducted in two phases. First, the model is pre-trained over large volumes of\ntext to minimize a generic objective function, such as the Masked Language\nModel (MLM). Second, the model is fine-tuned in specific downstream tasks.\nPre-training requires large volumes of data and high computational resources,\nwhile introducing many still unresolved design choices. For instance, selecting\nhyperparameters for language model pre-training is often carried out based on\nheuristics or grid-based searches. In this work, we propose a multi-armed\nbandit-based online optimization framework for the sequential selection of\npre-training hyperparameters to optimize language model performance. We pose\nthe pre-training procedure as a sequential decision-making task, where at each\npre-training step, an agent must determine what hyperparameters to use towards\noptimizing the pre-training objective. We propose a Thompson sampling bandit\nalgorithm, based on a surrogate Gaussian process reward model of the MLM\npre-training objective, for its sequential minimization. We empirically show\nhow the proposed Gaussian process based Thompson sampling pre-trains robust and\nwell-performing language models. Namely, by sequentially selecting masking\nhyperparameters of the TLM, we achieve satisfactory performance in less epochs,\nnot only in terms of the pre-training MLM objective, but in diverse downstream\nfine-tuning tasks. The proposed bandit-based technique provides an automated\nhyperparameter selection method for pre-training TLMs of interest to\npractitioners. In addition, our results indicate that, instead of MLM\npre-training with fixed masking probabilities, sequentially adapting the\nmasking hyperparameters improves both pre-training loss and downstream task\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Urteaga_I/0/1/0/all/0/1\">I&#xf1;igo Urteaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Draidia_M/0/1/0/all/0/1\">Moulay-Za&#xef;dane Dra&#xef;dia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1\">Tomer Lancewicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1\">Shahram Khadivi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergence of hierarchical reference systems in multi-agent communication. (arXiv:2203.13176v1 [cs.AI])","link":"http://arxiv.org/abs/2203.13176","description":"<p>In natural language, referencing objects at different levels of specificity\nis a fundamental pragmatic mechanism for efficient communication in context. We\ndevelop a novel communication game, the hierarchical reference game, to study\nthe emergence of such reference systems in artificial agents. We consider a\nsimplified world, in which concepts are abstractions over a set of primitive\nattributes (e.g., color, style, shape). Depending on how many attributes are\ncombined, concepts are more general (\"circle\") or more specific (\"red dotted\ncircle\"). Based on the context, the agents have to communicate at different\nlevels of this hierarchy. Our results show, that the agents learn to play the\ngame successfully and can even generalize to novel concepts. To achieve\nabstraction, they use implicit (omitting irrelevant information) and explicit\n(indicating that attributes are irrelevant) strategies. In addition, the\ncompositional structure underlying the concept hierarchy is reflected in the\nemergent protocols, indicating that the need to develop hierarchical reference\nsystems supports the emergence of compositionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohmer_X/0/1/0/all/0/1\">Xenia Ohmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duda_M/0/1/0/all/0/1\">Marko Duda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1\">Elia Bruni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct parsing to sentiment graphs. (arXiv:2203.13209v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13209","description":"<p>This paper demonstrates how a graph-based semantic parser can be applied to\nthe task of structured sentiment analysis, directly predicting sentiment graphs\nfrom text. We advance the state of the art on 4 out of 5 standard benchmark\nsets. We release the source code, models and predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_J/0/1/0/all/0/1\">Jeremy Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_R/0/1/0/all/0/1\">Robin Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oepen_S/0/1/0/all/0/1\">Stephan Oepen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velldal_E/0/1/0/all/0/1\">Erik Velldal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion. (arXiv:2203.13224v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13224","description":"<p>Language models (LMs) have recently been shown to generate more factual\nresponses by employing modularity (Zhou et al., 2021) in combination with\nretrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et\nal. (2021) to include internet search as a module. Our SeeKeR (Search\nengine-&gt;Knowledge-&gt;Response) method thus applies a single LM to three modular\ntasks in succession: search, generating knowledge, and generating a final\nresponse. We show that, when using SeeKeR as a dialogue model, it outperforms\nthe state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain\nknowledge-grounded conversations for the same number of parameters, in terms of\nconsistency, knowledge and per-turn engagingness. SeeKeR applied to topical\nprompt completions as a standard language model outperforms GPT2 (Radford et\nal., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,\ndespite GPT3 being a vastly larger model. Our code and models are made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shuster_K/0/1/0/all/0/1\">Kurt Shuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komeili_M/0/1/0/all/0/1\">Mojtaba Komeili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szlam_A/0/1/0/all/0/1\">Arthur Szlam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1\">Jason Weston</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMARAGD: Synthesized sMatch for Accurate and Rapid AMR Graph Distance. (arXiv:2203.13226v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13226","description":"<p>The semantic similarity of graph-based meaning representations, such as\nAbstract Meaning Representation (AMR), is typically assessed using graph\nmatching algorithms, such as SMATCH (Cai and Knight, 2013). However, SMATCH\nsuffers from NP-completeness, making its large-scale application, e.g., for AMR\nclustering or semantic search, infeasible. To mitigate this issue, we propose\nSMARAGD (Synthesized sMatch for accurate and rapid AMR graph distance). We show\nthe potential of neural networks to approximate the SMATCH scores and graph\nalignments, i) in linear time using a machine translation framework to predict\nthe alignments, or ii) in constant time using a Siamese CNN to directly predict\nSMATCH scores. We show that the approximation error can be substantially\nreduced by applying data augmentation and AMR graph anonymization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_P/0/1/0/all/0/1\">Philipp Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token Dropping for Efficient BERT Pretraining. (arXiv:2203.13240v1 [cs.CL])","link":"http://arxiv.org/abs/2203.13240","description":"<p>Transformer-based models generally allocate the same amount of computation\nfor each token in a given sequence. We develop a simple but effective \"token\ndropping\" method to accelerate the pretraining of transformer models, such as\nBERT, without degrading its performance on downstream tasks. In short, we drop\nunimportant tokens starting from an intermediate layer in the model to make the\nmodel focus on important tokens; the dropped tokens are later picked up by the\nlast layer of the model so that the model still produces full-length sequences.\nWe leverage the already built-in masked language modeling (MLM) loss to\nidentify unimportant tokens with practically no computational overhead. In our\nexperiments, this simple approach reduces the pretraining cost of BERT by 25%\nwhile achieving similar overall fine-tuning performance on standard downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Richard Yuanzhe Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaodan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCP Theorems, SETH and More: Towards Proving Sub-linear Time Inapproximability. (arXiv:2011.02320v4 [cs.CC] UPDATED)","link":"http://arxiv.org/abs/2011.02320","description":"<p>In this paper we propose the PCP-like theorem for sub-linear time\ninapproximability. Abboud et al. have devised the distributed PCP framework for\nsub-quadratic time inapproximability. We show that the distributed PCP theorem\ncan be generalized for proving arbitrary polynomial time inapproximability, but\nfails in the linear case. We prove the sub-linear PCP theorem by adapting from\nan MA-protocol for the Set Containment problem, and show how to use the theorem\nto prove both existing and new inapproximability results, exhibiting the power\nof the sub-linear PCP theorem. Considering the emerging research works on\nsub-linear time algorithms, the sub-linear PCP theorem is important in guiding\nthe research in sub-linear time approximation algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengzhao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianzhong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From Human Correction For Data-Centric Deep Learning. (arXiv:2102.00225v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% accuracy is trained on the corrected dataset, which improve the baseline\nfrom 83.3% to 91.7%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Hate Speech with GPT-3. (arXiv:2103.12407v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12407","description":"<p>Sophisticated language models such as OpenAI's GPT-3 can generate hateful\ntext that targets marginalized groups. Given this capacity, we are interested\nin whether large language models can be used to identify hate speech and\nclassify text as sexist or racist. We use GPT-3 to identify sexist and racist\ntext passages with zero-, one-, and few-shot learning. We find that with zero-\nand one-shot learning, GPT-3 can identify sexist or racist text with an average\naccuracy between 55 per cent and 67 per cent, depending on the category of text\nand type of learning. With few-shot learning, the model's accuracy can be as\nhigh as 85 per cent. Large language models have a role to play in hate speech\ndetection, and with further development they could eventually be used to\ncounter hate speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_K/0/1/0/all/0/1\">Ke-Li Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_A/0/1/0/all/0/1\">Annie Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_R/0/1/0/all/0/1\">Rohan Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning. (arXiv:2104.08718v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08718","description":"<p>Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n</p>\n<p>In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. (arXiv:2106.08694v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08694","description":"<p>A lively research field has recently emerged that uses experimental methods\nto probe the linguistic behavior of modern deep networks. While work in this\ntradition often reports intriguing results about the grammatical skills of deep\nnets, it is not clear what their implications for linguistic theorizing should\nbe. As a consequence, linguistically-oriented deep net analysis has had very\nlittle impact on linguistics at large. In this chapter, I suggest that deep\nnetworks should be treated as theories making explicit predictions about the\nacceptability of linguistic utterances. I argue that, if we overcome some\nobstacles standing in the way of seriously pursuing this idea, we will gain a\npowerful new theoretical tool, complementary to mainstream algebraic\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PALRACE: Reading Comprehension Dataset with Human Data and Labeled Rationales. (arXiv:2106.12373v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.12373","description":"<p>Pre-trained language models achieves high performance on machine reading\ncomprehension (MRC) tasks but the results are hard to explain. An appealing\napproach to make models explainable is to provide rationales for its decision.\nTo investigate whether human rationales can further improve current models and\nto facilitate supervised learning of human rationales, here we present PALRACE\n(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for\n800 passages selected from the RACE dataset. We further classified the question\nto each passage into 6 types. Each passage was read by at least 26 human\nreaders, who labeled their rationales to answer the question. It is\ndemonstrated that models such as RoBERTa-large outperforms human readers in all\n6 types of questions, including inference questions, but its performance can be\nfurther improved when having access to the human rationales. Simpler models and\npre-trained models that are not fine-tuned based on the task benefit more from\nhuman rationales, and their performance can be boosted by more than 30% by\nrationales. With access to human rationales, a simple model based on the GloVe\nword embedding can reach the performance of BERT-base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jiajie Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Peiqing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xunyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nai Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis. (arXiv:2106.15231v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15231","description":"<p>While state-of-the-art NLP models have been achieving the excellent\nperformance of a wide range of tasks in recent years, important questions are\nbeing raised about their robustness and their underlying sensitivity to\nsystematic biases that may exist in their training and test data. Such issues\ncome to be manifest in performance problems when faced with out-of-distribution\ndata in the field. One recent solution has been to use counterfactually\naugmented datasets in order to reduce any reliance on spurious patterns that\nmay exist in the original data. Producing high-quality augmented data can be\ncostly and time-consuming as it usually needs to involve human feedback and\ncrowdsourcing efforts. In this work, we propose an alternative by describing\nand evaluating an approach to automatically generating counterfactual data for\ndata augmentation and explanation. A comprehensive evaluation on several\ndifferent datasets and using a variety of state-of-the-art benchmarks\ndemonstrate how our approach can achieve significant improvements in model\nperformance when compared to models training on the original data and even when\ncompared to models trained with the benefit of human-generated augmented data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiazheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1\">P&#xe1;draig Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyth_B/0/1/0/all/0/1\">Barry Smyth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Ruihai Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepSTL -- From English Requirements to Signal Temporal Logic. (arXiv:2109.10294v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10294","description":"<p>Formal methods provide very powerful tools and techniques for the design and\nanalysis of complex systems. Their practical application remains however\nlimited, due to the widely accepted belief that formal methods require\nextensive expertise and a steep learning curve. Writing correct formal\nspecifications in form of logical formulas is still considered to be a\ndifficult and error prone task.\n</p>\n<p>In this paper we propose DeepSTL, a tool and technique for the translation of\ninformal requirements, given as free English sentences, into Signal Temporal\nLogic (STL), a formal specification language for cyber-physical systems, used\nboth by academia and advanced research labs in industry. A major challenge to\ndevise such a translator is the lack of publicly available informal\nrequirements and formal specifications. We propose a two-step workflow to\naddress this challenge. We first design a grammar-based generation technique of\nsynthetic data, where each output is a random STL formula and its associated\nset of possible English translations. In the second step, we use a\nstate-of-the-art transformer-based neural translation technique, to train an\naccurate attentional translator of English to STL. The experimental results\nshow high translation quality for patterns of English requirements that have\nbeen well trained, making this workflow promising to be extended for processing\nmore complex translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1\">Ezio Bartocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickovic_D/0/1/0/all/0/1\">Dejan Ni&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isakovic_H/0/1/0/all/0/1\">Haris Isakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MReD: A Meta-Review Dataset for Controllable Text Generation. (arXiv:2110.07474v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07474","description":"<p>When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited. A typical example is\nwhen using CNN/Daily Mail dataset for controllable text summarization, there is\nno guided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and the control signal to guide\nthe generation, which can only be built with a deep understanding of the domain\nknowledge. Motivated by this vision, our paper introduces a new text generation\ndataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its\n45k meta-review sentences are manually annotated with one of the 9 carefully\ndefined categories, including abstract, strength, decision, etc. We present\nexperimental results on start-of-the-art summarization models, and propose\nmethods for structure-controlled generation with both extractive and\nabstractive models using our annotated data. By exploring various settings and\nanalyzing the model behavior with respect to the control signal, we demonstrate\nthe challenges of our proposed task and the values of our dataset MReD.\nMeanwhile, MReD also allows us to have a better understanding of the\nmeta-review domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialFact: A Benchmark for Fact-Checking in Dialogue. (arXiv:2110.08222v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08222","description":"<p>Fact-checking is an essential tool to mitigate the spread of misinformation\nand disinformation. We introduce the task of fact-checking in dialogue, which\nis a relatively unexplored area. We construct DialFact, a testing benchmark\ndataset of 22,245 annotated conversational claims, paired with pieces of\nevidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable\nclaim detection task distinguishes whether a response carries verifiable\nfactual information; 2) Evidence retrieval task retrieves the most relevant\nWikipedia snippets as evidence; 3) Claim verification task predicts a dialogue\nresponse to be supported, refuted, or not enough information. We found that\nexisting fact-checking models trained on non-dialogue data like FEVER fail to\nperform well on our task, and thus, we propose a simple yet data-efficient\nsolution to effectively improve fact-checking performance in dialogue. We point\nout unique challenges in DialFact such as handling the colloquialisms,\ncoreferences and retrieval ambiguities in the error analysis to shed light on\nfuture research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech. (arXiv:2111.10139v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10139","description":"<p>In this paper we present VDTTS, a Visually-Driven Text-to-Speech model.\nMotivated by dubbing, VDTTS takes advantage of video frames as an additional\ninput alongside text, and generates speech that matches the video signal. We\ndemonstrate how this allows VDTTS to, unlike plain TTS models, generate speech\nthat not only has prosodic variations like natural pauses and pitch, but is\nalso synchronized to the input video. Experimentally, we show our model\nproduces well-synchronized outputs, approaching the video-speech\nsynchronization quality of the ground-truth, on several challenging benchmarks\nincluding \"in-the-wild\" content from VoxCeleb2. Supplementary demo videos\ndemonstrating video-speech synchronization, robustness to speaker ID swapping,\nand prosody, presented at the project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1\">Michael Hassid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shillingford_B/0/1/0/all/0/1\">Brendan Shillingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Miaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Show and Write: Entity-aware Article Generation with Image Information. (arXiv:2112.05917v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.05917","description":"<p>Many vision-language applications contain long articles of text paired with\nimages (e.g., news or Wikipedia articles). Prior work learning to encode and/or\ngenerate these articles has primarily focused on understanding the article\nitself and some related metadata like the title or date it was written.\nHowever, the images and their captions or alt-text often contain crucial\ninformation such as named entities that are difficult to be correctly\nrecognized and predicted by language models. To address this shortcoming, this\npaper introduces an ENtity-aware article Generation method with Image\niNformation, ENGIN, to incorporate an article's image information into language\nmodels. ENGIN represents articles that can be conditioned on metadata used by\nprior work and information such as captions and named entities extracted from\nimages. Our key contribution is a novel Entity-aware mechanism to help our\nmodel better recognize and predict the entity names in articles. We perform\nexperiments on three public datasets, GoodNews, VisualNews, and WikiText.\nQuantitative results show that our approach improves generated article\nperplexity by 4-5 points over the base models. Qualitative results demonstrate\nthe text generated by ENGIN is more consistent with embedded article images. We\nalso perform article quality annotation experiments on the generated articles\nto validate that our model produces higher-quality articles. Finally, we\ninvestigate the effect ENGIN has on methods that automatically detect\nmachine-generated articles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yiwen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks. (arXiv:2112.06825v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06825","description":"<p>Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&amp;L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&amp;L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&amp;L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&amp;L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGPT: GPT Sentence Embeddings for Semantic Search. (arXiv:2202.08904v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.08904","description":"<p>GPT transformers are the largest language models available, yet semantic\nsearch is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for\napplying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric\nsearch.\n</p>\n<p>SGPT-BE produces semantically meaningful sentence embeddings by contrastive\nfine-tuning of only bias tensors and a novel pooling method. A 5.8 billion\nparameter SGPT-BE outperforms the best available sentence embeddings by 6%\nsetting a new state-of-the-art on BEIR. It outperforms the concurrently\nproposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes\n250,000 times more parameters.\n</p>\n<p>SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1\nbillion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It\nbeats the supervised state-of-the-art on 7 datasets, but significantly loses on\nother datasets. We show how this can be alleviated by adapting the prompt.\n</p>\n<p>SGPT-BE and SGPT-CE performance scales with model size. Yet, increased\nlatency, storage and compute costs should be considered. Code, models and\nresult files are freely available at https://github.com/Muennighoff/sgpt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muennighoff_N/0/1/0/all/0/1\">Niklas Muennighoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. (arXiv:2203.08411v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08411","description":"<p>Sequence modeling has demonstrated state-of-the-art performance on natural\nlanguage and document understanding tasks. However, it is challenging to\ncorrectly serialize tokens in form-like documents in practice due to their\nvariety of layout patterns. We propose FormNet, a structure-aware sequence\nmodel to mitigate the suboptimal serialization of forms. First, we design Rich\nAttention that leverages the spatial relationship between tokens in a form for\nmore precise attention score calculation. Second, we construct Super-Tokens for\neach word by embedding representations from their neighboring tokens through\ngraph convolutions. FormNet therefore explicitly recovers local syntactic\ninformation that may have been lost during serialization. In experiments,\nFormNet outperforms existing methods with a more compact model size and less\npre-training data, establishing new state-of-the-art performance on CORD, FUNSD\nand Payment benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1\">Timothy Dozat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin. (arXiv:2203.10430v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10430","description":"<p>Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have approached this\nproblem using pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by these strategies, we\npropose a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments show that learning a soft-weighting function for the candidate\nphonemes benefits performance. In addition, our proposed g2pW does not require\nextra pre-trained POS tagging models while using POS tags as auxiliary features\nsince we train the POS tagging model simultaneously with the unified encoder.\nExperimental results show that our g2pW outperforms existing methods on the\npublic CPP dataset. All codes, model weights, and a user-friendly package are\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Cheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ren Yeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.10750","description":"<p>In this paper, we develop a new multi-singer Chinese neural singing voice\nsynthesis (SVS) system named WeSinger. To improve the accuracy and naturalness\nof synthesized singing voice, we design several specifical modules and\ntechniques: 1) A deep bi-directional LSTM based duration model with multi-scale\nrhythm loss and post-processing step; 2) A Transformer-alike acoustic model\nwith progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet\nneural vocoder to produce high-quality singing waveforms; 4) A novel data\naugmentation method with multi-singer pre-training for stronger robustness and\nnaturalness. Both quantitative and qualitative evaluation results demonstrate\nthe effectiveness of WeSinger in terms of accuracy and naturalness, and\nWeSinger achieves state-of-the-art performance on the public corpus Opencpop.\nSome synthesized singing samples are available online\n(https://zzw922cn.github.io/WeSinger/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zewang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks. (arXiv:2203.12257v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.12257","description":"<p>Traditionally, a debate usually requires a manual preparation process,\nincluding reading plenty of articles, selecting the claims, identifying the\nstances of the claims, seeking the evidence for the claims, etc. As the AI\ndebate attracts more attention these years, it is worth exploring the methods\nto automate the tedious process involved in the debating system. In this work,\nwe introduce a comprehensive and large dataset named IAM, which can be applied\nto a series of argument mining tasks, including claim extraction, stance\nclassification, evidence extraction, etc. Our dataset is collected from over 1k\narticles related to 123 topics. Near 70k sentences in the dataset are fully\nannotated based on their argument properties (e.g., claims, stances, evidence,\netc.). We further propose two new integrated argument mining tasks associated\nwith the debate preparation process: (1) claim extraction with stance\nclassification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a\npipeline approach and an end-to-end method for each integrated task separately.\nPromising experimental results are reported to show the values and challenges\nof our proposed tasks, and motivate future research on argument mining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Evaluation of Non-Invasive Thermal Imaging for detection of Viability of Onchocerciasis worms. (arXiv:2203.12620v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12620","description":"<p>Onchocerciasis is causing blindness in over half a million people in the\nworld today. Drug development for the disease is crippled as there is no way of\nmeasuring effectiveness of the drug without an invasive procedure. Drug\nefficacy measurement through assessment of viability of onchocerca worms\nrequires the patients to undergo nodulectomy which is invasive, expensive,\ntime-consuming, skill-dependent, infrastructure dependent and lengthy process.\nIn this paper, we discuss the first-ever study that proposes use of machine\nlearning over thermal imaging to non-invasively and accurately predict the\nviability of worms. The key contributions of the paper are (i) a unique thermal\nimaging protocol along with pre-processing steps such as alignment,\nregistration and segmentation to extract interpretable features (ii) extraction\nof relevant semantic features (iii) development of accurate classifiers for\ndetecting the existence of viable worms in a nodule. When tested on a\nprospective test data of 30 participants with 48 palpable nodules, we achieved\nan Area Under the Curve (AUC) of 0.85.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dedhiya_R/0/1/0/all/0/1\">Ronak Dedhiya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kakileti_S/0/1/0/all/0/1\">Siva Teja Kakileti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deepu_G/0/1/0/all/0/1\">Goutham Deepu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gopinath_K/0/1/0/all/0/1\">Kanchana Gopinath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Opoku_N/0/1/0/all/0/1\">Nicholas Opoku</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_C/0/1/0/all/0/1\">Christopher King</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manjunath_G/0/1/0/all/0/1\">Geetha Manjunath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion. (arXiv:2203.12621v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12621","description":"<p>Patient scans from MRI often suffer from noise, which hampers the diagnostic\ncapability of such images. As a method to mitigate such artifact, denoising is\nlargely studied both within the medical imaging community and beyond the\ncommunity as a general subject. However, recent deep neural network-based\napproaches mostly rely on the minimum mean squared error (MMSE) estimates,\nwhich tend to produce a blurred output. Moreover, such models suffer when\ndeployed in real-world sitautions: out-of-distribution data, and complex noise\ndistributions that deviate from the usual parametric noise models. In this\nwork, we propose a new denoising method based on score-based reverse diffusion\nsampling, which overcomes all the aforementioned drawbacks. Our network,\ntrained only with coronal knee scans, excels even on out-of-distribution in\nvivo liver MRI data, contaminated with complex mixture of noise. Even more, we\npropose a method to enhance the resolution of the denoised image with the same\nnetwork. With extensive experiments, we show that our method establishes\nstate-of-the-art performance, while having desirable properties which prior\nMMSE denoisers did not have: flexibly choosing the extent of denoising, and\nquantifying uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Eun Sun Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Q-FW: A Hybrid Classical-Quantum Frank-Wolfe for Quadratic Binary Optimization. (arXiv:2203.12633v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12633","description":"<p>We present a hybrid classical-quantum framework based on the Frank-Wolfe\nalgorithm, Q-FW, for solving quadratic, linearly-constrained, binary\noptimization problems on quantum annealers (QA). The computational premise of\nquantum computers has cultivated the re-design of various existing vision\nproblems into quantum-friendly forms. Experimental QA realizations can solve a\nparticular non-convex problem known as the quadratic unconstrained binary\noptimization (QUBO). Yet a naive-QUBO cannot take into account the restrictions\non the parameters. To introduce additional structure in the parameter space,\nresearchers have crafted ad-hoc solutions incorporating (linear) constraints in\nthe form of regularizers. However, this comes at the expense of a\nhyper-parameter, balancing the impact of regularization. To date, a true\nconstrained solver of quadratic binary optimization (QBO) problems has lacked.\nQ-FW first reformulates constrained-QBO as a copositive program (CP), then\nemploys Frank-Wolfe iterations to solve CP while satisfying linear (in)equality\nconstraints. This procedure unrolls the original constrained-QBO into a set of\nunconstrained QUBOs all of which are solved, in a sequel, on a QA. We use\nD-Wave Advantage QA to conduct synthetic and real experiments on two important\ncomputer vision problems, graph matching and permutation synchronization, which\ndemonstrate that our approach is effective in alleviating the need for an\nexplicit regularization coefficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yurtsever_A/0/1/0/all/0/1\">Alp Yurtsever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birdal_T/0/1/0/all/0/1\">Tolga Birdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Scene Flow in 3D Point Clouds with Noisy Pseudo Labels. (arXiv:2203.12655v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12655","description":"<p>We propose a novel scene flow method that captures 3D motions from point\nclouds without relying on ground-truth scene flow annotations. Due to the\nirregularity and sparsity of point clouds, it is expensive and time-consuming\nto acquire ground-truth scene flow annotations. Some state-of-the-art\napproaches train scene flow networks in a self-supervised learning manner via\napproximating pseudo scene flow labels from point clouds. However, these\nmethods fail to achieve the performance level of fully supervised methods, due\nto the limitations of point cloud such as sparsity and lacking color\ninformation. To provide an alternative, we propose a novel approach that\nutilizes monocular RGB images and point clouds to generate pseudo scene flow\nlabels for training scene flow networks. Our pseudo label generation module\ninfers pseudo scene labels for point clouds by jointly leveraging rich\nappearance information in monocular images and geometric information of point\nclouds. To further reduce the negative effect of noisy pseudo labels on the\ntraining, we propose a noisy-label-aware training scheme by exploiting the\ngeometric relations of points. Experiment results show that our method not only\noutperforms state-of-the-art self-supervised approaches, but also outperforms\nsome supervised approaches that use accurate ground-truth flows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Cheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computed Tomography Reconstruction using Generative Energy-Based Priors. (arXiv:2203.12658v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12658","description":"<p>In the past decades, Computed Tomography (CT) has established itself as one\nof the most important imaging techniques in medicine. Today, the applicability\nof CT is only limited by the deposited radiation dose, reduction of which\nmanifests in noisy or incomplete measurements. Thus, the need for robust\nreconstruction algorithms arises. In this work, we learn a parametric\nregularizer with a global receptive field by maximizing it's likelihood on\nreference CT data. Due to this unsupervised learning strategy, our trained\nregularizer truly represents higher-level domain statistics, which we\nempirically demonstrate by synthesizing CT images. Moreover, this regularizer\ncan easily be applied to different CT reconstruction problems by embedding it\nin a variational framework, which increases flexibility and interpretability\ncompared to feed-forward learning-based approaches. In addition, the\naccompanying probabilistic perspective enables experts to explore the full\nposterior distribution and may quantify uncertainty of the reconstruction\napproach. We apply the regularizer to limited-angle and few-view CT\nreconstruction problems, where it outperforms traditional reconstruction\nalgorithms by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zach_M/0/1/0/all/0/1\">Martin Zach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobler_E/0/1/0/all/0/1\">Erich Kobler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pock_T/0/1/0/all/0/1\">Thomas Pock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12667","description":"<p>A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefani_E/0/1/0/all/0/1\">Eliana Stefani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Based Manipulators Need to Also See from Their Hands. (arXiv:2203.12677v1 [cs.RO])","link":"http://arxiv.org/abs/2203.12677","description":"<p>We study how the choice of visual perspective affects learning and\ngeneralization in the context of physical manipulation from raw sensor\nobservations. Compared with the more commonly used global third-person\nperspective, a hand-centric (eye-in-hand) perspective affords reduced\nobservability, but we find that it consistently improves training efficiency\nand out-of-distribution generalization. These benefits hold across a variety of\nlearning algorithms, experimental settings, and distribution shifts, and for\nboth simulated and real robot apparatuses. However, this is only the case when\nhand-centric observability is sufficient; otherwise, including a third-person\nperspective is necessary for learning, but also harms out-of-distribution\ngeneralization. To mitigate this, we propose to regularize the third-person\ninformation stream via a variational information bottleneck. On six\nrepresentative manipulation tasks with varying hand-centric observability\nadapted from the Meta-World benchmark, this results in a state-of-the-art\nreinforcement learning agent operating from both perspectives improving its\nout-of-distribution generalization on every task. While some practitioners have\nlong put cameras in the hands of robots, our work systematically analyzes the\nbenefits of doing so and provides simple and broadly applicable insights for\nimproving end-to-end learned vision-based robotic manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_K/0/1/0/all/0/1\">Kyle Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Moo Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1\">Rafael Rafailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Multi-Scale Feature Fusion for Semantic Segmentation. (arXiv:2203.12683v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12683","description":"<p>It is commonly believed that high internal resolution combined with expensive\noperations (e.g. atrous convolutions) are necessary for accurate semantic\nsegmentation, resulting in slow speed and large memory usage. In this paper, we\nquestion this belief and demonstrate that neither high internal resolution nor\natrous convolutions are necessary. Our intuition is that although segmentation\nis a dense per-pixel prediction task, the semantics of each pixel often depend\non both nearby neighbors and far-away context; therefore, a more powerful\nmulti-scale feature fusion network plays a critical role. Following this\nintuition, we revisit the conventional multi-scale feature space (typically\ncapped at P5) and extend it to a much richer space, up to P9, where the\nsmallest features are only 1/512 of the input size and thus have very large\nreceptive fields. To process such a rich feature space, we leverage the recent\nBiFPN to fuse the multi-scale features. Based on these insights, we develop a\nsimplified segmentation model, named ESeg, which has neither high internal\nresolution nor expensive atrous convolutions. Perhaps surprisingly, our simple\nmethod can achieve better accuracy with faster speed than prior art across\nmultiple datasets. In real-time settings, ESeg-Lite-S achieves 76.0% mIoU on\nCityScapes [12] at 189 FPS, outperforming FasterSeg [9] (73.1% mIoU at 170\nFPS). Our ESeg-Lite-L runs at 79 FPS and achieves 80.1% mIoU, largely closing\nthe gap between real-time and high-performance segmentation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1\">Tianjian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghiasi_G/0/1/0/all/0/1\">Golnaz Ghiasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjorian_R/0/1/0/all/0/1\">Reza Mahjorian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to generate line drawings that convey geometry and semantics. (arXiv:2203.12691v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12691","description":"<p>This paper presents an unpaired method for creating line drawings from\nphotographs. Current methods often rely on high quality paired datasets to\ngenerate line drawings. However, these datasets often have limitations due to\nthe subjects of the drawings belonging to a specific domain, or in the amount\nof data collected. Although recent work in unsupervised image-to-image\ntranslation has shown much progress, the latest methods still struggle to\ngenerate compelling line drawings. We observe that line drawings are encodings\nof scene information and seek to convey 3D shape and semantic meaning. We build\nthese observations into a set of objectives and train an image translation to\nmap photographs into line drawings. We introduce a geometry loss which predicts\ndepth information from the image features of a line drawing, and a semantic\nloss which matches the CLIP features of a line drawing with its corresponding\nphotograph. Our approach outperforms state-of-the-art unpaired image\ntranslation and line drawing generation methods on creating line drawings from\narbitrary photographs. For code and demo visit our webpage\ncarolineec.github.io/informative_drawings\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Caroline Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fredo Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Feedback Synthesis Towards Multimodal Text and Image Data. (arXiv:2203.12692v1 [cs.MM])","link":"http://arxiv.org/abs/2203.12692","description":"<p>In this paper, we have defined a novel task of affective feedback synthesis\nthat deals with generating feedback for input text &amp; corresponding image in a\nsimilar way as humans respond towards the multimodal data. A feedback synthesis\nsystem has been proposed and trained using ground-truth human comments along\nwith image-text input. We have also constructed a large-scale dataset\nconsisting of image, text, Twitter user comments, and the number of likes for\nthe comments by crawling the news articles through Twitter feeds. The proposed\nsystem extracts textual features using a transformer-based textual encoder\nwhile the visual features have been extracted using a Faster region-based\nconvolutional neural networks model. The textual and visual features have been\nconcatenated to construct the multimodal features using which the decoder\nsynthesizes the feedback. We have compared the results of the proposed system\nwith the baseline models using quantitative and qualitative measures. The\ngenerated feedbacks have been analyzed using automatic and human evaluation.\nThey have been found to be semantically similar to the ground-truth comments\nand relevant to the given text-image input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Puneet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_G/0/1/0/all/0/1\">Gaurav Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingle_O/0/1/0/all/0/1\">Omkar Ingle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1\">Daksh Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_B/0/1/0/all/0/1\">Balasubramanian Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Classifier Conservativeness and Robustness by Polynomiality. (arXiv:2203.12693v1 [cs.LG])","link":"http://arxiv.org/abs/2203.12693","description":"<p>We illustrate the detrimental effect, such as overconfident decisions, that\nexponential behavior can have in methods like classical LDA and logistic\nregression. We then show how polynomiality can remedy the situation. This,\namong others, leads purposefully to random-level performance in the tails, away\nfrom the bulk of the training data. A directly related, simple, yet important\ntechnical novelty we subsequently present is softRmax: a reasoned alternative\nto the standard softmax function employed in contemporary (deep) neural\nnetworks. It is derived through linking the standard softmax to Gaussian\nclass-conditional models, as employed in LDA, and replacing those by a\npolynomial alternative. We show that two aspects of softRmax, conservativeness\nand inherent gradient regularization, lead to robustness against adversarial\nattacks without gradient obfuscation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation. (arXiv:2203.12707v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12707","description":"<p>Unpaired image-to-image translation (I2I) is an ill-posed problem, as an\ninfinite number of translation functions can map the source domain distribution\nto the target distribution. Therefore, much effort has been put into designing\nsuitable constraints, e.g., cycle consistency (CycleGAN), geometry consistency\n(GCGAN), and contrastive learning-based constraints (CUTGAN), that help better\npose the problem. However, these well-known constraints have limitations: (1)\nthey are either too restrictive or too weak for specific I2I tasks; (2) these\nmethods result in content distortion when there is a significant spatial\nvariation between the source and target domains. This paper proposes a\nuniversal regularization technique called maximum spatial perturbation\nconsistency (MSPC), which enforces a spatial perturbation function (T ) and the\ntranslation operator (G) to be commutative (i.e., TG = GT ). In addition, we\nintroduce two adversarial training components for learning the spatial\nperturbation function. The first one lets T compete with G to achieve maximum\nperturbation. The second one lets G and T compete with discriminators to align\nthe spatial variations caused by the change of object size, object distortion,\nbackground interruptions, etc. Our method outperforms the state-of-the-art\nmethods on most I2I benchmarks. We also introduce a new benchmark, namely the\nfront face to profile face dataset, to emphasize the underlying challenges of\nI2I for real-world applications. We finally perform ablation experiments to\nstudy the sensitivity of our method to the severity of spatial perturbation and\nits effectiveness for distribution alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shaoan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Challenges of Continuous Self-Supervised Learning. (arXiv:2203.12710v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12710","description":"<p>Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks\nin representation learning - the need for human annotations. As a result, SSL\nholds the promise to learn representations from data in-the-wild, i.e., without\nthe need for finite and static datasets. Instead, true SSL algorithms should be\nable to exploit the continuous stream of data being generated on the internet\nor by agents exploring their environments. But do traditional self-supervised\nlearning approaches work in this setup? In this work, we investigate this\nquestion by conducting experiments on the continuous self-supervised learning\nproblem. While learning in the wild, we expect to see a continuous (infinite)\nnon-IID data stream that follows a non-stationary distribution of visual\nconcepts. The goal is to learn a representation that can be robust, adaptive\nyet not forgetful of concepts seen in the past. We show that a direct\napplication of current methods to such continuous setup is 1) inefficient both\ncomputationally and in the amount of data required, 2) leads to inferior\nrepresentations due to temporal correlations (non-IID data) in some sources of\nstreaming data and 3) exhibits signs of catastrophic forgetting when trained on\nsources with non-stationary data distributions. We propose the use of replay\nbuffers as an approach to alleviate the issues of inefficiency and temporal\ncorrelations. We further propose a novel method to enhance the replay buffer by\nmaintaining the least redundant samples. Minimum redundancy (MinRed) buffers\nallow us to learn effective representations even in the most challenging\nstreaming scenarios composed of sequential visual data obtained from a single\nembodied agent, and alleviates the problem of catastrophic forgetting when\nlearning from data with non-stationary semantic distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1\">Senthil Purushwalkam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1\">Pedro Morgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What to Hide from Your Students: Attention-Guided Masked Image Modeling. (arXiv:2203.12719v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12719","description":"<p>Transformers and masked language modeling are quickly being adopted and\nexplored in computer vision as vision transformers and masked image modeling\n(MIM). In this work, we argue that image token masking is fundamentally\ndifferent from token masking in text, due to the amount and correlation of\ntokens in an image. In particular, to generate a challenging pretext task for\nMIM, we advocate a shift from random masking to informed masking. We develop\nand exhibit this idea in the context of distillation-based MIM, where a teacher\ntransformer encoder generates an attention map, which we use to guide masking\nfor the student encoder. We thus introduce a novel masking strategy, called\nattention-guided masking (AttMask), and we demonstrate its effectiveness over\nrandom masking for dense distillation-based MIM as well as plain\ndistillation-based self-supervised learning on classification tokens. We\nconfirm that AttMask accelerates the learning process and improves the\nperformance on a variety of downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kakogeorgiou_I/0/1/0/all/0/1\">Ioannis Kakogeorgiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidaris_S/0/1/0/all/0/1\">Spyros Gidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Psomas_B/0/1/0/all/0/1\">Bill Psomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avrithis_Y/0/1/0/all/0/1\">Yannis Avrithis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karantzalos_K/0/1/0/all/0/1\">Konstantinos Karantzalos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komodakis_N/0/1/0/all/0/1\">Nikos Komodakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection. (arXiv:2203.12745v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12745","description":"<p>Finding relevant moments and highlights in videos according to natural\nlanguage queries is a natural and highly valuable common need in the current\nvideo content explosion era. Nevertheless, jointly conducting moment retrieval\nand highlight detection is an emerging research topic, even though its\ncomponent problems and some related tasks have already been studied for a\nwhile. In this paper, we present the first unified framework, named Unified\nMulti-modal Transformers (UMT), capable of realizing such joint optimization\nwhile can also be easily degenerated for solving individual problems. As far as\nwe are aware, this is the first scheme to integrate multi-modal (visual-audio)\nlearning for either joint optimization or the individual moment retrieval task,\nand tackles moment retrieval as a keypoint detection problem using a novel\nquery generator and query decoder. Extensive comparisons with existing methods\nand ablation studies on QVHighlights, Charades-STA, YouTube Highlights, and\nTVSum datasets demonstrate the effectiveness, superiority, and flexibility of\nthe proposed method under various settings. Source code and pre-trained models\nare available at https://github.com/TencentARC/UMT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multidimensional Belief Quantification for Label-Efficient Meta-Learning. (arXiv:2203.12768v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12768","description":"<p>Optimization-based meta-learning offers a promising direction for few-shot\nlearning that is essential for many real-world computer vision applications.\nHowever, learning from few samples introduces uncertainty, and quantifying\nmodel confidence for few-shot predictions is essential for many critical\ndomains. Furthermore, few-shot tasks used in meta training are usually sampled\nrandomly from a task distribution for an iterative model update, leading to\nhigh labeling costs and computational overhead in meta-training. We propose a\nnovel uncertainty-aware task selection model for label efficient meta-learning.\nThe proposed model formulates a multidimensional belief measure, which can\nquantify the known uncertainty and lower bound the unknown uncertainty of any\ngiven task. Our theoretical result establishes an important relationship\nbetween the conflicting belief and the incorrect belief. The theoretical result\nallows us to estimate the total uncertainty of a task, which provides a\nprincipled criterion for task selection. A novel multi-query task formulation\nis further developed to improve both the computational and labeling efficiency\nof meta-learning. Experiments conducted over multiple real-world few-shot image\nclassification tasks demonstrate the effectiveness of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_D/0/1/0/all/0/1\">Deep Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Motion-Dependent Appearance for High-Fidelity Rendering of Dynamic Humans from a Single Camera. (arXiv:2203.12780v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12780","description":"<p>Appearance of dressed humans undergoes a complex geometric transformation\ninduced not only by the static pose but also by its dynamics, i.e., there\nexists a number of cloth geometric configurations given a pose depending on the\nway it has moved. Such appearance modeling conditioned on motion has been\nlargely neglected in existing human rendering methods, resulting in rendering\nof physically implausible motion. A key challenge of learning the dynamics of\nthe appearance lies in the requirement of a prohibitively large amount of\nobservations. In this paper, we present a compact motion representation by\nenforcing equivariance -- a representation is expected to be transformed in the\nway that the pose is transformed. We model an equivariant encoder that can\ngenerate the generalizable representation from the spatial and temporal\nderivatives of the 3D body surface. This learned representation is decoded by a\ncompositional multi-task decoder that renders high fidelity time-varying\nappearance. Our experiments show that our method can generate a temporally\ncoherent video of dynamic humans for unseen body poses and novel views given a\nsingle view video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jae Shin Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tuanfeng Y. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingwan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhixin Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyun Soo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Accuracy Meets Privacy: Two-Stage Federated Transfer Learning Framework in Classification of Medical Images on Limited Data: A COVID-19 Case Study. (arXiv:2203.12803v1 [eess.IV])","link":"http://arxiv.org/abs/2203.12803","description":"<p>COVID-19 pandemic has spread rapidly and caused a shortage of global medical\nresources. The efficiency of COVID-19 diagnosis has become highly significant.\nAs deep learning and convolutional neural network (CNN) has been widely\nutilized and been verified in analyzing medical images, it has become a\npowerful tool for computer-assisted diagnosis. However, there are two most\nsignificant challenges in medical image classification with the help of deep\nlearning and neural networks, one of them is the difficulty of acquiring enough\nsamples, which may lead to model overfitting. Privacy concerns mainly bring the\nother challenge since medical-related records are often deemed patients'\nprivate information and protected by laws such as GDPR and HIPPA. Federated\nlearning can ensure the model training is decentralized on different devices\nand no data is shared among them, which guarantees privacy. However, with data\nlocated on different devices, the accessible data of each device could be\nlimited. Since transfer learning has been verified in dealing with limited data\nwith good performance, therefore, in this paper, We made a trial to implement\nfederated learning and transfer learning techniques using CNNs to classify\nCOVID-19 using lung CT scans. We also explored the impact of dataset\ndistribution at the client-side in federated learning and the number of\ntraining epochs a model is trained. Finally, we obtained very high performance\nwith federated learning, demonstrating our success in leveraging accuracy and\nprivacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1\">Alexandros Shikun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1\">Naomi Fengqi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Simultaneous Learning for Camera Re-Localization and Depth Estimation from Video. (arXiv:2203.12804v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12804","description":"<p>We present an unsupervised simultaneous learning framework for the task of\nmonocular camera re-localization and depth estimation from unlabeled video\nsequences. Monocular camera re-localization refers to the task of estimating\nthe absolute camera pose from an instance image in a known environment, which\nhas been intensively studied for alternative localization in GPS-denied\nenvironments. In recent works, camera re-localization methods are trained via\nsupervised learning from pairs of camera images and camera poses. In contrast\nto previous works, we propose a completely unsupervised learning framework for\ncamera re-localization and depth estimation, requiring only monocular video\nsequences for training. In our framework, we train two networks that estimate\nthe scene coordinates using directions and the depth map from each image which\nare then combined to estimate the camera pose. The networks can be trained\nthrough the minimization of loss functions based on our loop closed view\nsynthesis. In experiments with the 7-scenes dataset, the proposed method\noutperformed the re-localization of the state-of-the-art visual SLAM,\nORB-SLAM3. Our method also outperforms state-of-the-art monocular depth\nestimation in a trained environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taguchi_S/0/1/0/all/0/1\">Shun Taguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirose_N/0/1/0/all/0/1\">Noriaki Hirose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient and Elastic Visual Question Answering with Doubly Slimmable Transformer. (arXiv:2203.12814v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12814","description":"<p>Transformer-based approaches have shown great success in visual question\nanswering (VQA). However, they usually require deep and wide models to\nguarantee good performance, making it difficult to deploy on\ncapacity-restricted platforms. It is a challenging yet valuable task to design\nan elastic VQA model that supports adaptive pruning at runtime to meet the\nefficiency constraints of diverse platforms. In this paper, we present the\nDoubly Slimmable Transformer (DST), a general framework that can be seamlessly\nintegrated into arbitrary Transformer-based VQA models to train one single\nmodel once and obtain various slimmed submodels of different widths and depths.\nTaking two typical Transformer-based VQA approaches, i.e., MCAN and UNITER, as\nthe reference models, the obtained slimmable MCAN_DST and UNITER_DST models\noutperform the state-of-the-art methods trained independently on two benchmark\ndatasets. In particular, one slimmed MCAN_DST submodel achieves a comparable\naccuracy on VQA-v2, while being 0.38x smaller in model size and having 0.27x\nfewer FLOPs than the reference MCAN model. The smallest MCAN_DST submodel has\n9M parameters and 0.16G FLOPs in the inference stage, making it possible to be\ndeployed on edge devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zitian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViT-FOD: A Vision Transformer based Fine-grained Object Discriminator. (arXiv:2203.12816v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12816","description":"<p>Recently, several Vision Transformer (ViT) based methods have been proposed\nfor Fine-Grained Visual Classification (FGVC).These methods significantly\nsurpass existing CNN-based ones, demonstrating the effectiveness of ViT in FGVC\ntasks.However, there are some limitations when applying ViT directly to\nFGVC.First, ViT needs to split images into patches and calculate the attention\nof every pair, which may result in heavy redundant calculation and unsatisfying\nperformance when handling fine-grained images with complex background and small\nobjects.Second, a standard ViT only utilizes the class token in the final layer\nfor classification, which is not enough to extract comprehensive fine-grained\ninformation. To address these issues, we propose a novel ViT based fine-grained\nobject discriminator for FGVC tasks, ViT-FOD for short. Specifically, besides a\nViT backbone, it further introduces three novel components, i.e, Attention\nPatch Combination (APC), Critical Regions Filter (CRF), and Complementary\nTokens Integration (CTI). Thereinto, APC pieces informative patches from two\nimages to generate a new image so that the redundant calculation can be\nreduced. CRF emphasizes tokens corresponding to discriminative regions to\ngenerate a new class token for subtle feature learning. To extract\ncomprehensive information, CTI integrates complementary information captured by\nclass tokens in different ViT layers. We conduct comprehensive experiments on\nwidely used datasets and the results demonstrate that ViT-FOD is able to\nachieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zi-Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhen-Duo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin-Shun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12818","description":"<p>In this paper we describe our approach to the arousal and valence track of\nthe 3rd Workshop and Competition on Affective Behavior Analysis in-the-wild\n(ABAW). We extracted facial features using OpenFace and used them to train a\nmultiple output random forest regressor. Our approach performed comparable to\nthe baseline approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hinduja_S/0/1/0/all/0/1\">Saurabh Hinduja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canavan_S/0/1/0/all/0/1\">Shaun Canavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jivnani_L/0/1/0/all/0/1\">Liza Jivnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannat_S/0/1/0/all/0/1\">Sk Rahatul Jannat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">V Sri Chakra Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subjective and Objective Analysis of Streamed Gaming Videos. (arXiv:2203.12824v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12824","description":"<p>The rising popularity of online User-Generated-Content (UGC) in the form of\nstreamed and shared videos, has hastened the development of perceptual Video\nQuality Assessment (VQA) models, which can be used to help optimize their\ndelivery. Gaming videos, which are a relatively new type of UGC videos, are\ncreated when skilled gamers post videos of their gameplay. These kinds of\nscreenshots of UGC gameplay videos have become extremely popular on major\nstreaming platforms like YouTube and Twitch. Synthetically-generated gaming\ncontent presents challenges to existing VQA algorithms, including those based\non natural scene/video statistics models. Synthetically generated gaming\ncontent presents different statistical behavior than naturalistic videos. A\nnumber of studies have been directed towards understanding the perceptual\ncharacteristics of professionally generated gaming videos arising in gaming\nvideo streaming, online gaming, and cloud gaming. However, little work has been\ndone on understanding the quality of UGC gaming videos, and how it can be\ncharacterized and predicted. Towards boosting the progress of gaming video VQA\nmodel development, we conducted a comprehensive study of subjective and\nobjective VQA models on UGC gaming videos. To do this, we created a novel UGC\ngaming video resource, called the LIVE-YouTube Gaming video quality\n(LIVE-YT-Gaming) database, comprised of 600 real UGC gaming videos. We\nconducted a subjective human study on this data, yielding 18,600 human quality\nratings recorded by 61 human subjects. We also evaluated a number of\nstate-of-the-art (SOTA) VQA models on the new database, including a new one,\ncalled GAME-VQP, based on both natural video statistics and CNN-learned\nfeatures. To help support work in this field, we are making the new\nLIVE-YT-Gaming Database, publicly available through the link:\nhttps://live.ece.utexas.edu/research/LIVE-YT-Gaming/index.html .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiangxu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zhenqiang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HMFS: Hybrid Masking for Few-Shot Segmentation. (arXiv:2203.12826v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12826","description":"<p>We study few-shot semantic segmentation that aims to segment a target object\nfrom a query image when provided with a few annotated support images of the\ntarget class. Several recent methods resort to a feature masking (FM)\ntechnique, introduced by [1], to discard irrelevant feature activations to\nfacilitate reliable segmentation mask prediction. A fundamental limitation of\nFM is the inability to preserve the fine-grained spatial details that affect\nthe accuracy of segmentation mask, especially for small target objects. In this\npaper, we develop a simple, effective, and efficient approach to enhance\nfeature masking (FM). We dub the enhanced FM as hybrid masking (HM).\nSpecifically, we compensate for the loss of fine-grained spatial details in FM\ntechnique by investigating and leveraging a complementary basic input masking\nmethod [2]. To validate the effectiveness of HM, we instantiate it into a\nstrong baseline [3], and coin the resulting framework as HMFS. Experimental\nresults on three publicly available benchmarks reveal that HMFS outperforms the\ncurrent state-of-the-art methods by visible margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seonghyeon Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Samuel S. Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Honglu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sejong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Haris Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1\">Mubbasir Kapadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Instance Activation for Real-Time Instance Segmentation. (arXiv:2203.12827v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12827","description":"<p>In this paper, we propose a conceptually novel, efficient, and fully\nconvolutional framework for real-time instance segmentation. Previously, most\ninstance segmentation methods heavily rely on object detection and perform mask\nprediction based on bounding boxes or dense centers. In contrast, we propose a\nsparse set of instance activation maps, as a new object representation, to\nhighlight informative regions for each foreground object. Then instance-level\nfeatures are obtained by aggregating features according to the highlighted\nregions for recognition and segmentation. Moreover, based on bipartite\nmatching, the instance activation maps can predict objects in a one-to-one\nstyle, thus avoiding non-maximum suppression (NMS) in post-processing. Owing to\nthe simple yet effective designs with instance activation maps, SparseInst has\nextremely fast inference speed and achieves 40 FPS and 37.9 AP on the COCO\nbenchmark, which significantly outperforms the counterparts in terms of speed\nand accuracy. Code and models are available at\nhttps://github.com/hustvl/SparseInst.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIMusicGuru: Music Assisted Human Pose Correction. (arXiv:2203.12829v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12829","description":"<p>Pose Estimation techniques rely on visual cues available through observations\nrepresented in the form of pixels. But the performance is bounded by the frame\nrate of the video and struggles from motion blur, occlusions, and temporal\ncoherence. This issue is magnified when people are interacting with objects and\ninstruments, for example playing the violin. Standard approaches for\npostprocessing use interpolation and smoothing functions to filter noise and\nfill gaps, but they cannot model highly non-linear motion. We present a method\nthat leverages our understanding of the high degree of a causal relationship\nbetween the sound produced and the motion that produces them. We use the audio\nsignature to refine and predict accurate human body pose motion models. We\npropose MAPnet (Music Assisted Pose network) for generating a fine grain motion\nmodel from sparse input pose sequences but continuous audio. To accelerate\nfurther research in this domain, we also open-source MAPdat, a new multi-modal\ndataset of 3D violin playing motion with music. We perform a comparison of\ndifferent standard machine learning models and perform analysis on input\nmodalities, sampling techniques, and audio and motion features. Experiments on\nMAPdat suggest multi-modal approaches like ours as a promising direction for\ntasks previously approached with visual methods only. Our results show both\nqualitatively and quantitatively how audio can be combined with visual\nobservation to help improve any pose estimation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1\">Snehesh Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1\">Cornelia Ferm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Win_P/0/1/0/all/0/1\">Pyone Thant Win</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zukerman_A/0/1/0/all/0/1\">Adam Zukerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parameshwara_C/0/1/0/all/0/1\">Chethan M. Parameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1\">Yiannis Aloimonos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Industrial Style Transfer with Large-scale Geometric Warping and Content Preservation. (arXiv:2203.12835v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12835","description":"<p>We propose a novel style transfer method to quickly create a new visual\nproduct with a nice appearance for industrial designers' reference. Given a\nsource product, a target product, and an art style image, our method produces a\nneural warping field that warps the source shape to imitate the geometric style\nof the target and a neural texture transformation network that transfers the\nartistic style to the warped source product. Our model, Industrial Style\nTransfer (InST), consists of large-scale geometric warping (LGW) and\ninterest-consistency texture transfer (ICTT). LGW aims to explore an\nunsupervised transformation between the shape masks of the source and target\nproducts for fitting large-scale shape warping. Furthermore, we introduce a\nmask smoothness regularization term to prevent the abrupt changes of the\ndetails of the source product. ICTT introduces an interest regularization term\nto maintain important contents of the warped product when it is stylized by\nusing the art style image. Extensive experimental results demonstrate that InST\nachieves state-of-the-art performance on multiple visual product design tasks,\ne.g., companies' snail logos and classical bottles (please see Fig. 1). To the\nbest of our knowledge, we are the first to extend the neural style transfer\nmethod to create industrial product appearances. Project page:\n\\ulr{https://jcyang98.github.io/InST/home.html}. Code available at:\n\\url{https://github.com/jcyang98/InST}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Nonparametric Submodular Video Partition for Robust Anomaly Detection. (arXiv:2203.12840v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12840","description":"<p>Multiple-instance learning (MIL) provides an effective way to tackle the\nvideo anomaly detection problem by modeling it as a weakly supervised problem\nas the labels are usually only available at the video level while missing for\nframes due to expensive labeling cost. We propose to conduct novel Bayesian\nnon-parametric submodular video partition (BN-SVP) to significantly improve MIL\nmodel training that can offer a highly reliable solution for robust anomaly\ndetection in practical settings that include outlier segments or multiple types\nof abnormal events. BN-SVP essentially performs dynamic non-parametric\nhierarchical clustering with an enhanced self-transition that groups segments\nin a video into temporally consistent and semantically coherent hidden states\nthat can be naturally interpreted as scenes. Each segment is assumed to be\ngenerated through a non-parametric mixture process that allows variations of\nsegments within the same scenes to accommodate the dynamic and noisy nature of\nmany real-world surveillance videos. The scene and mixture component assignment\nof BN-SVP also induces a pairwise similarity among segments, resulting in\nnon-parametric construction of a submodular set function. Integrating this\nfunction with an MIL loss effectively exposes the model to a diverse set of\npotentially positive instances to improve its training. A greedy algorithm is\ndeveloped to optimize the submodular function and support efficient model\ntraining. Our theoretical analysis ensures a strong performance guarantee of\nthe proposed algorithm. The effectiveness of the proposed approach is\ndemonstrated over multiple real-world anomaly video datasets with robust\ndetection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_H/0/1/0/all/0/1\">Hitesh Sapkota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steganalysis of Image with Adaptively Parametric Activation. (arXiv:2203.12843v1 [cs.MM])","link":"http://arxiv.org/abs/2203.12843","description":"<p>Steganalysis as a method to detect whether image contains se-cret message, is\na crucial study avoiding the imperils from abus-ing steganography. The point of\nsteganalysis is to detect the weak embedding signals which is hardly learned by\nconvolution-al layer and easily suppressed. In this paper, to enhance\nembed-ding signals, we study the insufficiencies of activation function,\nfilters and loss function from the aspects of reduce embedding signal loss and\nenhance embedding signal capture ability. Adap-tive Parametric Activation\nModule is designed to reserve nega-tive embedding signal. For embedding signal\ncapture ability enhancement, we add constraints on the high-pass filters to\nim-prove residual diversity which enables the filters extracts rich embedding\nsignals. Besides, a loss function based on contrastive learning is applied to\novercome the limitations of cross-entropy loss by maximum inter-class distance.\nIt helps the network make a distinction between embedding signals and semantic\nedges. We use images from BOSSbase 1.01 and make stegos by WOW and S-UNIWARD\nfor experiments. Compared to state-of-the-art methods, our method has a\ncompetitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hai Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Meiyin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junle Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Songsen Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Emotion Descriptors Estimation at the ABAW3 Challenge. (arXiv:2203.12845v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12845","description":"<p>To describe complex emotional states, psychologists have proposed multiple\nemotion descriptors: sparse descriptors like facial action units; continuous\ndescriptors like valence and arousal; and discrete class descriptors like\nhappiness and anger. According to Ekman and Friesen, 1969, facial action units\nare sign vehicles that convey the emotion message, while discrete or continuous\nemotion descriptors are the messages perceived and expressed by human.\n</p>\n<p>In this paper, we designed an architecture for multiple emotion descriptors\nestimation in participating the ABAW3 Challenge. Based on the theory of Ekman\nand Friesen, 1969, we designed distinct architectures to measure the sign\nvehicles (i.e., facial action units) and the message (i.e., discrete emotions,\nvalence and arousal) given their different properties. The quantitative\nexperiments on the ABAW3 challenge dataset has shown the superior performance\nof our approach over two baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Didan Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keypoints Tracking via Transformer Networks. (arXiv:2203.12848v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12848","description":"<p>In this thesis, we propose a pioneering work on sparse keypoints tracking\nacross images using transformer networks. While deep learning-based keypoints\nmatching have been widely investigated using graph neural networks - and more\nrecently transformer networks, they remain relatively too slow to operate in\nreal-time and are particularly sensitive to the poor repeatability of the\nkeypoints detectors. In order to address these shortcomings, we propose to\nstudy the particular case of real-time and robust keypoints tracking.\nSpecifically, we propose a novel architecture which ensures a fast and robust\nestimation of the keypoints tracking between successive images of a video\nsequence. Our method takes advantage of a recent breakthrough in computer\nvision, namely, visual transformer networks. Our method consists of two\nsuccessive stages, a coarse matching followed by a fine localization of the\nkeypoints' correspondences prediction. Through various experiments, we\ndemonstrate that our approach achieves competitive results and demonstrates\nhigh robustness against adverse conditions, such as illumination change,\nocclusion and viewpoint differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasypanyi_O/0/1/0/all/0/1\">Oleksii Nasypanyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rameau_F/0/1/0/all/0/1\">Francois Rameau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Image Manipulation with Background-guided Internal Learning. (arXiv:2203.12849v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12849","description":"<p>Image manipulation has attracted a lot of interest due to its wide range of\napplications. Prior work modifies images either from low-level manipulation,\nsuch as image inpainting or through manual edits via paintbrushes and\nscribbles, or from high-level manipulation, employing deep generative networks\nto output an image conditioned on high-level semantic input. In this study, we\npropose Semantic Image Manipulation with Background-guided Internal Learning\n(SIMBIL), which combines high-level and low-level manipulation. Specifically,\nusers can edit an image at the semantic level by applying changes on a scene\ngraph. Then our model manipulates the image at the pixel level according to the\nmodified scene graph. There are two major advantages of our approach. First,\nhigh-level manipulation of scene graphs requires less manual effort from the\nuser compared to manipulating raw image pixels. Second, our low-level internal\nlearning approach is scalable to images of various sizes without reliance on\nexternal visual datasets for training. We outperform the state-of-the-art in a\nquantitative and qualitative evaluation on the CLEVR and Visual Genome\ndatasets. Experiments show 8 points improvement on FID scores (CLEVR) and 27%\nimprovement on user evaluation (Visual Genome), demonstrating the effectiveness\nof our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhongping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huiwen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhenyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct evaluation of progression or regression of disease burden in brain metastatic disease with Deep Neuroevolution. (arXiv:2203.12853v1 [cs.NE])","link":"http://arxiv.org/abs/2203.12853","description":"<p>Purpose: A core component of advancing cancer treatment research is assessing\nresponse to therapy. Doing so by hand, for example as per RECIST or RANO\ncriteria, is tedious, time-consuming, and can miss important tumor response\ninformation; most notably, they exclude non-target lesions. We wish to assess\nchange in a holistic fashion that includes all lesions, obtaining simple,\ninformative, and automated assessments of tumor progression or regression. Due\nto often low patient enrolments in clinical trials, we wish to make response\nassessments with small training sets. Deep neuroevolution (DNE) can produce\nradiology artificial intelligence (AI) that performs well on small training\nsets. Here we use DNE for function approximation that predicts progression\nversus regression of metastatic brain disease.\n</p>\n<p>Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training\nset. Half of these pairs, separated in time, qualified as disease progression,\nwhile the other 25 images constituted regression. We trained the parameters of\na relatively small CNN via mutations that consisted of random CNN weight\nadjustments and mutation fitness. We then incorporated the best mutations into\nthe next generations CNN, repeating this process for approximately 50,000\ngenerations. We applied the CNNs to our training set, as well as a separate\ntesting set with the same class balance of 25 progression and 25 regression\nimages.\n</p>\n<p>Results: DNE achieved monotonic convergence to 100% training set accuracy.\nDNE also converged monotonically to 100% testing set accuracy.\n</p>\n<p>Conclusion: DNE can accurately classify brain-metastatic disease progression\nversus regression. Future work will extend the input from 2D image slices to\nfull 3D volumes, and include the category of no change. We believe that an\napproach such as our could ultimately provide a useful adjunct to RANO/RECIST\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stember_J/0/1/0/all/0/1\">Joseph Stember</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_R/0/1/0/all/0/1\">Robert Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1\">Hrithwik Shalu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Fixation: Dynamic Window Visual Transformer. (arXiv:2203.12856v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12856","description":"<p>Recently, a surge of interest in visual transformers is to reduce the\ncomputational cost by limiting the calculation of self-attention to a local\nwindow. Most current work uses a fixed single-scale window for modeling by\ndefault, ignoring the impact of window size on model performance. However, this\nmay limit the modeling potential of these window-based models for multi-scale\ninformation. In this paper, we propose a novel method, named Dynamic Window\nVision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT\ngoes beyond the model that employs a fixed single window setting. To the best\nof our knowledge, we are the first to use dynamic multi-scale windows to\nexplore the upper limit of the effect of window settings on model performance.\nIn DW-ViT, multi-scale information is obtained by assigning windows of\ndifferent sizes to different head groups of window multi-head self-attention.\nThen, the information is dynamically fused by assigning different weights to\nthe multi-scale window branches. We conducted a detailed performance evaluation\non three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related\nstate-of-the-art (SoTA) methods, DW-ViT obtains the best performance.\nSpecifically, compared with the current SoTA Swin Transformers\n\\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements\non all three datasets with similar parameters and computational costs. In\naddition, DW-ViT exhibits good scalability and can be easily inserted into any\nwindow-based visual transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Q/0/1/0/all/0/1\">Qing Du Xiaodan Liang Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12861","description":"<p>Convolutional neural networks (CNN) have demonstrated outstanding Compressed\nSensing (CS) performance compared to traditional, hand-crafted methods.\nHowever, they are broadly limited in terms of generalisability, inductive bias\nand difficulty to model long distance relationships. Transformer neural\nnetworks (TNN) overcome such issues by implementing an attention mechanism\ndesigned to capture dependencies between inputs. However, high-resolution tasks\ntypically require vision Transformers (ViT) to decompose an image into\npatch-based tokens, limiting inputs to inherently local contexts. We propose a\nnovel image decomposition that naturally embeds images into low-resolution\ninputs. These Kaleidoscope tokens (KD) provide a mechanism for global\nattention, at the same computational cost as a patch-based approach. To\nshowcase this development, we replace CNN components in a well-known CS-MRI\nneural network with TNN blocks and demonstrate the improvements afforded by KD.\nWe also propose an ensemble of image tokens, which enhance overall image\nquality and reduces model size. Supplementary material is available:\nhttps://github.com/uqmarlonbran/TCS.git}{https://github.com/uqmarlonbran/TCS.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenzana_M/0/1/0/all/0/1\">Marlon Bran Lorenzana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engstrom_C/0/1/0/all/0/1\">Craig Engstrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1\">Shekhar S. Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyRep: Bootstrapping Training with Dynamic Re-parameterization. (arXiv:2203.12868v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12868","description":"<p>Structural re-parameterization (Rep) methods achieve noticeable improvements\non simple VGG-style networks. Despite the prevalence, current Rep methods\nsimply re-parameterize all operations into an augmented network, including\nthose that rarely contribute to the model's performance. As such, the price to\npay is an expensive computational overhead to manipulate these unnecessary\nbehaviors. To eliminate the above caveats, we aim to bootstrap the training\nwith minimal cost by devising a dynamic re-parameterization (DyRep) method,\nwhich encodes Rep technique into the training process that dynamically evolves\nthe network structures. Concretely, our proposal adaptively finds the\noperations which contribute most to the loss in the network, and applies Rep to\nenhance their representational capacity. Besides, to suppress the noisy and\nredundant operations introduced by Rep, we devise a de-parameterization\ntechnique for a more compact re-parameterization. With this regard, DyRep is\nmore efficient than Rep since it smoothly evolves the given network instead of\nconstructing an over-parameterized network. Experimental results demonstrate\nour effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by $2.04\\%$\non ImageNet and reduces $22\\%$ runtime over the baseline. Code is available at:\nhttps://github.com/hunto/DyRep.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization. (arXiv:2203.12870v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12870","description":"<p>Direct estimating the 6-DoF object pose from a single color image is\nchallenging, and post-refinement is generally needed to achieve high-precision\nestimation. In this paper, we propose a framework based on a recurrent neural\nnetwork (RNN) for object pose refinement, which is robust to erroneous initial\nposes and occlusions. During the recurrent iterations, object pose refinement\nis formulated as a non-linear least squares problem based on the estimated\ncorrespondence field (between a rendered image and the observed image). The\nproblem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm\nfor end-toend training. The correspondence field estimation and pose refinement\nare conducted alternatively in each iteration to recover accurate object poses.\nFurthermore, to improve the robustness to occlusions, we introduce a\nconsistencycheck mechanism based on the learned descriptors of the 3D model and\nobserved 2D image, which downweights the unreliable correspondences during pose\noptimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and\nYCB-Video datasets validate the effectiveness of our method and demonstrate\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Bias Identification on Medical Image Datasets. (arXiv:2203.12872v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12872","description":"<p>Machine learning based medical image analysis highly depends on datasets.\nBiases in the dataset can be learned by the model and degrade the\ngeneralizability of the applications. There are studies on debiased models.\nHowever, scientists and practitioners are difficult to identify implicit biases\nin the datasets, which causes lack of reliable unbias test datasets to valid\nmodels. To tackle this issue, we first define the data intrinsic bias\nattribute, and then propose a novel bias identification framework for medical\nimage datasets. The framework contains two major components, KlotskiNet and\nBias Discriminant Direction Analysis(bdda), where KlostkiNet is to build the\nmapping which makes backgrounds to distinguish positive and negative samples\nand bdda provides a theoretical solution on determining bias attributes.\nExperimental results on three datasets show the effectiveness of the bias\nattributes discovered by the framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Senhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Dandan Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised End-to-End CAD Retrieval to Scan Objects. (arXiv:2203.12873v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12873","description":"<p>CAD model retrieval to real-world scene observations has shown strong promise\nas a basis for 3D perception of objects and a clean, lightweight mesh-based\nscene representation; however, current approaches to retrieve CAD models to a\nquery scan rely on expensive manual annotations of 1:1 associations of CAD-scan\nobjects, which typically contain strong lower-level geometric differences. We\nthus propose a new weakly-supervised approach to retrieve semantically and\nstructurally similar CAD models to a query 3D scanned scene without requiring\nany CAD-scan associations, and only object detection information as oriented\nbounding boxes. Our approach leverages a fully-differentiable top-$k$ retrieval\nlayer, enabling end-to-end training guided by geometric and perceptual\nsimilarity of the top retrieved CAD models to the scan queries. We demonstrate\nthat our weakly-supervised approach can outperform fully-supervised retrieval\nmethods on challenging real-world ScanNet scans, and maintain robustness for\nunseen class categories, achieving significantly improved performance over\nfully-supervised state of the art in zero-shot CAD retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beyer_T/0/1/0/all/0/1\">Tim Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ensemble Approach for Facial Expression Analysis in Video. (arXiv:2203.12891v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12891","description":"<p>Human emotions recognization contributes to the development of human-computer\ninteraction. The machines understanding human emotions in the real world will\nsignificantly contribute to life in the future. This paper will introduce the\nAffective Behavior Analysis in-the-wild (ABAW3) 2022 challenge. The paper\nfocuses on solving the problem of the valence-arousal estimation and action\nunit detection. For valence-arousal estimation, we conducted two stages:\ncreating new features from multimodel and temporal learning to predict\nvalence-arousal. First, we make new features; the Gated Recurrent Unit (GRU)\nand Transformer are combined using a Regular Networks (RegNet) feature, which\nis extracted from the image. The next step is the GRU combined with Local\nAttention to predict valence-arousal. The Concordance Correlation Coefficient\n(CCC) was used to evaluate the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hong-Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1\">Van-Thong Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Hyung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Heads or Tails: Towards Semantically Consistent Visual Counterfactuals. (arXiv:2203.12892v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12892","description":"<p>A visual counterfactual explanation replaces image regions in a query image\nwith regions from a distractor image such that the system's decision on the\ntransformed image changes to the distractor class. In this work, we present a\nnovel framework for computing visual counterfactual explanations based on two\nkey ideas. First, we enforce that the \\textit{replaced} and \\textit{replacer}\nregions contain the same semantic part, resulting in more semantically\nconsistent explanations. Second, we use multiple distractor images in a\ncomputationally efficient way and obtain more discriminative explanations with\nfewer region replacements. Our approach is $\\mathbf{27\\%}$ more semantically\nconsistent and an order of magnitude faster than a competing method on three\nfine-grained image recognition datasets. We highlight the utility of our\ncounterfactuals over existing works through machine teaching experiments where\nwe teach humans to classify different bird species. We also complement our\nexplanations with the vocabulary of parts and attributes that contributed the\nmost to the system's decision. In this task as well, we obtain state-of-the-art\nresults when using our counterfactual explanations relative to existing works,\nreinforcing the importance of semantically consistent explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1\">Simon Vandenhende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1\">Deepti Ghadiyaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAMLP: A Frequency-Aware MLP-Like Architecture For Domain Generalization. (arXiv:2203.12893v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12893","description":"<p>MLP-like models built entirely upon multi-layer perceptrons have recently\nbeen revisited, exhibiting the comparable performance with transformers. It is\none of most promising architectures due to the excellent trade-off between\nnetwork capability and efficiency in the large-scale recognition tasks.\nHowever, its generalization performance to heterogeneous tasks is inferior to\nother architectures (e.g., CNNs and transformers) due to the extensive\nretention of domain information. To address this problem, we propose a novel\nfrequency-aware MLP architecture, in which the domain-specific features are\nfiltered out in the transformed frequency domain, augmenting the invariant\ndescriptor for label prediction. Specifically, we design an adaptive Fourier\nfilter layer, in which a learnable frequency filter is utilized to adjust the\namplitude distribution by optimizing both the real and imaginary parts. A\nlow-rank enhancement module is further proposed to rectify the filtered\nfeatures by adding the low-frequency components from SVD decomposition.\nFinally, a momentum update strategy is utilized to stabilize the optimization\nto fluctuation of model parameters and inputs by the output distillation with\nweighted historical states. To our best knowledge, we are the first to propose\na MLP-like backbone for domain generalization. Extensive experiments on three\nbenchmarks demonstrate significant generalization performance, outperforming\nthe state-of-the-art methods by a margin of 3%, 4% and 9%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruijing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-Jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expression Classification using Concatenation of Deep Neural Network for the 3rd ABAW3 Competition. (arXiv:2203.12899v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12899","description":"<p>For computers to recognize human emotions, expression classification is an\nequally important problem in the human-computer interaction area. In the 3rd\nAffective Behavior Analysis In-The-Wild competition, the task of expression\nclassification includes 8 classes including 6 basic expressions of human faces\nfrom videos. In this paper, we perform combination representation from RegNet,\nAttention module, and Transformer Encoder for the expression classification\ntask. We achieve 35.87 \\% for F1-score on the validation set of Aff-Wild2\ndataset. This result shows the effectiveness of the proposed architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phan_K/0/1/0/all/0/1\">Kim Ngan Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hong-Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1\">Van-Thong Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soo-Hyung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privileged Attribution Constrained Deep Networks for Facial Expression Recognition. (arXiv:2203.12905v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12905","description":"<p>Facial Expression Recognition (FER) is crucial in many research domains\nbecause it enables machines to better understand human behaviours. FER methods\nface the problems of relatively small datasets and noisy data that don't allow\nclassical networks to generalize well. To alleviate these issues, we guide the\nmodel to concentrate on specific facial areas like the eyes, the mouth or the\neyebrows, which we argue are decisive to recognise facial expressions. We\npropose the Privileged Attribution Loss (PAL), a method that directs the\nattention of the model towards the most salient facial regions by encouraging\nits attribution maps to correspond to a heatmap formed by facial landmarks.\nFurthermore, we introduce several channel strategies that allow the model to\nhave more degrees of freedom. The proposed method is independent of the\nbackbone architecture and doesn't need additional semantic information at test\ntime. Finally, experimental results show that the proposed PAL method\noutperforms current state-of-the-art methods on both RAF-DB and AffectNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonnard_J/0/1/0/all/0/1\">Jules Bonnard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhombres_F/0/1/0/all/0/1\">Ferdinand Dhombres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1\">K&#xe9;vin Bailly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Reflectance for Shape Recovery with Shadow Handling. (arXiv:2203.12909v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12909","description":"<p>This paper aims at recovering the shape of a scene with unknown,\nnon-Lambertian, and possibly spatially-varying surface materials. When the\nshape of the object is highly complex and that shadows cast on the surface, the\ntask becomes very challenging. To overcome these challenges, we propose a\ncoordinate-based deep MLP (multilayer perceptron) to parameterize both the\nunknown 3D shape and the unknown reflectance at every surface point. This\nnetwork is able to leverage the observed photometric variance and shadows on\nthe surface, and recover both surface shape and general non-Lambertian\nreflectance. We explicitly predict cast shadows, mitigating possible artifacts\non these shadowing regions, leading to higher estimation accuracy. Our\nframework is entirely self-supervised, in the sense that it requires neither\nground truth shape nor BRDF. Tests on real-world images demonstrate that our\nmethod outperform existing methods by a significant margin. Thanks to the small\nsize of the MLP-net, our method is an order of magnitude faster than previous\nCNN-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks. (arXiv:2203.12915v1 [cs.LG])","link":"http://arxiv.org/abs/2203.12915","description":"<p>Deep learning has recently been widely applied to many applications across\ndifferent domains, e.g., image classification and audio recognition. However,\nthe quality of Deep Neural Networks (DNNs) still raises concerns in the\npractical operational environment, which calls for systematic testing,\nespecially in safety-critical scenarios. Inspired by software testing, a number\nof structural coverage criteria are designed and proposed to measure the test\nadequacy of DNNs. However, due to the blackbox nature of DNN, the existing\nstructural coverage criteria are difficult to interpret, making it hard to\nunderstand the underlying principles of these criteria. The relationship\nbetween the structural coverage and the decision logic of DNNs is unknown.\nMoreover, recent studies have further revealed the non-existence of correlation\nbetween the structural coverage and DNN defect detection, which further posts\nconcerns on what a suitable DNN testing criterion should be.\n</p>\n<p>In this paper, we propose the interpretable coverage criteria through\nconstructing the decision structure of a DNN. Mirroring the control flow graph\nof the traditional program, we first extract a decision graph from a DNN based\non its interpretation, where a path of the decision graph represents a decision\nlogic of the DNN. Based on the control flow and data flow of the decision\ngraph, we propose two variants of path coverage to measure the adequacy of the\ntest cases in exercising the decision logic. The higher the path coverage, the\nmore diverse decision logic the DNN is expected to be explored. Our large-scale\nevaluation results demonstrate that: the path in the decision graph is\neffective in characterizing the decision of the DNN, and the proposed coverage\ncriteria are also sensitive with errors including natural errors and\nadversarial examples, and strongly correlated with the output impartiality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation. (arXiv:2203.12917v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12917","description":"<p>We propose WarpingGAN, an effective and efficient 3D point cloud generation\nnetwork. Unlike existing methods that generate point clouds by directly\nlearning the mapping functions between latent codes and 3D shapes, Warping-GAN\nlearns a unified local-warping function to warp multiple identical pre-defined\npriors (i.e., sets of points uniformly distributed on regular 3D grids) into 3D\nshapes driven by local structure-aware semantics. In addition, we also\ningeniously utilize the principle of the discriminator and tailor a stitching\nloss to eliminate the gaps between different partitions of a generated shape\ncorresponding to different priors for boosting quality. Owing to the novel\ngenerating mechanism, WarpingGAN, a single lightweight network after one-time\ntraining, is capable of efficiently generating uniformly distributed 3D point\nclouds with various resolutions. Extensive experimental results demonstrate the\nsuperiority of our WarpingGAN over state-of-the-art methods in terms of\nquantitative metrics, visual quality, and efficiency. The source code is\npublicly available at https://github.com/yztang4/WarpingGAN.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yingzhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yue Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yiming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dense Correspondence from Synthetic Environments. (arXiv:2203.12919v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12919","description":"<p>Estimation of human shape and pose from a single image is a challenging task.\nIt is an even more difficult problem to map the identified human shape onto a\n3D human model. Existing methods map manually labelled human pixels in real 2D\nimages onto the 3D surface, which is prone to human error, and the sparsity of\navailable annotated data often leads to sub-optimal results. We propose to\nsolve the problem of data scarcity by training 2D-3D human mapping algorithms\nusing automatically generated synthetic data for which exact and dense 2D-3D\ncorrespondence is known. Such a learning strategy using synthetic environments\nhas a high generalisation potential towards real-world data. Using different\ncamera parameter variations, background and lighting settings, we created\nprecise ground truth data that constitutes a wider distribution. We evaluate\nthe performance of models trained on synthetic using the COCO dataset and\nvalidation framework. Results show that training 2D-3D mapping network models\non synthetic data is a viable alternative to using real data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lal_M/0/1/0/all/0/1\">Mithun Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paproki_A/0/1/0/all/0/1\">Anthony Paproki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habili_N/0/1/0/all/0/1\">Nariman Habili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvado_O/0/1/0/all/0/1\">Olivier Salvado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Fixed Sub-Center: A Better Way to Capture Data Complexity. (arXiv:2203.12928v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12928","description":"<p>Treating class with a single center may hardly capture data distribution\ncomplexities. Using multiple sub-centers is an alternative way to address this\nproblem. However, highly correlated sub-classes, the classifier's parameters\ngrow linearly with the number of classes, and lack of intra-class compactness\nare three typical issues that need to be addressed in existing multi-subclass\nmethods. To this end, we propose to use Fixed Sub-Center (F-SC), which allows\nthe model to create more discrepant sub-centers while saving memory and cutting\ncomputational costs considerably. The F-SC specifically, first samples a class\ncenter Ui for each class from a uniform distribution, and then generates a\nnormal distribution for each class, where the mean is equal to Ui. Finally, the\nsub-centers are sampled based on the normal distribution corresponding to each\nclass, and the sub-centers are fixed during the training process avoiding the\noverhead of gradient calculation. Moreover, F-SC penalizes the Euclidean\ndistance between the samples and their corresponding sub-centers, it helps\nremain intra-compactness. The experimental results show that F-SC significantly\nimproves the accuracy of both image classification and fine-grained recognition\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhemin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xun Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Escaping from Language Bias and OCR Error: Semantics-Centered Text Visual Question Answering. (arXiv:2203.12929v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12929","description":"<p>Texts in scene images convey critical information for scene understanding and\nreasoning. The abilities of reading and reasoning matter for the model in the\ntext-based visual question answering (TextVQA) process. However, current\nTextVQA models do not center on the text and suffer from several limitations.\nThe model is easily dominated by language biases and optical character\nrecognition (OCR) errors due to the absence of semantic guidance in the answer\nprediction process. In this paper, we propose a novel Semantics-Centered\nNetwork (SC-Net) that consists of an instance-level contrastive semantic\nprediction module (ICSP) and a semantics-centered transformer module (SCT).\nEquipped with the two modules, the semantics-centered model can resist the\nlanguage biases and the accumulated errors from OCR. Extensive experiments on\nTextVQA and ST-VQA datasets show the effectiveness of our model. SC-Net\nsurpasses previous works with a noticeable margin and is more reasonable for\nthe TextVQA task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chengyang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gangyan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Daiqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Can Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dayong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers Meet Visual Learning Understanding: A Comprehensive Review. (arXiv:2203.12944v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12944","description":"<p>Dynamic attention mechanism and global modeling ability make Transformer show\nstrong feature learning ability. In recent years, Transformer has become\ncomparable to CNNs methods in computer vision. This review mainly investigates\nthe current research progress of Transformer in image and video applications,\nwhich makes a comprehensive overview of Transformer in visual learning\nunderstanding. First, the attention mechanism is reviewed, which plays an\nessential part in Transformer. And then, the visual Transformer model and the\nprinciple of each module are introduced. Thirdly, the existing\nTransformer-based models are investigated, and their performance is compared in\nvisual learning understanding applications. Three image tasks and two video\ntasks of computer vision are investigated. The former mainly includes image\nclassification, object detection, and image segmentation. The latter contains\nobject tracking and video classification. It is significant for comparing\ndifferent models' performance in various tasks on several public benchmark data\nsets. Finally, ten general problems are summarized, and the developing\nprospects of the visual Transformer are given in this review.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Licheng Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhixi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focus-and-Detect: A Small Object Detection Framework for Aerial Images. (arXiv:2203.12976v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12976","description":"<p>Despite recent advances, object detection in aerial images is still a\nchallenging task. Specific problems in aerial images makes the detection\nproblem harder, such as small objects, densely packed objects, objects in\ndifferent sizes and with different orientations. To address small object\ndetection problem, we propose a two-stage object detection framework called\n\"Focus-and-Detect\". The first stage which consists of an object detector\nnetwork supervised by a Gaussian Mixture Model, generates clusters of objects\nconstituting the focused regions. The second stage, which is also an object\ndetector network, predicts objects within the focal regions. Incomplete Box\nSuppression (IBS) method is also proposed to overcome the truncation effect of\nregion search approach. Results indicate that the proposed two-stage framework\nachieves an AP score of 42.06 on VisDrone validation dataset, surpassing all\nother state-of-the-art small object detection methods reported in the\nliterature, to the best of authors' knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koyun_O/0/1/0/all/0/1\">Onur Can Koyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keser_R/0/1/0/all/0/1\">Reyhan Kevser Keser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akkaya_I/0/1/0/all/0/1\">&#x130;brahim Batuhan Akkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toreyin_B/0/1/0/all/0/1\">Beh&#xe7;et U&#x11f;ur T&#xf6;reyin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Geometry Enough for Matching in Visual Localization?. (arXiv:2203.12979v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12979","description":"<p>In this paper, we propose to go beyond the well-established approach to\nvision-based localization that relies on visual descriptor matching between a\nquery image and a 3D point cloud. While matching keypoints via visual\ndescriptors makes localization highly accurate, it has significant storage\ndemands, raises privacy concerns and increases map maintenance complexity. To\nelegantly address those practical challenges for large-scale localization, we\npresent GoMatch, an alternative to visual-based matching that solely relies on\ngeometric information for matching image keypoints to maps, represented as sets\nof bearing vectors. Our novel bearing vectors representation of 3D points,\nsignificantly relieves the cross-domain challenge in geometric-based matching\nthat prevented prior work to tackle localization in a realistic environment.\nWith additional careful architecture design, GoMatch improves over prior\ngeometric-based matching work with a reduction of ($10.67m, 95.7^{\\circ}$) and\n($1.43m$, $34.7^{\\circ}$) in average median pose errors on Cambridge Landmarks\nand 7-Scenes, while requiring as little as $1.5/1.7\\%$ of storage capacity in\ncomparison to the best visual-based matching methods. This confirms its\npotential and feasibility for real-world localization and opens the door to\nfuture efforts in advancing city-scale visual localization methods that do not\nrequire storing visual descriptors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qunjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agostinho_S/0/1/0/all/0/1\">Sergio Agostinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljosa Osep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Disentangled Representation for One-shot Progressive Face Swapping. (arXiv:2203.12985v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12985","description":"<p>Although face swapping has attracted much attention in recent years, it\nremains a challenging problem. The existing methods leverage a large number of\ndata samples to explore the intrinsic properties of face swapping without\ntaking into account the semantic information of face images. Moreover, the\nrepresentation of the identity information tends to be fixed, leading to\nsuboptimal face swapping. In this paper, we present a simple yet efficient\nmethod named FaceSwapper, for one-shot face swapping based on Generative\nAdversarial Networks. Our method consists of a disentangled representation\nmodule and a semantic-guided fusion module. The disentangled representation\nmodule is composed of an attribute encoder and an identity encoder, which aims\nto achieve the disentanglement of the identity and the attribute information.\nThe identity encoder is more flexible and the attribute encoder contains more\ndetails of the attributes than its competitors. Benefiting from the\ndisentangled representation, FaceSwapper can swap face images progressively. In\naddition, semantic information is introduced into the semantic-guided fusion\nmodule to control the swapped area and model the pose and expression more\naccurately. The experimental results show that our method achieves\nstate-of-the-art results on benchmark datasets with fewer training samples. Our\ncode is publicly available at https://github.com/liqi-casia/FaceSwapper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengzhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction. (arXiv:2203.12997v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12997","description":"<p>Dimensionality reduction is crucial both for visualization and preprocessing\nhigh dimensional data for machine learning. We introduce a novel method based\non a hierarchy built on 1-nearest neighbor graphs in the original space which\nis used to preserve the grouping properties of the data distribution on\nmultiple levels. The core of the proposal is an optimization-free projection\nthat is competitive with the latest versions of t-SNE and UMAP in performance\nand visualization quality while being an order of magnitude faster in run-time.\nFurthermore, its interpretable mechanics, the ability to project new data, and\nthe natural separation of data clusters in visualizations make it a general\npurpose unsupervised dimension reduction technique. In the paper, we argue\nabout the soundness of the proposed method and evaluate it on a diverse\ncollection of datasets with sizes varying from 1K to 11M samples and dimensions\nfrom 28 to 16K. We perform comparisons with other state-of-the-art methods on\nmultiple metrics and target dimensions highlighting its efficiency and\nperformance. Code is available at https://github.com/koulakis/h-nne\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarfraz_M/0/1/0/all/0/1\">M. Saquib Sarfraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koulakis_M/0/1/0/all/0/1\">Marios Koulakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Constantin Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep-Discrete Learning Framework for Spherical Surface Registration. (arXiv:2203.12999v1 [cs.CV])","link":"http://arxiv.org/abs/2203.12999","description":"<p>Cortical surface registration is a fundamental tool for neuroimaging analysis\nthat has been shown to improve the alignment of functional regions relative to\nvolumetric approaches. Classically, image registration is performed by\noptimizing a complex objective similarity function, leading to long run times.\nThis contributes to a convention for aligning all data to a global average\nreference frame that poorly reflects the underlying cortical heterogeneity. In\nthis paper, we propose a novel unsupervised learning-based framework that\nconverts registration to a multi-label classification problem, where each point\nin a low-resolution control grid deforms to one of fixed, finite number of\nendpoints. This is learned using a spherical geometric deep learning\narchitecture, in an end-to-end unsupervised way, with regularization imposed\nusing a deep Conditional Random Field (CRF). Experiments show that our proposed\nframework performs competitively, in terms of similarity and areal distortion,\nrelative to the most popular classical surface registration algorithms and\ngenerates smoother deformations than other learning-based surface registration\nmethods, even in subjects with atypical cortical morphology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suliman_M/0/1/0/all/0/1\">Mohamed A. Suliman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. J. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fawaz_A/0/1/0/all/0/1\">Abdulah Fawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compound Domain Generalization via Meta-Knowledge Encoding. (arXiv:2203.13006v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13006","description":"<p>Domain generalization (DG) aims to improve the generalization performance for\nan unseen target domain by using the knowledge of multiple seen source domains.\nMainstream DG methods typically assume that the domain label of each source\nsample is known a priori, which is challenged to be satisfied in many\nreal-world applications. In this paper, we study a practical problem of\ncompound DG, which relaxes the discrete domain assumption to the mixed source\ndomains setting. On the other hand, current DG algorithms prioritize the focus\non semantic invariance across domains (one-vs-one), while paying less attention\nto the holistic semantic structure (many-vs-many). Such holistic semantic\nstructure, referred to as meta-knowledge here, is crucial for learning\ngeneralizable representations. To this end, we present Compound Domain\nGeneralization via Meta-Knowledge Encoding (COMEN), a general approach to\nautomatically discover and model latent domains in two steps. Firstly, we\nintroduce Style-induced Domain-specific Normalization (SDNorm) to re-normalize\nthe multi-modal underlying distributions, thereby dividing the mixture of\nsource domains into latent clusters. Secondly, we harness the prototype\nrepresentations, the centroids of classes, to perform relational modeling in\nthe embedding space with two parallel and complementary modules, which\nexplicitly encode the semantic structure for the out-of-distribution\ngeneralization. Experiments on four standard DG benchmarks reveal that COMEN\nexceeds the state-of-the-art performance without the need of domain\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiongcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. (arXiv:2203.13009v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13009","description":"<p>Recently, significant progress has been made on image denoising with strong\nsupervision from large-scale datasets. However, obtaining well-aligned\nnoisy-clean training image pairs for each specific scenario is complicated and\ncostly in practice. Consequently, applying a conventional supervised denoising\nnetwork on in-the-wild noisy inputs is not straightforward. Although several\nstudies have challenged this problem without strong supervision, they rely on\nless practical assumptions and cannot be applied to practical situations\ndirectly. To address the aforementioned challenges, we propose a novel and\npowerful self-supervised denoising method called CVF-SID based on a Cyclic\nmulti-Variate Function (CVF) module and a self-supervised image disentangling\n(SID) framework. The CVF module can output multiple decomposed variables of the\ninput and take a combination of the outputs back as an input in a cyclic\nmanner. Our CVF-SID can disentangle a clean image and noise maps from the input\nby leveraging various self-supervised loss terms. Unlike several methods that\nonly consider the signal-independent noise models, we also deal with\nsignal-dependent noise components for real-world applications. Furthermore, we\ndo not rely on any prior assumptions about the underlying noise distribution,\nmaking CVF-SID more generalizable toward realistic noise. Extensive experiments\non real-world datasets show that CVF-SID achieves state-of-the-art\nself-supervised image denoising performance and is comparable to other existing\napproaches. The code is publicly available from\nhttps://github.com/Reyhanehne/CVF-SID_PyTorch .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neshatavar_R/0/1/0/all/0/1\">Reyhaneh Neshatavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavartanoo_M/0/1/0/all/0/1\">Mohsen Yavartanoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Emotion Recognition using Visual-audio-linguistic information: A Technical Report for ABAW3. (arXiv:2203.13031v1 [cs.MM])","link":"http://arxiv.org/abs/2203.13031","description":"<p>We propose a cross-modal co-attention model for continuous emotion\nrecognition using visual-audio-linguistic information. The model consists of\nfour blocks. The visual, audio, and linguistic blocks are used to learn the\nspatial-temporal features of the multimodal input. A co-attention block is\ndesigned to fuse the learned enbeddings with the multihead co-attention\nmechanism. The visual encoding from the visual block is concatenated with the\nattention feature to emphasize the visual information. To make full use of the\ndata and alleviate over-fitting, the cross-validation is carried out on the\ntraining and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. The achieved CCC on\nvalidation set is 0.450 for valence and 0.651 for arousal, which significantly\noutperforms the baseline method with the corresponding CCC of 0.310 and 0.170,\nrespectively. The code is available at https://github.com/sucv/ABAW3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_R/0/1/0/all/0/1\">Ruyi An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13032","description":"<p>In this paper, we briefly introduce our submission to the Valence-Arousal\nEstimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)\ncompetition. Our method utilizes the multi-modal information, i.e., the visual\nand audio information, and employs a temporal encoder to model the temporal\ncontext in the videos. Besides, a smooth processor is applied to get more\nreasonable predictions, and a model ensemble strategy is used to improve the\nperformance of our proposed method. The experiment results show that our method\nachieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation\nset of the Aff-Wild2 dataset, which prove the effectiveness of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Liyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhaopei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tenggan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuanyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruichen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yannan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1\">Fengsheng Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanhe Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Prediction of Pulmonary Hypertension in Newborns using Echocardiograms. (arXiv:2203.13038v1 [eess.IV])","link":"http://arxiv.org/abs/2203.13038","description":"<p>Pulmonary hypertension (PH) in newborns and infants is a complex condition\nassociated with several pulmonary, cardiac, and systemic diseases contributing\nto morbidity and mortality. Therefore, accurate and early detection of PH is\ncrucial for successful management. Using echocardiography, the primary\ndiagnostic tool in pediatrics, human assessment is both time-consuming and\nexpertise-demanding, raising the need for an automated approach. In this work,\nwe present an interpretable multi-view video-based deep learning approach to\npredict PH for a cohort of 194 newborns using echocardiograms. We use\nspatio-temporal convolutional architectures for the prediction of PH from each\nview, and aggregate the predictions of the different views using majority\nvoting. To the best of our knowledge, this is the first work for an automated\nassessment of PH in newborns using echocardiograms. Our results show a mean\nF1-score of 0.84 for severity prediction and 0.92 for binary detection using\n10-fold cross-validation. We complement our predictions with saliency maps and\nshow that the learned model focuses on clinically relevant cardiac structures,\nmotivating its usage in clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ragnarsdottir_H/0/1/0/all/0/1\">Hanna Ragnarsdottir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Manduchi_L/0/1/0/all/0/1\">Laura Manduchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michel_H/0/1/0/all/0/1\">Holger Michel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laumer_F/0/1/0/all/0/1\">Fabian Laumer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wellmann_S/0/1/0/all/0/1\">Sven Wellmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ozkan_E/0/1/0/all/0/1\">Ece Ozkan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vogt_J/0/1/0/all/0/1\">Julia Vogt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Action Unit Recognition With Multi-models Ensembling. (arXiv:2203.13046v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13046","description":"<p>The Affective Behavior Analysis in-the-wild (ABAW) 2022 Competition gives\nAffective Computing a large promotion. In this paper, we present our method of\nAU challenge in this Competition. We use improved IResnet100 as backbone. Then\nwe train AU dataset in Aff-Wild2 on three pertained models pretrained by our\nprivate au and expression dataset, and Glint360K respectively. Finally, we\nensemble the results of our models. We achieved F1 score (macro) 0.731 on AU\nvalidation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yannan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1\">Fengsheng Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Liyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuanyuan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanhe Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simulation Benchmark for Vision-based Autonomous Navigation. (arXiv:2203.13048v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13048","description":"<p>This work introduces a simulator benchmark for vision-based autonomous\nnavigation. The simulator offers control over real world variables such as the\nenvironment, time of day, weather and traffic. The benchmark includes a modular\nintegration of different components of a full autonomous visual navigation\nstack. In the experimental part of the paper, state-of-the-art visual\nlocalization methods are evaluated as a part of the stack in realistic\nnavigation tasks. To the authors' best knowledge, the proposed benchmark is the\nfirst to study modern visual localization methods as part of a full autonomous\nvisual navigation stack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suomela_L/0/1/0/all/0/1\">Lauri Suomela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dag_A/0/1/0/all/0/1\">Atakan Dag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelman_H/0/1/0/all/0/1\">Harry Edelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamarainen_J/0/1/0/all/0/1\">Joni-Kristian K&#xe4;m&#xe4;r&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning. (arXiv:2203.13049v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13049","description":"<p>Temporal grounding in videos aims to localize one target video segment that\nsemantically corresponds to a given query sentence. Thanks to the semantic\ndiversity of natural language descriptions, temporal grounding allows activity\ngrounding beyond pre-defined classes and has received increasing attention in\nrecent years. The semantic diversity is rooted in the principle of\ncompositionality in linguistics, where novel semantics can be systematically\ndescribed by combining known words in novel ways (compositional\ngeneralization). However, current temporal grounding datasets do not\nspecifically test for the compositional generalizability. To systematically\nmeasure the compositional generalizability of temporal grounding models, we\nintroduce a new Compositional Temporal Grounding task and construct two new\ndataset splits, i.e., Charades-CG and ActivityNet-CG. Evaluating the\nstate-of-the-art methods on our new dataset splits, we empirically find that\nthey fail to generalize to queries with novel combinations of seen words. To\ntackle this challenge, we propose a variational cross-graph reasoning framework\nthat explicitly decomposes video and language into multiple structured\nhierarchies and learns fine-grained semantic correspondence among them.\nExperiments illustrate the superior compositional generalizability of our\napproach. The repository of this work is at https://github.com/YYJMJC/\nCompositional-Temporal-Grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Junlin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Long Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13052","description":"<p>Facial expression recognition plays an important role in human-computer\ninteraction. In this paper, we propose the Coarse-to-Fine Cascaded networks\nwith Smooth Predicting (CFC-SP) to improve the performance of facial expression\nrecognition. CFC-SP contains two core components, namely Coarse-to-Fine\nCascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups\nseveral similar emotions to form a rough category, and then employs a network\nto conduct a coarse but accurate classification. Later, Then, an additional\nnetwork for these grouped emotions is further used to obtain fine-grained\npredictions. For SP, it improves the recognition capability of the model by\ncapturing both universal and unique effective features. To be specific, the\nuniversal features denote the general characteristic of facial emotions and the\nunique features denote the specific characteristic of each facial expression.\nExperiments on Aff-Wild2 show the effectiveness of the proposed CFSP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fanglei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zichang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhongsong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory. (arXiv:2203.13055v1 [cs.SD])","link":"http://arxiv.org/abs/2203.13055","description":"<p>Driving 3D characters to dance following a piece of music is highly\nchallenging due to the spatial constraints applied to poses by choreography\nnorms. In addition, the generated dance sequence also needs to maintain\ntemporal coherency with different music genres. To tackle these challenges, we\npropose a novel music-to-dance framework, Bailando, with two powerful\ncomponents: 1) a choreographic memory that learns to summarize meaningful\ndancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic\nGenerative Pre-trained Transformer (GPT) that composes these units to a fluent\ndance coherent to the music. With the learned choreographic memory, dance\ngeneration is realized on the quantized units that meet high choreography\nstandards, such that the generated dancing sequences are confined within the\nspatial constraints. To achieve synchronized alignment between diverse motion\ntempos and music beats, we introduce an actor-critic-based reinforcement\nlearning scheme to the GPT with a newly-designed beat-align reward function.\nExtensive experiments on the standard benchmark demonstrate that our proposed\nframework achieves state-of-the-art performance both qualitatively and\nquantitatively. Notably, the learned choreographic memory is shown to discover\nhuman-interpretable dancing-style poses in an unsupervised manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siyao_L/0/1/0/all/0/1\">Li Siyao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Weijiang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_T/0/1/0/all/0/1\">Tianpei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIFT and SURF based feature extraction for the anomaly detection. (arXiv:2203.13068v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13068","description":"<p>In this paper, we suggest a way, how to use SIFT and SURF algorithms to\nextract the image features for anomaly detection. We use those feature vectors\nto train various classifiers on a real-world dataset in the semi -supervised\n(with a small number of faulty samples) manner with a large number of\nclassifiers and in the one-class (with no faulty samples) manner using the SVDD\nand SVM classifier. We prove, that the SIFT and SURF algorithms could be used\nas feature extractors, that they could be used to train a semi-supervised and\none-class classifier with an accuracy around 89\\% and that the performance of\nthe one-class classifier could be comparable to the semi-supervised one. We\nalso made our dataset and source code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bilik_S/0/1/0/all/0/1\">Simon Bilik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horak_K/0/1/0/all/0/1\">Karel Horak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Emotion Recognition Model with Knowledge Distillation and Task Discriminator. (arXiv:2203.13072v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13072","description":"<p>Due to the collection of big data and the development of deep learning,\nresearch to predict human emotions in the wild is being actively conducted. We\ndesigned a multi-task model using ABAW dataset to predict valence-arousal,\nexpression, and action unit through audio data and face images at in real\nworld. We trained model from the incomplete label by applying the knowledge\ndistillation technique. The teacher model was trained as a supervised learning\nmethod, and the student model was trained by using the output of the teacher\nmodel as a soft label. As a result we achieved 2.40 in Multi Task Learning task\nvalidation dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1\">Euiseok Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_G/0/1/0/all/0/1\">Geesung Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Sejoon Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AziNorm: Exploiting the Radial Symmetry of Point Cloud for Azimuth-Normalized 3D Perception. (arXiv:2203.13090v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13090","description":"<p>Studying the inherent symmetry of data is of great importance in machine\nlearning. Point cloud, the most important data format for 3D environmental\nperception, is naturally endowed with strong radial symmetry. In this work, we\nexploit this radial symmetry via a divide-and-conquer strategy to boost 3D\nperception performance and ease optimization. We propose Azimuth Normalization\n(AziNorm), which normalizes the point clouds along the radial direction and\neliminates the variability brought by the difference of azimuth. AziNorm can be\nflexibly incorporated into most LiDAR-based perception methods. To validate its\neffectiveness and generalization ability, we apply AziNorm in both object\ndetection and semantic segmentation. For detection, we integrate AziNorm into\ntwo representative detection methods, the one-stage SECOND detector and the\nstate-of-the-art two-stage PV-RCNN detector. Experiments on Waymo Open Dataset\ndemonstrate that AziNorm improves SECOND and PV-RCNN by 7.03 mAPH and 3.01 mAPH\nrespectively. For segmentation, we integrate AziNorm into KPConv. On\nSemanticKitti dataset, AziNorm improves KPConv by 1.6/1.1 mIoU on val/test set.\nBesides, AziNorm remarkably improves data efficiency and accelerates\nconvergence, reducing the requirement of data amounts or training epochs by an\norder of magnitude. SECOND w/ AziNorm can significantly outperform fully\ntrained vanilla SECOND, even trained with only 10% data or 10% epochs. Code and\nmodels are available at https://github.com/hustvl/AziNorm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Preliminary Research on Space Situational Awareness Based on Event Cameras. (arXiv:2203.13093v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13093","description":"<p>Event camera is a new type of sensor that is different from traditional\ncameras. Each pixel is triggered asynchronously by an event. The trigger event\nis the change of the brightness irradiated on the pixel. If the increment or\ndecrement is higher than a certain threshold, the event is output. Compared\nwith traditional cameras, event cameras have the advantages of high temporal\nresolution, low latency, high dynamic range, low bandwidth and low power\nconsumption. We carried out a series of observation experiments in a simulated\nspace lighting environment. The experimental results show that the event camera\ncan give full play to the above advantages in space situational awareness. This\narticle first introduces the basic principles of the event camera, then\nanalyzes its advantages and disadvantages, then introduces the observation\nexperiment and analyzes the experimental results, and finally, a workflow of\nspace situational awareness based on event cameras is given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1\">Kun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengju Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yongfeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuqiang Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IA-FaceS, Bidirectional Method, Disentangled Attribute Manipulation, Flexible Component Editing. (arXiv:2203.13097v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13097","description":"<p>Semantic face editing has achieved substantial progress in recent years.\nKnown as a growingly popular method, latent space manipulation performs face\nediting by changing the latent code of an input face to liberate users from\npainting skills. However, previous latent space manipulation methods usually\nencode an entire face into a single low-dimensional embedding, which constrains\nthe reconstruction capacity and the control flexibility of facial components,\nsuch as eyes and nose. This paper proposes IA-FaceS as a bidirectional method\nfor disentangled face attribute manipulation as well as flexible, controllable\ncomponent editing without the need for segmentation masks or sketches in the\noriginal image. To strike a balance between the reconstruction capacity and the\ncontrol flexibility, the encoder is designed as a multi-head structure to yield\nembeddings for reconstruction and control, respectively: a high-dimensional\ntensor with spatial properties for consistent reconstruction and four\nlow-dimensional facial component embeddings for semantic face editing.\nManipulating the separate component embeddings can help achieve disentangled\nattribute manipulation and flexible control of facial components. To further\ndisentangle the highly-correlated components, a component adaptive modulation\n(CAM) module is proposed for the decoder. The semantic single-eye editing is\ndeveloped for the first time without any input visual guidance, such as\nsegmentation masks or sketches. According to the experimental results, IA-FaceS\nestablishes a good balance between maintaining image details and performing\nflexible face manipulation. Both quantitative and qualitative results indicate\nthat the proposed method outperforms the other techniques in reconstruction,\nface attribute manipulation, and component transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Shikui Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R-DFCIL: Relation-Guided Representation Learning for Data-Free Class Incremental Learning. (arXiv:2203.13104v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13104","description":"<p>Class-Incremental Learning (CIL) struggles with catastrophic forgetting when\nlearning new knowledge, and Data-Free CIL (DFCIL) is even more challenging\nwithout access to the training data of previous classes. Though recent DFCIL\nworks introduce techniques such as model inversion to synthesize data for\nprevious classes, they fail to overcome forgetting due to the severe domain gap\nbetween the synthetic and real data. To address this issue, this paper proposes\nrelation-guided representation learning (RRL) for DFCIL, dubbed R-DFCIL. In\nRRL, we introduce relational knowledge distillation to flexibly transfer the\nstructural relation of new data from the old model to the current model. Our\nRRL-boosted DFCIL can guide the current model to learn representations of new\nclasses better compatible with representations of previous classes, which\ngreatly reduces forgetting while improving plasticity. To avoid the mutual\ninterference between representation and classifier learning, we employ local\nrather than global classification loss during RRL. After RRL, the\nclassification head is fine-tuned with global class-balanced classification\nloss to address the data imbalance issue as well as learn the decision boundary\nbetween new and previous classes. Extensive experiments on CIFAR100,\nTiny-ImageNet200, and ImageNet100 demonstrate that our R-DFCIL significantly\nsurpasses previous approaches and achieves a new state-of-the-art performance\nfor DFCIL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiankun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Prediction of Action Target in 3D. (arXiv:2203.13116v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13116","description":"<p>We are interested in anticipating as early as possible the target location of\na person's object manipulation action in a 3D workspace from egocentric vision.\nIt is important in fields like human-robot collaboration, but has not yet\nreceived enough attention from vision and learning communities. To stimulate\nmore research on this challenging egocentric vision task, we propose a large\nmultimodality dataset of more than 1 million frames of RGB-D and IMU streams,\nand provide evaluation metrics based on our high-quality 2D and 3D labels from\nsemi-automatic annotation. Meanwhile, we design baseline methods using\nrecurrent neural networks and conduct various ablation studies to validate\ntheir effectiveness. Our results demonstrate that this new task is worthy of\nfurther study by researchers in robotics, vision, and learning communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Ziang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_A/0/1/0/all/0/1\">Andrew Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Benjamin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Luoyao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-ray Dissectography Improves Lung Nodule Detection. (arXiv:2203.13118v1 [eess.IV])","link":"http://arxiv.org/abs/2203.13118","description":"<p>Although radiographs are the most frequently used worldwide due to their\ncost-effectiveness and widespread accessibility, the structural superposition\nalong the x-ray paths often renders suspicious or concerning lung nodules\ndifficult to detect. In this study, we apply \"X-ray dissectography\" to dissect\nlungs digitally from a few radiographic projections, suppress the interference\nof irrelevant structures, and improve lung nodule detectability. For this\npurpose, a collaborative detection network is designed to localize lung nodules\nin 2D dissected projections and 3D physical space. Our experimental results\nshow that our approach can significantly improve the average precision by 20+%\nin comparison with the common baseline that detects lung nodules from original\nprojections using a popular detection network. Potentially, this approach could\nhelp re-design the current X-ray imaging protocols and workflows and improve\nthe diagnostic performance of chest radiographs in lung diseases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dasegowda_G/0/1/0/all/0/1\">Giridhar Dasegowda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalra_M/0/1/0/all/0/1\">Mannudeep K. Kalra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature visualization for convolutional neural network models trained on neuroimaging data. (arXiv:2203.13120v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13120","description":"<p>A major prerequisite for the application of machine learning models in\nclinical decision making is trust and interpretability. Current explainability\nstudies in the neuroimaging community have mostly focused on explaining\nindividual decisions of trained models, e.g. obtained by a convolutional neural\nnetwork (CNN). Using attribution methods such as layer-wise relevance\npropagation or SHAP heatmaps can be created that highlight which regions of an\ninput are more relevant for the decision than others. While this allows the\ndetection of potential data set biases and can be used as a guide for a human\nexpert, it does not allow an understanding of the underlying principles the\nmodel has learned. In this study, we instead show, to the best of our\nknowledge, for the first time results using feature visualization of\nneuroimaging CNNs. Particularly, we have trained CNNs for different tasks\nincluding sex classification and artificial lesion classification based on\nstructural magnetic resonance imaging (MRI) data. We have then iteratively\ngenerated images that maximally activate specific neurons, in order to\nvisualize the patterns they respond to. To improve the visualizations we\ncompared several regularization strategies. The resulting images reveal the\nlearned concepts of the artificial lesions, including their shapes, but remain\nhard to interpret for abstract features in the sex classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eitel_F/0/1/0/all/0/1\">Fabian Eitel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melkonyan_A/0/1/0/all/0/1\">Anna Melkonyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_K/0/1/0/all/0/1\">Kerstin Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving Window Regression: A Novel Approach to Ordinal Regression. (arXiv:2203.13122v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13122","description":"<p>A novel ordinal regression algorithm, called moving window regression (MWR),\nis proposed in this paper. First, we propose the notion of relative rank\n($\\rho$-rank), which is a new order representation scheme for input and\nreference instances. Second, we develop global and local relative regressors\n($\\rho$-regressors) to predict $\\rho$-ranks within entire and specific rank\nranges, respectively. Third, we refine an initial rank estimate iteratively by\nselecting two reference instances to form a search window and then estimating\nthe $\\rho$-rank within the window. Extensive experiments results show that the\nproposed algorithm achieves the state-of-the-art performances on various\nbenchmark datasets for facial age estimation and historical color image\nclassification. The codes are available at https://github.com/nhshin-mcl/MWR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_N/0/1/0/all/0/1\">Nyeong-Ho Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seon-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. (arXiv:2203.13131v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13131","description":"<p>Recent text-to-image generation methods provide a simple yet exciting\nconversion capability between text and image domains. While these methods have\nincrementally improved the generated image fidelity and text relevancy, several\npivotal gaps remain unanswered, limiting applicability and quality. We propose\na novel text-to-image method that addresses these gaps by (i) enabling a simple\ncontrol mechanism complementary to text in the form of a scene, (ii)\nintroducing elements that substantially improve the tokenization process by\nemploying domain-specific knowledge over key image regions (faces and salient\nobjects), and (iii) adapting classifier-free guidance for the transformer use\ncase. Our model achieves state-of-the-art FID and human evaluation results,\nunlocking the ability to generate high fidelity images in a resolution of\n512x512 pixels, significantly improving visual quality. Through scene\ncontrollability, we introduce several new capabilities: (i) Scene editing, (ii)\ntext editing with anchor scenes, (iii) overcoming out-of-distribution text\nprompts, and (iv) story illustration generation, as demonstrated in the story\nwe wrote.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gafni_O/0/1/0/all/0/1\">Oran Gafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashual_O/0/1/0/all/0/1\">Oron Ashual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheynin_S/0/1/0/all/0/1\">Shelly Sheynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taigman_Y/0/1/0/all/0/1\">Yaniv Taigman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-based Learning of Parameterized Thermodynamics from Real-time Thermography. (arXiv:2203.13148v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13148","description":"<p>Progress in automatic control of thermal processes has long been limited by\nthe difficulty of obtaining high-fidelity thermodynamic models. Traditionally,\nin complex thermodynamic systems, it is often infeasible to estimate the\nthermophysical parameters of spatiotemporally varying processes, forcing the\nadoption of model-free control architectures. This comes at the cost of losing\nany robustness guarantees, and implies a need for extensive real-life testing.\nIn recent years, however, infrared cameras and other thermographic equipment\nhave become readily applicable to these processes, allowing for a real-time,\nnon-invasive means of sensing the thermal state of a process. In this work, we\npresent a novel physics-based approach to learning a thermal process's dynamics\ndirectly from such real-time thermographic data, while focusing attention on\nregions with high thermal activity. We call this process, which applies to any\nhigher-dimensional scalar field, attention-based noise robust averaging (ANRA).\nGiven a partial-differential equation model structure, we show that our\napproach is robust against noise, and can be used to initialize optimization\nroutines to further refine parameter estimates. We demonstrate our method on\nseveral simulation examples, as well as by applying it to electrosurgical\nthermal response data on in vivo porcine skin tissue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+El_Kebir_H/0/1/0/all/0/1\">Hamza El-Kebir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentsman_J/0/1/0/all/0/1\">Joseph Bentsman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation. (arXiv:2203.13161v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13161","description":"<p>Generating speech-consistent body and gesture movements is a long-standing\nproblem in virtual avatar creation. Previous studies often synthesize pose\nmovement in a holistic manner, where poses of all joints are generated\nsimultaneously. Such a straightforward pipeline fails to generate fine-grained\nco-speech gestures. One observation is that the hierarchical semantics in\nspeech and the hierarchical structures of human gestures can be naturally\ndescribed into multiple granularities and associated together. To fully utilize\nthe rich connections between speech audio and human gestures, we propose a\nnovel framework named Hierarchical Audio-to-Gesture (HA2G) for co-speech\ngesture generation. In HA2G, a Hierarchical Audio Learner extracts audio\nrepresentations across semantic granularities. A Hierarchical Pose Inferer\nsubsequently renders the entire human pose gradually in a hierarchical manner.\nTo enhance the quality of synthesized gestures, we develop a contrastive\nlearning strategy based on audio-text alignment for better audio\nrepresentations. Extensive experiments and human evaluation demonstrate that\nthe proposed method renders realistic co-speech gestures and outperforms\nprevious methods in a clear margin. Project page:\nhttps://alvinliu0.github.io/projects/HA2G\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xinyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bolei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Video-centralised Transformer for Video Face Clustering. (arXiv:2203.13166v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13166","description":"<p>This paper presents a novel method for face clustering in videos using a\nvideo-centralised transformer. Previous works often employed contrastive\nlearning to learn frame-level representation and used average pooling to\naggregate the features along the temporal dimension. This approach may not\nfully capture the complicated video dynamics. In addition, despite the recent\nprogress in video-based contrastive learning, few have attempted to learn a\nself-supervised clustering-friendly face representation that benefits the video\nface clustering task. To overcome these limitations, our method employs a\ntransformer to directly learn video-level representations that can better\nreflect the temporally-varying property of faces in videos, while we also\npropose a video-centralised self-supervised framework to train the transformer\nmodel. We also investigate face clustering in egocentric videos, a\nfast-emerging field that has not been studied yet in works related to face\nclustering. To this end, we present and release the first large-scale\negocentric video face clustering dataset named EasyCom-Clustering. We evaluate\nour proposed method on both the widely used Big Bang Theory (BBT) dataset and\nthe new EasyCom-Clustering dataset. Results show the performance of our\nvideo-centralised transformer has surpassed all previous state-of-the-art\nmethods on both benchmarks, exhibiting a self-attentive understanding of face\nvideos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yiming Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13167","description":"<p>In this paper, we investigate the continual learning of Vision Transformers\n(ViT) for the challenging exemplar-free scenario, with special focus on how to\nefficiently distill the knowledge of its crucial self-attention mechanism\n(SAM). Our work takes an initial step towards a surgical investigation of SAM\nfor designing coherent continual learning methods in ViTs. We first carry out\nan evaluation of established continual learning regularization techniques. We\nthen examine the effect of regularization when applied to two key enablers of\nSAM: (a) the contextualized embedding layers, for their ability to capture\nwell-scaled representations with respect to the values, and (b) the prescaled\nattention maps, for carrying value-independent global contextual information.\nWe depict the perks of each distilling strategy on two image recognition\nbenchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall\naccuracy, (b) helps enhance the rigidity by maintaining competitive\nperformances. Furthermore, we identify the limitation imposed by the symmetric\nnature of regularization losses. To alleviate this, we propose an asymmetric\nvariant and apply it to the pooled output distillation (POD) loss adapted for\nViTs. Our experiments confirm that introducing asymmetry to POD boosts its\nplasticity while retaining stability across (a) and (b). Moreover, we\nacknowledge low forgetting measures for all the compared methods, indicating\nthat ViTs might be naturally inclined continual learner\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1\">Francesco Pelosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Saurav Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1\">Andrea Torsello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raducanu_B/0/1/0/all/0/1\">Bogdan Raducanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1\">Joost van de Weijer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Motion Segmentation. (arXiv:2203.13185v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13185","description":"<p>Motion segmentation is a challenging problem that seeks to identify\nindependent motions in two or several input images. This paper introduces the\nfirst algorithm for motion segmentation that relies on adiabatic quantum\noptimization of the objective function. The proposed method achieves on-par\nperformance with the state of the art on problem instances which can be mapped\nto modern quantum annealers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arrigoni_F/0/1/0/all/0/1\">Federica Arrigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menapace_W/0/1/0/all/0/1\">Willi Menapace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benkner_M/0/1/0/all/0/1\">Marcel Seelbach Benkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decouple-and-Sample: Protecting sensitive information in task agnostic data release. (arXiv:2203.13204v1 [cs.CR])","link":"http://arxiv.org/abs/2203.13204","description":"<p>We propose sanitizer, a framework for secure and task-agnostic data release.\nWhile releasing datasets continues to make a big impact in various applications\nof computer vision, its impact is mostly realized when data sharing is not\ninhibited by privacy concerns. We alleviate these concerns by sanitizing\ndatasets in a two-stage process. First, we introduce a global decoupling stage\nfor decomposing raw data into sensitive and non-sensitive latent\nrepresentations. Secondly, we design a local sampling stage to synthetically\ngenerate sensitive information with differential privacy and merge it with\nnon-sensitive latent features to create a useful representation while\npreserving the privacy. This newly formed latent information is a task-agnostic\nrepresentation of the original dataset with anonymized sensitive information.\nWhile most algorithms sanitize data in a task-dependent manner, a few\ntask-agnostic sanitization techniques sanitize data by censoring sensitive\ninformation. In this work, we show that a better privacy-utility trade-off is\nachieved if sensitive information can be synthesized privately. We validate the\neffectiveness of the sanitizer by outperforming state-of-the-art baselines on\nthe existing benchmark tasks and demonstrating tasks that are not possible\nusing existing techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Abhishek Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garza_E/0/1/0/all/0/1\">Ethan Garza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ayush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vepakomma_P/0/1/0/all/0/1\">Praneeth Vepakomma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Vivek Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Perturbation Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow. (arXiv:2203.13214v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13214","description":"<p>Recent optical flow methods are almost exclusively judged in terms of\naccuracy, while analyzing their robustness is often neglected. Although\nadversarial attacks offer a useful tool to perform such an analysis, current\nattacks on optical flow methods rather focus on real-world attacking scenarios\nthan on a worst case robustness assessment. Hence, in this work, we propose a\nnovel adversarial attack - the Perturbation Constrained Flow Attack (PCFA) -\nthat emphasizes destructivity over applicability as a real-world attack. More\nprecisely, PCFA is a global attack that optimizes adversarial perturbations to\nshift the predicted flow towards a specified target flow, while keeping the L2\nnorm of the perturbation below a chosen bound. Our experiments not only\ndemonstrate PCFA's applicability in white- and black-box settings, but also\nshow that it finds stronger adversarial samples for optical flow than previous\nattacking frameworks. Moreover, based on these strong samples, we provide the\nfirst common ranking of optical flow methods in the literature considering both\nprediction quality and adversarial robustness, indicating that high quality\nmethods are not necessarily robust. Our source code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmalfuss_J/0/1/0/all/0/1\">Jenny Schmalfuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholze_P/0/1/0/all/0/1\">Philipp Scholze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruhn_A/0/1/0/all/0/1\">Andr&#xe9;s Bruhn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Neighbor Style Transfer. (arXiv:2203.13215v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13215","description":"<p>We propose Neural Neighbor Style Transfer (NNST), a pipeline that offers\nstate-of-the-art quality, generalization, and competitive efficiency for\nartistic style transfer. Our approach is based on explicitly replacing neural\nfeatures extracted from the content input (to be stylized) with those from a\nstyle exemplar, then synthesizing the final output based on these rearranged\nfeatures. While the spirit of our approach is similar to prior work, we show\nthat our design decisions dramatically improve the final visual quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1\">Nicholas Kolkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucera_M/0/1/0/all/0/1\">Michal Kucera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paris_S/0/1/0/all/0/1\">Sylvain Paris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sykora_D/0/1/0/all/0/1\">Daniel Sykora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Expression Recognition based on Multi-head Cross Attention Network. (arXiv:2203.13235v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13235","description":"<p>Facial expression in-the-wild is essential for various interactive computing\ndomains. In this paper, we proposed an extended version of DAN model to address\nthe VA estimation and facial expression challenges introduced in ABAW 2022. Our\nmethod produced preliminary results of 0.44 of mean CCC value for the VA\nestimation task, and 0.33 of the average F1 score for the expression\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jae-Yeop Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yeong-Gi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yuchul Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jin-Woo Jeong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-set Recognition via Augmentation-based Similarity Learning. (arXiv:2203.13238v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13238","description":"<p>The primary assumption of conventional supervised learning or classification\nis that the test samples are drawn from the same distribution as the training\nsamples, which is called closed set learning or classification. In many\npractical scenarios, this is not the case because there are unknowns or unseen\nclass samples in the test data, which is called the open set scenario, and the\nunknowns need to be detected. This problem is referred to as the open set\nrecognition problem and is important in safety-critical applications. We\npropose to detect unknowns (or unseen class samples) through learning pairwise\nsimilarities. The proposed method works in two steps. It first learns a closed\nset classifier using the seen classes that have appeared in training and then\nlearns how to compare seen classes with pseudo-unseen (automatically generated\nunseen class samples). The pseudo-unseen generation is carried out by\nperforming distribution shifting augmentations on the seen or training samples.\nWe call our method OPG (Open set recognition based on Pseudo unseen data\nGeneration). The experimental evaluation shows that the learned\nsimilarity-based features can successfully distinguish seen from unseen in\nbenchmark datasets for open set recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1\">Sepideh Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+shu_L/0/1/0/all/0/1\">Lei shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Representation Separation Perspective to Correspondences-free Unsupervised 3D Point Cloud Registration. (arXiv:2203.13239v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13239","description":"<p>3D point cloud registration in remote sensing field has been greatly advanced\nby deep learning based methods, where the rigid transformation is either\ndirectly regressed from the two point clouds (correspondences-free approaches)\nor computed from the learned correspondences (correspondences-based\napproaches). Existing correspondences-free methods generally learn the holistic\nrepresentation of the entire point cloud, which is fragile for partial and\nnoisy point clouds. In this paper, we propose a correspondences-free\nunsupervised point cloud registration (UPCR) method from the representation\nseparation perspective. First, we model the input point cloud as a combination\nof pose-invariant representation and pose-related representation. Second, the\npose-related representation is used to learn the relative pose wrt a \"latent\ncanonical shape\" for the source and target point clouds respectively. Third,\nthe rigid transformation is obtained from the above two learned relative poses.\nOur method not only filters out the disturbance in pose-invariant\nrepresentation but also is robust to partial-to-partial point clouds or noise.\nExperiments on benchmark datasets demonstrate that our unsupervised method\nachieves comparable if not better performance than state-of-the-art supervised\nregistration methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiadai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xibin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VRNet: Learning the Rectified Virtual Corresponding Points for 3D Point Cloud Registration. (arXiv:2203.13241v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13241","description":"<p>3D point cloud registration is fragile to outliers, which are labeled as the\npoints without corresponding points. To handle this problem, a widely adopted\nstrategy is to estimate the relative pose based only on some accurate\ncorrespondences, which is achieved by building correspondences on the\nidentified inliers or by selecting reliable ones. However, these approaches are\nusually complicated and time-consuming. By contrast, the virtual point-based\nmethods learn the virtual corresponding points (VCPs) for all source points\nuniformly without distinguishing the outliers and the inliers. Although this\nstrategy is time-efficient, the learned VCPs usually exhibit serious collapse\ndegeneration due to insufficient supervision and the inherent distribution\nlimitation. In this paper, we propose to exploit the best of both worlds and\npresent a novel robust 3D point cloud registration framework. We follow the\nidea of the virtual point-based methods but learn a new type of virtual points\ncalled rectified virtual corresponding points (RCPs), which are defined as the\npoint set with the same shape as the source and with the same pose as the\ntarget. Hence, a pair of consistent point clouds, i.e. source and RCPs, is\nformed by rectifying VCPs to RCPs (VRNet), through which reliable\ncorrespondences between source and RCPs can be accurately obtained. Since the\nrelative pose between source and RCPs is the same as the relative pose between\nsource and target, the input point clouds can be registered naturally.\nSpecifically, we first construct the initial VCPs by using an estimated soft\nmatching matrix to perform a weighted average on the target points. Then, we\ndesign a correction-walk module to learn an offset to rectify VCPs to RCPs,\nwhich effectively breaks the distribution limitation of VCPs. Finally, we\ndevelop a hybrid loss function to enforce the shape and geometry structure\nconsistency ...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiadai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Bin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer. (arXiv:2203.13248v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13248","description":"<p>Recent studies on StyleGAN show high performance on artistic portrait\ngeneration by transfer learning with limited data. In this paper, we explore\nmore challenging exemplar-based high-resolution portrait style transfer by\nintroducing a novel DualStyleGAN with flexible control of dual styles of the\noriginal face domain and the extended artistic portrait domain. Different from\nStyleGAN, DualStyleGAN provides a natural way of style transfer by\ncharacterizing the content and style of a portrait with an intrinsic style path\nand a new extrinsic style path, respectively. The delicately designed extrinsic\nstyle path enables our model to modulate both the color and complex structural\nstyles hierarchically to precisely pastiche the style example. Furthermore, a\nnovel progressive fine-tuning scheme is introduced to smoothly transform the\ngenerative space of the model to the target domain, even with the above\nmodifications on the network architecture. Experiments demonstrate the\nsuperiority of DualStyleGAN over state-of-the-art methods in high-quality\nportrait style transfer and flexible style control.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigDetection: A Large-scale Benchmark for Improved Object Detector Pre-training. (arXiv:2203.13249v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13249","description":"<p>Multiple datasets and open challenges for object detection have been\nintroduced in recent years. To build more general and powerful object detection\nsystems, in this paper, we construct a new large-scale benchmark termed\nBigDetection. Our goal is to simply leverage the training data from existing\ndatasets (LVIS, OpenImages and Object365) with carefully designed principles,\nand curate a larger dataset for improved detector pre-training. Specifically,\nwe generate a new taxonomy which unifies the heterogeneous label spaces from\ndifferent sources. Our BigDetection dataset has 600 object categories and\ncontains over 3.4M training images with 36M bounding boxes. It is much larger\nin multiple dimensions than previous benchmarks, which offers both\nopportunities and challenges. Extensive experiments demonstrate its validity as\na new benchmark for evaluating different object detection methods, and its\neffectiveness as a pre-training dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Likun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Tracking Transformers. (arXiv:2203.13250v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13250","description":"<p>We present a novel transformer-based architecture for global multi-object\ntracking. Our network takes a short sequence of frames as input and produces\nglobal trajectories for all objects. The core component is a global tracking\ntransformer that operates on objects from all frames in the sequence. The\ntransformer encodes object features from all frames, and uses trajectory\nqueries to group them into trajectories. The trajectory queries are object\nfeatures from a single frame and naturally produce unique trajectories. Our\nglobal tracking transformer does not require intermediate pairwise grouping or\ncombinatorial association, and can be jointly trained with an object detector.\nIt achieves competitive performance on the popular MOT17 benchmark, with 75.3\nMOTA and 59.1 HOTA. More importantly, our framework seamlessly integrates into\nstate-of-the-art large-vocabulary detectors to track any objects. Experiments\non the challenging TAO dataset show that our framework consistently improves\nupon baselines that are based on pairwise association, outperforming published\nworks by a significant 7.7 tracking mAP. Code is available at\nhttps://github.com/xingyizhou/GTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_T/0/1/0/all/0/1\">Tianwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Phillip Kr&#xe4;henb&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation. (arXiv:2203.13251v1 [cs.RO])","link":"http://arxiv.org/abs/2203.13251","description":"<p>Optimizing behaviors for dexterous manipulation has been a longstanding\nchallenge in robotics, with a variety of methods from model-based control to\nmodel-free reinforcement learning having been previously explored in\nliterature. Perhaps one of the most powerful techniques to learn complex\nmanipulation strategies is imitation learning. However, collecting and learning\nfrom demonstrations in dexterous manipulation is quite challenging. The\ncomplex, high-dimensional action-space involved with multi-finger control often\nleads to poor sample efficiency of learning-based methods. In this work, we\npropose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning\nframework for dexterous manipulation. DIME only requires a single RGB camera to\nobserve a human operator and teleoperate our robotic hand. Once demonstrations\nare collected, DIME employs standard imitation learning methods to train\ndexterous manipulation policies. On both simulation and real robot benchmarks\nwe demonstrate that DIME can be used to solve complex, in-hand manipulation\ntasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro\nhand. Our framework along with pre-collected demonstrations is publicly\navailable at https://nyu-robot-learning.github.io/dime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arunachalam_S/0/1/0/all/0/1\">Sridhar Pandian Arunachalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1\">Sneha Silwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_B/0/1/0/all/0/1\">Ben Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1\">Lerrel Pinto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Instance Segmentation via Multi-scale Spatio-temporal Split Attention Transformer. (arXiv:2203.13253v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13253","description":"<p>State-of-the-art transformer-based video instance segmentation (VIS)\napproaches typically utilize either single-scale spatio-temporal features or\nper-frame multi-scale features during the attention computations. We argue that\nsuch an attention computation ignores the multi-scale spatio-temporal feature\nrelationships that are crucial to tackle target appearance deformations in\nvideos. To address this issue, we propose a transformer-based VIS framework,\nnamed MS-STS VIS, that comprises a novel multi-scale spatio-temporal split\n(MS-STS) attention module in the encoder. The proposed MS-STS module\neffectively captures spatio-temporal feature relationships at multiple scales\nacross frames in a video. We further introduce an attention block in the\ndecoder to enhance the temporal consistency of the detected instances in\ndifferent frames of a video. Moreover, an auxiliary discriminator is introduced\nduring training to ensure better foreground-background separability within the\nmulti-scale spatio-temporal feature space. We conduct extensive experiments on\ntwo benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves\nstate-of-the-art performance on both benchmarks. When using the ResNet50\nbackbone, our MS-STS achieves a mask AP of 50.1 %, outperforming the best\nreported results in literature by 2.7 % and by 4.8 % at higher overlap\nthreshold of AP_75, while being comparable in model size and speed on\nYoutube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS\nachieves mask AP of 61.0 % on Youtube-VIS 2019 val. set. Our code and models\nare available at https://github.com/OmkarThawakar/MSSTS-VIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thawakar_O/0/1/0/all/0/1\">Omkar Thawakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Sanath Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiale Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1\">Hisham Cholakkal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1\">Rao Muhammad Anwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Haris Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation. (arXiv:2203.13254v1 [cs.CV])","link":"http://arxiv.org/abs/2203.13254","description":"<p>Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is\na long-standing problem in computer vision. Driven by end-to-end deep learning,\nrecent studies suggest interpreting PnP as a differentiable layer, so that\n2D-3D point correspondences can be partly learned by backpropagating the\ngradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D\npoints from scratch fails to converge with existing approaches, since the\ndeterministic pose is inherently non-differentiable. In this paper, we propose\nthe EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation,\nwhich outputs a distribution of pose on the SE(3) manifold, essentially\nbringing categorical Softmax to the continuous domain. The 2D-3D coordinates\nand corresponding weights are treated as intermediate variables learned by\nminimizing the KL divergence between the predicted and target pose\ndistribution. The underlying principle unifies the existing approaches and\nresembles the attention mechanism. EPro-PnP significantly outperforms\ncompetitive baselines, closing the gap between PnP-based method and the\ntask-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D\nobject detection benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hansheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1\">Wei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Lu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A real-time and unsupervised face Re-Identification system for Human-Robot Interaction. (arXiv:1804.03547v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.03547","description":"<p>In the context of Human-Robot Interaction (HRI), face Re-Identification (face\nRe-ID) aims to verify if certain detected faces have already been observed by\nrobots. The ability of distinguishing between different users is crucial in\nsocial robots as it will enable the robot to tailor the interaction strategy\ntoward the users' individual preferences. So far face recognition research has\nachieved great success, however little attention has been paid to the realistic\napplications of Face Re-ID in social robots. In this paper, we present an\neffective and unsupervised face Re-ID system which simultaneously re-identifies\nmultiple faces for HRI. This Re-ID system employs Deep Convolutional Neural\nNetworks to extract features, and an online clustering algorithm to determine\nthe face's ID. Its performance is evaluated on two datasets: the TERESA video\ndataset collected by the TERESA robot, and the YouTube Face Dataset (YTF\nDataset). We demonstrate that the optimised combination of techniques achieves\nan overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on\nYTF dataset. We have implemented the proposed method into a software module in\nthe HCI^2 Framework for it to be further integrated into the TERESA robot, and\nhas achieved real-time performance at 10~26 Frames per second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1\">Stavros Petridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1\">Maja Pantic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue. (arXiv:2010.00361v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.00361","description":"<p>A goal-oriented visual dialogue involves multi-turn interactions between two\nagents, Questioner and Oracle. During which, the answer given by Oracle is of\ngreat significance, as it provides golden response to what Questioner concerns.\nBased on the answer, Questioner updates its belief on target visual content and\nfurther raises another question. Notably, different answers drive into\ndifferent visual beliefs and future questions. However, existing methods always\nindiscriminately encode answers after much longer questions, resulting in a\nweak utilization of answers. In this paper, we propose an Answer-Driven Visual\nState Estimator (ADVSE) to impose the effects of different answers on visual\nstates. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture\nthe answer-driven effect on visual attention by sharpening question-related\nattention and adjusting it by answer-based logical operation at each turn. Then\nbased on the focusing attention, we get the visual state estimation by\nConditional Visual Information Fusion (CVIF), where overall information and\ndifference information are fused conditioning on the question-answer state. We\nevaluate the proposed ADVSE to both question generator and guesser tasks on the\nlarge-scale GuessWhat?! dataset and achieve the state-of-the-art performances\non both tasks. The qualitative results indicate that the ADVSE boosts the agent\nto generate highly efficient questions and obtains reliable visual attentions\nduring the reasonable question generation and guess processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fangxiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yushu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huixing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XraySyn: Realistic View Synthesis From a Single Radiograph Through CT Priors. (arXiv:2012.02407v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.02407","description":"<p>A radiograph visualizes the internal anatomy of a patient through the use of\nX-ray, which projects 3D information onto a 2D plane. Hence, radiograph\nanalysis naturally requires physicians to relate the prior about 3D human\nanatomy to 2D radiographs. Synthesizing novel radiographic views in a small\nrange can assist physicians in interpreting anatomy more reliably; however,\nradiograph view synthesis is heavily ill-posed, lacking in paired data, and\nlacking in differentiable operations to leverage learning-based approaches. To\naddress these problems, we use Computed Tomography (CT) for radiograph\nsimulation and design a differentiable projection algorithm, which enables us\nto achieve geometrically consistent transformations between the radiography and\nCT domains. Our method, XraySyn, can synthesize novel views on real radiographs\nthrough a combination of realistic simulation and finetuning on real\nradiographs. To the best of our knowledge, this is the first work on radiograph\nview synthesis. We show that by gaining an understanding of radiography in 3D\nspace, our method can be applied to radiograph bone extraction and suppression\nwithout groundtruth bone labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Cheng Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1\">Haofu Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_G/0/1/0/all/0/1\">Gina Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shaohua Kevin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep One-Class Classification via Interpolated Gaussian Descriptor. (arXiv:2101.10043v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.10043","description":"<p>One-class classification (OCC) aims to learn an effective data description to\nenclose all normal training samples and detect anomalies based on the deviation\nfrom the data description. Current state-of-the-art OCC models learn a compact\nnormality description by hyper-sphere minimisation, but they often suffer from\noverfitting the training data, especially when the training set is small or\ncontaminated with anomalous samples. To address this issue, we introduce the\ninterpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a\none-class Gaussian anomaly classifier trained with adversarially interpolated\ntraining samples. The Gaussian anomaly classifier differentiates the training\nsamples based on their distance to the Gaussian centre and the standard\ndeviation of these distances, offering the model a discriminability w.r.t. the\ngiven samples during training. The adversarial interpolation is enforced to\nconsistently learn a smooth Gaussian descriptor, even when the training data is\nsmall or contaminated with anomalous samples. This enables our model to learn\nthe data description based on the representative normal samples rather than\nfringe or anomalous samples, resulting in significantly improved normality\ndescription. In extensive experiments on diverse popular benchmarks, including\nMNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves\nbetter detection accuracy than current state-of-the-art models. IGD also shows\nbetter robustness in problems with small or contaminated training sets. Code is\navailable at https://github.com/tianyu0207/IGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08573","description":"<p>The use of local detectors and descriptors in typical computer vision\npipelines work well until variations in viewpoint and appearance change become\nextreme. Past research in this area has typically focused on one of two\napproaches to this challenge: the use of projections into spaces more suitable\nfor feature matching under extreme viewpoint changes, and attempting to learn\nfeatures that are inherently more robust to viewpoint change. In this paper, we\npresent a novel framework that combines learning of invariant descriptors\nthrough data augmentation and orthographic viewpoint projection. We propose\nrotation-robust local descriptors, learnt through training data augmentation\nbased on rotation homographies, and a correspondence ensemble technique that\ncombines vanilla feature correspondences with those obtained through\nrotation-robust features. Using a range of benchmark datasets as well as\ncontributing a new bespoke dataset for this research domain, we evaluate the\neffectiveness of the proposed approach on key tasks including pose estimation\nand visual place recognition. Our system outperforms a range of baseline and\nstate-of-the-art techniques, including enabling higher levels of place\nrecognition precision across opposing place viewpoints and achieves\npractically-useful performance levels even under extreme viewpoint changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parihar_U/0/1/0/all/0/1\">Udit Singh Parihar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujarathi_A/0/1/0/all/0/1\">Aniket Gujarathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1\">Kinal Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tourani_S/0/1/0/all/0/1\">Satyajit Tourani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K. Madhava Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection. (arXiv:2103.09136v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09136","description":"<p>While general object detection with deep learning has achieved great success\nin the past few years, the performance and efficiency of detecting small\nobjects are far from satisfactory. The most common and effective way to promote\nsmall object detection is to use high-resolution images or feature maps.\nHowever, both approaches induce costly computation since the computational cost\ngrows squarely as the size of images and features increases. To get the best of\ntwo worlds, we propose QueryDet that uses a novel query mechanism to accelerate\nthe inference speed of feature-pyramid based object detectors. The pipeline\ncomposes two steps: it first predicts the coarse locations of small objects on\nlow-resolution features and then computes the accurate detection results using\nhigh-resolution features sparsely guided by those coarse positions. In this\nway, we can not only harvest the benefit of high-resolution feature maps but\nalso avoid useless computation for the background area. On the popular COCO\ndataset, the proposed method improves the detection mAP by 1.0 and mAP-small by\n2.0, and the high-resolution inference speed is improved to 3.0x on average. On\nVisDrone dataset, which contains more small objects, we create a new\nstate-of-the-art while gaining a 2.3x high-resolution acceleration on average.\nCode is available at https://github.com/ChenhongyiYang/QueryDet-PyTorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhongyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zehao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to Image Generation with Semantic-Spatial Aware GAN. (arXiv:2104.00567v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00567","description":"<p>Text-to-image synthesis (T2I) aims to generate photo-realistic images which\nare semantically consistent with the text descriptions. Existing methods are\nusually built upon conditional generative adversarial networks (GANs) and\ninitialize an image from noise with sentence embedding, and then refine the\nfeatures with fine-grained word embedding iteratively. A close inspection of\ntheir generated images reveals a major limitation: even though the generated\nimage holistically matches the description, individual image regions or parts\nof somethings are often not recognizable or consistent with words in the\nsentence, e.g. \"a white crown\". To address this problem, we propose a novel\nframework Semantic-Spatial Aware GAN for synthesizing images from input text.\nConcretely, we introduce a simple and effective Semantic-Spatial Aware block,\nwhich (1) learns semantic-adaptive transformation conditioned on text to\neffectively fuse text features and image features, and (2) learns a semantic\nmask in a weakly-supervised way that depends on the current text-image fusion\nprocess in order to guide the transformation spatially. Experiments on the\nchallenging COCO and CUB bird datasets demonstrate the advantage of our method\nover the recent state-of-the-art approaches, regarding both visual fidelity and\nalignment with input text description.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kai Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning. (arXiv:2104.08718v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08718","description":"<p>Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n</p>\n<p>In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oriented RepPoints for Aerial Object Detection. (arXiv:2105.11111v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11111","description":"<p>In contrast to the generic object, aerial targets are often non-axis aligned\nwith arbitrary orientations having the cluttered surroundings. Unlike the\nmainstreamed approaches regressing the bounding box orientations, this paper\nproposes an effective adaptive points learning approach to aerial object\ndetection by taking advantage of the adaptive points representation, which is\nable to capture the geometric information of the arbitrary-oriented instances.\nTo this end, three oriented conversion functions are presented to facilitate\nthe classification and localization with accurate orientation. Moreover, we\npropose an effective quality assessment and sample assignment scheme for\nadaptive points learning toward choosing the representative oriented reppoints\nsamples during training, which is able to capture the non-axis aligned features\nfrom adjacent objects or background noises. A spatial constraint is introduced\nto penalize the outlier points for roust adaptive learning. Experimental\nresults on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD\nand DIOR-R, demonstrate the efficacy of our proposed approach. The source code\nis availabel at: https://github.com/LiWentomng/OrientedRepPoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wentong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Kaixuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolarStream: Streaming Lidar Object Detection and Segmentation with Polar Pillars. (arXiv:2106.07545v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07545","description":"<p>Recent works recognized lidars as an inherently streaming data source and\nshowed that the end-to-end latency of lidar perception models can be reduced\nsignificantly by operating on wedge-shaped point cloud sectors rather then the\nfull point cloud. However, due to use of cartesian coordinate systems these\nmethods represent the sectors as rectangular regions, wasting memory and\ncompute. In this work we propose using a polar coordinate system and make two\nkey improvements on this design. First, we increase the spatial context by\nusing multi-scale padding from neighboring sectors: preceding sector from the\ncurrent scan and/or the following sector from the past scan. Second, we improve\nthe core polar convolutional architecture by introducing feature undistortion\nand range stratified convolutions. Experimental results on the nuScenes dataset\nshow significant improvements over other streaming based methods. We also\nachieve comparable results to existing non-streaming methods but with lower\nlatencies. The code and pretrained models are available at\n\\url{https://github.com/motional/polarstream}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vora_S/0/1/0/all/0/1\">Sourabh Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepMesh: Differentiable Iso-Surface Extraction. (arXiv:2106.11795v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11795","description":"<p>Geometric Deep Learning has recently made striking progress with the advent\nof continuous deep implicit fields. They allow for detailed modeling of\nwatertight surfaces of arbitrary topology while not relying on a 3D Euclidean\ngrid, resulting in a learnable parameterization that is unlimited in\nresolution.\n</p>\n<p>Unfortunately, these methods are often unsuitable for applications that\nrequire an explicit mesh-based surface representation because converting an\nimplicit field to such a representation relies on the Marching Cubes algorithm,\nwhich cannot be differentiated with respect to the underlying implicit field.\n</p>\n<p>In this work, we remove this limitation and introduce a differentiable way to\nproduce explicit surface mesh representations from Deep Implicit Fields. Our\nkey insight is that by reasoning on how implicit field perturbations impact\nlocal surface geometry, one can ultimately differentiate the 3D location of\nsurface samples with respect to the underlying deep implicit field. We exploit\nthis to define DeepMesh - an end-to-end differentiable mesh representation that\ncan vary its topology.\n</p>\n<p>We validate our theoretical insight through several applications: Single view\n3D Reconstruction via Differentiable Rendering, Physically-Driven Shape\nOptimization, Full Scene 3D Reconstruction from Scans and End-to-End Training.\nIn all cases our end-to-end differentiable parameterization gives us an edge\nover state-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillard_B/0/1/0/all/0/1\">Benoit Guillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remelli_E/0/1/0/all/0/1\">Edoardo Remelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukoianov_A/0/1/0/all/0/1\">Artem Lukoianov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1\">Stephan R. Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baque_P/0/1/0/all/0/1\">Pierre Baque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer. (arXiv:2108.00358v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00358","description":"<p>Microorganisms are widely distributed in the human daily living environment.\nThey play an essential role in environmental pollution control, disease\nprevention and treatment, and food and drug production. The analysis of\nmicroorganisms is essential for making full use of different microorganisms.\nThe conventional analysis methods are laborious and time-consuming. Therefore,\nthe automatic image analysis based on artificial neural networks is introduced\nto optimize it. However, the automatic microorganism image analysis faces many\nchallenges, such as the requirement of a robust algorithm caused by various\napplication occasions, insignificant features and easy under-segmentation\ncaused by the image characteristic, and various analysis tasks. Therefore, we\nconduct this review to comprehensively discuss the characteristics of\nmicroorganism image analysis based on artificial neural networks. In this\nreview, the background and motivation are introduced first. Then, the\ndevelopment of artificial neural networks and representative networks are\npresented. After that, the papers related to microorganism image analysis based\non classical and deep neural networks are reviewed from the perspectives of\ndifferent tasks. In the end, the methodology analysis and potential direction\nare discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yimin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpaired Deep Image Deraining Using Dual Contrastive Learning. (arXiv:2109.02973v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02973","description":"<p>Learning single image deraining (SID) networks from an unpaired set of clean\nand rainy images is practical and valuable as acquiring paired real-world data\nis almost infeasible. However, without the paired data as the supervision,\nlearning a SID network is challenging. Moreover, simply using existing unpaired\nlearning methods (e.g., unpaired adversarial learning and cycle-consistency\nconstraints) in the SID task is insufficient to learn the underlying\nrelationship from rainy inputs to clean outputs as there exists significant\ndomain gap between the rainy and clean images. In this paper, we develop an\neffective unpaired SID adversarial framework which explores mutual properties\nof the unpaired exemplars by a dual contrastive learning manner in a deep\nfeature space, named as DCD-GAN. The proposed method mainly consists of two\ncooperative branches: Bidirectional Translation Branch (BTB) and Contrastive\nGuidance Branch (CGB). Specifically, BTB exploits full advantage of the\ncirculatory architecture of adversarial consistency to generate abundant\nexemplar pairs and excavates latent feature distributions between two domains\nby equipping it with bidirectional mapping. Simultaneously, CGB implicitly\nconstrains the embeddings of different exemplars in the deep feature space by\nencouraging the similar feature distributions closer while pushing the\ndissimilar further away, in order to better facilitate rain removal and help\nimage restoration. Extensive experiments demonstrate that our method performs\nfavorably against existing unpaired deraining approaches on both synthetic and\nreal-world datasets, and generates comparable results against several\nfully-supervised or semi-supervised models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Caihua Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Longgang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhentao Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions. (arXiv:2109.04386v4 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2109.04386","description":"<p>An activation function is a crucial component of a neural network that\nintroduces non-linearity in the network. The state-of-the-art performance of a\nneural network depends also on the perfect choice of an activation function. We\npropose two novel non-monotonic smooth trainable activation functions, called\nErfAct and Pserf. Experiments suggest that the proposed functions improve the\nnetwork performance significantly compared to the widely used activations like\nReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and\n5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in\nCIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet\nV2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean\naverage precision (mAP) on SSD300 model in Pascal VOC dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution Image Harmonization via Collaborative Dual Transformations. (arXiv:2109.06671v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06671","description":"<p>Given a composite image, image harmonization aims to adjust the foreground to\nmake it compatible with the background. High-resolution image harmonization is\nin high demand, but still remains unexplored. Conventional image harmonization\nmethods learn global RGB-to-RGB transformation which could effortlessly scale\nto high resolution, but ignore diverse local context. Recent deep learning\nmethods learn the dense pixel-to-pixel transformation which could generate\nharmonious outputs, but are highly constrained in low resolution. In this work,\nwe propose a high-resolution image harmonization network with Collaborative\nDual Transformation (CDTNet) to combine pixel-to-pixel transformation and\nRGB-to-RGB transformation coherently in an end-to-end network. Our CDTNet\nconsists of a low-resolution generator for pixel-to-pixel transformation, a\ncolor mapping module for RGB-to-RGB transformation, and a refinement module to\ntake advantage of both. Extensive experiments on high-resolution benchmark\ndataset and our created high-resolution real composite images demonstrate that\nour CDTNet strikes a good balance between efficiency and effectiveness. Our\nused datasets can be found in\nhttps://github.com/bcmi/CDTNet-High-Resolution-Image-Harmonization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1\">Wenyan Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xinhao Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xuesong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qihao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Representation Learning for Reliable Robotic Monitoring of Fruit Anomalies. (arXiv:2109.10135v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.10135","description":"<p>Data augmentation can be a simple yet powerful tool for autonomous robots to\nfully utilise available data for selfsupervised identification of atypical\nscenes or objects. State-of-the-art augmentation methods arbitrarily embed\n\"structural\" peculiarity on typical images so that classifying these artefacts\ncan provide guidance for learning representations for the detection of\nanomalous visual signals. In this paper, however, we argue that learning such\nstructure-sensitive representations can be a suboptimal approach to some\nclasses of anomaly (e.g., unhealthy fruits) which could be better recognised by\na different type of visual element such as \"colour\". We thus propose Channel\nRandomisation as a novel data augmentation method for restricting neural\nnetworks to learn encoding of \"colour irregularity\" whilst predicting\nchannel-randomised images to ultimately build reliable fruit-monitoring robots\nidentifying atypical fruit qualities. Our experiments show that (1) this\ncolour-based alternative can better learn representations for consistently\naccurate identification of fruit anomalies in various fruit species, and also,\n(2) unlike other methods, the validation accuracy can be utilised as a\ncriterion for early stopping of training in practice due to positive\ncorrelation between the performance in the self-supervised\ncolour-differentiation task and the subsequent detection rate of actual\nanomalous fruits. Also, the proposed approach is evaluated on a new\nagricultural dataset, Riseholme-2021, consisting of 3.5K strawberry images\ngathered by a mobile robot, which we share online to encourage active\nagri-robotics research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1\">Taeyeong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Would_O/0/1/0/all/0/1\">Owen Would</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salazar_Gomez_A/0/1/0/all/0/1\">Adrian Salazar-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cielniak_G/0/1/0/all/0/1\">Grzegorz Cielniak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithm Fairness in AI for Medicine and Healthcare. (arXiv:2110.00603v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00603","description":"<p>In the current development and deployment of many artificial intelligence\n(AI) systems in healthcare, algorithm fairness is a challenging problem in\ndelivering equitable care. Recent evaluation of AI models stratified across\nrace sub-populations have revealed inequalities in how patients are diagnosed,\ngiven treatments, and billed for healthcare costs. In this perspective article,\nwe summarize the intersectional field of fairness in machine learning through\nthe context of current issues in healthcare, outline how algorithmic biases\n(e.g. - image acquisition, genetic variation, intra-observer labeling\nvariability) arise in current clinical workflows and their resulting healthcare\ndisparities. Lastly, we also review emerging technology for mitigating bias via\nfederated learning, disentanglement, and model explainability, and their role\nin AI-SaMD development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Richard J. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tiffany Y. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipkova_J/0/1/0/all/0/1\">Jana Lipkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Judy J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F.K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Y. Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahai_S/0/1/0/all/0/1\">Sharifa Sahai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1\">Faisal Mahmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchFormer: An Efficient Point Transformer with Patch Attention. (arXiv:2111.00207v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00207","description":"<p>The point cloud learning community witnesses a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have achieved top accuracy\non the major learning benchmarks. However, existing point Transformers are\ncomputationally expensive since they need to generate a large attention map,\nwhich has quadratic complexity (both in space and time) with respect to input\nsize. To solve this shortcoming, we introduce Patch ATtention (PAT) to\nadaptively learn a much smaller set of bases upon which the attention maps are\ncomputed. By a weighted summation upon these bases, PAT not only captures the\nglobal shape context but also achieves linear complexity to input size. In\naddition, we propose a lightweight Multi-Scale aTtention (MST) block to build\nattentions among features of different scales, providing the model with\nmulti-scale features. Equipped with the PAT and MST, we construct our neural\narchitecture called PatchFormer that integrates both modules into a joint\nframework for point cloud learning. Extensive experiments demonstrate that our\nnetwork achieves comparable accuracy on general point cloud learning tasks with\n9.2x speed-up than previous point Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Haocheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xinyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Document Generator for Annotation-free Layout Recognition. (arXiv:2111.06016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06016","description":"<p>Analyzing the layout of a document to identify headers, sections, tables,\nfigures etc. is critical to understanding its content. Deep learning based\napproaches for detecting the layout structure of document images have been\npromising. However, these methods require a large number of annotated examples\nduring training, which are both expensive and time consuming to obtain. We\ndescribe here a synthetic document generator that automatically produces\nrealistic documents with labels for spatial positions, extents and categories\nof the layout elements. The proposed generative process treats every physical\ncomponent of a document as a random variable and models their intrinsic\ndependencies using a Bayesian Network graph. Our hierarchical formulation using\nstochastic templates allow parameter sharing between documents for retaining\nbroad themes and yet the distributional characteristics produces visually\nunique samples, thereby capturing complex and diverse layouts. We empirically\nillustrate that a deep layout detection model trained purely on the synthetic\ndocuments can match the performance of a model that uses real documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raman_N/0/1/0/all/0/1\">Natraj Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1\">Manuela Veloso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech. (arXiv:2111.10139v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10139","description":"<p>In this paper we present VDTTS, a Visually-Driven Text-to-Speech model.\nMotivated by dubbing, VDTTS takes advantage of video frames as an additional\ninput alongside text, and generates speech that matches the video signal. We\ndemonstrate how this allows VDTTS to, unlike plain TTS models, generate speech\nthat not only has prosodic variations like natural pauses and pitch, but is\nalso synchronized to the input video. Experimentally, we show our model\nproduces well-synchronized outputs, approaching the video-speech\nsynchronization quality of the ground-truth, on several challenging benchmarks\nincluding \"in-the-wild\" content from VoxCeleb2. Supplementary demo videos\ndemonstrating video-speech synchronization, robustness to speaker ID swapping,\nand prosody, presented at the project page.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassid_M/0/1/0/all/0/1\">Michael Hassid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shillingford_B/0/1/0/all/0/1\">Brendan Shillingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Miaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imperceptible Transfer Attack and Defense on 3D Point Cloud Classification. (arXiv:2111.10990v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10990","description":"<p>Although many efforts have been made into attack and defense on the 2D image\ndomain in recent years, few methods explore the vulnerability of 3D models.\nExisting 3D attackers generally perform point-wise perturbation over point\nclouds, resulting in deformed structures or outliers, which is easily\nperceivable by humans. Moreover, their adversarial examples are generated under\nthe white-box setting, which frequently suffers from low success rates when\ntransferred to attack remote black-box models. In this paper, we study 3D point\ncloud attacks from two new and challenging perspectives by proposing a novel\nImperceptible Transfer Attack (ITA): 1) Imperceptibility: we constrain the\nperturbation direction of each point along its normal vector of the\nneighborhood surface, leading to generated examples with similar geometric\nproperties and thus enhancing the imperceptibility. 2) Transferability: we\ndevelop an adversarial transformation model to generate the most harmful\ndistortions and enforce the adversarial examples to resist it, improving their\ntransferability to unknown black-box models. Further, we propose to train more\nrobust black-box 3D models to defend against such ITA attacks by learning more\ndiscriminative point cloud representations. Extensive evaluations demonstrate\nthat our ITA attack is more imperceptible and transferable than\nstate-of-the-arts and validate the superiority of our defense strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel-wise Energy-biased Abstention Learning for Anomaly Segmentation on Complex Urban Driving Scenes. (arXiv:2111.12264v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12264","description":"<p>State-of-the-art (SOTA) anomaly segmentation approaches on complex urban\ndriving scenes explore pixel-wise classification uncertainty learned from\noutlier exposure, or external reconstruction models. However, previous\nuncertainty approaches that directly associate high uncertainty to anomaly may\nsometimes lead to incorrect anomaly predictions, and external reconstruction\nmodels tend to be too inefficient for real-time self-driving embedded systems.\nIn this paper, we propose a new anomaly segmentation method, named pixel-wise\nenergy-biased abstention learning (PEBAL), that explores pixel-wise abstention\nlearning (AL) with a model that learns an adaptive pixel-level anomaly class,\nand an energy-based model (EBM) that learns inlier pixel distribution. More\nspecifically, PEBAL is based on a non-trivial joint training of EBM and AL,\nwhere EBM is trained to output high-energy for anomaly pixels (from outlier\nexposure) and AL is trained such that these high-energy pixels receive adaptive\nlow penalty for being included to the anomaly class. We extensively evaluate\nPEBAL against the SOTA and show that it achieves the best performance across\nfour benchmarks. Code is available at https://github.com/tianyu0207/PEBAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces. (arXiv:2111.12448v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12448","description":"<p>Learning a disentangled, interpretable, and structured latent representation\nin 3D generative models of faces and bodies is still an open problem. The\nproblem is particularly acute when control over identity features is required.\nIn this paper, we propose an intuitive yet effective self-supervised approach\nto train a 3D shape variational autoencoder (VAE) which encourages a\ndisentangled latent representation of identity features. Curating the\nmini-batch generation by swapping arbitrary features across different shapes\nallows to define a loss function leveraging known differences and similarities\nin the latent representations. Experimental results conducted on 3D meshes show\nthat state-of-the-art methods for latent disentanglement are not able to\ndisentangle identity features of faces and bodies. Our proposed method properly\ndecouples the generation of such features while maintaining good representation\nand reconstruction capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foti_S/0/1/0/all/0/1\">Simone Foti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koo_B/0/1/0/all/0/1\">Bongjin Koo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarkson_M/0/1/0/all/0/1\">Matthew J. Clarkson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation. (arXiv:2111.12903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12903","description":"<p>Consistency learning using input image, feature, or network perturbations has\nshown remarkable results in semi-supervised semantic segmentation, but this\napproach can be seriously affected by inaccurate predictions of unlabelled\ntraining images. There are two consequences of these inaccurate predictions: 1)\nthe training based on the \"strict\" cross-entropy (CE) loss can easily overfit\nprediction mistakes, leading to confirmation bias; and 2) the perturbations\napplied to these inaccurate predictions will use potentially erroneous\npredictions as training signals, degrading consistency learning. In this paper,\nwe address the prediction accuracy problem of consistency learning methods with\nnovel extensions of the mean-teacher (MT) model, which include a new auxiliary\nteacher, and the replacement of MT's mean square error (MSE) by a stricter\nconfidence-weighted cross-entropy (Conf-CE) loss. The accurate prediction by\nthis model allows us to use a challenging combination of network, input data\nand feature perturbations to improve the consistency learning generalisation,\nwhere the feature perturbations consist of a new adversarial perturbation.\nResults on public benchmarks show that our approach achieves remarkable\nimprovements over the previous SOTA methods in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predict, Prevent, and Evaluate: Disentangled Text-Driven Image Manipulation Empowered by Pre-Trained Vision-Language Model. (arXiv:2111.13333v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13333","description":"<p>To achieve disentangled image manipulation, previous works depend heavily on\nmanual annotation. Meanwhile, the available manipulations are limited to a\npre-defined set the models were trained for. We propose a novel framework,\ni.e., Predict, Prevent, and Evaluate (PPE), for disentangled text-driven image\nmanipulation that requires little manual annotation while being applicable to a\nwide variety of manipulations. Our method approaches the targets by deeply\nexploiting the power of the large-scale pre-trained vision-language model CLIP.\nConcretely, we firstly Predict the possibly entangled attributes for a given\ntext command. Then, based on the predicted attributes, we introduce an\nentanglement loss to Prevent entanglements during training. Finally, we propose\na new evaluation metric to Evaluate the disentangled image manipulation. We\nverify the effectiveness of our method on the challenging face editing task.\nExtensive experiments show that the proposed PPE framework achieves much better\nquantitative and qualitative results than the up-to-date StyleCLIP baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAFITE: Towards Language-Free Training for Text-to-Image Generation. (arXiv:2111.13792v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13792","description":"<p>One of the major challenges in training text-to-image generation models is\nthe need of a large number of high-quality image-text pairs. While image\nsamples are often easily accessible, the associated text descriptions typically\nrequire careful human captioning, which is particularly time- and\ncost-consuming. In this paper, we propose the first work to train text-to-image\ngeneration models without any text data. Our method leverages the well-aligned\nmulti-modal semantic space of the powerful pre-trained CLIP model: the\nrequirement of text-conditioning is seamlessly alleviated via generating text\nfeatures from image features. Extensive experiments are conducted to illustrate\nthe effectiveness of the proposed method. We obtain state-of-the-art results in\nthe standard text-to-image generation tasks. Importantly, the proposed\nlanguage-free model outperforms most existing models trained with full\nimage-text pairs. Furthermore, our method can be applied in fine-tuning\npre-trained models, which saves both training time and cost in training\ntext-to-image generation models. Our pre-trained model obtains competitive\nresults in zero-shot text-to-image generation on the MS-COCO dataset, yet with\naround only 1% of the model size and training data size relative to the\nrecently proposed large DALL-E model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yufan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tensmeyer_C/0/1/0/all/0/1\">Chris Tensmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinhui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection. (arXiv:2112.00322v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00322","description":"<p>Recently, promising applications in robotics and augmented reality have\nattracted considerable attention to 3D object detection from point clouds. In\nthis paper, we present FCAF3D - a first-in-class fully convolutional\nanchor-free indoor 3D object detection method. It is a simple yet effective\nmethod that uses a voxel representation of a point cloud and processes voxels\nwith sparse convolutions. FCAF3D can handle large-scale scenes with minimal\nruntime through a single fully convolutional feed-forward pass. Existing 3D\nobject detection methods make prior assumptions on the geometry of objects, and\nwe argue that it limits their generalization ability. To get rid of any prior\nassumptions, we propose a novel parametrization of oriented bounding boxes that\nallows obtaining better results in a purely data-driven way. The proposed\nmethod achieves state-of-the-art 3D object detection results in terms of\nmAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The\ncode and models are available at https://github.com/samsunglabs/fcaf3d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rukhovich_D/0/1/0/all/0/1\">Danila Rukhovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorontsova_A/0/1/0/all/0/1\">Anna Vorontsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konushin_A/0/1/0/all/0/1\">Anton Konushin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encouraging Disentangled and Convex Representation with Controllable Interpolation Regularization. (arXiv:2112.03163v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03163","description":"<p>We focus on controllable disentangled representation learning (C-Dis-RL),\nwhere users can control the partition of the disentangled latent space to\nfactorize dataset attributes (concepts) for downstream tasks. Two general\nproblems remain under-explored in current methods: (1) They lack comprehensive\ndisentanglement constraints, especially missing the minimization of mutual\ninformation between different attributes across latent and observation domains.\n(2) They lack convexity constraints, which is important for meaningfully\nmanipulating specific attributes for downstream tasks. To encourage both\ncomprehensive C-Dis-RL and convexity simultaneously, we propose a simple yet\nefficient method: Controllable Interpolation Regularization (CIR), which\ncreates a positive loop where disentanglement and convexity can help each\nother. Specifically, we conduct controlled interpolation in latent space during\ntraining, and we reuse the encoder to help form a 'perfect disentanglement'\nregularization. In that case, (a) disentanglement loss implicitly enlarges the\npotential understandable distribution to encourage convexity; (b) convexity can\nin turn improve robust and precise disentanglement. CIR is a general module and\nwe merge CIR with three different algorithms: ELEGANT, I2I-Dis, and GZS-Net to\nshow the compatibility and effectiveness. Qualitative and quantitative\nexperiments show improvement in C-Dis-RL and latent convexity by CIR. This\nfurther improves downstream tasks: controllable image synthesis, cross-modality\nimage translation, and zero-shot synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yunhao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_G/0/1/0/all/0/1\">Gan Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yunkui Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1\">Laurent Itti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BT-Unet: A self-supervised learning framework for biomedical image segmentation using Barlow Twins with U-Net models. (arXiv:2112.03916v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.03916","description":"<p>Deep learning has brought the most profound contribution towards biomedical\nimage segmentation to automate the process of delineation in medical imaging.\nTo accomplish such task, the models are required to be trained using huge\namount of annotated or labelled data that highlights the region of interest\nwith a binary mask. However, efficient generation of the annotations for such\nhuge data requires expert biomedical analysts and extensive manual effort. It\nis a tedious and expensive task, while also being vulnerable to human error. To\naddress this problem, a self-supervised learning framework, BT-Unet is proposed\nthat uses the Barlow Twins approach to pre-train the encoder of a U-Net model\nvia redundancy reduction in an unsupervised manner to learn data\nrepresentation. Later, complete network is fine-tuned to perform actual\nsegmentation. The BT-Unet framework can be trained with a limited number of\nannotated samples while having high number of unannotated samples, which is\nmostly the case in real-world problems. This framework is validated over\nmultiple U-Net models over diverse datasets by generating scenarios of a\nlimited number of labelled samples using standard evaluation metrics. With\nexhaustive experiment trials, it is observed that the BT-Unet framework\nenhances the performance of the U-Net models with significant margin under such\ncircumstances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperdimensional Feature Fusion for Interpretable Out-Of-Distribution Detection. (arXiv:2112.05341v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05341","description":"<p>We introduce powerful ideas from Hyperdimensional Computing into the\nchallenging field of Out-of-Distribution (OOD) detection. In contrast to most\nexisting work that performs OOD detection based on only a single layer of a\nneural network, we use similarity-preserving semi-orthogonal projection\nmatrices to project the feature maps from multiple layers into a common vector\nspace. By repeatedly applying the bundling operation $\\oplus$, we create\nexpressive class-specific descriptor vectors for all in-distribution classes.\nAt test time, a simple and efficient cosine similarity calculation between\ndescriptor vectors consistently identifies OOD samples with better performance\nthan the current state-of-the-art. We show that the hyperdimensional fusion of\nmultiple network layers is critical to achieve best general performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Samuel Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1\">Niko S&#xfc;nderhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1\">Feras Dayoub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-based Generative Face Anonymisation with Pose Preservation. (arXiv:2112.05496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05496","description":"<p>We propose AnonyGAN, a GAN-based solution for face anonymisation which\nreplaces the visual information corresponding to a source identity with a\ncondition identity provided as any single image. With the goal to maintain the\ngeometric attributes of the source face, i.e., the facial pose and expression,\nand to promote more natural face generation, we propose to exploit a Bipartite\nGraph to explicitly model the relations between the facial landmarks of the\nsource identity and the ones of the condition identity through a deep model. We\nfurther propose a landmark attention model to relax the manual selection of\nfacial landmarks, allowing the network to weight the landmarks for the best\nvisual naturalness and pose preservation. Finally, to facilitate the appearance\nlearning, we propose a hybrid training strategy to address the challenge caused\nby the lack of direct pixel-level supervision. We evaluate our method and its\nvariants on two public datasets, CelebA and LFW, in terms of visual\nnaturalness, facial pose preservation and of its impacts on face detection and\nre-identification. We prove that AnonyGAN significantly outperforms the\nstate-of-the-art methods in terms of visual naturalness, face detection and\npose preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DallAsen_N/0/1/0/all/0/1\">Nicola Dall&#x27;Asen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanella_L/0/1/0/all/0/1\">Luca Zanella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"gACSON software for automated segmentation and morphology analyses of myelinated axons in 3D electron microscopy. (arXiv:2112.06476v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.06476","description":"<p>Background and Objective: Advances in electron microscopy (EM) now allow\nthree-dimensional (3D) imaging of hundreds of micrometers of tissue with\nnanometer-scale resolution, providing new opportunities to study the\nultrastructure of the brain. In this work, we introduce a freely available\nMatlab-based gACSON software for visualization, segmentation, assessment, and\nmorphology analysis of myelinated axons in 3D-EM volumes of brain tissue\nsamples. Methods: The software is equipped with a graphical user interface\n(GUI). It automatically segments the intra-axonal space of myelinated axons and\ntheir corresponding myelin sheaths and allows manual segmentation,\nproofreading, and interactive correction of the segmented components. gACSON\nanalyzes the morphology of myelinated axons, such as axonal diameter, axonal\neccentricity, myelin thickness, or g-ratio. Results: We illustrate the use of\nthe software by segmenting and analyzing myelinated axons in six 3D-EM volumes\nof rat somatosensory cortex after sham surgery or traumatic brain injury (TBI).\nOur results suggest that the equivalent diameter of myelinated axons in\nsomatosensory cortex was decreased in TBI animals five months after the injury.\nConclusions: Our results indicate that gACSON is a valuable tool for\nvisualization, segmentation, assessment, and morphology analysis of myelinated\naxons in 3D-EM volumes. It is freely available at\nhttps://github.com/AndreaBehan/g-ACSON under the MIT license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Behanova_A/0/1/0/all/0/1\">Andrea Behanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdollahzadeh_A/0/1/0/all/0/1\">Ali Abdollahzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belevich_I/0/1/0/all/0/1\">Ilya Belevich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jokitalo_E/0/1/0/all/0/1\">Eija Jokitalo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sierra_A/0/1/0/all/0/1\">Alejandra Sierra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1\">Jussi Tohka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks. (arXiv:2112.06825v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06825","description":"<p>Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&amp;L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&amp;L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&amp;L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&amp;L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_Y/0/1/0/all/0/1\">Yi-Lin Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Jaemin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09081","description":"<p>We present a visual localization system that learns to estimate camera poses\nin the real world with the help of synthetic data. Despite significant progress\nin recent years, most learning-based approaches to visual localization target\nat a single domain and require a dense database of geo-tagged images to\nfunction well. To mitigate the data scarcity issue and improve the scalability\nof the neural localization models, we introduce TOPO-DataGen, a versatile\nsynthetic data generation tool that traverses smoothly between the real and\nvirtual world, hinged on the geographic camera viewpoint. New large-scale\nsim-to-real benchmark datasets are proposed to showcase and evaluate the\nutility of the said synthetic data. Our experiments reveal that synthetic data\ngenerically enhances the neural network performance on real data. Furthermore,\nwe introduce CrossLoc, a cross-modal visual representation learning approach to\npose estimation that makes full use of the scene coordinate ground truth via\nself-supervision. Without any extra data, CrossLoc significantly outperforms\nthe state-of-the-art methods and achieves substantially higher real-data sample\nefficiency. Our code and datasets are all available at\nhttps://crossloc.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reding_S/0/1/0/all/0/1\">Simon Reding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shanci Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doytchinov_I/0/1/0/all/0/1\">Iordan Doytchinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Microfossil Identification via Deep Metric Learning. (arXiv:2112.09490v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09490","description":"<p>We apply deep metric learning for the first time to the problem of\nclassifying planktic foraminifer shells on microscopic images. This species\nrecognition task is an important information source and scientific pillar for\nreconstructing past climates. All foraminifer CNN recognition pipelines in the\nliterature produce black-box classifiers that lack visualization options for\nhuman experts and cannot be applied to open-set problems. Here, we benchmark\nmetric learning against these pipelines, produce the first scientific\nvisualization of the phenotypic planktic foraminifer morphology space, and\ndemonstrate that metric learning can be used to cluster species unseen during\ntraining. We show that metric learning outperforms all published CNN-based\nstate-of-the-art benchmarks in this domain. We evaluate our approach on the\n34,640 expert-annotated images of the Endless Forams public library of 35\nmodern planktic foraminifera species. Our results on this data show leading 92%\naccuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,\nand 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered\nin training. We conclude that metric learning is highly effective for this\ndomain and serves as an important tool towards expert-in-the-loop automation of\nmicrofossil identification. Keycode, network weights, and data splits are\npublished with this paper for full reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karaderi_T/0/1/0/all/0/1\">Tayfun Karaderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiang_A/0/1/0/all/0/1\">Allison Y. Hsiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaer_J/0/1/0/all/0/1\">Jacob Ramaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Daniela N. Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D Medical Image Segmentation using HyperNet. (arXiv:2112.10652v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10652","description":"<p>Semantic segmentation of 3D medical images is a challenging task due to the\nhigh variability of the shape and pattern of objects (such as organs or\ntumors). Given the recent success of deep learning in medical image\nsegmentation, Neural Architecture Search (NAS) has been introduced to find\nhigh-performance 3D segmentation network architectures. However, because of the\nmassive computational requirements of 3D data and the discrete optimization\nnature of architecture search, previous NAS methods require a long search time\nor necessary continuous relaxation, and commonly lead to sub-optimal network\narchitectures. While one-shot NAS can potentially address these disadvantages,\nits application in the segmentation domain has not been well studied in the\nexpansive multi-scale multi-path search space. To enable one-shot NAS for\nmedical image segmentation, our method, named HyperSegNAS, introduces a\nHyperNet to assist super-net training by incorporating architecture topology\ninformation. Such a HyperNet can be removed once the super-net is trained and\nintroduces no overhead during architecture search. We show that HyperSegNAS\nyields better performing and more intuitive architectures compared to the\nprevious state-of-the-art (SOTA) segmentation networks; furthermore, it can\nquickly and accurately find good architecture candidates under different\ncomputing constraints. Our method is evaluated on public datasets from the\nMedical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1\">Cheng Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myronenko_A/0/1/0/all/0/1\">Andriy Myronenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nath_V/0/1/0/all/0/1\">Vish Nath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddiquee_M/0/1/0/all/0/1\">Md Mahfuzur Rahman Siddiquee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yufan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning. (arXiv:2112.10982v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10982","description":"<p>Generalized few-shot semantic segmentation was introduced to move beyond only\nevaluating few-shot segmentation models on novel classes to include testing\ntheir ability to remember base classes. While the current state-of-the-art\napproach is based on meta-learning, it performs poorly and saturates in\nlearning after observing only a few shots. We propose the first fine-tuning\nsolution, and demonstrate that it addresses the saturation problem while\nachieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We\nalso show that it outperforms existing methods, whether fine-tuning multiple\nfinal layers or only the final layer. Finally, we present a triplet loss\nregularization that shows how to redistribute the balance of performance\nbetween novel and base categories so that there is a smaller gap between them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Myers_Dean_J/0/1/0/all/0/1\">Josh Myers-Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yinan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QuadTree Attention for Vision Transformers. (arXiv:2201.02767v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02767","description":"<p>Transformers have been successful in many vision tasks, thanks to their\ncapability of capturing long-range dependency. However, their quadratic\ncomputational complexity poses a major obstacle for applying them to vision\ntasks requiring dense predictions, such as object detection, feature matching,\nstereo, etc. We introduce QuadTree Attention, which reduces the computational\ncomplexity from quadratic to linear. Our quadtree transformer builds token\npyramids and computes attention in a coarse-to-fine manner. At each level, the\ntop K patches with the highest attention scores are selected, such that at the\nnext level, attention is only evaluated within the relevant regions\ncorresponding to these top K patches. We demonstrate that quadtree attention\nachieves state-of-the-art performance in various vision tasks, e.g. with 4.0%\nimprovement in feature matching on ScanNet, about 50% flops reduction in stereo\nmatching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,\n1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on\nsemantic segmentation over previous state-of-the-art transformers. The codes\nare available at https://github.com/Tangshitao/QuadtreeAttention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shitao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Siyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2201.02831v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02831","description":"<p>Domain Adaptation (DA) has recently raised strong interests in the medical\nimaging community. While a large variety of DA techniques has been proposed for\nimage segmentation, most of these techniques have been validated either on\nprivate datasets or on small publicly available datasets. Moreover, these\ndatasets mostly addressed single-class problems. To tackle these limitations,\nthe Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in\nconjunction with the 24th International Conference on Medical Image Computing\nand Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large\nand multi-class benchmark for unsupervised cross-modality DA. The challenge's\ngoal is to segment two key brain structures involved in the follow-up and\ntreatment planning of vestibular schwannoma (VS): the VS and the cochleas.\nCurrently, the diagnosis and surveillance in patients with VS are performed\nusing contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in\nusing non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore,\nwe created an unsupervised cross-modality segmentation benchmark. The training\nset provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105).\nThe aim was to automatically perform unilateral VS and bilateral cochlea\nsegmentation on hrT2 as provided in the testing set (N=137). A total of 16\nteams submitted their algorithm for the evaluation phase. The level of\nperformance reached by the top-performing teams is strikingly high (best median\nDice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice -\nVS:92.5%; Cochleas:87.7%). All top-performing methods made use of an\nimage-to-image translation approach to transform the source-domain images into\npseudo-target-domain images. A segmentation network was then trained using\nthese generated images and the manual annotations provided for the source\nimage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dorent_R/0/1/0/all/0/1\">Reuben Dorent</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kujawa_A/0/1/0/all/0/1\">Aaron Kujawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ivory_M/0/1/0/all/0/1\">Marina Ivory</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rieke_N/0/1/0/all/0/1\">Nicola Rieke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joutard_S/0/1/0/all/0/1\">Samuel Joutard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardoso_J/0/1/0/all/0/1\">Jorge Cardoso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belkov_A/0/1/0/all/0/1\">Arseniy Belkov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Calisto_M/0/1/0/all/0/1\">Maria Baldeon Calisto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_J/0/1/0/all/0/1\">Jae Won Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dawant_B/0/1/0/all/0/1\">Benoit M. Dawant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_H/0/1/0/all/0/1\">Hexin Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heinrich_M/0/1/0/all/0/1\">Mattias P. Heinrich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joshi_S/0/1/0/all/0/1\">Smriti Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kashtanova_V/0/1/0/all/0/1\">Victoriya Kashtanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hyeon Gyu Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kondo_S/0/1/0/all/0/1\">Satoshi Kondo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kruse_C/0/1/0/all/0/1\">Christian N. Kruse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_Yuen_S/0/1/0/all/0/1\">Susana K. Lai-Yuen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ly_B/0/1/0/all/0/1\">Buntheng Ly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shin_H/0/1/0/all/0/1\">Hyungseob Shin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_Z/0/1/0/all/0/1\">Zixian Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guotai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jianghao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shapey_J/0/1/0/all/0/1\">Jonathan Shapey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut. (arXiv:2202.11539v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11539","description":"<p>Transformers trained with self-supervised learning using self-distillation\nloss (DINO) have been shown to produce attention maps that highlight salient\nforeground objects. In this paper, we demonstrate a graph-based approach that\nuses the self-supervised transformer features to discover an object from an\nimage. Visual tokens are viewed as nodes in a weighted graph with edges\nrepresenting a connectivity score based on the similarity of tokens. Foreground\nobjects can then be segmented using a normalized graph-cut to group\nself-similar regions. We solve the graph-cut problem using spectral clustering\nwith generalized eigen-decomposition and show that the second smallest\neigenvector provides a cutting solution since its absolute value indicates the\nlikelihood that a token belongs to a foreground object. Despite its simplicity,\nthis approach significantly boosts the performance of unsupervised object\ndiscovery: we improve over the recent state of the art LOST by a margin of\n6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The\nperformance can be further improved by adding a second stage class-agnostic\ndetector (CAD). Our proposed method can be easily extended to unsupervised\nsaliency detection and weakly supervised object detection. For unsupervised\nsaliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,\nDUT-OMRON respectively compared to previous state of the art. For weakly\nsupervised object detection, we achieve competitive performance on CUB and\nImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yangtao Wang</a> (M-PSI), <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xi Shen</a> (LIGM), <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shell Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a> (MIT CSAIL), <a href=\"http://arxiv.org/find/cs/1/au:+Crowley_J/0/1/0/all/0/1\">James Crowley</a> (M-PSI), <a href=\"http://arxiv.org/find/cs/1/au:+Vaufreydaz_D/0/1/0/all/0/1\">Dominique Vaufreydaz</a> (M-PSI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Avalanche RL: a Continual Reinforcement Learning Library. (arXiv:2202.13657v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.13657","description":"<p>Continual Reinforcement Learning (CRL) is a challenging setting where an\nagent learns to interact with an environment that is constantly changing over\ntime (the stream of experiences). In this paper, we describe Avalanche RL, a\nlibrary for Continual Reinforcement Learning which allows to easily train\nagents on a continuous stream of tasks. Avalanche RL is based on PyTorch and\nsupports any OpenAI Gym environment. Its design is based on Avalanche, one of\nthe more popular continual learning libraries, which allow us to reuse a large\nnumber of continual learning strategies and improve the interaction between\nreinforcement learning and continual learning researchers. Additionally, we\npropose Continual Habitat-Lab, a novel benchmark and a high-level library which\nenables the usage of the photorealistic simulator Habitat-Sim for CRL research.\nOverall, Avalanche RL attempts to unify under a common framework continual\nreinforcement learning applications, which we hope will foster the growth of\nthe field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucchesi_N/0/1/0/all/0/1\">Nicol&#xf2; Lucchesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding. (arXiv:2203.00680v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00680","description":"<p>Manual annotation of large-scale point cloud dataset for varying tasks such\nas 3D object classification, segmentation and detection is often laborious\nowing to the irregular structure of point clouds. Self-supervised learning,\nwhich operates without any human labeling, is a promising approach to address\nthis issue. We observe in the real world that humans are capable of mapping the\nvisual concepts learnt from 2D images to understand the 3D world. Encouraged by\nthis insight, we propose CrossPoint, a simple cross-modal contrastive learning\napproach to learn transferable 3D point cloud representations. It enables a\n3D-2D correspondence of objects by maximizing agreement between point clouds\nand the corresponding rendered 2D image in the invariant space, while\nencouraging invariance to transformations in the point cloud modality. Our\njoint training objective combines the feature correspondences within and across\nmodalities, thus ensembles a rich learning signal from both 3D point cloud and\n2D image modalities in a self-supervised fashion. Experimental results show\nthat our approach outperforms the previous unsupervised learning methods on a\ndiverse range of downstream tasks including 3D object classification and\nsegmentation. Further, the ablation studies validate the potency of our\napproach for a better point cloud understanding. Code and pretrained models are\navailable at <a href=\"http://github.com/MohamedAfham/CrossPoint.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afham_M/0/1/0/all/0/1\">Mohamed Afham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dissanayake_I/0/1/0/all/0/1\">Isuru Dissanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dissanayake_D/0/1/0/all/0/1\">Dinithi Dissanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharmasiri_A/0/1/0/all/0/1\">Amaya Dharmasiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thilakarathna_K/0/1/0/all/0/1\">Kanchana Thilakarathna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Aware Contrastive Semi-Supervised Learning. (arXiv:2203.02261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02261","description":"<p>Pseudo-label-based semi-supervised learning (SSL) has achieved great success\non raw data utilization. However, its training procedure suffers from\nconfirmation bias due to the noise contained in self-generated artificial\nlabels. Moreover, the model's judgment becomes noisier in real-world\napplications with extensive out-of-distribution data. To address this issue, we\npropose a general method named Class-aware Contrastive Semi-Supervised Learning\n(CCSSL), which is a drop-in helper to improve the pseudo-label quality and\nenhance the model's robustness in the real-world setting. Rather than treating\nreal-world data as a union set, our method separately handles reliable\nin-distribution data with class-wise clustering for blending into downstream\ntasks and noisy out-of-distribution data with image-wise contrastive for better\ngeneralization. Furthermore, by applying target re-weighting, we successfully\nemphasize clean label learning and simultaneously reduce noisy label learning.\nDespite its simplicity, our proposed CCSSL has significant performance\nimprovements over the state-of-the-art SSL methods on the standard datasets\nCIFAR100 and STL10. On the real-world dataset Semi-iNat 2021, we improve\nFixMatch by 9.80% and CoMatch by 3.18%. Code is available\nhttps://github.com/TencentYoutuResearch/Classification-SemiCLS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guannan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1\">Long Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nuclei instance segmentation and classification in histopathology images with StarDist. (arXiv:2203.02284v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02284","description":"<p>Instance segmentation and classification of nuclei is an important task in\ncomputational pathology. We show that StarDist, a deep learning based nuclei\nsegmentation method originally developed for fluorescence microscopy, can be\nextended and successfully applied to histopathology images. This is\nsubstantiated by conducting experiments on the Lizard dataset, and through\nentering the Colon Nuclei Identification and Counting (CoNIC) challenge 2022.\nAt the end of the preliminary test phase of CoNIC, our approach ranked first on\nthe leaderboard for the segmentation and classification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weigert_M/0/1/0/all/0/1\">Martin Weigert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_U/0/1/0/all/0/1\">Uwe Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05151","description":"<p>Current adversarial attack research reveals the vulnerability of\nlearning-based classifiers against carefully crafted perturbations. However,\nmost existing attack methods have inherent limitations in cross-dataset\ngeneralization as they rely on a classification layer with a closed set of\ncategories. Furthermore, the perturbations generated by these methods may\nappear in regions easily perceptible to the human visual system (HVS). To\ncircumvent the former problem, we propose a novel algorithm that attacks\nsemantic similarity on feature representations. In this way, we are able to\nfool classifiers without limiting attacks to a specific dataset. For\nimperceptibility, we introduce the low-frequency constraint to limit\nperturbations within high-frequency components, ensuring perceptual similarity\nbetween adversarial examples and originals. Extensive experiments on three\ndatasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online\nplatforms indicate that our attack can yield misleading and transferable\nadversarial examples across architectures and datasets. Additionally,\nvisualization results and quantitative performance (in terms of four different\nmetrics) show that the proposed algorithm generates more imperceptible\nperturbations than the state-of-the-art methods. Code is made available at.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinliang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weicheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bizhu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.08368","description":"<p>The exponentially large discrete search space in mixed-precision quantization\n(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous\nworks usually resort to iterative search methods on the training set, which\nconsume hundreds or even thousands of GPU-hours. In this study, we reveal that\nsome unique learnable parameters in quantization, namely the scale factors in\nthe quantizer, can serve as importance indicators of a layer, reflecting the\ncontribution of that layer to the final accuracy at certain bit-widths. These\nimportance indicators naturally perceive the numerical transformation during\nquantization-aware training, which can precisely and correctly provide\nquantization sensitivity metrics of layers. However, a deep network always\ncontains hundreds of such indicators, and training them one by one would lead\nto an excessive time cost. To overcome this issue, we propose a joint training\nscheme that can obtain all indicators at once. It considerably speeds up the\nindicators training process by parallelizing the original sequential training\nprocesses. With these learned importance indicators, we formulate the MPQ\nsearch problem as a one-time integer linear programming (ILP) problem. That\navoids the iterative search and significantly reduces search time without\nlimiting the bit-width search space. For example, MPQ search on ResNet18 with\nour indicators takes only 0.06 seconds. Also, extensive experiments show our\napproach can achieve SOTA accuracy on ImageNet for far-ranging models with\nvarious constraints (e.g., BitOps, compress rate).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_K/0/1/0/all/0/1\">Kai Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. (arXiv:2203.08411v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08411","description":"<p>Sequence modeling has demonstrated state-of-the-art performance on natural\nlanguage and document understanding tasks. However, it is challenging to\ncorrectly serialize tokens in form-like documents in practice due to their\nvariety of layout patterns. We propose FormNet, a structure-aware sequence\nmodel to mitigate the suboptimal serialization of forms. First, we design Rich\nAttention that leverages the spatial relationship between tokens in a form for\nmore precise attention score calculation. Second, we construct Super-Tokens for\neach word by embedding representations from their neighboring tokens through\ngraph convolutions. FormNet therefore explicitly recovers local syntactic\ninformation that may have been lost during serialization. In experiments,\nFormNet outperforms existing methods with a more compact model size and less\npre-training data, establishing new state-of-the-art performance on CORD, FUNSD\nand Payment benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dozat_T/0/1/0/all/0/1\">Timothy Dozat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perot_V/0/1/0/all/0/1\">Vincent Perot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guolong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_N/0/1/0/all/0/1\">Nan Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContrastMask: Contrastive Learning to Segment Every Thing. (arXiv:2203.09775v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09775","description":"<p>Partially-supervised instance segmentation is a task which requests\nsegmenting objects from novel unseen categories via learning on limited seen\ncategories with annotated masks thus eliminating demands of heavy annotation\nburden. The key to addressing this task is to build an effective class-agnostic\nmask segmentation model. Unlike previous methods that learn such models only on\nseen categories, in this paper, we propose a new method, named ContrastMask,\nwhich learns a mask segmentation model on both seen and unseen categories under\na unified pixel-level contrastive learning framework. In this framework,\nannotated masks of seen categories and pseudo masks of unseen categories serve\nas a prior for contrastive learning, where features from the mask regions\n(foreground) are pulled together, and are contrasted against those from the\nbackground, and vice versa. Through this framework, feature discrimination\nbetween foreground and background is largely improved, facilitating learning of\nthe class-agnostic mask segmentation model. Exhaustive experiments on the COCO\ndataset demonstrate the superiority of our method, which outperforms previous\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuehui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Human-Gaze-Target Detection with Transformers. (arXiv:2203.10433v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10433","description":"<p>In this paper, we propose an effective and efficient method for\nHuman-Gaze-Target (HGT) detection, i.e., gaze following. Current approaches\ndecouple the HGT detection task into separate branches of salient object\ndetection and human gaze prediction, employing a two-stage framework where\nhuman head locations must first be detected and then be fed into the next gaze\ntarget prediction sub-network. In contrast, we redefine the HGT detection task\nas detecting human head locations and their gaze targets, simultaneously. By\nthis way, our method, named Human-Gaze-Target detection TRansformer or HGTTR,\nstreamlines the HGT detection pipeline by eliminating all other additional\ncomponents. HGTTR reasons about the relations of salient objects and human gaze\nfrom the global image context. Moreover, unlike existing two-stage methods that\nrequire human head locations as input and can predict only one human's gaze\ntarget at a time, HGTTR can directly predict the locations of all people and\ntheir gaze targets at one time in an end-to-end manner. The effectiveness and\nrobustness of our proposed method are verified with extensive experiments on\nthe two standard benchmark datasets, GazeFollowing and VideoAttentionTarget.\nWithout bells and whistles, HGTTR outperforms existing state-of-the-art methods\nby large margins (6.4 mAP gain on GazeFollowing and 10.3 mAP gain on\nVideoAttentionTarget) with a much simpler architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Danyang Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Huiyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11442","description":"<p>This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computation resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\njointly and collaboratively. In detail, AOT employs an identification mechanism\nto associate multiple targets into the same high-dimensional embedding space.\nThus, we can simultaneously process multiple objects' matching and segmentation\ndecoding as efficiently as processing a single object. To sufficiently model\nmulti-object association, a Long Short-Term Transformer (LSTT) is devised to\nconstruct hierarchical matching and propagation. Based on AOT, we further\npropose a more flexible and robust framework, Associating Objects with Scalable\nTransformers (AOST), in which a scalable version of LSTT is designed to enable\nrun-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces\na better layer-wise manner to couple identification and vision embeddings. We\nconduct extensive experiments on multi-object and single-object benchmarks to\nexamine AOT series frameworks. Compared to the state-of-the-art competitors,\nour methods can maintain times of run-time efficiency with superior\nperformance. Notably, we achieve new state-of-the-art performance on three\npopular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test\n(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:\nhttps://github.com/z-x-yang/AOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jiaxu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. (arXiv:2203.11799v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11799","description":"<p>Blind-spot network (BSN) and its variants have made significant advances in\nself-supervised denoising. Nevertheless, they are still bound to synthetic\nnoisy inputs due to less practical assumptions like pixel-wise independent\nnoise. Hence, it is challenging to deal with spatially correlated real-world\nnoise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has\nbeen proposed to remove the spatial correlation of real-world noise. However,\nit is not trivial to integrate PD and BSN directly, which prevents the fully\nself-supervised denoising model on real-world images. We propose an Asymmetric\nPD (AP) to address this issue, which introduces different PD stride factors for\ntraining and inference. We systematically demonstrate that the proposed AP can\nresolve inherent trade-offs caused by specific PD stride factors and make BSN\napplicable to practical scenarios. To this end, we develop AP-BSN, a\nstate-of-the-art self-supervised denoising method for real-world sRGB images.\nWe further propose random-replacing refinement, which significantly improves\nthe performance of our AP-BSN without any additional parameters. Extensive\nstudies demonstrate that our method outperforms the other self-supervised and\neven unpaired denoising methods by a large margin, without using any additional\nknowledge, e.g., noise level, regarding the underlying unknown noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.11834","description":"<p>Models trained in federated settings often suffer from degraded performances\nand fail at generalizing, especially when facing heterogeneous scenarios. In\nthis work, we investigate such behavior through the lens of geometry of the\nloss and Hessian eigenspectrum, linking the model's lack of generalization\ncapacity to the sharpness of the solution. Motivated by prior studies\nconnecting the sharpness of the loss surface and the generalization gap, we\nshow that i) training clients locally with Sharpness-Aware Minimization (SAM)\nor its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on\nthe server-side can substantially improve generalization in Federated Learning\nand help bridging the gap with centralized models. By seeking parameters in\nneighborhoods having uniform low loss, the model converges towards flatter\nminima and its generalization significantly improves in both homogeneous and\nheterogeneous scenarios. Empirical results demonstrate the effectiveness of\nthose optimizers across a variety of benchmark vision datasets (e.g.\nCIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,\nsemantic segmentation, domain generalization).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caldarola_D/0/1/0/all/0/1\">Debora Caldarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1\">Marco Ciccone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11894","description":"<p>In this work we demonstrate the vulnerability of vision transformers (ViTs)\nto gradient-based inversion attacks. During this attack, the original data\nbatch is reconstructed given model weights and the corresponding gradients. We\nintroduce a method, named GradViT, that optimizes random noise into naturally\nlooking images via an iterative process. The optimization objective consists of\n(i) a loss on matching the gradients, (ii) image prior in the form of distance\nto batch-normalization statistics of a pretrained CNN model, and (iii) a total\nvariation regularization on patches to guide correct recovery locations. We\npropose a unique loss scheduling function to overcome local minima during\noptimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and\nobserve unprecedentedly high fidelity and closeness to the original (hidden)\ndata. During the analysis we find that vision transformers are significantly\nmore vulnerable than previously studied CNNs due to the presence of the\nattention mechanism. Our method demonstrates new state-of-the-art results for\ngradient inversion in both qualitative and quantitative metrics. Project page\nat https://gradvit.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hatamizadeh_A/0/1/0/all/0/1\">Ali Hatamizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_H/0/1/0/all/0/1\">Holger Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Daguang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework. (arXiv:2203.11991v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11991","description":"<p>The current popular two-stream, two-stage tracking framework extracts the\ntemplate and the search region features separately and then performs relation\nmodeling, thus the extracted features lack the awareness of the target and have\nlimited target-background discriminability. To tackle the above issue, we\npropose a novel one-stream tracking (OSTrack) framework that unifies feature\nlearning and relation modeling by bridging the template-search image pairs with\nbidirectional information flows. In this way, discriminative target-oriented\nfeatures can be dynamically extracted by mutual guidance. Since no extra heavy\nrelation modeling module is needed and the implementation is highly\nparallelized, the proposed tracker runs at a fast speed. To further improve the\ninference efficiency, an in-network candidate early elimination module is\nproposed based on the strong similarity prior calculated in the one-stream\nframework. As a unified framework, OSTrack achieves state-of-the-art\nperformance on multiple benchmarks, in particular, it shows impressive results\non the one-shot tracking benchmark GOT-10k, i.e., achieving 73.7% AO, improving\nthe existing best result (SwinTrack) by 4.3%. Besides, our method maintains a\ngood performance-speed trade-off and shows faster convergence. The code and\nmodels will be available at https://github.com/botaoye/OSTrack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1\">Botao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bingpeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Portrait Delighting. (arXiv:2203.12088v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12088","description":"<p>We present a deep neural network for removing undesirable shading features\nfrom an unconstrained portrait image, recovering the underlying texture. Our\ntraining scheme incorporates three regularization strategies: masked loss, to\nemphasize high-frequency shading features; soft-shadow loss, which improves\nsensitivity to subtle changes in lighting; and shading-offset estimation, to\nsupervise separation of shading and texture. Our method demonstrates improved\ndelighting quality and generalization when compared with the state-of-the-art.\nWe further demonstrate how our delighting method can enhance the performance of\nlight-sensitive computer vision tasks such as face relighting and semantic\nparsing, allowing them to handle extreme lighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weir_J/0/1/0/all/0/1\">Joshua Weir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Junhong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalmers_A/0/1/0/all/0/1\">Andrew Chalmers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_T/0/1/0/all/0/1\">Taehyun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin. (arXiv:2203.12341v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12341","description":"<p>Only parts of unlabeled data are selected to train models for most\nsemi-supervised learning methods, whose confidence scores are usually higher\nthan the pre-defined threshold (i.e., the confidence margin). We argue that the\nrecognition performance should be further improved by making full use of all\nunlabeled data. In this paper, we learn an Adaptive Confidence Margin (Ada-CM)\nto fully leverage all unlabeled data for semi-supervised deep facial expression\nrecognition. All unlabeled samples are partitioned into two subsets by\ncomparing their confidence scores with the adaptively learned confidence margin\nat each training epoch: (1) subset I including samples whose confidence scores\nare no lower than the margin; (2) subset II including samples whose confidence\nscores are lower than the margin. For samples in subset I, we constrain their\npredictions to match pseudo labels. Meanwhile, samples in subset II participate\nin the feature-level contrastive objective to learn effective facial expression\nfeatures. We extensively evaluate Ada-CM on four challenging datasets, showing\nthat our method achieves state-of-the-art performance, especially surpassing\nfully-supervised baselines in a semi-supervised manner. Ablation study further\nproves the effectiveness of our method. The source code is available at\nhttps://github.com/hangyu94/Ada-CM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hangyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction. (arXiv:2203.12613v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12613","description":"<p>We propose a novel method to reconstruct the 3D shapes of transparent objects\nusing hand-held captured images under natural light conditions. It combines the\nadvantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid\nrepresentation, to simplify the capture setting used in recent contributions.\nAfter obtaining an initial shape through the multi-view silhouettes, we\nintroduce surface-based local MLPs to encode the vertex displacement field\n(VDF) for the reconstruction of surface details. The design of local MLPs\nallows to represent the VDF in a piece-wise manner using two layer MLP\nnetworks, which is beneficial to the optimization algorithm. Defining local\nMLPs on the surface instead of the volume also reduces the searching space.\nSuch a hybrid representation enables us to relax the ray-pixel correspondences\nthat represent the light path constraint to our designed ray-cell\ncorrespondences, which significantly simplifies the implementation of\nsingle-image based environment matting algorithm. We evaluate our\nrepresentation and reconstruction algorithm on several transparent objects with\nground truth models. Our experiments show that our method can produce\nhigh-quality reconstruction results superior to state-of-the-art methods using\na simplified data acquisition setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiamin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}